{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"6751fcad-3546-4b99-aff0-7a21f136efd7","_cell_guid":"d492d485-cf5e-4b5d-bab5-14cc831d06f5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-09T04:32:03.8277Z","iopub.execute_input":"2021-07-09T04:32:03.82811Z","iopub.status.idle":"2021-07-09T04:32:03.854573Z","shell.execute_reply.started":"2021-07-09T04:32:03.828026Z","shell.execute_reply":"2021-07-09T04:32:03.853611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==1.14\n!pip install pytorch==1.6.0\n!pip install tqdm==4.47\n!pip install transformers==3.0","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:03.856148Z","iopub.execute_input":"2021-07-09T04:32:03.856539Z","iopub.status.idle":"2021-07-09T04:32:23.344797Z","shell.execute_reply.started":"2021-07-09T04:32:03.8565Z","shell.execute_reply":"2021-07-09T04:32:23.34378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -V","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:23.348783Z","iopub.execute_input":"2021-07-09T04:32:23.349086Z","iopub.status.idle":"2021-07-09T04:32:23.98617Z","shell.execute_reply.started":"2021-07-09T04:32:23.349056Z","shell.execute_reply":"2021-07-09T04:32:23.98524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = '/kaggle/input/nlp-news-text/train_set.csv'\ntest_set_a = '/kaggle/input/nlp-news-text/test_a.csv'\ntest_set_b = '/kaggle/input/nlp-news-text/test_b.csv'","metadata":{"_uuid":"a19996b2-ff2b-48fe-9bc0-8e49f0dc2805","_cell_guid":"b7fca3c8-527e-4680-a73d-65503c66cf90","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-09T04:32:23.989706Z","iopub.execute_input":"2021-07-09T04:32:23.989987Z","iopub.status.idle":"2021-07-09T04:32:23.996362Z","shell.execute_reply.started":"2021-07-09T04:32:23.989957Z","shell.execute_reply":"2021-07-09T04:32:23.995557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_rows = 3000\ntest_rows = 300","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport random\n\n\nimport torch\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n\n# set seed\nseed = 666\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\n\n# set cuda\ngpu = 0\nuse_cuda = gpu >= 0 and torch.cuda.is_available()\nif use_cuda:\n    torch.cuda.set_device(gpu)\n    device = torch.device(\"cuda\", gpu)\nelse:\n    device = torch.device(\"cpu\")\nprint(\"Use cuda: %s, gpu id: %d.\"%(use_cuda, gpu))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:23.997743Z","iopub.execute_input":"2021-07-09T04:32:23.998489Z","iopub.status.idle":"2021-07-09T04:32:24.459247Z","shell.execute_reply.started":"2021-07-09T04:32:23.998453Z","shell.execute_reply":"2021-07-09T04:32:24.45839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir user_data\n!mkdir prediction_result\n!mkdir model","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:24.460489Z","iopub.execute_input":"2021-07-09T04:32:24.461021Z","iopub.status.idle":"2021-07-09T04:32:26.37529Z","shell.execute_reply.started":"2021-07-09T04:32:24.46098Z","shell.execute_reply":"2021-07-09T04:32:26.374331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 数据加载","metadata":{}},{"cell_type":"markdown","source":"## 构造全局变量","metadata":{}},{"cell_type":"code","source":"_global_dict = {}\nclass gl(object):\n    def set_value(name, value):\n        _global_dict[name] = value\n\n    def get_value(name, defValue=None):\n        try:\n            return _global_dict[name]\n        except KeyError:\n            return defValue\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:26.378762Z","iopub.execute_input":"2021-07-09T04:32:26.379061Z","iopub.status.idle":"2021-07-09T04:32:26.387128Z","shell.execute_reply.started":"2021-07-09T04:32:26.379032Z","shell.execute_reply":"2021-07-09T04:32:26.386326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 加载数据方法","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm, tqdm_notebook\n\n# ### 定义 sentence_split，把文章划分为句子\n# \n# 作用是：根据一篇文章，把这篇文章分割成多个句子\n# text 是一个新闻的文章\n# vocab 是词典\n# max_sent_len 表示每句话的长度\n# max_segment 表示最多有几句话\n# 最后返回的 segments 是一个list，其中每个元素是 tuple：(句子长度，句子本身)\ndef sentence_split(text, vocab, max_sent_len=254, max_segment=16):\n    \n    words = text.strip().split()\n    document_len = len(words)\n    # 划分句子的索引，句子长度为 max_sent_len\n    index = list(range(0, document_len, max_sent_len))\n    index.append(document_len)\n\n    segments = []\n    for i in range(len(index) - 1):\n        # 根据索引划分句子\n        segment = words[index[i]: index[i + 1]]\n        assert len(segment) > 0\n        # 把出现太少的词替换为 UNK\n        segment = [word if word in vocab._id2word else '<UNK>' for word in segment]\n        # 添加 tuple:(句子长度，句子本身)\n        segments.append([len(segment), segment])\n\n    assert len(segments) > 0\n    # 如果大于 max_segment 句话，则局数减少一半，返回一半的句子\n    if len(segments) > max_segment:\n        segment_ = int(max_segment / 2)\n        return segments[:segment_] + segments[-segment_:]\n    else:\n        # 否则返回全部句子\n        return segments\n    \n# ### 定义 get_examples\n# 里面调用 sentence_split\n\n# 最后返回的数据是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n# 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, token_type_ids)\ndef get_examples(data, word_encoder, vocab, max_sent_len=256, max_segment=8):\n    label2id = vocab.label2id\n    examples = []\n\n    for text, label in zip(data['text'], data['label']):\n        # label\n        id = label2id(label)\n\n        # sents_words: 是一个list，其中每个元素是 tuple：(句子长度，句子本身)\n        # 由于后面需要添加 一个 CLS 和一个 SEP 的token，所以句子的长度为 max_sent_len-2\n        sents_words = sentence_split(text, vocab, max_sent_len-2, max_segment)\n        doc = []\n        for sent_len, sent_words in sents_words:\n            # 把 word 转为 id\n            token_ids = word_encoder.encode(sent_words)\n            # 这里重新取 sent_len，是因为：在上一步的 encode 函数中，\n            # 会给每个句子加上 CLS 和 SEP 的 token，句子的长度会增加 2。\n            sent_len = len(token_ids)\n            # 构造句子 id\n            token_type_ids = [0] * sent_len\n            doc.append([sent_len, token_ids, token_type_ids])\n        examples.append([id, len(doc), doc])\n\n    print('Total %d docs.' % len(examples))\n    return examples\n\n# ### 定义 batch_slice\n\n# build loader\n# data 参数就是 get_examples() 得到的\n# data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n# 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, token_type_ids)\ndef batch_slice(data, batch_size):\n    batch_num = int(np.ceil(len(data) / float(batch_size)))\n    for i in range(batch_num):\n        # 如果 i < batch_num - 1，那么大小为 batch_size，否则就是最后一批数据\n        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n        docs = [data[i * batch_size + b] for b in range(cur_batch_size)]\n\n        yield docs\n\n# ### 定义 data_iter\n# 里面调用 batch_slice\n\n\n# data 参数就是 get_examples() 得到的\n# data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n# 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, token_type_ids)\ndef data_iter(data, batch_size, shuffle=True, noise=1.0):\n    \"\"\"\n    randomly permute data, then sort by source length, and partition into batches\n    ensure that the length of  sentences in each batch\n    \"\"\"\n\n    batched_data = []\n    if shuffle:\n        # 这里是打乱所有数据\n        np.random.shuffle(data)\n        # lengths 表示的是 每篇文章的句子数量\n        lengths = [example[1] for example in data] \n        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) for l in lengths]\n        sorted_indices = np.argsort(noisy_lengths).tolist()\n        sorted_data = [data[i] for i in sorted_indices]\n    else:\n        sorted_data = data\n    # 把 batch 的数据放进一个 list    \n    batched_data.extend(list(batch_slice(sorted_data, batch_size)))\n\n    if shuffle:\n        # 打乱 多个 batch\n        np.random.shuffle(batched_data)\n\n    for batch in batched_data:\n        yield batch\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:26.391715Z","iopub.execute_input":"2021-07-09T04:32:26.392066Z","iopub.status.idle":"2021-07-09T04:32:26.410506Z","shell.execute_reply.started":"2021-07-09T04:32:26.392028Z","shell.execute_reply":"2021-07-09T04:32:26.409665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 构造字典","metadata":{}},{"cell_type":"markdown","source":"Vocab 的作用是：\n\n- 创建 词 和 index 对应的字典，这里包括 2 份字典，分别是：_id2word 和 _id2extword。\n- 其中 _id2word 是从新闻得到的， 把词频小于 5 的词替换为了 UNK。对应到模型输入的 batch_inputs1。\n- _id2extword 是从 word2vec.txt 中得到的，有 5976 个词。对应到模型输入的 batch_inputs2。\n- 后面会有两个 embedding 层，其中 _id2word 对应的 embedding 是可学习的，_id2extword 对应的 embedding 是从文件中加载的，是固定的。\n- 创建 label 和 index 对应的字典。\n- 上面这些字典，都是基于train_data创建的。","metadata":{}},{"cell_type":"code","source":"\"\"\"字典\"\"\"\nfrom collections import Counter\nfrom transformers import BasicTokenizer\n\n\nbasic_tokenizer = BasicTokenizer()\n\nclass Vocab():\n    def __init__(self, train_data):\n        self.min_count = 5\n        self.pad = 0\n        self.unk = 1\n        self._id2word = ['[PAD]', '[UNK]']\n        self._id2extword = ['[PAD]', '[UNK]']\n\n        self._id2label = []\n        self.target_names = []\n\n        self.build_vocab(train_data)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        # 创建词和 index 对应的字典\n        self._word2id = reverse(self._id2word)\n        # 创建 label 和 index 对应的字典\n        self._label2id = reverse(self._id2label)\n\n        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n    \n    # 创建词典\n    def build_vocab(self, data):\n        self.word_counter = Counter()\n        # 计算每个词出现的次数\n        for text in data['text']:\n            words = text.split()\n            for word in words:\n                self.word_counter[word] += 1\n\n        for word, count in self.word_counter.most_common():\n            # 去掉频次小于 min_count = 5 的词，把词存到 _id2word\n            if count >= self.min_count:\n                self._id2word.append(word)\n\n        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n\n        self.label_counter = Counter(data['label'])\n\n        for label in range(len(self.label_counter)):\n            count = self.label_counter[label] # 取出 label 对应的次数\n            self._id2label.append(label)\n            self.target_names.append(label2name[label])# 根据label数字取出对应的名字\n    \n    # 第一行分别是单词数量、词向量维度\n    def load_pretrained_embs(self, embfile):\n        with open(embfile, encoding='utf-8') as f:\n            lines = f.readlines()\n            items = lines[0].split()\n            word_count, embedding_dim = int(items[0]), int(items[1])\n\n        index = len(self._id2extword)# 首先添加第一列的单词\n        embeddings = np.zeros((word_count + index, embedding_dim))\n        # 下面的代码和 vocab.txt 的结构有关\n        for line in lines[1:]:\n            values = line.split()\n            self._id2extword.append(values[0])# 首先添加第一列的单词\n            vector = np.array(values[1:], dtype='float64')# 然后添加后面 100 列的词向量\n            embeddings[self.unk] += vector\n            embeddings[index] = vector\n            index += 1\n         # unk 的词向量是所有词的平均\n        embeddings[self.unk] = embeddings[self.unk] / word_count\n        embeddings = embeddings / np.std(embeddings)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        self._extword2id = reverse(self._id2extword)\n\n        assert len(set(self._id2extword)) == len(self._id2extword)\n\n        return embeddings\n    \n    # 根据单词得到 id\n    def word2id(self, xs):\n        if isinstance(xs, list):\n            return [self._word2id.get(x, self.unk) for x in xs]\n        return self._word2id.get(xs, self.unk)\n    \n    # 根据 label 得到 id\n    def extword2id(self, xs):\n        if isinstance(xs, list):\n            return [self._extword2id.get(x, self.unk) for x in xs]\n        return self._extword2id.get(xs, self.unk)\n\n    def label2id(self, xs):\n        if isinstance(xs, list):\n            return [self._label2id.get(x, self.unk) for x in xs]\n        return self._label2id.get(xs, self.unk)\n\n    @property\n    def word_size(self):\n        return len(self._id2word)\n\n    @property\n    def extword_size(self):\n        return len(self._id2extword)\n\n    @property\n    def label_size(self):\n        return len(self._id2label)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:26.41252Z","iopub.execute_input":"2021-07-09T04:32:26.413123Z","iopub.status.idle":"2021-07-09T04:32:28.036261Z","shell.execute_reply.started":"2021-07-09T04:32:26.413083Z","shell.execute_reply":"2021-07-09T04:32:28.035156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 模型及各个模块","metadata":{}},{"cell_type":"code","source":"import logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertModel\n\n\nbert_path = '/kaggle/input/nlp-news-text/bert-mini/bert-mini/'\ndropout = 0.15\n\nsent_hidden_size = 256\nsent_num_layers = 2\n\nclass WhitespaceTokenizer():\n    \"\"\"WhitespaceTokenizer with vocab.\"\"\"\n\n    def __init__(self):\n        vocab_file = bert_path + 'vocab.txt'\n        self._token2id = self.load_vocab(vocab_file)\n        # 构造从 id 到 token 的映射\n        self._id2token = {v: k for k, v in self._token2id.items()}\n        self.max_len = 256\n        self.unk = 1 # UNK 在词典中的索引是 1\n\n        logging.info(\"Build Bert vocab with size %d.\" % (self.vocab_size))\n    \n    # 加载词典，返回的是 dict。key 是单词，value 是索引。从 token 到 id 的映射\n    def load_vocab(self, vocab_file):\n        f = open(vocab_file, 'r')\n        lines = f.readlines()\n        lines = list(map(lambda x: x.strip(), lines))\n        vocab = dict(zip(lines, range(len(lines))))\n        return vocab\n\n    def tokenize(self, tokens):\n        assert len(tokens) <= self.max_len - 2\n        # 在句子前面加上 CLS，最后加上 SEP \n        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n         # 把 token 转为 id\n        output_tokens = self.token2id(tokens)\n        return output_tokens\n    \n    # 把 token 转为 id，如果没有出现过的词，转为 UNK 对应的 id:1\n    def token2id(self, xs):\n        if isinstance(xs, list):\n            return [self._token2id.get(x, self.unk) for x in xs]\n        return self._token2id.get(xs, self.unk)\n\n    @property\n    def vocab_size(self):\n        return len(self._id2token)\n\n\nclass WordBertEncoder(nn.Module):\n    def __init__(self):\n        super(WordBertEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        self.tokenizer = WhitespaceTokenizer()\n        # 加载 Bert 模型\n        self.bert = BertModel.from_pretrained(bert_path)\n\n        self.pooled = False\n        logging.info('Build Bert encoder with pooled {}.'.format(self.pooled))\n\n    def encode(self, tokens):\n        tokens = self.tokenizer.tokenize(tokens)\n        return tokens\n\n    # 如果参数名字里，包含 ['bias', 'LayerNorm.weight']，那么没有 decay\n    # 其他参数都有 0.01 的 decay\n    def get_bert_parameters(self):\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_parameters = [\n            {'params': [p for n, p in self.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n             'weight_decay': 0.01},\n            {'params': [p for n, p in self.bert.named_parameters() if any(nd in n for nd in no_decay)],\n             'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    def forward(self, input_ids, token_type_ids):\n        # bert_len 是句子的长度\n        # input_ids: sen_num * bert_len\n        # token_type_ids: sen_num  * bert_len\n\n    \n        # 256 是 hidden_size\n        # sequence_output：sen_num * bert_len * 256。是最后一个 Encoder 输出的 hidden-states\n        # pooled_output：sen_num * 256。首先取最后一个 Encoder 层输出的 hidden-states 的第一个位置对应的 hidden-state，\n        # 也就是 CLS 对应的 hidden state，是一个 256 维的向量。经过线性变换和 Tanh 激活函数得到最终的 256 维向量。\n        # 可以直接用于分类\n        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n        # Bert 模型的输出是一个 tuple，包含 4 个元素：last_hidden_state、pooler_output、hidden_states、attentions\n        \n        if self.pooled:\n            reps = pooled_output             # 取第一个元素的 hidden state： sen_num * 256\n        else:\n            reps = sequence_output[:, 0, :]  # 取第一个元素的 hidden state： sen_num * 256\n\n        if self.training:\n            reps = self.dropout(reps)\n\n        return reps # sen_num * 256\n\nclass SentEncoder(nn.Module):\n    def __init__(self, sent_rep_size):\n        super(SentEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        self.sent_lstm = nn.LSTM(\n            input_size=sent_rep_size, # 每个句子经过 CNN 后得到 256 维向量\n            hidden_size=sent_hidden_size,# 输出的维度\n            num_layers=sent_num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n\n    def forward(self, sent_reps, sent_masks):\n        # sent_reps:  b * doc_len * sent_rep_size\n        # sent_masks: b * doc_len\n        # sent_hiddens:  b * doc_len * hidden*2\n        # sent_hiddens:  batch, seq_len, num_directions * hidden_size\n        sent_hiddens, _ = self.sent_lstm(sent_reps)  \n        # 对应相乘，用到广播，是为了只保留有句子的位置的数值\n        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n        \n        if self.training:\n            sent_hiddens = self.dropout(sent_hiddens)\n\n        return sent_hiddens\n\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        self.weight.data.normal_(mean=0.0, std=0.05)\n\n        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n        b = np.zeros(hidden_size, dtype=np.float32)\n        self.bias.data.copy_(torch.from_numpy(b))\n\n        self.query = nn.Parameter(torch.Tensor(hidden_size))\n        self.query.data.normal_(mean=0.0, std=0.05)\n\n    def forward(self, batch_hidden, batch_masks):\n        # batch_hidden: b * doc_len * hidden_size (2 * hidden_size of lstm)\n        # batch_masks:  b x doc_len\n\n        # linear\n        # key： b * doc_len * hidden\n        key = torch.matmul(batch_hidden, self.weight) + self.bias \n\n        # compute attention\n        # matmul 会进行广播\n        #outputs: b * doc_len\n        outputs = torch.matmul(key, self.query)  \n        # 1 - batch_masks 就是取反，把没有单词的句子置为 0\n        # masked_fill 的作用是 在 为 1 的地方替换为 value: float(-1e32)\n        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n        #attn_scores：b * doc_len\n        attn_scores = F.softmax(masked_outputs, dim=1)  \n\n        # 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0\n        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n\n        # sum weighted sources\n        # masked_attn_scores.unsqueeze(1)：# b * 1 * doc_len\n        # key：b * doc_len * hidden\n        # batch_outputs：b * hidden\n        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)  \n\n        return batch_outputs, attn_scores\n\n\nclass Model(nn.Module):\n    \"\"\"Model Complete Flow\"\"\"\n\n    def __init__(self, vocab):\n        super(Model, self).__init__()\n        self.sent_rep_size = 256 # 经过 Bert 后得到的 256 维向量\n        self.doc_rep_size = sent_hidden_size * 2 # lstm 最后输出的向量长度\n        self.all_parameters = {}\n        parameters = []\n        self.word_encoder = WordBertEncoder()\n        bert_parameters = self.word_encoder.get_bert_parameters()\n        \n        parameters.extend(list(filter(lambda p: p.requires_grad, self.word_encoder.parameters())))\n\n        self.sent_encoder = SentEncoder(self.sent_rep_size)\n        self.sent_attention = Attention(self.doc_rep_size)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n        # doc_rep_size\n        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n\n        if use_cuda:\n            self.to(device)\n\n        if len(parameters) > 0:\n            self.all_parameters[\"basic_parameters\"] = parameters\n\n        print('Build model with cnn word encoder, lstm sent encoder.')\n\n        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n        logging.info('Model param num: %.2f M.' % (para_num / 1e6))\n        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n        print('Model param num: %.2f M.' % (para_num / 1e6))\n\n    def forward(self, batch_inputs):\n        # batch_inputs(batch_inputs1, token_type_ids): b * doc_len * sentence_len\n        # batch_masks : b * doc_len * sentence_len\n        batch_inputs1, token_type_ids, batch_masks = batch_inputs\n        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n        # batch_inputs1: sentence_num * sentence_len\n        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  \n        # token_type_ids: sentence_num * sentence_len\n        token_type_ids = token_type_ids.view(batch_size * max_doc_len, max_sent_len)\n        # batch_masks: sentence_num * sentence_len \n        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  \n        # sent_reps: sentence_num * sentence_rep_size\n        # sen_num * (3*out_channel) =  sen_num * 256\n        sent_reps = self.word_encoder(batch_inputs1, token_type_ids) \n        \n        \n        # sent_reps：b * doc_len * sent_rep_size\n        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  \n        # batch_masks：b * doc_len * max_sent_len\n        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  \n        # sent_masks：b * doc_len any(2) 表示在 第二个维度上判断\n        # 表示如果如果一个句子中有词 true，那么这个句子就是 true，用于给 lstm 过滤\n        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n        # sent_hiddens:  batch, seq_len, 2 * hidden_size\n        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  \n        \n        \n        # doc_reps: b * (2 * hidden_size)\n        # atten_scores: b * doc_len\n        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  \n        \n        # b * num_labels\n        batch_outputs = self.out(doc_reps)  \n\n        return batch_outputs","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.037932Z","iopub.execute_input":"2021-07-09T04:32:28.038286Z","iopub.status.idle":"2021-07-09T04:32:28.078238Z","shell.execute_reply.started":"2021-07-09T04:32:28.038253Z","shell.execute_reply":"2021-07-09T04:32:28.077384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 优化器","metadata":{}},{"cell_type":"code","source":"\"\"\"优化器\"\"\"\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nlearning_rate = 2e-4\nbert_lr = 5e-5\ndecay = .75\ndecay_step = 5000\n\nclass Optimizer:\n    \"\"\"优化器\"\"\"\n\n    def __init__(self, model_parameters, steps):\n        self.all_params = []\n        self.optims = []\n        self.schedulers = []\n\n        for name, parameters in model_parameters.items():\n            if name.startswith(\"basic\"):\n                optim = torch.optim.Adam(parameters, lr=learning_rate)\n                self.optims.append(optim)\n\n                l = lambda step: decay ** (step // decay_step)\n                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n                self.schedulers.append(scheduler)\n                self.all_params.extend(parameters)\n            elif name.startswith(\"bert\"):\n                optim_bert = AdamW(parameters, bert_lr, eps=1e-8)\n                self.optims.append(optim_bert)\n\n                scheduler_bert = get_linear_schedule_with_warmup(optim_bert, 0, steps)\n                self.schedulers.append(scheduler_bert)\n\n                for group in parameters:\n                    for p in group['params']:\n                        self.all_params.append(p)\n            else:\n                Exception(\"no nameed parameters.\")\n\n        self.num = len(self.optims)\n\n    def step(self):\n        for optim, scheduler in zip(self.optims, self.schedulers):\n            optim.step()\n            scheduler.step()\n            optim.zero_grad()\n\n    def zero_grad(self):\n        for optim in self.optims:\n            optim.zero_grad()\n\n    def get_lr(self):\n        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n        lr = ' %.5f' * self.num\n        res = lr % lrs\n        return res","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.079654Z","iopub.execute_input":"2021-07-09T04:32:28.080214Z","iopub.status.idle":"2021-07-09T04:32:28.094477Z","shell.execute_reply.started":"2021-07-09T04:32:28.080175Z","shell.execute_reply":"2021-07-09T04:32:28.093185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 训练器","metadata":{"execution":{"iopub.status.busy":"2021-07-08T09:41:43.440995Z","iopub.execute_input":"2021-07-08T09:41:43.441382Z","iopub.status.idle":"2021-07-08T09:41:43.44576Z","shell.execute_reply.started":"2021-07-08T09:41:43.441348Z","shell.execute_reply":"2021-07-08T09:41:43.444565Z"}}},{"cell_type":"code","source":"\"\"\"训练器\"\"\"\n\n\nimport time\n\nimport torch.nn as nn\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n\nclip = 5.0\nepochs = 18\n# epochs = 1\nearly_stops = 3\nlog_interval = 50\n\ntest_batch_size = 32\ntrain_batch_size = 32\n# test_batch_size = 8\n# train_batch_size = 8\n\nclass Trainer():\n    \"\"\"训练器\"\"\"\n\n    def __init__(self, model, vocab, is_train=True):\n\n        train_data = gl.get_value('train_data')\n        dev_data = gl.get_value('dev_data')\n        test_data = gl.get_value('test_data')\n\n        self.model = model\n        self.report = True\n        # get_examples() 返回的结果是 一个 list\n        # 每个元素是一个 tuple: (label, 句子数量，doc)\n        # 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, token_type_ids)\n        if is_train:\n            self.train_data = get_examples(train_data, model.word_encoder, vocab)\n            self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n            self.dev_data = get_examples(dev_data, model.word_encoder, vocab)\n        self.test_data = get_examples(test_data, model.word_encoder, vocab)\n\n        # criterion\n        self.criterion = nn.CrossEntropyLoss()\n\n        # label name\n        self.target_names = vocab.target_names\n\n        # optimizer\n        if is_train:\n            self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)\n\n        # count\n        self.step = 0\n        self.early_stop = -1\n        self.best_train_f1, self.best_dev_f1 = 0, 0\n        self.last_epoch = epochs\n\n    def train(self):\n        logging.info('Start training...')\n        for epoch in range(1, epochs + 1):\n            train_f1 = self._train(epoch)\n\n            dev_f1 = self._eval(epoch)\n\n            if self.best_dev_f1 <= dev_f1:\n                logging.info(\n                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n                \n                save_model = gl.get_value('save_model')\n                torch.save(self.model.state_dict(), save_model)\n\n                self.best_train_f1 = train_f1\n                self.best_dev_f1 = dev_f1\n                self.early_stop = 0\n            else:\n                self.early_stop += 1\n                if self.early_stop == early_stops:\n                    logging.info(\n                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n                    self.last_epoch = epoch\n                    break\n    def test(self):\n        save_model = gl.get_value('save_model')\n        self.model.load_state_dict(torch.load(save_model))\n        self._eval(self.last_epoch + 1, test=True)\n\n    def _train(self, epoch):\n        self.optimizer.zero_grad()\n        self.model.train()\n\n        start_time = time.time()\n        epoch_start_time = time.time()\n        overall_losses = 0\n        losses = 0\n        batch_idx = 1\n        y_pred = []\n        y_true = []\n        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n            torch.cuda.empty_cache()\n            # batch_inputs: (batch_inputs1, token_type_ids, batch_masks)\n            # 形状都是：batch_size * doc_len * sent_len\n            # batch_labels: batch_size\n            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n            # batch_outputs：b * num_labels\n            batch_outputs = self.model(batch_inputs)\n            # criterion 是 CrossEntropyLoss，真实标签的形状是：N\n            # 预测标签的形状是：(N,C)\n            loss = self.criterion(batch_outputs, batch_labels)\n            \n            loss.backward()\n\n            loss_value = loss.detach().cpu().item()\n            losses += loss_value\n            overall_losses += loss_value\n            # 把预测值转换为一维，方便下面做 classification_report，计算 f1\n            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n            y_true.extend(batch_labels.cpu().numpy().tolist())\n            # 梯度裁剪\n            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n                optimizer.step()\n                scheduler.step()\n            self.optimizer.zero_grad()\n\n            self.step += 1\n\n            if batch_idx % log_interval == 0:\n                elapsed = time.time() - start_time\n                \n                lrs = self.optimizer.get_lr()\n                logging.info(\n                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n                        epoch, self.step, batch_idx, self.batch_num, lrs,\n                        losses / log_interval,\n                        elapsed / log_interval))\n                \n                losses = 0\n                start_time = time.time()\n                \n            batch_idx += 1\n            \n        overall_losses /= self.batch_num\n        during_time = time.time() - epoch_start_time\n\n        # reformat 保留 4 位数字\n        overall_losses = reformat(overall_losses, 4)\n        score, f1 = get_score(y_true, y_pred)\n\n        logging.info(\n            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n                                                                                  overall_losses,\n                                                                                  during_time))\n        # 如果预测和真实的标签都包含相同的类别数目，才能调用 classification_report                                                                        \n        if set(y_true) == set(y_pred) and self.report:\n            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n            logging.info('\\n' + report)\n\n        return f1\n\n     # 这里验证集、测试集都使用这个函数，通过 test 来区分使用哪个数据集\n    def _eval(self, epoch, test=False):\n        self.model.eval()\n        start_time = time.time()\n        data = self.test_data if test else self.dev_data\n        y_pred = []\n        y_true = []\n        with torch.no_grad():\n            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n                torch.cuda.empty_cache()\n                            # batch_inputs: (batch_inputs1, token_type_ids, batch_masks)\n            # 形状都是：batch_size * doc_len * sent_len\n            # batch_labels: batch_size                                                                  \n                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n                # batch_outputs：b * num_labels                                                                  \n                batch_outputs = self.model(batch_inputs)\n                # 把预测值转换为一维，方便下面做 classification_report，计算 f1                                                                  \n                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n                y_true.extend(batch_labels.cpu().numpy().tolist())\n\n            score, f1 = get_score(y_true, y_pred)\n\n            during_time = time.time() - start_time\n            \n            if test:\n                save_test = gl.get_value('save_test')\n                df = pd.DataFrame({'label': y_pred})\n                df.to_csv(save_test, index=False, sep=',')\n            else:\n                logging.info('| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1, during_time))\n                                                                                  \n                if set(y_true) == set(y_pred) and self.report:\n                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n                    logging.info('\\n' + report)\n\n        return f1\n\n    # data 参数就是 get_examples() 得到的，经过了分 batch\n    # batch_data是一个 list，每个元素是一个 tuple: (label, 句子数量，doc)\n    # 其中 doc 又是一个 list，每个 元素是一个 tuple: (句子长度，word_ids, token_type_ids)\n    def batch2tensor(self, batch_data):\n        '''\n            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n        '''\n        batch_size = len(batch_data)\n        doc_labels = []\n        doc_lens = []\n        doc_max_sent_len = []\n        for doc_data in batch_data:\n            doc_labels.append(doc_data[0])\n            doc_lens.append(doc_data[1])\n            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n            max_sent_len = max(sent_lens)\n            doc_max_sent_len.append(max_sent_len)\n\n        max_doc_len = max(doc_lens)\n        max_sent_len = max(doc_max_sent_len)\n\n        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n        batch_labels = torch.LongTensor(doc_labels)\n\n        for b in range(batch_size):\n            for sent_idx in range(doc_lens[b]):\n                sent_data = batch_data[b][2][sent_idx]\n                for word_idx in range(sent_data[0]):\n                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n                    batch_masks[b, sent_idx, word_idx] = 1\n\n\n        if use_cuda:\n            batch_inputs1 = batch_inputs1.to(device)\n            batch_inputs2 = batch_inputs2.to(device)\n            batch_masks = batch_masks.to(device)\n            batch_labels = batch_labels.to(device)\n\n        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels\n\ndef get_score(y_ture, y_pred):\n    y_ture = np.array(y_ture)\n    y_pred = np.array(y_pred)\n    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n    p = precision_score(y_ture, y_pred, average='macro') * 100\n    r = recall_score(y_ture, y_pred, average='macro') * 100\n\n    return str((reformat(p, 2), reformat(r, 2), reformat(f1, 2))), reformat(f1, 2)\n\n\ndef reformat(num, n):\n    return float(format(num, '0.' + str(n) + 'f'))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.096118Z","iopub.execute_input":"2021-07-09T04:32:28.096525Z","iopub.status.idle":"2021-07-09T04:32:28.136814Z","shell.execute_reply.started":"2021-07-09T04:32:28.096487Z","shell.execute_reply":"2021-07-09T04:32:28.136053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 一次训练全流程","metadata":{}},{"cell_type":"markdown","source":"#### K折交叉验证\n初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的 ","metadata":{}},{"cell_type":"code","source":"\"\"\"训练流程\"\"\"\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n\ndef dataset_split(data_file, test_data_file, fold_num):\n    \"\"\"划分数据集\"\"\"\n\n    # train data\n    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8',nrows=train_rows)\n    #f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8', nrows=3000) # 小规模数据测试\n\n    traincsv_texts = f['text'].tolist()\n    traincsv_labels = f['label'].tolist()\n\n    # 交叉验证数据集，随机采样\n    fold_idx = {}\n    # StratifiedKFold 将X_train和 X_test 做有放回抽样，随机分三次，取出索引\n    kfold = StratifiedKFold(fold_num, shuffle=True, random_state=seed)\n\n    for fold_i, [train_idx, val_idx] in enumerate(kfold.split(traincsv_texts, traincsv_labels)):\n\n        logging.info(\"Fold id: %s, Train lens %s, Val lens %s\", str(fold_i), str(len(train_idx)), str(len(val_idx)))\n        # print(val_idx[:10])\n\n        # shuffle\n        np.random.seed(seed)\n        np.random.shuffle(train_idx)\n        np.random.seed(seed)\n        np.random.shuffle(val_idx)\n        # 交叉验证保留取得的索引,根据索引再取数据\n        fold_idx[fold_i] = {'train': train_idx, 'val': val_idx}\n\n    # test data\n    f = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8',nrows=test_rows)\n    #f = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8', nrows=300) # 小规模数据测试\n\n    texts = f['text'].tolist()\n    test_data = {'label': [0] * len(texts), 'text': texts}\n    print(\"Test lens %s\"%str(len(texts)))\n\n    gl.set_value('test_data', test_data)\n\n    return [traincsv_texts, traincsv_labels, fold_idx, test_data]\n\n\n\ndef train_flow(data_file, test_data_file, fold_num, run_fold, save_name, is_train=True):\n    \"\"\"训练全流程\"\"\"\n\n    # 划分数据集\n    [traincsv_texts, traincsv_labels, fold_idx, test_data] = dataset_split(data_file, test_data_file, fold_num)\n\n\n    for fold_i in range(fold_num):\n        if fold_i not in run_fold:\n            continue\n\n        print(\"======Fold id: %s, Start Data Loader and Encoder\"%str(fold_i))\n        train_idx = fold_idx[fold_i]['train']\n        val_idx = fold_idx[fold_i]['val']\n\n        labels = []\n        texts = []\n        for idx in train_idx:\n            labels.append(traincsv_labels[idx])\n            texts.append(traincsv_texts[idx])\n        train_data = {'label': labels, 'text': texts}\n    \n        labels = []\n        texts = []\n        for idx in val_idx:\n            labels.append(traincsv_labels[idx])\n            texts.append(traincsv_texts[idx])\n        dev_data = {'label': labels, 'text': texts}\n\n        vocab = Vocab(train_data)\n        model = Model(vocab)\n\n\n        save_model = './model/' + save_name + '_' + str(fold_i) + '.bin'\n        save_test = './user_data/' + save_name + '_' + str(fold_i) + '.csv'\n\n        gl.set_value('train_data', train_data)\n        gl.set_value('dev_data', dev_data)\n        gl.set_value('save_model', save_model)\n        gl.set_value('save_test', save_test)\n\n        trainer = Trainer(model, vocab, is_train)\n\n        # train\n        if is_train:\n            print(\"======Fold id: %s, Start Training \"%str(fold_i))\n            trainer.train()\n\n        # test\n        print(\"======Fold id: %s, Start Testing \"%str(fold_i))\n        trainer.test()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.138058Z","iopub.execute_input":"2021-07-09T04:32:28.138459Z","iopub.status.idle":"2021-07-09T04:32:28.163224Z","shell.execute_reply.started":"2021-07-09T04:32:28.138424Z","shell.execute_reply":"2021-07-09T04:32:28.162369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 伪标签标注","metadata":{}},{"cell_type":"code","source":"def pseudo_label(save_name, run_fold, data_file, test_file, data_file_pseudo):\n    \"\"\"伪标签生成\"\"\"\n\n    save_tests = {}\n    for fold_i in run_fold:\n        save_test = './user_data/' + save_name + '_' + str(fold_i) + '.csv'\n        save_tests[fold_i] = save_test\n\n\n    df_merge = pd.DataFrame()\n    for fold_i in run_fold:\n        df = pd.read_csv(save_tests[fold_i])\n        col = 'label_'+str(fold_i)\n        df_merge[col] = df['label']\n    df_merge.to_csv('./user_data/' + save_name + '_merge.csv', index=None)\n\n    # 投票\n    df_vote = pd.DataFrame()\n    df_vote['label'] = df_merge.apply(lambda x:x.value_counts().idxmax(), axis=1)\n    df_vote.to_csv('./user_data/' + save_name + '_vote.csv', index=None)\n\n    # 可信度评估\n    df_look = pd.DataFrame()\n    df_look = df_merge\n    df_look['vote'] = df_vote['label']\n\n    def is_all_same(ser):\n        for idx in ser.index:\n            if ser.iloc[0] != ser.loc[idx]:\n                return 0\n        return 1\n    df_look['all_same'] = df_look.apply(is_all_same, axis=1)\n\n\n    # 伪标签数据生成\n    traincsv_data = pd.read_csv(data_file, sep='\\t', encoding='UTF-8', nrows=train_rows)\n    testcsv_data = pd.read_csv(test_file, sep='\\t', encoding='UTF-8', nrows=test_rows)\n    #traincsv_data = pd.read_csv(data_file, sep='\\t', encoding='UTF-8', nrows=3000)\n    #testcsv_data = pd.read_csv(test_file, sep='\\t', encoding='UTF-8', nrows=300)\n\n    test_weaklabel_same = pd.DataFrame()\n    test_weaklabel_same['label'] = df_vote[df_look['all_same'] == 1]['label']\n    test_weaklabel_same['text'] = testcsv_data[df_look['all_same'] == 1]['text']\n\n    traincsv_weaklabel_same = pd.concat([traincsv_data, test_weaklabel_same]).reset_index()\n    del traincsv_weaklabel_same['index']\n    traincsv_weaklabel_same.to_csv(data_file_pseudo, index=None, sep='\\t')\n\n    logging.info(\"Pseudo_label_num: %s\", str(test_weaklabel_same.shape[0]))\n    logging.info(\"New_train_data_num: %s\", str(traincsv_weaklabel_same.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.164918Z","iopub.execute_input":"2021-07-09T04:32:28.165312Z","iopub.status.idle":"2021-07-09T04:32:28.177559Z","shell.execute_reply.started":"2021-07-09T04:32:28.165279Z","shell.execute_reply":"2021-07-09T04:32:28.176712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 各模型集成","metadata":{}},{"cell_type":"code","source":"\"\"\"模型融合集成\"\"\"\n\n\ndef vote_weight(save_names, run_folds, weights, pred_name):\n    \"\"\"加权投票\"\"\"\n\n    def get_file_name(save_names, fold_ids):\n        save_tests = []\n        for i in range(len(save_names)):\n            save_name = save_names[i]\n            for fold_i in fold_ids[i]:\n                save_test = './user_data/' + save_name + '_' + str(fold_i) + '.csv'\n                save_tests.append(save_test)\n\n        return save_tests\n\n    save_tests = get_file_name(save_names, run_folds)\n\n    file_name = '-'.join(save_names)\n\n    df_merge = pd.DataFrame()\n    for save_test in save_tests:\n        df = pd.read_csv(save_test)\n        df_merge[save_test] = df['label']\n\n    df_merge.to_csv('./user_data/' + file_name + '-merge.csv', index=None)\n\n\n    def vote_w(ser):\n        group_cols_ls = []\n        for name in save_names:\n            cols_ls = []\n            for col in ser.index:\n                if name in col:\n                    cols_ls.append(col)\n            group_cols_ls.append(cols_ls)\n\n        group_value_counts = []\n        for i, cols_ls in enumerate(group_cols_ls):\n            group_value_counts.append(ser[cols_ls].value_counts() * weights[i])\n\n        for i, count in enumerate(group_value_counts):\n            if i == 0:\n                value_count = group_value_counts[0]\n            else:\n                value_count = value_count.add(count, fill_value=0)\n\n        return value_count.idxmax()\n\n    df_vote = pd.DataFrame()\n\n    df_vote['label'] = df_merge.apply(lambda x:x.value_counts().idxmax(), axis=1)\n    df_vote.to_csv('./user_data/' + file_name + '-vote.csv', index=None)\n\n    df_vote['label'] = df_merge.apply(vote_w, axis=1)\n    df_vote.to_csv('./user_data/' + file_name + '-vote_wight.csv', index=None)\n    df_vote.to_csv('./prediction_result/' + pred_name, index=None)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.179123Z","iopub.execute_input":"2021-07-09T04:32:28.179466Z","iopub.status.idle":"2021-07-09T04:32:28.192994Z","shell.execute_reply.started":"2021-07-09T04:32:28.179431Z","shell.execute_reply":"2021-07-09T04:32:28.192149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 全流程代码入口","metadata":{}},{"cell_type":"code","source":"def _main():\n\n    # 模型和预测结果输出名称\n    save_names = ['model_trainset_pseudo0', 'model_trainset_pseudoa', 'model_trainset_pseudob']\n    # 训练折数\n    fold_nums = [10, 10, 10]\n    # 要训练的折\n    run_folds = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 3, 6, 9]]\n    # 模型权重\n    weights = [0.9*2, 0.9*2.5, 0.9*3*2.5]\n\n    # 小规模数据测试\n    # # 模型和预测结果输出名称\n    # save_names = ['model_trainset_pseudo0', 'model_trainset_pseudoa', 'model_trainset_pseudob']\n    # # 训练折数\n    # fold_nums = [10, 10, 10]\n    # # 要训练的折\n    # run_folds = [[0, 1], [0, 1], [0]]\n    # # 模型权重\n    # weights = [0.9*2, 0.9*2.5, 0.9*3*2.5]\n\n\n    print('============Semi-Supervised Train 1 (for test_a).')\n    # 第一次半监督训练\n    data_file = train_set\n    train_data_file = data_file\n    test_data_file = test_set_a\n    fold_num = fold_nums[0]\n    run_fold = run_folds[0]\n    save_name = save_names[0]\n    semi_a_times = 1\n    for semi_i in range(semi_a_times):\n        train_flow(train_data_file, test_data_file, fold_num, run_fold, save_name, is_train=True)\n        data_file_pseudo = './user_data/' + 'train_set_pseudo_a.csv'\n        pseudo_label(save_name, run_fold, data_file, test_data_file, data_file_pseudo)\n        train_data_file = data_file_pseudo\n\n    print('============Semi-Supervised Train 2 (for test_b).')\n    # 第二次半监督训练\n    data_file = './user_data/train_set_pseudo_a.csv'\n    train_data_file = data_file\n    test_data_file = test_set_b\n    fold_num = fold_nums[1]\n    run_fold = run_folds[1]\n    save_name = save_names[1]\n    semi_b_times = 1\n    for semi_i in range(semi_b_times):\n        train_flow(train_data_file, test_data_file, fold_num, run_fold, save_name, is_train=True)\n        data_file_pseudo = './user_data/' + 'train_set_pseudo_b.csv'\n        pseudo_label(save_name, run_fold, data_file, test_data_file, data_file_pseudo)\n        train_data_file = data_file_pseudo\n\n\n    print('============Last Train (for test_b).')\n    # 最后一次数据再次训练\n    data_file = './user_data/train_set_pseudo_b.csv'\n    test_data_file = test_set_b\n    fold_num = fold_nums[2]\n    run_fold = run_folds[2]\n    save_name = save_names[2]\n    train_flow(data_file, test_data_file, fold_num, run_fold, save_name, is_train=True)\n\n\n    print('============No Pseudo Test (for test_b).')\n    # 预测未加伪标签数据的模型的test_b的结果\n    data_file = train_set\n    test_data_file = test_set_b\n    fold_num = fold_nums[0]\n    run_fold = run_folds[0]\n    save_name = save_names[0]\n    train_flow(data_file, test_data_file, fold_num, run_fold, save_name, is_train=False)\n\n\n    print('============Model Ensemble, Predicted Results Vote.')\n    # 模型集成融合\n    pred_name = 'predictions.csv'\n    vote_weight(save_names, run_folds, weights, pred_name)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.194517Z","iopub.execute_input":"2021-07-09T04:32:28.194878Z","iopub.status.idle":"2021-07-09T04:32:28.208887Z","shell.execute_reply.started":"2021-07-09T04:32:28.194842Z","shell.execute_reply":"2021-07-09T04:32:28.207645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_main()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T04:32:28.210198Z","iopub.execute_input":"2021-07-09T04:32:28.210557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}