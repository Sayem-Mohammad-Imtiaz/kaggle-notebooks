{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nfrom numpy import median\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV, KFold\nfrom scipy import stats\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom imblearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report, recall_score, precision_score, plot_precision_recall_curve\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.feature_selection import RFECV, SelectKBest, f_classif\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC","metadata":{"id":"d53UapAbV6qm","execution":{"iopub.status.busy":"2021-05-21T13:57:43.22984Z","iopub.execute_input":"2021-05-21T13:57:43.230279Z","iopub.status.idle":"2021-05-21T13:57:44.799466Z","shell.execute_reply.started":"2021-05-21T13:57:43.230195Z","shell.execute_reply":"2021-05-21T13:57:44.798428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction","metadata":{"id":"21cd4v1VdZSJ"}},{"cell_type":"markdown","source":"In this notebook, we're going to predict companies that go bankrupt. I'm new here on Kaggle and I'd really like an upvote from your part if you liked this work, this will really encourage me to keep doing projects like that. Also if you have any suggestions or remarks please let me know on the comment section. Thanks ! \n\nDataset description provided: \"The data were collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.\"","metadata":{"id":"QQEOjDrPnKdI"}},{"cell_type":"code","source":"# Ignore warnings\nwarnings.filterwarnings(\"ignore\") \n# Import data\ndata = pd.read_csv('../input/company-bankruptcy-prediction/data.csv')","metadata":{"id":"XgqLBy9pmChr","execution":{"iopub.status.busy":"2021-05-21T13:57:56.591653Z","iopub.execute_input":"2021-05-21T13:57:56.592058Z","iopub.status.idle":"2021-05-21T13:57:57.046708Z","shell.execute_reply.started":"2021-05-21T13:57:56.592016Z","shell.execute_reply":"2021-05-21T13:57:57.045982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect data\ndata.head()","metadata":{"id":"S-48mIBnWy7z","outputId":"44d3923c-c24c-4286-c1aa-190307107399","execution":{"iopub.status.busy":"2021-05-21T13:58:00.495677Z","iopub.execute_input":"2021-05-21T13:58:00.496219Z","iopub.status.idle":"2021-05-21T13:58:00.541013Z","shell.execute_reply.started":"2021-05-21T13:58:00.496185Z","shell.execute_reply":"2021-05-21T13:58:00.54025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create copy of df\ndf = data.copy()\n# Check Missing data\ndf.isnull().sum().any()","metadata":{"id":"WN8dsln7ndRh","outputId":"8aadadb8-d230-44a1-fd7b-fcc20cef5cc4","execution":{"iopub.status.busy":"2021-05-21T13:58:04.353184Z","iopub.execute_input":"2021-05-21T13:58:04.353669Z","iopub.status.idle":"2021-05-21T13:58:04.365526Z","shell.execute_reply.started":"2021-05-21T13:58:04.353636Z","shell.execute_reply":"2021-05-21T13:58:04.364567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data contains no missing values. ","metadata":{"id":"2z_C-xLQnjgP"}},{"cell_type":"code","source":"# Shape of df\nprint(\"Number of rows : {}\".format(df.shape[0]), '\\n'\n      \"Number of cols : {}\".format(df.shape[1]))","metadata":{"id":"pPhBidVinirt","outputId":"4adad755-1491-43c2-bcd6-1cafc571bd8b","execution":{"iopub.status.busy":"2021-05-21T13:58:08.419368Z","iopub.execute_input":"2021-05-21T13:58:08.419734Z","iopub.status.idle":"2021-05-21T13:58:08.424765Z","shell.execute_reply.started":"2021-05-21T13:58:08.419699Z","shell.execute_reply":"2021-05-21T13:58:08.423993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A) Recursive Feature Elimination with Cross-validation (RFECV):\n\n\n","metadata":{"id":"uBz51w3coUkY"}},{"cell_type":"markdown","source":"This data has a large number of features. In order to make the EDA easier and improve the accuracy of machine learning models, we need to reduce the dimensionality of the data. For this purpose, I'm going to use a famous feature selection method named Recursive Feature Elimination. Recursive feature elimination is an example of backward feature elimination in which we essentially first fit our model using all the features in a given set, then progressively one by one we remove the least significant features, each time re-fitting, until we are left with the desired number of features","metadata":{"id":"iq2zVLh8nuwm"}},{"cell_type":"code","source":"# Set X and y \nX = df.iloc[:, 1:].values\ny = df.iloc[:, 0].values\n# Set training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"id":"9tGQHal1nrxB","execution":{"iopub.status.busy":"2021-05-21T13:58:11.096031Z","iopub.execute_input":"2021-05-21T13:58:11.096531Z","iopub.status.idle":"2021-05-21T13:58:11.114539Z","shell.execute_reply.started":"2021-05-21T13:58:11.096497Z","shell.execute_reply":"2021-05-21T13:58:11.113748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set RandomForestClassifier as estimator for RFECV\ncart = RandomForestClassifier(random_state=42)\n# Minimum number of features to consider\nmin_features_to_select = 1  \n# Set number of folds\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1) \n# Set cross-validation process\nrfecv = RFECV(estimator=cart, step=1, cv=cv,\n              scoring='accuracy',\n              min_features_to_select=min_features_to_select, n_jobs=1)\n# Fit the model\nrfecv.fit(X_train, y_train)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(min_features_to_select,\n               len(rfecv.grid_scores_) + min_features_to_select),\n         rfecv.grid_scores_)\nplt.show()","metadata":{"id":"R3L1OZSso4Is","outputId":"256ef39e-c8b1-4496-99e2-8e98a2456324","execution":{"iopub.status.busy":"2021-05-21T13:58:14.564062Z","iopub.execute_input":"2021-05-21T13:58:14.564588Z","iopub.status.idle":"2021-05-21T14:14:22.843847Z","shell.execute_reply.started":"2021-05-21T13:58:14.564553Z","shell.execute_reply":"2021-05-21T14:14:22.842946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display features' names\nmost_relevent_cols = df.iloc[:, 1:].columns[np.where(rfecv.support_ == True)]\nprint(\"Most relevant features are: \")\nprint(most_relevent_cols)","metadata":{"id":"eVJSI-1C6Z2k","outputId":"9a9eb505-7ce8-4ad7-e63d-bb916796c0b4","execution":{"iopub.status.busy":"2021-05-21T14:15:15.008522Z","iopub.execute_input":"2021-05-21T14:15:15.00898Z","iopub.status.idle":"2021-05-21T14:15:15.022417Z","shell.execute_reply.started":"2021-05-21T14:15:15.008929Z","shell.execute_reply":"2021-05-21T14:15:15.021414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Append target\nmost_relevent_cols = list(most_relevent_cols)\nmost_relevent_cols.append(\"Bankrupt?\")","metadata":{"id":"p51vRwpR6jJE","execution":{"iopub.status.busy":"2021-05-21T14:15:18.443276Z","iopub.execute_input":"2021-05-21T14:15:18.44366Z","iopub.status.idle":"2021-05-21T14:15:18.448382Z","shell.execute_reply.started":"2021-05-21T14:15:18.443626Z","shell.execute_reply":"2021-05-21T14:15:18.447119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display df\nrfecv_df = df[most_relevent_cols]\nrfecv_df.head()","metadata":{"id":"W0tyAzm87f6T","outputId":"da38a20d-8b26-4d07-dd9a-f8f1b554942e","execution":{"iopub.status.busy":"2021-05-21T14:15:20.898788Z","iopub.execute_input":"2021-05-21T14:15:20.899166Z","iopub.status.idle":"2021-05-21T14:15:20.936881Z","shell.execute_reply.started":"2021-05-21T14:15:20.899133Z","shell.execute_reply":"2021-05-21T14:15:20.935608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B) EDA","metadata":{"id":"LphVuWI88R8F"}},{"cell_type":"markdown","source":"In this part, I'm going to do some Exploratory Data Analysis on the features previously selected by RFECV. Understanding our features is key for building more accurate machine learning models.","metadata":{"id":"0sJZwWPXemu8"}},{"cell_type":"markdown","source":"### a) Descriptive statistics","metadata":{"id":"dmrTVLi--7KB"}},{"cell_type":"code","source":"# Descriptive stats\nrfecv_df.describe()","metadata":{"id":"E5U9r7c48Hh-","outputId":"c8e8a122-0de5-442c-a004-9f18477e2e29","execution":{"iopub.status.busy":"2021-05-21T14:15:25.092864Z","iopub.execute_input":"2021-05-21T14:15:25.093329Z","iopub.status.idle":"2021-05-21T14:15:25.220321Z","shell.execute_reply.started":"2021-05-21T14:15:25.093293Z","shell.execute_reply":"2021-05-21T14:15:25.219067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the values in the data range between 0 and 1. But we can see that there are some extreme values (outliers). ","metadata":{"id":"kHEpRJNR8sqm"}},{"cell_type":"code","source":"# Analyse target var\nsns.countplot(rfecv_df['Bankrupt?'])\n# Target \nprint(\"% of Data\") \nprint(df['Bankrupt?'].value_counts(normalize=True))\nprint(\"Count\")\nprint(df['Bankrupt?'].value_counts())","metadata":{"id":"5Fnyz2vg8_Xy","outputId":"be802a3a-8888-4802-9fa3-4e75a2141c8c","execution":{"iopub.status.busy":"2021-05-21T14:15:28.103953Z","iopub.execute_input":"2021-05-21T14:15:28.104344Z","iopub.status.idle":"2021-05-21T14:15:28.248267Z","shell.execute_reply.started":"2021-05-21T14:15:28.104312Z","shell.execute_reply":"2021-05-21T14:15:28.247198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that our dataset is very imbalanced. The minority class which is the one we're most interested by predicting represents about 3% of total observations. This can pose a real challenge to machine learning models.","metadata":{"id":"9GJgcSGf9VYa"}},{"cell_type":"code","source":"# Looking at the histograms of numerical data\nrfecv_df.hist(figsize = (35,30), bins = 50)\nplt.show()","metadata":{"id":"14ELhbaR9tJO","outputId":"d1c98f94-4bf7-44d0-e85a-a34499bfb100","execution":{"iopub.status.busy":"2021-05-21T14:15:30.71852Z","iopub.execute_input":"2021-05-21T14:15:30.718863Z","iopub.status.idle":"2021-05-21T14:15:38.337273Z","shell.execute_reply.started":"2021-05-21T14:15:30.718835Z","shell.execute_reply":"2021-05-21T14:15:38.33651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that most features don't follow a normal distribution and many of them have very skwed distribution. This might be solved by applying a log transformation but as you'll see later in the notebook, I'll chose to simply replace extreme positive values with the median of the target group. ","metadata":{"id":"cYC9lxAIhDli"}},{"cell_type":"code","source":"# Correlations\nrfecv_df.corr('spearman')[\"Bankrupt?\"].sort_values() \n# Correlation Heatmap (Spearman)\nf, ax = plt.subplots(figsize=(30, 25))\nmat = rfecv_df.corr('spearman')  \nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0,# annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","metadata":{"id":"wb54JJyW-FH5","outputId":"5fbf5558-fbe6-4878-a3f3-520b94133b9f","execution":{"iopub.status.busy":"2021-05-21T14:15:42.761735Z","iopub.execute_input":"2021-05-21T14:15:42.76231Z","iopub.status.idle":"2021-05-21T14:15:44.351035Z","shell.execute_reply.started":"2021-05-21T14:15:42.762274Z","shell.execute_reply":"2021-05-21T14:15:44.350008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### b) Analysis of variance and hypothesis testing","metadata":{"id":"K7A8Yjuw_Gug"}},{"cell_type":"code","source":"# Seperate dfs\nbankrupt_df = rfecv_df[rfecv_df['Bankrupt?']==True]\nnot_bankrupt_df = rfecv_df[rfecv_df['Bankrupt?']==False]\n\n# Analyze distributions of selected features using rfecv\ncols = rfecv_df.drop(\"Bankrupt?\", axis=1).columns\n\nfor feature in cols:\n  a = bankrupt_df[feature]\n  b = not_bankrupt_df[feature]\n  b = b.sample(n=len(a), random_state=42) # Take random sample from each feature to match length of target\n  # Running t-tests\n  test = stats.ttest_ind(a,b)   \n  plt.figure() \n  sns.distplot(bankrupt_df[feature], kde=True, label=\"Bankrupt\")\n  sns.distplot(not_bankrupt_df[feature], kde=True, label=\"Not Bankrupt\") \n  plt.title(\"{} / p-value of t-test = :{}\".format(feature, test[1]))\n  plt.legend()","metadata":{"id":"gJoThqr4-w3T","outputId":"2a648997-3765-403a-b701-53094a653a16","execution":{"iopub.status.busy":"2021-05-21T14:15:55.137869Z","iopub.execute_input":"2021-05-21T14:15:55.138238Z","iopub.status.idle":"2021-05-21T14:16:09.69452Z","shell.execute_reply.started":"2021-05-21T14:15:55.138208Z","shell.execute_reply":"2021-05-21T14:16:09.693384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Conclusions:** \nThe following are some features that significantly differentiate the two groups of our target. The definitions of these indicators were all taken from Investopedia which is a widely known finance website.\n\n- **ROA (C) + ROA(A)** : Companies that encounter financial difficulties  tend to have lower ROA. ROA is an indicator of how profitable a company is relative to its total assets. ROA gives a manager, investor, or analyst an idea as to how efficient a company's management is at using its assets to generate earnings. ROA is displayed as a percentage; the higher the ROA the better.\n\n- **Net Value per Share** : This is the ratio of equity available to common shareholders divided by the number of outstanding shares. This figure represents the minimum value of a company's equity and measures the book value of a firm on a per-share basis. \n\n- **Persistent EPS & Per Share Net Profit**  : Earnings per share (EPS) is calculated as a company's profit divided by the outstanding shares of its common stock. The resulting number serves as an indicator of a company's profitability. \n\n- **Net worth/Total Assets** : The higher the equity-to-asset ratio, the less leveraged the company is, meaning that a larger percentage of its assets are owned by the company and its investors.\n\n- **Cash/Totat Assets** : This figure is used to measure a firm's liquidity or its ability to pay its short-term obligations. The higher the better.\n","metadata":{"id":"pHobrXa3_rZn"}},{"cell_type":"markdown","source":"## C) Outliers's Analysis","metadata":{"id":"Bt3PtIzh_-Tw"}},{"cell_type":"code","source":"# Visulize outliers using boxplots\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data= rfecv_df, orient=\"h\")\nax.set_title('Features_selected Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","metadata":{"id":"Z5yBsZXf_qpg","outputId":"98d55f3d-3bb9-4a9f-b761-080da3e85b29","execution":{"iopub.status.busy":"2021-05-21T14:16:16.555406Z","iopub.execute_input":"2021-05-21T14:16:16.555781Z","iopub.status.idle":"2021-05-21T14:16:18.313884Z","shell.execute_reply.started":"2021-05-21T14:16:16.55575Z","shell.execute_reply":"2021-05-21T14:16:18.312817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset contains too many outliers. ","metadata":{"id":"FRnLq1CiAIgB"}},{"cell_type":"markdown","source":"## D) Predicting Bankruptcy","metadata":{"id":"IlsPi2y9Ah93"}},{"cell_type":"markdown","source":"Since this is a binary classification problem, we are more interested by predicting the positive value namely the Bankrupt category = 1. The positive category is our minority class and this will pose a real challenge for machine learning models. The best way to measure the performance of our models in this case is by computing the f1 score which is the average of our recall and precision. Recall or also called sensitivity is the metric that gives us true positive rate which is the correctly classified positives out of all possible positives, while precision will tell us what is the proportion of correctly classified positive out of all predicted positives. Recall has as main goal to minimize false negatives while precision aims to minimize false positives. ","metadata":{"id":"4RuKq_pVZIBN"}},{"cell_type":"markdown","source":"### a) Testing models","metadata":{"id":"2ybxkN_NEVKq"}},{"cell_type":"markdown","source":"I'll first use the features selected by the RFECV to test the models on. Later on, I'll use the SelectKbest features' selection.","metadata":{"id":"X3AdB0JbhSXV"}},{"cell_type":"code","source":"# Evaluation function \ndef evaluation(model):\n  model.fit(X_train, y_train)\n  ypred = model.predict(X_test)\n  print(confusion_matrix(y_test, ypred))\n  print(classification_report(y_test, ypred))\n    \n  N, train_score, val_score = learning_curve(model, X_train, y_train,\n                                              cv=5, scoring='f1',\n                                               train_sizes=np.linspace(0.1, 1, 10))\n    \n    \n  plt.figure(figsize=(12, 8))\n  plt.plot(N, train_score.mean(axis=1), label='train score')\n  plt.plot(N, val_score.mean(axis=1), label='validation score')\n  plt.legend()","metadata":{"id":"tETge0phAO-W","execution":{"iopub.status.busy":"2021-05-21T15:20:29.821722Z","iopub.execute_input":"2021-05-21T15:20:29.822147Z","iopub.status.idle":"2021-05-21T15:20:29.829686Z","shell.execute_reply.started":"2021-05-21T15:20:29.822111Z","shell.execute_reply":"2021-05-21T15:20:29.828571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set X and y \nX = rfecv_df.iloc[:, :-1]\ny = rfecv_df.iloc[:, -1]\n# Set training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"id":"phD4xJy6BJkU","execution":{"iopub.status.busy":"2021-05-21T15:20:53.657608Z","iopub.execute_input":"2021-05-21T15:20:53.65824Z","iopub.status.idle":"2021-05-21T15:20:53.667044Z","shell.execute_reply.started":"2021-05-21T15:20:53.658189Z","shell.execute_reply":"2021-05-21T15:20:53.666106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set models with default params\nRandomForest = RandomForestClassifier(random_state=0)\nAdaBoost = AdaBoostClassifier(random_state=0)\nKNN = make_pipeline(StandardScaler(), KNeighborsClassifier())\n# Set dictionary of models\ndict_of_models = {'RandomForest': RandomForest,\n                  'AdaBoost' : AdaBoost,\n                  'KNN': KNN\n                  \n                 }","metadata":{"id":"18grVPW8BVVC","execution":{"iopub.status.busy":"2021-05-21T15:20:55.891794Z","iopub.execute_input":"2021-05-21T15:20:55.892495Z","iopub.status.idle":"2021-05-21T15:20:55.899161Z","shell.execute_reply.started":"2021-05-21T15:20:55.892449Z","shell.execute_reply":"2021-05-21T15:20:55.897851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate models\nfor name, model in dict_of_models.items():\n    print(name)\n    evaluation(model)","metadata":{"id":"s62lAPWrCq6q","outputId":"98dbb735-8b8c-41ba-927d-c003b82f7de1","execution":{"iopub.status.busy":"2021-05-21T15:20:58.95855Z","iopub.execute_input":"2021-05-21T15:20:58.959087Z","iopub.status.idle":"2021-05-21T15:22:32.746636Z","shell.execute_reply.started":"2021-05-21T15:20:58.959051Z","shell.execute_reply":"2021-05-21T15:22:32.745506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RFC and AdaBoost seem to yield highest f1 score. We'll continue using these models. But since the data is very imbalanced, the model will obviously be biased towards the majority class. We'll use SMOTE in order to rebalance the data. SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.","metadata":{"id":"51GNOAr9D1S5"}},{"cell_type":"markdown","source":"### b) SMOTE & RandomForestClassifier","metadata":{"id":"xMYYfQWDEcE4"}},{"cell_type":"markdown","source":"We can use SMOTE to either oversample the minority class or undersample the majority class in order to rebalance the data. Other strategies of rebalancing the data are possible for example using both oversampling and undersampling at the same time. In our approach we'll test both the oversampling and over/undersampling strategy. Since machine learning models are automatically biased towards the majority class, using SMOTE will hopefully make the training process of the model more reliable.","metadata":{"id":"Er-br6VIErGf"}},{"cell_type":"code","source":"# USING OVERSAMPLING ONLY\nkf = RepeatedStratifiedKFold(n_splits=5)\n# Set empty lists to store key metrics\naccuracy = []\nf1 = []\nrecall =[]\nprecision = []\n# Loop over kfolds\nfor kf, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n    X_train = X[train_index, :]\n    y_train = y[train_index]  \n    X_test = X[test_index, :]\n    y_test = y[test_index]  \n    # Set pipeline where SMOTE with oversampling is applied before model fitting\n    model = make_pipeline(SMOTE() ,RandomForestClassifier(random_state=0))\n    # Fit model\n    model.fit(X_train, y_train)\n    # Predict on original test set  \n    y_pred = model.predict(X_test)\n    # Compute key metrics for target \"Bankrupt = 1\"\n    accuracy.append(model.score(X_test, y_test))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n# Print key metrics      \nprint(\"Mean accuracy:\", np.mean(accuracy))\nprint(\"Mean f1\", np.mean(f1))\nprint(\"Mean recall\", np.mean(recall))\nprint(\"Mean precision\", np.mean(precision))","metadata":{"id":"c7Ad4k9rDyd8","outputId":"146bb7a2-0bfe-47c9-c4b6-98ed2b001c10","execution":{"iopub.status.busy":"2021-05-21T14:20:07.71981Z","iopub.execute_input":"2021-05-21T14:20:07.720198Z","iopub.status.idle":"2021-05-21T14:24:19.877183Z","shell.execute_reply.started":"2021-05-21T14:20:07.720168Z","shell.execute_reply":"2021-05-21T14:24:19.876189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that by applying oversampling, the f1 score slightly improve. We sig improved our recall but our precision went down. ","metadata":{"id":"DCJHP52tI6Z_"}},{"cell_type":"code","source":"# SMOTE startegy with both over and undersampling\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)","metadata":{"id":"BMN8cfmsLo0T","execution":{"iopub.status.busy":"2021-05-21T14:35:48.586955Z","iopub.execute_input":"2021-05-21T14:35:48.587415Z","iopub.status.idle":"2021-05-21T14:35:48.591732Z","shell.execute_reply.started":"2021-05-21T14:35:48.587377Z","shell.execute_reply":"2021-05-21T14:35:48.590559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USING BOTH OVER AND UNDERSAMPLING\nkf = RepeatedStratifiedKFold(n_splits=5)\n# Set empty lists to store key metrics\naccuracy = []\nf1 = []\nrecall =[]\nprecision = []\n# Loop over kfolds\nfor kf, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n    X_train = X[train_index, :]\n    y_train = y[train_index]  \n    X_test = X[test_index, :]\n    y_test = y[test_index]  \n    # Set pipeline where SMOTE is applied before model fitting\n    model = make_pipeline(over, under ,RandomForestClassifier(random_state=0))\n    # Fit model\n    model.fit(X_train, y_train)\n    # Predict on original test set  \n    y_pred = model.predict(X_test)\n    # Compute key metrics for target \"Bankrupt = 1\"\n    accuracy.append(model.score(X_test, y_test))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n# Print key metrics      \nprint(\"Mean accuracy:\", np.mean(accuracy))\nprint(\"Mean f1\", np.mean(f1))\nprint(\"Mean recall\", np.mean(recall))\nprint(\"Mean precision\", np.mean(precision))","metadata":{"id":"_rOftcMRLKzk","outputId":"541ff968-a1b7-4994-8339-cae1957b25a2","execution":{"iopub.status.busy":"2021-05-21T14:35:52.362705Z","iopub.execute_input":"2021-05-21T14:35:52.363111Z","iopub.status.idle":"2021-05-21T14:36:26.82601Z","shell.execute_reply.started":"2021-05-21T14:35:52.363073Z","shell.execute_reply":"2021-05-21T14:36:26.824933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using both over and undersampling yielded a less greater f1 score but the recall increased sharply.","metadata":{"id":"wANFTG6AMecq"}},{"cell_type":"markdown","source":"### c) SMOTE & AdaBoost","metadata":{"id":"G5v22DAhJcsp"}},{"cell_type":"code","source":"# USING OVERSAMPLING ONLY\nkf = RepeatedStratifiedKFold(n_splits=5)\n# Set empty lists to store key metrics\naccuracy = []\nf1 = []\nrecall =[]\nprecision = []\n# Loop over kfolds\nfor kf, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n    X_train = X[train_index, :]\n    y_train = y[train_index]  \n    X_test = X[test_index, :]\n    y_test = y[test_index]  \n    # Set pipeline where SMOTE is applied before model fitting\n    model = make_pipeline(SMOTE() ,AdaBoostClassifier(random_state=0))\n    # Fit model\n    model.fit(X_train, y_train)\n    # Predict on original test set  \n    y_pred = model.predict(X_test)\n    # Compute key metrics for target \"Bankrupt = 1\"\n    accuracy.append(model.score(X_test, y_test))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n# Print key metrics      \nprint(\"Mean accuracy:\", np.mean(accuracy))\nprint(\"Mean f1\", np.mean(f1))\nprint(\"Mean recall\", np.mean(recall))\nprint(\"Mean precision\", np.mean(precision))","metadata":{"id":"MWwjeu2-JZRu","outputId":"b0f9f3ed-412a-464d-cb18-0ed6e54a3e2e","execution":{"iopub.status.busy":"2021-05-21T14:36:46.59127Z","iopub.execute_input":"2021-05-21T14:36:46.591621Z","iopub.status.idle":"2021-05-21T14:38:50.438549Z","shell.execute_reply.started":"2021-05-21T14:36:46.59159Z","shell.execute_reply":"2021-05-21T14:38:50.437381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adaboost yielded lower f1 score, although the recall drastically improved. This model will have a quite high sensitivity meaning it will correctly detect True Positives namely companies that will go bankrupt but will also give high proportion of False Positives (around 80%). ","metadata":{"id":"MJ5rUGZGKflG"}},{"cell_type":"code","source":"# # USING BOTH OVER AND UNDERSAMPLING\nkf = RepeatedStratifiedKFold(n_splits=5)\n# Set empty lists to store key metrics\naccuracy = []\nf1 = []\nrecall =[]\nprecision = []\n# Loop over kfolds\nfor kf, (train_index, test_index) in enumerate(kf.split(X, y), 1):\n    X_train = X[train_index, :]\n    y_train = y[train_index]  \n    X_test = X[test_index, :]\n    y_test = y[test_index]  \n    # Set pipeline where SMOTE is applied before model fitting\n    model = make_pipeline(over, under ,AdaBoostClassifier(random_state=0))\n    # Fit model\n    model.fit(X_train, y_train)\n    # Predict on original test set  \n    y_pred = model.predict(X_test)\n    # Compute key metrics for target \"Bankrupt = 1\"\n    accuracy.append(model.score(X_test, y_test))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n# Print key metrics      \nprint(\"Mean accuracy:\", np.mean(accuracy))\nprint(\"Mean f1\", np.mean(f1))\nprint(\"Mean recall\", np.mean(recall))\nprint(\"Mean precision\", np.mean(precision))","metadata":{"id":"7r1APPxCM48B","outputId":"0102f37b-41c6-41ca-fbb5-cf3a0a065fd1","execution":{"iopub.status.busy":"2021-05-21T14:38:59.875014Z","iopub.execute_input":"2021-05-21T14:38:59.875418Z","iopub.status.idle":"2021-05-21T14:39:24.453951Z","shell.execute_reply.started":"2021-05-21T14:38:59.87538Z","shell.execute_reply":"2021-05-21T14:39:24.452693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adaboost seem to yield slightly better f1 score with this SMOTE startegy. ","metadata":{"id":"yq-1R9P0NYrK"}},{"cell_type":"markdown","source":"### Conclusion:\n\nRandomForestClasifier + Oversampling gave us the highest f1 score of about 43% with 58% Recall and 34% Precision. ","metadata":{"id":"VGoWcLpGNrCd"}},{"cell_type":"markdown","source":"  ## Final Conclusion","metadata":{"id":"YIuNN4cAhyA7"}}]}