{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification Project\n\nIn this project, I have used [Cardiovascular Disease dataset](https://www.kaggle.com/sulianova/cardiovascular-disease-dataset) from kaggle. Based on some health conditions of an individual my model will predict whether he has any cardiovascular disease or not.\n\nFeatures:\n\n* Age | Objective Feature | age | int (days)\n* Height | Objective Feature | height | int (cm) |\n* Weight | Objective Feature | weight | float (kg) |\n* Gender | Objective Feature | gender | categorical code |\n* Systolic blood pressure | Examination Feature | ap_hi | int |\n* Diastolic blood pressure | Examination Feature | ap_lo | int |\n* Cholesterol | Examination Feature | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n* Glucose | Examination Feature | gluc | 1: normal, 2: above normal, 3: well above normal |\n* Smoking | Subjective Feature | smoke | binary |\n* Alcohol intake | Subjective Feature | alco | binary |\n* Physical activity | Subjective Feature | active | binary |\n* Presence or absence of cardiovascular disease | Target Variable | cardio | binary |","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#importing basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n\nraw_data = pd.read_csv('../input/cardiovascular-disease-dataset/cardio_train.csv',sep=';')\n# Check the data\nraw_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Missing values are present: {raw_data.isnull().sum().any()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no mising value in the data.\n* I will drop column 'id' as it is irrelevant to target variable.\n* Transform age column into years instead of days.\n* Gender feature should not be categorized into 1 and 2 because 2 is always numerically bigger than 1, the model would take into account that and give a bigger ratio to one gender for having a disease. So, I will make that binary.\n* I will check and drop duplicates.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.drop('id',axis=1,inplace=True)\nraw_data.age = np.round(raw_data.age/365.25,decimals=1)\nraw_data.gender = raw_data.gender.replace(2,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis and Data Preprocessing","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.countplot(raw_data.cardio,palette='summer')\nplt.xlabel('Presence of cardiovascular disease',fontdict={'fontsize': 15,'color':'Green'},labelpad=3);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, data is almost balanced. Let's see which gender has more cases of disease. As in this data there was no knowledge of which gender is denoted by which number, I will use simple fact that women's average age is less than that of men.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a = raw_data[raw_data[\"gender\"]==0][\"height\"].mean()\nb = raw_data[raw_data[\"gender\"]==1][\"height\"].mean()\nif a > b:\n    gender = \"male\"\n    gender1 = \"female\"\nelse:\n    gender = \"female\"\n    gender1 = \"male\"\nprint(\"Gender:0 is \"+ gender +\" & Gender:1 is \" + gender1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.countplot(raw_data.gender,hue=raw_data.cardio, palette=\"Set2\");","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set_style('dark')\nsns.boxplot(raw_data.height,palette='pink')\nplt.title('Distribution of height');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set_style('white')\nsns.boxplot(raw_data.weight,palette='terrain')\nplt.title('Distribution of weight');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will remove extremely rare cases of height and weight. As data is quite big, there will be no prblem while modelling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data = raw_data[(raw_data['height']<250) & (raw_data['weight']>20.0)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many outliers in height and weight features. I combine both of these into a new feature bmi.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data[\"bmi\"] = (raw_data[\"weight\"]/ (raw_data[\"height\"]/100)**2).round(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data[raw_data['bmi']<10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data[raw_data['bmi']>100].sort_values(by='weight',ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further, I will remove extremely underweight and obese people because such cases seems impossible. For example, there are observations with 80 cm height and 165 kgs weight which is quite impossible. May be it was a fake observation or typing mistake. Also, health conditions of dwarf and abnormally tall people are totally different so I don't want to include them.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data= raw_data[(raw_data['bmi']>10) & (raw_data['bmi']<100)].copy()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.boxplot(data.bmi,color='Green')\nplt.title('Distribution of BMI');","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"data.drop(['weight','height'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.violinplot(data.age,color='orange')\nprint(\"Observations have been recorded mostly for people with age between 40 and 65\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I will remove outliers and abrupt blood pressure values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['ap_lo']>360).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(data['ap_hi']>360).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= data[(data['ap_lo']<360) & (data['ap_hi']<360)].copy()\ndata= data[(data['ap_lo']>20) & (data['ap_hi']>20)].copy()\ndata=data[data['ap_hi']>data['ap_lo']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(data.ap_hi,color='orange');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(data.ap_lo,color='orange');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dummy variables for categorical column\ndata['cholesterol']=data['cholesterol'].map({ 1: 'normal', 2: 'above normal', 3: 'well above normal'})\ndata['gluc']=data['gluc'].map({ 1: 'normal', 2: 'above normal', 3: 'well above normal'})\ndummies = pd.get_dummies(data[['cholesterol','gluc']],drop_first=True)\nfinal_data = pd.concat([data,dummies],axis=1)\nfinal_data.drop(['cholesterol','gluc'],axis=1,inplace=True)\nfinal_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#plotting using plotly\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\n\nprint('Correlation of features with target variable')\nfinal_data.corr()['cardio'].sort_values()[:-1].iplot(kind='barh');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting and Standardizing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(final_data.drop('cardio',axis=1),final_data.cardio,test_size=0.30)\n\nto_be_scaled_feat = ['age', 'ap_hi', 'ap_lo','bmi']\nother_feat = ['gender', 'cholesterol_normal', 'cholesterol_well above normal',\n       'gluc_normal', 'gluc_well above normal', 'smoke', 'alco', 'active']\nscaler=StandardScaler()\nscaler.fit(X_train[to_be_scaled_feat])\nX_train[to_be_scaled_feat] = scaler.transform(X_train[to_be_scaled_feat])\nX_test[to_be_scaled_feat] = scaler.transform(X_test[to_be_scaled_feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# importing classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score,accuracy_score,classification_report\n\nclassifiers = {\n    'Logistic Regression' : LogisticRegression(),\n    'Decision Tree' : DecisionTreeClassifier(),\n    'Random Forest' : RandomForestClassifier(),\n    'Support Vector Machines' : SVC(),\n    'K-nearest Neighbors' : KNeighborsClassifier(),\n    'XGBoost' : XGBClassifier()\n}\nresults=pd.DataFrame(columns=['Accuracy in %','F1-score'])\nfor method,func in classifiers.items():\n    func.fit(X_train,y_train)\n    pred = func.predict(X_test)\n    results.loc[method]= [100*np.round(accuracy_score(y_test,pred),decimals=4),\n                         round(f1_score(y_test,pred),2)]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improving Accuracy by Hyperparameter Tuning\n\n## K- Nearest Neighbors (by elbow method)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\n\nfor i in range(1,15):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,15),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=12)\nknn.fit(X_train,y_train)\nknn_pred = knn.predict(X_test)\nprint(classification_report(y_test,knn_pred))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results.loc['K-nearest Neighbors(Improved)']= [100*np.round(accuracy_score(y_test,knn_pred),decimals=4),\n                         round(f1_score(y_test,knn_pred),2)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using **elbow method** we have increased accuracy of this model from 69.4% to 72%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Random Forest (by GridSearchCV)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth': [80, 90],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [8, 10],\n    'n_estimators': [100, 200]}\ngrid=GridSearchCV(RandomForestClassifier(),param_grid,verbose=1)\ngrid.fit(X_train,y_train)\ngrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"grid_pred = grid.predict(X_test)\nprint(classification_report(y_test,grid_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using **GridSearchCV** we have increased accuracy of this model from 69% to 73%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Deep Neural Networks","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#splitting further into validation set\nX_train, X_val, y_train, y_val = train_test_split(X_train,y_train,test_size=0.20)\n\nmodel = Sequential()\nmodel.add(Dense(12,activation='relu'))\nmodel.add(Dense(50,activation='relu',kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n    bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(50,activation='relu',kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n    bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(50,activation='relu',kernel_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1),\n    bias_initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nmodel.fit(x=X_train.values,y=y_train.values,\n          validation_data=(X_val,y_val.values),\n          batch_size=100,epochs=150,callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot();","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"dnn_pred = model.predict_classes(X_test)\nprint(classification_report(y_test,dnn_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results.loc['Random Forest(Improved)']= [100*np.round(accuracy_score(y_test,grid_pred),decimals=4),\n                         round(f1_score(y_test,grid_pred),2)]\nresults.loc['Deep Neural Network']= [100*np.round(accuracy_score(y_test,dnn_pred),decimals=4),\n                         round(f1_score(y_test,dnn_pred),2)]\nresults.sort_values(by='Accuracy in %',ascending=False).style.highlight_max()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}