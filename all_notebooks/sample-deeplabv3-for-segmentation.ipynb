{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\n#import imageio\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import models\nfrom torchvision.utils import make_grid\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations as A\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n         'batch_size': 2,\n         'learning_rate': 0.001,\n         }\n\ndevice=config['device']\nbatch_size=config['batch_size']\nlr=config['learning_rate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread(\n    '../input/camseq-semantic-segmentation/0016E5_07961.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img.transpose(0,1,2));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread(\n    '../input/camseq-semantic-segmentation/0016E5_07961_L.png')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img.transpose(0,1,2));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamSeq01/\ncolor_codes = OrderedDict({\n                'Animal': [64, 128, 64],\n                'Archway': [192, 0, 128],\n                'Bicyclist': [0, 128, 192],\n                'Bridge': [0, 128, 64],\n                'Building': [128, 0, 0],\n                'Car': [64, 0, 128],\n                'CartLuggagePram': [64, 0, 192],\n                'Child': [192, 128, 64], \n                'Column_Pole': [192, 192, 128],\n                'Fence': [64, 64, 128],\n                'LaneMkgsDriv': [128, 0, 192],\n                'LaneMkgsNonDriv': [192, 0, 64],\n                'Misc_Text': [128, 128, 64],\n                'MotorcycleScooter': [192, 0, 192],\n                'OtherMoving': [128, 64, 64],\n                'ParkingBlock': [64, 192, 128],\n                'Pedestrian': [64, 64, 0],\n                'Road': [128, 64, 128],\n                'RoadShoulder': [128, 128, 192],\n                'Sidewalk': [0, 0, 192],\n                'SignSymbol': [192, 128, 128],\n                'Sky': [128, 128, 128],\n                'SUVPickupTruck': [64, 128, 192],\n                'TrafficCone': [0, 0, 64],\n                'TrafficLight': [0, 64, 64],\n                'Train': [192, 64, 128],\n                'Tree': [128, 128, 0],\n                'Truck_Bus': [192, 128, 192],\n                'Tunnel': [64, 0, 64],\n                'VegetationMisc': [192, 192, 0],\n                'Void': [0, 0, 0],\n                'Wall': [64, 192, 0]\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CamSeqDataset(Dataset):\n    \n    def __init__(self, \n                 img_dir, \n                 color_codes=color_codes,\n                 transforms=None):\n        \n        super().__init__()\n        \n        self.images = sorted([os.path.join(\n            img_dir, x) for x in os.listdir(img_dir)\n                      if not x.split('.')[0].endswith('_L')])\n        self.images = [x for x in self.images if not x.endswith('.txt')]\n        self.masks = sorted([os.path.join(\n            img_dir, x) for x in os.listdir(img_dir)\n                     if x.split('.')[0].endswith('_L')])\n        self.color_codes = color_codes\n        self.num_classes = len(self.color_codes)\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        \n        img = Image.open(self.images[idx])\n        mask = Image.open(self.masks[idx])\n        \n        if img.mode != 'RGB':\n            img = img.convert('RGB')\n        if mask.mode != 'RGB':\n            mask = mask.convert('RGB')\n            \n        img = np.asarray(img)\n        mask = np.asarray(mask)\n        mask_channels = np.zeros(\n            (mask.shape[0], mask.shape[1]), dtype=np.int64)\n        \n        for i, cls in enumerate(self.color_codes.keys()):\n            color = self.color_codes[cls]\n            sub_mask = np.all(mask==color, axis=-1)*i\n            mask_channels += sub_mask #*i\n\n        if self.transforms is not None:\n            transformed = self.transforms(image=img, masks=mask_channels)\n            img = transformed['image']\n            mask_channels = transformed['masks']\n            \n        mask_channels = mask_channels.astype(np.float32)\n        img = img.astype(np.float32) #/255\n        \n        instance = {'image': torch.from_numpy(img.transpose(2,0,1)),\n                    'mask': torch.from_numpy(mask_channels)}\n        \n        return instance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_deeplab(out_channels=32):\n    \n    model = models.segmentation.deeplabv3_resnet50(\n        pretrained=True)\n    model.classifier = models.segmentation.deeplabv3.DeepLabHead(\n        2048, num_classes=out_channels)\n\n    model.train()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, \n                train_loader, \n                val_loader, \n                criterion= nn.CrossEntropyLoss(),\n                num_epochs=1,\n                device=device):\n\n    model.to(device)\n    #model.eval()\n    model.train()\n    \n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    for epoch in range(1, num_epochs + 1):\n        tr_loss = []\n        val_loss = []\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        \n        for sample in tqdm(train_loader):\n            if sample['image'].shape[0]==1:\n                break\n            inputs = sample['image'].to(device)\n            masks = sample['mask'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            y_pred = outputs['out']\n            y_true = masks\n            loss = criterion(y_pred.float(), y_true.long())\n            loss.backward()\n            tr_loss.append(loss)\n            optimizer.step()\n            #break\n            \n        print(f'Train loss: {torch.mean(torch.Tensor(tr_loss))}')\n        \n        for sample in tqdm(val_loader):\n            if sample['image'].shape[0]==1:\n                break\n            inputs = sample['image'].to(device)\n            masks = sample['mask'].to(device)\n            \n            with torch.no_grad():\n                outputs = model(inputs)\n            y_pred = outputs['out']\n            y_true = masks\n            loss = criterion(y_pred.float(), y_true.long())\n            val_loss.append(loss)\n            optimizer.step()\n            #break\n            \n        print(f'Validation loss: {torch.mean(torch.Tensor(val_loss))}')\n        \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = CamSeqDataset(img_dir='../input/camseq-semantic-segmentation')\ntrain_size = int(len(dataset)*0.85)\ntrain_set, val_set = random_split(dataset, [train_size, len(dataset)-train_size])\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size)\nval_loader = DataLoader(val_set, batch_size=batch_size)\n\nmodel = make_deeplab()\nmodel = train_model(model=model,\n                    train_loader=train_loader,\n                    val_loader=val_loader,\n                    num_epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nfor img in val_loader:\n    image = model(img['image'].to(device))['out']\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img['image'][0].cpu().permute(1,2,0).numpy().astype(np.uint8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=4, ncols=8, sharex=True, sharey=True, figsize=(16,20)) \naxes_list = [item for sublist in axes for item in sublist]\n\nthresh=0.3\nres = image[0].detach().cpu().numpy()\nfor i, mask in enumerate(res):\n    ax = axes_list.pop(0)\n    ax.imshow(np.where(mask>thresh, 255, 0), cmap='gray')\n    ax.set_title(list(color_codes.keys())[i])\n\nfor ax in axes_list: \n    ax.remove()\n    \nplt.tight_layout()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}