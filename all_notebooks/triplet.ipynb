{"cells":[{"metadata":{"_uuid":"33f44d02-744f-4956-8acf-434e56322537","_cell_guid":"5cf3d7dd-5335-4cf1-aa00-275a6f25388b","trusted":true},"cell_type":"code","source":"pip install cached_property","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4c4cae3-6550-4b41-b91d-0e0567f01a64","_cell_guid":"e2ec19f5-3e39-480f-873f-ceca331390d1","trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom cached_property import cached_property\nfrom skimage import io\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms, datasets\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nis_cuda = torch.cuda.is_available()\n%matplotlib inline\n\n\nclass FashionDataset(Dataset):\n    def __init__(self, csv_file, target_column, root_dir, transform=None, is_train=True, training_size=0.8, is_debug=True):\n        self.target_column = target_column\n        self.training_size = training_size\n        self.csv_file = csv_file\n        self.root_dir = root_dir\n        self.transform = transform\n        self.train = is_train\n        self.is_debug = is_debug\n        self.train_df, self.test_df = self._get_df()\n        \n        if self.train: \n            self.train_labels = self.train_df[self.target_column].to_list()\n            self.labels_set = set(self.train_labels)\n            print(f\"# of labels: {len(self.labels_set)}\")\n        else:\n            self.test_labels = self.test_df[self.target_column].to_list()\n            self.labels_set = set(self.test_labels)\n            print(f\"# of labels: {len(self.labels_set)}\")\n            \n\n    def _get_df(self):\n        df = pd.read_csv(os.path.join(self.root_dir, self.csv_file), error_bad_lines=False, warn_bad_lines=False)\n        df = df.sample(frac=1, random_state=29, axis=\"index\")\n        df = df.dropna(axis=0, subset=[self.target_column])\n        \n        if self.is_debug:\n            df = df.head(10000)\n        \n        image_ids = []\n        for fd in glob.glob(os.path.join(self.root_dir, \"images/*.jpg\")):\n            image_id = os.path.split(fd)[1][:-4]\n            image_ids.append(int(image_id))\n        # Take a inner set\n        common_ids = set(df.id) & set(image_ids)\n        \n        print(\"Common ids: \", len(common_ids))\n        df = df.loc[df.id.isin(common_ids)]\n        \n        # Split to training and test\n        random_state = np.random.RandomState(29)\n        df[\"is_train\"] = random_state.choice([True, False], size=df.shape[0], p=[self.training_size, 1-self.training_size])\n        \n        train_df = df.loc[(df.is_train==True)]\n        self.train_df = self.filter_insufficient_labels(train_df, 5)\n        \n        # Only keep the labels in the training set.\n        train_labels_set = set(self.train_df[self.target_column].to_list())\n        test_df = df.loc[(df.is_train==False)]\n        test_df = test_df.loc[(test_df[self.target_column].isin(train_labels_set))]\n        self.test_df = self.filter_insufficient_labels(test_df, 1)\n        \n        return self.train_df, self.test_df\n    \n    \n    def filter_insufficient_labels(self, df, thredhold):\n        count_df = df.groupby(self.target_column).count().id.reset_index(name=\"counts\")\n        df = df.merge(count_df, on=self.target_column, how=\"left\")\n        return df.loc[(df.counts > thredhold)].reset_index()\n\n\n    @cached_property\n    def train_data(self):\n        print(f\"is_training: {self.train}\")\n        self.image_ids = self.train_df.id.to_list()\n        return self._read_images()\n    \n    @cached_property\n    def test_data(self):\n        print(f\"is_training: {self.train}\")\n        self.image_ids = self.test_df.id.to_list()\n        return self._read_images()\n          \n        \n    def _read_images(self):\n        data = []\n        for i in self.image_ids:\n            filename = os.path.join(self.root_dir, \"images\", str(i) + \".jpg\")\n            img = Image.open(filename).convert(\"L\")\n            if self.transform:\n                img = self.transform(img)\n            data.append(img)\n        data = torch.stack(data, dim=0)\n        return data\n    \n    def __len__(self):\n        if self.train:\n            return len(self.train_df)\n        else:\n            return len(self.test_df)\n    \n    def __getitem__(self, idx):\n        if self.train:\n            filename = os.path.join(self.root_dir, \"images\", str(self.train_df.loc[idx].id) + \".jpg\")\n            label = self.train_df.loc[idx][self.target_column]\n        else:\n            filename = os.path.join(self.root_dir, \"images\", str(self.test_df.loc[idx].id) + \".jpg\")\n            label = self.test_df.loc[idx][self.target_column]\n            \n        sample = Image.open(filename).convert(\"L\")\n\n        if self.transform:\n            sample = self.transform(sample)\n            \n        return (sample, label)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fe219f5-eaa7-4c26-997c-aab7c7c651b7","_cell_guid":"26734093-1939-4674-afda-9cc478222d13","trusted":true},"cell_type":"code","source":"class TripletDataset(Dataset):\n    \"\"\"\n    Train: For each sample (anchor) randomly chooses a positive and negative samples\n    Test: Creates fixed triplets for testing\n    \"\"\"\n\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.train = self.dataset.train\n\n        if self.train:\n            self.train_labels = self.dataset.train_labels\n            self.train_data = self.dataset.train_data\n            self.labels_set = set(self.train_labels)\n            self.label_to_indices = {label: np.where(np.array(self.train_labels) == label)[0]\n                                     for label in self.labels_set}\n        else:\n            self.test_labels = self.dataset.test_labels\n            self.test_data = self.dataset.test_data\n            \n            self.labels_set = set(self.test_labels)\n            self.label_to_indices = {label: np.where(np.array(self.test_labels) == label)[0]\n                                     for label in self.labels_set}\n\n            random_state = np.random.RandomState(29)\n            \n            # Generate fixed triplets for testing len(self.test_data)\n            triplets = [[i,\n                         random_state.choice(self.label_to_indices[self.test_labels[i]]),\n                         random_state.choice(self.label_to_indices[\n                                                 np.random.choice(\n                                                     list(self.labels_set - set([self.test_labels[i]]))\n                                                 )\n                                             ])\n                         ]\n                        for i in range(len(self.test_data))] \n            self.test_triplets = triplets\n\n    def __getitem__(self, index):\n        if self.train:\n            img1, label1 = self.train_data[index], self.train_labels[index]            \n            positive_index = index\n#             print(f\"Anchor: {index}\")\n            while positive_index == index:\n                positive_index = np.random.choice(self.label_to_indices[label1])\n#                 print(\"Randomly selecting the Postive sample...\")\n#             print(f\"Positive: {positive_index}\")\n            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n            negative_index = np.random.choice(self.label_to_indices[negative_label])\n#             print(f\"Negative: {negative_index}\")\n            img2 = self.train_data[positive_index]\n            img3 = self.train_data[negative_index]\n        else:\n            img1 = self.test_data[self.test_triplets[index][0]]\n            img2 = self.test_data[self.test_triplets[index][1]]\n            img3 = self.test_data[self.test_triplets[index][2]]\n        \n        # (Anchor, Positive, Negative)\n        return (img1, img2, img3), []\n\n    def __len__(self):\n        return len(self.dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58002c58-fa0f-43d3-9626-a2c08da5bf4a","_cell_guid":"b96a1039-b1f8-4b97-8ece-be71c3ffe51e","trusted":true},"cell_type":"code","source":"class EmbeddingNet(nn.Module):\n    def __init__(self):\n        super(EmbeddingNet, self).__init__()\n        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n                                     nn.MaxPool2d(2, stride=2),\n                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n                                     nn.MaxPool2d(2, stride=2))\n\n        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n                                nn.PReLU(),\n                                nn.Linear(256, 256),\n                                nn.PReLU(),\n                                nn.Linear(256, 2)  # the embedding space is 2\n                                )\n\n    def forward(self, x):\n        output = self.convnet(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc(output)\n        return output\n\n    def get_embedding(self, x):\n        return self.forward(x)\n    \n    \n    \nclass TripletNet(nn.Module):\n    def __init__(self, embedding_net):\n        super(TripletNet, self).__init__()\n        self.embedding_net = embedding_net\n\n    def forward(self, x1, x2, x3):\n        output1 = self.embedding_net(x1)\n        output2 = self.embedding_net(x2)\n        output3 = self.embedding_net(x3)\n        return output1, output2, output3\n\n    def get_embedding(self, x):\n        return self.embedding_net(x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02748cd9-0483-45cb-b7b0-a43c9f4eaffb","_cell_guid":"46a48177-9d61-4440-ab51-e99ad64e7047","trusted":true},"cell_type":"code","source":"def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, is_cuda, log_interval, metrics=[],\n        start_epoch=0):\n    \"\"\"\n    Trainer\n    \"\"\"\n    for epoch in range(0, start_epoch):\n        scheduler.step()\n        \n    print(\"===> Start training...\")\n    for epoch in range(start_epoch, n_epochs):\n        scheduler.step()\n\n        # Train stage\n        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, is_cuda, log_interval, metrics)\n        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n        for metric in metrics:\n            message += '\\t{}: {}'.format(metric.name(), metric.value())\n            \n        # Validation stage    \n        val_loss, metrics = test_epoch(val_loader, model, loss_fn, is_cuda, metrics)\n        val_loss /= len(val_loader)\n        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, val_loss)\n        for metric in metrics:\n            message += '\\t{}: {}'.format(metric.name(), metric.value())\n\n        print(message)\n    print(\"===> Finish training!\")\n\n\ndef train_epoch(train_loader, model, loss_fn, optimizer, is_cuda, log_interval, metrics):\n    for metric in metrics:\n        metric.reset()\n\n    model.train()\n    losses = []\n    total_loss = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        target = target if len(target) > 0 else None\n        if not type(data) in (tuple, list):\n            data = (data,)\n        if is_cuda:\n            data = tuple(d.cuda() for d in data)\n            if target is not None:\n                target = target.cuda()\n\n        \n        optimizer.zero_grad()\n#         data = [len(triplet)=3, batch_size, c, w, h]\n        outputs = model(*data)\n\n        if type(outputs) not in (tuple, list):\n            outputs = (outputs,)\n\n        loss_inputs = outputs\n        if target is not None:\n            target = (target,)\n            loss_inputs += target\n\n        loss_outputs = loss_fn(*loss_inputs)\n        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n        losses.append(loss.item())\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n        for metric in metrics:\n            metric(outputs, target, loss_outputs)\n\n        if batch_idx % log_interval == 0:\n            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                batch_idx * len(data[0]), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), np.mean(losses))\n            for metric in metrics:\n                message += '\\t{}: {}'.format(metric.name(), metric.value())\n\n            print(message)\n            losses = []\n\n    total_loss /= (batch_idx + 1)\n    return total_loss, metrics\n\n\ndef test_epoch(val_loader, model, loss_fn, is_cuda, metrics):\n    with torch.no_grad():\n        for metric in metrics:\n            metric.reset()\n        model.eval()\n        val_loss = 0\n        for batch_idx, (data, target) in enumerate(val_loader):\n            target = target if len(target) > 0 else None\n            if not type(data) in (tuple, list):\n                data = (data,)\n            if is_cuda:\n                data = tuple(d.cuda() for d in data)\n                if target is not None:\n                    target = target.cuda()\n\n            outputs = model(*data)\n\n            if type(outputs) not in (tuple, list):\n                outputs = (outputs,)\n            loss_inputs = outputs\n            if target is not None:\n                target = (target,)\n                loss_inputs += target\n\n            loss_outputs = loss_fn(*loss_inputs)\n            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n            val_loss += loss.item()\n\n            for metric in metrics:\n                metric(outputs, target, loss_outputs)\n\n    return val_loss, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/fashion-product-images-small/myntradataset/styles.csv\", error_bad_lines=False, warn_bad_lines=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34ca57ab-72ab-4568-b8de-4f50ed29b348","_cell_guid":"2e6e0665-400e-40fb-ad8c-a69ce9ae41c8","trusted":true},"cell_type":"code","source":"# Read raw dataset\nmean, std = 0.1307, 0.3081\ntransform = transforms.Compose([\n    transforms.Resize((28, 28)),\n    transforms.ToTensor(),\n#     transforms.Normalize((mean,), (std,))\n])\n\n# target_column = \"subCategory\"\ntarget_column = \"masterCategory\"\nis_debug = True\nroot_dir=\"/kaggle/input/fashion-product-images-small/myntradataset\"\ntrain_dataset = FashionDataset(is_train=True, target_column=target_column, root_dir=root_dir, csv_file=\"styles.csv\", transform=transform, is_debug=is_debug)\nprint(train_dataset.train_data.shape)\ntest_dataset = FashionDataset(is_train=False, target_column=target_column, root_dir=root_dir, csv_file=\"styles.csv\", transform=transform, is_debug=is_debug)\nprint(test_dataset.test_data.shape)\n\n\n# Construct triplet dataset\ntriplet_train_dataset = TripletDataset(train_dataset)\ntriplet_test_dataset = TripletDataset(test_dataset)\n\n\n# Construct triplet dataset loader\nbatch_size = 128\nkwargs = {'num_workers': 10, 'pin_memory': True} if is_cuda else {}\ntriplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\ntriplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n\n\n# Construct raw dataset loader for embedding and plotting\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n\n\n# Set up the network\n# from networks import EmbeddingNet, TripletNet\nembedding_net = EmbeddingNet()\nmodel = TripletNet(embedding_net)\nif is_cuda:\n    model.cuda()\n\n# Set up the loss function\nloss_fn = nn.TripletMarginLoss(margin=1.0)\n\n# Set up the optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n\n\n# Set training parameters\nn_epochs = 20\nlog_interval = 50\nfit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, is_cuda, log_interval)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"071b66c5-2e03-48b2-bc0c-3ab6e401fa71","_cell_guid":"38ab3a13-ffc9-4dd0-9b6e-d6a4cec94465","trusted":true},"cell_type":"code","source":"def plot_embeddings(embeddings, labels, classes, xlim=None, ylim=None):\n    plt.figure(figsize=(8, 8))\n    for i, c in enumerate(classes):\n        inds = np.where(labels==c)[0]\n        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[c])\n    if xlim:\n        plt.xlim(xlim[0], xlim[1])\n    if ylim:\n        plt.ylim(ylim[0], ylim[1])\n    plt.legend(classes, bbox_to_anchor=(1, 1.))\n\n    \ndef extract_embeddings(dataloader, dataset, model):\n    with torch.no_grad():\n        classes = dataset.labels_set\n        model.eval()\n        embeddings = np.zeros((len(dataloader.dataset), 2)) # the embedding space is 2\n        labels = []\n        k = 0\n        for images, targets in dataloader:\n            if is_cuda:\n                images = images.cuda()\n            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n            labels[k:k+len(images)] = list(targets)\n            k += len(images)\n        labels = np.array(labels)\n    return embeddings, labels, classes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d69c4349-b7d3-4325-b3e5-0e45452f515b","_cell_guid":"523f3fca-2940-4b40-9f2d-dac46c2988bd","trusted":true},"cell_type":"code","source":"classes = train_dataset.labels_set\n\n\nrandom_state = np.random.RandomState(30)\ncolors = {}\nfor c in classes:\n    r = lambda: random_state.randint(0, 255)\n    color = '#%02X%02X%02X' % (r(),r(),r())\n    colors[c] = color","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc3127ed-c3d9-4068-8526-61ef920263c9","_cell_guid":"7b168963-c601-4387-8102-1a2ec6f20e76","trusted":true},"cell_type":"code","source":"train_embeddings, train_labels, classes = extract_embeddings(train_loader, train_dataset, model)    \nplot_embeddings(train_embeddings, train_labels, classes)\n\nval_embeddings, val_labels, classes = extract_embeddings(test_loader, test_dataset, model)\nplot_embeddings(val_embeddings, val_labels, classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9aed0dbd-4f00-4ecc-848f-793c283f4aa8","_cell_guid":"9f4cb410-216c-49f3-9721-e8ec7d9fabe9","trusted":true},"cell_type":"code","source":"# model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73ba93d8-e2bf-49dc-9f3e-afeaa3edeef0","_cell_guid":"015a7464-af46-4a2f-a3c1-f9cd173b7e26","trusted":true},"cell_type":"code","source":"embedding_net","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}