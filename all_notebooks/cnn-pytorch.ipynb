{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.porter import PorterStemmer\nfrom gensim.models import Word2Vec\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read dataset\ntrain_df = pd.read_csv(\"../input/imdb-dataset-sentiment-analysis-in-csv-format/Train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first few lines\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of dataset\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see weather our dataset is balanced or imbalanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_df['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph we can see that our dataset is balanced dataset.So that's great"},{"metadata":{},"cell_type":"markdown","source":"## Tokenization"},{"metadata":{},"cell_type":"markdown","source":"**Tokenization** is the process of tokenizing or splitting a string, text into a list of tokens.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in train_df['text']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming"},{"metadata":{},"cell_type":"markdown","source":"**Stemming** is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.The input to the stemmer is tokenized words.\nA stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate”   "},{"metadata":{"trusted":true},"cell_type":"code","source":"porter_stemmer = PorterStemmer()\ntrain_df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in train_df['tokenized_text']]\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Train Test Split Function\ndef split_train_test(train_df, test_size=0.3, shuffle_state=True):\n    X_train, X_test, Y_train, Y_test = train_test_split(train_df[['stemmed_tokens']], \n                                                        train_df['label'], \n                                                        shuffle=shuffle_state,\n                                                        test_size=test_size, \n                                                        random_state=15)\n    print(\"Value counts for Train sentiments\")\n    print(Y_train.value_counts())\n    print(\"Value counts for Test sentiments\")\n    print(Y_test.value_counts())\n    print(type(X_train))\n    print(type(Y_train))\n    X_train = X_train.reset_index()\n    X_test = X_test.reset_index()\n    Y_train = Y_train.to_frame()\n    Y_train = Y_train.reset_index()\n    Y_test = Y_test.to_frame()\n    Y_test = Y_test.reset_index()\n    print(X_train.head())\n    return X_train, X_test, Y_train, Y_test\n\n# Call the train_test_split\nX_train, X_test, Y_train, Y_test = split_train_test(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec Model Creation"},{"metadata":{},"cell_type":"markdown","source":"**word2vec** algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. "},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 500\nwindow = 3\nmin_count = 1\nworkers = 3\n# 0 for CBOW, 1 for skip-gram\nsg = 0\nOUTPUT_FOLDER = '/kaggle/working/'\n# Function to train word2vec model\ndef make_word2vec_model(train_df, padding, sg, min_count, size, workers, window):\n    if  padding:\n        #print(len(train))\n        temp_df = pd.Series(train_df['stemmed_tokens']).values\n        temp_df = list(temp_df)\n        temp_df.append(['pad'])\n        #print(str(size))\n        word2vec_file = OUTPUT_FOLDER + '2ata' + '_PAD.model'\n    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n\n    w2v_model.save(word2vec_file)\n    return w2v_model, word2vec_file\n\n# Train Word2vec model\nw2vmodel, word2vec_file = make_word2vec_model(train_df, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Padding"},{"metadata":{},"cell_type":"markdown","source":"All the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. In other words, naturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, this is where the padding is necessary.\n**Padding** will make all sentences of same length by inserting 0 in the end or bigenning of the sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sen_len = train_df.stemmed_tokens.map(len).max()\n\npadding_idx = w2vmodel.wv.vocab['pad'].index\nprint(padding_idx)\ndef make_word2vec_vector_cnn(sentence):\n    padded_X = [padding_idx for i in range(max_sen_len)]\n    i = 0\n    for word in sentence:\n        if word not in w2vmodel.wv.vocab:\n            padded_X[i] = 0\n        else:\n            padded_X[i] = w2vmodel.wv.vocab[word].index\n        i += 1\n    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sen_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padding_idx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN model "},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_SIZE = 500\nNUM_FILTERS = 10\n\n#torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], \n#stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, \n#dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\n\nclass CnnTextClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n        super(CnnTextClassifier, self).__init__()\n        w2vmodel = gensim.models.KeyedVectors.load(OUTPUT_FOLDER + '2ata_PAD.model')\n        weights = w2vmodel.wv\n        # With pretrained embeddings\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n        \n        # like a python list, it was designed to store any desired number of nn.Module\n        self.convs = nn.ModuleList([\n                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n                                   for window_size in window_sizes\n        ])\n    \n        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x) # [B, T, E]\n\n        # Apply a convolution + max_pool layer for each window size\n        x = torch.unsqueeze(x, 1)\n        xs = []\n        for conv in self.convs:\n            x2 = torch.tanh(conv(x))\n            x2 = torch.squeeze(x2, -1)\n            x2 = F.max_pool1d(x2, x2.size(2))\n            xs.append(x2)\n        x = torch.cat(xs, 2)\n\n        # FC\n        x = x.view(x.size(0), -1)\n        logits = self.fc(x)\n\n        probs = F.softmax(logits, dim = 1)\n\n        return probs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_target(label):\n    if label == 0:\n        return torch.tensor([0], dtype=torch.long, device=device)\n    elif label == 1:\n        return torch.tensor([1], dtype=torch.long, device=device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_CLASSES = 2\nVOCAB_SIZE = len(w2vmodel.wv.vocab)\nprint(VOCAB_SIZE)\ncnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\ncnn_model.to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)\nnum_epochs = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Open the file for writing loss\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nf = open(loss_file_name,'w')\nf.write('iter, loss')\nf.write('\\n')\nlosses = []\n\ncnn_model.train()\nfor epoch in range(num_epochs):\n    print(\"Epoch\" + str(epoch + 1))\n    train_loss = 0\n\n    for index, row in X_train.iterrows():\n        # Clearing the accumulated gradients\n        cnn_model.zero_grad()\n\n        # Make the bag of words vector for stemmed tokens \n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n       \n        # Forward pass to get output\n        probs = cnn_model(bow_vec)\n\n        # Get the target label\n        #print(Y_train['label'][index])\n        target = make_target(Y_train['label'][index])\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = loss_function(probs, target)\n        train_loss += loss.item()\n        \n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n    print(f'train_loss : {train_loss / len(X_train)}')\n    print(\"Epoch ran :\"+ str(epoch+1))\n    \n    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n    f.write('\\n')\n    train_loss = 0\n\ntorch.save(cnn_model, OUTPUT_FOLDER + 'cnn_big_model_500_with_padding.pth')\n\nf.close()\nprint(\"Input vector\")\nprint(\"Probs\")\nprint(probs)\nprint(torch.argmax(probs, dim=1).cpu().numpy()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_cnn_predictions = []\noriginal_lables_cnn_bow = []\ncnn_model.eval()\nloss_df = pd.read_csv(OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv')\nprint(loss_df.columns)\n# loss_df.plot('loss')\n\ny_pred_list = []\ny_true_list = []\n\nwith torch.no_grad():\n    for index, row in X_test.iterrows():\n        #print(row['stemmed_tokens'])\n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n        #print(bow_vec)\n        probs = cnn_model(bow_vec)\n        #print(probs.data)\n        _, predicted = torch.max(probs.data,  1)\n        \n        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n        original_lables_cnn_bow.append(make_target(Y_test['label'][index]).cpu().numpy()[0])\n\nprint(confusion_matrix(original_lables_cnn_bow, bow_cnn_predictions))\n#print(original_lables_cnn_bow)\nprint(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nloss_df = pd.read_csv(loss_file_name)\nprint(loss_df.columns)\nplt_500_padding_30_epochs = loss_df[' loss'].plot()\nfig = plt_500_padding_30_epochs.get_figure()\nfig.savefig(OUTPUT_FOLDER + '1loss_plt_500_padding_30_epochs.pdf')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}