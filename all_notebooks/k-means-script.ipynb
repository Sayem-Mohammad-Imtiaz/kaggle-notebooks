{"cells":[{"metadata":{},"cell_type":"markdown","source":"# K - Means Overview\n\nThis is a pure Python implementation of the K-Means clustering algorithm.\n"},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:44:54.167244Z","iopub.status.busy":"2020-04-23T08:44:54.166706Z","iopub.status.idle":"2020-04-23T08:45:05.973676Z","shell.execute_reply":"2020-04-23T08:45:05.971845Z","shell.execute_reply.started":"2020-04-23T08:44:54.166949Z"},"trusted":true},"cell_type":"code","source":"import math\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom copy import deepcopy\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics.pairwise import euclidean_distances\n","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:05.976777Z","iopub.status.busy":"2020-04-23T08:45:05.976382Z","iopub.status.idle":"2020-04-23T08:45:09.143455Z","shell.execute_reply":"2020-04-23T08:45:09.142692Z","shell.execute_reply.started":"2020-04-23T08:45:05.976723Z"},"trusted":true},"cell_type":"code","source":"from bokeh.plotting import figure, show, ColumnDataSource\nimport bokeh.models as bmo\nfrom bokeh.io import output_notebook\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simulated Data\nSKlearn has a couple different methods for simulating data for testing and development purposes. We're generating a serries of fake cluster data with 5 clusters and 500 observations. THe data will only have two dimensions."},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.145046Z","iopub.status.busy":"2020-04-23T08:45:09.144765Z","iopub.status.idle":"2020-04-23T08:45:09.16192Z","shell.execute_reply":"2020-04-23T08:45:09.160567Z","shell.execute_reply.started":"2020-04-23T08:45:09.145Z"},"trusted":true},"cell_type":"code","source":"#Going to make fake data using Sklearn\nX, y = make_blobs(n_samples=500, centers=5, n_features=2,\n                  random_state=745)\n\ndf = pd.DataFrame({'X_1': X[:,0],\n                  'X_2':X[:,1],\n                  'Y':y})\n\ndf['group'] = df['Y'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Charting in Bokeh"},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.163356Z","iopub.status.busy":"2020-04-23T08:45:09.163154Z","iopub.status.idle":"2020-04-23T08:45:09.198035Z","shell.execute_reply":"2020-04-23T08:45:09.19643Z","shell.execute_reply.started":"2020-04-23T08:45:09.163322Z"},"trusted":true},"cell_type":"code","source":"deloitte_palette = [\"#000000\",\"#86BC25\",\"#C4D600\",\"#43B02A\",\"#046A38\",\"#2C5234\", \"#0097A9\", \"#62B5E5\",\n                   \"#00A3E0\", \"#0076A8\", \"#012169\"]\n\nsource = ColumnDataSource(df)\n\ncolor_map = bmo.CategoricalColorMapper(factors=df['group'].unique(), palette=deloitte_palette)\n\np = figure()\n\np.circle(x='X_1', y=\"X_2\", radius=.23, \n         fill_alpha=0.6, source=source, \n         fill_color={'field' : 'group', 'transform': color_map})","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.200085Z","iopub.status.busy":"2020-04-23T08:45:09.199675Z","iopub.status.idle":"2020-04-23T08:45:09.355704Z","shell.execute_reply":"2020-04-23T08:45:09.354724Z","shell.execute_reply.started":"2020-04-23T08:45:09.200036Z"},"trusted":true},"cell_type":"code","source":"show(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Means Algorithm: *By-Hand Example*\n\n1. [Choose $k$ initial centroids (note that $k$ is an input)](#Step1)\n2. For each point $p$:\n  - Find distance to each centroid\n  - Assign point to nearest centroid\n3. Recalculate centroid positions\n4. Repeat steps 2-3 until stopping criteria met"},{"metadata":{},"cell_type":"markdown","source":"### Step 1: Choosing Initial Centroids <a id=Step1></a>\n\nThere are several optiosn to pick an initial centroid positions:\n1. Randomly (may yield divergent behavior)\n2. Perform alternative clustering task, use resulting centroids as initial k-means centroids\n3. Start with global centroid, choose point at max distance, repeat (but might select outlier)"},{"metadata":{},"cell_type":"markdown","source":"#### Random initial centroids"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.357466Z","iopub.status.busy":"2020-04-23T08:45:09.357049Z","iopub.status.idle":"2020-04-23T08:45:09.380891Z","shell.execute_reply":"2020-04-23T08:45:09.379086Z","shell.execute_reply.started":"2020-04-23T08:45:09.357419Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"k = 5\n\nr = np.random.randint(low=0, high=X.shape[0], size=k)\ninitial = X[r,:]\n\nprint(\"Our initial centroids:\")\ninitial","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.384243Z","iopub.status.busy":"2020-04-23T08:45:09.383213Z","iopub.status.idle":"2020-04-23T08:45:09.417174Z","shell.execute_reply":"2020-04-23T08:45:09.411659Z","shell.execute_reply.started":"2020-04-23T08:45:09.383774Z"},"trusted":true},"cell_type":"code","source":"p.diamond_cross(x=initial[:,0], \n                y=initial[:,1], \n                size=20, \n                color=\"#386CB0\", \n                fill_color=None, \n                line_width=2)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.424984Z","iopub.status.busy":"2020-04-23T08:45:09.424185Z","iopub.status.idle":"2020-04-23T08:45:09.536168Z","shell.execute_reply":"2020-04-23T08:45:09.534851Z","shell.execute_reply.started":"2020-04-23T08:45:09.42465Z"},"trusted":true},"cell_type":"code","source":"show(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2: Assessing Similarity\n\nHow do you determine which centroid a given point is most similar to? The similarity criterion is determined by the measure we choose. In the case of k-means clustering, the most common similarity metric is *__Euclidean distance:__*\n\n$$ d(x_1,x_2) = \\sqrt{\\sum_{i=1}^N(x_{1i} - x_{2i})^2} $$\n\nBoth `numpy` and `sklearn` have implementations of euclidian which we can leverage. "},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.540055Z","iopub.status.busy":"2020-04-23T08:45:09.538169Z","iopub.status.idle":"2020-04-23T08:45:09.553332Z","shell.execute_reply":"2020-04-23T08:45:09.551475Z","shell.execute_reply.started":"2020-04-23T08:45:09.538899Z"},"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import euclidean_distances\n\ndist = euclidean_distances(X, initial)\ndist","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.556569Z","iopub.status.busy":"2020-04-23T08:45:09.555919Z","iopub.status.idle":"2020-04-23T08:45:09.563713Z","shell.execute_reply":"2020-04-23T08:45:09.561674Z","shell.execute_reply.started":"2020-04-23T08:45:09.55646Z"},"trusted":true},"cell_type":"code","source":"cluster = np.argmin(dist, axis=1)\ncluster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Recompute the Center\nHow do we recompute the positions of the centers at each iteration of the algorithm?\n\nWe calculate the centroid at the geometric center of our new assigned clusters. `Pandas` has significanly eaiser ways to perform group by operations over `numpy`."},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.566263Z","iopub.status.busy":"2020-04-23T08:45:09.565312Z","iopub.status.idle":"2020-04-23T08:45:09.623908Z","shell.execute_reply":"2020-04-23T08:45:09.621285Z","shell.execute_reply.started":"2020-04-23T08:45:09.566171Z"},"trusted":true},"cell_type":"code","source":"points = pd.DataFrame.from_records(X, columns=['x', 'y'])\npoints['cluster'] = cluster\n\npoints.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.627401Z","iopub.status.busy":"2020-04-23T08:45:09.626638Z","iopub.status.idle":"2020-04-23T08:45:09.663402Z","shell.execute_reply":"2020-04-23T08:45:09.66144Z","shell.execute_reply.started":"2020-04-23T08:45:09.627281Z"},"trusted":true},"cell_type":"code","source":"centroids = points.groupby('cluster').mean()\ncentroids","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.668576Z","iopub.status.busy":"2020-04-23T08:45:09.667709Z","iopub.status.idle":"2020-04-23T08:45:09.780018Z","shell.execute_reply":"2020-04-23T08:45:09.778165Z","shell.execute_reply.started":"2020-04-23T08:45:09.668473Z"},"trusted":true},"cell_type":"code","source":"p.diamond_cross(x=centroids.x, \n                y=centroids.y, \n                size=20, \n                color=\"#ff0000\", \n                fill_color=None, \n                line_width=2)\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.786647Z","iopub.status.busy":"2020-04-23T08:45:09.781525Z","iopub.status.idle":"2020-04-23T08:45:09.998323Z","shell.execute_reply":"2020-04-23T08:45:09.997495Z","shell.execute_reply.started":"2020-04-23T08:45:09.786549Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"old = pd.DataFrame.from_records(initial, columns=[\"x_old\", \"y_old\"])\ncentroids = pd.concat([centroids,old],axis=1)\ncentroids.head()\n\ndef x_line(row):\n    return [row['x'],row['x_old']]\n\ndef y_line(row):\n    return [row['y'], row['y_old']]\n\ncentroids['xs'] = centroids.apply(x_line, axis=1)\ncentroids['ys'] = centroids.apply(y_line, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.999371Z","iopub.status.busy":"2020-04-23T08:45:09.999151Z","iopub.status.idle":"2020-04-23T08:45:10.089823Z","shell.execute_reply":"2020-04-23T08:45:10.087459Z","shell.execute_reply.started":"2020-04-23T08:45:09.999332Z"},"trusted":true},"cell_type":"code","source":"p.multi_line(xs=centroids['xs'], ys=centroids['ys'],color=\"navy\", alpha=0.3, line_width=4)\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4: Converge\nWe iterate until some stopping criteria are met; in general, suitable convergence is achieved in a small number of steps. \n\nStopping criteria can be based on the centroids (eg, if positiosn change by no more than $\\epsilon$) or on the points (if no more than x% change clusters between iterations).\n\nUp to this point, we have been using illustrative examples of the steps. Now, we will wrap up our work in a KMeans class with some helper functions to iterate through the steps and fit the model. "},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:10.09215Z","iopub.status.busy":"2020-04-23T08:45:10.091699Z","iopub.status.idle":"2020-04-23T08:45:10.127252Z","shell.execute_reply":"2020-04-23T08:45:10.125385Z","shell.execute_reply.started":"2020-04-23T08:45:10.092079Z"},"trusted":true},"cell_type":"code","source":"def centroid(data):\n    \"\"\"Find the centroid of the given data.\"\"\"\n    return np.mean(data, 0)\n\n\ndef sse(data):\n    \"\"\"Calculate the SSE of the given data.\"\"\"\n    u = centroid(data)\n    return np.sum(np.linalg.norm(data - u, 2, 1))\n\n\nclass KMeansClusterer:\n    \"\"\"The standard k-means clustering algorithm.\"\"\"\n\n    def __init__(self, data=None, k=2, min_gain=0.01, max_iter=100,\n                 max_epoch=10, verbose=True):\n        \"\"\"Learns from data if given.\"\"\"\n        if data is not None:\n            self.fit(data, k, min_gain, max_iter, max_epoch, verbose)\n\n    def fit(self, data, k=2, min_gain=0.01, max_iter=100, max_epoch=10,\n            verbose=True):\n        \"\"\"Learns from the given data.\n        Args:\n            data:      The dataset with m rows each with n features\n            k:         The number of clusters\n            min_gain:  Minimum gain to keep iterating\n            max_iter:  Maximum number of iterations to perform\n            max_epoch: Number of random starts, to find global optimum\n            verbose:   Print diagnostic message if True\n        Returns:\n            self\n        \"\"\"\n        # Pre-process\n        self.data = np.matrix(data)\n        self.k = k\n        self.min_gain = min_gain\n        self.meta = []\n\n        # Perform multiple random init for global optimum\n        min_sse = np.inf\n        for epoch in range(max_epoch):\n\n            # Randomly initialize k centroids\n            indices = np.random.choice(len(data), k, replace=False)\n            u = self.data[indices, :]\n\n            # Loop\n            t = 0\n            old_sse = np.inf\n            while True:\n                t += 1\n\n                # Cluster assignment\n                C = [None] * k\n                for x in self.data:\n                    j = np.argmin(np.linalg.norm(x - u, 2, 1))\n                    C[j] = x if C[j] is None else np.vstack((C[j], x))\n\n                # Centroid update\n                for j in range(k):\n                    u[j] = centroid(C[j])\n\n                # Loop termination condition\n                if t >= max_iter:\n                    break\n                new_sse = np.sum([sse(C[j]) for j in range(k)])\n                gain = old_sse - new_sse\n                if verbose:\n                    line = \"Epoch {:2d} Iter {:2d}: SSE={:10.4f}, GAIN={:10.4f}\"\n                    print(line.format(epoch, t, new_sse, gain))\n                if gain < self.min_gain:\n                    if new_sse < min_sse:\n                        min_sse, self.C, self.u = new_sse, C, u\n                    break\n                else:\n                    old_sse = new_sse\n\n            if verbose:\n                print('')  # blank line between every epoch\n\n        return self","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:10.13015Z","iopub.status.busy":"2020-04-23T08:45:10.129197Z","iopub.status.idle":"2020-04-23T08:45:13.244777Z","shell.execute_reply":"2020-04-23T08:45:13.242521Z","shell.execute_reply.started":"2020-04-23T08:45:10.130074Z"},"trusted":true},"cell_type":"code","source":"t = KMeansClusterer(data=X, k=5)\nt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### K means CLustering using sklearn\n# X is the mocked up data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Find the optimum value of K \nX, y = make_blobs(n_samples=500, centers=5, n_features=2,\n                  random_state=745)\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### k = 5 is giving best Silhuette constant value\n# we can create 5 clusters\n\nkmeans = KMeans(n_clusters=5, random_state=0).fit(X)\nkmeans.labels_\nkmeans.cluster_centers_\n\n# Adding this k means label to original data\n# cluster_sklearn=pd.Dat\npoints['cluster_sklearn'] = kmeans.labels_\npoints.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"facet = sns.lmplot(data=points, x='x', y='y', hue='cluster_sklearn', \n                   fit_reg=False, legend=True, legend_out=True)\nfacet = sns.lmplot(data=points, x='x', y='y', hue='cluster', \n                   fit_reg=False, legend=True, legend_out=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"points['cluster_sklearn'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### Change the hyperparameters\n\n### k = 5 is giving best Silhuette constant value\n# we can create 5 clusters\n\nkmeans = KMeans(n_clusters=5, random_state=0).fit(X)\nkmeans.labels_\nkmeans.cluster_centers_\n\n# Adding this k means label to original data\n# cluster_sklearn=pd.Dat\npoints['cluster_sklearn'] = kmeans.labels_\npoints.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}