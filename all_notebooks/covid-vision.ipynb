{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary\n\n- I approach the challenge as a two level classification problem. \n- The null values in the dataset are filled with predefined labels (See the code).\n- I dropped the Label column and used the first and second category labels to classify the image in two levels.\n- I randomly select 15% of the COVID-19 labeled images from train set and moved them to test set.\n- I removed the samples with labels, Stress-smoking, Streptococcus and SARS to reduce the dimensionality.\n- I used pretrained resnet18 as the main feature extractor to benefit from transfer learning. \n- I implemented a 2 blocks of CNN-BatchNorm-Relu layers as a side network to resnet.\n- Input image is passed through resnet and the model predicts the Level-1 classes\n- The **center-cropped version** of the input image is passed through the side network. \n- The model predicts the Level-2 classes using both side network's output and Level-1 predictions.\n- The combination of Level-1 and Level-2 prediction losses are combined and the model's weights are updated using the combined loss\n\nI was inspired by this excellent [notebook](https://www.kaggle.com/timstefaniak/multi-classification-of-x-ray-images) for data preparing and visualization."},{"metadata":{},"cell_type":"markdown","source":"## Import Necessary Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Union, List\n\nimport os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\n\nfrom torchvision import models\nfrom torch.cuda import device_count\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.metrics.functional import f1, accuracy\n\nfrom PIL import Image\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/coronahack-chest-xraydataset\"\nimage_dir = os.path.join(data_dir, \"Coronahack-Chest-XRay-Dataset\", \"Coronahack-Chest-XRay-Dataset\")\ntrain_dir = os.path.join(data_dir, \"Coronahack-Chest-XRay-Dataset\", \"Coronahack-Chest-XRay-Dataset\", \"train\")\ntest_dir = os.path.join(data_dir, \"Coronahack-Chest-XRay-Dataset\", \"Coronahack-Chest-XRay-Dataset\", \"test\")\nmodel_path = \"model\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_RESIZE = 224","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Metadata Dataframe and Analyze"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df = pd.read_csv(os.path.join(data_dir, 'Chest_xray_Corona_Metadata.csv'), index_col=[0])\nmeta_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Summary of MetaData"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"meta_summary_df = pd.read_csv(os.path.join(data_dir, 'Chest_xray_Corona_dataset_Summary.csv'), index_col=[0])\nprint(meta_summary_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are only 2 samples with Pnemonia that have Stress-Smoking as Label_1. I dropped these rows to reduce the output dimensionality. \n\nAgain, there are only 5 samples with Label_2 \"Streptococcus\" and 4 samples with Label_2 \"SARS\", to keep it simple I also removed these labels from dataset and assigned NaN to Label_2 of these samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop rows with Label1, 'Stress-Smoking'\nmeta_df.drop(meta_df[meta_df['Label_1_Virus_category'] == 'Stress-Smoking'].index, inplace=True)\n# assign None value to Label2, to samples with Label2, 'SARS' and 'Streptococcus'\nmeta_df.loc[meta_df[meta_df['Label_2_Virus_category'] == 'SARS'].index, 'Label_2_Virus_category'] = np.NaN\nmeta_df.loc[meta_df[meta_df['Label_2_Virus_category'] == 'Streptococcus'].index, 'Label_2_Virus_category'] = np.NaN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find Null Values and Plot Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = meta_df.isnull().sum()\nmissing_values.loc[['Label_2_Virus_category', 'Label_1_Virus_category']].plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Replace Null Values with Prespecified Labels"},{"metadata":{},"cell_type":"markdown","source":"Since I will do a multi-stage classification, I fill the NaN values in Label_1 column of samples with 'Normal' label as 'Normal' again\n\nThen, I assign \n- 'Bacteria-unknown' to Label_2 column of all the samples with 'bacteria' Label_1\n- 'Normal-2' to Label_2 column of all the samples with 'Normal' Label_1\n- 'Virus-unknown' to Label_2 column of all the samples with 'Virus' Label_1 and without 'COVID-19' Label_2\n\nFinally I change the names in Label_1 as below\n- 'Normal' -> 'Normal'\n- 'Virus'-> 'Pnemonia-Virus'\n- 'bacteria' -> 'Pnemonia-Bacteria'"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"column_nan_values = {'Label_1_Virus_category': 'Normal'}\nmeta_df.fillna(value=column_nan_values, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.loc[meta_df['Label_1_Virus_category'] == 'bacteria', 'Label_2_Virus_category'] = 'Bacteria-unknown'\nmeta_df.loc[meta_df['Label_1_Virus_category'] == 'Normal', 'Label_2_Virus_category'] = 'Normal-2'\nmeta_df.loc[(meta_df['Label_1_Virus_category'] == 'Virus') & (meta_df['Label_2_Virus_category'] != 'COVID-19'), 'Label_2_Virus_category'] = 'Virus-unknown'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_label_2_labels = {'Virus': 'Pnemonia-Virus', 'bacteria': 'Pnemonia-Bacteria'}\nmeta_df['Label_1_Virus_category'].replace(new_label_2_labels, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Assign Integer Labels to Use in Training and Testing the Model\nWe can drop the 'Label' column as we will not be using it anymore"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.drop(columns=['Label'], inplace=True)\nprint(meta_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"level_1_labels_to_ids = {\n    'Normal' : 0,\n    'Pnemonia-Virus': 1,\n    'Pnemonia-Bacteria': 2\n}\n\nlevel_2_labels_to_ids = {\n    'Normal-2' : 0,\n    'Virus-unknown' : 1,\n    'COVID-19' : 2,\n    'Bacteria-unknown': 3\n}\n\n# Keep the reverse mappings as well to use it when restoring label names from model predictions later on.\nlevel_1_id2label = {v: k for k, v in level_1_labels_to_ids.items()}\nlevel_2_id2label = {v: k for k, v in level_2_labels_to_ids.items()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we will be classifying the images in two stages, we will assign two-level labels as level_1_target and level_2_target.\n\nlevel_1_target and level_2_target will be representing the Label_1 and Label_2 columns respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df['level_1_target'] = meta_df['Label_1_Virus_category'].map(level_1_labels_to_ids)\nmeta_df['level_2_target'] = meta_df['Label_2_Virus_category'].map(level_2_labels_to_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the new version of our dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Balance Test Set Labels"},{"metadata":{},"cell_type":"markdown","source":"There are 58 samples with Label_2 COVID-19 in train set but there is no samples with such a condition in test set. In order to measure the accuracy on COVID-19 labels during testing, I randomly selected 15% of the samples with COVID-19 labels and added them to test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get 15% of the COVID-19 labeled samples' indices randomly\ntest_idx = meta_df[meta_df['Label_2_Virus_category'] == 'COVID-19'].sample(frac=0.15, random_state=1).index\n# meta_df.loc[test_idx, 'Dataset_type'] = 'TEST'","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(\"Number of test samples before adding COVID-19 samples: \", len(meta_df[meta_df['Dataset_type'] == 'TEST']))\nprint(\"Number of train samples before removing COVID-19 samples: \", len(meta_df[meta_df['Dataset_type'] == 'TRAIN']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the COVID-19 labeled samples as a separate dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_samples = meta_df.loc[test_idx]\nprint(f\"{len(covid_samples)} COVID-19 samples will be moved to test set\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Concat covid_samples with test samples\n2. Remove the intersection of train samples and covid_samples from train_df"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.concat([meta_df[meta_df['Dataset_type'] == 'TEST'], covid_samples])\ntrain_df = meta_df[meta_df['Dataset_type'] == 'TRAIN']\ntrain_df = train_df[~train_df['X_ray_image_name'].isin(covid_samples['X_ray_image_name'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of test samples after adding COVID-19 samples: \", len(test_df))\nprint(\"Number of train_df samples after removing COVID-19 samples: \", len(train_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just in case, let's verify that all image files are accessible."},{"metadata":{"trusted":true},"cell_type":"code","source":"assert all([os.path.isfile(os.path.join(image_dir,dset.lower(),filename)) for filename, dset in train_df[['X_ray_image_name', 'Dataset_type']].values])\nassert all([os.path.isfile(os.path.join(image_dir,dset.lower(),filename)) for filename, dset in test_df[['X_ray_image_name', 'Dataset_type']].values])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Print the Frequency of Labels in Train and Test Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"***Train data***\\n\")\nprint(\"-Label_1 Frequency-\\n\", train_df['Label_1_Virus_category'].value_counts(), \"\\n\")\nprint(\"-Label_2 Frequency-\\n\", train_df['Label_2_Virus_category'].value_counts(), \"\\n\")\n\nprint(\"***Test data***\\n\")\nprint(\"-Label_1 Frequency-\\n\", test_df['Label_1_Virus_category'].value_counts(), \"\\n\")\nprint(\"-Label_2 Frequency-\\n\", test_df['Label_2_Virus_category'].value_counts(), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Print the Length of Train and Test Sample Counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train set length: {len(train_df)}\")\nprint(f\"Test set length: {len(test_df)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CovidDataset(Dataset):\n    \"\"\"Covid19 Chest X-Ray dataset class\n\n    Args:\n        df (pandas.DataFrame): DataFrame that contains meta_data about dataset.\n        root_dir: (str): Relative path to root directory that contains images\n        transform (callable, optional): Optional transform to be applied on a sample.\n    \"\"\"\n\n    def __init__(self,\n                 df,\n                 root_dir,\n                 transform):\n        self.df = df\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_metadata = self.df.iloc[idx]\n        img_path = os.path.join(self.root_dir,img_metadata['Dataset_type'].lower(), img_metadata['X_ray_image_name'])\n\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        # apply transforms to imaga, i.e. resize, normalize, rescale\n        image = self.transform(image)\n\n        target_1 = torch.as_tensor(img_metadata['level_1_target'])\n        target_2 = torch.as_tensor(img_metadata['level_2_target'])\n\n        sample = {\n            'image': image,\n            'target_1': target_1,\n            'target_2': target_2\n        }\n        return sample","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Identity(nn.Module):\n    \"\"\"\n    No operation layer. \n    \"\"\"\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass TwoLevelClassifier(nn.Module):\n\n    def __init__(self,\n                 num_level_1_classes,\n                 num_level_2_classes,\n                 img_size):\n        super().__init__()\n\n        h, w = img_size\n        # edges of cropped image\n        self.h1 = h - h // 2\n        self.h2 = h + h // 2\n        self.w1 = w - w // 2\n        self.w2 = w + w // 2\n\n        self.resnet = models.resnet18(pretrained=True, progress=True)\n        resnet_features = self.resnet.fc.in_features\n        \n        # We will not be using resnet's classifier directly, so change it to identity layer\n        self.resnet.fc = Identity()\n        \n        # side stack is where the center cropped image is fed. It consists of\n        # 2 Conv-BatchNorm-Relu blocks, AdaptiveAvgPooling and a Linear layer that\n        # projects the features to number of level 1 dimensions\n        self.side_stack = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32,64, kernel_size=3, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64,64, kernel_size=3, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(64, num_level_1_classes)\n        )\n\n        # Possible outcomes of level1 'Normal', 'Virus' or 'Bacteria'\n        self.level1_classifier = nn.Linear(resnet_features, num_level_1_classes)\n        # Possible outcomes of level2 'Normal-2', 'Virus-unknown', 'Virus-COVID-19', 'Bacteria-unknown'\n        self.level2_classifier = nn.Linear(num_level_1_classes, num_level_2_classes)\n\n    def forward(self, x):\n        \n        # get the features from pretrained model\n        features = self.resnet(x)\n        \n        # predict level 1 classes\n        logits1 = self.level1_classifier(features)\n        \n        # center crop the image\n        cropped_x = x[:, :, self.h1:self.h2, self.w1:self.w2]\n        \n        # pass the cropped image to side_stack to calculate level 2 features\n        # and add level 1 class predictions to level 2 features so that \n        # level 1 classes can have an impact on level 2 classes\n        level_2_feed = self.side_stack(cropped_x) + logits1\n        \n        # predict level 2 classes\n        logits2 = self.level2_classifier(level_2_feed)\n\n        return logits1, logits2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define a wrapper Pytorch Lightning Class to Train and Evaluate the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ModelWrapper(LightningModule):\n    def __init__(self, hparams, df_train=None, df_test=None):\n        super().__init__()\n\n        self.df_train = df_train\n        self.df_test = df_test\n        self.hparams = hparams\n        self.batch_size = self.hparams['batch_size']\n        self.lr = self.hparams['lr']\n        self.num_workers = self.hparams['num_workers']\n\n        if df_train is not None:  #\n            train_transforms = transforms.Compose([\n                # transforms.ToPILImage(mode='RGB'),\n                transforms.Resize([IMG_RESIZE, IMG_RESIZE]),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n\n            test_transforms = transforms.Compose([\n                # transforms.ToPILImage('RGB'),\n                transforms.Resize([IMG_RESIZE, IMG_RESIZE]),\n                transforms.ToTensor(),\n                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n\n            self.train_dataset = CovidDataset(df=df_train, root_dir=self.hparams['image_dir'], transform=train_transforms)\n            self.test_dataset = CovidDataset(df=df_test, root_dir=self.hparams['image_dir'], transform=test_transforms)\n\n        self.model = TwoLevelClassifier(\n            num_level_1_classes=self.hparams['num_level1_classes'],\n            num_level_2_classes=self.hparams['num_level2_classes'],\n            img_size=self.hparams['img_size']\n        )\n\n        # self.model.to(self.device)\n\n        self.loss_level1 = nn.CrossEntropyLoss()\n        self.loss_level2 = nn.CrossEntropyLoss(weight=torch.as_tensor(self.hparams['label2_weights']))\n\n        self.loss_weights = self.hparams['loss_weights']\n\n    def forward(self, batch):\n        return self.model(batch)\n\n    def train_dataloader(self) -> DataLoader:\n        return DataLoader(dataset=self.train_dataset, batch_size=self.batch_size,\n                          num_workers=self.num_workers, shuffle=True)\n\n    def val_dataloader(self) -> Union[DataLoader, List[DataLoader]]:\n        return DataLoader(dataset=self.test_dataset, batch_size=self.batch_size,\n                          num_workers=self.num_workers, shuffle=False)\n\n    def training_step(self, batch, batch_idx):\n\n        x, y_level1, y_level2 = batch['image'], \\\n                                batch['target_1'], \\\n                                batch['target_2']\n\n        logits1, logits2 = self(x)\n\n        loss1 = self.loss_level1(logits1, y_level1)\n        loss2 = self.loss_level2(logits2, y_level2)\n\n        loss = loss1 * self.loss_weights[0] + loss2 * self.loss_weights[1]\n\n        self.log('train/loss', loss, on_step=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n\n        x, y_level1, y_level2 = batch['image'], \\\n                                batch['target_1'], \\\n                                batch['target_2']\n        logits1, logits2 = self(x)\n\n        loss1 = self.loss_level1(logits1, y_level1)\n        loss2 = self.loss_level2(logits2, y_level2)\n        loss = loss1 + loss2\n\n        level1_preds = torch.argmax(logits1, dim=1)\n        level2_preds = torch.argmax(logits2, dim=1)\n\n        level1_acc = accuracy(level1_preds, y_level1)\n        level2_acc = accuracy(level2_preds, y_level2)\n        level1_f1 = f1(level1_preds, y_level1, self.hparams['num_level1_classes'])\n        level2_f1 = f1(level2_preds, y_level2, self.hparams['num_level2_classes'])\n\n        logs = loss, level1_acc, level2_acc, level1_f1, level2_f1\n\n        return logs\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[0] for x in outputs]).mean()\n        avg_level1_acc = torch.stack([x[1] for x in outputs]).mean()\n        avg_level2_acc = torch.stack([x[2] for x in outputs]).mean()\n        avg_level1_f1 = torch.stack([x[3] for x in outputs]).mean()\n        avg_level2_f1 = torch.stack([x[4] for x in outputs]).mean()\n\n        self.log('val/loss', avg_loss, prog_bar=True, logger=True, on_epoch=True)\n        self.log('val/level1_acc', avg_level1_acc, logger=True, on_epoch=True)\n        self.log('val/level2_acc', avg_level2_acc, logger=True, on_epoch=True)\n        self.log('val/level1_f1', avg_level1_f1, prog_bar=True, logger=True, on_epoch=True)\n        self.log('val/level2_f1', avg_level2_f1, prog_bar=True, logger=True, on_epoch=True)\n\n    def configure_optimizers(self):\n\n        if self.hparams['optimizer'] == 'adam':\n            optimizer = optim.Adam(self.model.parameters(), self.lr)\n        else:  # SGDWithMomentum\n            optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9)\n\n        return {\n            'optimizer': optimizer,\n            'lr_scheduler': optim.lr_scheduler.StepLR(optimizer, \n                                                      step_size=self.hparams['sch_step_size'], \n                                                      gamma=self.hparams['sch_gamma'])\n        }\n        # return optimizer\n\n    def on_train_end(self):\n        ckpt_path = os.path.join(self.trainer.log_dir, \"checkpoints\", \"min_val_loss.ckpt\")\n        print(f\"Loading best checkpoint from {ckpt_path}\")\n        best_model_ = ModelWrapper.load_from_checkpoint(ckpt_path)\n\n        save_dir = self.hparams['model_save_dir']\n        \n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        print(f\"Saving only pytorch model without the wrapper properties to {os.path.join(save_dir, 'best_model.pt')}\")\n        torch.save(best_model_.model, os.path.join(save_dir, \"best_model.pt\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set Training Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_EPOCHS = 10\nBATCH_SIZE = 64\nlr = 4e-4\n\nparams = {\n    'batch_size': BATCH_SIZE,\n    'lr': lr,\n    'sch_step_size': 3,  # StelLR scheduler step size\n    'sch_gamma': 0.5,  # StepLR scheduler gamma value\n    'optimizer': 'adam',  # SGD with momentum or Adam\n    'num_workers': 4,  # number of worker processed for dataloaders\n    'num_level1_classes': 3,  # number of level 1 classes\n    'num_level2_classes': 4,  # number of level 2 classes\n    'label2_weights': [0.1, 0.1, 0.8, 0.1],  # level 2 class weights, because of unbalanced dataset\n    'loss_weights': [0.5, 1],  # penalize level 1 predictions as it converges faster than level 2\n    'img_size': (IMG_RESIZE, IMG_RESIZE),  # img resize\n    'image_dir': image_dir,\n    'model_save_dir': 'model',\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the model with Pytorch Lightning"},{"metadata":{"trusted":true},"cell_type":"code","source":"wrapper = ModelWrapper(hparams=params, df_train=train_df, df_test=test_df)\n\ngpu_num = device_count()\n\ncheckpoint_callback = ModelCheckpoint(\n    save_top_k=1,\n    verbose=True,\n    monitor='val/loss',\n    mode='min',\n    filename='min_val_loss'\n)\n\ntrainer = Trainer(\n    default_root_dir=os.getcwd(),\n    gpus=gpu_num,\n    max_epochs=MAX_EPOCHS,\n    callbacks=[checkpoint_callback]\n)\n\ntrainer.fit(wrapper)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the Best Model\nWhile training, we saved the model checkpoint where it reached the minimum validation loss to model_path/best_model.pt"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = torch.load(os.path.join(model_path, \"best_model.pt\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Test Set for Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_transforms = transforms.Compose([\n    # transforms.ToPILImage('RGB'),\n    transforms.Resize([IMG_RESIZE, IMG_RESIZE]),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\ntest_dataset = CovidDataset(df=test_df, root_dir=image_dir, transform=test_transforms)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Evaluation Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, dataloader, device_):\n    print(\"Evaluating...\")\n    model.to(device_).eval()\n    with torch.no_grad():\n\n        level1_preds = []\n        level1_targets = []\n\n        level2_preds = []\n        level2_targets = []\n\n        for batch in dataloader:\n            x, y_level1, y_level2 = batch['image'].to(device_), \\\n                                    batch['target_1'].to(device_), \\\n                                    batch['target_2'].to(device_)\n            logits1, logits2 = model(x)\n\n            batch_level1_preds = torch.argmax(logits1, dim=1)\n            batch_level2_preds = torch.argmax(logits2, dim=1)\n\n            level1_preds.extend(batch_level1_preds.tolist())\n            level2_preds.extend(batch_level2_preds.tolist())\n\n            level1_targets.extend(y_level1.tolist())\n            level2_targets.extend(y_level2.tolist())\n\n    return level1_preds, level1_targets, level2_preds, level2_targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\npreds_1, targets_1, preds_2, targets_2 = evaluate(best_model, test_dataloader, device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Print Precision, Recall and F1 Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\t\\t***\\tLEVEL 1 CLASSIFICATION METRICS\\t***\")\nprint(classification_report(targets_1, preds_1, target_names=list(level_1_id2label.values()), zero_division=0))\nprint(\"\\t\\t***\\tLEVEL 2 CLASSIFICATION METRICS\\t***\")\nprint(classification_report(targets_2, preds_2, target_names=list(level_2_id2label.values()), zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate Confusion Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"level_1_conf_mat = confusion_matrix(targets_1, preds_1)\nlevel_1_conf_mat = level_1_conf_mat.astype(np.float) / level_1_conf_mat.sum(axis=1)[:, np.newaxis]\n\nlevel_2_conf_mat = confusion_matrix(targets_2, preds_2)\nlevel_2_conf_mat = level_2_conf_mat.astype(np.float) / level_2_conf_mat.sum(axis=1)[:, np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot Confusion Matrix Heatmaps for Level 1 and Level 2 Predictions"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n\naxs[0].title.set_text(\"LEVEL 1 CONFUSION MATRIX\")\naxs[1].title.set_text(\"LEVEL 2 CONFUSION MATRIX\")\n\nsns.heatmap(\n    level_1_conf_mat,\n    cmap='coolwarm',\n    yticklabels=list(level_1_id2label.values()),\n    xticklabels=list(level_1_id2label.values()),\n    annot=True,\n    ax=axs[0]\n)\n\nsns.heatmap(\n    level_2_conf_mat,\n    cmap='coolwarm',\n    yticklabels=list(level_2_id2label.values()),\n    xticklabels=list(level_2_id2label.values()),\n    annot=True,\n    ax=axs[1]\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}