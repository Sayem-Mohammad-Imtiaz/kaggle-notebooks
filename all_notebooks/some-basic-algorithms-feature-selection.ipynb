{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa1670d6-3b0a-7e5c-4b5d-f956ede8fc34"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b8b9c32-6025-001e-7267-d02603472330"},"outputs":[],"source":"dataset = pd.read_csv('../input/diabetes.csv')\ndataset.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"78729751-b3ea-b149-148a-6eef3c7911ab"},"source":"## Logistic Regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca5c0a1c-49d2-174f-18dd-9d9249f4ba1b"},"outputs":[],"source":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\narray = dataset.values\nX = array[:, 0:8]\ny = array[:, 8]\n\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"dad0b822-05ff-7de4-7512-c334cce107b4"},"source":"## Linear Discriminant Analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96088442-e311-b843-c768-6b94f737477f"},"outputs":[],"source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nmodel = LinearDiscriminantAnalysis()\nresults = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fdb50f6c-53a9-eb55-603c-a9a0c5909147"},"source":"## K-Nearest Neighbors"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af074b02-ba92-4027-2a22-020f6bffb2ce"},"outputs":[],"source":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nresults = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"a861da5b-c0b1-8726-cb41-ca36ddbe5329"},"source":"## Naive Bayes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e2fa8ba-e5ea-4873-9c54-6332636d8cd0"},"outputs":[],"source":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nresults = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"78001f6b-309b-af28-78a4-b33870b501aa"},"source":"## Classification and Regression Trees"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"310243fc-ad19-5f92-da3b-09aa42d9ed8d"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nresults = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"28dde8f6-5ca1-2e6d-1572-18203e00bd45"},"source":"## Support Vector Machines"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7b2044d-05e4-fbc1-cd64-86ae42b552bf"},"outputs":[],"source":"from sklearn.svm import SVC\nmodel = SVC()\nresults = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"b5dc118d-e4c7-8a68-42cc-f63dc474075b"},"source":"# Feature Selection"},{"cell_type":"markdown","metadata":{"_cell_guid":"8b0ff0a4-64aa-1bb1-f299-c7d24348c4bf"},"source":"## Univariate Selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b7b61cb-bf0e-1573-4a8b-84d05dd4bc43"},"outputs":[],"source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n\ntest = SelectKBest(score_func=chi2, k=4)\nfit = test.fit(X, y)\n# summarize scores\nnp.set_printoptions(precision=3)\n\nprint(\"scores\")\nprint(fit.scores_)\nfeatures = fit.transform(X)\nprint(\"features\")\nprint(features[0:5, :])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58cb7a84-76a3-73bb-a1c3-0c253d623b89"},"outputs":[],"source":"X = features\nmodel = LogisticRegression()\nscoring = 'accuracy'\nresults = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"38a9341d-9611-61cb-c864-6fcd8f5d0be2"},"source":"Half of the features were removed without any significant loss to the accuracy"},{"cell_type":"markdown","metadata":{"_cell_guid":"bff2814c-2628-cca8-8d8c-6e0caaad1d47"},"source":"## Recursive Feature Elimination"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f12df5ea-ec46-ceb7-bcbe-1a5896cd8318"},"outputs":[],"source":"from sklearn.feature_selection import RFE\nmodel = LogisticRegression()\narray = dataset.values\nX = array[:, 0:8]\ny = array[:, 8]\nrfe = RFE(model, 4)\nfit = rfe.fit(X,y)\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"248d5c18-1161-6363-400a-40b9618bdf05"},"outputs":[],"source":"reduced_dataset = dataset.loc[:, fit.support_]\nreduced_dataset.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b0bb9aa-34ed-5156-83ed-65738c2757ee"},"outputs":[],"source":"X = reduced_dataset.values\nkfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nresults = cross_val_score(model, X, y, cv=kfold, scoring=\"accuracy\")\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"17dec321-914e-ea6b-f2d9-f26ae33b5682"},"source":"## Principal Component Analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52d465bf-5c4a-a127-a2ea-b739db6195d5"},"outputs":[],"source":"from sklearn.decomposition import PCA\narray = dataset.values\nX = array[:, 0:8]\ny = array[:, 8]\npca = PCA(n_components=3)\nfit = pca.fit(X)\n#summarise components\nprint(\"Explained Variance: \", fit.explained_variance_ratio_)\nprint(fit.components_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04868b72-22f0-9090-1863-e4477b87b982"},"outputs":[],"source":"fit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb7944ea-fde1-96c5-10d2-44a993a8619e"},"outputs":[],"source":"kfold = KFold(n_splits=10, random_state=7)\nmodel = LogisticRegression()\nX = pca.transform(X)\nresults = cross_val_score(model, X, y, cv=kfold, scoring=\"accuracy\")\nprint(\"Accuracy: %.3f (%.df)\" % (results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{"_cell_guid":"9f674451-066d-3adc-d8e7-7cd8c80fd831"},"source":"## Feature Importance - using ExtraTreesClassifier"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9eb1c76-683d-b830-0bef-77472a0447e2"},"outputs":[],"source":"from sklearn.ensemble import ExtraTreesClassifier\narray = dataset.values\nX = array[:, 0:8]\ny = array[:, 8]\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\nprint(model.feature_importances_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"475eac43-1b46-507f-6889-5827c22a93dd"},"outputs":[],"source":"dataset.columns"},{"cell_type":"markdown","metadata":{"_cell_guid":"41ea86bf-2ad1-b419-450f-065911402918"},"source":"The high scores are for Glucose(0.226), Age(0.15), BMI(0.147)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f061c50c-2c9f-4225-ca62-7fbb1d307597"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}