{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Correlation between genes and symptoms"},{"metadata":{},"cell_type":"markdown","source":"This notebook will try to find most common symptoms, organs and genes that appear in this dataset. Also this notebook will try find topics that are related to some organ, gene etc.\n\n**NOTE**\n\nAs dataset is updated I will try to update this notebook accordingly"},{"metadata":{},"cell_type":"markdown","source":"Following imports are imports of `scispacy`, `langdetect` and `en_ner_bionlp13cg_md`. They are used for language detection and NER for genes, organs and chemicals. Imports of those are optional since I have prepared dataset where everything is precomputed on much faster machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install scispacy\n#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_ner_bionlp13cg_md-0.3.0.tar.gz\n#!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- imports of other libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nfrom urllib.parse import urlparse\n\nimport re\n\nimport os\nimport json\nfrom os import path\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n#from langdetect import detect, lang_detect_exception\n\n#import scispacy\nimport spacy\n\n#import en_ner_bionlp13cg_md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_metadata = pd.read_csv('../input/CORD-19-research-challenge/metadata.csv')\narticles_metadata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of articles in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(articles_metadata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic dataset stats\n\nIn this section we will try to find out number of articles per time and their source (pdf or pmc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,40))\ngs = gridspec.GridSpec(nrows=4, ncols=1)\n\nax1 = fig.add_subplot(gs[0,0])\nax1.set_title('Sources')\n\nsource_counts = articles_metadata.groupby('source_x').count()\ncord_uid_count = source_counts.cord_uid.sort_values(ascending=True)\nlabels = cord_uid_count.index\nvalues = cord_uid_count.values\n\nax1.barh(np.arange(len(labels)), values, tick_label=labels)\nax1.set_xlabel('Number of articles')\n\n\nax2 = fig.add_subplot(gs[1, 0])\nax2.set_title('Number of articles per url domain (NaN exclued)')\n\ndomains = map(lambda x: urlparse(x).netloc, articles_metadata[~pd.isna(articles_metadata.url)].url)\ncounter = collections.Counter(domains)\nmost_common_domains = counter.most_common(20)\nlabels = list(map(lambda x: x[0], most_common_domains))[-1::-1]\nvalues = list(map(lambda x: x[1], most_common_domains))[-1::-1]\n\nax2.barh(np.arange(len(labels)), values, tick_label=labels)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_year(date: str) -> int:\n    return int(date[:4]) if (date and type(date) is str) else -1\n\ndef extract_month(date: str) -> int:\n    if type(date) is str:\n        tokens = date.split('-')\n        if len(tokens) >= 2:\n            return int(tokens[1])\n    \n    return -1\n\ndef has_text(title) -> bool:\n    return (title is not None) and (type(title) == str) and len(title)>0\n\ndef detect_language(abstract) -> bool:\n    try:\n        return detect(abstract) == 'en'\n    except lang_detect_exception.LangDetectException:\n        return False\n    \narticles_metadata['year_publish'] = list(map(lambda x: extract_year(x), articles_metadata.publish_time))\narticles_metadata['month_publish'] = list(map(lambda x: extract_month(x), articles_metadata.publish_time))\narticles_metadata['has_title'] = list(map(lambda x: has_text(x), articles_metadata.title))\narticles_metadata['has_abstract'] = list(map(lambda x: has_text(x), articles_metadata.abstract))\n\n\narticles_metadata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- in the following cell we can see how many articles have titles and abstracts"},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_metadata.groupby(['has_title', 'has_abstract']).count()['cord_uid']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,28))\ngs = gridspec.GridSpec(nrows=2, ncols=1)\n\narticles_metadata['year_publish'] = list(map(lambda x: extract_year(x), articles_metadata.publish_time))\n\nper_year_counter = articles_metadata.groupby('year_publish').count().cord_uid\nax1 = fig.add_subplot(gs[0,0])\n\nplt.xticks(rotation=90)\nax1.set_title('Number of articles per year')\nax1.bar(np.arange(len(per_year_counter)), per_year_counter.values, tick_label=per_year_counter.index, log=True)\nax1.set_xlabel('Number of articles')\n\n\nax2 = fig.add_subplot(gs[1, 0])\nax2.set_title('Number of articles per month in 2020')\n\nmonths_counter = collections.Counter(list(map(lambda x: extract_month(x), articles_metadata[articles_metadata.year_publish == 2020].publish_time)))\nmonths_data = sorted(months_counter.items())[1:]\nmonths_values = [x[1] for x in months_data]\nmonths_labels = [x[0] for x in months_data]\nax2.plot(months_labels, months_values)\nax2.grid(True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(months_counter.items())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From articles published in 2020, 149103 of them don't have month or day of publishing."},{"metadata":{},"cell_type":"markdown","source":"## Analysis of genes and chemical compounds"},{"metadata":{},"cell_type":"markdown","source":"This section will examine co-occurences of genes, checmical compounds and organs that appear in abstracts of articles. Since this dataset is diverse we will limit analysis to the following subset:\n* only articles in English\n* only articles that have date of creation 2020 or later"},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_2020 = articles_metadata[articles_metadata.year_publish >= 2020].copy()\narticles_2020.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(articles_2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time condition limited analysis to 387554 articles"},{"metadata":{},"cell_type":"markdown","source":"Remove duplicate titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_2020 = articles_2020.drop_duplicates(subset=['title'])\nlen(articles_2020)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing duplicates further reduced dataset to 297871 articles"},{"metadata":{},"cell_type":"markdown","source":"### Trying to filter english papers and extract genes, organs and chemical compounds"},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_2020_tagged = None\nPREPROCESSED_PATH = '../input/preprocessed-tagged-articles-for-cord19/preprocessed-articles-v1.0.csv'\nif path.exists(PREPROCESSED_PATH):\n    preprocessed_data = pd.read_csv(PREPROCESSED_PATH)\n    articles_2020_tagged = pd.merge(articles_2020, preprocessed_data, how='right', right_on='cord_uid', left_on='cord_uid')\nelse:\n    language_labels = []\n    for t in tqdm(articles_2020.title):\n        try:\n            language_labels.append(detect(t))\n        except lang_detect_exception.LangDetectException:\n            language_labels.append('unknown')\n            print(f\"Error with title {t}\")\n        except TypeError:\n            language_labels.append('unknown')\n            print(f\"Type error with title {t}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_data = pd.read_csv(PREPROCESSED_PATH, compression=None)\npreprocessed_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_2020_tagged.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(articles_2020_tagged)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of genes, chemical compunds and organs"},{"metadata":{"trusted":true},"cell_type":"code","source":"articles_2020_tagged['genes'] = articles_2020_tagged['genes'].apply(eval)\narticles_2020_tagged['organs'] = articles_2020_tagged['organs'].apply(eval)\narticles_2020_tagged['chems'] = articles_2020_tagged['chems'].apply(eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_counter = collections.Counter()\nfor g in articles_2020_tagged['genes']:\n    gene_counter.update(g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#false_postive_genes = ['COVID-19','Covid-19','UK','PPE','COVID-19patients','2019-nCoV',\n#                       'MERS-CoV','stay-at-home','USA','e.g.', '', 'Iran', 'Food']\nfalse_postive_genes = ['covid-19','uk','ppe','covid-19patients','2019-ncov','mers-cov','stay-at-home','usa','e.g.', '', 'iran', 'food', 'covid-19 patients']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fpg in false_postive_genes:\n    del gene_counter[fpg]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_counter.most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"organ_counter = collections.Counter()\nfor o in articles_2020_tagged['organs']:\n    organ_counter.update(o)\n\norgan_counter.most_common(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Co-occurences of organs and genes in dataset corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"cooccurence = collections.Counter()\nfor index, article in articles_2020_tagged.iterrows():\n    #print(article)\n    gene_list = article.genes\n    organ_list = article.organs\n    for g in gene_list:\n        for o in organ_list:\n            if g not in false_postive_genes:\n                cooccurence[f\"{g}#{o}\"] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cooccurence.most_common(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting heatmap of coocurences**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_most_common(counter, num_of_items=20):\n    items = counter.most_common(num_of_items)\n    return [i[0] for i in items]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gene_axis = extract_most_common(gene_counter)\norgan_axis = extract_most_common(organ_counter)\n#gene_axis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heatmap = []\nfor g in gene_axis:\n    row = []\n    for o in organ_axis:\n        c = cooccurence[f\"{g}#{o}\"]\n        row.append(c if c>0 else 1)\n    heatmap.append(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,15))\n\nax = fig.add_subplot(111)\n\nax.imshow(heatmap)\n\nax.set_xticks(np.arange(len(gene_axis)))\nax.set_yticks(np.arange(len(organ_axis)))\n\nax.set_xticklabels(gene_axis)\nax.set_yticklabels(organ_axis)\n\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting topics for genes and organs"},{"metadata":{},"cell_type":"markdown","source":"In this section we will try to extract topics for combinations of genes and organs. In the cell bellow there will be defined class for such purpose"},{"metadata":{},"cell_type":"markdown","source":"- first, let's import gensim LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import LdaModel\nfrom gensim.utils import simple_preprocess\nfrom gensim.test.utils import common_texts\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.corpora.dictionary import Dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TopicExtractor:\n    \n    _genes=[]\n    _organs=[]\n    \n    _data_subset=None\n    \n    def __init__(self, genes=[], organs=[]):\n        self._genes = genes\n        self._organs = organs\n        \n        self._data_subset = articles_2020_tagged\n        for o in self._organs:\n            indices = self._data_subset.organs.apply(lambda organ_list: o in organ_list)\n            self._data_subset = self._data_subset[indices]\n\n        for g in self._genes:\n            indices = self._data_subset.genes.apply(lambda  gene_list: g in gene_list)\n            self._data_subset = self._data_subset[indices]\n        \n    def get_topics(self, num_of_topics=20):\n        article_chapters = []\n\n        for doc in self._data_subset.pdf_json_files:\n            if doc and type(doc) is str:\n                path = doc.split('; ')[0]\n                with open(f\"./../input/CORD-19-research-challenge/{path}\") as article:\n                    raw_data = article.read()\n                    obj = json.loads(raw_data)\n                    body_text = obj['body_text']\n\n                    texts = [x['text'] for x in body_text if 'text' in x]\n                    # print(texts)\n                    article_chapters.extend(texts)\n        print(len(article_chapters))          \n        corpus_data = [simple_preprocess(remove_stopwords(text)) for text in article_chapters]\n                    \n        common_dictionary = Dictionary(corpus_data)\n        corpus = [common_dictionary.doc2bow(text) for text in corpus_data]\n\n        self.lda_model = LdaModel(corpus, num_topics=num_of_topics, id2word=common_dictionary)\n        self.lda_model.print_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lungs articles topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"lungs_lda = TopicExtractor(organs=['lung'])\nlungs_lda.get_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lungs_lda.lda_model.print_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Topics for articles about heart and Angiotensin-converting enzyme 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_igg_lda = TopicExtractor(genes=['ace2'], organs=['heart'])\nheart_igg_lda.get_topics(num_of_topics=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart_igg_lda.lda_model.print_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Topics for articles about liver"},{"metadata":{"trusted":true},"cell_type":"code","source":"liver_lda = TopicExtractor(organs=['liver'])\nliver_lda.get_topics(num_of_topics=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"liver_lda.lda_model.print_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can use the following class for playing and finding more data about other combination about organs and genes.\n\n__More improvements to notebook will follow :-)__"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}