{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the Dataset","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/dataisbeautiful/r_dataisbeautiful_posts.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization and Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deleting columns.\ndel df['author_flair_text']\ndel df['removed_by']\ndel df['total_awards_received']\ndel df['awarders']\ndel df['id']\ndel df['created_utc']\ndel df['full_link']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets take a look at our dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taking care of nan in title\ndf.title.fillna(\" \",inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['title'] + ' ' + df['author']\ndel df['title']\ndel df['author']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.over_18.replace([True, False], [1, 0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.over_18.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Splitting Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"over18text_false = df[df.over_18 == 0.0].text\nover18text_true = df[df.over_18 == 1.0].text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = df.text.values[:100000]\ntest_text = df.text.values[100000:]\ntrain_category = df.over_18[:100000]\ntest_category = df.over_18[100000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(over18text_true)))\nplt.imshow(wc,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(over18text_false)))\nplt.imshow(wc,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the most frequently used words from wordcloud\ntext_true = wc.process_text(str(\" \".join(over18text_true)))\ntext_true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_true = sorted(text_true.items(),key = lambda kv:(kv[1], kv[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ans_true = []\nfor i in text_true:\n    ans_true.append(i[0])\nans_true [:5] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now for each word in every test data point , we will just check that if any word of that test data point is present in our dictionary ans_true which contains the most frequent 3000 words of label 1. If the word is present , then we will simply predict 1, otherwise 0.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor i in test_text:\n    x = i.split()\n    for j in x:\n        if j in ans_true:\n            predictions.append(1)\n            break\n        else:\n            predictions.append(0)\n            break\nlen(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\nfor i in range(len(predictions)):\n    test_category = list(test_category)\n    if(predictions[i] == int(test_category[i])):\n        count += 1\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = (count/len(predictions))*100\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy using WordCloud is : \", accuracy , \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using just WordCloud, we have got an 86 accuracy! Thats cool. Now we will compare this result by testing this dataset on different classifiers.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**WHAT ARE STOPWORDS?**\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\npunctuation = list(punctuation)\nstop.update(punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**WHAT IS LEMMATIZATION AND STEMMING?**\n\n- For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n- The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\n**You guyz can read more here -> https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to clean our text.\nlemmatizer = WordNetLemmatizer()\ndef clean_review(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return clean_text\n\ndef join_text(text):\n    return \" \".join(text) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.apply(clean_review)\ndf.text = df.text.apply(join_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting the data into training and testing data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text, test_text, train_category, test_category = train_test_split(df.text, df.over_18, random_state=0)\ntrain_text.shape, test_text.shape, train_category.shape, test_category.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,2))\n#transformed train reviews\ncv_train_reviews=cv.fit_transform(train_text)\n#transformed test reviews\ncv_test_reviews=cv.transform(test_text)\n\nprint('cv_train:',cv_train_reviews.shape)\nprint('cv_test:',cv_test_reviews.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(train_text)\n#transformed test reviews\ntv_test_reviews=tv.transform(test_text)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAINING WITH DIFFERENT CLASSIFIERS AND ANALYSIS AFTER TESTING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. **Logistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression()\n\n# Fitting the model\nlr_cv=lr.fit(cv_train_reviews,train_category)\nlr_tfidf=lr.fit(tv_train_reviews,train_category)\n\n# Predicting for model\nlr_cv_predict=lr_cv.predict(cv_test_reviews)\nlr_tfidf_predict=lr_tfidf.predict(tv_test_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting Score\n\nlr_cv_score=accuracy_score(test_category,lr_cv_predict)\nprint(\"lr_cv_score :\",lr_cv_score)\n\nlr_tfidf_score=accuracy_score(test_category,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_category,lr_cv_predict))\nprint(classification_report(test_category,lr_tfidf_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(lr_cv, cv_test_reviews, test_category, cmap=\"Accent\", values_format = '')\nplot_confusion_matrix(lr_tfidf, tv_test_reviews, test_category, cmap=\"Accent\", values_format = '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. **Multinomial NaiveBayes**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#training the model\nnb=MultinomialNB()\n\n# Fitting the model\nnb_cv=nb.fit(cv_train_reviews,train_category)\nnb_tfidf=nb.fit(tv_train_reviews,train_category)\n\n# Predicting for model\nnb_cv_predict=nb_cv.predict(cv_test_reviews)\nnb_tfidf_predict=nb_tfidf.predict(tv_test_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting Score\n\nnb_cv_score=accuracy_score(test_category,nb_cv_predict)\nprint(\"nb_cv_score :\",nb_cv_score)\n\nnb_tfidf_score=accuracy_score(test_category,nb_tfidf_predict)\nprint(\"nb_tfidf_score :\",nb_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_category,nb_cv_predict))\nprint(classification_report(test_category,nb_tfidf_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(nb_cv, cv_test_reviews, test_category, cmap=\"Blues\", values_format = '')\nplot_confusion_matrix(nb_tfidf, tv_test_reviews, test_category, cmap=\"Blues\", values_format = '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. **Support Vector Machine**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Svc is taking so much time to fit the data so i didnt run these cells","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#svc=SVC()\n# Fitting the model\n#svc_cv=svc.fit(cv_train_reviews,train_category)\n#svc_tfidf=svc.fit(tv_train_reviews,train_category)\n\n# Predicting for model\n#svc_cv_predict=nb_cv.predict(cv_test_reviews)\n#svc_tfidf_predict=nb_tfidf.predict(tv_test_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting Score\n\n#svc_cv_score=accuracy_score(test_category,svc_cv_predict)\n#print(\"svc_cv_score :\", svc_cv_score)\n\n#svc_tfidf_score=accuracy_score(test_category,svc_tfidf_predict)\n#print(\"svc_tfidf_score :\", svc_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(classification_report(test_category,svc_cv_predict))\n#print(classification_report(test_category,svc_tfidf_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_confusion_matrix(svc_cv, cv_test_reviews, test_category, cmap=\"Blues\", values_format = '')\n#plot_confusion_matrix(svc_tfidf, tv_test_reviews, test_category, cmap=\"Blues\", values_format = '')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. **Creating Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(units = 100, activation = 'relu' , input_dim = cv_train_reviews.shape[1]))\nmodel.add(Dense(units = 20, activation = 'relu'))\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(cv_train_reviews,train_category , epochs=1, batch_size = 512, validation_data=(cv_test_reviews,test_category))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(cv_test_reviews)\npredictions[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_category, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(test_category,predictions)\ncm = pd.DataFrame(cm , index=['0','1'] , columns=['0','1'])\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor='black', annot=True, fmt='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Don't forget to upvote! It's free.\n# Any kind of suggestions is appreciated, feel free to comment below :-)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}