{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Variable Information","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"    pc\t          Primary Camera mega pixels\n    fc\t          Front Camera mega pixels\n    sc_h\t        Screen Height of mobile in cm\n    sc_w\t        Screen Width of mobile in cm\n    m_dep\t       Mobile Depth in cm\n    px_width\t    Pixel Resolution Width\n    px_height\t   Pixel Resolution Height\n    ram\t         Random Access Memory in Mega Bytes\n    int_memory\t  Internal Memory in Giga Bytes\n    four_g\t      Has 4G or not\n    three_g\t     Has 3G or not\n    dual_sim\t    Has dual sim support or not\n    battery_power   Total energy a battery can store in one time measured in mAh\n    touch_screen\tHas touch screen or not\n    clock_speed\t Speed at which microprocessor executes instructions\n    n_cores\t     Number of cores of processor\n    wifi\t        Has wifi or not\n    blue\t        Has bluetooth or not\n    mobile_wt\t   Weight of mobile phone\n    talk_time\t   Longest time that a single battery charge will last when you are\n    price_range   This is the target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Contents of the notebook\n        1. Importing libraries\n        2. Data Exploration and simple visualisations\n        3. Missing value/ data collection error check\n        4. Variable skewness check and treatment if required\n        5. Multicollinearity check\n        6. Preparing list of models to train\n        7. Create pipelines for data preprocessing\n        8. Compare results of various classification algorithms\n        9. Creating a submission file for test data\n        10. Interpretation of model using SHAP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, PolynomialFeatures\nfrom category_encoders import WOEEncoder, BinaryEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/mobile-price-classification/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/mobile-price-classification/test.csv\")\nsub = pd.DataFrame(test[\"id\"])\nsub[\"price_range\"] = 2\ntest.drop(\"id\", axis=1, inplace=True)\nprint(f\"train data :{train.shape} test data :{test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating categorical and continuous variable list","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_var = [\"blue\",\"dual_sim\",\"four_g\",\"three_g\",\"touch_screen\",\"wifi\"]\ncon_var = ['px_height', 'sc_h', 'sc_w', 'clock_speed', 'battery_power', 'int_memory', 'talk_time', 'pc',\n           'n_cores', 'px_width', 'fc', 'mobile_wt', 'm_dep', 'ram']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def con_plot(var):\n    fig, ax = plt.subplots(int(np.ceil(len(con_var)/3)), 3, figsize=(16,16))\n    ax = ax.flatten()\n    i = 0\n    for col in var:\n        skew = train[col].skew()\n        sns.distplot(train[col], fit = stats.norm, ax=ax[i])\n        ax[i].set_title(\"Variable %s skew : %.4f\"%(col, skew))\n        i+=1\n    plt.tight_layout()\n    plt.show()\n    \ncon_plot(con_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_plot(var):\n    fig, ax = plt.subplots(int(np.ceil(len(var)/3)), 3, figsize=(16,8))\n    ax = ax.flatten()\n    i = 0\n    for col in var:\n        sns.countplot(train[col], ax=ax[i])\n        ax[i].set_title(\"devices in each category for %s\"%(col))\n        i+=1\n    plt.tight_layout()\n    plt.show()\n    \ncat_plot(cat_var)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing value check","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Device Count in each class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.price_range.value_counts().plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train, hue='price_range', diag_kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"    we can see from pairplot that RAM and battery power can help in classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Separating features and target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop([\"price_range\"], axis=1)\nY = train[\"price_range\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for multicollinearity","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nvif_val = pd.DataFrame({\"Col\":X.columns})\nvif_val[\"VIF\"] = [vif(X.values, i) for i in range(X.shape[1])]\nvif_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bulding list of models to be trained","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf = RandomForestClassifier(random_state=1, n_jobs=-1)\nmodel_logr = LogisticRegression(random_state=1, n_jobs=-1, multi_class='multinomial')\nmodel_lgbm = LGBMClassifier(random_state=1, n_jobs=-1)\nmodel_xgb = XGBClassifier(random_state=1, n_jobs=-1)\nmodel_gbr = GradientBoostingClassifier(random_state=1)\nmodel_cat = CatBoostClassifier(random_state=1, verbose=0)\n\nmodels = []\nmodels.append(('LR',model_logr))\nmodels.append(('RF',model_rf))\nmodels.append(('GBR',model_gbr))\nmodels.append(('XGB',model_xgb))\nmodels.append(('LGB',model_lgbm))\nmodels.append(('CAT',model_cat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing Pipeline Steps","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nonehot = OneHotEncoder(handle_unknown='ignore', sparse=False)\ncv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\nfeature = SelectFromModel(model_rf, threshold=0.001)\nct = ColumnTransformer([('onehot', onehot, cat_var),\n                        ('scaler', scaler, con_var)], remainder='passthrough', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    #pipe = Pipeline([('ct', ct), ('fselect', feature), (name, model)]) # including feature selection step using RF\n    pipe = Pipeline([('ct', ct), (name, model)])\n    scores = cross_val_score(pipe, X, Y, scoring='accuracy', cv=cv, n_jobs=-1, verbose=0)\n    names.append(name)\n    results.append(scores)\n    print(\"model %s accuracy: %.4f variance: %.4f\"%(name, np.mean(scores), np.std(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nplt.boxplot(results)\nplt.xticks(np.arange(1,len(names)+1),names)\nplt.title(\"Accuracy for different machine learning algorithms\")\nplt.xlabel(\"Model Name\")\nplt.ylabel(\"Cross val Accuracies\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Logistic for checking performance and creating a submission file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logr_pipe = Pipeline([('ct', ct), ('LR', model_logr)])\nlogr_pipe.fit(X, Y)\ntrainpred = logr_pipe.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(Y, trainpred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = logr_pipe.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(prediction, model):\n    sub[\"price_range\"] = prediction\n    sub.price_range.value_counts()\n    sub.to_csv(\"model_\"+model+\"_mobile_price.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission(prediction, 'logr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting list of new features after transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_categories = logr_pipe.named_steps['ct'].transformers_[0][1].categories_\nonehot_features = [f\"{col}__{val}\" for col, vals in zip(cat_var, onehot_categories) for val in vals]\nall_features = onehot_features + con_var\nprint(all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### creating a dataframe for the coefficients","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff = pd.DataFrame(logr_pipe['LR'].coef_, columns=all_features)\ncoeff.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model interpretation using Shap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\npd.set_option(\"display.max_columns\",None)\nshap.initjs()\nimport xgboost\nimport eli5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Explainer for Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ct.fit(X)\nX_shap = ct.fit_transform(X)\ntest_shap  = ct.transform(test)\nexplainer = shap.LinearExplainer(logr_pipe.named_steps['LR'], X_shap, feature_perturbation=\"interventional\")\nshap_values = explainer.shap_values(test_shap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, test_shap, feature_names=all_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### we can clearly see that only four variables are very important and influencing the class prediction, while rest of the variables have no importance\n    - ram\n    - battery power\n    - px width\n    - px height","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"# prediction class 2, shap values for class 2\nshap.force_plot(explainer.expected_value[2], shap_values[2][2], test_shap[2], feature_names=all_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction class 2, shap values for class 3\nshap.force_plot(explainer.expected_value[3], shap_values[3][2], test_shap[2], feature_names=all_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction class 0, shap values for class 0\nshap.force_plot(explainer.expected_value[0], shap_values[0][997], test_shap[997], feature_names=all_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction class 0, shap values for class 3\nshap.force_plot(explainer.expected_value[3], shap_values[3][997], test_shap[997], feature_names=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### We can straight away see that shap value model interpretability is very effective. It explains the variable contribution in additive sense which is easier to grasp and also shows which variables are influencing the decision.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Thanks for viewing my work. If you like it dont forget to upvote it.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}