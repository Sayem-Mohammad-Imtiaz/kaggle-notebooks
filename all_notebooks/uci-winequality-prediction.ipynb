{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"from time import time\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\n% matplotlib inline\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n\nimport os\nprint(os.listdir(\"../input\"))\n\nraw = pd.read_csv('../input/winequality-red.csv')\nraw.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Exploratory Data Analysis\n"},{"metadata":{"trusted":true,"_uuid":"c526a5cea6c30cff1e6fa59eb8da2f323d090c08","collapsed":true},"cell_type":"code","source":"raw.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e90c130435866b5eb069f96b91cd249036f35a29","collapsed":true},"cell_type":"code","source":"sns.pairplot(raw,diag_kind='kde',kind='reg',vars=['quality','fixed acidity','chlorides','pH','sulphates','alcohol'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"8e4480c766576450f9e138ca024dc32f4e278d14","collapsed":true},"cell_type":"code","source":"# Principal Component Analysis to find most important features\nX = raw.drop('quality',axis=1)\nX\n# step 1: center and normalize features \nC = (X-np.mean(X))/np.std(X)\nC\n# step 2: compute covariance matrix of centered features\nV = np.cov(C.T)\nV\nprint('shape of cov={}'.format(V.shape))\n# step 3: compute PC loadings (directions in feature space which have most variation)\neigvals,eigvecs = np.linalg.eig(V)\n# enforce descending variance (eigenvalues)\nix = eigvals.argsort()[::-1] \neigvals,eigvecs = eigvals[ix],eigvecs[:,ix]\nloadingsheader = ['L'+str(i) for i in range(1,len(X.columns)+1)]\nloadingsdf = pd.DataFrame(eigvecs,columns=loadingsheader,index=X.columns)\ndisplay(loadingsdf)\n# step 4: compute PCs (i.e. scores: project features X onto loading vectors)\nscores = loadingsdf.values.T.dot(C.T)\nscoresheader = ['PC'+str(i) for i in range(1,len(C.columns)+1)]\nscoresdf = pd.DataFrame(scores.T,columns=scoresheader,index=C.index)\ndisplay(scoresdf.head())\ndef screeplot(eigvals):\n    '''\n    function which computes percent variance explained plot\n    eigvals   : eigenvalues returned by PCA\n    '''\n    with plt.style.context('seaborn-white'):\n        f,ax=plt.subplots(figsize=(14,8))\n        x = np.arange(1,len(eigvals)+1,1)\n        ax.set_xticks(x)\n        totalvar = eigvals.sum()\n        pve = eigvals/totalvar\n        cumpve = np.cumsum(pve)\n        ax.plot(x,pve,label='pve')\n        ax.plot(x,cumpve,label='cumpve')\n        ax.set(title='Percent Variance Explained',xlabel='PC',ylabel='eigenvalue (loading variance %)')\n        ax.axhline(y=0,color='k',linestyle='dotted')\n        ax.axhline(y=1,color='k',linestyle='dotted')\n        ax.legend(loc='best')\n\ndef biplot(loadingdf,scoredf,loadcolor='',scorecolor='',load_axlim=7,score_axlim=7,load_arrows=4):\n    '''\n    functon which plots first two PCs\n    loadingdf        : loading vectors, DataFrame\n    scoredf          : score vectors, DataFrame\n    load,score_color : color of loadings,scores,str\n    load,score_axlim : scale of loading,score axes, flt\n    load_arrows      : size of loading arrow heads, flt\n    '''\n    with plt.style.context('seaborn-white'):\n        f = plt.figure(figsize=(12,12))\n        ax0 = plt.subplot(111)\n        for ix in scoredf.index:\n            # scatter scores onto 2d surface\n            ax0.annotate(ix,(scoredf['PC1'][ix],-scoredf['PC2'][ix]),ha='center',color=scorecolor)\n        ax0.set(xlim=(-score_axlim,score_axlim),ylim=(-score_axlim,score_axlim))\n        ax0.set_xlabel('Principal Component 1',color=scorecolor)\n        ax0.set_ylabel('Principal Component 2',color=loadcolor)\n        # add ref line sthrough origin\n        ax0.hlines(y=0,xmin=-score_axlim,xmax=score_axlim,linestyle='dotted',color='grey')\n        ax0.vlines(x=0,ymin=-score_axlim,ymax=score_axlim,linestyle='dotted',color='grey')\n        # overlay first two loading vector weights\n        ax1 = ax0.twinx().twiny()\n        ax1.set(xlim=(-load_axlim,load_axlim),ylim=(-load_axlim,load_axlim))\n        ax1.tick_params(axis='y',color='red')\n        ax1.set_xlabel('Principal Component Loading Weights')\n        offset_scalar = 1.15\n        for feature in loadingdf.index:\n            ax1.annotate(feature,(loadingdf['L1'].loc[feature]*offset_scalar,-loadingdf['L2'].loc[feature]*offset_scalar),color=loadcolor)\n        # display PCs as arrows\n        for i in range(0,load_arrows):\n            ax1.arrow(x=0,y=0,dx=loadingdf['L1'][i],dy=-loadingdf['L2'][i],head_width=0.009,shape='full')\nscreeplot(eigvals)\nbiplot(loadingsdf,scoresdf,loadcolor='red',scorecolor='lightblue',load_axlim=1,score_axlim=6,load_arrows=len(loadingsdf.columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fff116c4eb7d52b07c485757aa0fe6d9d7a357c"},"cell_type":"markdown","source":"### PCA Results\n  - The first principal component includes the features:\n    - fixed acidity, citric acid, density, sulphates, chlorides, residual sugars, pH\n      - pH is negatively correlated with the other features in PC1 \n  - The second principal component includes the features: alcohol, total sulfur dioxide, free sulfur dioxide, volatile acidity, residual sugar\n      - alcohol is correlated in opposite direction to all other features in PC2   \n"},{"metadata":{"trusted":true,"_uuid":"a9a5080ca466e287fe2b5598150c30c9803260ff"},"cell_type":"markdown","source":"### Create Pipeline for Linear Estimators of Wine Quality\n  - assign response quality as response\n  - assign all other columns as features X\n  - split train, test and validate on train\n  - consider polynomial features\n  - try estimators:\n    - LinearRegression\n    - Ridge\n    - Lasso\n    - ElasticNet\n    - Regression Tree\n    - KNN Regression\n    - SVC Regression\n    - PCA Lineaer Regression\n    - PCA Lasso Regression\n    - PCA Ridge Regression"},{"metadata":{"_uuid":"69633f91480388453b8624d4d9226a04c4c92023"},"cell_type":"markdown","source":"### PCA Regression\n  - utilize PC1 as features in linear estimator\n  - utilize interactions between primary PC1 and PC2 features"},{"metadata":{"trusted":true,"_uuid":"3e43185162d671658d181488f99c0a55daafaa91"},"cell_type":"code","source":"# Assign target and features\nX,y = raw.drop('quality',axis=1),raw[['quality']]\nfrom sklearn.model_selection import train_test_split\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.3,random_state=0)\n\n\nfrom sklearn.preprocessing import PolynomialFeatures,StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split,cross_validate,cross_val_predict,cross_val_score,validation_curve\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\ndef pca_regression(estimator,X,y,testsplit=.3,seed=0,paramgrid={},folds=5):\n    '''\n    function which computes PCA regression\n    estimator : linear estimator, sklearn linear estim ator object\n    X,y       : features, target, ndarrays\n    seed      : random seed, int\n    folds     : cross validation folds, int \n    '''\n    # interaction features\n    pc1features = X[['citric acid','fixed acidity','sulphates','density','chlorides','density','pH']]\n    Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=testsplit,random_state=seed)\n    pc1features['pH2'] = pc1features['pH']*pc1features['pH']\n    pc1features['fixedacidity2'] = pc1features['fixed acidity'] * pc1features['fixed acidity']\n    pc1features['alcohol_pH'] = X['alcohol'] * pc1features['pH']\n    pc1features['alcohol_fixedacidity'] = X['alcohol'] * pc1features['fixed acidity']\n    pc1features['totalsulfurdioxide_pH'] = X['total sulfur dioxide'] * pc1features['pH']\n    pc1features['freesulfurdioxide_fixedacidity'] = X['free sulfur dioxide'] * pc1features['fixed acidity']\n    degbest,rmsebest,r2best,optimalestimator=1,np.inf,0,None\n    pipe = make_pipeline(SimpleImputer(strategy='median'),\n                         estimator)\n    # cross validate hyperparameters (arbitrarily choose highest PC1 vs. PC2 interactions)\n    gscv = GridSearchCV(pipe,cv=folds,param_grid=paramgrid,scoring='neg_mean_squared_error')\n    gscv.fit(X,y)\n    bestestimator = gscv.best_estimator_\n    # cross validate scores\n    cvobj = cross_validate(bestestimator,X,y,cv=folds,return_train_score=True,scoring=['r2','neg_mean_squared_error'])\n    cv_trainr2,cv_valr2 = cvobj['train_r2'],cvobj['test_r2']\n    cv_trainrmse,cv_valrmse =np.sqrt(-cvobj['train_neg_mean_squared_error']),np.sqrt(-cvobj['test_neg_mean_squared_error'])\n    # enforce nonnegative score values\n    cv_trainr2 = np.where(cv_trainr2<0,np.nan,cv_trainr2)\n    cv_valr2 = np.where(cv_valr2<0,np.nan,cv_valr2)\n    trainr2,valr2 = np.nanmean(cv_trainr2),np.nanmean(cv_valr2)\n    trainrmse,valrmse = np.nanmean(cv_trainrmse),np.nanmean(cv_valrmse)\n    print('\\nDegree {} Train CV Mean R2 = {:,.3f}'.format(degbest,trainr2))\n    print('Degree {} Validate CV Mean R2 = {:,.3f}'.format(degbest,valr2))\n    print('\\nDegree {} Train CV Mean RMSE = {:,.3f}'.format(degbest,trainrmse))\n    print('Degree {} Validate CV Mean RMSE = {:,.3f}'.format(degbest,valrmse))\n    # cv test yhat\n    cv_testpred = cross_val_predict(bestestimator,Xtest,ytest,cv=folds).ravel()\n    # cv residuals\n    cv_testresids = (ytest.as_matrix()-cv_testpred.mean().ravel()).ravel()\n    cv_testmse = np.square(cv_testresids).mean()\n    cv_testrmse = np.sqrt(cv_testmse)\n    # test cv r2score\n    cv_testscore = cross_val_score(bestestimator,Xtest,ytest,cv=folds,scoring='r2')\n    cv_testr2 = np.where(cv_testscore<0,np.nan,cv_testscore)\n    testr2 = np.nanmean(cv_testr2)\n    # enforce nonnegative test r2\n    \n    return degbest,cv_testrmse,testr2,bestestimator\n\n# pca_regression(LinearRegression(),X,y,testsplit=.3,seed=0,paramgrid={'linearregression__normalize':[True,False],'linearregression__fit_intercept':[True,False]},folds=5)\npca_regression(Ridge(),X,y,testsplit=.3,seed=0,paramgrid={'ridge__normalize':[True,False],'ridge__fit_intercept':[True,False],'ridge__alpha':np.arange(0,.1,1e-3)},folds=5)\n# pca_regression(Lasso(),X,y,testsplit=.3,seed=0,paramgrid={'lasso__normalize':[True,False],'lasso__fit_intercept':[True,False],'lasso__alpha':np.arange(.005,.01,1e-5)},folds=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b93e25978d2b72986e71df6a63d8e0e48d849c3","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures,StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split,cross_validate,cross_val_predict,cross_val_score,validation_curve\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nX,y = raw.drop('quality',axis=1),raw[['quality']]\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.3,random_state=0)\n\ndef optimal_estimator(estimator,X,y,folds=5,paramgrid={}):\n    '''\n    function which applies GridSearchCV\n    estimator    : sklearn linear regularized model\n    X,y          : features,response data, ndarrays\n    folds        : folds in CV, int\n    paramgrid    : key:values to use in GridSearchCV exhaustive search, dict\n    '''\n    # init GS object\n    gscv = GridSearchCV(estimator,cv=folds,param_grid=paramgrid,scoring='neg_mean_squared_error')\n    # fit gs obj to data\n    gscv.fit(X,y)\n    return  gscv.best_estimator_\n\ndef linear_estimator(estname,estimator,X,y,degrees=[1,2],seed=0,folds=10,paramgrid={},showcv=True):\n    '''\n    function which computes cross-validated linear model score\n    estname     : name of linear estimator, str\n    estimator   : sklearn linear estimator, sklearn linear obj\n    X,y         : UCI wine quality features, response, ndarrays\n    degrees     : polynomial feature degress arg, list of ints\n    seed        : train_test_split random_state arg, int\n    folds       : cv k number of folds, int\n    paramgrid   : GridSearchCV hyperparameter list, list of flts\n    showcv      : displays cv score array,bool\n    '''\n    # split data \n    Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=0.25,random_state=seed)\n    # init best scores\n    degbest,rmsebest,r2best,optimalestimator = 1,np.inf,0,None\n    for i,deg in enumerate(degrees):\n        print('-'*30+ 'Degree {} {}'.format(deg,estname) +'-'*30)\n#         if str(estimator)[:16]!='LinearRegression':\n#             print('regularized estimator specified:\\n')\n            # regular pipeline doesn't work\n#             pipe = Pipeline(steps=[('poly',PolynomialFeatures(degree=deg)),\n#                                    ('stdscaler',StandardScaler),\n#                                    ('linear_est',estimator)])\n        pipe = make_pipeline(PolynomialFeatures(degree=deg),\n                             SimpleImputer(strategy='median'),\n                             StandardScaler(with_std=True),\n                             estimator)\n        # optimal hyper on pipeline for regularized \n        gscv = GridSearchCV(pipe,cv=folds,param_grid=paramgrid,scoring='neg_mean_squared_error')\n        # now fit gscv to data\n        gscv.fit(Xtrain,ytrain)\n        bestestimator = gscv.best_estimator_\n        print('GridSearchCV.best_estimator_:\\n{}'.format(bestestimator))\n        print('\\nGridSearchCV.best_score_:\\n{}'.format(gscv.best_params_))\n        # now use gscv optimized estimator to data\n        cvresult = cross_validate(bestestimator,Xtrain,ytrain,\n                                  cv=folds,return_train_score=True,\n                                  scoring=('r2','neg_mean_squared_error'))\n        # reassign negative scores to nan \n        cv_r2train = np.where(cvresult['train_r2']>=0,cvresult['train_r2'],np.nan)\n        cv_r2val = np.where(cvresult['test_r2']>=0,cvresult['test_r2'],np.nan)\n        # compute cv rmse\n        cv_msetrain = cvresult['train_neg_mean_squared_error']\n#         print(cv_msetrain)\n        cv_rmsetrain = np.sqrt(-cv_msetrain)\n        cv_mseval = cvresult['test_neg_mean_squared_error']\n        cv_rmseval = np.sqrt(-cv_mseval) \n        if showcv:\n            print('\\nDegree {} {}-fold CV train rmse scores:\\n{}\\n'.format(deg,folds,cv_rmsetrain))\n            print('\\nDegree {} {}-fold CV train r2 scores:\\n{}\\n'.format(deg,folds,cv_r2train))\n            print('Degree {} {}-fold CV validate r2 scores:\\n{}'.format(deg,folds,cv_r2val))\n        # compute mean cv r2 scores with np.nanmean to ignore nans in mean computation\n        trainr2,valr2 = np.nanmean(cv_r2train),np.nanmean(cv_r2val)\n        print('\\nDegree {} Train CV Mean R2 = {:,.3f}'.format(deg,trainr2))\n        print('Degree {} Validate CV Mean R2 = {:,.3f}'.format(deg,valr2))\n        # compute mean cv rmse scores\n        trainrmse,valrmse = np.mean(cv_rmsetrain),np.mean(cv_rmseval)\n        print('\\nDegree {} Train CV Mean RMSE = {:,.3f}'.format(deg,trainrmse))\n        print('Degree {} Validate CV Mean RMSE = {:,.3f}'.format(deg,valrmse))\n        # compute test yhat as cv'd \n        cv_testpred = cross_val_predict(bestestimator,Xtest,ytest).ravel()\n        # prediction fit plot\n        f,ax = plt.subplots(2,figsize=(16,16))\n        ax[0].scatter(ytest,cv_testpred,edgecolor=(0,0,0))\n        x = np.linspace(*ax[0].get_xlim())\n        ax[0].plot(x,x,color='k',linestyle='dotted',label='identity')\n        ax[0].set(title='Degree {} {} Prediction Fit'.format(deg,estname),\n                 xlabel='test y',ylabel='CV yhat')\n        ax[0].legend()\n        # test residuals\n        cv_testresids = (ytest.as_matrix()-cv_testpred.mean().ravel()).ravel()\n#         print('cv_testresids.shape={}'.format(cv_testresids.shape))\n#         print('cv_testpred.shape={}'.format(cv_testpred.shape))\n        cv_testmse = np.square(cv_testresids).mean()\n        cv_testrmse = np.sqrt(cv_testmse)\n#         cv_testr2 = r2_score(ytest,cv_testpred)\n        # cross_val_score instead of r2_score to control for negative scores\n        cvscore = cross_val_score(bestestimator,Xtest,ytest,cv=folds,scoring='r2')\n        cv_testr2 = np.where(cvscore<0,np.nan,cvscore)\n        # np.nanmean instead of .mean() to ignore nan values\n        cv_testr2 = np.nanmean(cv_testr2)\n#         print('cv_testr2\\n{}'.format(cv_testr2))\n        # determine best result by degree\n        if cv_testrmse<rmsebest:\n            degbest,rmsebest,r2best,optimalestimator = deg,cv_testrmse,cv_testr2,bestestimator\n        print('\\nDegree {} CV test R2 = {}'.format(deg,cv_testr2))\n        print('Degree {} CV test RMSE = {}'.format(deg,cv_testrmse))\n        sns.regplot(cv_testresids,cv_testpred,ax=ax[1],lowess=False,\n                    scatter_kws={'alpha':.8},\n                    line_kws={'color':'red'})\n        ax[1].set(title='Degree {} {} Residual Plot'.format(deg,estname),\n                 xlabel='CV residuals',ylabel='CV yhat')\n        ax[1].legend(['lowess'])\n    return degbest,rmsebest,r2best,optimalestimator\n\nrandomseed=1\ncvfolds=5\n# narrow estimator hyper parameters\n# linear_estimator('linreg',LinearRegression(),X,y,degrees=[1,2],seed=randomseed,folds=cvfolds,paramgrid={'linearregression__fit_intercept':[True,False]},showcv=False)    \n# linear_estimator('ridge',Ridge(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'ridge__alpha':np.arange(65,75,1)})     # alpha=69\n# linear_estimator('ridge',Ridge(),X,y,degrees=[2],seed=randomseed,folds=cvfolds,paramgrid={'ridge__alpha':np.arange(0,10,1)})   # alpha=6 \n# linear_estimator('ridge',Ridge(),X,y,degrees=[3],seed=randomseed,folds=cvfolds,paramgrid={'ridge__alpha':np.arange(62,75,1)})   # alpha=67 \n# linear_estimator('lasso',Lasso(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'lasso__alpha':np.arange(0.01,.017,1e-4)},showcv=False) # alpha=.0157    \n# linear_estimator('lasso',Lasso(),X,y,degrees=[2],seed=randomseed,folds=cvfolds,paramgrid={'lasso__alpha':np.arange(0.0005,.01,1e-4)},showcv=False) # alpha=.0008    \n# linear_estimator('lasso',Lasso(),X,y,degrees=[3],seed=randomseed,folds=cvfolds,paramgrid={'lasso__alpha':np.arange(0.002,.0025,1e-4)},showcv=False) # alpha=.0022    \n# linear_estimator('knnreg',KNeighborsRegressor(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'kneighborsregressor__n_neighbors':np.arange(24,32,1)})  # n=28    \n# linear_estimator('forestreg',RandomForestRegressor(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'randomforestregressor__n_estimators':np.arange(29,33,1),'randomforestregressor__min_samples_split':np.arange(3,7,1)}) # 31,4  # n=28    \n# linear_estimator('treereg',DecisionTreeRegressor(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'decisiontreeregressor__max_depth':np.arange(2,7,1)}) # 4  \n# linear_estimator('svr',SVR(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'svr__kernel':['rbf'],'svr__C':np.arange(.69,.72,1e-3)}) # rbf,C=.701    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0be56911aed71412a138cf88cc8701198e819936","scrolled":true},"cell_type":"code","source":"randomseed=0\ncvfolds=5\n\n# bundle linear estimators to run through pipeline\nestimatorlist = [\n                 ('linreg',linear_estimator('linreg',LinearRegression(),X,y,degrees=[1,2],seed=randomseed,folds=cvfolds,paramgrid={'linearregression__fit_intercept':[True,False]},showcv=False)),    \n                 ('ridge',linear_estimator('ridge',Ridge(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'ridge__alpha':np.arange(65,75,1)})),     # alpha=69\n                 ('ridge',linear_estimator('ridge',Ridge(),X,y,degrees=[2],seed=randomseed,folds=cvfolds,paramgrid={'ridge__alpha':np.arange(0,10,1)})),   # alpha=6 \n                 ('ridge',linear_estimator('ridge',Ridge(),X,y,degrees=[3],seed=randomseed,folds=cvfolds,paramgrid={'ridge__alpha':np.arange(64,75,1)})),   # alpha=67 \n                 ('lasso',linear_estimator('lasso',Lasso(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'lasso__alpha':np.arange(0.01,.017,1e-4)},showcv=False)), # alpha=.0157    \n                 ('lasso',linear_estimator('lasso',Lasso(),X,y,degrees=[2],seed=randomseed,folds=cvfolds,paramgrid={'lasso__alpha':np.arange(0.0005,.01,1e-4)},showcv=False)), # alpha=.0008    \n                 ('lasso',linear_estimator('lasso',Lasso(),X,y,degrees=[3],seed=randomseed,folds=cvfolds,paramgrid={'lasso__alpha':np.arange(0.002,.0025,1e-4)},showcv=False)), # alpha=.0022    \n                 ('knnreg',linear_estimator('knnreg',KNeighborsRegressor(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'kneighborsregressor__n_neighbors':np.arange(24,32,1)})),  # n=28    \n                 ('forestreg',linear_estimator('forestreg',RandomForestRegressor(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'randomforestregressor__n_estimators':np.arange(29,33,1),'randomforestregressor__min_samples_split':np.arange(3,7,1)})), # 31,4  # n=28    \n                 ('treereg',linear_estimator('treereg',DecisionTreeRegressor(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'decisiontreeregressor__max_depth':np.arange(2,7,1)})), # 4  \n                 ('svr',linear_estimator('svr',SVR(),X,y,degrees=[1],seed=randomseed,folds=cvfolds,paramgrid={'svr__kernel':['rbf'],'svr__C':np.arange(.69,.72,1e-3)})) # rbf,C=.701    \n                 ]\n\nresultlist = []\n# for i,est in enumerate(estimatorlist):\nfor tup in estimatorlist:\n    resultlist.append((tup[0],tup[1]))\n# append PCA reg separately    \nresultlist.append(('pcareg',pca_regression(LinearRegression(),X,y,testsplit=.3,seed=randomseed,paramgrid={'linearregression__normalize':[False,True],'linearregression__fit_intercept':[True,False]},folds=cvfolds)))\nresultlist.append(('pcaridge',pca_regression(Ridge(),X,y,testsplit=.3,seed=randomseed,paramgrid={'ridge__normalize':[False,True],'ridge__fit_intercept':[True,False],'ridge__alpha':np.arange(1e-1,.24,1e-3)},folds=cvfolds)))\nresultsorted = sorted(resultlist,key=lambda x: x[1][1])\nresultsorted\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"169b99e8ae82c0a6f0be1f0997173e68b563ea83"},"cell_type":"markdown","source":"### Final Linear Estimator Results (RMSE)\n  - Polynomial degree 3 Ridge regression outperformed\n  - Bespoke PCA and PCA ridge L2-norm regularized (sum squared coefs) regression outperformed\n  - As a first estimator, I'd select PCA ridge as it has a larger test r2"},{"metadata":{"trusted":true,"_uuid":"116b5371e03c4f8b919c1f375dbce67fda54c918"},"cell_type":"code","source":"for i,result in enumerate(resultsorted):\n    # index pipeline\n    estname = result[0]\n    polydegree = result[1][0]\n    rmse = result[1][1]\n    r2 = result[1][2]\n    print('{}.{}\\npoly={}\\trmse={:,.10f}\\tr2={:,.10f}\\n'.format(i+1,estname,polydegree,rmse,r2))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}