{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"4f30779f-532c-4b8c-32d8-e0d84da30150"},"source":"# Machine Learning Notebook - 911 Calls Regression "},{"cell_type":"markdown","metadata":{"_cell_guid":"69acf4c7-d753-b2b5-1c87-8ab49de0cf5a"},"source":"## 00. Goal"},{"cell_type":"markdown","metadata":{"_cell_guid":"f4999a28-5a7e-f1b6-8786-974902790264"},"source":"Goal of this project is the following:\n 1. predict number of help & rescue (911 calls) events in US (PA) Montgomery County in any arbitrary day of 2017\n 2. prediction should provide number of events by time of day and by general location\n 3. prediction should be based on 2016 data\n 4. we must know the accuracy of the prediction\n 5. some guidance, regarding type of the event would be nice, too."},{"cell_type":"markdown","metadata":{"_cell_guid":"86d0adf2-3421-19f4-ea2d-08fce96af492"},"source":"## 01. Notebook Intro & Imports"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e19e0bd-18a2-147a-2802-1e04d47455e3"},"source":"US Montgomery 911 Calls Regression Notebook\n* Notebook @author Lukasz KM, lucas.mlexp@gmail.com, http://machinelearningexp.com\n* Notebook License: Creative Commons CC-BY-SA https://creativecommons.org/licenses/by-sa/4.0/\n* Dataset source: https://www.kaggle.com/mchirico/montcoalert\n* Dataset provided by montcoalert.org\n* Database released under Open Database License, individual contents under Database Contents License\n* Maps source: OpenStreetMap.org\n* Maps license: Open Data Commons Open Database License (ODbL).\n* Data Description:\n* Events Location: Montgomery County, PA, USA\n* Timespan: 2015-12-10 to 2017-01-27\n* Dataset columns:\n    - lat : String variable, Latitude\n    - lng: String variable, Longitude\n    - desc: String variable, Description of the Emergency Call\n    - zip: String variable, Zipcode\n    - title: String variable, Title\n    - timeStamp: String variable, YYYY-MM-DD HH:MM:SS\n    - twp: String variable, Township\n    - addr: String variable, Address\n    - e: String variable, Dummy variable (always 1 - counter)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b9f69390-869d-1257-24c4-be97c8011be2"},"source":"Montgomery County, locally also referred to as Montco, is a county located in the Commonwealth of Pennsylvania. As of the 2010 census, the population was 799,874, making it the third-most populous county in Pennsylvania, after Philadelphia and Allegheny Counties, and the 71st most populous in the United States. The county seat is Norristown. Montgomery County is very diverse, ranging from farms and open land in Upper Hanover to densely populated rowhouse streets in Cheltenham.\n\nSource: Wikipedia, https://en.wikipedia.org/wiki/Montgomery_County,_Pennsylvania, Creative Commons Attribution-ShareAlike License"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abb33d50-d2fe-1767-2a0b-1c8aa6e1360a"},"outputs":[],"source":"# imports\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nimport sklearn\nfrom __future__ import print_function\nfrom IPython.display import Image\nfrom IPython.display import display\nfrom IPython.display import HTML\nfrom sklearn import metrics\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn import preprocessing"},{"cell_type":"markdown","metadata":{"_cell_guid":"26d64cae-18f1-bac0-8bab-1692c680958c"},"source":"## 02. Load & Preview data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0befd3f1-3201-07d2-5f5b-af705a710bde"},"outputs":[],"source":"# Load data from disk and preview data\ndt1 = pd.read_csv(\"../input/911.csv\")\ndt1.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a39d0eee-2af2-bb2d-23e9-aba63f97c0d3"},"source":"## 03. Feature selection and data cleaning"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"174dca3b-f7f6-d6fb-3ed9-4a0a94953c08"},"outputs":[],"source":"# restrict data to 2016 to get full year picture (data contain also events from Dec 2015 and Jan 2017)\n# remove unnecessary columns\n# we are interested only in accident time, title and geolocation data\n# preview limited data\ndt1 = dt1[dt1['timeStamp'].str.contains('2016', na = False)]\ndt2 = dt1.drop([\"desc\",\"addr\",\"e\"],axis=1)\nprint (\"Dataset shape :\",dt1.shape)\ndt2.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a0ccf4d7-c4f4-3ac4-0629-ff124a8eaf8a"},"source":"We have around 142 thousands of events in our dataset. That seems to be enough to run our experiment and expect to have a pretty decent result."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f7d70de-95f6-5e37-91e0-89782881fce7"},"outputs":[],"source":"# check if we have rows with empty data in the dataset\nprint (\"lat empty count :\", dt2['lat'].isnull().sum())\nprint (\"lng empty count :\", dt2['lng'].isnull().sum())\nprint (\"zip empty count :\", dt2['zip'].isnull().sum())\nprint (\"title empty count :\", dt2['title'].isnull().sum())\nprint (\"timeStamp empty count :\", dt2['timeStamp'].isnull().sum())\nprint (\"twp empty count :\", dt2['twp'].isnull().sum())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6963eef5-7025-84b0-1d2a-52270b0ebcc0"},"outputs":[],"source":"# cleaning columns with empty values, we can achieve our goals without them \n# it is better to remove columns than get rid of around 18k of events\ndt3 = dt2.drop([\"zip\",\"twp\"],axis=1)\ndt3.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3282152f-fa1b-2f8b-c1fd-1705b134f97e"},"outputs":[],"source":"# let's check the statistical properties of the numerical data\ndt3.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93616e03-a848-de96-da7f-3964f30dc16a"},"outputs":[],"source":"# it seems we have some outliers in the geographical data. \n# Montgomery County (PA) does not have any locations with min lat 30.333596 or min lng -95.595595. \n# there must be some human (data entry operator) mistakes and we need to identify them\noutliers = dt3.loc[((dt3['lat'] < 39.00) | (dt3['lat'] > 41.00)) & \n                   ((dt3['lng'] < -77.00) | (dt3['lng'] > -74.00))]\nprint (\"Outliers :\\n\",outliers)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7effec7-31f5-45be-7648-24e41e724f54"},"outputs":[],"source":"# we have just a few outliers, removing them will not affect the result. Lets filter out the outliers\ndt4 = dt3.loc[((dt3['lat'] > 39.00) & (dt3['lat'] < 41.00)) & \n              ((dt3['lng'] > -77.00) & (dt3['lng'] < -74.00))]\n# and describe the dataset again to check effect of the cleaning\ndt4.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"39763d7e-4772-8d18-55d6-b6714013d624"},"source":"## 04. Features engineering"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03e17852-454a-8f38-2d4c-218bc1dc00d2"},"outputs":[],"source":"# we want to get event type from its title\n# first we list unique values in title\ntitles_unique = pd.DataFrame(dt4.title.unique())\ntitles_unique = titles_unique.sort_values([0],ascending =  True)\nprint (\"Unique titles size :\",len(titles_unique))\ntitles_unique.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2d836b5-3bc1-718a-7fe3-14b5408be32d"},"outputs":[],"source":"# as we have more than 100 unique categories of events, let's  modify dataset and assign\n# to each event only the master category (the one before the colon)\ndt5 = dt4.copy()\ndt5['category'],dt5['category2'] = dt5['title'].str.split(':',1).str\ndt5 = dt5.drop(['title','category2'],axis = 1)\ncat_unique = pd.DataFrame(dt5.category.unique())\ncat_unique = cat_unique.sort_values([0],ascending =  True)\ncat_unique.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19d29213-a185-85a0-ded7-ffd7865d5864"},"outputs":[],"source":"# We now have the dictionary of the unique categories. We can change strings in dataset to the category numbers. \n# That is necessary as we want to feed the Machine Learning model with this dataset\n# and it must contain only numeric values. Here is the mapping:\n# 0 = EMS (Emergency Medical Services)\n# 1 = FIRE\n# 2 =  TRAFFIC\nCATEGORIES = {'EMS':0,'Fire':1,'Traffic':2}\ndt5['category'].replace(CATEGORIES,inplace=True)\ndt5.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f9dc090-43f4-fe6e-3c0b-cf67e180521f"},"outputs":[],"source":"# now we want to parse timestamp to get more information from it.\n# we will extend the dataset with more time related values\n# hours_range allows us to split day into several periods, each hours_range long\nhours_range = 8\ndt6 = dt5\ndt6['datetime'] = pd.to_datetime(dt5['timeStamp'])\ndt6['year'] = dt5['datetime'].dt.year\ndt6['month'] = dt5['datetime'].dt.month\ndt6['day'] = dt5['datetime'].dt.day\ndt6['day_part'] = np.floor(dt5['datetime'].dt.hour/hours_range)\ndt6['day_part'] = dt5.day_part.astype(int)\ndt6['dayofweek'] = dt5['datetime'].dt.dayofweek\ndt6['week'] = dt5['datetime'].dt.week\n#let's describe the dat again\ndt6.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79d7b958-1238-45a5-a47a-7cd435313dae"},"outputs":[],"source":"# the geo coordinates have limited range\n# we want to split the whole location into the geo grid\n# epsilon is to extend the upper bound minimally \n# to avoid assigning locations at the end of the range to new slot beyond the grid\nepsilon = 0.0001\nlat_max = dt6['lat'].max() + epsilon\nlat_min = dt6['lat'].min()\nlat_range = lat_max - lat_min\nprint (\"Latitude min-max: <\",lat_min,lat_max,\"> | range :\",lat_range)\nlng_max = dt6['lng'].max() + epsilon\nlng_min = dt6['lng'].min()\nlng_range = lng_max - lng_min\nprint (\"Longitude min-max: <\",lng_min,lng_max,\"> | range :\",lng_range)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ece99ce-f1df-6ec1-33fd-896cae5417e5"},"outputs":[],"source":"# Let's then split the area set by these coordinates into an grid\n# we will divide the lat and lng range, thus creating grid of rectangles\nlat_split = 5 # number of horizontal parts\nlng_split = 7 # number of vertical parts\nlat_hop = lat_range/lat_split # lat divided to N parts gives us length of one part\nprint (\"Lat hop : \",lat_hop)\nlng_hop = lng_range/lng_split # lng divided to N parts gives us length of one part\nprint (\"Lng hop : \",lng_hop)\n# now we need to assign coordinates to proper geogrid squares\ndt6['lat_grid'] = (np.floor(((dt6['lat']-lat_min)/lat_hop)))\ndt6['lng_grid'] = (np.floor(((dt6['lng']-lng_min)/lng_hop)))\ndt6.lat_grid = dt6.lat_grid.astype(int)\ndt6.lng_grid = dt6.lng_grid.astype(int)\ndt7 = dt6.drop(['lat','lng'],axis = 1)\ndt7 = dt6\ndt7.head(3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"39bd6792-c21b-6eff-b585-35ded931ceca"},"source":"## 05. Visualize data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"690dc015-29ba-3a7d-7c5d-e0783ee72367"},"outputs":[],"source":"# let's check number of events per month\nfig, ax = plt.subplots(figsize=(7,3))  \nax = sns.countplot(x=\"month\", data=dt7,ax=ax)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7892f756-b5d0-d370-c8de-317aa5816a8d"},"source":"October-January is definetely the worst period, probably due to the weather. Also, vacation period has higher number of events."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd89023e-b311-1b8d-9725-6eac048d1e80"},"outputs":[],"source":"# let's check number of events per day of the week\nfig, ax = plt.subplots(figsize=(5,3))\nax = sns.countplot(x=\"dayofweek\", data=dt7)\nax.axes.set_xticklabels([\"MON\", \"TUE\",\"WED\",\"THU\",\"FRI\",\"SAT\",\"SUN\"])\npass"},{"cell_type":"markdown","metadata":{"_cell_guid":"13b7f525-85ac-29ab-8ac3-90f7d7eca011"},"source":"Friday is definetely the leader in this pack. Start of the weekend - that explains a lot."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51f1306f-702d-1345-f88e-666536ff64a9"},"outputs":[],"source":"#let's see the size of each category (class)\n# 0 = EMS (Emergency Medical Services), 1 = FIRE, 2 =  TRAFFIC\nfig, ax = plt.subplots(figsize=(5,3))\nax = sns.countplot(x=\"category\", data=dt7)\nax.axes.set_xticklabels([\"EMS\",\"FIRE\",\"TRAFFIC\"])\npass"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd2f321c-d12f-efcb-315e-77344cc5d829"},"source":"EMS conditions are the majority of all events, followed by traffic (mostly car accidents)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db4a5e56-ed9e-699a-4d4f-e128e40f49eb"},"outputs":[],"source":"# lets check the time impact on the events\ndt_timegrid = dt7.groupby(['dayofweek','day_part']).size().reset_index(name='count')\ndt_timeheatmap = dt_timegrid.pivot(index='day_part', columns='dayofweek', values='count')\n# generate heatmap\nfig, ax = plt.subplots(figsize=(5,3))\nax = sns.heatmap(dt_timeheatmap,annot=True, fmt=\"d\",cbar=False)\nax.invert_yaxis()\nax.axes.set_yticklabels([\"16-24 h\",\"08-16 h\",\"00-08 h\"])\nax.axes.set_xticklabels([\"MON\", \"TUE\",\"WED\",\"THU\",\"FRI\",\"SAT\",\"SUN\"])\npass"},{"cell_type":"markdown","metadata":{"_cell_guid":"612da518-1c01-4ca7-e994-80814e091d31"},"source":"Luckily, events in the middle of the day dominates the whole 24h. That is also the time when most of the emergency teams operate with full performance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff6c4a10-2afb-3187-129f-5625fa156937"},"outputs":[],"source":"# now we can visualize our data on the geogrid.\ndt_geogrid = dt7.groupby(['lat_grid','lng_grid']).size().reset_index(name='count')\ndt_geoheatmap = dt_geogrid.pivot(index='lat_grid',columns='lng_grid', values='count')\n# generate heatmap\nfig, ax = plt.subplots(figsize=(5,5))  \nax = sns.heatmap(dt_geoheatmap,annot=True,fmt=\".0f\",cbar=False)\nax.invert_yaxis()\nsns.plt.show()\nprint (\"Longitude min-max: <\",lng_min,lng_max,\"> | range :\",lng_range)\nprint (\"Latitude min-max: <\",lat_min,lat_max,\"> | range :\",lat_range)\n#draw reference map\nprint (\"\\nUS PA Montgomery County Reference map\") \nprint (\"Map source: OpenStreetMap.org, Map license: Open Data Commons Open Database License (ODbL).\")\n# reference grid image is on my blog: \n# http://machinelearningexp.com/machine-learning-regression-911-calls/\n# Kaggle does not allow to upload additional files yet\nprint (\"See http://machinelearningexp.com/machine-learning-regression-911-calls/\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccadf254-21a4-77f1-1344-00acf9c05c45"},"outputs":[],"source":"# reorganize table to have mor intuitive order of the features\nfinal_columns = [\"month\",\"week\",\"dayofweek\",\"day\",\"day_part\",\"lat_grid\",\"lng_grid\",\"category\"]\ndt7 = dt6[final_columns]\ndt7.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b0520d3-6309-698e-8768-777c3498cb18"},"outputs":[],"source":"# let's describe the data again\ndt7.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a64f49b6-b358-2ebb-d778-f085a74c77fd"},"source":"## 06. Model data for Machine Learning regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1624836-d84a-fc17-64e0-9c397c724071"},"outputs":[],"source":"# create separate datasets for categories and group them by all parameters to get count of events for a given group\ngroupby_list = ['month','week','dayofweek','day','day_part','lat_grid','lng_grid']\ndt_cat = dict() # holder for subdatasets with categories. \nfor item in CATEGORIES:\n    dt_temp = dt7.loc[(dt7['category'] == CATEGORIES[item])]\n    dt_cat[item] =  dt_temp.groupby(groupby_list).size().reset_index(name='count')\ndt_cat['ALL'] = dt7.groupby(groupby_list).size().reset_index(name='count') # All data, without category grouping\ndt_cat['ALL'].head(3) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af7e43e0-c485-bb4e-a410-58082825a07e"},"outputs":[],"source":"dt_cat['ALL'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98d68798-9662-5849-b667-263932c44095"},"outputs":[],"source":"# let's now create a function that will split data into train and test sets and run regresion algorithm on the data\n\ndef run_regression(name,input_dt):\n    X = input_dt.iloc[:,[0,1,2,3,4,5,6]]\n    Y = input_dt.iloc[:,[7]]\n    Y = Y.values.reshape(len(X))\n    validation_size = 0.20\n    seed = 7\n    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = validation_size,random_state = seed)\n    model = GradientBoostingRegressor(n_estimators=200, \n                                      learning_rate=0.1, max_depth=5, random_state=0, loss='ls', warm_start =  True)\n    model.fit(X_train,Y_train)\n    return name,model,r2_score(Y_test, model.predict(X_test))\n\n# run model for all categories and put results into the table.\n# also save trained models for later use\nresults_table = [[\"CATEGORY\",\"R2 SCORE\"]]\ntrained_models = dict() # holder for trained models\nfor item in dt_cat:\n    results = run_regression(item,dt_cat[item])\n    results_table.append([item,results[2]])\n    trained_models[item] =  results[1]\n\nfor row in results_table:\n    print (row)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5bb9ead5-7dd3-6354-8ea1-e2fca3e08907"},"source":"## 07. Predict events for a single day in 2017"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"777ec8ac-dddb-936a-7a70-7351ecc871fd"},"outputs":[],"source":"# we will use trained GradientBoostingRegressor model to estimate 911 calls\n# in a single day of 2017, based on the 2016 year data\n# we need to generate list containing all time slots in a single day and \"active\" geogrid locations\n# the selected date will be 19 May 2017 (arbitrary date)\n# note we cannot use all geogrid locations as not for all we have the data\n# and model will not be able to predict anything meaningful for them\n# the county map does not cover the whole grid. \n# So we will use the previous dt_geogrid variable to get \"active\" locations\nsingleday_dt = []\n# record structure is month,week,dayofweek,day,day_part,lat_grid,lng_grid\nrow_base = [5,20,4,19] #base row with date 19 May 2017, Wednesday. Change it to get another day.\nfor day_idx in range(int(24/hours_range)):\n    for idx,row in dt_geogrid.iterrows():\n        singleday_dt.append(row_base+[day_idx,row['lat_grid'],row['lng_grid']]) \nsingleday_dt = pd.DataFrame(singleday_dt,columns=final_columns[:7])\nsingleday_dt.head(3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ca29b2b-3916-3691-024a-d5227d623b87"},"outputs":[],"source":"# we will pass generated data to scikit-learn model predict method to see the result\npredictions_all = trained_models['ALL'].predict(singleday_dt)\nsingleday_dt_full = singleday_dt\nsingleday_dt_full['events'] = predictions_all\nprint (\"Total number of 911 events in selected day is : \", round(singleday_dt_full['events'].sum()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10d6783e-16f6-68e2-656d-b0c2e3305854"},"outputs":[],"source":"# now we can visualize our data for 19 May 2017 on the map.\ndt_geogrid = singleday_dt_full.groupby(['lat_grid','lng_grid']).agg({'events': np.sum}).reset_index()\ndt_geoheatmap = dt_geogrid.pivot(index='lat_grid', columns='lng_grid', values='events')\n# generate heatmap\nfig, ax = plt.subplots(figsize=(5,5))  \nax = sns.heatmap(dt_geoheatmap,annot=True,fmt=\".0f\",cbar=False)\nax.invert_yaxis()\nsns.plt.show()\nfig = ax.get_figure()\nprint (\"US PA Montgomery County Reference map\") \nprint (\"Map source: OpenStreetMap.org, Map license: Open Data Commons Open Database License (ODbL).\")\n# reference grid image is on my blog: \n# http://machinelearningexp.com/machine-learning-regression-911-calls/\n# Kaggle does not allow to upload additional files yet\nprint (\"See http://machinelearningexp.com/machine-learning-regression-911-calls/\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d38ec52c-bb01-a110-5558-45e3752b314b"},"outputs":[],"source":"data_timeevents = singleday_dt_full.groupby(['day_part']).agg({'events': np.sum}).reset_index()\nfig, ax = plt.subplots(figsize=(5,3))\nax = sns.barplot(x=\"day_part\", y=\"events\", data=data_timeevents)\nax.axes.set_xticklabels([\"00-08 h\",\"08-16 h\",\"16-24 h\"])\npass"},{"cell_type":"markdown","metadata":{"_cell_guid":"5ac3fc49-2b03-e89b-378a-e901c8fb2b08"},"source":"## 08. Summary"},{"cell_type":"markdown","metadata":{"_cell_guid":"6dfd9bff-4104-b643-3b56-f0e41a106312"},"source":"Let's check whether we have achieved our goals:\n1. predict number of help & rescue (911) events in US (PA) Montgomery County in arbitrary day of 2017\n    * PASSED, we can predict data for any day using our trained model.\n2. prediction should provide number of events by time of day and by general location\n    * PASSED, we can get prediction per time of day and per geolocation grid\n3. prediction should be based on 2016 data\n    * PASSED, model is trained on 2016 data\n4. we must know the accuracy of the prediction\n    * PASSED, The overall accuracy (R2 score) for 2017 is 0.81\n5. some guidance, regarding type of the event would be nice, too.\n    * PARTIALLY PASSED. We know general relation between number of events in each category. So we know that most of events is in EMS category, followed by Traffic. Fire events are the last. However, the prediction accuracy for categories is too low to use it (is acceptable for for EMS where equals 0.75 , but for FIRE is just  0.32. Seems that fire events are quite hard to predict)."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}