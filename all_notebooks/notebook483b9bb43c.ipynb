{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport graphviz \nfrom sklearn import model_selection\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, learning_curve, StratifiedKFold, train_test_split\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score \nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier as MLPC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"_cell_guid":"37867649-a1fc-44f7-acd7-9794152a9c04","_uuid":"8a89dacf039a9f136a8d06799ffe5068d1a60e39","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"est_sdef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, tcores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndef compareABunchOfDifferentModelsAccuracy(a, b, c, d):\n    \"\"\"\n    compare performance of classifiers on X_train, X_test, Y_train, Y_test\n    http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n    http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n    \"\"\"    \n    print('\\nCompare Multiple Classifiers: \\n')\n    print('K-Fold Cross-Validation Accuracy: \\n')\n    names = []\n    models = []\n    resultsAccuracy = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('RF', RandomForestClassifier()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('SVM', SVC()))\n    models.append(('LSVM', LinearSVC()))\n    models.append(('GNB', GaussianNB()))\n    models.append(('DTC', DecisionTreeClassifier()))\n    models.append(('GBC', GradientBoostingClassifier()))\n    for name, model in models:\n        model.fit(a, b)\n        kfold = model_selection.KFold(n_splits=10, random_state=7)\n        accuracy_results = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n        resultsAccuracy.append(accuracy_results)\n        names.append(name)\n        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n        print(accuracyMessage) \n    # Boxplot\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparison: Accuracy')\n    ax = fig.add_subplot(111)\n    plt.boxplot(resultsAccuracy)\n    ax.set_xticklabels(names)\n    ax.set_ylabel('Cross-Validation: Accuracy Score')\n    plt.show()    \n      \ndef defineModels():\n    print('\\nLR = LogisticRegression')\n    print('RF = RandomForestClassifier')\n    print('KNN = KNeighborsClassifier')\n    print('SVM = Support Vector Machine SVC')\n    print('LSVM = LinearSVC')\n    print('GNB = GaussianNB')\n    print('DTC = DecisionTreeClassifier')\n    print('GBC = GradientBoostingClassifier \\n\\n')\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"MLPClassifier\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]    \n\nclassifiers = [\n    KNeighborsClassifier(),\n    SVC(kernel=\"linear\"),\n    SVC(kernel=\"rbf\"),\n    GaussianProcessClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    MLPClassifier(),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]\n\ndict_characters = {0: 'Healthy', 1: 'Diabetes'}","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"_cell_guid":"f42887f6-2a04-4eb4-a912-764a05776d64","_uuid":"89fb0cd2287578fec1c291e1d610d0a9116663b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = read_csv('../input/diabetes.csv')\ndataset.head(10)","metadata":{"_uuid":"924edc193c853395713bcfdade0a26f36633bf03","_cell_guid":"65475636-e376-491c-9ff5-f558598afd37","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It would be a serious medical problem if a patient had an insulin level and skin thickness measurement of zero.  As such, we can conclude that this dataset uses the number zero to represent missing or null data.  Here we can see that as many as half of the rows contain columns with missing data.","metadata":{"_uuid":"3c1e827efa192861b80ea25355106d6600ef9273","_cell_guid":"67fd6314-6176-4449-9839-b209ab0bd373"}},{"cell_type":"code","source":"def plotHistogram(values,label,feature,title):\n    sns.set_style(\"whitegrid\")\n    plotOne = sns.FacetGrid(values, hue=label,aspect=2)\n    plotOne.map(sns.distplot,feature,kde=False)\n    plotOne.set(xlim=(0, values[feature].max()))\n    plotOne.add_legend()\n    plotOne.set_axis_labels(feature, 'Proportion')\n    plotOne.fig.suptitle(title)\n    plt.show()\nplotHistogram(dataset,\"Outcome\",'Insulin','Insulin vs Diagnosis (Blue = Healthy; Orange = Diabetes)')\nplotHistogram(dataset,\"Outcome\",'SkinThickness','SkinThickness vs Diagnosis (Blue = Healthy; Orange = Diabetes)')","metadata":{"_uuid":"d87a65f1087b89f570fc14ffc96115577e6824bf","_cell_guid":"033d06fa-6a63-4c88-8745-6ab0eb9dc80a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset2 = dataset.iloc[:, :-1]\nprint(\"# of Rows, # of Columns: \",dataset2.shape)\nprint(\"\\nColumn Name           # of Null Values\\n\")\nprint((dataset2[:] == 0).sum())","metadata":{"scrolled":true,"_uuid":"0031b33c44f66dfc7437586c0b7a3e466979c42f","_cell_guid":"6c9eff67-66d0-46c4-80ea-3e2451471683","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"# of Rows, # of Columns: \",dataset2.shape)\nprint(\"\\nColumn Name              % Null Values\\n\")\nprint(((dataset2[:] == 0).sum())/768*100)","metadata":{"_uuid":"6c8a90ee5f18fd5e214eec426ae38802c66503e1","_cell_guid":"4751b521-e977-46d6-948c-de0bf90e6082","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.heatmap(dataset.corr(),cmap=\"BrBG\",annot=False)","metadata":{"_uuid":"c2039ea71625f74a46452421ecd2ada3618465fa","_cell_guid":"d9459be8-3ac9-477c-846b-471ce09cda23","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.corr()","metadata":{"_uuid":"39901d449b5617ce676357f85c85a051b2b0fe6b","_cell_guid":"c8d41550-c10e-4b7d-b5fd-7baa1c4a8560","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = read_csv('../input/diabetes.csv')\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nimputer = Imputer(missing_values=0,strategy='median')\nX_train2 = imputer.fit_transform(X_train)\nX_test2 = imputer.transform(X_test)\nX_train3 = pd.DataFrame(X_train2)\nplotHistogram(X_train3,None,4,'Insulin vs Diagnosis (Blue = Healthy; Orange = Diabetes)')\nplotHistogram(X_train3,None,3,'SkinThickness vs Diagnosis (Blue = Healthy; Orange = Diabetes)')","metadata":{"_uuid":"305e3128b63c6ec838c9c804ffc7e5bf7ef54b92","_cell_guid":"8aac39aa-0047-4963-ad7a-f1b98e92f4f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = {0:'Pregnancies',1:'Glucose',2:'BloodPressure',3:'SkinThickness',4:'Insulin',5:'BMI',6:'DiabetesPedigreeFunction',7:'Age'}\nprint(labels)\nprint(\"\\nColumn #, # of Zero Values\\n\")\nprint((X_train3[:] == 0).sum())\n# data[:] = data[:].replace(0, np.NaN)\n# print(\"\\nColumn #, # of Null Values\\n\")\n# print(np.isnan(X_train3).sum())","metadata":{"_uuid":"2f9b3686bc6a2cfc70379cb76872454f7d871f0b","_cell_guid":"0a5497a2-c66d-4e8a-936b-317e5bb3ee69","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Step 4: Evaluate Classification Models*","metadata":{"_uuid":"1047f9eab8fd3ba251509aeb7147d678fb8a58fd","_cell_guid":"ffa86ef6-7a50-4110-aea0-198d208134c5"}},{"cell_type":"markdown","source":"Because we have replaced all of the erroneous, missing, and null values with median values we are now ready to train and evaluate our models for predicting diabetes.","metadata":{"_uuid":"289dab0f5b01ad7713d1571b9998d6a216caaed2","_cell_guid":"1743b613-a0b7-4371-af2d-ba22ee28f839"}},{"cell_type":"code","source":"compareABunchOfDifferentModelsAccuracy(X_train2, y_train, X_test2, y_test)\ndefineModels()\n# iterate over classifiers; adapted from https://www.kaggle.com/hugues/basic-ml-best-of-10-classifiers\nresults = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train2, y_train, cv=5)\n    results[name] = scores\nfor name, scores in results.items():\n    print(\"%20s | Accuracy: %0.2f%% (+/- %0.2f%%)\" % (name, 100*scores.mean(), 100*scores.std() * 2))","metadata":{"_uuid":"e4fc9b2ffc3999bf85badc844ec7d1bda9df07ba","_cell_guid":"f42c3b2f-7926-4d4d-bb14-cfd2088cc29c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Step 5: Examine Decision Tree Model in More Detail*","metadata":{"_uuid":"da888d8ba44fd872cbd6365a34fc15b9f962a72a","_cell_guid":"fb320556-96f2-4ee3-b8c5-5d7c2e4dea2a"}},{"cell_type":"markdown","source":"Next let's explore the decision tree model in more detail.","metadata":{"_uuid":"33b00d6abeafbb0dae97a359ad690f4e42da9ede","_cell_guid":"139c8318-7534-429a-95b8-93cfa50889fe"}},{"cell_type":"code","source":"def runDecisionTree(a, b, c, d):\n    model = DecisionTreeClassifier()\n    accuracy_scorer = make_scorer(accuracy_score)\n    model.fit(a, b)\n    kfold = model_selection.KFold(n_splits=10, random_state=7)\n    accuracy = model_selection.cross_val_score(model, a, b, cv=kfold, scoring='accuracy')\n    mean = accuracy.mean() \n    stdev = accuracy.std()\n    prediction = model.predict(c)\n    cnf_matrix = confusion_matrix(d, prediction)\n    #plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,title='Normalized confusion matrix')\n    plot_learning_curve(model, 'Learning Curve For DecisionTreeClassifier', a, b, (0.60,1.1), 10)\n    #learning_curve(model, 'Learning Curve For DecisionTreeClassifier', a, b, (0.60,1.1), 10)\n    plt.show()\n    plot_confusion_matrix(cnf_matrix, classes=dict_characters,title='Confusion matrix')\n    plt.show()\n    print('DecisionTreeClassifier - Training set accuracy: %s (%s)' % (mean, stdev))\n    return\nrunDecisionTree(X_train2, y_train, X_test2, y_test)\nfeature_names1 = X.columns.values\ndef plot_decision_tree1(a,b):\n    dot_data = tree.export_graphviz(a, out_file=None, \n                             feature_names=b,  \n                             class_names=['Healthy','Diabetes'],  \n                             filled=False, rounded=True,  \n                             special_characters=False)  \n    graph = graphviz.Source(dot_data)  \n    return graph \nclf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\nclf1.fit(X_train2, y_train)\nplot_decision_tree1(clf1,feature_names1)","metadata":{"_kg_hide-input":true,"_uuid":"b61c2106275d1d58033b65a1a0d8c6067dbf7b03","_cell_guid":"8a7f778a-1191-48e9-818b-d6914ab1dfd9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = X.columns.values\nclf1 = tree.DecisionTreeClassifier(max_depth=3,min_samples_leaf=12)\nclf1.fit(X_train2, y_train)\nprint('Accuracy of DecisionTreeClassifier: {:.2f}'.format(clf1.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf1.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('DecisionTreeClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n\nfeature_names = X.columns.values\nclf2 = RandomForestClassifier(max_depth=3,min_samples_leaf=12)\nclf2.fit(X_train2, y_train)\nprint('Accuracy of RandomForestClassifier: {:.2f}'.format(clf2.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf2.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('RandomForestClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n\nclf3 = XGBClassifier()\nclf3.fit(X_train2, y_train)\nprint('Accuracy of XGBClassifier: {:.2f}'.format(clf3.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf3.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('XGBClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n","metadata":{"_kg_hide-input":true,"_uuid":"a4fafaf035a47504df06e17a6b0ba539957d8cc2","_cell_guid":"73925719-607e-4041-9012-4890c5e28867","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = read_csv('../input/diabetes.csv')\ndata2 = data.drop(['Pregnancies','BloodPressure','DiabetesPedigreeFunction', 'Age','SkinThickness','Insulin'], axis=1)\nX2 = data2.iloc[:, :-1]\ny2 = data2.iloc[:, -1]\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X2, y2, test_size=0.2, random_state=1)\nimputer = Imputer(missing_values=0,strategy='median')\nX_train3 = imputer.fit_transform(X_train3)\nX_test3 = imputer.transform(X_test3)\nclf4 = XGBClassifier()\nclf4.fit(X_train3, y_train3)\nprint('Accuracy of XGBClassifier in Reduced Feature Space: {:.2f}'.format(clf4.score(X_test3, y_test3)))\ncolumns = X2.columns\ncoefficients = clf4.feature_importances_.reshape(X2.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('\\nXGBClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n\nclf3 = XGBClassifier()\nclf3.fit(X_train2, y_train)\nprint('\\n\\nAccuracy of XGBClassifier in Full Feature Space: {:.2f}'.format(clf3.score(X_test2, y_test)))\ncolumns = X.columns\ncoefficients = clf3.feature_importances_.reshape(X.columns.shape[0], 1)\nabsCoefficients = abs(coefficients)\nfullList = pd.concat((pd.DataFrame(columns, columns = ['Variable']), pd.DataFrame(absCoefficients, columns = ['absCoefficient'])), axis = 1).sort_values(by='absCoefficient', ascending = False)\nprint('XGBClassifier - Feature Importance:')\nprint('\\n',fullList,'\\n')\n","metadata":{"_kg_hide-input":true,"_uuid":"420c5fd425ff2de2bde384df9f5874b1f1ed025b","_cell_guid":"257d3eca-95c0-4fdf-90d2-589c1f34985c","trusted":true},"execution_count":null,"outputs":[]}]}