{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Find similar brand names with FuzzyWuzzy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are different ways to make data dirty, and inconsistent data entry is one of them. Inconsistent values are even worse than duplicates, and sometimes difficult to detect.<br>\nIn this notebook, I apply [FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy) package to find similar ramen brand names in a ramen review dataset.<br>\nData source: [The ramen rater Big list](https://www.theramenrater.com/resources-2/the-list/)\n\nOn my [GitHub](http://localhost:8888/notebooks/Desktop/Github/Python/Ramen/Find%20similar%20strings%20with%20FuzzyWuzzy.ipynb)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom fuzzywuzzy import process, fuzz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview of the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in the dataset\nramen = pd.read_csv('/kaggle/input/ramen-ratings-latest-update-jan-25-2020/Ramen_ratings_2020.csv')\n\n# Display the first columns\nramen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramen.drop('URL', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check data type and null values\nramen.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove leading and trailing spaces in each string value\nfor col in ramen[['Brand','Variety','Style','Country']]:\n    ramen[col] = ramen[col].str.strip()\n    print('Number of unique values in ' + str(col) +' is ' + str(ramen[col].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ramen['Country'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would like to know if one brand can have multiple manufacturers in different countries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"brand_country = ramen['Brand'] +' '+ ramen['Country']\nbrand_country.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"586 is greater than 510 brands, so yes, one brand can have different country values. Next, we need to get the list of unique brand names.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_brand = ramen['Brand'].unique().tolist()\nsorted(unique_brand)[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some suspicious names right at the beginning. Let's have some tests first.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# FuzzyWuzzy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"FuzzyWuzzy has four scorer options to find the Levenshtein distance between two strings. In this example, I would check on the token sort ratio and the token set ratio, for I believe they are more suitable for this dataset which might have mixed words order and duplicated words.<br>\nI would pick four brand names and find their similar names in the Brand column. Since we're matching the Brand column with itself, the result would always include the selected name with a score of 100.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Token Sort Ratio\nThe token sort ratio scorer tokenizes the strings and cleans them by returning these strings to lower cases, removing punctuations, and then sorting them alphabetically. After that, it finds the Levenshtein distance and returns the similarity percentage.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('7 Select', unique_brand, scorer=fuzz.token_sort_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This result means that '7 Select/Nissin' has 70% similarity when referring to '7 Select'. Not bad if I set the threshold at 70% to get the pair of 7 Select - 7 Select/Nissin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('A-Sha', unique_brand, scorer=fuzz.token_sort_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below 70%, there's no match for A-sha.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('Acecook', unique_brand, scorer=fuzz.token_sort_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still good at 70% threshold.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract(\"Chef Nic's Noodles\", unique_brand, scorer=fuzz.token_sort_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have a problem here. Token sort ratio scorer will gets the wrong pair of Chef Nic's Noodles - Mr. Lee's Noodles if I set 70% threshold.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('Chorip Dong', unique_brand, scorer=fuzz.token_sort_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This one looks good enough.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Token Set Ratio\nThe token set ratio scorer also tokenizes the strings, and follows processing steps just like the token sort ratio. Then it collects common tokens between two strings and performs pairwise comparisons to find the similarity percentage.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('7 Select', unique_brand, scorer=fuzz.token_set_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the token set ratio is more flexible, the score has increased from 70% to 100% for 7 Select - 7 Select/Nissin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('A-Sha', unique_brand, scorer=fuzz.token_set_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we see A-Sha has another name as A-Sha Dry Noodle. And we can see this only by using token set ratio.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('Acecook', unique_brand, scorer=fuzz.token_set_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This one got 100% just like the 7 Select case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract(\"Chef Nic's Noodles\", unique_brand, scorer=fuzz.token_set_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This one got much worse when token set ratio returns 100% for the pair of Chef Nic's Noodles - S&S.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"process.extract('Chorip Dong', unique_brand, scorer=fuzz.token_set_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the same result for this one.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Although the token set ratio is more flexible and can detect more similar strings than the token sort ratio, it might also bring in more wrong matches.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Apply FuzzyWuzzy in one column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Token Sort Ratio","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to create a dataframe with brand names, matched brands, and their scores.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create tuples of brand names, matched brand names, and the score\nscore_sort = [(x,) + i\n             for x in unique_brand \n             for i in process.extract(x, unique_brand, scorer=fuzz.token_sort_ratio)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create dataframe from the tuples\nsimilarity_sort = pd.DataFrame(score_sort, columns=['brand_sort','match_sort','score_sort'])\nsimilarity_sort.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we're looking for matched values from the same column, one value pair would have another same pair in a reversed order. For example, we will find one pair of EDO Pack - Gau Do, and another pair of Gau Do - EDO Pack. To eliminate one of them later, I need to find a representative value for each of two same pairs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Derive representative values\nsimilarity_sort['sorted_brand_sort'] = np.minimum(similarity_sort['brand_sort'], similarity_sort['match_sort'])\nsimilarity_sort.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the tests above, I would care about those pairs which have at least 80% similarity. I also exclude those which match to themselves (brand value and match value are exactly the same), and those which are duplicated pairs.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"high_score_sort = similarity_sort[(similarity_sort['score_sort'] >= 80) &\n                                      (similarity_sort['brand_sort'] != similarity_sort['match_sort']) &\n                                      (similarity_sort['sorted_brand_sort'] != similarity_sort['match_sort'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop the representative value column\nhigh_score_sort = high_score_sort.drop('sorted_brand_sort',axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see the result.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Group matches by brand names and scores\n#pd.set_option('display.max_rows', None)\nhigh_score_sort.groupby(['brand_sort','score_sort']).agg(\n                        {'match_sort': ', '.join}).sort_values(\n                        ['score_sort'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the score of 95 and above, everything looks good. In each pair, the two values might have typos, one missing character, or inconsistent format, but overal they obviously refer to each other. Below 95, it would be harder to tell. We can look at some examples by listing out data from each pair.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Souper - Super - 91%\nramen[(ramen['Brand'] == 'Souper') | (ramen['Brand'] == 'Super')].sort_values(['Brand'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this pair, we see that these two brands come from different manufacturers, and there's also no similarity in their ramen types or styles. I would say that these brands are not the same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sura - Suraj - 89%\nramen[(ramen['Brand'] == 'Sura') | (ramen['Brand'] == 'Suraj')].sort_values(['Brand'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sura and Suraj are two different brands.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ped Chef - Red Chef - 88%\nramen[(ramen['Brand'] == 'Ped Chef') | (ramen['Brand'] == 'Red Chef')].sort_values(['Brand'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we only have one record of the Ped Chef brand, and we also see the same pattern in its variety name in comparison with the Red Chef brand. I'm pretty sure these two brands are the same.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can continue to check other pairs by the same method. From the threshold of 84% and below, we can ignore some pairs which are obviously different or we can make a quick check as above. Next, I will apply token set ratio scorer to find matched brand names, and then compare the results.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Token Set Ratio","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will go over the same steps as above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create tuples of brand names, matched brand names, and the score\nscore_set = [(x,) + i\n             for x in unique_brand \n             for i in process.extract(x, unique_brand, scorer=fuzz.token_set_ratio)]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Create dataframe from the tuples and derive representative values\nsimilarity_set = pd.DataFrame(score_set, columns=['brand_set','match_set','score_set'])\nsimilarity_set['sorted_brand_set'] = np.minimum(similarity_set['brand_set'], similarity_set['match_set'])\n\n#Pick values\nhigh_score_set = similarity_set[(similarity_set['score_set'] >= 80) & \n                                    (similarity_set['brand_set'] != similarity_set['match_set']) & \n                                    (similarity_set['sorted_brand_set'] != similarity_set['match_set'])]\n\n#Drop the representative value column\nhigh_score_set = high_score_set.drop('sorted_brand_set',axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the token set ratio scorer will tolerate more 'noise' when matching two values. I would group the result by matched values to reduce the number of rows.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Group brands by matches and scores\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', None)\nhigh_score_set.groupby(['match_set','score_set']).agg(\n                       {'brand_set': ', '.join}).sort_values(\n                       ['score_set'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this part, I will create a merged table including results from token sort ratio and token set ratio with some changes.<br>\nThe tables will be merged by matched values to shorten the result table, and because I would like to keep the scores after grouping all values, I need to create new columns which combine brand names and scores. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create columns with brand names combining scores\nhigh_score_sort['brand_sort'] = high_score_sort['brand_sort'] + ': ' + high_score_sort['score_sort'].astype(str)\nhigh_score_set['brand_set'] = high_score_set['brand_set'] + ': ' + high_score_set['score_set'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I can group the two tables by matched names, and then rename the columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Group data by matched name and store in new dataframe\ntoken_sort = high_score_sort.groupby(['match_sort']).agg({'brand_sort': ', '.join}).reset_index()\ntoken_set = high_score_set.groupby(['match_set']).agg({'brand_set': ', '.join}).reset_index()\n\n#Rename columns\ntoken_sort = token_sort.rename(columns={'match_sort':'brand'})\ntoken_set = token_set.rename(columns={'match_set':'brand'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would choose 'outer' option to get all the data from two tables.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Outer join two tables by brand (matched names)\nsimilarity = pd.merge(token_sort, token_set, how='outer', on='brand')\n\n#Replace NaN values and rename columns for readability\nsimilarity = similarity.replace(np.nan,'')\nsimilarity = similarity.rename(columns={'brand_set':'token_set_ratio','brand_sort':'token_sort_ratio'})","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"similarity.sort_values('brand')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see how different it is between two scorers. As expected, the token set ratio matches wrong names with high scores (e.g. S&S, Mr.Noodles). However, it does bring in more matches that the token sort ratio could not get (e.g. 7 Select/Nissin, Sugakiya Foods, Vina Acecook).<br>\nIt would be beneficial to apply both methods in this case.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}