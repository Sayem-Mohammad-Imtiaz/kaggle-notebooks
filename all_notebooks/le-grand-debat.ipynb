{"cells":[{"metadata":{"id":"Lv0Dj6JWqZCX"},"cell_type":"markdown","source":"## Le grand debat"},{"metadata":{"id":"H8Ej87XSp-Fw"},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"id":"Fr7cUfyPtxSk","outputId":"19a56c26-028b-454c-b3c6-ec20ed30e9c2","trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git","execution_count":null,"outputs":[]},{"metadata":{"id":"cJdQtpjNzzIE","outputId":"51cf5cd0-a8d0-4f56-94eb-8c4b782c0de4","trusted":true},"cell_type":"code","source":"# basics\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport os\n\n# string\nimport string\n!pip install unidecode\nimport unidecode\nimport re\nfrom textwrap import wrap # wrapping long text into lines\n\n# plot\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n# %matplotlib inline\n\n# text mining\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\n\n\n# Because we have some long strings to deal with:\npd.options.display.max_colwidth = 300\n\n\n\nreplacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g<1> will'),\n    (r'(\\w+)n\\'t', '\\g<1> not'),\n    (r'(\\w+)\\'ve', '\\g<1> have'),\n    (r'(\\w+)\\'s', '\\g<1> is'),\n    (r'(\\w+)\\'re', '\\g<1> are'),\n    (r'(\\w+)\\'d', '\\g<1> would'),\n]\n\nclass RegexpReplacer(object):\n    def __init__(self, patterns=replacement_patterns): \n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    def replace(self, text):\n        s = text\n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s) \n        return s\n\nreplacer=RegexpReplacer()\n\nfrom nltk.tokenize import WordPunctTokenizer\ntokenizer = WordPunctTokenizer()\n\nfrom french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\nlemmatizer = FrenchLefffLemmatizer()\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstops=set(stopwords.words('french'))\n\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Because we have some long strings to deal with:\npd.options.display.max_colwidth = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/granddebat')","execution_count":null,"outputs":[]},{"metadata":{"id":"b2sY2Uw5B_Yo"},"cell_type":"markdown","source":"## Import dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"themes = {\n    'LA_FISCALITE_ET_LES_DEPENSES_PUBLIQUES.csv':'La fiscalité et les dépenses publiques',\n    'ORGANISATION_DE_LETAT_ET_DES_SERVICES_PUBLICS.csv':\"Organisation de l'état et des services publics\",\n    'DEMOCRATIE_ET_CITOYENNETE.csv':'Démocratie et citoyenneté',\n    'LA_TRANSITION_ECOLOGIQUE.csv':'La transition écologique'\n}\n\nfilenames = list(themes.keys())\nthemes = list(themes.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepaths = [os.path.join(\"..\", \"input\", \"granddebat\", filename) for filename in filenames]\ncol_date = ['createdAt', 'publishedAt', 'updatedAt']\ndf_list = [pd.read_csv(filepath, low_memory=False,\n                       dtype={'authorZipCode':'str'},\n                       parse_dates=col_date) for filepath in filepaths]","execution_count":null,"outputs":[]},{"metadata":{"id":"H63_Tejb0YNR"},"cell_type":"markdown","source":"We can now import each file, all in one list of dataframes for easier use.\n\nWe pay special attention to data types: ZipCode must be read as strings and date columns as timestamps."},{"metadata":{"id":"DevfhHEF03wF"},"cell_type":"markdown","source":"## Discovering the dataset"},{"metadata":{"id":"gxAvFGrh05yr"},"cell_type":"markdown","source":"The 4 dataframes share some common variables, other columns are questions that are specific to the theme. The common variables are the following:\n"},{"metadata":{"id":"jGeTWkj50bvL","outputId":"8c960c9b-10c1-4268-bb8d-7d29951eee5a","trusted":true},"cell_type":"code","source":"col_common = set.intersection(*[set(df.columns) for df in df_list])\ncol_common","execution_count":null,"outputs":[]},{"metadata":{"id":"QWkdmTur1Ad-"},"cell_type":"markdown","source":"Let's have a look at the missing values\n"},{"metadata":{"id":"L5BmybEN07eZ","outputId":"b1b287ce-5909-468b-ca5d-6f653386a0b9","trusted":true},"cell_type":"code","source":"pd.concat([df[df.columns.intersection(col_common)] for df in df_list]).isnull().mean() * 100\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tf08hTEH1HHW"},"cell_type":"markdown","source":"Each line of the dataframes corresponds to one contribution: the answers of an author to the questions of the corresponding theme. Let's see how many contributions we have for each dataset, and how many questions:\n"},{"metadata":{"id":"B04EfmWx1EwQ","outputId":"b887e48e-e671-4ce5-925f-602a78541340","trusted":true},"cell_type":"code","source":"df_infos = pd.DataFrame({\n     'theme': themes,\n     'nb_contributions': [df.shape[0] for df in df_list],\n     'nb_questions': [sum(~df.columns.isin(col_common)) for df in df_list]\n    })\ndf_infos","execution_count":null,"outputs":[]},{"metadata":{"id":"BDN5miNa1OHj"},"cell_type":"markdown","source":"##### We can ask ourself when were the contributions submitted?"},{"metadata":{"id":"Kk8_yhha1Po4"},"cell_type":"markdown","source":"We will have a look at the createdAt variable to spot when the contributions were submitted, and at what time of the day.\n"},{"metadata":{"id":"Tt9sq7Wu1S5H"},"cell_type":"markdown","source":"## Daily contributions\n"},{"metadata":{"id":"ZDmliEwu1I5w","outputId":"af8eb41b-ed40-4c41-cc3a-52252ec9cce4","trusted":true},"cell_type":"code","source":"day_contrib = pd.concat([df.createdAt for df in df_list]).dt.date.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nday_contrib.plot()\nax.set_title('Daily contributions')\nax.set_xlabel('Date')\nfig.autofmt_xdate()\nax.set_ylim(bottom=0)\nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"Xe5PcxL41c6e"},"cell_type":"markdown","source":"We can see a first peak at the very beginning of the Grand Débat.\n\nLet's look also at the time the contributions were made:"},{"metadata":{"id":"w2DakOlf1Vn5","outputId":"ee9d1426-a54d-4118-bca6-1e5c1855f728","trusted":true},"cell_type":"code","source":"hour_contrib = pd.concat([df.createdAt for df in df_list]).dt.hour.value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize = (18,6))\nhour_contrib.plot()\nax.set_title('Hourly contributions')\nax.set_xlabel('Hour')\nax.set_ylim(bottom=0)\nplt.show(fig)","execution_count":null,"outputs":[]},{"metadata":{"id":"wERJEb6-1le5"},"cell_type":"markdown","source":"The number of contribution per hour reaches a peak in the late afternoon, between 18h and 19h"},{"metadata":{"id":"sYGvXM5B1oGF"},"cell_type":"markdown","source":"### Who are the contributors?"},{"metadata":{"id":"9GuDT1OZ1fMT"},"cell_type":"markdown","source":"In this section we will have a closer look at the authors of the Grand Débat. For each contribution we have an authorID that is shared among datasets.\n"},{"metadata":{"id":"U9elvBsn1ri8","outputId":"634228c0-d0b4-4c0e-807b-25f7bfb352a8","trusted":true},"cell_type":"code","source":"pd.DataFrame({'theme':themes,\n              'max_contrib_per_author':[df.groupby('authorId').size().max() for df in df_list]})","execution_count":null,"outputs":[]},{"metadata":{"id":"qxdfjpbJ2DMQ"},"cell_type":"markdown","source":"Since we focus on contributors, we aggregate the table by authorId in order to have one line per author. If an author has several authorType or authorZipCode, we keep the most frequent one: the mode.\n\nWe also add a count statistics: how many contributions that author made over the whole dataset."},{"metadata":{"id":"h4HdN7FN107u","trusted":true},"cell_type":"code","source":"def mode_na(x): \n    m = pd.Series.mode(x)\n    return m.values[0] if not m.empty else np.nan\n\nauthors = pd.concat([df[df.columns.intersection(col_common)] for df in df_list])\n# With pandas>=0.24, we would use: pandas.Series.mode\nauthors = authors.groupby('authorId').agg({'id':'count', # number of contributions\n                                           'authorType':mode_na,\n                                           'authorZipCode':mode_na})","execution_count":null,"outputs":[]},{"metadata":{"id":"Lh1XJEKi2X8T"},"cell_type":"markdown","source":"The first statistics we can get out of this new dataframe is the number of distinct contributors"},{"metadata":{"id":"j4rvZZ-52Fpc","outputId":"2bb3b99b-5c4a-4661-e4ff-53065972126d","trusted":true},"cell_type":"code","source":"authors.shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"OEGgQk7N2en-"},"cell_type":"markdown","source":"There are more than 150,000 distinct contributors."},{"metadata":{"id":"mQyzuEdA2aYk","outputId":"83db1527-0ea9-43b4-cc21-2df7e48603fe","trusted":true},"cell_type":"code","source":"n_contrib = authors.id.value_counts().reset_index(name='counts')\nn_contrib.loc[n_contrib['index'] > 4, 'index'] = '>4'\nn_contrib = n_contrib.groupby('index').agg(sum)\nfig, ax = plt.subplots(figsize=(18,6))\nax = sns.barplot(x='index',\n            y='counts',\n            data=n_contrib.reset_index(),\n            palette=sns.color_palette('Blues'))\nax.set_xlabel('Number of contributions')\nax.set_title('Authors per number of contributions')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qM0VyGGC2jSs"},"cell_type":"markdown","source":"As can be seen, around 50% of the authors submitted a single contribution."},{"metadata":{"id":"3uj_1q-Z2dFn","outputId":"cf3d4a7e-f730-47fd-80cd-c6dfeefeb868","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,6))\nax = sns.countplot(x='authorType',\n                   data=authors,\n                   palette=sns.color_palette('Blues'))\nax.set_yscale('log')\nax.set_title('Author types')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"k5bkiwjS2qwb"},"cell_type":"markdown","source":"We notice that the great majority of respondents, are citizens  i.e. they are neither politicals, officials nor part of an organisation.\n\nAfter this very brief dataset analysis, it is time to focus on the variables of interest: the questions. Each dataframe contains several questions, but we will try to treat them all at once.\n\nThe column names for the questions are a bit messy, we will rename them for more clarity. We build a dataframe containing information about each question: old and new name, title, and the theme and dataframe they are linked to.\n"},{"metadata":{"id":"6_tVS65w2mKn","trusted":true},"cell_type":"code","source":"questions = pd.concat([pd.DataFrame({'old_name':df_list[i].columns,\n                                     'df_id':i,\n                                     'theme':themes[i]}) for i in range(len(df_list))])\nquestions = questions[-questions[\"old_name\"].isin(col_common)].reset_index(drop=True)\nquestions = questions.assign(new_name=(pd.Series(\n    ['Q{}'.format(i) for i in range(1, questions.shape[0] + 1)])))\nquestions = questions.assign(question=pd.Series(\n    [name.split(' - ')[1] for name in questions.old_name]))","execution_count":null,"outputs":[]},{"metadata":{"id":"qFzqYjPD2poS","outputId":"a762e9f3-52f2-4af1-e36b-739b2f13b507","trusted":true},"cell_type":"code","source":"# Questions rename\ndict_rename = {old:new for old, new in zip(questions.old_name,questions.new_name)}\nfor df in df_list:\n    df.rename(columns=dict_rename,inplace=True)\n    \nquestions.head()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tcCJem8z2udY","outputId":"1d0f791e-e09b-4e04-d0f4-7441158c37d1","trusted":true},"cell_type":"code","source":"questions.shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KrfBJbQe23DM"},"cell_type":"markdown","source":"We can see that the dataset concatened contains 94 questions.\n\nWe created for ourself a csv to a better understanding of all the questions."},{"metadata":{"id":"QcQPtzHs2yop","trusted":true},"cell_type":"code","source":"fichier_csv = questions.to_csv(r\"questions.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"gpOriIrH27ki"},"cell_type":"markdown","source":"For each question, we compute the following statistics:\n\n- nbrow: number of rows (i.e. number of contributions for the corresponding theme)\n- nbnnull: number of answers that are not null (answer is null if the contributor skipped that question)\n- nbunique: number of distinct answers\n- nnull_rate: nbnnull/nbrow * 100\n- unique_rate: nbunique/nbnnull * 100"},{"metadata":{"id":"kpKM93p825BB","trusted":true},"cell_type":"code","source":"questions['nbrow'] = questions.apply(lambda g: df_list[g.df_id].shape[0], axis=1)\nquestions['nbnnull'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                       .notnull().sum(), axis=1)\nquestions['nbunique'] = questions.apply(lambda g: df_list[g.df_id].loc[:,g.new_name]\\\n                                        .nunique(), axis=1)\n\nquestions['nnull_rate'] = questions.nbnnull/questions.nbrow * 100\nquestions['unique_rate'] = questions.nbunique/questions.nbnnull * 100","execution_count":null,"outputs":[]},{"metadata":{"id":"hZvFWtL53CPe"},"cell_type":"markdown","source":"We can notice that some questions have very few distinct answers:"},{"metadata":{"id":"fqYq7OHZ2-P0","outputId":"57d4ef0b-d3b9-49d4-ae25-aef413cc6792","trusted":true},"cell_type":"code","source":"questions['closed'] = questions['nbunique'] <= 3\nsum(questions.closed)","execution_count":null,"outputs":[]},{"metadata":{"id":"SKhANBAL3G5J"},"cell_type":"markdown","source":"Those 19 questions are closed-ended question: the answer is forced into a few choices, mainly Yes or No.\n\nWe can now aggregate at the theme scale:"},{"metadata":{"id":"yYQgZzJh3EYg","outputId":"fbd4f2f4-f609-4bdd-c69f-52ba2ad915d7","trusted":true},"cell_type":"code","source":"questions.groupby(['theme']).agg({'question':'count', 'closed':'sum',\n                                  'nbrow':'mean', 'nnull_rate':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"id":"vUBqz1eW3KxW"},"cell_type":"markdown","source":"we see that there are lot of null values, we want to understand that. Let's see which questions have the most null values:"},{"metadata":{"id":"ObMa9XKG3IGB","outputId":"d734f46b-8c81-4400-e20a-2f889a2e0e47","trusted":true},"cell_type":"code","source":"questions.sort_values('nnull_rate').head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"PCRLM60s3Tpz"},"cell_type":"markdown","source":"We can see that all of those questions start with \"Si\". They are conditional: an answer is not necessarily expected.\nThat's eplain why.\n\nIf we pay attention we can notice that the unique_rate is also very low, this is because a lot of contributors answered \"non concerné\" (\"not applicable\"), for instance with question Q40:"},{"metadata":{"id":"sBhK82vc3J-w","outputId":"f98b9c8d-44f0-4415-9d83-6ffecdaec555","trusted":true},"cell_type":"code","source":"df_list[1].Q40.value_counts().head(30)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Los0C6lI3hPC"},"cell_type":"markdown","source":"Some other questions have low unique_rate because they are guided question: choices were given but the respondant could decide to answer something else. This is the case for instance for questions Q91, Q79 and Q4:"},{"metadata":{"id":"Dg1UADo93hGC","outputId":"d3eecd67-ff03-4b86-a527-88ed6144a0db","trusted":true},"cell_type":"code","source":"df_list[3].Q91.value_counts().head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"gKhgRAU43k-U"},"cell_type":"markdown","source":"### Closed questions analysis"},{"metadata":{"id":"1V2mAuIl3nO4"},"cell_type":"markdown","source":"\nFor each of the 19 close questions, we plot the count of each answer in order to identify most popular opinions.\n\nWe use the seaborn library for plotting."},{"metadata":{"id":"nl0RHdRd3VZR","trusted":true},"cell_type":"code","source":"def add_frequencies(ax, ncount):\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.1f} %'.format(100.*y/ncount), (x.mean(), y), \n                ha='center', va='bottom', size='small', color='black', weight='bold')","execution_count":null,"outputs":[]},{"metadata":{"id":"rpe5vQ8K3pIU","trusted":true},"cell_type":"code","source":"# Countplot of questions_df\ndef countplot_qdf(questions_df, suptitle):\n    n = questions_df.shape[0]\n    \n    # If there is nothing to plot, we stop here\n    if n==0:\n        return\n    \n    # Numbers of rows and cols in the subplots\n    ncols = 3\n    nrows = (n+3)//ncols\n    fig,ax = plt.subplots(nrows, ncols, figsize=(25,6*nrows))\n    fig.tight_layout(pad=9, w_pad=10, h_pad=7)\n    fig.suptitle(suptitle, size=30, fontweight='bold')\n    \n    # Hide exceeding subplots\n    for i in range(n, ncols*nrows):\n        ax.flatten()[i].axis('off')\n        \n    # Countplot for each question\n    for index, row in questions_df.iterrows():\n        plt.sca(ax.flatten()[index])\n        # We add the sort_values argument to always have the same order: Oui, Non...\n        xlabels = df_list[row.df_id].loc[:,row.new_name]\n        xlabels = xlabels.value_counts().index.sort_values(ascending=False)\n        axi = sns.countplot(x=row.new_name,\n                           data=df_list[row.df_id],\n                           order = xlabels)\n        # Wrap long questions into lines\n        axi.set_title(\"\\n\".join(wrap(row.new_name + '. ' + row.question, 60)))\n        axi.set_xlabel('')\n        # We also set a wrap here (for one very long answer...)\n        axi.set_xticklabels([\"\\n\".join(wrap(s, 17)) for s in xlabels])\n        axi.set_ylabel('Nombre de réponses')\n        add_frequencies(axi, row.nbnnull)","execution_count":null,"outputs":[]},{"metadata":{"id":"HGrukSwS3r-I","outputId":"46df0f73-68b2-4721-96be-1b391334db97","scrolled":false,"trusted":true},"cell_type":"code","source":"# Plotting questions, grouped by theme\nfor i in range(len(themes)):\n    countplot_qdf(questions[(questions.closed) & (questions.df_id == i)].reset_index(), themes[i])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"beSyUOQT3yA1"},"cell_type":"markdown","source":"On the themes of State organisation, democracy and citizenship: when asked their opinion, contributors always take side for change.\n\nIt's very interesting to see all these opinions.\n\n### Open questions analysis\n\nMost of the information of the dataset lies in the open questions, but they are the most difficult to analysis!\n\nWe can start with with a basic statistic, the number of words contained in the whole dataset."},{"metadata":{"id":"vgUHIj-m3t-X","trusted":true},"cell_type":"code","source":"# Count words in a string, a word being here any sequence of characters between white spaces\ndef count_words(s):\n    if s is np.nan:\n        return(0)\n    return(len(s.split()))","execution_count":null,"outputs":[]},{"metadata":{"id":"DFw-2QOX3zvg","outputId":"289ae2ec-5eca-4336-83ca-7e60bd9f491b","trusted":true},"cell_type":"code","source":"# For each dataframe:\n# filter on questions and title\n# count words for each contribution of each question\n# sum it all\nn_words = [df.filter(regex=r'title|^Q', axis=1).apply(np.vectorize(count_words)).sum().sum()\\\n           for df in df_list]\nsum(n_words)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"D0S-UVwD35oZ"},"cell_type":"markdown","source":"The contributions contain 95 million words!\n\nLet's focus on the 75 open questions. \nWe first remove all stop words, those are the most common words that don't give any insight, and must be filtered out when doing natural language processing."},{"metadata":{"id":"xxuw3H4N310A","trusted":true},"cell_type":"code","source":"stop_words = [unidecode.unidecode(w.lower()) for w in stops]\n# Add punctuation and some missing stopwords using this website : https://www.ranks.nl/stopwords/french\nstop_words = set(stop_words +\n                 list(string.punctuation) +\n                 [\"’\", \"...\", \"'\", \"\", \">>\", \"<<\"] +\n                 [\"oui\", \"non\", \"plus\", \"toute\", \"toutes\", \"faut\",\"à\",\"tous\",\"tandis\",\"quels\",\n                  \"alors\",\"au\",\"aucuns\",\"aussi\",\"autre\",\"avant\",\"avec\",\"avoir\",\"juste\",\"la\",\"tout\",\"toutes\",\"très\",\"trop\",\n\"www\",\"http\",\"html\",\"peu\",\"en\",\"etc\",\"chaque\",\"sans\",\"ne\",\"ils\",\"il\",\"que\",\"quand\",\"quoi\",\"qui\",\"plupart\",\n\"doit\",\"donc\",\"dos\",\"elle\",\"elles\",\"comme\",\"comment\",\"ci\",\"ni\",\"même\",\"mais\",\"mes\",\"aussi\",\"alors\",\"an\",\"je\",\"ça\",\"où\",\"org\",\"moi\"\n                 \n                 ])","execution_count":null,"outputs":[]},{"metadata":{"id":"ZjoI-MCM4H64"},"cell_type":"markdown","source":"The next important step is to run a tokenization, i.e. splitting text into words. This might be tricky because of punctuation, wich is slightly different according to the language. There are some important features we have to take into considreation: punctuation, case, encoding and stop words."},{"metadata":{"id":"3adiijjb3_-R","trusted":true},"cell_type":"code","source":"# Get tokens from list of strings (can probably be optimised)\ndef get_tokens(s):\n    # MosesTokenizer has been moved out of NLTK due to licensing issues\n    # So we define a simple tokenizer based on regex, designed for French language\n    pattern = r\"[cdjlmnstCDJLMNST]['´`]|\\w+|\\$[\\d\\.]+|\\S+\"\n    tokenizer = RegexpTokenizer(pattern)\n    tokens = tokenizer.tokenize(\" \".join(s.dropna()))\n    # remove punctuation (for words like \"j'\")\n    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n    # lowercase ASCII\n    tokens = [unidecode.unidecode(w.lower()) for w in tokens]\n    # remove stop words from tokens\n    tokens = [w for w in tokens if w not in stop_words]\n    return(tokens)","execution_count":null,"outputs":[]},{"metadata":{"id":"jjVJvTjl4PwK"},"cell_type":"markdown","source":"We will use the tokens to draw a word cloud. This is a visual representation of n-gram counts. The more frequent a term is, the bigger it will appear on the plot.\n\nLet's plot a wordcloud for each of the 4 themes. We will see what are the most raised topics among each of them."},{"metadata":{"id":"YthOJzmx4KMf","outputId":"58f33b93-483f-406b-ad43-4224e6cb5c68","scrolled":false,"trusted":true},"cell_type":"code","source":"def plot_wordcloud(s, title, mw = 500):\n    wordcloud = WordCloud(width=1200, height=600, max_words=mw,\n                          background_color=\"white\").generate(\" \".join(s))\n    plt.figure(figsize=(20, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title, fontsize=50, pad=50)\n    plt.show()\n\ncol_q = questions.new_name[~questions.closed].append(pd.Series('title'))\nfor i in range(len(themes)):\n    col_q_i = df_list[i].columns.intersection(col_q)\n    tokens = pd.concat([df_list[i][col].dropna() for col in col_q_i])\n    tokens = get_tokens(tokens)\n    plot_wordcloud(tokens, title = themes[i])","execution_count":null,"outputs":[]},{"metadata":{"id":"3dE1QrzN4VMb"},"cell_type":"markdown","source":"## Let's start with La fiscalité et les dépenses publiques"},{"metadata":{"id":"wf8dtQTH4Uu6","trusted":true},"cell_type":"code","source":"df_list[0] = df_list[0].astype(str) \n\n#contains all the answers of questions 1 to 8\nreponse_question1 = df_list[0].Q1\nreponse_question2 = df_list[0].Q2\nreponse_question3 = df_list[0].Q3\nreponse_question4 = df_list[0].Q4\nreponse_question5 = df_list[0].Q5\nreponse_question6 = df_list[0].Q6\nreponse_question7 = df_list[0].Q7\nreponse_question8 = df_list[0].Q8","execution_count":null,"outputs":[]},{"metadata":{"id":"OfTwAEY45Hl8"},"cell_type":"markdown","source":"### Let's have a look at the different possible answer"},{"metadata":{"id":"dbrTjEQW4fWF","outputId":"b214dae3-986e-4421-c345-3cccaee91058","trusted":true},"cell_type":"code","source":"reponse_question1.value_counts().head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Yn0HsDSE4flI","outputId":"2bf592a0-84ab-41fd-85e0-2788c9c8ec60","trusted":true},"cell_type":"code","source":"reponse_question2.value_counts().head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"LsBbgYOU5RFp","outputId":"5ad9e723-71b1-4612-d169-02c14aaace75","trusted":true},"cell_type":"code","source":"reponse_question3.value_counts().head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ZvI3V6405Smz","outputId":"2d19b2c8-62d3-49c0-eebb-e0a444a9de11","trusted":true},"cell_type":"code","source":"reponse_question4.value_counts().head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"L8Tw2T5U5XTk"},"cell_type":"markdown","source":"# Pretreatments"},{"metadata":{"id":"PEeH8Rt65jw-","outputId":"52571558-b823-41c6-badd-2d33f6fcb65e","trusted":true},"cell_type":"code","source":"nltk.download('punkt')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"RW7UhMYH5T0o","trusted":true},"cell_type":"code","source":"def preprocess_text(test):\n\n  \n\n    #test = test.lower()\n    #Removing Numbers\n    test=re.sub(r'\\d+','',test)\n\n    \n    #Removing white spaces\n    test=test.strip()\n    \n    #Replacer replace\n    text_replaced = replacer.replace(test)\n\n      #Tokenize\n    tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n    sentences = tokenizer.tokenize(text_replaced)\n\n     #Tokenize words\n    tokenizer = WordPunctTokenizer()\n    for i in range(len(sentences)):\n        sentences[i] = tokenizer.tokenize(sentences[i])\n        \n     #Remove stop words\n\n\n    for i in range(len(sentences)):\n        sentences[i] = [word for word in sentences[i] if word not in stop_words]\n\n    for i in range(len(sentences)):\n        for j in range(len(sentences[i])):\n            sentences[i][j] = lemmatizer.lemmatize(sentences[i][j])\n\n\n    #Join the words back into a sentence.\n    a=[' '.join(s) for s in sentences]\n    b=['. '.join(a)]\n\n    return b","execution_count":null,"outputs":[]},{"metadata":{"id":"3tOn2lwV5hww","trusted":true},"cell_type":"code","source":"reponse_question1_cleaned = [preprocess_text(doc) for doc in reponse_question1]\nreponse_question1 = [' '.join(r) for r in reponse_question1_cleaned]\n\n\nreponse_question2_cleaned = [preprocess_text(doc) for doc in reponse_question2]\nreponse_question2 = [' '.join(r) for r in reponse_question2_cleaned]\n\n\nreponse_question3_cleaned = [preprocess_text(doc) for doc in reponse_question3]\nreponse_question3 = [' '.join(r) for r in reponse_question3_cleaned]\n\n\nreponse_question4_cleaned = [preprocess_text(doc) for doc in reponse_question4]\nreponse_question4 = [' '.join(r) for r in reponse_question4_cleaned]\n\nreponse_question5_cleaned = [preprocess_text(doc) for doc in reponse_question5]\nreponse_question5 = [' '.join(r) for r in reponse_question5_cleaned]\n\n\nreponse_question6_cleaned = [preprocess_text(doc) for doc in reponse_question6]\nreponse_question6 = [' '.join(r) for r in reponse_question6_cleaned]\n\n\nreponse_question7_cleaned = [preprocess_text(doc) for doc in reponse_question7]\nreponse_question7 = [' '.join(r) for r in reponse_question7_cleaned]\n\n\nreponse_question8_cleaned = [preprocess_text(doc) for doc in reponse_question8]\nreponse_question8 = [' '.join(r) for r in reponse_question8_cleaned]","execution_count":null,"outputs":[]},{"metadata":{"id":"_yld3M7xnOVR","outputId":"be01f9f0-71ce-4805-b4f6-617762fd8621","trusted":true},"cell_type":"code","source":"!pip install pyLdavis","execution_count":null,"outputs":[]},{"metadata":{"id":"7sPmy_F_5mkM","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None \nimport numpy as np\nimport re\nimport nltk\nfrom pprint import pprint\n\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nSTOP_WORDS = nltk.corpus.stopwords.words()\n\nfrom gensim.utils import simple_preprocess\nfrom smart_open import smart_open\nimport pyLDAvis.gensim # To visualise LDA model effectively\n\nimport os\nfrom collections import defaultdict # For accumlating values\nfrom nltk.corpus import stopwords # To remove stopwords\nfrom gensim import corpora # To create corpus and dictionary for the LDA model\nfrom gensim.models import LdaModel # To use the LDA model","execution_count":null,"outputs":[]},{"metadata":{"id":"QzQQpgeG7SZ2"},"cell_type":"markdown","source":"# Unsupervised analysis"},{"metadata":{"id":"QEBGRDgNTe58"},"cell_type":"markdown","source":"# Let's start with La fiscalité et les dépenses publiques"},{"metadata":{"id":"sQ4XTmPH7Aro","outputId":"45581d22-0703-4522-e215-55988a7b0a2b","trusted":true},"cell_type":"code","source":"reponse1 = pd.DataFrame(reponse_question1)\nreponse1.columns = ['Question1_Quelles sont toutes les choses qui pourraient être faites pour améliorer information des citoyens sur utilisation des impôts ?']\nreponse1 = reponse1[reponse1['Question1_Quelles sont toutes les choses qui pourraient être faites pour améliorer information des citoyens sur utilisation des impôts ?']!= 'nan']\nreponse1.head()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"cM59ohxq7Emm","outputId":"bc3805ce-bddc-46b4-f88c-1c291bcb423d","trusted":true},"cell_type":"code","source":"# Create gensim dictionary form a single tet file\ndictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse1['Question1_Quelles sont toutes les choses qui pourraient être faites pour améliorer information des citoyens sur utilisation des impôts ?'])\n\n# Token to Id map\ndictionary.token2id","execution_count":null,"outputs":[]},{"metadata":{"id":"-1i7W4wh7aIy","trusted":true},"cell_type":"code","source":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse1['Question1_Quelles sont toutes les choses qui pourraient être faites pour améliorer information des citoyens sur utilisation des impôts ?']]\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]","execution_count":null,"outputs":[]},{"metadata":{"id":"dj9xN1_p7qjO","trusted":true},"cell_type":"code","source":"NUM_TOPICS = 10 # This is an assumption. \nldamodel = LdaModel(mycorpus, num_topics = NUM_TOPICS, id2word=mydict, passes=15)#This might take some time.","execution_count":null,"outputs":[]},{"metadata":{"id":"kcFVHlL7_8MF","outputId":"88d898d3-04e7-429e-a8f7-22717b3f93d5","trusted":true},"cell_type":"code","source":"topics = ldamodel.show_topics()\nfor topic in topics:\n    print(topic)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"DuAhjlPGHDhR","outputId":"14506ba8-28d8-4845-d3cd-4d08f092e630","trusted":true},"cell_type":"code","source":"word_dict = {};\nfor i in range(NUM_TOPICS):\n    words = ldamodel.show_topic(i, topn = 15)\n    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\npd.DataFrame(word_dict)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"j0wYI2XuHFAG","outputId":"c5fe3a97-9be0-43e9-f424-3c426ba55728","trusted":true},"cell_type":"code","source":"lda_display = pyLDAvis.gensim.prepare(ldamodel, mycorpus, mydict, sort_topics=False)\npyLDAvis.display(lda_display)","execution_count":null,"outputs":[]},{"metadata":{"id":"UR3oFbgJOB1c"},"cell_type":"markdown","source":"## Interpretation \n\nAvant de commenter notre tableau, on peut voir avec un value_counts() qu'une des choses qui pourraient être faites pour améliorer information des citoyens sur utilisation des impôts est bien la transparence. Ce qui resort sont les médias ou des sites publiques dédiés à l’utilisation des impôts bien expliquer avec des informations concrètes seraient bénéfiques. Il faut faire des debats, partager l'information via la tv, les journaux et des emissions. Mais surtout etre transparent, il faudrait des forme simples. On peut voir également une sorte de plainte au niveauu des avantages des salaires de haut fonctionnaires travaillant dans la politique."},{"metadata":{"id":"uyVEQMBpHO8x"},"cell_type":"markdown","source":"## Let's analyze question 2 : Que faudrait-il faire pour rendre la fiscalité plus juste et plus efficace ?"},{"metadata":{"id":"nctHXuKUHQPC","outputId":"02367fc4-b718-4a45-c844-874971113d09","trusted":true},"cell_type":"code","source":"reponse2 = pd.DataFrame(reponse_question2)\nreponse2.columns = ['Question_2_Que faudrait-il faire pour rendre la fiscalité plus juste et plus efficace ?']\nreponse2 = reponse2[reponse2['Question_2_Que faudrait-il faire pour rendre la fiscalité plus juste et plus efficace ?']!= 'nan']\n\nreponse2.head()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"OJfXux0yHRev","outputId":"2cc31fdb-0ee6-44b0-b8a8-1e7cefa63369","trusted":true},"cell_type":"code","source":"# Create gensim dictionary form a single tet file\ndictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse2['Question_2_Que faudrait-il faire pour rendre la fiscalité plus juste et plus efficace ?'])\n\n# Token to Id map\ndictionary.token2id","execution_count":null,"outputs":[]},{"metadata":{"id":"CbMWaDuGHca5","trusted":true},"cell_type":"code","source":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse2['Question_2_Que faudrait-il faire pour rendre la fiscalité plus juste et plus efficace ?']]\n\n# Create the Corpus\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]","execution_count":null,"outputs":[]},{"metadata":{"id":"a2MpwDHsHgLa","trusted":true},"cell_type":"code","source":"NUM_TOPICS = 10 # This is an assumption. \nldamodel = LdaModel(mycorpus, num_topics = NUM_TOPICS, id2word=mydict, passes=15)#This might take some time.","execution_count":null,"outputs":[]},{"metadata":{"id":"0NjONVtnHkrM","outputId":"cc4e4b82-f1ad-41dc-8eb5-c531487cae09","trusted":true},"cell_type":"code","source":"topics = ldamodel.show_topics()\nfor topic in topics:\n    print(topic)","execution_count":null,"outputs":[]},{"metadata":{"id":"XZGpDgToJhzu","outputId":"27ed2e70-29a5-4d14-eca6-1c54ec8475c5","trusted":true},"cell_type":"code","source":"word_dict = {};\nfor i in range(NUM_TOPICS):\n    words = ldamodel.show_topic(i, topn = 15)\n    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\npd.DataFrame(word_dict)","execution_count":null,"outputs":[]},{"metadata":{"id":"Tpq0sJEkQB9k"},"cell_type":"markdown","source":"## Interprétation\nIl faudrait diminuer le prix du carburant, faire attention à l'évasion fiscale.S upprimer la CSG pour tous les retraités ainsi que la taxe d'habitation. Il faudrait également selon eux rétablir l'ISF.On peut voir qu'il faut supprimer la CSG. Il faut faire attention à l'évasion fiscale également. Il faudrait également supprimer les niches fiscales et réduire les avantages des hauts fonctionnaires."},{"metadata":{"id":"Wv2qRx8_Jyu3"},"cell_type":"markdown","source":"##  Let's focus on question 3 :  Quels sont selon vous les impôts qu'il faut baisser en priorité ?"},{"metadata":{"id":"orIKl6QgJjFs","trusted":true},"cell_type":"code","source":"reponse3 = pd.DataFrame(reponse_question3)\nreponse3.columns = ['Question_3_Quels sont selon vous les impôts qui faut baisser en priorité ?']\nreponse3 = reponse3[reponse3['Question_3_Quels sont selon vous les impôts qui faut baisser en priorité ?']!= 'nan']","execution_count":null,"outputs":[]},{"metadata":{"id":"upxfEKQcJ4jO","outputId":"9d79565d-62a5-409f-939a-e37cc65a62d8","trusted":true},"cell_type":"code","source":"# Create gensim dictionary form a single tet file\ndictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse3['Question_3_Quels sont selon vous les impôts qui faut baisser en priorité ?'])\n\n# Token to Id map\ndictionary.token2id","execution_count":null,"outputs":[]},{"metadata":{"id":"Ll4arwzgJ7am","trusted":true},"cell_type":"code","source":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse3['Question_3_Quels sont selon vous les impôts qui faut baisser en priorité ?']]\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Gd3P2r1hKUcx","trusted":true},"cell_type":"code","source":"NUM_TOPICS = 10 # This is an assumption. \nldamodel = LdaModel(mycorpus, num_topics = NUM_TOPICS, id2word=mydict, passes=15)#This might take some time.","execution_count":null,"outputs":[]},{"metadata":{"id":"XKlAyjIeKe3t","outputId":"5825738d-eec4-4b00-f469-33559396254d","trusted":true},"cell_type":"code","source":"topics = ldamodel.show_topics()\nfor topic in topics:\n    print(topic)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"PnG84dSVKryH","outputId":"72543699-87a5-4a41-dc27-5ab75e963fce","trusted":true},"cell_type":"code","source":"word_dict = {};\nfor i in range(NUM_TOPICS):\n    words = ldamodel.show_topic(i, topn = 15)\n    word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\npd.DataFrame(word_dict)","execution_count":null,"outputs":[]},{"metadata":{"id":"30fBnYZeRacz"},"cell_type":"markdown","source":"## Interprétation\n\nLes impôts qui faut baisser en priorité sont la CSG des retraités, la TVA, ISF mais également l'impot sur le revenu. Les charges sociales pour les PME ainsi que la taxe habitation et la taxe fonciere. Il est vraiment tres interessant de voir ce que pense les français à travers cette analyse."},{"metadata":{"id":"Q45fThByKz6P"},"cell_type":"markdown","source":"##  Let's focus on question 6 :  Quels sont les domaines prioritaires où notre protection sociale doit être renforcée ?"},{"metadata":{"id":"ntNc0Zt2KvK0","outputId":"17b14dbd-8b6f-4b65-d4be-1867441b617a","trusted":true},"cell_type":"code","source":"reponse6 = pd.DataFrame(reponse_question6)\nreponse6.columns = ['Question6_Quels sont les domaines prioritaires où notre protection sociale doit être renforcée ?']\nreponse6 = reponse6[reponse6['Question6_Quels sont les domaines prioritaires où notre protection sociale doit être renforcée ?']!= 'nan']\nreponse6.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"t7fQpgkwK2to","outputId":"c40ccbc9-2651-4b2b-c729-fd827410ac3b","trusted":true},"cell_type":"code","source":"dictionary= corpora.Dictionary(simple_preprocess(line, deacc=True) for line in reponse6['Question6_Quels sont les domaines prioritaires où notre protection sociale doit être renforcée ?'])\n\n# Token to Id map\ndictionary.token2id","execution_count":null,"outputs":[]},{"metadata":{"id":"3Rbb_tFxK9OZ","trusted":true},"cell_type":"code","source":"# Tokenize the docs\ntokenized_list = [simple_preprocess(doc) for doc in reponse6['Question6_Quels sont les domaines prioritaires où notre protection sociale doit être renforcée ?']]\nmydict = corpora.Dictionary()\nmycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]","execution_count":null,"outputs":[]},{"metadata":{"id":"pQsbbXZ8LMnR"},"cell_type":"markdown","source":"## Interprétation \nLes domaines prioritaires où notre protection sociale doit être renforcée sont sans aucun doute la santé et l'éducation qui ressort beaucoup de ce tableau. Au niveau de tout ce qui est medicale( hôpital, médecin, etc.). Ainsi qu'au niveau des soins et de la prise en charge avec notamment le remboursement. (mutuelle, médicament, etc.)Aider les personnes âgées et handicapées. Mais aussi les allocations pour les familles en difficultés. Il faudrait aussi mettre en place des formations. Les français insiste sur l'assurance chomage."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}