{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(\"[INFO] loading input data...\")\nf = open(\"/kaggle/input/facial-expression/fer2013.csv\")\nf.__next__() # f.next() for Python 2.7\n(trainImages, trainLabels) = ([], [])\n(valImages, valLabels) = ([], [])\n(testImages, testLabels) = ([], [])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in f:\n    (label,image,usage) = row.strip().split(\",\")\n    label=int(label)\n    image = np.array(image.split(\" \"), dtype=\"uint8\")\n    image = image.reshape((48, 48))\n    \n    if usage == 'Training':\n        trainImages.append(image)\n        trainLabels.append(label)\n    elif usage == 'PrivateTest':\n        valImages.append(image)\n        valLabels.append(label)\n    else:\n        testImages.append(image)\n        testLabels.append(label)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = [\n(trainImages, trainLabels, 'train'),\n(valImages, valLabels, 'validation'),\n(testImages, testLabels, 'test')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile hdf5datasetwriter.py\nimport h5py\nimport os\n\nclass HDF5DatasetWriter:\n    def __init__(self, dims, outputPath, dataKey=\"images\",\n    bufSize=500):\n        # check to see if the output path exists, and if so, raise\n        # an exception\n        if os.path.exists(outputPath):\n            raise ValueError(\"The supplied ‘outputPath‘ already \"\n            \"exists and cannot be overwritten.Manually delete \"\n            \"the file before continuing.\", outputPath)\n        self.db = h5py.File(outputPath, \"w\")\n        self.data = self.db.create_dataset(dataKey, dims,\n                                           dtype=\"float\",compression='gzip',compression_opts=9)\n        self.labels = self.db.create_dataset(\"labels\", (dims[0],),\n                                             dtype=\"int\",compression='gzip',compression_opts=9)\n        self.bufSize = bufSize\n        self.buffer = {\"data\": [], \"labels\": []}\n        self.idx = 0\n\n    def add(self, rows, labels):\n\n        # add the rows and labels to the buffer\n        self.buffer[\"data\"].extend(rows)\n        self.buffer[\"labels\"].extend(labels)\n        if len(self.buffer[\"data\"]) >= self.bufSize:\n            self.flush()\n\n    def flush(self):\n\n        # write the buffers to disk then reset the buffer\n        i = self.idx + len(self.buffer[\"data\"])\n        self.data[self.idx:i] = self.buffer[\"data\"]\n        self.labels[self.idx:i] = self.buffer[\"labels\"]\n        self.idx = i\n        self.buffer = {\"data\": [], \"labels\": []}\n\n    def storeClassLabels(self, classLabels):\n\n        # create a dataset to store the actual class label names,\n        # then store the class labels\n        dt = h5py.special_dtype(vlen=str)\n        labelSet = self.db.create_dataset(\"label_names\",\n                                          (len(classLabels),), dtype=dt)\n        labelSet[:] = classLabels\n\n    def close(self):\n\n        # check to see if there are any other entries in the buffer\n        # that need to be flushed to disk\n        if len(self.buffer[\"data\"]) > 0:\n            self.flush()\n\n        # close the dataset\n        self.db.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hdf5datasetwriter import HDF5DatasetWriter\nfrom tqdm import tqdm\n\nfor (images,labels,outputPath) in tqdm(datasets):\n    writer = HDF5DatasetWriter((len(images), 48, 48), outputPath)\n    for image,label in zip(images,labels):\n        writer.add([image],[label])\n    \n    writer.close()\n\nf.close()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D,MaxPooling2D\nfrom keras.layers.advanced_activations import ELU\nfrom keras.layers.core import Activation,Flatten,Dropout,Dense\nfrom keras import backend as K\nheight=48\nwidth=48\ndepth=1\nmodel=Sequential()\ninputShape = (height, width, depth)\nchanDim = -1\n\nmodel.add(Conv2D(32, (3, 3), padding=\"same\",kernel_initializer=\"he_normal\", input_shape=inputShape))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(32, (3, 3), kernel_initializer=\"he_normal\",\npadding=\"same\"))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\",\npadding=\"same\"))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\",\npadding=\"same\"))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(128, (3, 3), kernel_initializer=\"he_normal\",\npadding=\"same\"))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(Conv2D(128, (3, 3), kernel_initializer=\"he_normal\",\npadding=\"same\"))\nmodel.add(ELU())\nmodel.add(BatchNormalization(axis=chanDim))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(64, kernel_initializer=\"he_normal\"))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, kernel_initializer=\"he_normal\"))\nmodel.add(Activation('relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(7, kernel_initializer=\"he_normal\"))\nmodel.add(Activation(\"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile hdf5datasetgenerator.py\nfrom keras.utils import np_utils\nimport numpy as np\nimport h5py\n\nclass HDF5DatasetGenerator:\n    def __init__(self, dbPath, batchSize, preprocessors=None,\n    aug=None, binarize=True, classes=2):\n        # store the batch size, preprocessors, and data augmentor,\n        # whether or not the labels should be binarized, along with\n        # the total number of classes\n        self.batchSize = batchSize\n        self.preprocessors = preprocessors\n        self.aug = aug\n        self.binarize = binarize\n        self.classes = classes\n\n        # open the HDF5 database for reading and determine the total\n        # number of entries in the database\n        self.db = h5py.File(dbPath)\n        self.numImages = self.db[\"labels\"].shape[0]\n\n    def generator(self, passes=np.inf):\n        # initialize the epoch count\n        epochs = 0\n        while epochs < passes:\n            # loop over the HDF5 dataset\n            for i in np.arange(0, self.numImages, self.batchSize):\n                # extract the images and labels from the HDF dataset\n                images = self.db[\"images\"][i: i + self.batchSize]\n                labels = self.db[\"labels\"][i: i + self.batchSize]\n                if self.binarize:\n                    labels = np_utils.to_categorical(labels,\n                                                     self.classes)\n\n                # check to see if our preprocessors are not None\n                if self.preprocessors is not None:\n                    # initialize the list of processed images\n                    procImages = []\n\n                    for image in images:\n                        # loop over the preprocessors and apply each\n                        # to the image\n                        for p in self.preprocessors:\n                            image = p.preprocess(image)\n                        procImages.append(image)\n                    images = np.array(procImages)\n\n                if self.aug is not None:\n                    (images, labels) = next(self.aug.flow(images,\n                                                          labels, batch_size=self.batchSize))\n                yield (images, labels)\n\n            epochs += 1\n\n    def close(self):\n        # close the datab\n        self.db.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile imagetoarraypreprocessor.py\nfrom keras.preprocessing.image import img_to_array\n\nclass ImageToArrayPreprocessor:\n    def __init__(self, dataFormat=None):\n        # store the image data format\n        self.dataFormat = dataFormat\n\n    def preprocess(self, image):\n        # apply the Keras utility function that correctly rearranges\n        # the dimensions of the image\n        return img_to_array(image, data_format=self.dataFormat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hdf5datasetgenerator import HDF5DatasetGenerator\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom imagetoarraypreprocessor import ImageToArrayPreprocessor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainAug = ImageDataGenerator(rotation_range=10, zoom_range=0.1,\nhorizontal_flip=True, rescale=1 / 255.0, fill_mode=\"nearest\")\nvalAug = ImageDataGenerator(rescale=1 / 255.0)\niap = ImageToArrayPreprocessor()\n\ntrainGen = HDF5DatasetGenerator('/kaggle/working/train', 128,\naug=trainAug,preprocessors=[iap],  classes=7)\nvalGen = HDF5DatasetGenerator('/kaggle/working/validation', 128,\naug=valAug,preprocessors=[iap], classes=7)\n\n\nopt = Adam(lr=1e-3)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\nmetrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit_generator(\ntrainGen.generator(),\nsteps_per_epoch=trainGen.numImages // 128,\nvalidation_data=valGen.generator(),\nvalidation_steps=valGen.numImages // 128,\nepochs=50,verbose=1)\n\n\ntrainGen.close()\nvalGen.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.hdf5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}