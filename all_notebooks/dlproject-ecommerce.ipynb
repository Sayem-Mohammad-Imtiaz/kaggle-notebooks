{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport numpy as np\nimport pandas as pd\nfrom numpy import nan,isnan,loadtxt\nfrom pandas import read_csv, DataFrame, concat\nfrom numpy import array, concatenate, hstack\nfrom matplotlib import pyplot as plt\nimport pickle\nimport os\nfrom keras.models import model_from_json\nimport time\n# multivariate multi-step encoder-decoder lstm\nfrom math import sqrt,cos,radians,sin,atan2\nfrom numpy import split\nfrom numpy import array\nfrom pandas import read_csv\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom matplotlib import pyplot\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.models import model_from_json #Modeli kaydetmek iÃ§in\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport statsmodels.api as sm\nfrom math import *","metadata":{"_uuid":"b5bd3eb6-5d7e-4584-9e8a-77fdc12845cb","_cell_guid":"97a92cdc-8f64-441f-91f2-4234ce5456ef","_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-09T17:32:15.714859Z","iopub.execute_input":"2021-06-09T17:32:15.715214Z","iopub.status.idle":"2021-06-09T17:32:15.735707Z","shell.execute_reply.started":"2021-06-09T17:32:15.715179Z","shell.execute_reply":"2021-06-09T17:32:15.734609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set()\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport statsmodels.api as sm\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\n# import the_module_that_warns\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom fbprophet import Prophet\n\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD,Adadelta,Adam,RMSprop \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout","metadata":{"_uuid":"57c58f51-1e36-4c43-8db0-0a4e84188000","_cell_guid":"b23b7e00-d2bb-4a81-9f37-c12a133749c2","execution":{"iopub.status.busy":"2021-06-09T17:32:15.774571Z","iopub.execute_input":"2021-06-09T17:32:15.774902Z","iopub.status.idle":"2021-06-09T17:32:15.791043Z","shell.execute_reply.started":"2021-06-09T17:32:15.774873Z","shell.execute_reply":"2021-06-09T17:32:15.790041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#date,number,name,rating,n_reviews,price","metadata":{"_uuid":"05a242af-a0d7-42df-b637-2532587f8a9c","_cell_guid":"37a98359-9575-499c-8a2c-46fe6f22a724","trusted":true}},{"cell_type":"code","source":"# prepare data for lstm\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:32:15.792544Z","iopub.execute_input":"2021-06-09T17:32:15.79285Z","iopub.status.idle":"2021-06-09T17:32:15.79986Z","shell.execute_reply.started":"2021-06-09T17:32:15.792822Z","shell.execute_reply":"2021-06-09T17:32:15.798827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data = \"../input/amazon-top-100-best-sellers-in-electronics-2021/dataset.csv\"\ndata = \"../input/dlproject/dataset.csv\"","metadata":{"_uuid":"c088e128-f37c-408f-b2b4-0bc070e39767","_cell_guid":"fa8e649b-0430-4916-9d24-114e0efdc293","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:15.837447Z","iopub.execute_input":"2021-06-09T17:32:15.837791Z","iopub.status.idle":"2021-06-09T17:32:15.841399Z","shell.execute_reply.started":"2021-06-09T17:32:15.837759Z","shell.execute_reply":"2021-06-09T17:32:15.840474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Context**\n> Amazon.com, Inc. is an American multinational technology company based in Seattle, Washington, which focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence.\n> This dataset contains daily data on the top 100 most popular products based on sales. It will be updated on a weekly basis. The data in this dataset was extracted from Amazon Best Sellers page: https://www.amazon.com/Best-Sellers-Electronics/zgbs/electronics/\n> ","metadata":{}},{"cell_type":"markdown","source":"# **Content**","metadata":{}},{"cell_type":"markdown","source":"As of now, the dataset consists of data for February 2021 and 6 features:","metadata":{}},{"cell_type":"markdown","source":"**Date, Number in rating, Product name, Rating, Number of reviews, Price**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead =100\n#df1 = pd.read_csv(data,usecols=[\"date\",\"number\",\"name\",\"rating\",\"n_reviews\",\"price\"],infer_datetime_format=True,error_bad_lines=False,nrows=nRowsRead)\ndf1 = pd.read_csv(data,usecols=[\"date\",\"number\",\"name\",\"rating\",\"n_reviews\",\"price\"],infer_datetime_format=True,error_bad_lines=False)\n#df.replace('?', nan, inplace=True)\n#df.drop('number', axis=1, inplace=True)\n#Rename the columns\ndf1.columns = ['date','numberInRating','productName','rating', 'numberOfReviews', 'price']\ndf1","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:32:16.362996Z","iopub.execute_input":"2021-06-09T17:32:16.363531Z","iopub.status.idle":"2021-06-09T17:32:16.400936Z","shell.execute_reply.started":"2021-06-09T17:32:16.363496Z","shell.execute_reply":"2021-06-09T17:32:16.400103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndate = df1.iloc[:,0:1].values\ndate_ = date.reshape((date.shape[0]))\nnumberInRating = df1.iloc[:,1:2].values\nproductName = df1.iloc[:,2:3].values\nrating = df1.iloc[:,3:4].values\nnumberOfReviews = df1.iloc[:,4:5].values\nprice = df1.iloc[:,5:6].values\ndf2 = DataFrame()\ndf2[\"date\"] = date_\ndf2[\"productName\"] = productName\ndf2[\"numberInRating\"] = numberInRating\ndf2[\"rating\"] = rating\ndf2[\"numberOfReviews\"] = numberOfReviews\ndf2[\"price\"] = price\ndf1 = df2\ndf1 = df1.set_index('date')\n#df1.index.name = 'date'\ndf1['numberInRating'].fillna(0, inplace=True)\ndf1['productName'].fillna(0, inplace=True)\ndf1['rating'].fillna(0, inplace=True)\ndf1['numberOfReviews'].fillna(0, inplace=True)\ndf1['price'].fillna(0, inplace=True)\ndf1.sort_index(ascending=True)\ndf1.to_csv('dldata.csv')\ndf1.head()\nprint(df1)","metadata":{"_uuid":"d61bb5c4-ba6b-41bc-bb31-1959e5ece2c1","_cell_guid":"f35c8432-d624-4677-acfa-bbbad4d5b59e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:16.412845Z","iopub.execute_input":"2021-06-09T17:32:16.413199Z","iopub.status.idle":"2021-06-09T17:32:16.481799Z","shell.execute_reply.started":"2021-06-09T17:32:16.413167Z","shell.execute_reply":"2021-06-09T17:32:16.480709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nencoder = LabelEncoder()\ncategorical_features = df1.columns.tolist()\nfor each in categorical_features:\n    df1[each] = encoder.fit_transform(df1[each])\ndf1.head()\n\"\"\"","metadata":{"_uuid":"5bc2d9f8-08ba-4baf-b791-592c1f67d557","_cell_guid":"4ce95566-6eea-4609-8529-1d46c4258000","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:16.483028Z","iopub.execute_input":"2021-06-09T17:32:16.483313Z","iopub.status.idle":"2021-06-09T17:32:16.488097Z","shell.execute_reply.started":"2021-06-09T17:32:16.483289Z","shell.execute_reply":"2021-06-09T17:32:16.487206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:32:16.852971Z","iopub.execute_input":"2021-06-09T17:32:16.853307Z","iopub.status.idle":"2021-06-09T17:32:16.865808Z","shell.execute_reply.started":"2021-06-09T17:32:16.853276Z","shell.execute_reply":"2021-06-09T17:32:16.864891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#productNames = df1.iloc[:,1:2].values\n\n#print(max(productNames))\n#productrange = max(productNames)\n#date","metadata":{"_uuid":"8cedf213-3140-45fa-b4aa-abd3f5d37047","_cell_guid":"2843c9d1-8c4d-4760-81cf-aa14ddea89ef","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-06-09T17:32:16.870448Z","iopub.execute_input":"2021-06-09T17:32:16.870838Z","iopub.status.idle":"2021-06-09T17:32:16.877521Z","shell.execute_reply.started":"2021-06-09T17:32:16.870802Z","shell.execute_reply":"2021-06-09T17:32:16.876484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas import read_csv\nfrom matplotlib import pyplot\n# load dataset\n#dataset = read_csv('dldata.csv', header=0, index_col=0)\nvalues = df1.values\n# specify columns to plot\ngroups = [ 1,2,3, 4]\ni = 1\n# plot each column\npyplot.figure()\nfor group in groups:\n    pyplot.subplot(len(groups), 1, i)\n    pyplot.plot(values[:, group])\n    pyplot.title(df1.columns[group], y=0.5, loc='right')\n    i += 1\npyplot.show()","metadata":{"_uuid":"18a0d602-e946-42c9-96be-ec351ce76013","_cell_guid":"e1fb8899-2d08-4aab-a717-a908f65b2115","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:16.889896Z","iopub.execute_input":"2021-06-09T17:32:16.890209Z","iopub.status.idle":"2021-06-09T17:32:17.241317Z","shell.execute_reply.started":"2021-06-09T17:32:16.890182Z","shell.execute_reply":"2021-06-09T17:32:17.240597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Plot closing prices\nnp.arange(0,100)\ndf_close = stock_data['Close']\n\nplt.figure(figsize=(10,6))\nplt.grid()\nplt.plot(df_close)\nplt.xlabel('Date')\nplt.ylabel('Closing Prices')\nplt.title('Tesla stock closing price')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:32:17.242508Z","iopub.execute_input":"2021-06-09T17:32:17.242935Z","iopub.status.idle":"2021-06-09T17:32:17.247396Z","shell.execute_reply.started":"2021-06-09T17:32:17.242892Z","shell.execute_reply":"2021-06-09T17:32:17.246613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# values","metadata":{"_uuid":"16184ac2-1370-487e-aa35-9b8d15d31a4d","_cell_guid":"8f0c13cf-ff8c-4a41-8ebc-02a0cb2dc34a","trusted":true}},{"cell_type":"code","source":"\n# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # put it all together\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","metadata":{"_uuid":"9175d4b1-dd24-40e6-b5bc-0e9711a2f4b0","_cell_guid":"0a2feb7c-3839-41cd-bc86-98159d00bfb2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:17.249034Z","iopub.execute_input":"2021-06-09T17:32:17.249291Z","iopub.status.idle":"2021-06-09T17:32:17.265382Z","shell.execute_reply.started":"2021-06-09T17:32:17.249267Z","shell.execute_reply":"2021-06-09T17:32:17.264249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load dataset\n#dataset = read_csv('pollution.csv', header=0, index_col=0)\n#values = dataset.values\n\n# integer encode direction\nencoder = LabelEncoder()\nvalues[:,0] = encoder.fit_transform(values[:,0])\n# ensure all data is float\n#values = values.astype('float32')\n#values","metadata":{"_uuid":"c8c452e5-529b-4f47-b16f-37185a347315","_cell_guid":"fb5eaf64-ee2a-41d7-80aa-273d2a3ce7e0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:17.267151Z","iopub.execute_input":"2021-06-09T17:32:17.267447Z","iopub.status.idle":"2021-06-09T17:32:17.280739Z","shell.execute_reply.started":"2021-06-09T17:32:17.267418Z","shell.execute_reply":"2021-06-09T17:32:17.279662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[1,2,3,4]], axis=1, inplace=True)\nprint(reframed.head())","metadata":{"_uuid":"4897885c-c110-4563-a008-0189823bc81c","_cell_guid":"9ad61765-2112-42a3-baf6-d76a2b3c4311","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:17.281888Z","iopub.execute_input":"2021-06-09T17:32:17.282354Z","iopub.status.idle":"2021-06-09T17:32:17.306556Z","shell.execute_reply.started":"2021-06-09T17:32:17.282322Z","shell.execute_reply":"2021-06-09T17:32:17.305878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train and test sets\nvalues = reframed.values\nn_train_hours = 100  #2*30 #365 * 24\ntrain = values[:n_train_hours, :]\ntest = values[n_train_hours:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n","metadata":{"_uuid":"f3f28be2-7d4e-4332-93f3-f1714b39171a","_cell_guid":"d4f64c07-a8c8-47df-a323-e9f346821355","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:17.30797Z","iopub.execute_input":"2021-06-09T17:32:17.308393Z","iopub.status.idle":"2021-06-09T17:32:17.316419Z","shell.execute_reply.started":"2021-06-09T17:32:17.30836Z","shell.execute_reply":"2021-06-09T17:32:17.315637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" train_y.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:32:17.317877Z","iopub.execute_input":"2021-06-09T17:32:17.318238Z","iopub.status.idle":"2021-06-09T17:32:17.333259Z","shell.execute_reply.started":"2021-06-09T17:32:17.318206Z","shell.execute_reply":"2021-06-09T17:32:17.332169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_X.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T17:32:17.335457Z","iopub.execute_input":"2021-06-09T17:32:17.335789Z","iopub.status.idle":"2021-06-09T17:32:17.347137Z","shell.execute_reply.started":"2021-06-09T17:32:17.335758Z","shell.execute_reply":"2021-06-09T17:32:17.346248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD,Adadelta,Adam,RMSprop\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout\n# design network\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n#model.add(RepeatVector(train_y.shape[0]))\n#model.add(LSTM(200, activation='relu', return_sequences=True))\n#model.add(Dropout(0.2))\nmodel.add(Dense(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n# fit network\nhistory = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n# plot history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()\n","metadata":{"_uuid":"10f06075-2213-456c-817a-26a6cbe0b595","_cell_guid":"07801880-8931-444f-b5db-c7d8f6384c34","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:17.348455Z","iopub.execute_input":"2021-06-09T17:32:17.348906Z","iopub.status.idle":"2021-06-09T17:32:28.194397Z","shell.execute_reply.started":"2021-06-09T17:32:17.348865Z","shell.execute_reply":"2021-06-09T17:32:28.193298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom numpy import nan,isnan,loadtxt\nfrom pandas import read_csv, DataFrame, concat\nfrom numpy import array, concatenate, hstack\nfrom matplotlib import pyplot as plt\nimport pickle\nimport os\nfrom keras.models import model_from_json\nimport time\n# multivariate multi-step encoder-decoder lstm\nfrom math import sqrt,cos,radians,sin,atan2\nfrom numpy import split\nfrom numpy import array\nfrom pandas import read_csv\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom matplotlib import pyplot\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.models import model_from_json #Modeli kaydetmek iÃ§in\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport statsmodels.api as sm","metadata":{"_uuid":"4aa4039e-c9ad-4b8c-ac95-de923e731a04","_cell_guid":"82a678c6-5f98-423b-91a2-a25350d31dbc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:28.195712Z","iopub.execute_input":"2021-06-09T17:32:28.195987Z","iopub.status.idle":"2021-06-09T17:32:28.203624Z","shell.execute_reply.started":"2021-06-09T17:32:28.195961Z","shell.execute_reply":"2021-06-09T17:32:28.202649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import *\n# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n# invert scaling for forecast\ninv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","metadata":{"_uuid":"64641efd-7e9f-446c-99c1-bb120bd6ede3","_cell_guid":"3fd758bf-464c-47b6-b5ee-87479ad84f91","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:28.204898Z","iopub.execute_input":"2021-06-09T17:32:28.205155Z","iopub.status.idle":"2021-06-09T17:32:28.768634Z","shell.execute_reply.started":"2021-06-09T17:32:28.205131Z","shell.execute_reply":"2021-06-09T17:32:28.767737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"################################Kodlar buraya kadar #######################################","metadata":{}},{"cell_type":"code","source":"#productName iÃ§in one hot encoding uygulandÄ±\n#df1=pd.get_dummies(df1,columns=[\"productName\"],prefix=[\"productName\"])\n#df1.head()","metadata":{"_uuid":"82962636-885d-44c0-8ddd-1fe7f8277a91","_cell_guid":"13e61c64-6639-4f13-bab4-a43e1b10d723","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:28.76979Z","iopub.execute_input":"2021-06-09T17:32:28.770046Z","iopub.status.idle":"2021-06-09T17:32:28.775137Z","shell.execute_reply.started":"2021-06-09T17:32:28.770021Z","shell.execute_reply":"2021-06-09T17:32:28.774372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"faed6d9e-a0d7-4643-b788-136e2aa958b2","_cell_guid":"64383fa1-cd8c-4ca0-9d09-6b6ce57a9f36","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = pd.read_csv(data,usecols=[\"date\",\"number\",\"name\",\"rating\",\"n_reviews\",\"price\"],infer_datetime_format=True,error_bad_lines=False)\n#df.replace('?', nan, inplace=True)\n#df.drop('number', axis=1, inplace=True)\n#Rename the columns\ndf1.columns = ['date','numberInRating','productName','rating', 'numberOfReviews', 'price']\ndate = df1.iloc[:,0:1].values\n\n#df1.index.name = 'date'\ndf1['numberInRating'].fillna(0, inplace=True)\ndf1['productName'].fillna(0, inplace=True)\ndf1['rating'].fillna(0, inplace=True)\ndf1['numberOfReviews'].fillna(0, inplace=True)\ndf1['price'].fillna(0, inplace=True)\ndf1.sort_index(ascending=True)\ndf1.to_csv('dldata.csv')\ndf1.head()","metadata":{"_uuid":"c750b1c2-d6cc-4a5b-8a15-8082dece21bd","_cell_guid":"eff8945d-3913-4b1f-a0dd-9f4a8c9f07ec","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:28.77708Z","iopub.execute_input":"2021-06-09T17:32:28.77737Z","iopub.status.idle":"2021-06-09T17:32:28.864509Z","shell.execute_reply.started":"2021-06-09T17:32:28.777343Z","shell.execute_reply":"2021-06-09T17:32:28.863584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def changeDate4sort(date):\n    x = 0\n    dindex = []\n    for i in range(len(date)):\n        x = x+1\n        #if(x == 6):\n        #    print(dindex)\n        #    break\n        #print(x)\n        day = date[i,:].tolist()\n        day = day[0]\n        day=day.split(\"-\")\n        dateindex = int(day[1])*10\n        dateindex = dateindex + int(day[2])\n        dindex.append(dateindex)\n        #print(day)\n        #print(type(day))\n    return dindex","metadata":{"_uuid":"751dda79-05ff-4855-8f7a-0220fefbe7a4","_cell_guid":"563921a1-8914-441a-bd54-be658a19a9dc","execution":{"iopub.status.busy":"2021-06-09T17:32:28.865945Z","iopub.execute_input":"2021-06-09T17:32:28.86622Z","iopub.status.idle":"2021-06-09T17:32:28.871608Z","shell.execute_reply.started":"2021-06-09T17:32:28.866191Z","shell.execute_reply":"2021-06-09T17:32:28.870683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df3 = DataFrame()\ndindex = changeDate4sort(date)\n#df1.drop('date', axis=1, inplace=True)\ndf1[\"date\"] = dindex\ndf1 = df1.set_index('date')\ndf1.head()","metadata":{"_uuid":"a30d284c-88bd-4a29-b239-77bb75df8133","_cell_guid":"a2874a49-2ff5-4fe5-bba6-fe297ffe606b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:28.872887Z","iopub.execute_input":"2021-06-09T17:32:28.873172Z","iopub.status.idle":"2021-06-09T17:32:28.908986Z","shell.execute_reply.started":"2021-06-09T17:32:28.873143Z","shell.execute_reply":"2021-06-09T17:32:28.90769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"values = df1.values\n# specify columns to plot\ngroups = [0, 1, 2, 3, 5, 6, 7]\ni = 1\n# plot each column\npyplot.figure()\nfor group in groups:\n\tpyplot.subplot(len(groups), 1, i)\n\tpyplot.plot(values[:, group])\n\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n\ti += 1\npyplot.show()","metadata":{"_uuid":"b0f3ae98-8406-4acd-a222-bc1b3a9d2eef","_cell_guid":"d5d8c284-59ef-4437-b9d4-c042b9e1d3d2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:28.910237Z","iopub.execute_input":"2021-06-09T17:32:28.910534Z","iopub.status.idle":"2021-06-09T17:32:29.099437Z","shell.execute_reply.started":"2021-06-09T17:32:28.910502Z","shell.execute_reply":"2021-06-09T17:32:29.09787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","metadata":{"_uuid":"ed4269ed-9e20-4bcc-be5e-e3364cfa0786","_cell_guid":"5724835a-fd8e-4d62-b187-76694bfea039","execution":{"iopub.status.busy":"2021-06-09T17:32:29.100607Z","iopub.status.idle":"2021-06-09T17:32:29.101004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()","metadata":{"_uuid":"af03e977-7d88-4d63-8f1d-a48afa391855","_cell_guid":"9012206e-2e25-4902-a49f-f94cd7d1ca31","execution":{"iopub.status.busy":"2021-06-09T17:32:29.102274Z","iopub.status.idle":"2021-06-09T17:32:29.102873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","metadata":{"_uuid":"21405871-2453-462b-95a2-002cd7f1ecf4","_cell_guid":"a6a1ff90-64fb-4f52-b0db-ba0c74101df2","execution":{"iopub.status.busy":"2021-06-09T17:32:29.104222Z","iopub.status.idle":"2021-06-09T17:32:29.104829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPerColumnDistribution(df, 10, 5)","metadata":{"_uuid":"310f9e88-3149-4988-8d6b-e9fb0f2ae185","_cell_guid":"9e44bd15-0cdd-4397-a5aa-d1473ca47aa6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:29.106194Z","iopub.status.idle":"2021-06-09T17:32:29.106805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandas import read_csv\nfrom datetime import datetime\n# load data\ndef parse(x):\n    return datetime.strptime(x, '%Y %m %d %H')\ndataset = read_csv('raw.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\ndataset.drop('No', axis=1, inplace=True)\n# manually specify column names\ndataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\ndataset.index.name = 'date'\n# mark all NA values with 0\ndataset['pollution'].fillna(0, inplace=True)\n# drop the first 24 hours\ndataset = dataset[24:]\n# summarize first 5 rows\nprint(dataset.head(5))\n# save to file\ndataset.to_csv('pollution.csv')","metadata":{"_uuid":"b0fb9763-44d7-4916-b812-42de9394f532","_cell_guid":"b7e3c7b6-b068-4717-970f-74b23453f291","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:29.108327Z","iopub.status.idle":"2021-06-09T17:32:29.108959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# First let us load the datasets into different Dataframes\ndef load_data(datapath):\n    data = pd.read_csv(datapath)\n   # Dimensions\n    print('Shape:', data.shape)\n    # Set of features we have are: date, store, and item\n    display(data.sample(10))\n    return data\n    \n    \ntrain_df = load_data('../input/datasource/train.csv')\ntest_df = load_data('../input/datasource/test.csv')\nsample_df = load_data('../input/datasource/sample_submission.csv')","metadata":{"_uuid":"20b0d947-ad02-43e5-93f6-ad0f8dfe2c45","_cell_guid":"c7d4994a-a0f6-4430-8be5-38b59b1b49c2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-09T17:32:29.110507Z","iopub.status.idle":"2021-06-09T17:32:29.111143Z"},"trusted":true},"execution_count":null,"outputs":[]}]}