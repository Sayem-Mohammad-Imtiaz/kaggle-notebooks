{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# i am beginner in the world of ML and deep leanring. I am eager to interact and learn from the experts.  here below my proposed model to analyze the stroke dataset.\n**I reached approx. 97% F1 score on validation set **using SMOTE for oversampling, standardization of 3 features i.e. BMI, AGE and AVG-GLUCOSE-LEVEL and as model I used XGBClassifier.\nI thank anyone willing to comment and give feedback on what could be improved in terms of EDA, preprocessing, models, etc.**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# this is my first autonomous and indipnendet work after following some Kaggle courses on ML, DL and data visualization.","metadata":{}},{"cell_type":"markdown","source":"# very much eager to interact with outher users and read feedbacks/comments to speed up my leanring curve and build up more and more experience.\n#  ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"datafilepath=\"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\"\ndata=pd.read_csv(datafilepath, index_col='id', parse_dates=True)\nimport seaborn as sns\nX=data.copy()\ntarget_var=X.pop('stroke')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***NaN check***","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"for r in data.columns:\n    tmp=data[r].isnull().values.any()\n    tmp2=data[r].isnull().sum()\n    print(r + \"=\" + str(tmp*1.0) + \"// % NaN = \" + str(tmp2*1.0 / (1.0*len(data[r]))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****bmi is only var with NaN. NaN percentage is 4%, low. I replace NaN with mean****","metadata":{}},{"cell_type":"code","source":"data['stroke'].eq(0).sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****1D data visualization**** ","metadata":{}},{"cell_type":"code","source":"data.info()\ndata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import *\nnumcat = data.select_dtypes(include=['int64', 'float64'])\nprint(numcat.columns)\nfor p in (numcat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stringscat = data.select_dtypes(include=['object'])\nprint(stringscat.columns)\nfor p in (stringscat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_smoke_unkwn = data[data['smoking_status']=='Unknown']\ndata_smoke_unkwn['stroke'].eq(0).sum()*100/(len(data_smoke_unkwn['stroke']))\ndata_smoke_unkwn['stroke'].eq(1).sum()*100/(len(data_smoke_unkwn['stroke']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selection={'stroke': [1]}\ndata_withstroke = data[data['stroke']==1]\ndata_withstroke.info()\ndata_withOutstroke = data[data['stroke']==0]\ndata_withOutstroke.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for p in (stringscat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p, hue='stroke')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for p in (numcat.columns):\n    pyplot.figure()\n    sns.histplot(data=data, x=p, hue='stroke')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****search for correlation / ongoing****","metadata":{}},{"cell_type":"code","source":"#only numeric\ndatacorr=data.corr()\nsns.heatmap(datacorr, annot=True)\n#only numeric after standardization\ndataSc= (data - data.mean(axis=0))/data.std(axis=0)\ndataSccorr=dataSc.corr()\npyplot.figure()\nsns.heatmap(dataSccorr, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****data encoding****","metadata":{}},{"cell_type":"code","source":"Xcat=X.select_dtypes('object')\nXcat.describe()\nfrom sklearn.preprocessing import LabelEncoder\nle_Xcat = X.copy()\nle = LabelEncoder()\nfor p in X.select_dtypes('object'):\n    le_Xcat[p] = le.fit_transform(X[p])\nle_Xcat.describe()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****imputing NaN****","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nmyimputer = SimpleImputer()\nimp_le_Xcat = pd.DataFrame(myimputer.fit_transform(le_Xcat))\nprint(le_Xcat.info())\nfeaturesName = le_Xcat.columns\nprint(featuresName)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# handling the imbalanced dataset","metadata":{}},{"cell_type":"code","source":"print('% no stroke = ' + str(100*data['stroke'].eq(0).sum() / len(data['stroke'])))\nprint('% stroke = ' + str(100*data['stroke'].eq(1).sum() / len(data['stroke'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****easy approach / undersample the 'no stroke case' so to have 50/50****","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\n\nmySampler = SMOTE(random_state=0)\nxres, yres = mySampler.fit_resample(imp_le_Xcat,target_var)\nxres.columns = featuresName","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xres.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xres.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#standardize bmi avg glucose level and age\nstdfeat = ['age', 'avg_glucose_level', 'bmi']\nxstd = xres[stdfeat]\nxstdn = (xstd - xstd.mean(axis=0)) / (xstd.std(axis=0))\n#nonstd_age = xres.drop('age')\n#nonstd_avggluc = xres.drop('avg_glucose_level')\n#nonstd_bmi = xres.drop('bmi')\n#xresn = pd.concat([xres, xstdn], axis=0)\nxrescopy=xres.copy()\nxrescopy[stdfeat] = xstdn[stdfeat]\nxresn = xrescopy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.preprocessing import StandardScaler\n#myScaler = StandardScaler()\n#xres_sc = (xres - xres.mean(axis=0)) / xres.std(axis=0)\n#xres_sc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****XGB classifier as first investigated approach****\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import *\nfrom sklearn.utils import shuffle\nxtrain, xval, ytrain, yval = train_test_split(xresn, yres, train_size=0.8, test_size=0.2, random_state=0)\nfrom xgboost import XGBClassifier\nxtrain = shuffle(xtrain, random_state=1)\nytrain = shuffle(ytrain, random_state=1)\nmodel= XGBClassifier(\n    n_estimators=256,\n    learning_rate=0.1,\n    subsample=0.5,\n    use_label_encoder=False,\n    random_state=1)\nmodel.fit(xtrain,ytrain)\nypred=model.predict(xval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(yval,ypred))\nff=classification_report(yval,ypred)\nff1=f1_score(yval,ypred)\nprint(ff1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xval2, yval2 = shuffle(xval, yval, random_state=21)\nypred2=model.predict(xval2)\nprint(classification_report(yval2,ypred2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}