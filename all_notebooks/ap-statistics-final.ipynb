{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AP Statistics Final Project\n**Neural Network for Predicting Airfoil Self-Nosie**\n\nThe [data](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise) from this project is a Kaggle mirror of a NASA dataset from the open source UC Irvine Machine Learning Repository.\n\nMore details about data features and the data set itself can be found at the link above, or in my presentation.\n\nThe objective of this model is the sound pressure level of the airfoil, measured in decibels (dB). ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the input data.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_data = pd.read_csv(\"/kaggle/input/nasa-airfoil-self-noise/NASA_airfoil_self_noise.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following is a statistical summary of each variable, incuding the objective (sound). Listed for each variable is the five-number summary, the sample size (n), the mean, and the standard deviation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following is a correlation table showing the value of *r*, the correlation coefficient, for linear regressions using each combination of variables. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following six graphs are histograms of the five feature variables, and the single output variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data[\"Frequency\"].plot(kind=\"hist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data[\"AngleAttack\"].plot(kind=\"hist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data[\"ChordLength\"].plot(kind=\"hist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data[\"FreeStreamVelocity\"].plot(kind=\"hist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data[\"SuctionSide\"].plot(kind=\"hist\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data[\"Sound\"].plot(kind=\"hist\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting data into features and objective.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = input_data[\"Sound\"]\nX = input_data.drop(\"Sound\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a train/test split of 80/20.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Constructing the actual model layer by layer. The \"relu\" activation function is used for all Dense layers except the last one, where a linear one is used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import *\nmodel = keras.models.Sequential()\nmodel.add(Dense(128, input_dim = 5, activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(512, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"linear\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compiling the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import metrics\nmodel.compile(optimizer=\"adam\", loss = \"mean_squared_error\", metrics=[metrics.MeanSquaredError()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting the model to the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, batch_size = 64, epochs = 5000, verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary of the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating the loss of the model on the test set (in this case, it is mean squared error).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = model.evaluate(X_test, y_test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean Squared Error:\", loss)\nprint(\"Root Mean Squared Error:\", np.sqrt(loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a residual plot of predicted vs actual values to see if it appropriate to calculate the correlation coefficient *r*.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nsns.residplot(y_test, y_pred, lowess=True, color=\"g\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A scatterplot of our predictions vs the actual values for the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\npyplot.scatter(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing the dimensionality of our predictions to match the input data so we can calculate the correlation coefficient.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_1 = y_pred.flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating the value of the correlation coefficient *r* between our predictions on the test set and the actual values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport scipy.stats\ncorr , _ = scipy.stats.pearsonr(y_test, y_pred_1)\nprint(\"Pearsons correlation:\", corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating *r^2*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r2 = np.power(corr,2)\nprint(r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Regression of Observed vs Predicted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nimport numpy as np\nslope, intercept, r_value, p_value, std = stats.linregress(y_test,y_pred_1)\nprint(slope, intercept)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}