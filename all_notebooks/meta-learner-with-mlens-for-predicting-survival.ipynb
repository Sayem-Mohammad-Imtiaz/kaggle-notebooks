{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Total Passengers by countries on World Map -Using Library Pycountry\n1. Read the file \n2. Use pycountry to get ISO3 code for countries\n3. Use plotly to plot the data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nestonia=pd.read_csv('../input/passenger-list-for-the-estonia-ferry-disaster/estonia-passenger-list.csv')\ncountry_wise=estonia.groupby(['Country'],as_index=False)['Survived'].count()\ncountry_wise['Total_passengers']=country_wise['Survived']\nimport pycountry\nimport plotly.express as px\nimport pandas as pd\nimport plotly.graph_objs as gobj\nlist_countries = country_wise['Country'].unique().tolist()\nd_country_code = {}  # To hold the country names and their ISO\nfor country in list_countries:\n    try:\n        country_data = pycountry.countries.search_fuzzy(country)      \n        country_code = country_data[0].alpha_3\n        d_country_code.update({country: country_code})\n    except:\n        print('could not add ISO 3 code for ->', country)\n        # If could not find country, make ISO code ' '\n        d_country_code.update({country: ' '})\nfor k, v in d_country_code.items():\n    country_wise.loc[(country_wise.Country == k), 'iso_alpha'] = v\n\nfig = px.choropleth(data_frame = country_wise,\n                    locations= \"iso_alpha\",\n                    color= \"Total_passengers\",  \n                    hover_name=\"Country\",                   \n                    color_continuous_scale=px.colors.sequential.Plasma\n                    )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of them are Swedes and Estonians, followed by Finns and Russians.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA -Age, LastName, Category, Sex","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"age_wise=estonia.groupby(['Age'],as_index=False)['Survived'].count()\nage_wise_sum=estonia.groupby(['Age'],as_index=False)['Survived'].sum()\nage_wise_per =(age_wise_sum['Survived'])/(age_wise['Survived'])*100\nage_wise_per=pd.DataFrame(age_wise_per)\nage_wise_per_d=pd.concat([age_wise['Age'],age_wise_per],axis=1)\nimport plotly.express as px\npx.bar(age_wise_per_d,x='Age',y='Survived',title=\"Age wise survival %\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The chances of survival will be averaging around 20% if you are in age bucket *20-40* years. 10% if you are *40-50* and 5% if 5*0-65*, if you are *65 +* the chnaces of survival is 0%**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"passenger_wise=estonia.groupby(['Category'],as_index=False)['Survived'].count()\npassenger_wise_surv=estonia.groupby(['Category'],as_index=False)['Survived'].sum()\nsurvival_perce_pass_type=(passenger_wise_surv['Survived']/passenger_wise['Survived'])*100\nsurvival_perce_pass_type_df=pd.concat([passenger_wise['Category'],survival_perce_pass_type],axis=1)\nsurvival_perce_pass_type_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Crew had 8% more survival % then passengers**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Plotting the Lastnames Frequency for Survived and Deceased","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"survived=estonia[estonia['Survived']==1]\ndeceased=estonia[estonia['Survived']==0]\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nimport numpy as np\nimport seaborn as sns\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n        count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n    \n    plt.figure(2, figsize=(15, 15/1.6180))\n    plt.subplot(title='10 most common surnames in deceased')\n    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n    sns.barplot(x_pos, counts, palette='husl')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.show()\ntext2=deceased['Lastname'].values\ncount_vectorizer = CountVectorizer(stop_words='english')\ncount_data = count_vectorizer.fit_transform(text2)\nplot_10_most_common_words(count_data, count_vectorizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**15 people with *Andersson* family name couldnt survive , these might be from same family. Similarly Eriksson and Karlsson also lost lot of family members**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nimport numpy as np\nimport seaborn as sns\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n        count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n    \n    plt.figure(2, figsize=(15, 15/1.6180))\n    plt.subplot(title='10 most common surnames in survived')\n    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n    sns.barplot(x_pos, counts, palette='husl')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.show()\ntext2=survived['Lastname'].values\ncount_vectorizer = CountVectorizer(stop_words='english')\ncount_data = count_vectorizer.fit_transform(text2)\nplot_10_most_common_words(count_data, count_vectorizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Age and Sex Distribution for Deceased and Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(deceased,x=\"Age\",color=\"Sex\",title=\"Age wise distribution of sex in deceased\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Females death are higher as compared to males in majority of age categories.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"px.histogram(survived,x=\"Age\",color=\"Sex\",title=\"Age wise distribution of sex in survived\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Male survival numbers are higher in every age bucket**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Trying TF-IDF on Lastname","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The TFIDF will not work here because the occurence of Name will be unique and majoriry of vectors will be zero , this will not add value to prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nv = TfidfVectorizer()\ntf = v.fit_transform(estonia['Lastname'])\nestonia['tfidf']=tf.toarray()\nestonia['tfidf'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estonia['agebin'] = pd.cut(estonia['Age'].astype(int), 4)\nestonia['country_bin']= [1 if x =='Estonia' or x =='Sweden' else 0 for x in estonia['Country']] \nfrom sklearn.preprocessing import LabelEncoder\nencode=LabelEncoder()\nestonia['sex_cat']=encode.fit_transform(estonia['Sex'])\nestonia['agecat'] =encode.fit_transform(estonia['agebin'])\nestonia['surname_cat']=encode.fit_transform(estonia['Lastname'])\nestonia['passcat'] =encode.fit_transform(estonia['Category'])\nx=estonia[['country_bin', 'sex_cat','agecat','passcat','tfidf','surname_cat']]\ny=estonia['Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trying Label encoding on Lastname, this method is strange since as per my analysis this will make algorthms to think names as numeric because of too many categories in the variable. But neverthless we will try out.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Char2Vec for converting last name to word embedding- Didn't got that good :P","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install chars2vec\nimport chars2vec\nimport sklearn.decomposition\nimport matplotlib.pyplot as plt\nc2v_model = chars2vec.load_model('eng_50')\nword_embeddings = c2v_model.vectorize_words(estonia['Lastname'].to_list())\nprojection_2d = sklearn.decomposition.PCA(n_components=2).fit_transform(word_embeddings)\nf = plt.figure(figsize=(8, 6))\nfor j in range(len(projection_2d)):\n    plt.scatter(projection_2d[j, 0], projection_2d[j, 1],\n                marker=('$' + estonia['Lastname'][j] + '$'),\n                s=500 * len(estonia['Lastname'][j]), label=j)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Char2Vec is new library like word2vec , but names are not proper words of dictionary hence there embedding wouldnt be proper while using word2vec, though this library too fails to yield relational embeddings , it is just combinings strings at random to come up with new strings that doesnt make any sense \n\nLink : https://github.com/Lettria/Char2Vec","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SMOTE for oversampling the 1's the survived ones","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nover = SMOTE(random_state=0)\nov_x,ov_y=over.fit_sample(x, y)\nfrom sklearn.model_selection import train_test_split\ntrainx,testx,trainy,testy=train_test_split(ov_x,ov_y,test_size=0.2,random_state=123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### xgboost with scale_pos_weight for class imbalance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nfrom xgboost import XGBClassifier\nxgc=XGBClassifier(scale_pos_weight=2)\nmodel1=xgc.fit(trainx,trainy)\nprediction=model1.predict(testx)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(testy,prediction))\nprint(classification_report(testy,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgc.get_booster().get_score(importance_type= \"gain\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\nimport matplotlib.pyplot as plt\nplot_importance(model1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**O My My XGB Feature importance shows that surname category to be have the highest f1 score, now there is a problem because label encoding names has somewhat yielded lot many labels and thus it is tricking the model to believe that this is some nuemrical feature that might have some relation with survival**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Trying neural networks with custom learning rate scheduler","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Dense,Activation\nfrom keras.layers.normalization import BatchNormalization\nopt2=tf.keras.optimizers.Adam(\n    learning_rate=0.01, beta_1=0.5, beta_2=0.5, epsilon=1e-07, amsgrad=False,\n    name='Adam')\nsgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nfrom keras.models import Sequential\nmodel2 = Sequential()\nmodel2.add(Dense(300,input_dim=(6))),  \nmodel2.add(Activation('selu')),\nmodel2.add(Dense(100,kernel_regularizer=keras.regularizers.l2(0.01))),\nmodel2.add(Activation('selu')),\nmodel2.add(Dense(20,kernel_regularizer=keras.regularizers.l2(0.01))),\nmodel2.add(LeakyReLU(alpha=0.1)),\nmodel2.add(Dense(2))\nmodel2.add(Activation('softmax'))\n\nepochs=100\noptimizers=keras.optimizers.SGD(clipvalue=1.0)\ndef exp_decay(lr0,s):\n    def exp_decay_fn(epcohs):\n        return lr0*0.1**(epochs/s)\n    return exp_decay_fn\n\nexp_decay_fn=exp_decay(lr0=0.1,s=50)\nlr_sch=keras.callbacks.LearningRateScheduler(exp_decay_fn)\nlr_sch2=keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=5)\nmodel2.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"Nadam\",metrics=[\"accuracy\"])\nhistory=model2.fit(trainx,trainy,epochs=100,callbacks=[lr_sch],verbose=0)\nimport matplotlib.pyplot as plt\npd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\npredict=model2.predict_classes(testx)\nprint(confusion_matrix(testy, predict))\nprint(classification_report(testy, predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model has yielded 75% accurcay, the Nesterov Adam optimizer with combination of Selu or Leaky relu activations yields best results, added L2 regularization to compensate for overfitting.The best accurcay we got was by using sparse categorical cross entropy loss function.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Trying light gbm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\ntrain_data=lgb.Dataset(trainx,label=trainy)\nparam = {'num_leaves':200, 'objective':'binary','max_depth':10,'learning_rate':.01,'max_bin':200}\nparam['metric'] = ['auc', 'binary_logloss']\nnum_round=100\nlgbm=lgb.train(param,train_data,num_round)\nypred2=lgbm.predict(testx)\nfor i in range(0,len(testx)):\n    if ypred2[i]>=.5:\n        ypred2[i]=1\n    else:\n        ypred2[i]=0\nconfusion_matrix(testy,ypred2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBM fit the data most fast , but accurcay wise it is behind XGBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# NN with drop out : Poorest :(","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Dropout\nfrom keras.layers import Dense,Activation\nfrom keras.layers.normalization import BatchNormalization\nopt2=tf.keras.optimizers.Adam(\n    learning_rate=0.01, beta_1=0.5, beta_2=0.5, epsilon=1e-07, amsgrad=False,\n    name='Adam')\nsgd = keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nfrom keras.models import Sequential\nmodel3 = Sequential()\n\nmodel3.add(Dense(300,input_dim=(6))),  \nmodel3.add(Activation('selu')),\nmodel3.add(Dropout(0.2)),\nmodel3.add(Dense(100,kernel_regularizer=keras.regularizers.l2(0.01))),\nmodel3.add(Activation('selu')),\nmodel3.add(Dropout(0.2)),\nmodel3.add(Dense(20,kernel_regularizer=keras.regularizers.l2(0.01))),\nmodel3.add(Dropout(0.2)),\nmodel3.add(Activation('selu')),\nmodel3.add(Dense(2))\n\nmodel3.add(Activation('softmax'))\n\nepochs=100\noptimizers=keras.optimizers.SGD(clipvalue=1.0)\ndef exp_decay(lr0,s):\n    def exp_decay_fn(epcohs):\n        return lr0*0.1**(epochs/s)\n    return exp_decay_fn\n\nexp_decay_fn=exp_decay(lr0=0.1,s=100)\nlr_sch=keras.callbacks.LearningRateScheduler(exp_decay_fn)\nlr_sch2=keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=5)\nmodel3.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"Nadam\",metrics=[\"accuracy\"])\nhistory=model3.fit(trainx,trainy,epochs=100,callbacks=[lr_sch],verbose=0)\nimport matplotlib.pyplot as plt\npd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\npredict=model3.predict_classes(testx)\nprint(confusion_matrix(testy, predict))\nprint(classification_report(testy, predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropout is regularization technique where we drop % of nodes to reduce overfitting, we tried 20% but that screwed the model. Bad idea","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm=SVC(kernel='linear')\nsvm.fit(trainx,trainy)\npred_new=svm.predict(testx)\nconfusion_matrix(testy,pred_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Interestingly the conventional linear kernel based SVC is classifying data better then neural networks, the reason is data has no outliers and is not huge in size. The decision boundary linear are fitting better then RBF and polynomial Kernels**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Stacking using MLens","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I came to know about wondeful library for stacking all scikit learn models,this is also called metalearning. this package also stacks xgboost library model as well.\n\n**This doesnt fit keras model.**\n\nLink https://machinelearningmastery.com/super-learner-ensemble-in-python/\n\nCreator of package :http://flennerhag.com/\n\nHere is the process\n1. Select a k-fold split of the training dataset.\n2. Select m base-models or model configurations.\n3. For each basemodel:\n              a. Evaluate using k-fold cross-validation.\n              b. Store all out-of-fold predictions.\n              c. Fit the model on the full training dataset and store.\n4. Fit a meta-model on the out-of-fold predictions.\n5. Evaluate the model on a holdout dataset or use model to make predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom mlens.ensemble import SuperLearner\nfrom sklearn.metrics import accuracy_score\ndef get_models():\n    models = list()\n    models.append(LogisticRegression(solver='liblinear'))\n    models.append(DecisionTreeClassifier())\n    models.append(SVC(kernel='linear'))\n    models.append(GaussianNB())\n    models.append(KNeighborsClassifier())\n    models.append(AdaBoostClassifier())    \n    models.append(BaggingClassifier(n_estimators=100))\n    models.append(RandomForestClassifier(n_estimators=100))\n    models.append(ExtraTreesClassifier(n_estimators=100))\n    models.append(XGBClassifier(scale_pos_weight=2))\n    return models\n# create the super learner\ndef get_super_learner(X):\n    ensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=False, sample_size=len(X))\n    models = get_models()\n    ensemble.add(models)\n    ensemble.add_meta(DecisionTreeClassifier())\n    return ensemble\n# create the super learner\nensemble = get_super_learner(trainx)\n# fit the super learner\nensemble.fit(trainx, trainy)\n# summarize base learners\nprint(ensemble.data)\n# make predictions on hold out set\nyhat = ensemble.predict(testx)\nprint('Super Learner: %.3f' % (accuracy_score(testy, yhat) * 100))\nprint(confusion_matrix(testy,yhat))\nprint(classification_report(testy,yhat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Benifit : To overcome overfitting\n    \nBy training a meta-model on out-of-sample predictions of other models, the meta-model learns how to both correct the out-of-sample predictions for each model and to best combine the out-of-sample predictions from multiple models; actually, it does both tasks at the same time.\nHere we can try out different models in meta learning in ensemble.meta_model layer","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}