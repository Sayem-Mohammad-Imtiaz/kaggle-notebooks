{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Part-1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Objectives:\n1. To explore the dataset","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle//input/bigmart-sales-data'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('/kaggle//input/bigmart-sales-data/Test.csv')\ntrain_data = pd.read_csv('/kaggle//input/bigmart-sales-data/Train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for every unique item their is unique id ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BigMart Sales Prediction practice problem**\n\nWe have train (8523) and test (5681) data set, train data set has both input and output variable(s). We need to predict the sales for test data set.\n\n* Item_Identifier: Unique product ID\n\n* Item_Weight: Weight of product\n\n* Item_Fat_Content: Whether the product is low fat or not\n\n* Item_Visibility: The % of total display area of all products in a store allocated to the particular product\n\n* Item_Type: The category to which the product belongs\n\n* Item_MRP: Maximum Retail Price (list price) of the product\n\n* Outlet_Identifier: Unique store ID\n\n* Outlet_Establishment_Year: The year in which store was established\n\n* Outlet_Size: The size of the store in terms of ground area covered\n\n* Outlet_Location_Type: The type of city in which the store is located\n\n* Outlet_Type: Whether the outlet is just a grocery store or some sort of supermarket\n\n* Item_Outlet_Sales: Sales of the product in the particulat store. This is the outcome variable to be predicted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.select_dtypes(include='object').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Upto here what we can conclude that :\n1. **Our data consist of two sections one is for item and one is outlet where each contains two type of data ,categorical\n     and numerical.**\n2. **There are missing values in item weight which is float, and outlet size which is categorical. Similar in the test data.**     \n3. **Also we could see that obejct data has different categorical values which needed to be encoded.**     ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Part-2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Objectives**\n1. Visualize each columns and check the correlation between each of them.\n2. Fill the null or missing values.\n3. One hot encode every categorical data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets visualize some data and make conclusions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.barplot(x='Item_Fat_Content',y='Item_Outlet_Sales',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20,8)})\nchart = sns.barplot(x='Item_Type',y='Item_Weight',data=train_data)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is important as what we can infer from this : \nOn an average every item type vs weight didnt vary , like for eg. Canned type or Dairy both are near to 12 etc. Neither one has different max value. \nHence for the missing values we would fill them with mean value.\nBut think of situation where max value of Meat is upto 6 , Canned is to 3, Seafood to 12. Think of how do we fill the null value than?\nFor that case we have to fill the null value by item_type. Each mean value correspond to each item_type.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,8)})\nsns.barplot(x='Outlet_Type',y='Item_Outlet_Sales',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer from these that Grocery stores in the outlet type has poor Item outlet sales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Item_Weight'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will merge data of train and test to data engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['source'] = 'train'\ntest_data['source'] = 'test'\ntest_data['Item_Outlet_Sales'] = 0\ndata = pd.concat([train_data, test_data], sort = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handling Na values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Item weight to be filled with mean value\nmean = data['Item_Weight'].mean()\ndata['Item_Weight'] = data['Item_Weight'].fillna(value=mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data['Item_Outlet_Sales'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This Item Outlet Sales seems to be preety rightly skewed to apply it to the model we might need to perform some standard scaler operation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,8)})\nsns.barplot(x='Outlet_Size',y='Item_Outlet_Sales',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Outlet_Size',data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Outlet size Na value we are going to fill with mode","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import mode\n\n#Determing the mode for each\noutlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x.astype('str')).mode[0]))\nprint ('Mode for each Outlet_Type:')\nprint (outlet_size_mode)\n\n#Get a boolean variable specifying missing Item_Weight values\nmissing_values = data['Outlet_Size'].isnull() \n\n#Impute data and check #missing values before and after imputation to confirm\nprint ('\\nOrignal #missing: %d'% sum(missing_values))\ndata.loc[missing_values,'Outlet_Size'] = data.loc[missing_values,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\nprint (sum(data['Outlet_Size'].isnull()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Item_Fat_Content'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have same category but with different names. Hence we merge them to same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets merge contents to Low fat and Regular\ndata['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat','low fat':'Low Fat','reg':'Regular'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Item visibilty must be some value as 0 visibilty didnt make any sense , as with 0 visibilty the product  outlet sale should be zero, but is isn't.\n#Hence let change the zero value.\nsns.scatterplot('Item_Visibility','Item_Outlet_Sales',data=data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We are gonna fill 0 values with mean.\nmean = data['Item_Visibility'].mean()\ndata=data.replace({'Item_Visibility': {0.0: mean}})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Item_Type'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we one hot code for the categorical values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(data[['Item_Fat_Content','Item_Type','Outlet_Size','Outlet_Location_Type','Outlet_Type']])\ndummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processed = pd.concat([data,dummies],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#New variable for outlet\ndata_processed['Outlet'] = le.fit_transform(data['Outlet_Identifier'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processed.drop(['Item_Fat_Content','Item_Type','Outlet_Identifier','Outlet_Size','Outlet_Location_Type','Outlet_Type'],inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processed['Outlet_Year'] = 2009 - data_processed['Outlet_Establishment_Year']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_processed.drop(['Outlet_Establishment_Year'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data_processed.loc[data['source']==\"train\"]\ntest = data_processed.loc[data['source']==\"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we complete our Part 2.\n**What we conclude from this**\n* Our data is cleaned and all the categorical data is converted to encode understandable by the model.\n* We still have to work on perform standard scaler operation, but we should be aware that Standard scaler isn't always give good feedback on dummies, for why? Go through the following link.\n\n[https://www.quora.com/How-bad-is-it-to-standardize-dummy-variables]\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}