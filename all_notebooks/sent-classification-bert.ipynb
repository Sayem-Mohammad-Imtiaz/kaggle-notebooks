{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls '/kaggle/input/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport random\nimport numpy as np\nimport re\nfrom transformers import BertTokenizer\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom string import punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv',engine='python')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['p_review'] = df['review']\ndf['p_review'].replace(to_replace = '.<br\\s+\\/'.format(punctuation),inplace=True,value='',regex=True)\ndf['p_review'].replace(to_replace = '[{}]'.format(punctuation),inplace=True,value='',regex=True)\ndf['p_review'].replace(to_replace = '\\s+',inplace=True,value=' ',regex=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting polarity to numnber\ndf['p_polarity']=df['sentiment']\ndf['p_polarity'] = df['p_polarity'].map({'positive':0,'negative':1})\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all=[]\nfor i,row in df.iterrows():\n    data_all.append((row['p_review'],row['p_polarity']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_list = list(range(0,len(data_all)))\nnp.random.seed(3)\nnp.random.shuffle(id_list)\ntrain_data = [data_all[i] for i in id_list[:int(len(data_all)*.90)]]\ntest_data = [data_all[i] for i in id_list[int(len(data_all)*.90):]]\nlen(train_data),len(test_data)\ntrain_data=train_data[:2000]\ntest_data=test_data[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Loader\nclass MyLoader(Dataset):\n    def __init__(self,data_set):\n        \n        self.data=data_set\n        \n    def __getitem__(self,index):\n        return self.data[index][0],self.data[index][1]\n        \n        \n    def __len__(self):\n        return len(self.data)\ndef my_collate_fn(batch):\n    sent_list = [i for i,j in batch]\n    lbl_list = [j for i,j in batch]\n    t_encode = tokenizer.batch_encode_plus(sent_list,pad_to_max_length=True,max_length=512)\n    return torch.tensor(t_encode['input_ids']),torch.tensor(t_encode['token_type_ids']),torch.tensor(t_encode['attention_mask']),torch.tensor(lbl_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_sz=4\ntrain_obj = MyLoader(train_data)\ntrain_loader=torch.utils.data.DataLoader(train_obj,batch_size=batch_sz,collate_fn=my_collate_fn)\n\nval_obj = MyLoader(test_data)\nval_loader=torch.utils.data.DataLoader(val_obj,batch_size=batch_sz,collate_fn=my_collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,(input_ids,token_type_ids,attention_mask,lbl_list) in enumerate(train_loader): \n    print(type(input_ids[0]))\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load and Fine tune PreTrained Bert Model\n\nmodel = BertForSequenceClassification.from_pretrained(\\\n                                                      \n    \"/kaggle/input/bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\\\n                                                      \n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\\\n                                                      \n    output_hidden_states = False, # Whether the model returns all hidden-states.\n                                                      \n)\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_values = []\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Reset the total loss for this epoch.\n    total_loss = 0\n    # Put the model into training mode. Don't be mislead--the call to \n    model.train()\n    # For each batch of training data...\n    for step, batch in enumerate(train_loader):\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[2].to(device)\n        b_labels = batch[3].to(device)\n        # Always clear any previously calculated gradients before performing a\n        \n        model.zero_grad()        \n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n        loss = outputs[0]\n        # Accumulate the training loss over all of the batches so that we can\n        total_loss += loss.item()\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n        \n        # Update the learning rate.\n#         scheduler.step()\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss / len(train_loader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    #print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    print(\"\")\n    print(\"Running Validation...\")\n    \n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Evaluate data for one epoch\n    for batch in val_loader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids,abc, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n        # Track the number of batches\n        nb_eval_steps += 1\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n   # print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\nprint(\"\")\nprint(\"Training complete!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.save(model,'checkpoint.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_model = torch.load('checkpoint.pth')\nloaded_model.eval()\nloaded_model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_obj1 = MyLoader([('I found this movie which is good.',0),('one of the rubbish movie ever watched',1)])\nval_loader1=torch.utils.data.DataLoader(val_obj1,batch_size=batch_sz,collate_fn=my_collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (b_input_ids,abc,b_input_mask,lbl) in val_loader1:\n    outputs = loaded_model(b_input_ids.to(device), \n                            token_type_ids=None, \n                            attention_mask=b_input_mask.to(device))\n    \n    print(outputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}