{"cells":[{"metadata":{"id":"KNbL00c6WduD"},"cell_type":"markdown","source":"## Problem Overview\nEfforts to understand the influence of historical climate change, at global and regional levels, have been increasing over the past decade. In particular, the estimates of air temperatures have been considered as a key factor in climate impact studies on agricultural, ecological, environmental, and industrial sectors.\n\n![](https://dphi-courses.s3.ap-south-1.amazonaws.com/Datathons/temperature_change.png)\n\nMitigating climate change is one of the biggest challenges of humankind. Despite the complexity of predicting the effects of climate change on earth, there is a scientific consensus about its negative impacts. Among them, the affectation of ecosystems, decrease of biodiversity, soil erosion, extreme changes in temperature, sea level rise, and global warming have been identified. Likewise, impacts on the economy, human health, food security and energy consumption are expected.\n\nSpecifically, air temperature forecasting has been a crucial climatic factor required for many different applications in areas such as agriculture, industry, energy, environment, tourism, etc. Some of these applications include short-term load forecasting for power utilities, air conditioning and solar energy systems development, adaptive temperature control in greenhouses, prediction and assessment of natural hazards, and prediction of cooling and energy consumption in residential buildings. Therefore, there is a need to accurately predict temperature values because, in combination with the analysis of additional features in the subject of interest, they would help to establish a planning horizon for infrastructure upgrades, insurance, energy policy, and business development. [source of information: mdpi]\n\n## Objective\nBuild a Machine Learning model to predict the future temperature of the city.\n\n## Evaluation Criteria\nSubmissions are evaluated using the Root Mean Squared Error (RMSE).\n\n![](https://dphi-courses.s3.ap-south-1.amazonaws.com/Datathons/rmse+formula.png)\n\n"},{"metadata":{"id":"CWcX8sgfW9vF"},"cell_type":"markdown","source":"## About the Dataset\nThe train dataset contains the per day temperature values of the city - ‘XYZ’ for 30 years (1980 - 2010). The test set contains per day dates for the years 2011 - 2020 for which you will be predicting the temperature using the model you have built using the train dataset.\n\nYou can download the datasets from the given links:\n\n- train - You can download the train dataset from [here](https://dphi-courses.s3.ap-south-1.amazonaws.com/data-science-challenges/DTU_RoundTable/new_train.csv). Use this data to build machine learning models.\n- test - You can download the test dataset from [here](https://dphi-courses.s3.ap-south-1.amazonaws.com/data-science-challenges/DTU_RoundTable/new_test.csv). Predict the target values for this data using the model you built with the train data.\n- sample_submission: This is a csv file that contains the sample submission for the challenge. You can download the file from [here](https://dphi-courses.s3.ap-south-1.amazonaws.com/data-science-challenges/DTU_RoundTable/sample_submission.csv)."},{"metadata":{"id":"HkB1nBNwXUZZ"},"cell_type":"markdown","source":"## Data Description\n`date`: date on which the temperature was observed  \n`temp`: observed temperature on the given date\n\n"},{"metadata":{"id":"NPWJLN7hXcT-"},"cell_type":"markdown","source":"## What to Expect from this Notebook?\n\nThis notebook includes detailed exlanations and implementations of state-of-the-art Time Series Firecasting Algorithms. There are many variations of the algorithms, I have only shown a few. \n\nI have linked some great resources to most of the stuff in this notebook so that you can have loads of materials and examples to learn from. Some of these resources were even used by me to learn these concepts."},{"metadata":{"id":"_cLJAho-m99i"},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"id":"Y0STprYqm9dg","trusted":true},"cell_type":"code","source":"#Import Libraries\n\nimport pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-dark')\n\n#DateTime\nimport datetime as dt\n\n#Models\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\n\n#Sklearn\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n#Time to run Program\nimport time ","execution_count":null,"outputs":[]},{"metadata":{"id":"8W1wNztKmi3q"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"id":"aq1voJAOmXUm","trusted":true},"cell_type":"code","source":"def load_data():\n  '''\n  Function to Load the Train, Test and Submission Data\n\n  returns: train, test, submission dataframes\n  '''  \n\n  train = pd.read_csv('../input/d2c-climate-change-hackathon/new_train.csv')\n  test = pd.read_csv('../input/d2c-climate-change-hackathon/new_test.csv')\n  submission = pd.read_csv('../input/d2c-climate-change-hackathon/sample_submission.csv')\n\n  return train, test, submission","execution_count":null,"outputs":[]},{"metadata":{"id":"GaE1HEQhPhyn","trusted":true},"cell_type":"code","source":"#Declare Traget and Feature\nTARGET = 'temp'\nfeature = ['date']","execution_count":null,"outputs":[]},{"metadata":{"id":"ULOAYmTwPaCp","trusted":true},"cell_type":"code","source":"train, test, submission = load_data()","execution_count":null,"outputs":[]},{"metadata":{"id":"s1rrcooxnaps","outputId":"120fb9e1-14a2-4170-ae13-0b777f2a22d6","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"2TjtypcYneNe","outputId":"a41e31ab-9712-449f-f00b-43b98a96c02d","trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"c2LhCUPTngOn","outputId":"45623dbe-4b64-4bef-962d-ac65c972a7e9","trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"bFc0nHWInXzq"},"cell_type":"markdown","source":"## Functions"},{"metadata":{"id":"e1mfFJbxnVTY","trusted":true},"cell_type":"code","source":"#RMSE\ndef rmse():\n  y_pred = train.iloc[10000:11322, 2]\n  y = train.iloc[10000:11322, 0]\n  metric = np.sqrt(mean_squared_error(y, y_pred))\n  print(f\"RMSE of Data is: {metric}\")\n\n#Hackathon Metric\ndef predict(model, model_features):\n  pred_train = model.predict(X_train[model_features])\n  pred_val = model.predict(X_val[model_features])\n\n  print(f\"Train RMSE = {np.sqrt(mean_squared_error(y_train, pred_train))}\")\n  print(f\"Test RMSE = {np.sqrt(mean_squared_error(y_val, pred_val))}\")\n\ndef run_gradient_boosting(clf, fit_params, train, test, features):\n  N_SPLITS = 5\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  target = train[TARGET]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[TARGET], 10, labels = False, duplicates='drop')\n\n  feature_importances = pd.DataFrame()\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n\n    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\n    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = metric(y_val, preds_val)\n    print(f'\\nRMSE score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test / N_SPLITS\n\n\n  oofs_score = metric(target, oofs)\n  print(f'\\n\\nRMSE for oofs is {oofs_score}')\n\n  feature_importances = feature_importances.reset_index(drop = True)\n  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\n  fi.plot(kind = 'barh', figsize=(12, 6))\n\n  return oofs, preds, fi\n\ndef metric(y_true, y_pred):\n  return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef download_preds(preds_test, file_name = 'hacklive_sub.csv'):\n\n  ## 1. Setting the target column with our obtained predictions\n  submission['prediction'] = preds_test\n\n  ## 2. Saving our predictions to a csv file\n\n  submission.to_csv(file_name, index = False)\n\n  ## 3. Downloading and submitting the csv file\n  from google.colab import files\n  files.download(file_name)\n\n#Download Submission File\ndef download(model, model_features, file_name = 'prophet.csv'):\n\n  pred_test = model.predict(model_features)\n\n  #Setting the target column with our obtained predictions\n  submission['prediction'] = pred_test\n\n  #Saving our predictions to a csv file\n  submission.to_csv(file_name, index = False)\n  \n  #Downloadingthe csv file\n  files.download(file_name)\n\ndef join_df(train, test):\n\n  df = pd.concat([train, test], axis=0).reset_index(drop = True)\n  features = [c for c in df.columns if c not in [feature, TARGET]]\n  df[TARGET] = df[TARGET].apply(lambda x: np.log1p(x))\n\n  return df, features\n\ndef split_df_and_get_features(df, train_nrows):\n\n  train, test = df[:train_nrows].reset_index(drop = True), df[train_nrows:].reset_index(drop = True)\n  features = [c for c in train.columns if c not in [feature, TARGET]]\n  \n  return train, test, features","execution_count":null,"outputs":[]},{"metadata":{"id":"hx1wbTE_nsaO"},"cell_type":"markdown","source":"## EDA and Data Preprocessing"},{"metadata":{"id":"BjnhxR78qjpH","trusted":true},"cell_type":"code","source":"#Combine Train and Test Dataframe\ndf, features = join_df(train, test)","execution_count":null,"outputs":[]},{"metadata":{"id":"S54k5KOLPqK7","outputId":"2ddab94d-6bea-4e10-e105-c635f245d5bb","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"UsTjm49on2Na"},"cell_type":"markdown","source":"### Data Details"},{"metadata":{"id":"1oL9DRHzn5BO","outputId":"5294efe7-2f59-4ac4-b268-5e94075c9826","trusted":true},"cell_type":"code","source":"print(f\"train.shape: {train.shape}\")\nprint(f\"test.shape: {test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"VYqxLm6OoDih","outputId":"11821c39-2f2c-4e2f-9fb4-54c055bee726","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"wUcla5jyrpA1","outputId":"b47fc6de-e821-47b0-c6fb-20805f60e329","trusted":true},"cell_type":"code","source":"#Check Datatypes\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"q5Og71ZfPwm5"},"cell_type":"markdown","source":"Datatype of `date` is incorrect. It should be datetime. We will correct it in a later stage."},{"metadata":{"id":"UcPPi64eokYe"},"cell_type":"markdown","source":"### Null Values"},{"metadata":{"id":"ZXT305YRoNil","outputId":"2d383bca-26ef-497b-82a8-2a8d8a9e2b71","trusted":true},"cell_type":"code","source":"print(f\"Train Null Value Count: {train.isnull().sum()}\")\nprint(f\"Test Null Value Count: {test.isnull().sum()}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zJsfFYc9P71i"},"cell_type":"markdown","source":"Thus, Training Data does not have any null values."},{"metadata":{"id":"OnsmtAsCnww9"},"cell_type":"markdown","source":"### Target Distribution"},{"metadata":{"id":"sO1Y8BrwQB_c"},"cell_type":"markdown","source":"Let us check the distribution of the TARGET i.e., Temperature."},{"metadata":{"id":"seMl43Kknvq3","outputId":"564fc302-5b31-4e4a-8cd1-e2d90dba80f1","trusted":true},"cell_type":"code","source":"#Temperature Distribution\ntrain[TARGET].plot(kind = 'density', title = 'Temperature Distribution', fontsize=14, figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{"id":"jenNSYrapeyi","outputId":"8e7bf037-6c3e-4ccd-f25d-464f339efb4c","trusted":true},"cell_type":"code","source":"#Log Temperature Distribution\n_ = pd.Series(np.log1p(train[TARGET])).plot(kind = 'density', title = 'Log Temperature Distribution', fontsize=14, figsize=(10, 6))","execution_count":null,"outputs":[]},{"metadata":{"id":"lUTs0HwGpnvm","outputId":"9b68361b-93a2-475c-de36-fde2adabe943","trusted":true},"cell_type":"code","source":"#Temperature Boxplot\ntrain[TARGET].plot(kind = 'box', vert=False, figsize=(12, 4), title = 'Temperature Boxplot', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"id":"8sNuujjvprvt","outputId":"115fc276-0cb8-4980-c40a-6ffc8f2feb53","trusted":true},"cell_type":"code","source":"#Log Temperature BoxPlot\npd.Series(np.log1p(train[TARGET])).plot(kind = 'box', vert=False, figsize=(12, 4), title = 'Log Temperature Boxplot', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"id":"MpIlc9gUrmAZ"},"cell_type":"markdown","source":"## Date Feature"},{"metadata":{"id":"dy0pB7VRQdI6"},"cell_type":"markdown","source":"Now we shall create some features using the `date` column."},{"metadata":{"id":"Y5QAvXlSrwKA","outputId":"9d4e6802-2829-4b31-9630-343127c47e31","trusted":true},"cell_type":"code","source":"#Convert `date` column datatype to `datetime`\ndf['date'] = pd.to_datetime(df['date'])\n\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"JbD-6F86SjjQ","outputId":"d4f6ee5b-0c09-42c9-a190-7ac7ca0c9966","trusted":true},"cell_type":"code","source":"print(f\"Train Null Value Count: {train.isnull().sum()}\")\nprint(f\"Test Null Value Count: {test.isnull().sum()}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"GJylFlFFsRV4","outputId":"cd95972e-cf86-4664-ad76-1daab0a075bd","trusted":true},"cell_type":"code","source":"#Make basic datetime features\n# df['day_of_week'] = df['date'].dt.dayofweek\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['week'] = df['date'].dt.isocalendar().week  \n\n#Get Train and Test sets from df\ntrain, test, features = split_df_and_get_features(df, train.shape[0])\n\n#Define the features\nfeatures = [c for c in df.columns if c not in [feature, TARGET]]\nfeatures = features[1:]\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"id":"x4LlOthYYQrq"},"cell_type":"markdown","source":"There are many more functions in `datetime` library which you can try out for yourself. Check the [documentation](https://docs.python.org/3/library/datetime.html) for more such functions."},{"metadata":{"id":"e6e9wLaeuIkB","outputId":"8d445384-36b3-4e87-a0ef-1d95a7403ff7","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"vP6tS8puTxeD","trusted":true},"cell_type":"code","source":"train.fillna(np.mean(train['temp']), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Oixq3qzMrRWO"},"cell_type":"markdown","source":"## Model"},{"metadata":{"id":"QywQeocmrcqL","trusted":true},"cell_type":"code","source":"#Declare Features and Target from Training Dataset\nX = train[features]\ny = train[TARGET]\n\n#Split Training and Validation Datasets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"id":"W0hR9njXSGnD","outputId":"3c0e5241-f40f-494a-94ec-a66e187ed371","trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Uu40Oow-YrvC"},"cell_type":"markdown","source":"We begin with a simple Linear Regression baseline model and then move ahead with more complex algorithms."},{"metadata":{"id":"obG8eq1Q4o7t"},"cell_type":"markdown","source":"### Linear Regression\n\nLinear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n\nWhen there is a single input variable (x), the method is referred to as simple linear regression. When there are multiple input variables, literature from statistics often refers to the method as multiple linear regression.\n\nDifferent techniques can be used to prepare or train the linear regression equation from data, the most common of which is called Ordinary Least Squares. It is common to therefore refer to a model prepared this way as Ordinary Least Squares Linear Regression or just Least Squares Regression.\n\n**RESOURCES:**\n1. [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n2. [Linear Regression for Machine Learning](https://machinelearningmastery.com/linear-regression-for-machine-learning/#:~:text=Linear%20regression%20is%20a%20linear,the%20input%20variables%20(x).)\n3. [Linear Regression](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm)\n4. [Linear Regression Detailed View](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)\n5. [Linear Regression Analysis using SPPS Statistics](https://statistics.laerd.com/spss-tutorials/linear-regression-using-spss-statistics.php)\n6. [What is Linear Regression](https://www.statisticssolutions.com/what-is-linear-regression/?__cf_chl_jschl_tk__=3dc2b2683edb0777a0554f2afb84752a9d9e2ba2-1615132368-0-ASd1CvX_kwIZyocziRkwCHi7dmsfVurLvcRPIWFS5iKOECprgS1IJUetJqnAgimS2yijxcLIY7vRWIBshkoGno20f6Acx5Na3C_pkR0nP9WBd95Ma7v_arU9Owlh-LPuL5SofBxaHvN10g3d4xwKnMRtyNpaWGMOaQcCaZhPcls6UZhoI1gA0EXZoAe0yp5e2ULs0MirLFu_ezt07RgEAzTg1Od5LhTa585tKdIR0Y2ajxYfzpr9u35pdH26h05c8QS_e_3cEQh_iGPk1f8bmK3NHHYSUyvEv1wy1J8uLnB5XY3sHuMG2rCRuf-jBUQ7PDCPyLXlmi4by7bgw3n2UzaTY1u9sca3wI3uKlpb-8Tc)\n7. [Introduction to Linear Regression Analysis](http://people.duke.edu/~rnau/regintro.htm)"},{"metadata":{"id":"_HegucZtrQHm","outputId":"9da85362-dbde-4cf1-fe5e-e3c9a20130d1","trusted":true},"cell_type":"code","source":"#Linear Regression\nmodel = LinearRegression()\n\nmodel.fit(X_train[features], y_train)\n\npredict(model, features)","execution_count":null,"outputs":[]},{"metadata":{"id":"NY4Ncgds4xjK"},"cell_type":"markdown","source":"### LGBMRegressor\n\nLightGBM short for Light Gradient Boosted Machine, is a library developed at Microsoft that provides an efficient implementation of the gradient boosting algorithm.\n\nThe primary benefit of the LightGBM is the changes to the training algorithm that make the process dramatically faster, and in many cases, result in a more effective model.\n\nFor more technical details on the LightGBM algorithm, see the paper:\n\n- [LightGBM: A Highly Efficient Gradient Boosting Decision Tree, 2017](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree).\n\n**RESOURCES:**\n1. [Documentation](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html)\n2. [How to use LightGBM](https://www.dezyre.com/recipes/use-lightgbm-classifier-and-regressor-in-python)\n3. [What is LightGBM, How to implement it? How to fine tune the parameters?](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)\n4. [Python Examples of LightGBM](https://www.programcreek.com/python/example/88794/lightgbm.LGBMRegressor)\n5. [Understanding LightGBM Parameters](https://neptune.ai/blog/lightgbm-parameters-guide)\n6. [Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost](https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/)"},{"metadata":{"id":"raEIa10-4zVe","outputId":"41893dd7-5e5a-41e1-b3fa-e2cf64150654","trusted":true},"cell_type":"code","source":"model = LGBMRegressor(n_estimators = 5000,\n                        learning_rate = 0.01,\n                        colsample_bytree = 0.76,\n                        metric = 'None',\n                        )\nfit_params = {'verbose': 300, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n\nlgb_oofs, lgb_preds, fi = run_gradient_boosting(clf = model, fit_params = fit_params, train = train, test = test, features = features)","execution_count":null,"outputs":[]},{"metadata":{"id":"Hhc0yPelPfcr"},"cell_type":"markdown","source":"## Time Series Forecasting using ARIMA and SARIMAX\n\nARIMA, short for ‘Auto Regressive Integrated Moving Average’ is actually a class of models that ‘explains’ a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.\n\nAny ‘non-seasonal’ time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models.\n\nAn ARIMA model is characterized by 3 terms: p, d, q\n\nwhere,\n\np is the order of the AR term\n\nq is the order of the MA term\n\nd is the number of differencing required to make the time series stationary\n\nIf a time series, has seasonal patterns, then you need to add seasonal terms and it becomes SARIMA, short for ‘Seasonal ARIMA’.\n\n**RESOURCES:**\n\n1. [Documentation](https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima_model.ARIMA.html)\n2. [Forecasting Future Sales using ARIMA and SARIMAX](https://www.youtube.com/watch?v=2XGSIlgUBDI)\n3. [ARIMA for Time Series Forecasting in Python](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/#:~:text=ARIMA%20is%20an%20acronym%20that,structures%20in%20time%20series%20data.) \n4. [Autoregressive Integrated Moving Average](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)\n5. [ARIMA Model – Complete Guide to Time Series Forecasting in Python](https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/)\n6. [Introduction to ARIMA](https://people.duke.edu/~rnau/411arim.htm)\n7. [Autoregressive Integrated Moving Average (ARIMA)](https://www.investopedia.com/terms/a/autoregressive-integrated-moving-average-arima.asp)\n8. [Time Series Forecasting - ARIMA Models](https://towardsdatascience.com/time-series-forecasting-arima-models-7f221e9eee06)\n9. [Understanding ARIMA Time Series Modelling](https://towardsdatascience.com/understanding-arima-time-series-modeling-d99cd11be3f8)\n10. [How to Create an ARIMA Model for Time Series Forecasting in Python](https://www.analyticsvidhya.com/blog/2020/10/how-to-create-an-arima-model-for-time-series-forecasting-in-python/)\n11. [Time Series ARIMA Models](https://sites.google.com/site/econometricsacademy/econometrics-models/time-series-arima-models)\n"},{"metadata":{"id":"Q4uEIrTNOQ0x"},"cell_type":"markdown","source":"**Autoregressive Integrated Moving Averages**  \n\nThe general process is as follows:  \n- Visualize the time series data\n- Make the time series data stationary\n- Plot the Correlation and Autocorrelation charts\n- Use the model to make predictions"},{"metadata":{"id":"vmRgkeWFQytk"},"cell_type":"markdown","source":"### Preprocess Data"},{"metadata":{"id":"rxjSpTw7P61t","trusted":true},"cell_type":"code","source":"#Load Data\ntrain, test, submission = load_data()","execution_count":null,"outputs":[]},{"metadata":{"id":"O27KzvCjPjT3","trusted":true},"cell_type":"code","source":"#Convert `date` column to datetime\ntrain.date = pd.to_datetime(train.date)","execution_count":null,"outputs":[]},{"metadata":{"id":"b_ZbI8jrO7GZ","trusted":true},"cell_type":"code","source":"#Set `date` as index\ntrain.set_index('date', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"JRMPmH-JP-5y","outputId":"9a2d793d-e6e7-4a93-df45-be2c092521a9","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Sd7O_ASgQJVU","outputId":"b7973d5c-53be-48ae-ed01-9c0d77209949","trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"z3c9iz3MQ1SD"},"cell_type":"markdown","source":"### Visualize Data"},{"metadata":{"id":"B6YBv6dwQUFv","outputId":"b816f427-e4de-45ee-9c6f-075012c1f343","trusted":true},"cell_type":"code","source":"train.plot(figsize = (20, 10))","execution_count":null,"outputs":[]},{"metadata":{"id":"qwAVMKvDQnSn"},"cell_type":"markdown","source":"From the plot we observe that the data is seasonal."},{"metadata":{"id":"ni-vvQIdQs0b"},"cell_type":"markdown","source":"### Make Data Stationary"},{"metadata":{"id":"PNQxbClYUm6e"},"cell_type":"markdown","source":"To check if the data is stationary, we perform the `adfuller` test."},{"metadata":{"id":"peYbq3DdQZzo","trusted":true},"cell_type":"code","source":"#Import adfuller test\nfrom statsmodels.tsa.stattools import adfuller","execution_count":null,"outputs":[]},{"metadata":{"id":"DnvHOYlVRG6W","trusted":false},"cell_type":"code","source":"# test_result = adfuller(train.temp)","execution_count":null,"outputs":[]},{"metadata":{"id":"PTLoUeiWRQzb","trusted":true},"cell_type":"code","source":"#H0: It is not stationary\n#H1: It is stationary\n\ndef adfuller_test(temp):\n    result=adfuller(temp)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")","execution_count":null,"outputs":[]},{"metadata":{"id":"bJeK2fz6UzqX","outputId":"f784832c-e2c0-486d-9983-2410d519fae3","trusted":true},"cell_type":"code","source":"adfuller_test(train.temp)","execution_count":null,"outputs":[]},{"metadata":{"id":"uRRIs6giU-zs"},"cell_type":"markdown","source":"Since data is already stationary, we do not need to perform differencing and can set `d=0` directly."},{"metadata":{"id":"l4Agt6paU28F","outputId":"9568dd58-20a3-4e79-8763-986523658573","trusted":true},"cell_type":"code","source":"train['Seasonal First Difference']=train['temp']-train['temp'].shift(12) #Because 1 year has 12 months\n\n## Again test dickey fuller test\nadfuller_test(train['Seasonal First Difference'].dropna())\n\ntrain['Seasonal First Difference'].plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"GBWPvrjlWK1R"},"cell_type":"markdown","source":"## Auto Regressive Model\n\n**Autocorrelation and Partial Autocorrelation**  \n\n- Identification of an AR model is often best done with the PACF.\n - For an AR model, the theoretical PACF “shuts off” past the order of the model. The phrase “shuts off” means that in theory the partial autocorrelations are equal to 0 beyond that point. Put another way, the number of non-zero partial autocorrelations gives the order of the AR model. By the “order of the model” we mean the most extreme lag of x that is used as a predictor.\n - Look for sudden drop\n\n- Identification of an MA model is often best done with the ACF rather than the PACF.\n - For an MA model, the theoretical PACF does not shut off, but instead tapers toward 0 in some manner. A clearer pattern for an MA model is in the ACF. The ACF will have non-zero autocorrelations only at lags involved in the model.\n - Look for exponential drop"},{"metadata":{"id":"wW0BBG7fXr36"},"cell_type":"markdown","source":"**Parameters for ARIMA**\n1. p - Autoregressive (AR) Model Lags - Use PACF\n2. d - No. of times differencing performed\n3. q - Moving Average (MA) Lags - Use ACF"},{"metadata":{"id":"B0qqc4kNWe7n","trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_pacf ,plot_acf","execution_count":null,"outputs":[]},{"metadata":{"id":"yjP_gx7KYPeU","outputId":"5bfd0e23-b52b-43ec-dee3-1810d4aae3d9","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (12, 8))\nax1 = fig.add_subplot(211)\nfig = plot_pacf(train['Seasonal First Difference'].iloc[13:],lags=40,ax=ax1)\nax2 = fig.add_subplot(212)\nfig = plot_acf(train['Seasonal First Difference'].iloc[13:],lags=40,ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"id":"9UOeFQYicuiZ"},"cell_type":"markdown","source":"- From PACF sudden drop is observed at 2 so `p=2`.\n\n- From ACF exponential drop is till 10 so `q=10`"},{"metadata":{"id":"I3yxfw6OcHfh","outputId":"bf61b884-3922-4921-b413-0975d7ad4032","trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel=ARIMA(train['temp'],order=(2,0,2))\nmodel_fit=model.fit()\n\nmodel_fit.summary()\n\ntrain['forecast']=model_fit.predict(start=10000,end=11321,dynamic=True)\ntrain[['temp','forecast']].plot(figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"id":"EuCPCYvLeggI"},"cell_type":"markdown","source":"The forecast is poor because data is seasonal, so we need to use `SARIMAX`"},{"metadata":{"id":"PmPrwWDaecNe","trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"id":"dWLIWyiJepaP","trusted":false},"cell_type":"code","source":"#Start time \nbegin = time.time() \n\nmodel=sm.tsa.statespace.SARIMAX(train['temp'],order=(2, 1, 2),seasonal_order=(2, 1, 2, 12))\nresults=model.fit()\n\n#End TIme\nend = time.time()\nprint(f\"\\n\\nTime of execution = {end - begin}\")\n\n#Forecast\ntrain['forecast']=results.predict(start=10000,end=11321,dynamic=True)\ntrain[['temp','forecast']].plot(figsize=(12,8))\n\nrmse()","execution_count":null,"outputs":[]},{"metadata":{"id":"CVikEEmLfZUA","trusted":false},"cell_type":"code","source":"df = pd.concat([train, test])\ndf['forecast'] = results.predict(start = 11322, end = 14883, dynamic= True)  \ndf[['temp', 'forecast']].plot(figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{"id":"a1R8naA1dmjG","trusted":false},"cell_type":"code","source":"rmse()","execution_count":null,"outputs":[]},{"metadata":{"id":"0ZhiS7jNFJPY"},"cell_type":"markdown","source":"## FBProphet"},{"metadata":{"id":"WnGQOeoveAaz"},"cell_type":"markdown","source":"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nProphet is open source software released by Facebook's Core Data Science team .\n\n**RESOURCES:**\n1. [Documentation](https://pypi.org/project/fbprophet/)\n2. [Quick Start](https://facebook.github.io/prophet/docs/quick_start.html)\n3. [Hacking Time-Series Forecasting Like a Pro with FBProphet](https://medium.com/tokopedia-data/hacking-time-series-forecasting-like-a-pro-with-fbprophet-76f276f0a058)\n4. [Time Series Forecasting With Prophet in Python](https://machinelearningmastery.com/time-series-forecasting-with-prophet-in-python/)\n5. [A Quick Start of Time Series Forecasting with a Practical Example using FB Prophet](https://towardsdatascience.com/a-quick-start-of-time-series-forecasting-with-a-practical-example-using-fb-prophet-31c4447a2274)\n6. [FBProphet TowardsDataScience](https://towardsdatascience.com/a-quick-start-of-time-series-forecasting-with-a-practical-example-using-fb-prophet-31c4447a2274)\n7. [Generate Quick and Accurate Time Series Forecasts using Facebook’s Prophet (with Python & R codes)](https://www.analyticsvidhya.com/blog/2018/05/generate-accurate-forecasts-facebook-prophet-python-r/)"},{"metadata":{"id":"Slr5uB9IYbHd","trusted":true},"cell_type":"code","source":"def prophet_rmse(y_true, y_pred):\n\n  y_true, y_pred = np.array(y_true), np.array(y_pred)\n  return np.sqrt(mean_squared_error(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"vX-jLphPdnwR","trusted":true},"cell_type":"code","source":"import fbprophet\nfrom fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"id":"hIjkSq8vFTqE","trusted":true},"cell_type":"code","source":"#Load Data\ntrain, test, submission = load_data()\ntrain['date'] = pd.to_datetime(train['date'])\n\n#Update column names\ntrain.columns = ['ds', 'y']","execution_count":null,"outputs":[]},{"metadata":{"id":"DbfGwxKfm9te","trusted":true},"cell_type":"code","source":"END_DATE = '2009'\nX_train = train[train['ds'] <= END_DATE]\nX_test = train[train['ds'] >= END_DATE]","execution_count":null,"outputs":[]},{"metadata":{"id":"WBa_yvrKm18R","outputId":"fe231ee6-33fc-4242-c1d6-99be13bf84b9","trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"xr0sj0sRpMvt","trusted":true},"cell_type":"code","source":"#Add holidays to model\n\nimport holidays\n\n\nholiday = pd.DataFrame([])\nfor date, name in sorted(holidays.UnitedStates(years=[2018,2019,2020]).items()):\n    holiday = holiday.append(pd.DataFrame({'ds': date, 'holiday': \"US-Holidays\"}, index=[0]), ignore_index=True)\nholiday['ds'] = pd.to_datetime(holiday['ds'], format='%Y-%m-%d', errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{"id":"sq2bJaLE0n6s","trusted":true},"cell_type":"code","source":"# #Box Cox Transformation\n# from scipy.stats import boxcox\n\n# # Apply Box-Cox Transform to value column and assign to new column y\n# train['y'] = train['y'] + 5.12 #Make Data Positive\n# train['y'], lam = boxcox(train['y'])","execution_count":null,"outputs":[]},{"metadata":{"id":"1ZnmX0tZ1Jwe","outputId":"6163680d-0167-4359-a448-ec749646fe8a","trusted":true},"cell_type":"code","source":"# train['y'].min()","execution_count":null,"outputs":[]},{"metadata":{"id":"3u_d-MCOFhmJ","outputId":"4ef10a5b-fd12-45b2-8fc0-2947e51fe013","trusted":true},"cell_type":"code","source":"#Initialize model\nmodel = Prophet(growth = 'linear', \n                  seasonality_mode = 'multiplicative',  \n                  changepoint_prior_scale = 30,\n                  seasonality_prior_scale = 15,\n                  )\n\n#Fit Model\nmodel.fit(X_train)\n\nprediction = model.predict(X_test[['ds']])\nrmse = np.sqrt(mean_squared_error(X_test['y'], prediction['yhat']))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"id":"9Z1GplZ7Zff6","trusted":false},"cell_type":"code","source":"# from fbprophet.diagnostics import cross_validation\n# cv_results = cross_validation(model, initial='730 days', period='180 days', horizon = '365 days')\n# cv_results.head()\n\n#Calculate RMSE\n# prophet_rmse(cv_results.y, cv_results.yhat)","execution_count":null,"outputs":[]},{"metadata":{"id":"UAaQSs_xJ9xJ","outputId":"c69fa5e8-461a-42d2-ce33-f0c25bee0b48","trusted":true},"cell_type":"code","source":"#Alternate way to calculate rmse (without using cv)\nprediction = model.predict(train[['ds']])\n# model.plot(prediction);\n# model.plot_components(prediction);\n\nrmse = np.sqrt(mean_squared_error(train['y'], prediction['yhat']))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"id":"zE2bAKmEK16e"},"cell_type":"markdown","source":"### Inference"},{"metadata":{"id":"ur1cHy77GwfG","trusted":true},"cell_type":"code","source":"def submit(model, file_name = 'prophet.csv'):\n\n  #Load Test Data\n  test = pd.read_csv('/content/drive/MyDrive/Data Science/dare2compete/DTU RoundHacks Data Science Hackathon/new_test.csv')\n\n  #Convert `date` to datetime\n  test['date'] = pd.to_datetime(test['date'])\n\n  #Update column names\n  test.columns = ['ds']\n\n  #Make Predictions\n  test_preds = model.predict(test)\n\n  #Inverse Box Transform\n  # from scipy.special import inv_boxcox\n\n  #Apply inverse Box-Cox transform to specific forecast columns\n  # test_preds[['yhat','yhat_upper','yhat_lower']] = test_preds[['yhat','yhat_upper','yhat_lower']].apply(lambda x: inv_boxcox(x, lam))\n\n  #Inverse Data Addition\n  # test_preds['yhat'] = test_preds['yhat'] - 5.12\n\n  #Add prediction to submission file\n  submission['prediction'] = test_preds['yhat']\n\n  #Saving our predictions to a csv file\n  submission.to_csv(file_name, index = False)\n    \n  #Downloadingthe csv file\n  files.download(file_name)     #Works with Google Colab","execution_count":null,"outputs":[]},{"metadata":{"id":"eY6CRw2ZqaHh"},"cell_type":"markdown","source":"### Hyperparameter Tuning using Parameter Grid"},{"metadata":{"id":"aBcOAuFagVLo","outputId":"d97ef783-5e1b-4878-befd-83f2088cde5d","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid\nparams_grid = {'seasonality_mode':('multiplicative','additive'),\n               'changepoint_prior_scale':[0.1,0.2,0.3,0.4,0.5],\n              'holidays_prior_scale':[0.1,0.2,0.3,0.4,0.5],\n              'n_changepoints' : [100,150,200]}\ngrid = ParameterGrid(params_grid)\ncnt = 0\nfor p in grid:\n    cnt = cnt+1\n\nprint('Total Possible Models',cnt)","execution_count":null,"outputs":[]},{"metadata":{"id":"P_r0loBKqtJR"},"cell_type":"markdown","source":"### Prophet Model Tuning"},{"metadata":{"id":"iQ1Nd10Wr1cO","trusted":true},"cell_type":"code","source":"import random","execution_count":null,"outputs":[]},{"metadata":{"id":"jJbdmf_Aqiqr","outputId":"7f2fbada-24dd-4bda-e7e5-3b2d40e29577","trusted":false},"cell_type":"code","source":"strt='1980-01-01'\nend='2009-01-01'\n\nmodel_parameters = pd.DataFrame(columns = ['RMSE','Parameters'])\ni = 0\n\nfor p in grid:\n    test_tune = pd.DataFrame()\n    print(f\"Iteration: {i+1}/150\")\n    print(p)\n    random.seed(0)\n\n    train_model =Prophet(changepoint_prior_scale = p['changepoint_prior_scale'],\n                         holidays_prior_scale = p['holidays_prior_scale'],\n                         n_changepoints = p['n_changepoints'],\n                         seasonality_mode = p['seasonality_mode'],\n                         weekly_seasonality=True,\n                         daily_seasonality = True,\n                         yearly_seasonality = True,\n                         holidays=holiday, \n                         interval_width=0.95)\n    \n    train_model.add_country_holidays(country_name='US')\n    train_model.fit(X_train)\n\n    # train_forecast = train_model.make_future_dataframe(periods=57, freq='D',include_history = False)\n    # train_forecast = train[train['ds']>end]\n    # train_forecast = train_model.predict(train_forecast[])\n\n    train_forecast = train_model.predict(X_test)\n    test_tune=train_forecast[['ds','yhat']]\n    # Actual = train[(train['ds']>strt) & (train['ds']<=end)]\n    Actual = X_test\n\n    # print(Actual['y'].shape,test_tune['yhat'].shape)\n\n    rmse = np.sqrt(mean_squared_error(Actual['y'],test_tune['yhat']))\n    print('RMSE------------------------------------',rmse)\n    model_parameters = model_parameters.append({'RMSE':rmse,'Parameters':p},ignore_index=True)\n    print()\n\n    i = i+1\n\n    # break","execution_count":null,"outputs":[]},{"metadata":{"id":"PappT644m6xk","outputId":"0fa97572-fc79-4c0a-aef4-42548ce94bb2","trusted":false},"cell_type":"code","source":"parameters = model_parameters.sort_values(by=['RMSE'])\nparameters = parameters.reset_index(drop=True)\nparameters.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"AOpHNqnJnEuf","outputId":"36bcc8a3-8c29-461d-dad1-be6f3267efbf","trusted":false},"cell_type":"code","source":"parameters['Parameters'][0]","execution_count":null,"outputs":[]},{"metadata":{"id":"dMjIM_s4nKsY","outputId":"35a86cfb-463d-4415-861e-9f594b85aa58","trusted":false},"cell_type":"code","source":"# Setup and train model with holidays\nfinal_model = Prophet(holidays=holiday,\n                      changepoint_prior_scale= 0.1,\n                      holidays_prior_scale = 0.1,\n                      n_changepoints = 150,\n                      seasonality_mode = 'multiplicative',\n                      weekly_seasonality=True,\n                      daily_seasonality = True,\n                      yearly_seasonality = True,\n                      interval_width=0.95)\nfinal_model.add_country_holidays(country_name='US')\nfinal_model.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"jMP1KVkrnukm","outputId":"0466d1d2-5ba5-4211-eb1b-95214ea50b20","trusted":false},"cell_type":"code","source":"tuned_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"r2ycKqa1nZcp","outputId":"2b5d81f8-7f69-42ce-8f4f-695696611052","trusted":false},"cell_type":"code","source":"tuned_pred = final_model.predict(X_test)\nprophet_rmse(tuned_pred['yhat'], X_test['y'])","execution_count":null,"outputs":[]},{"metadata":{"id":"Rk5t06chZw7M"},"cell_type":"markdown","source":"## TBATS"},{"metadata":{"id":"O6eytkx6fFiu"},"cell_type":"markdown","source":"There are two interesting time series forecasting methods called BATS and TBATS that are capable of modeling time series with multiple seasonalities.\nThe names are acronyms for key features of the models: Trigonometric seasonality, Box-Cox transformation, ARMA errors, Trend and Seasonal components.\nTBATS model takes it roots in exponential smoothing methods and can be described by the following equations:\n![](https://miro.medium.com/max/700/1*yfruZnSYsNt1X1uyag46eQ.png)\n\nEach seasonality is modeled by a trigonometric representation based on Fourier series. One major advantage of this approach is that it requires only 2 seed states regardless of the length of period. Another advantage is the ability to model seasonal effects of non-integer lengths. For example, given a series of daily observations, one can model leap years with a season of length 365.25.\nBATS differs from TBATS only in the way it models seasonal effects. In BATS we have a more traditional approach where each seasonality is modeled by:\n![](https://miro.medium.com/max/700/1*D8KY2jQRmVRLHY5f4RNqKA.png)\n\nThis implies that BATS can only model integer period lengths. Approach taken in BATS requires m_i seed states for season i, if this season is long the model may become intractable.\n\n**RESOURCES:**\n1. [Documentation](https://pypi.org/project/tbats/)\n2. [Forecasting Time Series with Multiple Seasonalities using TBATS in Python](https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a)\n3. [Time-Series Forecasting using TBATS model](https://blog.tenthplanet.in/time-series-forecasting-tbats/)\n"},{"metadata":{"id":"c1WyMniFaCV6","trusted":false},"cell_type":"code","source":"!pip install tbats","execution_count":null,"outputs":[]},{"metadata":{"id":"uHyG-k7LahbN","trusted":false},"cell_type":"code","source":"train, test, submission = load_data()\ntrain['date'] = pd.to_datetime(train['date'])\n\nEND_DATE = '2009'\nX_train = train[train['date'] <= END_DATE]\nX_test = train[train['date'] >= END_DATE]","execution_count":null,"outputs":[]},{"metadata":{"id":"UlQI3Zxxa6GC","outputId":"9c28b314-4f0b-46d5-a093-303706c14285","trusted":false},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"IISDKdwFbTV5","outputId":"8319421f-9747-46ae-c63a-4a79cc0cc77b","trusted":false},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"cBftbN84b8U1","trusted":false},"cell_type":"code","source":"X_train.set_index('date', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"pvICFDDCcDcr","trusted":false},"cell_type":"code","source":"X_test.set_index('date', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"gMKLKr-7aFIC","trusted":false},"cell_type":"code","source":"from tbats import TBATS, BATS\n\nestimator = TBATS(seasonal_periods = (7, 365.25)  )\nmodel = estimator.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"3vIEtU_Yf3Dv"},"cell_type":"markdown","source":"## End Notes\n\nThere are many more methods that you may try out such as using XGBRegressor, LSTM, etc. For me FBProphet worked best so I worked more on it.\n\nARIMA and TBATS are computationally expensive, so be careful while implementing them. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}