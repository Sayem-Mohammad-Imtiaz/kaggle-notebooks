{"cells":[{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Library imports\nimport re\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nimport tensorflow as tf\n\n# Reading the Data\ndata=pd.read_csv('../input/TechCrunch.csv',sep=',',error_bad_lines=False,encoding='ISO-8859-1')\n\n# Cleaning the Data\ndata['title']=data['title'].map(lambda x:re.sub(r'[^\\x00-\\x7F]+',' ',x))\ndata['url']=data['url'].map(lambda x:re.sub(r'[^\\x00-\\x7F]+',' ',x))\n\n# ANALYSIS 1\n# Create a graph of all the related elements with the size of the node denoting the importance\n# and edges denoting the relationship with others\\\n\ndef combineProperNouns(a):\n    y=0\n    while y <= len(a)-2:\n        if(a[y][0].isupper()==True and a[y+1][0].isupper()==True):\n            a[y]=str(a[y]) + '+' + str(a[y+1])\n            a[y+1:]=a[y+2:]\n        else:\n            y=y+1\n    return(a)\n\ndef recreateDataWithCombinedProperNouns(data):\n    tempData=[]\n    for x in data.split('.'):\n        tempPhrase=[]\n        for y in x.split(','):\n            z=y.split(' ')\n            z=[a for a in z if len(a) > 0]\n            tempPhrase.append(' '.join(combineProperNouns(z)))\n        tempData.append(','.join(tempPhrase))\n    data='.'.join(tempData)\n    return(data)\n\ndef removeDotsFromAcronyms(data):\n    counter=0\n    while counter < len(data) -2:\n        if(data[counter]=='.' and data[counter+2]=='.'):\n            #print(\"######{}#####{}#######{}####\".format(counter,data[counter-1:counter+3],data[counter+1]))\n            data=data[:counter] + str(data[counter+1]) + ' ' + data[counter+3:]\n            counter=counter+1\n        elif(data[counter]=='.' and data[counter-1].isupper()==True):\n            #print(\"####{}####\".format(data[counter-1:counter+1]))\n            data=data[:counter] + data[counter+1:]\n        else:\n            counter=counter+1\n    return(data)\n\n# Stemming and Lemmatizing the data\ndef stemAndLemmatize(data,columnNames):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    porter_stemmer=PorterStemmer()\n    for columnName in columnNames:\n        data[columnName]=data[columnName].map(lambda x: ' '.join([porter_stemmer.stem(y) for y in x.split(' ')]))\n        data[columnName]=data[columnName].map(lambda x: ' '.join([wordnet_lemmatizer.lemmatize(y) for y in x.split(' ')]))\n    return(data)\n\n\n\ndata['newTitle']=data['title'].map(lambda x:recreateDataWithCombinedProperNouns(x))\ndata=stemAndLemmatize(data,['title'])\ndata['newTitle']=data['newTitle'].map(lambda x: ' '.join([ y for y in x.split(' ') if nltk.pos_tag(y.split())[0][1] not in ['DT','IN','PDT','TO'] ]))\ndata['newTitle']=data['newTitle'].map(lambda x: ' '.join([ y for y in x.split(' ') if len(y) > 1]))\n#strData=recreateDataWithCombinedProperNouns(strData)\n","metadata":{"collapsed":true,"_uuid":"9099ef985d9f4479b9d2ddd5b61f050d08ab770e","_cell_guid":"42749bc7-d543-4b52-b9f6-eca6f614a292"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# We need access to certain bigrams from the internet\n# This will be another problem statement\n\n# Noun verb relationship\n# For each line\n# a) Get the Noun or consecutive Nouns\n# b) Get the adjective just before and after the NOUN and attach it to that Noun\n# c) Get the Verb\n# d) Get the adverb just before and after the Verb and attach it to that Verb\n\n# We will run a word2vec using tensorflow and check the related words that will come out\ntagList=['NNS','NNP','NNPS','VB','VBD','VBG']\ndata['newTitle']=data['newTitle'].map(lambda x: ' '.join([ y for y in x.split(' ') if nltk.pos_tag(y.split())[0][1] in tagList ]))\nwordList=set([y  for x in data['newTitle'].values for y in x.split(' ')])\n#words=[y  for x in data['newTitle'].values for y in x.split(' ')]\n#words=set(words)\nprint(\"The number of words are {}\".format(len(wordList)))\n# Number of unique words\nvocab_size=len(wordList)\n\nword2int={}\nint2word={}\n\nfor i,word in enumerate(wordList):\n    word2int[word]=i\n    int2word[word]=i\n\nwords=[]\nWINDOW_SIZE=2\nfor sentence in data['newTitle'].values:\n    newSentence=sentence.split(' ')\n    for word_index,word in enumerate(newSentence):\n        for nb_word in newSentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(newSentence)) + 1]: \n            if nb_word != word:\n                words.append([word,nb_word])\n\ndef to_one_hot(data_point_index, vocab_size):\n    temp = np.zeros(vocab_size)\n    temp[data_point_index] = 1\n    return temp\n\nx_train = [] # input word\ny_train = [] # output word\nfor data_word in words:\n    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n# convert them to numpy arrays\nx_train = np.asarray(x_train)\ny_train = np.asarray(y_train)\n\n# Tensorflow Model\nx = tf.placeholder(tf.float32, shape=(None, vocab_size))\ny_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\nEMBEDDING_DIM = 5 # you can choose your own number\nW1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\nb1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\nhidden_representation = tf.add(tf.matmul(x,W1), b1)\nW2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\nb2 = tf.Variable(tf.random_normal([vocab_size]))\nprediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))    ","metadata":{"_uuid":"e08bea27f4b0719bff002ca834af0ceabbdb7524","_cell_guid":"739b81c5-0e05-4d1f-8b2b-95a1ef9e21ce"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init) #make sure you do this!\n# define the loss function:\ncross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n# define the training step:\ntrain_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\nn_iters = 10\n\nprint(\"We will start training now\")\n\n# train for n_iter iterations\nfor _ in range(n_iters):\n    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# We will get the intermediate representation\nvectors = sess.run(W1 + b1)\n\nfrom sklearn.manifold import TSNE\nmodel = TSNE(n_components=2, random_state=0)\nnp.set_printoptions(suppress=True)\nvectors = model.fit_transform(vectors)\n\nfrom sklearn import preprocessing\nnormalizer = preprocessing.Normalizer()\nvectors =  normalizer.fit_transform(vectors, 'l2')","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"wordList=list(set([y  for x in data['newTitle'].values for y in x.split(' ')]))\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(10,5))\nfor word in wordList[0:100]:\n    #print(word, vectors[word2int[word]][1])\n    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\nplt.show()\n\nfig, ax = plt.subplots(figsize=(10,5))\nfor word in wordList[100:200]:\n    #print(word, vectors[word2int[word]][1])\n    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\nplt.show()\n","metadata":{}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}