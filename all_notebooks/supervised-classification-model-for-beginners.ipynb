{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello everyone\n\ntoday I will do homework about supervised classification algorithms. Firstly, I have to choose good dataset, I guess hotel-booking-demand is good dataset for classification models.\n\nLet's look nearly this dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#read data\ndata=pd.read_csv(\"../input/youtube-new/USvideos.csv\")\n#Datamızın bilgilerine bakalım \ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Continue data analyze, let's look correlation "},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.describe()\ndata.corr()\nf,ax = plt.subplots(figsize=(8,8)) #this command create 8x8 subplot\nsns.heatmap(data.corr(),annot=True, linewidths=1, fmt= '.1f',ax=ax) #user seaborn library for virtualization\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"there is 0.8 corr with views and likes.\n\nI will do lineer regression sklearn model and I will look accurancy my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#sklearn library \nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nlinear_reg = LinearRegression();\n\nx = data.views.values.reshape(-1,1)\ny = data.likes.values.reshape(-1,1)\n\nlinear_reg.fit(x, y)\nplt.scatter(x, y)\n\ny_head = linear_reg.predict(x)\nplt.plot(x, y_head, color = \"red\")\n\nplt.xlabel('Views')\nplt.ylabel('likes')\nplt.show()\n\nprint('Accuracy : lineer regression score :{} %'.format(linear_reg.score(x, y)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multiple lineer regression\n\nx = views and comments\ny = likes\n\nand result is wonderful, accuracy = %84   "},{"metadata":{"trusted":true},"cell_type":"code","source":"#sklearn library \nfrom sklearn.linear_model import LinearRegression\n\n#linear regression model\nmultiple_linear_reg = LinearRegression();\n\n#x = data.views.values.reshape(-1,1)\nx = data.iloc[:,[7,10]].values\n\ny = data.likes.values.reshape(-1,1)\n\nmultiple_linear_reg.fit(x, y)\n\nprint('Accuracy : multiple lineer regression score :{} %'.format(multiple_linear_reg.score(x, y)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Polynomial Lineer regression\n\nour data is not fit polynomial regression model because our data proportional, I can understand . You can see this graphic."},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%\n# polynomial regression =  y = b0 + b1*x +b2*x^2 + b3*x^3 + ... + bn*x^n\n\nfrom sklearn.preprocessing import PolynomialFeatures\npolynomial_regression = PolynomialFeatures(degree = 2)\n\nx_polynomial = polynomial_regression.fit_transform(x)\n\n\n# %% fit\nlinear_regression2 = LinearRegression()\nlinear_regression2.fit(x_polynomial,y)\n\n# %%\n\ny_head2 = linear_regression2.predict(x_polynomial)\n\nplt.plot(x,y_head2,color= \"green\",label = \"poly\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will classification algorithm, decision tree, random forest, knn algorithms so I need another dataset.\n\nI think Red Wine Quality dataset really good for classification samples.\n\nlet's look this dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n#read data\ndata_redwine=pd.read_csv(\"../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\n#Datamızın bilgilerine bakalım \ndata_redwine.info()\ndata_redwine.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_redwine['quality'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"our classification quality,\n\nlet's look correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data.describe()\ndata_redwine.corr()\nf,ax = plt.subplots(figsize=(12,12)) #this command create 8x8 subplot\nsns.heatmap(data_redwine.corr(),annot=True, linewidths=1, fmt= '.1f',ax=ax) #user seaborn library for virtualization\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = data_redwine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Composition of citric acid go higher as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = data_redwine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Composition of chloride also go down as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = data_redwine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx = data_redwine.drop('quality', axis = 1)\ny = data_redwine['quality']\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.4,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use Decision Tree Classifier model, and we will look our score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nreg = DecisionTreeClassifier()\n\nreg.fit(x_train,y_train)\nprint(\"Accuracy Score : Decision Tree classification {}\".format(reg.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use Random Forest Classifier model, and we will look our score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier#RandomForestRegressor\nreg = RandomForestClassifier(n_estimators = 200, random_state = 42)\n\nreg.fit(x_train,y_train)\nprint(\"Accuracy Score : Random Forest classification {}\".format(reg.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest accuracy more than decision tree model, maybe We can loon confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_pretest = reg.predict(x_test)\ncm = confusion_matrix(y_test, y_pretest)\n\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\n\nsns.heatmap(cm, annot=True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax=ax)\nplt.xlabel(\"Quality-test\")\nplt.ylabel(\"Quality-pred\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_redwine['quality'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN model our dataset\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we need ideal K value you can find with this code "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# %%\n# find k value\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machine (SVM) classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":" from sklearn.svm import SVC\n \n svm = SVC(random_state = 1)\n svm.fit(x_train,y_train)\n \n # %% test\n print(\"accuracy of svm algo: \",svm.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"naive bayes classification Model"},{"metadata":{"trusted":true},"cell_type":"code","source":" # %% Naive bayes \n from sklearn.naive_bayes import GaussianNB\n nb = GaussianNB()\n nb.fit(x_train,y_train)\n \n # %% test\n print(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's look all models accuracy percent\n\ndecision tree : % 58.2\nrandom forest : % 66.2\nknn           : % 50.3\nsvm           : % 49.5\nnaive bayes   : % 53\n\nRandom forest looking best model predict our data.\n\nThis is my homework so I'm training :) I'm glad if you comment or invoke\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}