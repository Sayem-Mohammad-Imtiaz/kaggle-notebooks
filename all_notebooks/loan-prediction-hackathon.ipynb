{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics as ms\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost.sklearn import XGBClassifier\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import RandomizedSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Data\n\ntrain_data = pd.read_csv('/kaggle/input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv')\ntrain_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Data\n\ntest_data = pd.read_csv('/kaggle/input/loan-prediction-problem-dataset/test_Y3wMUE5_7gLdaTN.csv')\ntest_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Handling Null Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Null value columns in train dataset are :\n\n['Gender' , 'Married' , 'Dependents' ,\n'Self_Employed' , 'LoanAmount' , 'Loan_Amount_Term' , 'Credit_History']","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Null Value columns in test dataset are :\n\n ['Gender' , 'Dependents' , 'Self_Employed' , 'LoanAmount' , 'Loan_Amount_Term' , 'Credit_History']","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nHere we are replacing null values of categorical_feature with mode and null values of numerical_feature with median.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorical_Feature = ['Loan_ID', 'Gender' , 'Married' , 'Dependents' , 'Education' , 'Self_Employed' , 'Loan_Amount_Term' , 'Property_Area', 'Credit_History']\nNumerical_Feature = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n\nfor i in Categorical_Feature:\n  train_data[i].fillna(train_data[i].mode()[0], inplace=True)\n  test_data[i].fillna(test_data[i].mode()[0], inplace=True)\n\nfor j in Numerical_Feature:\n  train_data[j] = train_data[j].replace(np.nan , train_data[j].median())\n  test_data[j] = test_data[j].replace(np.nan , test_data[j].median())\n\nprint(\"count of null values for train_data\")\nprint(train_data.isnull().sum())\nprint(\" \")\nprint(\"Count of null values for test_data\")\nprint(test_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"unique values for categorical features in train and test datasets\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Objecttype_feature_list = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area']\n\nprint(\"Unique value in train_data\")\nfor i in Objecttype_feature_list:\n  print(\"Count of unique values in column \", i, \"is\", train_data[i].nunique(), \"which are\", train_data[i].unique())\n\nprint(\" \")\nprint(\" \")\n\nprint(\"Unique value in test_data\")\nfor i in Objecttype_feature_list:\n  print(\"count of unique values in column \", i, \"is\", test_data[i].nunique(), \"which are\", test_data[i].unique())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Encoding I am using one hot encoding because the count of unique values are less than 5.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_LoanID = train_data['Loan_ID']\ntrain_data = train_data.drop(columns=['Loan_ID'])\ntrain_data_encoded = pd.get_dummies(train_data,drop_first=True)\n\ntest_data_LoanID = test_data['Loan_ID']\ntest_data = test_data.drop(columns=['Loan_ID'])\ntest_data_encoded = pd.get_dummies(test_data,drop_first=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_encoded.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_encoded.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving loan status and dropping it from train dataset.\ntrain_Loan_status=train_data_encoded['Loan_Status_Y']\ntrain_data_encoded=train_data_encoded.drop('Loan_Status_Y',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded = train_data_encoded\neda_test_data_encoded = test_data_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_test_data_encoded.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe describe() function is used to generate descriptive statistics that summarize the central tendency, dispersion and shape of a datasetâ€™s distribution, excluding NaN values. Refer https://www.w3resource.com/pandas/dataframe/dataframe-describe.php#:~:text=DataFrame%20%2D%20describe()%20function,dataset's%20distribution%2C%20excluding%20NaN%20values.&text=The%20percentiles%20to%20include%20in,fall%20between%200%20and%201.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### univariate analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking skewness\n\nfor i in eda_train_data_encoded:\n  print('skewness of',i,'is',eda_train_data_encoded[i].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n*   High skewness means -->  skewness is less than -1 or skewnwss is greater that 1\n\n*   Moderate skewness means --> skewness is in range [-1,-0.5] or skewness is in range [0.5,1].\n\n*   nearly symmetric means --> skewness is in range [-0.5,0.5]\n\nObservations:\n\nHighly skewed features are - Gender_Male,, Dependents_1, Dependents_2, Dependents_3+, Education_Not Graduate, Self_Employed, ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History\n\n\nModerate skewed features are - Married_Yes, Property_Area_Urban\n\n\nsymmetric skewness fatures are - Property_Area_Semiurban\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded['Loan_Status_Y'] = train_Loan_status","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded['Loan_Status_Y'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the counts, our dataset is imbalanced dataset. Count of 1' is nearly twice as of 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded['Loan_Status_Y'].hist(grid = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above graph we can say that our data is imbalanced.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded['Education_Not Graduate'].hist(grid = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"{'Not Graduate': 1 , 'Graduate': 0}, as we can see count of 0 is greater than \ncount of 1 - means no of people who applied for Loan are mostly Educated people.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Applicant Income","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"counts, bin_edges = np.histogram(eda_train_data_encoded['ApplicantIncome'], bins=20, density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['ApplicantIncome'], bins=20, density = True)\npdf_1 = counts_1/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of ApplicantIncome')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that both train_data and test_data dist plot looks similar, means having similar kind of distributions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LoanAmount","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"counts, bin_edges = np.histogram(eda_train_data_encoded['LoanAmount'], bins=20, density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['LoanAmount'], bins=20, density = True)\npdf_1 = counts_1/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of LoanAmount')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that both train_data and test_data dist plot looks similar, means having similar kind of distributions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Loan_Amount_Term","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"counts, bin_edges = np.histogram(eda_train_data_encoded['Loan_Amount_Term'], bins=20, density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['Loan_Amount_Term'], bins=20, density = True)\npdf_1 = counts_1/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of Loan_Amount_Term')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda_train_data_encoded['Loan_Amount_Term'].value_counts().sort_values().plot(kind = 'barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most People take Term of loan for 360 Months.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"CoapplicantIncome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"counts, bin_edges = np.histogram(eda_train_data_encoded['CoapplicantIncome'], bins=20, density = True)\npdf = counts/(sum(counts))\nplt.plot(bin_edges[1:],pdf)\n\ncounts_1, bin_edges_1 = np.histogram(eda_test_data_encoded['CoapplicantIncome'], bins=20, density = True)\npdf_1 = counts_1/(sum(counts_1))\nplt.plot(bin_edges_1[1:],pdf_1)\n\nplt.title('pdf of CoapplicantIncome')\nplt.legend(['eda_train_data_encoded_pdf', 'eda_test_data_encoded_pdf'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that both train_data and test_data dist plot looks similar, means having similar kind of distributions. \nwe can see that the graph follows the power law distributions \nVery few people have higher Income","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Education_Not Graduate {'Not Graduate': 1 , 'Graduate': 0}","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status_Y', hue='Education_Not Graduate', data=eda_train_data_encoded)\nplt.xlabel(\"Education_Not Graduate and Loan_Status_Y\")\nplt.ylabel(\"Count\")\n\nplt.title(\"count plot for Education_Not Graduate w.r.t. Loan_Status_Y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that Education column have impact on Loan_Status. People, who are\neducated have higher chance of getting the loan. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Gender_Male - {'Male': 1 , 'Female': 0}","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status_Y', hue='Gender_Male', data=eda_train_data_encoded)\nplt.xlabel(\"Gender_Male and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Gender_Male w.r.t. Loan_Status_Y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that Gender column has an impact on Loan_Status. Count of male is greater than the count of female for Loan_status_Y. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Self_Employed_Yes - {'No': 0 , 'Yes': 1}","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status_Y', hue='Self_Employed_Yes', data=eda_train_data_encoded)\nplt.xlabel(\"Self_Employed_Yes and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Self_Employed_Yes w.r.t. Loan_Status_Y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that count of non-self_Employed is much greater than the self_employed, means people who are self_Employed have lower chance of getting the Loan","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"ApplicantIncome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=\"Loan_Status_Y\", y=\"ApplicantIncome\", data=eda_train_data_encoded, size=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(eda_train_data_encoded['ApplicantIncome'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that most of the people have income in very short range. The graph is looking like Gaussian with skewness. Values are centered around 5403","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LoanAmount","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('box plot')\nsns.boxplot(x='Loan_Status_Y',y='LoanAmount', data=eda_train_data_encoded)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that range is similar for Loan_Status 'Y' and 'N'. and We have outliers in LoanAmount. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=\"Loan_Status_Y\", y=\"LoanAmount\", data=eda_train_data_encoded, size=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Married_Yes - {'No': 0 , 'Yes': 1}","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status_Y', hue='Married_Yes', data=eda_train_data_encoded)\nplt.xlabel(\"Married_Yes and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Married_Yes w.r.t. Loan_Status_Y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the chances of married people is high in Loan approval ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credit_History","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status_Y', hue='Credit_History', data=eda_train_data_encoded)\nplt.xlabel(\"Credit_History and Loan_Status_Y\")\nplt.ylabel(\"Count\")\nplt.title(\"count plot for Credit_History w.r.t. Loan_Status_Y\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Cedit_History for 1, which means meeting guidelines have higher chances of loan approval","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LoanAmount and Loan_Amount_Term","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(eda_train_data_encoded['LoanAmount'], eda_train_data_encoded['Loan_Amount_Term'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can observe that there is little correlation between LoanAmount and Loan_Amount_Term. For less no of months we have less Loan Amount","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"LoanAmount and ApplicantIncome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(eda_train_data_encoded['LoanAmount'], eda_train_data_encoded['ApplicantIncome'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can observe that we have positive correlation between LoanAmount and ApplicantIncome.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Norm_train_data = train_data_encoded\nNorm_test_data = test_data_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlst = ['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']\nfor i in lst:\n  Norm_train_data[i]=((Norm_train_data[i]-Norm_train_data[i].min())/(Norm_train_data[i].max()-Norm_train_data[i].min()))\n  Norm_test_data[i]=((Norm_test_data[i]-Norm_test_data[i].min())/(Norm_test_data[i].max()-Norm_test_data[i].min()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Norm_train_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Norm_test_data.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA and Outliers ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"after_eda_train_data = Norm_train_data\nafter_eda_test_data = Norm_test_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ApplicantIncome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='ApplicantIncome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR = (np.percentile(after_eda_train_data['ApplicantIncome'], 75)) - (np.percentile(after_eda_train_data['ApplicantIncome'], 25))\nmin = ((np.percentile(after_eda_train_data['ApplicantIncome'], 25)) - 1.5 * IQR)\nmax = ((np.percentile(after_eda_train_data['ApplicantIncome'], 75)) + 1.5 * IQR)\n  \nfor j in range(len(after_eda_train_data['ApplicantIncome'])):\n  if (after_eda_train_data['ApplicantIncome'][j]>=max):\n    after_eda_train_data['ApplicantIncome'][j]=max\n  elif (after_eda_train_data['ApplicantIncome'][j]<=min):\n    after_eda_train_data['ApplicantIncome'][j]=min\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='ApplicantIncome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LoanAmount","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='LoanAmount')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR = (np.percentile(after_eda_train_data['LoanAmount'], 75)) - (np.percentile(after_eda_train_data['LoanAmount'], 25))\nmin = ((np.percentile(after_eda_train_data['LoanAmount'], 25)) - 1.5 * IQR)\nmax = ((np.percentile(after_eda_train_data['LoanAmount'], 75)) + 1.5 * IQR)\n  \nfor j in range(len(after_eda_train_data['LoanAmount'])):\n  if (after_eda_train_data['LoanAmount'][j]>=max):\n    after_eda_train_data['LoanAmount'][j]=max\n  elif (after_eda_train_data['LoanAmount'][j]<=min):\n    after_eda_train_data['LoanAmount'][j]=min\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='LoanAmount')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CoapplicantIncome","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='CoapplicantIncome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR = (np.percentile(after_eda_train_data['CoapplicantIncome'], 75)) - (np.percentile(after_eda_train_data['CoapplicantIncome'], 25))\nmin = ((np.percentile(after_eda_train_data['CoapplicantIncome'], 25)) - 1.5 * IQR)\nmax = ((np.percentile(after_eda_train_data['CoapplicantIncome'], 75)) + 1.5 * IQR)\n  \nfor j in range(len(after_eda_train_data['CoapplicantIncome'])):\n  if (after_eda_train_data['CoapplicantIncome'][j]>=max):\n    after_eda_train_data['CoapplicantIncome'][j]=max\n  elif (after_eda_train_data['CoapplicantIncome'][j]<=min):\n    after_eda_train_data['CoapplicantIncome'][j]=min\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('box plot')\nafter_eda_train_data.boxplot(column='CoapplicantIncome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### correlation Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"after_eda_train_data.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### VIF (Variance Inflation factor)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"var\"] = after_eda_train_data.columns\nvif[\"VIF\"] = [variance_inflation_factor(after_eda_train_data.values, i) for i in range(after_eda_train_data.shape[1])]\nvif2=vif.sort_values(by=['VIF'], ascending=False)\nvif2.reset_index(inplace = True)\nprint(vif2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nAs we have already seen in the scatter plat - EDA part, there is little correlation between LoanAmount and Loan_Amount_Term. \nHere also we can see vif for LoanAmount is higher.\nBut We cannot remove LoanAmount feature because it is important for our modeling.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m_train_data = after_eda_train_data\nm_test_data = after_eda_test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = m_train_data['Loan_Status_Y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_train_data = m_train_data.drop('Loan_Status_Y', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(m_train_data, y, stratify=y, test_size=0.2, random_state=1)\nprint(train_X.shape, val_X.shape, train_y.shape, val_y.shape )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CatBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = CatBoostClassifier(loss_function='MultiClass', depth=5, iterations= 350, l2_leaf_reg= 1, learning_rate= 0.02)\nclf.fit(train_X,train_y)\n\ntr_pred=clf.predict(train_X)\nval_pred=clf.predict(val_X)\nprint (\"Accuracy for train is for \",ms.accuracy_score(train_y,tr_pred))\nprint (\"Accuracy for val is for \",ms.accuracy_score(val_y,val_pred))\n\ncatboost_val_acc = ms.accuracy_score(val_y,val_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(val_y,val_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting test data\n\ny_test_pred=clf.predict(m_test_data)\ntest_sub=pd.DataFrame(y_test_pred,columns=['Loan_Status'])\ntest_sub['Loan_ID']=test_data_LoanID","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sub['Loan_Status']= test_sub['Loan_Status'].map({0: 'N' , 1: 'Y'})\ntest_sub=test_sub[['Loan_ID','Loan_Status']]\n\ntest_sub.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_sub.to_csv('catbboost_submission.csv',index=False)\n#The test score for catboost classifier is : 0.7847222222222222.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SGD Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Always scale the input. The most convenient way is to use a pipeline.\n\nalph=[0.001,0.01,0.1,1,10,100]\nacc=[]\n\nfor i in alph:\n  clf = make_pipeline(StandardScaler(), SGDClassifier(loss='log', max_iter=1000, tol=1e-3, class_weight=\"balanced\", alpha=i ))\n  clf.fit(train_X, train_y)\n  val_pred=clf.predict(val_X)\n  acc.append(ms.accuracy_score(val_y,val_pred))\n  print (\"Accuracy for \",i, ms.accuracy_score(val_y,val_pred))\n  \nsgd_val_acc = np.max(acc)\nprint(sgd_val_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the validation accuracy of catboost is greater than the accuracy of SGD Classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Ramdom Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r_cfl=RandomForestClassifier(random_state=42,n_jobs=-1)\nr_cfl=RandomForestClassifier(n_estimators=40,random_state=42,n_jobs=-1, max_depth=4)\nr_cfl.fit(train_X, train_y)\ntr_pred=r_cfl.predict(train_X)\nval_pred=r_cfl.predict(val_X)\nprint (\"Accuracy for train is for \",i, ms.accuracy_score(train_y,tr_pred))\nprint (\"Accuracy for val is for \",i, ms.accuracy_score(val_y,val_pred))\n\nrandom_forest_val_acc = ms.accuracy_score(val_y,val_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(data=[['CatBoostClassifier Model',catboost_val_acc], ['SGDClassifier',sgd_val_acc], ['RamdomForestClassifier',random_forest_val_acc]], columns=['Model','Validation accuracy'])\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Catboost is the best model of all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}