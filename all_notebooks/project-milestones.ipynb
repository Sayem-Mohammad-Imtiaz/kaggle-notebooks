{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom seaborn import heatmap\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nimport plotly.express as px\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\n%matplotlib inline\n\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing\nWe use several methods to deal with the data, the first one is to drop some columns not appearing in all years.  \nwe have six years data of happiness in all contries in the world preprocess of the data:   \n1. rename the columns that has the same meaning in all years\n2. drop the uesless column or some columns only appears in just some years\n3. fullfill the shortage data with mean value\n4. put all trainning data in a same set","metadata":{}},{"cell_type":"code","source":"df_2019=pd.read_csv('../input/world-happiness-report/2019.csv')\ndf_2018=pd.read_csv('../input/world-happiness-report/2018.csv')\ndf_2017=pd.read_csv('../input/world-happiness-report/2017.csv')\ndf_2016=pd.read_csv('../input/world-happiness-report/2016.csv')\ndf_2015=pd.read_csv('../input/world-happiness-report/2015.csv')\n\n# pick up common features and rename\n\n# print(df_2015.columns)\n# print(df_2016.columns)\n# print(df_2017.columns)\n# print(df_2018.columns)\n# print(df_2019.columns)\n\ndf_2017 = df_2017.rename(columns = {'Happiness.Score' : 'Happiness Score', 'Happiness.Rank' : 'Happiness Rank', 'Economy..GDP.per.Capita.' : 'Economy (GDP per Capita)', 'Health..Life.Expectancy.' : 'Health (Life Expectancy)','Trust..Government.Corruption.' : 'Trust (Government Corruption)', 'Dystopia.Residual' : 'Dystopia Residual'})\ndf_2018 = df_2018.rename(columns = {'Healthy life expectancy' : 'Health (Life Expectancy)', 'Social support' : 'Family', 'Score' : 'Happiness Score', 'Country or region':'Country', 'Overall rank' : 'Happiness Rank', 'GDP per capita' : 'Economy (GDP per Capita)', 'Perceptions of corruption' : 'Trust (Government Corruption)'})\ndf_2019 = df_2019.rename(columns = {'Freedom to make life choices' : 'Freedom', 'Score' : 'Happiness Score', 'Country or region':'Country', 'Overall rank' : 'Happiness Rank', 'GDP per capita' : 'Economy (GDP per Capita)', 'Social support' : 'Family','Healthy life expectancy' : 'Health (Life Expectancy)','Freedom to make life choices' : 'Freedom', 'Perceptions of corruption' : 'Trust (Government Corruption)'})\n\ndf_2015.drop([\"Happiness Rank\", \"Standard Error\", \"Dystopia Residual\"], inplace = True, axis = 1)\ndf_2016.drop([\"Dystopia Residual\", \"Happiness Rank\", \"Lower Confidence Interval\", \"Upper Confidence Interval\"], inplace = True, axis = 1)\ndf_2017.drop([\"Happiness Rank\", \"Dystopia Residual\", \"Whisker.low\", \"Whisker.high\"], inplace = True, axis = 1)\ndf_2018.drop([\"Freedom to make life choices\", \"Happiness Rank\"], inplace = True, axis = 1)\ndf_2019.drop(\"Happiness Rank\", inplace = True, axis = 1)\n\n#add country's region\ncountry_rigion_map = pd.DataFrame({'Country': df_2015['Country'], 'Region': df_2015['Region']})\ndf_2017 = pd.merge(country_rigion_map, df_2017, how='right', on=['Country'])\ndf_2018 = pd.merge(country_rigion_map, df_2018, how='right', on=['Country'])\ndf_2019 = pd.merge(country_rigion_map, df_2019, how='right', on=['Country'])\n\ndf_2015['year'] = 2015\ndf_2016['year'] = 2016\ndf_2017['year'] = 2017\ndf_2018['year'] = 2018\ndf_2019['year'] = 2019\n\ndf_all = pd.concat([df_2019,df_2018,df_2017,df_2016,df_2015])\n# fill null values with mean\ndf_all.fillna((df_all.mean()), inplace = True)\nprint(df_all.info())\n\ndf_all.to_csv('/kaggle/working/df_all.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# analysis of the raw data\n","metadata":{}},{"cell_type":"markdown","source":"**the average score of each years**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplt.plot(df_all.groupby(\"year\").mean()[\"Happiness Score\"])\nplt.ylabel(\"Happiness Score\")\nplt.ylim(5, 5.6)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**the top 10 countries which has the happinest value**","metadata":{}},{"cell_type":"code","source":"df_top = df_all.head(10).sort_values('Happiness Score', ascending = True)\n\npx.bar(df_top, x='Happiness Score', y='Country', title = \"top 10\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visulaize the Variability**","metadata":{}},{"cell_type":"code","source":"# Visulaize the Variability\nplt.rcParams['figure.figsize'] = (10,10)\ndf_all[['Happiness Score',\n       'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)',\n       'Freedom', 'Generosity', 'Trust (Government Corruption)',]].hist();\n\ndf_all.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\ncol=df_all.corr()\nsns.heatmap(col,annot=True,fmt='.2f');\nplt.title(\"Correlation Map from Past 4 years from 2015 - 2019\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# baseline method\nHere, we try to use constant value as a baseline","metadata":{}},{"cell_type":"code","source":"# y = df_all['Happiness Score']\n# X = df_all[['Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)', 'Freedom', 'Generosity', \n#           'Trust (Government Corruption)']] \n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\ny = df_all['Happiness Score']\nX = pd.get_dummies(df_all.drop([\"Happiness Score\",'year','Country'], axis=1)).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0, shuffle=False)\nfinal_mse = []\nuse_method = []\nprint('X shape', end=\"\")\nprint(X_train.shape)\nconstant_mse = mean_squared_error(np.ones(y_test.shape) * np.mean(y_train), y_test)\nfinal_mse.append(constant_mse)\nuse_method.append('baseline')\nprint(\"r2 score is %.2f\" % r2_score(y_test, np.ones(y_test.shape) * np.mean(y_train)))\nprint(\"Constant MSE: %.2f\" % constant_mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nmse_list = []\nk_range = range(1, 51)\nfor k in k_range:\n    model = KNeighborsRegressor(k)\n    model.fit(X_train, y_train)\n    mse_list.append(mean_squared_error(model.predict(X_test), y_test))\nk_list = [k for k in k_range]\nplt.plot(k_list, mse_list, label = \"mse\")\nplt.plot(k_list, [constant_mse for i in range(len(k_list))], label=\"baseline\")\nplt.legend()\nplt.title(\"use KNN\")\nplt.show()\nfinal_mse.append(min(mse_list))\nuse_method.append('KNN')\n\n# best parameter for knn\nknnReg = KNeighborsRegressor (n_neighbors = 4)\nknnReg.fit(X_train,y_train)\ny_pred = knnReg.predict(X_test).reshape(-1, 1)\n\nscore = r2_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# lasso regression\nAdd some description","metadata":{}},{"cell_type":"code","source":"y = df_all['Happiness Score']\nX = pd.get_dummies(df_all.drop([\"Happiness Score\",'year','Country'], axis=1)).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=False)\n# X_train = pd.get_dummies(df_all.drop([\"Happiness Score\",'year','Country'], axis=1)).values\n# X_test = pd.get_dummies(df_2020.drop([\"Happiness Score\",'year','Country'], axis=1)).values\n# y_train = df_all['Happiness Score']\n# y_test = df_2020['Happiness Score']\n\n\nlasso = Pipeline(steps = [\n    ('ss', StandardScaler()),\n    ('poly', PolynomialFeatures(1)),\n    ('sgd',  SGDRegressor(penalty=\"l1\", alpha=0.1,random_state=0)),\n])\n\nlasso.fit(X_train,y_train)\nprint(\"mse:\", end=\"\")\nprint(mean_squared_error(lasso.predict(X_test), y_test))\nprint(\"r2 score:\", end=\"\")\nprint(r2_score(y_test, lasso.predict(X_test)))\nfinal_mse.append(mean_squared_error(lasso.predict(X_test), y_test))\nuse_method.append('lasso')\ndef find_zero_and_nonezero_index(coefs):\n    nonzero_index = []\n    zero_index = []\n    for i in range(len(coefs)-1):\n        if coefs[i] != 0:\n            nonzero_index.append(i)\n        else:\n            zero_index.append(i)\n    return nonzero_index,zero_index\n\nnonzero_index, zero_index = find_zero_and_nonezero_index(lasso['sgd'].coef_)\nprint(\"find non zero index and zero index\", end=\" \")\nprint(nonzero_index, zero_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### after delete the inrelevant features","metadata":{}},{"cell_type":"code","source":"n_X_train = X_train[:,nonzero_index].reshape(-1,8)\nn_X_test = X_test[:,nonzero_index].reshape(-1,8)\n\nlasso.fit(n_X_train,y_train)\nprint(\"mse:\", end=\"\")\nprint(mean_squared_error(lasso.predict(n_X_test),y_test))\nprint(\"r2 score:\", end=\"\")\nprint(r2_score(y_test, lasso.predict(n_X_test)))\nplt.figure(figsize=(5,5))\nplt.bar(['before','after'],[final_mse[-1],mean_squared_error(lasso.predict(n_X_test),y_test)])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It results that all the data are relevent with the happiness ","metadata":{}},{"cell_type":"markdown","source":"# Ridge regression\n","metadata":{}},{"cell_type":"code","source":"ridge = pipeline = Pipeline([\n    ('ss', StandardScaler()),\n    ('poly', PolynomialFeatures(1)),\n    ('sgd',  SGDRegressor(penalty=\"l2\",alpha=1,random_state=0)),\n])\nridge.fit(X_train, y_train)\nprint(\"mse:\", end=\"\")\nprint(mean_squared_error(ridge.predict(X_test), y_test))\nprint(\"r2 score:\", end=\"\")\nprint(r2_score(y_test, ridge.predict(X_test)))\nfinal_mse.append(mean_squared_error(ridge.predict(X_test), y_test))\nuse_method.append('ridge')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"model = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(\"mse:\", end=\" \")\nprint(mean_squared_error(model.predict(X_test), y_test))\nprint(\"r2 score:\", end=\" \")\nprint(r2_score(y_test, model.predict(X_test)))\nprint(model.score(X_test, y_test))\nfinal_mse.append(mean_squared_error(model.predict(X_test), y_test))\nuse_method.append('linear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# random forest","metadata":{}},{"cell_type":"code","source":"res = []\nx_axis = []\nfor i in range(1,30):\n    rfrg = RandomForestRegressor(n_estimators = 100,max_depth=i, random_state=0)\n    rfrg.fit(X_train, y_train)\n    res.append(mean_squared_error(rfrg.predict(X_test), y_test))\n    x_axis.append(i)\n\nplt.figure(figsize=(5,5))\nplt.plot(x_axis,res)\nplt.plot(x_axis, [constant_mse for i in range(len(res))], label=\"baseline\")\nplt.legend()\nplt.title(\"use random forest\")\nplt.show()\nprint(\"mes: \", end=\"\")\nprint(min(res))\nfinal_mse.append(min(res))\nuse_method.append('random_forest')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.bar(['Economy', 'Family', 'Health ', 'Freedom', 'Generosity', \n          'Government Trust'], rfrg.feature_importances_[:6])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"also use decision tree we can know wich feature influnences the happiness most","metadata":{}},{"cell_type":"markdown","source":"# GradientBoostingRegressor\nuse subsample to lead to a reduction of variance and an increase in bias","metadata":{}},{"cell_type":"code","source":"#reg = GradientBoostingRegressor(random_state=0)\n#reg.fit(X_train, y_train)\nres = []\nx_axis = []\nlearning_rate = 0.01\nwhile learning_rate <= 0.2:\n    reg = GradientBoostingRegressor(random_state=0,subsample=0.7,learning_rate = learning_rate)\n    reg.fit(X_train, y_train)\n    res.append(mean_squared_error(reg.predict(X_test), y_test))\n    x_axis.append(learning_rate)\n    learning_rate+= 0.01\n    \nplt.figure(figsize=(5,5))\nplt.plot(x_axis,res,label=\"boosting\")\nplt.plot(x_axis, [constant_mse for i in range(len(res))], label=\"baseline\")\nplt.legend()\nplt.title(\"use boosting\")\nplt.show()\nprint(min(res),x_axis[res.index(min(res))])\nfinal_mse.append(min(res))\nuse_method.append('boosting')\n# print(\"mse: \", end=\" \")\n# print(mean_squared_error(regr.predict(X_test), y_test))\n# print(\"r2 score:\", end=\" \")\n# print(r2_score(y_test, regr.predict(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**from now on, we conclude all the methods we used and the result mse we get from the best parameters we tried**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.bar(use_method,final_mse)\nplt.title(\"result\")\nfor i in range(len(use_method)):\n    plt.text(use_method[i] , final_mse[i],round(final_mse[i],3),color='red', fontweight='bold',horizontalalignment = 'center')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"divede the date into different rigions","metadata":{}},{"cell_type":"code","source":"mse_list_r = []\nregions = ['Australia and New Zealand','Central and Eastern Europe','Eastern Asia','Latin America and Caribbean','Middle East and Northern Africa',\n              'North America','Southeastern Asia','Southern Asia','Sub-Saharan Africa','Western Europe']\nfor region in regions:\n    df_all_region = df_all[df_all['Region'] == region]\n    y_region = df_all_region['Happiness Score']\n    X_region = pd.get_dummies(df_all_region.drop([\"Happiness Score\",'year','Country'], axis=1)).values\n\n    r_X_train, r_X_test, r_y_train, r_y_test = train_test_split(X_region, y_region, test_size=0.20, random_state=0, shuffle=False)\n    reg = GradientBoostingRegressor(random_state=0,subsample=0.7,learning_rate = 0.1)\n    reg.fit(r_X_train, r_y_train)\n    mse_list_r.append(mean_squared_error(reg.predict(r_X_test), r_y_test))\n\n\n\nregions_abre = ['au&nz','EU','EA','la&c','me&NA','NA','SAa','SAf','SS','WE']\nplt.figure(figsize=(5,5))\nplt.bar(regions_abre,mse_list_r)\nplt.title(\"result\")\nfor i in range(len(regions)):\n    plt.text(regions_abre[i] , mse_list_r[i],round(mse_list_r[i],3),color='red', fontweight='bold',horizontalalignment = 'center')\nplt.show()\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural network\n## try neural network here to deal with data from 2016 - 2019 all","metadata":{}},{"cell_type":"code","source":"y = df_all['Happiness Score']\nX = df_all[['Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)', 'Freedom', 'Generosity', \n          'Trust (Government Corruption)']] \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = torch.tensor(X_train.values, dtype=torch.float32)\ny_train = torch.tensor(y_train.values, dtype=torch.float32)\nX_test = torch.tensor(X_test.values, dtype=torch.float32)\ny_test = torch.tensor(y_test.values, dtype=torch.float32)\nprint(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's first try to use a simple neural network","metadata":{}},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, net_in):\n        super(Net, self).__init__()\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(net_in,5),\n            torch.nn.SiLU(),\n            torch.nn.Linear(5, 1),\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nnet = Net(X_train.shape[1])\noptim=torch.optim.Adam(Net.parameters(net),lr=0.001)\nLoss=torch.nn.MSELoss()\n\nepoch_all = 40000\nlen1 = len(X_train)\ntest_error = []\nr2_score_test = []\ntrain_error = []\nr2_score_train = []\ncount = []\nprev = 0.0\ncurrent = 0.0\nfor epoch in range(epoch_all):\n    loss=None\n    for i in range(100):\n        index = random.randint(0, len1 - 1)\n        y_predict=net(X_train[index])\n        loss=Loss(y_predict,y_train[index])\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    if (epoch + 1) % 100 == 0:\n        prev = current\n        current = mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())\n        if abs(prev - current) < 1e-6:\n            break\n        train_error.append(mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy()))\n        test_error.append(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\n        r2_score_test.append(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n        r2_score_train.append(r2_score(y_train.data.numpy(), net(X_train).data.numpy()))\n        count.append(epoch)\n        if (epoch + 1) % 2000 == 0:\n            print(\"step: {0} , loss: {1}\".format(epoch+1,mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())))\n\nplt.figure(figsize=(5,5))\nplt.plot(count, train_error, label = \"train\")\nplt.plot(count, test_error, label=\"test\")\nplt.title(\"MSE for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.plot(count, r2_score_train, label = \"train\")\nplt.plot(count, r2_score_test, label=\"test\")\nplt.title(\"r2 score for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\nprint(net)\nprint(\"mse: \", end=\" \")\nprint(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\nprint(\"r2 score = \", end= \" \")\nprint(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n\nprint(\"the best score of neural network is :\")\nprint(\"r2 score = \", end= \" \")\nprint(max(r2_score_test))\nprint(\"mse: \", end=\" \")\nprint(min(test_error))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Then let's try the neural network with more nodes","metadata":{}},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, net_in):\n        super(Net, self).__init__()\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(net_in,5),\n            torch.nn.SiLU(),\n            torch.nn.Linear(5, 3),\n            torch.nn.SiLU(),\n            torch.nn.Linear(3, 1)\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nnet = Net(X_train.shape[1])\noptim=torch.optim.Adam(Net.parameters(net),lr=0.001)\nLoss=torch.nn.MSELoss()\n\nepoch_all = 40000\nlen1 = len(X_train)\ntest_error = []\nr2_score_test = []\ntrain_error = []\nr2_score_train = []\ncount = []\nprev = 0.0\ncurrent = 0.0\nfor epoch in range(epoch_all):\n    loss=None\n    for i in range(100):\n        index = random.randint(0, len1 - 1)\n        y_predict=net(X_train[index])\n        loss=Loss(y_predict,y_train[index])\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    if (epoch + 1) % 100 == 0:\n        prev = current\n        current = mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())\n        if abs(prev - current) < 1e-6:\n            break\n        train_error.append(mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy()))\n        test_error.append(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\n        r2_score_test.append(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n        r2_score_train.append(r2_score(y_train.data.numpy(), net(X_train).data.numpy()))\n        count.append(epoch)\n        if (epoch + 1) % 2000 == 0:\n            print(\"step: {0} , loss: {1}\".format(epoch+1,mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())))\n\nplt.figure(figsize=(5,5))\nplt.plot(count, train_error, label = \"train\")\nplt.plot(count, test_error, label=\"test\")\nplt.title(\"MSE for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(5,5))\nplt.plot(count, r2_score_train, label = \"train\")\nplt.plot(count, r2_score_test, label=\"test\")\nplt.title(\"r2 score for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\nprint(net)\nprint(\"mse: \", end=\" \")\nprint(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\nprint(\"r2 score = \", end= \" \")\nprint(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n\nprint(\"the best score of neural network is :\")\nprint(\"r2 score = \", end= \" \")\nprint(max(r2_score_test))\nprint(\"mse: \", end=\" \")\nprint(min(test_error))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's try to fit the data with more neurons then previous one ","metadata":{}},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, net_in):\n        super(Net, self).__init__()\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(net_in,10),\n            torch.nn.SiLU(),\n            torch.nn.Linear(10, 10),\n            torch.nn.SiLU(),\n            torch.nn.Linear(10, 1)\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nnet = Net(X_train.shape[1])\noptim=torch.optim.Adam(Net.parameters(net),lr=0.0001)\nLoss=torch.nn.MSELoss()\n\nepoch_all = 40000\nlen1 = len(X_train)\ntest_error = []\nr2_score_test = []\ntrain_error = []\nr2_score_train = []\ncount = []\nprev = 0.0\ncurrent = 0.0\nfor epoch in range(epoch_all):\n    loss=None\n    for i in range(100):\n        index = random.randint(0, len1 - 1)\n        y_predict=net(X_train[index])\n        loss=Loss(y_predict,y_train[index])\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    if (epoch + 1) % 100 == 0:\n        prev = current\n        current = mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())\n        if abs(prev - current) < 1e-6:\n            break\n        train_error.append(mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy()))\n        test_error.append(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\n        r2_score_test.append(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n        r2_score_train.append(r2_score(y_train.data.numpy(), net(X_train).data.numpy()))\n        count.append(epoch)\n        if (epoch + 1) % 2000 == 0:\n            print(\"step: {0} , loss: {1}\".format(epoch+1,mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())))\n\n            \n\nplt.figure(figsize=(5,5))\nplt.plot(count, train_error, label = \"train\")\nplt.plot(count, test_error, label=\"test\")\nplt.title(\"MSE for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\n\nplt.figure(figsize=(5,5))\nplt.plot(count, r2_score_train, label = \"train\")\nplt.plot(count, r2_score_test, label=\"test\")\nplt.title(\"r2 score for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\n\nprint(net)\nprint(\"mse: \", end=\" \")\nprint(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\nprint(\"r2 score = \", end= \" \")\nprint(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n\nprint(\"the best score of neural network is :\")\nprint(\"r2 score = \", end= \" \")\nprint(max(r2_score_test))\nprint(\"mse: \", end=\" \")\nprint(min(test_error))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, net_in):\n        super(Net, self).__init__()\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(net_in,30),\n            torch.nn.SiLU(),\n            torch.nn.Linear(30, 30),\n            torch.nn.SiLU(),\n            torch.nn.Linear(30, 1)\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nnet = Net(X_train.shape[1])\noptim=torch.optim.Adam(Net.parameters(net),lr=0.0001)\nLoss=torch.nn.MSELoss()\n\nepoch_all = 40000\nlen1 = len(X_train)\ntest_error = []\nr2_score_test = []\ntrain_error = []\nr2_score_train = []\ncount = []\nprev = 0.0\ncurrent = 0.0\nfor epoch in range(epoch_all):\n    loss=None\n    for i in range(100):\n        index = random.randint(0, len1 - 1)\n        y_predict=net(X_train[index])\n        loss=Loss(y_predict,y_train[index])\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    if (epoch + 1) % 100 == 0:\n        prev = current\n        current = mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())\n        if abs(prev - current) < 1e-5:\n            break\n        train_error.append(mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy()))\n        test_error.append(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\n        r2_score_test.append(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n        r2_score_train.append(r2_score(y_train.data.numpy(), net(X_train).data.numpy()))\n        count.append(epoch)\n        if (epoch + 1) % 2000 == 0:\n            print(\"step: {0} , loss: {1}\".format(epoch+1,mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())))\n\n            \n\nplt.figure(figsize=(5,5))\nplt.plot(count, train_error, label = \"train\")\nplt.plot(count, test_error, label=\"test\")\nplt.title(\"MSE for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\n\nplt.figure(figsize=(5,5))\nplt.plot(count, r2_score_train, label = \"train\")\nplt.plot(count, r2_score_test, label=\"test\")\nplt.title(\"r2 score for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\n\nprint(net)\nprint(\"mse: \", end=\" \")\nprint(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\nprint(\"r2 score = \", end= \" \")\nprint(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n\nprint(\"the best score of neural network is :\")\nprint(\"r2 score = \", end= \" \")\nprint(max(r2_score_test))\nprint(\"mse: \", end=\" \")\nprint(min(test_error))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, net_in):\n        super(Net, self).__init__()\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(net_in,50),\n            torch.nn.SiLU(),\n            torch.nn.Linear(50, 50),\n            torch.nn.SiLU(),\n            torch.nn.Linear(50, 1)\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nnet = Net(X_train.shape[1])\noptim=torch.optim.Adam(Net.parameters(net),lr=0.0001)\nLoss=torch.nn.MSELoss()\n\nepoch_all = 40000\nlen1 = len(X_train)\ntest_error = []\nr2_score_test = []\ntrain_error = []\nr2_score_train = []\ncount = []\nprev = 0.0\ncurrent = 0.0\nfor epoch in range(epoch_all):\n    loss=None\n    for i in range(100):\n        index = random.randint(0, len1 - 1)\n        y_predict=net(X_train[index])\n        loss=Loss(y_predict,y_train[index])\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    if (epoch + 1) % 100 == 0:\n        prev = current\n        current = mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())\n        if abs(prev - current) < 1e-5:\n            break\n        train_error.append(mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy()))\n        test_error.append(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\n        r2_score_test.append(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n        r2_score_train.append(r2_score(y_train.data.numpy(), net(X_train).data.numpy()))\n        count.append(epoch)\n        if (epoch + 1) % 2000 == 0:\n            print(\"step: {0} , loss: {1}\".format(epoch+1,mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())))\n\n            \n\nplt.figure(figsize=(5,5))\nplt.plot(count, train_error, label = \"train\")\nplt.plot(count, test_error, label=\"test\")\nplt.title(\"MSE for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\n\nplt.figure(figsize=(5,5))\nplt.plot(count, r2_score_train, label = \"train\")\nplt.plot(count, r2_score_test, label=\"test\")\nplt.title(\"r2 score for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\n\nprint(net)\nprint(\"mse: \", end=\" \")\nprint(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\nprint(\"r2 score = \", end= \" \")\nprint(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n\nprint(\"the best score of neural network is :\")\nprint(\"r2 score = \", end= \" \")\nprint(max(r2_score_test))\nprint(\"mse: \", end=\" \")\nprint(min(test_error))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Try to use few layers to eliminate the problem of overfitting","metadata":{}},{"cell_type":"code","source":"class Net(torch.nn.Module):\n    def __init__(self, net_in):\n        super(Net, self).__init__()\n        self.net=torch.nn.Sequential(\n            torch.nn.Linear(net_in,30),\n            torch.nn.Tanh(),\n            torch.nn.Linear(30, 1),\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n\nnet = Net(X_train.shape[1])\noptim=torch.optim.Adam(Net.parameters(net),lr=0.0001)\nLoss=torch.nn.MSELoss()\n\nepoch_all = 40000\nlen1 = len(X_train)\ntest_error = []\nr2_score_test = []\ntrain_error = []\nr2_score_train = []\ncount = []\nprev = 0.0\ncurrent = 0.0\nfor epoch in range(epoch_all):\n    loss=None\n    for i in range(100):\n        index = random.randint(0, len1 - 1)\n        y_predict=net(X_train[index])\n        loss=Loss(y_predict,y_train[index])\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n    if (epoch + 1) % 100 == 0:\n        prev = current\n        current = mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())\n        if abs(prev - current) < 1e-6:\n            break\n        train_error.append(mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy()))\n        test_error.append(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\n        r2_score_test.append(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n        r2_score_train.append(r2_score(y_train.data.numpy(), net(X_train).data.numpy()))\n        count.append(epoch)\n        if (epoch + 1) % 2000 == 0:\n            print(\"step: {0} , loss: {1}\".format(epoch+1,mean_squared_error(net(X_train).data.numpy(), y_train.data.numpy())))\n\n\nplt.figure(figsize=(5,5))\nplt.plot(count, train_error, label = \"train\")\nplt.plot(count, test_error, label=\"test\")\nplt.title(\"MSE for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\n\nplt.figure(figsize=(5,5))\nplt.plot(count, r2_score_train, label = \"train\")\nplt.plot(count, r2_score_test, label=\"test\")\nplt.title(\"r2 score for train and test\")\nplt.ylim((0,1))\nplt.legend()\nplt.show()\n\nprint(\"mse: \", end=\" \")\nprint(mean_squared_error(net(X_test).data.numpy(), y_test.data.numpy()))\nprint(\"r2 score = \", end= \" \")\nprint(r2_score(y_test.data.numpy(), net(X_test).data.numpy()))\n\nprint(\"the best score of neural network is :\")\nprint(\"r2 score = \", end= \" \")\nprint(max(r2_score_test))\nprint(\"mse: \", end=\" \")\nprint(min(test_error))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, due to the size and dimension of the data, it's easy to overfit the model.   \nThe best model for using neural network is \n```\nNet(\n  (net): Sequential(\n    (0): Linear(in_features=6, \n        out_features=5, bias=True)\n    (1): SiLU()\n    (2): Linear(in_features=5, \n        out_features=1, bias=True)\n  )\n)\n```","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}