{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist, pdist\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_data = pd.read_csv('../input/unsupervised-learning-on-country-data/Country-data.csv')\ncountry_data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Clustering Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 8))\nsns.heatmap(country_data.corr(), vmin = -1, vmax = 1, annot = True, cmap = 'coolwarm')\nplt.title('Correlation Map Of Country Data', fontdict={'fontsize':12}, pad=12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this heatmap, I can define the top 3 highest positive correlation score:\n* ***gdpp*** and income with 0.9\n* total_fer and ***child_mort*** with 0.85\n* ***imports*** and exports with 0.74\n\nI pick one of each correlation pair to see how the cluster will be made from those data. So the next step, I show you clustering between GDPP VS Child Mortality and GDPP VS Imports. Will it represent a good clustering model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_data = country_data[['child_mort', 'imports', 'gdpp']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x = 'gdpp', y = 'child_mort', data = count_data)\nplt.title('Child Mortality')\nplt.xlabel('GDPP')\nplt.ylabel('Child Mortality')\nplt.subplot(1, 2, 2)\nsns.scatterplot(x = 'gdpp', y = 'imports', data = count_data)\nplt.title('Imports')\nplt.xlabel('GDPP')\nplt.ylabel('Imports')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's determine optimal number of cluster using 2 methods:\n* KMeans Clustering\n* DBSCAN"},{"metadata":{},"cell_type":"markdown","source":"# KMeans Clustering"},{"metadata":{},"cell_type":"markdown","source":"### Elbow Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"number_of_cluster = range(1,11)\nclusterings = [KMeans(n_clusters = k).fit(count_data) for k in number_of_cluster]\ncentroids = [k.cluster_centers_ for k in clusterings]\n\nD_k = [cdist(count_data, cent, 'euclidean') for cent in centroids] \ncIdx = [np.argmin(D, axis = 1) for D in D_k] \ndist = [np.min(D, axis = 1) for D in D_k] \navg_withinSS = [sum(d)/count_data.shape[0] for d in dist] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,8))\nsns.lineplot(number_of_cluster, avg_withinSS)\nsns.scatterplot(number_of_cluster, avg_withinSS)\nplt.xticks(number_of_cluster)\nplt.xlabel('Number Of Cluster')\nplt.ylabel('Average Within SS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best n_clusters is 2 based on elbow method."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 2)\nkmeans.fit(count_data)\ncountry_clust = count_data.copy()\ncountry_clust['clustkmeans'] = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,8))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x = 'gdpp', y = 'child_mort', data = country_clust, hue = 'clustkmeans', palette = 'bright')\nplt.title('Silhouette Plot Child Mortality')\nplt.xlabel('GDPP')\nplt.ylabel('Child Mortality')\nplt.subplot(1, 2, 2)\nsns.scatterplot(x = 'gdpp', y = 'imports', data = country_clust, hue = 'clustkmeans', palette = 'bright')\nplt.title('Silhouette Plot Imports')\nplt.xlabel('GDPP')\nplt.ylabel('Imports')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_name = ['Cluster 0', 'Cluster 1']\ncolor = ['Blue', 'Orange']\nmortality = ['Low GDPP VS Low to Medium Child Mortality', 'Medium GDPP VS Low Child Mortality']\nimports = ['Low GDPP VS Medium Imports', 'Medium GDPP VS Medium Imports']\nbest_summary = pd.DataFrame({'Color': color, 'Name': cluster_name,\n                        'Child Mortality': mortality, 'Imports': imports})\nbest_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these plots, I can easily define that there are only 2 clusters but I can spot a few outliers or noise. On DBSCAN, I hope it can determine if there are any outliers or noise, so I can have a better clustering model."},{"metadata":{},"cell_type":"markdown","source":"# DBSCAN"},{"metadata":{},"cell_type":"markdown","source":"*Optimizing Minimum Sample And Epsilon*"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ncountry_clust_scaled = scaler.fit_transform(country_clust)\n\nfor eps in [i/10 for i in range(2,5)]:\n    for min_samples in range (7,9):\n        print(f'\\neps {eps}')\n        print(f'\\min samples {min_samples}')\n        \n        dbscan = DBSCAN(eps = eps, min_samples = min_samples)\n        labels = dbscan.fit_predict(country_clust_scaled)\n        score = silhouette_score(country_clust_scaled, labels)\n        \n        print(f'clusters present: {np.unique(labels)}')\n        print(f'clusters sizes: {np.bincount(labels + 1)}')\n        print(f'Silhouette Score: {score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best hyperparam are eps: 0.4 and min samples: 7, because it has the highest silhouette score, but samples is inlcuded with noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan = DBSCAN(eps = 0.4, min_samples = 7)\nlabels = dbscan.fit_predict(country_clust_scaled)\ncountry_clust['clustdbscan'] = labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,8))\nplt.subplot(1, 2, 1)\nsns.scatterplot(x = 'gdpp', y = 'child_mort', data = country_clust, hue = 'clustdbscan', palette = 'bright')\nplt.legend(loc = 1)\nplt.title('DBSCAN Child Mortality')\nplt.xlabel('GDPP')\nplt.ylabel('Child Mortality')\nplt.subplot(1, 2, 2)\nsns.scatterplot(x = 'gdpp', y = 'imports', data = country_clust, hue = 'clustdbscan', palette = 'bright')\nplt.legend(loc = 1)\nplt.title('DBSCAN Imports')\nplt.xlabel('GDPP')\nplt.ylabel('Imports')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_name = ['Cluster -1', 'Cluster 0', 'Cluster 1']\ncolor = ['Blue', 'Orange', 'Green']\nmortality = ['Noise', 'Low GDPP VS Low to Medium Imports', 'Medium GDPP VS Low Imports']\nimports = ['Noise', 'Low GDPP VS Medium Imports', 'Medium GDPP VS Medium Imports']\ndbscan_summary = pd.DataFrame({'Color': color, 'Name': cluster_name,\n                        'Child Mortality': mortality, 'Imports': imports})\ndbscan_summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprisingly, there is a lot of noise more than I think. With DBSCAN, I can see the accurate model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}