{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Used BERT to classify the texts into 4 classes","metadata":{}},{"cell_type":"code","source":"import transformers \nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n\nimport torch \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nfrom pylab import rcParams \nimport matplotlib.pyplot as plt \nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom collections import defaultdict\nfrom textwrap import wrap \n\n\nfrom torch import nn, optim \nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale = 1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/ag-news-classification-dataset/train.csv')\ntest = pd.read_csv('/kaggle/input/ag-news-classification-dataset/test.csv')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape \ntrain.shape, test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labeling = {\n    1:0, \n    2:1,\n    3:2,\n    4:3\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Class Index'] = train['Class Index'].apply(lambda x : labeling[x])\ntest['Class Index'] = test['Class Index'].apply(lambda x: labeling[x])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train['Class Index'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA PREPROCESSING \nclass_names = ['1', '2', '3', '4']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_trained_model_name = 'bert-base-cased'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(pre_trained_model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"small_text = \"This data is really really really huge but this time I will do it in detail\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = tokenizer.tokenize(small_text)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f'Sentence: {small_text}')\nprint(f'Tokens : {tokens}')\nprint(f'Token-IDs : {token_ids}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SPECIAL TOKENS \n\ntokenizer.sep_token, tokenizer.sep_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.cls_token, tokenizer.cls_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token, tokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.unk_token, tokenizer.unk_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ALL of the above work can be done with simple encode_plus() methods \n\nencoding = tokenizer.encode_plus(\n    small_text,\n    max_length=32, \n    add_special_tokens=True,\n    return_token_type_ids=False, \n    padding='max_length', \n    return_attention_mask=True, \n    return_tensors='pt'\n)\nencoding.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(encoding['input_ids'][0]))\nencoding['input_ids'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(encoding['attention_mask'][0]))\nencoding['attention_mask']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choosing Sequence Length \ntoken_lens = []\ntrain['content'] = train['Title']+' '+train['Description']\ndel train['Title']\ndel train['Description']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for txt in train.content:\n    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n    token_lens.append(len(tokens))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(token_lens)\nplt.xlim([0, 256])\nplt.xlabel('Token COunt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AgNewsData(Dataset):\n    \n    def __init__(self, content, targets, tokenizer, max_len):\n        self.content = content\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len \n        \n    def __len__(self):\n        return len(self.content)\n    \n    def __getitem__(self, item):\n        content = str(self.content[item])\n        target = self.targets[item]\n        \n        encoding = self.tokenizer.encode_plus(\n            content, \n            max_length=self.max_len, \n            add_special_tokens=True,\n            return_token_type_ids=False, \n            padding=\"max_length\",\n            truncation = True,\n            return_attention_mask=True, \n            return_tensors='pt'\n        )\n        \n        return {\n            'content_text':content, \n            'input_ids':encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets':torch.tensor(target, dtype=torch.long)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(train, test_size=0.1, random_state=RANDOM_SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape, test.shape, df_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['content'] = test['Title']+\" \"+test['Description']\ndel test['Title']\ndel test['Description']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test= test\ndel test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to create dataloaders \n\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = AgNewsData(\n        content = df.content.to_numpy(),\n        targets = df['Class Index'].to_numpy(),\n        tokenizer = tokenizer, \n        max_len = max_len\n        \n    )\n    \n    \n    return DataLoader(\n        ds, \n        batch_size = batch_size, \n        num_workers = 4\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\n\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = next(iter(train_data_loader))\ndata.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = BertModel.from_pretrained(pre_trained_model_name)\n\no=bert_model(\n    input_ids = encoding['input_ids'],\n    attention_mask = encoding['attention_mask']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o.get('last_hidden_state').shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o.get('pooler_output').shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.config.hidden_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    \n    \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(pre_trained_model_name)\n        self.drop = nn.Dropout(p=0.45)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        o = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask\n        )\n        \n        output = self.drop(o.get('pooler_output'))\n        \n        return self.out(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifier(len(class_names))\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\n\nprint(input_ids.shape)\nprint(attention_mask.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F.softmax(model(input_ids, attention_mask), dim=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING \n\nEPOCHS = 1 \noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0,\n    num_training_steps = total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(\n    model, \n    data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    n_examples\n):\n    model = model.train()\n    \n    losses= []\n    correct_predictions = 0 \n    \n    for d in data_loader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        outputs = model(\n            input_ids = input_ids, \n            attention_mask = attention_mask\n        )\n        \n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n\n        nn.utils.clip_grad_norm(model.parameters(), max_norm = 1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    \n    \n    return correct_predictions.double()/n_examples, np.mean(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    \n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            targets = d['targets'].to(device)\n            outputs = model(\n                input_ids = input_ids, \n                attention_mask = attention_mask\n            )\n\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n        \n    return correct_predictions.double() / n_examples , np.mean(losses)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    \n    print(f'Epoch {epoch+1}/{EPOCHS}')\n    print('-'*10)\n    \n    \n    train_acc, train_loss = train_epoch(\n         model, \n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(df_train)\n    )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    \n    val_acc, val_loss = eval_model(\n        model, \n        val_data_loader,\n        loss_fn,\n        device,\n        len(df_val)\n    )\n    \n    print(f'Val loss {val_loss} val accuracy {val_acc}')\n    print()\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EVALUATION","metadata":{}},{"cell_type":"code","source":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\ntest_acc.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_predictions(model, data_loader):\n  model = model.eval()\n  \n  content_texts = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n\n  with torch.no_grad():\n    for d in data_loader:\n\n      texts = d[\"content_text\"]\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n\n      probs = F.softmax(outputs, dim=1)\n\n      content_texts.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(probs)\n      real_values.extend(targets)\n\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return content_texts, predictions, prediction_probs, real_values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred, target_names=class_names))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef show_confusion_matrix(confusion_matrix):\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment');\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 2\n\ncontent_text = y_review_texts[idx]\ntrue_sentiment = y_test[idx]\npred_df = pd.DataFrame({\n  'class_names': class_names,\n  'values': y_pred_probs[idx]\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\".join(wrap(content_text)))\nprint()\nprint(f'True sentiment: {class_names[true_sentiment]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\nplt.ylabel('sentiment')\nplt.xlabel('probability')\nplt.xlim([0, 1]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREDICTING WITH RAW TEXT ","metadata":{}},{"cell_type":"code","source":"content_text = 'Woah what a match! .. I love the way they played but my team failed'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_review = tokenizer.encode_plus(\n  content_text,\n    max_length=100, \n    add_special_tokens=True,\n    return_token_type_ids=False, \n    padding='max_length', \n    return_attention_mask=True, \n    return_tensors='pt'\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = encoded_review['input_ids'].to(device)\nattention_mask = encoded_review['attention_mask'].to(device)\n\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\n\nprint(f'Review text: {content_text}')\nprint(f'Sentiment  : {class_names[prediction]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}