{"cells":[{"metadata":{},"cell_type":"markdown","source":"### 2. Timeseries Practice - 2 Training Methods - with Forex Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Purpose is to practice 2 popular methods for splitting training data on timeseries example on an easy dataset that does not run into memory issues.\nI am only going to predict 1 currency, the AUD/USD rate, and use other countries lagged values as features. <br>\n* 1. Train Validation Test Method \n* 2. Timeseries Split Method\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"About the dataset - the data is foreign exchange rates from 2000 to 2019 across the major trading pairs. \nThis data is reasonably clean and requires minimal data preparation. We want to be able to make a prediction on the daily AUD/USD rate at the end of this notebook.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook is useful for absolute beginners who are new to Python, through to intermediate users who I will introduce some more advanced features or a refresher. The code deliberately avoids functions where possible to keep things easy to read for all levels.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So let's get straight into it!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2. Install Packages","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #charting\nfrom scipy.stats import mode #statistics for slope\nfrom sklearn.metrics import mean_squared_error #error metric to optimise when we build a model\nfrom math import sqrt #Other math functions\nimport plotly.express as px #alternative charting function\nimport lightgbm as lgb #popular model choice\nimport seaborn as sns #alternative charting function\nimport gc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change display when printing .head for example the default settings only displays limited number of columns\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Import & Cleanse Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#The data we will be using is a foreign exhange rates dataset kindly provided to Kaggle. \nforex_df = pd.read_csv('/kaggle/input/foreign-exchange-rates-per-dollar-20002019/Foreign_Exchange_Rates.csv',engine = 'python')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's go ahead and preview the top or bottom n rows\nforex_df.tail(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that there is 'ND' in the above example for 25th December, markets closed for Christmas, so we could either drop this or apply some cleansing to enable us to convert to a numeric field.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#you can see that the currencies are 'objects' which effectively means they are strings. We will need to convert later on to numeric to enable calculations.\nforex_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#What is the number of rows, and columns in this dataset?\nforex_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1 Cleanse data and Convert to Numeric Format","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a list of all the currency columns in the dataset\ncurrency_list = ['AUSTRALIA - AUSTRALIAN DOLLAR/US$','EURO AREA - EURO/US$','NEW ZEALAND - NEW ZELAND DOLLAR/US$','UNITED KINGDOM - UNITED KINGDOM POUND/US$','BRAZIL - REAL/US$','CANADA - CANADIAN DOLLAR/US$','CHINA - YUAN/US$','HONG KONG - HONG KONG DOLLAR/US$','INDIA - INDIAN RUPEE/US$','KOREA - WON/US$','MEXICO - MEXICAN PESO/US$','SOUTH AFRICA - RAND/US$','SINGAPORE - SINGAPORE DOLLAR/US$','DENMARK - DANISH KRONE/US$','JAPAN - YEN/US$','MALAYSIA - RINGGIT/US$','NORWAY - NORWEGIAN KRONE/US$','SWEDEN - KRONA/US$','SRI LANKA - SRI LANKAN RUPEE/US$','SWITZERLAND - FRANC/US$','TAIWAN - NEW TAIWAN DOLLAR/US$','THAILAND - BAHT/US$']\n#cleanse data\nfor c in currency_list:\n    #ffill simply takes the previous row and applies it to the next row. We have conditioned this to only be applied to non numeric data.\n    forex_df[c] = forex_df[c].where(~forex_df[c].str.isalpha()).ffill()\n    #we then want to convert the currency columns into numeric so that we can apply functions to it.\n    forex_df[c] = pd.to_numeric(forex_df[c], errors='coerce') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check that this actually did what we intended.\nforex_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"note that the value for 24th Dec is carried forward to 25th Dec for those countries, but not for others that did not have this holiday.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's check that the columns are now numeric, yep that worked!\nforex_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Generate Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Generally with timeseries there are two main approaches. One being getting the data into a stationary format, accounting for the trend and seasonality. You then apply more traditional models you may have come across such as 'ARIMA', 'GARCH' etc. In this case, we won't be doing that, but will be applying machine learning models to the data directly without the need for this step which is simpler.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 4.1 Date Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#generate features\n\n# time features\nforex_df['date'] = pd.to_datetime(forex_df['Time Serie'])\nforex_df['year'] = forex_df['date'].dt.year\nforex_df['month'] = forex_df['date'].dt.month\nforex_df['week'] = forex_df['date'].dt.week\nforex_df['day'] = forex_df['date'].dt.day\nforex_df['dayofweek'] = forex_df['date'].dt.dayofweek\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Lag features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to shift the data by 1 or more days, so that we can use yesterday's data to predict today and so on. Later on we will remove today's data as we don't want to cause 'data leakage' whereby our model has information available to it that is not known at the time, which would not help it to work in practice.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lag features\nforex_df['lag_t1'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR/US$'].transform(lambda x: x.shift(1))\nforex_df['lag_t3'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR/US$'].transform(lambda x: x.shift(3))\nforex_df['lag_t7'] = forex_df['AUSTRALIA - AUSTRALIAN DOLLAR/US$'].transform(lambda x: x.shift(7))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also use other countries values from yesterday to aid in predicting today's data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lag other country features\nfor c in [x for x in currency_list if x != \"AUSTRALIA - AUSTRALIAN DOLLAR/US$\"]:\n    forex_df['lag_t1_%s' % c] = forex_df[c].transform(lambda x: x.shift(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We may want to add in a ratio as the raw value for GBP may not be as stable as a ratio value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio lag other country features\nfor c in [x for x in currency_list if x != \"AUSTRALIA - AUSTRALIAN DOLLAR/US$\"]:\n    forex_df['lag_t1_ratio_%s' % c] = forex_df['lag_t1']  / forex_df['lag_t1_' + c] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forex_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.3 Rolling Features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"With rolling features you can set min_periods as 1, that way you don't lose that much data (if you were to drop nulls in a subsequent step) as for example a 7 day rolling average the previous 6 days would be 'n/a'. This may help especially for longer lags e.g.365 days.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So let's go ahead and create a bunch of rolling features across different days and metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#rolling features\n#mean\nforex_df['rolling_mean_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).mean()\nforex_df['rolling_mean_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).mean()\nforex_df['rolling_mean_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).mean()\nforex_df['rolling_mean_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).mean()\nforex_df['rolling_mean_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).mean()\nforex_df['rolling_mean_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).mean()\n\n#max\nforex_df['rolling_max_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).max()\nforex_df['rolling_max_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).max()\nforex_df['rolling_max_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).max()\nforex_df['rolling_max_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).max()\nforex_df['rolling_max_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).max()\nforex_df['rolling_max_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).max()\n\n#min\nforex_df['rolling_min_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).min()\nforex_df['rolling_min_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).min()\nforex_df['rolling_min_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).min()\nforex_df['rolling_min_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).min()\nforex_df['rolling_min_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).min()\nforex_df['rolling_min_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).min()\n\n#standard deviation\nforex_df['rolling_std_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).std()\nforex_df['rolling_std_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).std()\nforex_df['rolling_std_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).std()\nforex_df['rolling_std_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).std()\nforex_df['rolling_std_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).std()\nforex_df['rolling_std_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).std()\n\n#median\nforex_df['rolling_med_t1_t7'] = forex_df['lag_t1'].rolling(7,min_periods=1).median()\nforex_df['rolling_med_t1_t14'] = forex_df['lag_t1'].rolling(14,min_periods=1).median()\nforex_df['rolling_med_t1_t28'] = forex_df['lag_t1'].rolling(28,min_periods=1).median()\nforex_df['rolling_med_t1_t90'] = forex_df['lag_t1'].rolling(90,min_periods=1).median()\nforex_df['rolling_med_t1_t180'] = forex_df['lag_t1'].rolling(180,min_periods=1).median()\nforex_df['rolling_med_t1_t360'] = forex_df['lag_t1'].rolling(360,min_periods=1).median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exponential moving averages provide more weight to recent values, which in finance are generally useful. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# exponential moving averages\nforex_df['rolling_ema_t1_t7'] = forex_df['lag_t1'].ewm(span=7,adjust=False).mean()\nforex_df['rolling_ema_t1_t14'] = forex_df['lag_t1'].ewm(span=14,adjust=False).mean()\nforex_df['rolling_ema_t1_t28'] = forex_df['lag_t1'].ewm(span=28,adjust=False).mean()\nforex_df['rolling_ema_t1_t90'] = forex_df['lag_t1'].ewm(span=90,adjust=False).mean()\nforex_df['rolling_ema_t1_t180'] = forex_df['lag_t1'].ewm(span=180,adjust=False).mean()\nforex_df['rolling_ema_t1_t360'] = forex_df['lag_t1'].ewm(span=360,adjust=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take a quick look at the data over time now that we have some features to compare against:\n# This is a relatively easy method to plot multiple values on a line chart plus it allows you to dynamically interact with the chart\ndf_long=pd.melt(forex_df, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR/US$', 'rolling_ema_t1_t7', 'rolling_mean_t1_t7', 'rolling_ema_t1_t360', 'rolling_med_t1_t360'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.4 Other Features - Decimals, Rounding, Mode, Coefficient of Variation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It's interesting to try decimal or rounded features if you remember, it can often give a small boost to results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#round the value to 0 decimals\nforex_df['lag_t1_round_0'] = forex_df['lag_t1'].round(0)\nforex_df['lag_t3_round_0'] = forex_df['lag_t3'].round(0)\nforex_df['lag_t7_round_0'] = forex_df['lag_t7'].round(0)\n\n#get the decimal place\nforex_df['lag_t1_dec'] = forex_df['lag_t1'] - forex_df['lag_t1_round_0']\nforex_df['lag_t3_dec'] = forex_df['lag_t3'] - forex_df['lag_t3_round_0']\nforex_df['lag_t7_dec'] = forex_df['lag_t7'] - forex_df['lag_t7_round_0']\n\n#round the value to 1 decimals, as the rounded value to 0 decimals is nearly always 1 in the case of AUD/USD\nforex_df['lag_t1_round_1'] = forex_df['lag_t1'].round(1)\nforex_df['lag_t3_round_1'] = forex_df['lag_t3'].round(1)\nforex_df['lag_t7_round_1'] = forex_df['lag_t7'].round(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mode is often overlooked, don't forget to give it a try when tackling your next problem","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#rolling mode of rounded figure\nforex_df['lag_t1_mode_7'] = forex_df['lag_t1_round_1'].rolling(window=7,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_14'] = forex_df['lag_t1_round_1'].rolling(window=14,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_28'] = forex_df['lag_t1_round_1'].rolling(window=28,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_90'] = forex_df['lag_t1_round_1'].rolling(window=90,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_180'] = forex_df['lag_t1_round_1'].rolling(window=180,min_periods=1).apply(lambda x: mode(x)[0])\nforex_df['lag_t1_mode_360'] = forex_df['lag_t1_round_1'].rolling(window=360,min_periods=1).apply(lambda x: mode(x)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#frequency of mode\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ranges\nforex_df['rolling_range_t1_t7'] = forex_df['rolling_max_t1_t7'] - forex_df['rolling_min_t1_t7']\nforex_df['rolling_range_t1_t14'] = forex_df['rolling_max_t1_t14'] - forex_df['rolling_min_t1_t14']\nforex_df['rolling_range_t1_t28'] = forex_df['rolling_max_t1_t28'] - forex_df['rolling_min_t1_t28']\nforex_df['rolling_range_t1_t90'] = forex_df['rolling_max_t1_t90'] - forex_df['rolling_min_t1_t90']\nforex_df['rolling_range_t1_t180'] = forex_df['rolling_max_t1_t180'] - forex_df['rolling_min_t1_t180']\nforex_df['rolling_range_t1_t360'] = forex_df['rolling_max_t1_t360'] - forex_df['rolling_min_t1_t360']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#coefficient of variation - the ratio of standard deviation to mean\nforex_df['rolling_coefvar_t1_t7'] =  forex_df['rolling_std_t1_t7'] / forex_df['rolling_mean_t1_t7']\nforex_df['rolling_coefvar_t1_t14'] = forex_df['rolling_std_t1_t14'] / forex_df['rolling_mean_t1_t14']\nforex_df['rolling_coefvar_t1_t28'] = forex_df['rolling_std_t1_t28'] / forex_df['rolling_mean_t1_t28']\nforex_df['rolling_coefvar_t1_t90'] = forex_df['rolling_std_t1_t90'] / forex_df['rolling_mean_t1_t90']\nforex_df['rolling_coefvar_t1_t180'] = forex_df['rolling_std_t1_t180'] / forex_df['rolling_mean_t1_t180']\nforex_df['rolling_coefvar_t1_t360'] = forex_df['rolling_std_t1_t360'] / forex_df['rolling_mean_t1_t360']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ratio of change to standard deviation\n#I like this because if the currency is normally volatile (high std dev), then a change in the rolling mean may be normal. \n#On the other hand if the currency is not normally volatile (low std dev), then it adds weight to any changes observed\nforex_df['rolling_meanstd_t1_t14'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t14']) / forex_df['rolling_std_t1_t14']\nforex_df['rolling_meanstd_t1_t28'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t28']) / forex_df['rolling_std_t1_t28']\nforex_df['rolling_meanstd_t1_t90'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t90']) / forex_df['rolling_std_t1_t90']\nforex_df['rolling_meanstd_t1_t180'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t180']) / forex_df['rolling_std_t1_t180']\nforex_df['rolling_meanstd_t1_t360'] = (forex_df['rolling_mean_t1_t7'] - forex_df['rolling_mean_t1_t360']) / forex_df['rolling_std_t1_t360']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cardinality is the number of unique values, e.g. [0,0,0,1,1] has 2 unique values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#cardinality\nforex_df['lag_t1_card_180'] = forex_df['lag_t1_round_1'].rolling(window=180,min_periods=1).apply(lambda x: np.unique(x).shape[0])\nforex_df['lag_t1_card_360'] = forex_df['lag_t1_round_1'].rolling(window=360,min_periods=1).apply(lambda x: np.unique(x).shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.5 Trends","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If the shorter moving average crosses over a longer one, it could be a trend indicator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#moving average crossover trends, 1 = positive, 0 = negative\nforex_df['lag_t1_trend_7'] = np.where(forex_df['lag_t1'] >= forex_df['rolling_ema_t1_t7'],1,0)\nforex_df['lag_t1_trend_14'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t14'],1,0)\nforex_df['lag_t1_trend_28'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t28'],1,0)\nforex_df['lag_t1_trend_90'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t90'],1,0)\nforex_df['lag_t1_trend_180'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t180'],1,0)\nforex_df['lag_t1_trend_360'] = np.where(forex_df['rolling_ema_t1_t7'] >= forex_df['rolling_ema_t1_t360'],1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of crossovers last n days\nforex_df['lag_t1_no_crossover_7'] = forex_df['lag_t1_trend_7'].rolling(window=7,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_14'] = forex_df['lag_t1_trend_14'].rolling(window=14,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_28'] = forex_df['lag_t1_trend_28'].rolling(window=28,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_90'] = forex_df['lag_t1_trend_90'].rolling(window=90,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_180'] = forex_df['lag_t1_trend_180'].rolling(window=180,min_periods=1).sum()\nforex_df['lag_t1_no_crossover_360'] = forex_df['lag_t1_trend_360'].rolling(window=360,min_periods=1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#decay","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#slope or 1st derivative\nforex_df['lag_t1_slope_7'] = forex_df['lag_t1'].rolling(7).apply(lambda x: np.polyfit(range(7), x, 1)[0]).values\nforex_df['lag_t1_slope_14'] = forex_df['lag_t1'].rolling(14).apply(lambda x: np.polyfit(range(14), x, 1)[0]).values\nforex_df['lag_t1_slope_28'] = forex_df['lag_t1'].rolling(28).apply(lambda x: np.polyfit(range(28), x, 1)[0]).values\nforex_df['lag_t1_slope_90'] = forex_df['lag_t1'].rolling(90).apply(lambda x: np.polyfit(range(90), x, 1)[0]).values\nforex_df['lag_t1_slope_180'] = forex_df['lag_t1'].rolling(180).apply(lambda x: np.polyfit(range(180), x, 1)[0]).values\nforex_df['lag_t1_slope_360'] = forex_df['lag_t1'].rolling(360).apply(lambda x: np.polyfit(range(360), x, 1)[0]).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2nd derivative, slope of the 1st derivative, again for detecting trend changes\nforex_df['lag_t1_deriv2_7'] = forex_df['lag_t1_slope_7'].rolling(7).apply(lambda x: np.polyfit(range(7), x, 1)[0]).values\nforex_df['lag_t1_deriv2_14'] = forex_df['lag_t1_slope_7'].rolling(14).apply(lambda x: np.polyfit(range(14), x, 1)[0]).values\nforex_df['lag_t1_deriv2_28'] = forex_df['lag_t1_slope_7'].rolling(28).apply(lambda x: np.polyfit(range(28), x, 1)[0]).values\nforex_df['lag_t1_deriv2_90'] = forex_df['lag_t1_slope_7'].rolling(90).apply(lambda x: np.polyfit(range(90), x, 1)[0]).values\nforex_df['lag_t1_deriv2_180'] = forex_df['lag_t1_slope_7'].rolling(180).apply(lambda x: np.polyfit(range(180), x, 1)[0]).values\nforex_df['lag_t1_deriv2_360'] = forex_df['lag_t1_slope_7'].rolling(360).apply(lambda x: np.polyfit(range(360), x, 1)[0]).values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forex_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Prepare Dataset for Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have a heap of features:\nlist(forex_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.1 Drop unnecessary columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Create a list of the features to drop, as previously mentioned we can't use the feature from today else it would cause target leakage - the model knows something that it can't know in advance.\nuseless_cols = ['Unnamed: 0', \n                \"date\", \n                'AUSTRALIA - AUSTRALIAN DOLLAR/US$',\n                'Time Serie', \n                'EURO AREA - EURO/US$',\n                 'NEW ZEALAND - NEW ZELAND DOLLAR/US$',\n                 'UNITED KINGDOM - UNITED KINGDOM POUND/US$',\n                 'BRAZIL - REAL/US$',\n                 'CANADA - CANADIAN DOLLAR/US$',\n                 'CHINA - YUAN/US$',\n                 'HONG KONG - HONG KONG DOLLAR/US$',\n                 'INDIA - INDIAN RUPEE/US$',\n                 'KOREA - WON/US$',\n                 'MEXICO - MEXICAN PESO/US$',\n                 'SOUTH AFRICA - RAND/US$',\n                 'SINGAPORE - SINGAPORE DOLLAR/US$',\n                 'DENMARK - DANISH KRONE/US$',\n                 'JAPAN - YEN/US$',\n                 'MALAYSIA - RINGGIT/US$',\n                 'NORWAY - NORWEGIAN KRONE/US$',\n                 'SWEDEN - KRONA/US$',\n                 'SRI LANKA - SRI LANKAN RUPEE/US$',\n                 'SWITZERLAND - FRANC/US$',\n                 'TAIWAN - NEW TAIWAN DOLLAR/US$',\n                 'THAILAND - BAHT/US$']\n\n#define train columns to use in model\ntrain_cols = forex_df.columns[~forex_df.columns.isin(useless_cols)]\n\n\n \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 6. Method 1 - Train/ Validation/ Test Split","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**6.1 Manually split data into Train/ Validation/ Test datasets based on the date**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> **How it Works**<br> \nUsing the date column, we will split the data into the necessary components. <br>\nHow might you decide what is the ideal split of data in training and test? Weigh up having more data in training against the chances of overfitting and vice versa. Run several iterations to compare performance. Also consider what the problem is trying to solve for, e.g. if you are predicting the next month, quarter or year of data for instance, you may want to take that into account for your validation set for example. <br> \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Advantages**<br> \nThe main benefit of this method is its simplicity and a great deal of control.  <br>\n**Disadvantages**<br> \nThe downside may be lower model performance, but not always.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's simply use historical data up until Oct 2019\nx_train = forex_df[forex_df['date'] <= '2019-10-31'].copy()\n#The variable we want to predict is AUD to USD rate.\ny_train = x_train['AUSTRALIA - AUSTRALIAN DOLLAR/US$']\n\n#The LGBM model needs a train and validation dataset to be fed into it, let's use Nov 2019\nx_val = forex_df[(forex_df['date'] > '2019-10-31') & (forex_df['date'] <= '2019-11-30')].copy()\ny_val = x_val['AUSTRALIA - AUSTRALIAN DOLLAR/US$']\n\n#We shall test the model on data it hasn't seen before or been used in the training process\ntest = forex_df[(forex_df['date'] > '2019-12-01')].copy()\n\n#Setup the data in the necessary format the LGB requires\ntrain_set = lgb.Dataset(x_train[train_cols], y_train)\nval_set = lgb.Dataset(x_val[train_cols], y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6.2 Model**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Light GBM will be used to build this model.\nLight GBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n*     Faster training speed and higher efficiency.\n\n*     Lower memory usage.\n\n*     Better accuracy.\n\n*     Support of parallel and GPU learning.\n\n*     Capable of handling large-scale data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set the model parameters\nparams = {\n        \"objective\" : \"regression\", # regression is the type of business case we are running\n        \"metric\" :\"rmse\", #root mean square error is a standard metric to use\n        \"learning_rate\" : 0.05, #the pace at which the model is allowed to reach it's objective of minimising the rsme.\n        'num_iterations' : 2000,\n        'num_leaves': 50, # minimum number of leaves in each boosting round\n        \"early_stopping\": 50, #if the model does not improve after this many consecutive rounds, call a halt to training\n        \"max_bin\": 200,\n        \"seed\":888\n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run the model\nm1_lgb = lgb.train(params, train_set, num_boost_round = 2500, valid_sets = [train_set, val_set], verbose_eval = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.3 Model Interpretation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot feature importance\nfeature_imp = pd.DataFrame({'Value':m1_lgb.feature_importance(),'Feature':train_cols})\nplt.figure(figsize=(20, 10))\nsns.set(font_scale = 1)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                    ascending=False)[0:40])\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows how many times the feature was used by the model. \n<br>We can see that the model did like a lot of the slope and derivative type features generated at the end.\n<br>The model also liked the ratio of other currencies a fair amount. NB when I ran with these excluded it did not make much difference to performance. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 6.4 Predictions on Test Data for out of time performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#generate predictions on test data\ny_pred = m1_lgb.predict(test[train_cols])\ntest['AUSTRALIA - AUSTRALIAN DOLLAR/US$_pred'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view the test data in chart form\ndf_long=pd.melt(test, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR/US$', 'AUSTRALIA - AUSTRALIAN DOLLAR/US$_pred'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the model generally captures trends well, there will always be spikes and volatility that is hard to predict.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 6.5 Model Performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE metric\nrms_m1 = sqrt(mean_squared_error(test['AUSTRALIA - AUSTRALIAN DOLLAR/US$'], test['AUSTRALIA - AUSTRALIAN DOLLAR/US$_pred']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms_m1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. TimeSeries Split","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> **How it Works**<br> \nAlso known as nested cross validation, the aim of this method is to avoid using the values from the future to forecast values in the past. <br>\nIf I simply used regular cross validation then the order of the train and validation data would be random and could incur the above issue. <br>\nThe training data becomes cumulative in order to achieve this e.g. \n\n*     fold 1 : training [1], test [2]\n*     fold 2 : training [1, 2], test [3]\n*     fold 3 : training [1, 2, 3], test [4]\n*     fold 4 : training [1, 2, 3, 4], test [5]\n*     fold 5 : training [1, 2, 3, 4, 5], test [6]\n\nIf it is still not clear, for illustrative purposes if you had data from Jan-Jun 2019, think of fold 1 being training [Jan] test [Feb]; fold 2 being training [Jan,Feb] test [Mar] etc. so that the test set is always occuring after the training set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 7.1 Apply Time Series Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing, metrics\nfrom sklearn.model_selection import KFold, TimeSeriesSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 100\nfolds = TimeSeriesSplit(n_splits=n_fold)\nsplits = folds.split(x_train, y_train)\n\ny_preds = np.zeros(test.shape[0])\ny_oof = np.zeros(x_train.shape[0])\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = train_cols\nmean_score = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set the model parameters\nparams = {\n        \"objective\" : \"regression\", # regression is the type of business case we are running\n        \"metric\" :\"rmse\", #root mean square error is a standard metric to use\n        \"learning_rate\" : 0.05, #the pace at which the model is allowed to reach it's objective of minimising the rsme.\n        'num_iterations' : 2000,\n        'num_leaves': 50, # minimum number of leaves in each boosting round\n        \"early_stopping\": 50, #if the model does not improve after this many consecutive rounds, call a halt to training\n        \"max_bin\": 200,\n        \"seed\":888\n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Include the additional month reserved from validation under the previous method\nx_train_tss = forex_df[forex_df['date'] <= '2019-11-30'].copy() #changed month from Oct to Nov\ny_train_tss = x_train_tss['AUSTRALIA - AUSTRALIAN DOLLAR/US$'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_n, (train_index, valid_index) in enumerate(splits):\n    print('Fold:',fold_n+1)\n    #generate the train and validation datasets for each fold\n    X_train1, X_valid1 = x_train_tss[train_cols].iloc[train_index], x_train_tss[train_cols].iloc[valid_index]\n    y_train1, y_valid1 = y_train_tss.iloc[train_index], y_train_tss.iloc[valid_index]\n    #convert the data into lgb format \n    dtrain = lgb.Dataset(X_train1, label=y_train1)\n    dvalid = lgb.Dataset(X_valid1, label=y_valid1)\n    #train lgb model, using same parameters as previous model for more direct comparison\n    clf = lgb.train(params, dtrain, valid_sets = [dtrain, dvalid], verbose_eval=100)\n    #feature importance\n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    #make predictions on validation set\n    y_pred_valid = clf.predict(X_valid1,num_iteration=clf.best_iteration)\n    y_oof[valid_index] = y_pred_valid\n    #validation score\n    val_score = np.sqrt(metrics.mean_squared_error(y_pred_valid, y_valid1))\n    print(f'val rmse score is {val_score}')\n    mean_score.append(val_score)\n    y_preds += clf.predict(test[train_cols], num_iteration=clf.best_iteration)/n_fold\n    del X_train1, X_valid1, y_train1, y_valid1\n    gc.collect()\nprint('mean rmse score over folds is',np.mean(mean_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7.2 Model Interpretation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.3 Predictions on Test Data for out of time performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#generate predictions on test data\ny_pred_ts_split = clf.predict(test[train_cols])\ntest['AUSTRALIA - AUSTRALIAN DOLLAR/US$_pred_ts_split'] = y_pred_ts_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.4 Model Performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_long=pd.melt(test, id_vars=['date'], value_vars=['AUSTRALIA - AUSTRALIAN DOLLAR/US$', 'AUSTRALIA - AUSTRALIAN DOLLAR/US$_pred','AUSTRALIA - AUSTRALIAN DOLLAR/US$_pred_ts_split'])\n\n# plotly \nfig = px.line(df_long, x='date', y='value', color='variable')\n\n# Show plot \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RSME metric\nrms_ts_split = sqrt(mean_squared_error(test['AUSTRALIA - AUSTRALIAN DOLLAR/US$'], test['AUSTRALIA - AUSTRALIAN DOLLAR/US$_pred_ts_split']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms_ts_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Summary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ultimately the nature of the business problem will show through trial and error the appropriate method you may want to use. In this example the first method was slightly better against this particular dataset and modelling technique. This is also tradeoff in terms of model run time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Thanks for viewing this notebook and wish you a good day :) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}