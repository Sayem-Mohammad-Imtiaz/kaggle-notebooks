{"cells":[{"metadata":{"_uuid":"f3b7fb38647eae958d17f5fb82b1a9326c005114"},"cell_type":"markdown","source":"# Building a Machine Learning Model For Predicting Employee Retention"},{"metadata":{"_uuid":"3e787d13d6143ac7fe9a41a4cd3b685e92bbe417"},"cell_type":"markdown","source":"### IMPORTANT: This notebook is still a work in progress. I still need to present the rationale behind the code written to solve the problem."},{"metadata":{"_uuid":"7d9863520d9bf24e42df9b7ca8461117dcf85308"},"cell_type":"markdown","source":"**Importing and Configuring Necessary Packages**"},{"metadata":{"trusted":true,"_uuid":"eabe0f64df9a30b4412cc5d9c19782a5f1481108"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\nimport time\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Configuring Plot Appearance\n\n%config InlineBackend.figure_format='retina'\nsns.set() # Revert to matplotlib defaults\nplt.rcParams['figure.figsize'] = (9, 6)\nplt.rcParams['axes.labelpad'] = 10\nsns.set_style(\"darkgrid\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3cd914e1924560043ff3ed277fe9e7f9a9f5503"},"cell_type":"markdown","source":"**Prerequisites and General Information for Reproducing this Notebook**"},{"metadata":{"trusted":true,"_uuid":"d407877d706e48c1325decb2c0ec712f7d0d1721"},"cell_type":"code","source":"%load_ext version_information\n%version_information pandas, numpy, matplotlib, seaborn, sklearn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d891234570728d86999b6ae37bf5af849d0f196"},"cell_type":"markdown","source":"**Importing the Dataset into a Pandas DataFrame**"},{"metadata":{"trusted":true,"_uuid":"4ee158b2f04ed6c4925642b18514bbc36af75b43"},"cell_type":"code","source":"hr_data_df = pd.read_csv('hr_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57c21a4a97ec252a38505a64514954fc29bda22b"},"cell_type":"code","source":"hr_data_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2dbc062c2d2738db9c052a8b3871c161d8b4a35"},"cell_type":"code","source":"hr_data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8214715ed7efb7e22c14d8b2041241f10fc238f"},"cell_type":"code","source":"hr_data_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb7e2da7df44ec5cb2edf4610162ae5e5e3e3dc6"},"cell_type":"markdown","source":"**Assesssing the Target Variable ('left')**"},{"metadata":{"trusted":true,"_uuid":"540cd075cdcb56dabc4c46b8c40ae170c3f2ede0"},"cell_type":"code","source":"hr_data_df.left.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a017703d4a0bc7700d3daf5b9f78f5dde5f27c2a"},"cell_type":"code","source":"# How is the target distributed?\nhr_data_df.left.value_counts().plot('barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c95037edb47ea7e49a4427ce57078573f4467593"},"cell_type":"code","source":"# Is data missing?\nhr_data_df.left.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f785761faf50fc3f6f3831ea7920e554673c2d6"},"cell_type":"markdown","source":"**Assessing the Features**"},{"metadata":{"trusted":true,"_uuid":"117cb8f8dff24e7879b08341a29d4729c7520206"},"cell_type":"code","source":"# Which of the features are numerical (continuous or discrete) and which are categorical?\nhr_data_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59bea46390ce6204e66de847ef8436c37fcdd118"},"cell_type":"code","source":"# Plotting Feature Distributions\n\nfor f in hr_data_df.columns:\n    fig = plt.figure()\n    s = hr_data_df[f]\n    if s.dtype in ('float', 'int'): # histograms for numerical features\n        num_bins = min((30, len(hr_data_df[f].unique())))\n        s.hist(bins=num_bins)\n    else:                           # bar plots for categorical features\n        s.value_counts().plot.bar()    \n    plt.xlabel(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccadb9caaaacd08641d33992477ac89da5c91f43"},"cell_type":"code","source":"# Percentage of Missing Values (NaNs) for each Feature\nhr_data_df.isnull().sum() / len(hr_data_df) * 100","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32cc0c3f3167fbda16d3ac2f0bf6d7f67c76dee0"},"cell_type":"markdown","source":"**Preprocessing and Cleaning the Dataset**"},{"metadata":{"trusted":true,"_uuid":"100edd0f2e8b406da2e9902212a617f729e987c0"},"cell_type":"code","source":"# Removing 'is_smoker' since it is full of missing values\ndel hr_data_df['is_smoker']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"675c256dcdc90cc0189f777ffa11f82d72437e60"},"cell_type":"code","source":"# Filling 'time_spend_company' Missing Values with Its Median (More Adequate for Discrete Numerical Features)\nhr_data_df.time_spend_company = hr_data_df.time_spend_company.fillna(hr_data_df.time_spend_company.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9143d52907197c13dcf7d700db58918d197ed1cf"},"cell_type":"code","source":"# Checking that the values were correctly filled in\nhr_data_df.isnull().sum() / len(hr_data_df) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ada6e8339c88c7effd721f12132af9ad79461aa1"},"cell_type":"code","source":"# Trying to take advantage of the relation between average_montly_hours and number_project\nsns.boxplot(x='number_project', y='average_montly_hours', data=hr_data_df)\nplt.savefig('employee-retention-hours-num-proj-boxplot.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc1aafeb9711ad76e76a312fcb77a227028750a0"},"cell_type":"code","source":"# Calculating fill values for average_montly_hours given a number of projects\n# This will result in more accurate fill values\n\nmean_per_number_project = hr_data_df.groupby('number_project').average_montly_hours.mean()\nmean_per_project = dict(mean_per_number_project)\nmean_per_number_project","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b01528b13433b27d8262ea606cfff735244f0834"},"cell_type":"code","source":"# Fill in average_monthly_hours with the appropriate values\n\nhr_data_df.average_montly_hours = hr_data_df.average_montly_hours.fillna(hr_data_df.number_project.map(mean_per_number_project))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2829196b6d115bdbd014bae2899b648187d0749"},"cell_type":"code","source":"# Checking that the values were correctly filled in\nhr_data_df.isnull().sum() / len(hr_data_df) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"232371d41f3af39eb9b6db329faa74b1dc609a37"},"cell_type":"code","source":"# Converting categorical features to binary integer representation and to one-hot encoding\n\nhr_data_df.left = hr_data_df.left.map({'no': 0, 'yes': 1})\nhr_data_df = pd.get_dummies(hr_data_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c8dfd942ffd363275ece84f3176ac91c4771aa5"},"cell_type":"code","source":"# Verifying the final processed dataset\n\nhr_data_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4524164bc2400b804cc323bac212855674152da"},"cell_type":"code","source":"# Saving the preprocessed dataset to a file, which will then be used as input to the learning algorithm\nhr_data_df.to_csv('hr_data_preprocessed.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5db390d7376591cf361c15bafeaf21cb9cada3be"},"cell_type":"markdown","source":"**Training Classification Models**"},{"metadata":{"trusted":true,"_uuid":"bd1ee4b6a05635e0cb0e190661c0f91303226ac6"},"cell_type":"code","source":"# Loading the processed data\n\nhr_data_df = pd.read_csv('hr_data_preprocessed.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8cc27401277bde53b1e07ff81f6e7306c8feeac"},"cell_type":"code","source":"# The two features we'll use for training in this section\n# These features are going to be used only for illustration purposes, to show how the different models work.\n# At the end of this section, we will build a learning model based on all the features.\n\nsns.jointplot('satisfaction_level', 'last_evaluation', data=hr_data_df, kind='hex')\nplt.savefig('employee-retention-satisfaction-evaluation-jointplot.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57f4777c3a24d0928cfdf7d155575fffcde810c4"},"cell_type":"code","source":"# Segmenting the plot by the target variable\n\nfig, ax = plt.subplots()\nplot_args = dict(shade=True, shade_lowest=False)\n\nfor i, c in zip((0, 1), ('Reds', 'Blues')):\n    sns.kdeplot(hr_data_df.loc[hr_data_df.left==i, 'satisfaction_level'],\n                hr_data_df.loc[hr_data_df.left==i, 'last_evaluation'],\n                cmap=c, **plot_args)\n\nax.text(0.05, 1.05, 'left = 0', size=16, color=sns.color_palette('Reds')[-2])\nax.text(0.35, 1.05, 'left = 1', size=16, color=sns.color_palette('Blues')[-2])\nplt.savefig('employee-retention-satisfaction-evaluation-bivariate-segmented.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9567f6efc526771c26d0fae5cbbdc617133d3459"},"cell_type":"markdown","source":"**Training a Very Silly SVM model (Obvious Guessing)**"},{"metadata":{"trusted":true,"_uuid":"bd2d0baef772dd3ee961063d0dba2d48305b77b4"},"cell_type":"code","source":"# Splitting the dataset into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nfeatures = ['satisfaction_level', 'last_evaluation']\nX_train, X_test, y_train, y_test = train_test_split(\n    hr_data_df[features].values, hr_data_df['left'].values,\n    test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31addfb885e1cbbe425adbb8b7260e1ad8aba63a"},"cell_type":"code","source":"# Scale the data for SVMs and K-Nearest Neighbors\n# We should do this scaling operation always AFTER splitting the dataset\n# Test data should not be influenced by operations on training data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"453309ee7236e1c006248d12113d13ae3ea1f0bc"},"cell_type":"code","source":"# Train a support vector machine classifier\n# The \"very silly\" qualifier of our initial SVM model comes from the rather naïve linear kernel for a nonlinear problem\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(kernel='linear', C=1, random_state=1)\nsvm.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"936cea15d818af7013cd018d191e886f2f8e3906"},"cell_type":"code","source":"# Determining its classification accuracy\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred = svm.predict(X_test_scaled)\nacc = accuracy_score(y_test, y_pred)\nprint('accuracy = {:.1f}%'.format(acc*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e338b593a3f85ea30c9347baac4aeff02b7b6927"},"cell_type":"code","source":"# Determining its accuracy by class\n# Here, we clearly see the \"obvious guessing\" decision process of our silly model\n\nfrom sklearn.metrics import confusion_matrix\n\nprint('percent accuracy score per class:')\ncmat = confusion_matrix(y_test, y_pred)\nscores = cmat.diagonal() / cmat.sum(axis=1) * 100\nprint('left = 0 : {:.2f}%'.format(scores[0]))\nprint('left = 1 : {:.2f}%'.format(scores[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80574905a717b01bc0dfa252d4748cab369b4cbe"},"cell_type":"code","source":"# Plot the resulting decision regions\n# Note that all samples are being classified as left = 0\n\nfrom mlxtend.plotting import plot_decision_regions\n\nN_samples = 200\nX, y = X_train_scaled[:N_samples], y_train[:N_samples]\nplot_decision_regions(X, y, clf=svm);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7638e5215898e1d94e0dd99ae5eca3e0a963f5b0"},"cell_type":"markdown","source":"**Training a SVM model with a Nonlinear Kernel (Kernel Trick)**"},{"metadata":{"trusted":true,"_uuid":"3fe89ffdc2d35b87d0fd3cbd52aa372478a1f517"},"cell_type":"code","source":"# Training a more adequate SVM model for a nonlinear problem by means of radial basis functions\n\nsvm = SVC(kernel='rbf', C=1, random_state=1)\nsvm.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef06815266585f78c2831c3b09fea5f1950f2d50"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom IPython.display import display\nfrom mlxtend.plotting import plot_decision_regions\n\ndef check_model_fit(clf, X_test, y_test):\n    # Print overall test-set accuracy\n    y_pred = clf.predict(X_test)\n    acc = accuracy_score(y_test, y_pred, normalize=True) * 100\n    print('total accuracy = {:.1f}%'.format(acc))\n    \n    # Print confusion matrix\n    cmat = confusion_matrix(y_test, y_pred)\n    cols = pd.MultiIndex.from_tuples([('predictions', 0), ('predictions', 1)])\n    indx = pd.MultiIndex.from_tuples([('actual', 0), ('actual', 1)])\n    display(pd.DataFrame(cmat, columns=cols, index=indx))\n    print()\n    \n    # Print test-set accuracy grouped by the target variable \n    print('percent accuracy score per class:')\n    cmat = confusion_matrix(y_test, y_pred)\n    scores = cmat.diagonal() / cmat.sum(axis=1) * 100\n    print('left = 0 : {:.2f}%'.format(scores[0]))\n    print('left = 1 : {:.2f}%'.format(scores[1]))\n    print()\n    \n    # Plot decision regions\n    fig = plt.figure(figsize=(8, 8))\n    N_samples = 200\n    X, y = X_test[:N_samples], y_test[:N_samples]\n    plot_decision_regions(X, y, clf=clf)\n    \n    plt.xlabel('satisfaction_level')\n    plt.ylabel('last_evaluation')\n    plt.legend(loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2ea2e0e2b0991cd59f72b922b4af5435501702b"},"cell_type":"code","source":"check_model_fit(svm, X_test_scaled, y_test)\nplt.savefig('employee-retention-svm-rbf.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"197e813f66585a83c27934c82644f866621af3bb"},"cell_type":"markdown","source":"**Training a k-Neighbors Classifier (Effect of Overfitting)**"},{"metadata":{"trusted":true,"_uuid":"1553f13dca166306fd6fc8ead9644b782523be1a"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train_scaled, y_train)\n\ncheck_model_fit(knn, X_test_scaled, y_test)\nplt.savefig('employee-retention-knn-overfit.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"593d4d8d4aa44d44eea43602173888e81081a0bc"},"cell_type":"code","source":"# Increasing the number of \"nearest neighbors\" to reduce overfitting\n\nknn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(X_train_scaled, y_train)\n\ncheck_model_fit(knn, X_test_scaled, y_test)\nplt.savefig('employee-retention-knn.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d7ff66f42ac36a00b0476c3be151265f112eab0"},"cell_type":"markdown","source":"**Training a Random Forest Classifier**"},{"metadata":{"trusted":true,"_uuid":"ce4c81e6241e9a8d68271b3b3a47958e1734ef5c"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Please, remember to limit max_depth in order to reduce overfitting\nforest = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=1)\nforest.fit(X_train, y_train)\n\ncheck_model_fit(forest, X_test, y_test)\nplt.xlim(-0.1, 1.2)\nplt.ylim(0.2, 1.2)\nplt.savefig('employee-retention-forest.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d13d0eb4e1a15dc09884ab6521220512d092663"},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport graphviz\n\ndot_data = export_graphviz(\n                forest.estimators_[0],\n                out_file=None, \n                feature_names=features,  \n                class_names=['no', 'yes'],  \n                filled=True, rounded=True,  \n                special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d191a1f392b5bd65c78ee9da41a59f4101e9a7f"},"cell_type":"markdown","source":"**Working with K-Fold Cross Validation and Plotting Validation Curves to Assess Overfitting**"},{"metadata":{"trusted":true,"_uuid":"6013fcc36609308929f77686ca49ea54188e4ee6"},"cell_type":"code","source":"# Stratified k-fold cross validation\n# Model's predictive accuracy calculation\n\nfrom sklearn.model_selection import cross_val_score\n\nX = hr_data_df[features].values\ny = hr_data_df.left.values\n\n# Instantiate the model\nclf = RandomForestClassifier(n_estimators=100, max_depth=5)\n\nnp.random.seed(1) # for reproducibility purposes\nscores = cross_val_score(estimator=clf, X=X, y=y, cv=10)\n\nprint('accuracy = {:.3f} +/- {:.3f}'.format(scores.mean(), scores.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c153d2202457b11860740300fcbb28d9eb683b3b"},"cell_type":"code","source":"# Custom function for class accuracy calculation\n\nfrom sklearn.model_selection import StratifiedKFold\n\ndef cross_val_class_score(clf, X, y, cv=10):\n    kfold = StratifiedKFold(n_splits=cv).split(X, y)\n\n    class_accuracy = []\n    for k, (train, test) in enumerate(kfold):\n        clf.fit(X[train], y[train])\n        y_test = y[test]\n        y_pred = clf.predict(X[test])\n        cmat = confusion_matrix(y_test, y_pred)\n        class_acc = cmat.diagonal()/cmat.sum(axis=1)\n        class_accuracy.append(class_acc)\n        print('fold: {:d} accuracy: {:s}'.format(k+1, str(class_acc)))\n        \n    return np.array(class_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dc5363e80b5296da6db1c754beafb047a119058"},"cell_type":"code","source":"# Stratified k-fold cross validation\n# This time, including class accuracy calculation\n\nnp.random.seed(1) # for reproducibility purposes\nscores = cross_val_class_score(clf, X, y)\n\nprint('accuracy = {} +/- {}'.format(scores.mean(axis=0), scores.std(axis=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20759fdceaede074948b110830c8a8315a81cd0d"},"cell_type":"code","source":"# Calcualte a validation curve\n\nfrom sklearn.model_selection import validation_curve\n\nclf = RandomForestClassifier(n_estimators=10)\nmax_depths = np.arange(3, 16, 3)\n\ntrain_scores, test_scores = validation_curve(\n            estimator=clf,\n            X=X,\n            y=y,\n            param_name='max_depth',\n            param_range=max_depths,\n            cv=10);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bbbe5c5d1be6af0844a3107438907134749a1b9"},"cell_type":"code","source":"# Function to draw the validation curve\n\ndef plot_validation_curve(train_scores, test_scores,\n                          param_range, xlabel='', log=False):\n    '''\n    This code is from scikit-learn docs:\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n    '''\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    fig = plt.figure()\n    \n    # Accuracy on training set\n    \n    plt.plot(param_range, train_mean, \n             color=sns.color_palette('Set1')[1], marker='o', \n             markersize=5, label='training accuracy')\n\n    plt.fill_between(param_range, train_mean + train_std,\n                     train_mean - train_std, alpha=0.15,\n                     color=sns.color_palette('Set1')[1])\n\n    # Accuracy on testing set\n    \n    plt.plot(param_range, test_mean, \n             color=sns.color_palette('Set1')[0], linestyle='--', \n             marker='s', markersize=5, \n             label='validation accuracy')\n\n    plt.fill_between(param_range, \n                     test_mean + test_std,\n                     test_mean - test_std, \n                     alpha=0.15, color=sns.color_palette('Set1')[0])\n\n    if log:\n        plt.xscale('log')\n    plt.legend(loc='lower right')\n    if xlabel:\n        plt.xlabel(xlabel)\n    plt.ylabel('Accuracy')\n    plt.ylim(0.9, 1.0)\n    return fig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc87d4d155afd1a8ee6369591de56e7d06ace7ae"},"cell_type":"code","source":"# The max_depth parameter to be used is determined by the \"elbow\" in validation curves\n# In this case, max_depth would equal 6\n\nplot_validation_curve(train_scores, test_scores, max_depths, xlabel='max_depth')\nplt.ylim(0.9, 0.95)\nplt.savefig('employee-retention-validation-curve-overfitting.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9eb86d7661e2f10c9d829c596a42932dce83060"},"cell_type":"markdown","source":"**Training a Random Forest Classifier for our Employee Retention Problem**\n\n*Do not forget to emphasize the high knowledge extraction capability of decision trees. It suits very well our business needs for this problem.*"},{"metadata":{"trusted":true,"_uuid":"da771f4993f3358051ccdb15649df561f8bddeb2"},"cell_type":"code","source":"features = ['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_montly_hours', 'time_spend_company', 'work_accident',\n       'promotion_last_5years', 'department_IT', 'department_RandD',\n       'department_accounting', 'department_hr', 'department_management',\n       'department_marketing', 'department_product_mng', 'department_sales',\n       'department_support', 'department_technical', 'salary_high',\n       'salary_low', 'salary_medium']\n\nX = hr_data_df[features].values\ny = hr_data_df.left.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"354d9bbffb68fd44951e0dd059b16677bc00ee57"},"cell_type":"code","source":"# Jupyter magic function to assess CPU usage time\n%time\n\n# Calculating a validation curve for max_depth using a Random Forest classifier\n\nnp.random.seed(1)\nclf = RandomForestClassifier(n_estimators=20)\nmax_depths = [3, 4, 5, 6, 7,\n              9, 12, 15, 18, 21]\nprint('Training {} models ...'.format(len(max_depths)))\ntrain_scores, test_scores = validation_curve(\n            estimator=clf,\n            X=X,\n            y=y,\n            param_name='max_depth',\n            param_range=max_depths,\n            cv=5);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"967a071f02633c97950c510b7a5626e2ad201389"},"cell_type":"code","source":"# Drawing the validation curve\n# Note the \"elbow\" at max_depth = 6\n\nplot_validation_curve(train_scores, test_scores, max_depths, xlabel='max_depth')\nplt.xlim(3, 21)\nplt.savefig('employee-retention-max-depth-val.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0257aff9cbb20c70a507105011f3e6e1fc8cb8c1"},"cell_type":"code","source":"# Performing a k-fold cross validation for the selected model:\n# a random forest with max_depth = 6 and n_estimators = 200\n\nnp.random.seed(1)\nclf = RandomForestClassifier(n_estimators=200, max_depth=6)\nscores = cross_val_class_score(clf, X, y)\n\nprint('accuracy = {} +/- {}'.format(scores.mean(axis=0), scores.std(axis=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e54c87ca3c6afc065a3e5fb6917ac56985091aa9"},"cell_type":"code","source":"# Box plot of result\n# Note the higher uncertainty for 'left = 1' class (class imbalance)\n\nfig = plt.figure(figsize=(5, 7))\nsns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]), palette=sns.color_palette('Set1'))\nplt.xlabel('Left')\nplt.ylabel('Accuracy')\nplt.savefig('employee-retention-full-acc-wo-pca.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ab4e61eee30d06b48844f681e9cbd1c5f240b3f"},"cell_type":"code","source":"# Visualizing the feature importances\n# Useful for extracting from the model knowledge about why employees are leaving, in accordance with business needs\n\npd.Series(clf.feature_importances_, name='Feature Importance', index=hr_data_df[features].columns).sort_values().plot.barh()\nplt.xlabel('Feature Importance')\nplt.savefig('employee-retention-full-feature-importance.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d86b629fd48f65faf0daaffed24096422525d1a"},"cell_type":"markdown","source":"**Using Principal Component Analysis for Dimensionality Reduction (On the Features with Low Importance)**"},{"metadata":{"trusted":true,"_uuid":"dad6a13cb2302da4056271e8dd9659502590a57b"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca_features = ['work_accident', 'salary_low', 'salary_high', 'salary_medium',\n       'promotion_last_5years', 'department_RandD', 'department_hr',\n       'department_technical', 'department_support',\n       'department_management', 'department_sales',\n       'department_accounting', 'department_IT', 'department_product_mng',\n       'department_marketing']\n\nX_reduce = hr_data_df[pca_features]\n\npca = PCA(n_components=3)\npca.fit(X_reduce)\nX_pca = pca.transform(X_reduce)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ccd8ab8aa5fde7c08fb57e873f3b18eeb837a49"},"cell_type":"code","source":"# Adding principal components to hr_data_df\n\nhr_data_df['first_principle_component'] = X_pca.T[0]\nhr_data_df['second_principle_component'] = X_pca.T[1]\nhr_data_df['third_principle_component'] = X_pca.T[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a5747927b33b8441216104e204e081a218225c1"},"cell_type":"code","source":"# Selecting reduced-dimension feature set\n\nfeatures = ['satisfaction_level', 'number_project', 'time_spend_company',\n            'average_montly_hours', 'last_evaluation',\n            'first_principle_component',\n            'second_principle_component',\n            'third_principle_component']\n\nX = hr_data_df[features].values\ny = hr_data_df.left.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4f5676e3f8e12846021b212e29395036a91ec31"},"cell_type":"code","source":"# Performing a (k=10)-fold cross validation for the selected model with reduced dimensionality:\n# a random forest with max_depth = 6 and n_estimators = 200\n\nnp.random.seed(1)\nclf = RandomForestClassifier(n_estimators=200, max_depth=6)\nscores = cross_val_class_score(clf, X, y)\n\nprint('accuracy = {} +/- {}'.format(scores.mean(axis=0), scores.std(axis=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61d0dd86452bce67184504fbef7c6bc7a6d56220"},"cell_type":"code","source":"# Box plot of result\n# Note the higher accuracy and lower standard deviation for 'left = 1' class\n# However, class imbalance still plays a quite significant role in the accuracy values\n\nfig = plt.figure(figsize=(5, 7))\nsns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]), palette=sns.color_palette('Set1'))\nplt.xlabel('Left')\nplt.ylabel('Accuracy')\nplt.savefig('employee-retention-full-acc-pca.png', bbox_inches='tight', dpi=300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a962c6a693df4b8320330015caa6952dece581c"},"cell_type":"code","source":"# IMPORTANT FINAL STEP: Train the final model on all the samples\n\nnp.random.seed(1)\nclf = RandomForestClassifier(n_estimators=200, max_depth=6)\nclf.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cacd9f34fdc995baf036d19c1e82b7b2910e549"},"cell_type":"code","source":"# Saving the model for use in an external application\n\nfrom sklearn.externals import joblib\njoblib.dump(clf, 'random-forest-trained.pkl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d49ff0a8b7c031af9f66778b10580ecd5e08293"},"cell_type":"markdown","source":"**Usage Example of the Trained Model by an External Application**"},{"metadata":{"trusted":true,"_uuid":"884f1a6ce1bfa9f5c3ce560205536e72000db4d2"},"cell_type":"code","source":"# Load model from pkl file\n\nclf = joblib.load('random-forest-trained.pkl')\nclf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0076fdcad14108a9c0c53043a35e5516fb8d0ef1"},"cell_type":"code","source":"# Example of using the model for a specific employee\n\nsandra = hr_data_df.iloc[573]\nX = sandra[features]\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4448e0f317d6fc8b356afcdcb922cf8458248259"},"cell_type":"code","source":"# Predict the class label for Sandra\n# She would LEAVE the job\n\nclf.predict([list(X.values)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be2053166297a648e7ff9aea649bfb49b34ee858"},"cell_type":"code","source":"# Predict the probability of class labels for Sandra\n# P('left = 0') = 0.06576239\n# P('left = 1') = 0.93423761\n\nclf.predict_proba([X])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}