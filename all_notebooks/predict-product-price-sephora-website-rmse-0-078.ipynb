{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sephora Website\n","metadata":{}},{"cell_type":"markdown","source":"## Dataset\nThe dataset was collected by **Raghad Alharbi** using web scraping methods like selenium and beautiful soup to collect more than 1,000 useful records from Sephora website.\n\n## Goals\nPredict the price of product based on the features available\n\n## Objective\nThe objective is to analyze product based on several variables, determine what variables affect product price the most, then build a model that can predict the price of a Product.","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint('numpy version : ',np.__version__)\nprint('pandas version : ',pd.__version__)\nprint('seaborn version : ',sns.__version__)\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aadasd = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20.7,8.27)})\nsns.set_style(\"whitegrid\")\nsns.color_palette(\"dark\")\nplt.style.use(\"fivethirtyeight\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import rcParams\nrcParams['figure.figsize'] = 12, 4\nrcParams['lines.linewidth'] = 3\nrcParams['xtick.labelsize'] = 'x-large'\nrcParams['ytick.labelsize'] = 'x-large'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/all-products-available-on-sephora-website/sephora_website_dataset.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Description","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the information above, it shows us: \n* Dataframe has a total of 9268 rows and 21 columns \n* Target Regression is the column 'value_price' with data type 'float64' ","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data numeric","metadata":{}},{"cell_type":"code","source":"numeric=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf_num=df.select_dtypes(include=numeric)\ndf_num.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data categorical","metadata":{}},{"cell_type":"code","source":"df_cat=df.select_dtypes(include='object')\ndf_cat.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Numerical Approach","metadata":{}},{"cell_type":"markdown","source":"### Statistical Summary","metadata":{}},{"cell_type":"code","source":"describeNum = df.describe(include =['float64', 'int64', 'float', 'int'])\ndescribeNum.T.style.background_gradient(cmap='viridis',low=0.2,high=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the table above can be seen some columns that have abnormal data distribution among them because it has mean values and medians that are far linked.","metadata":{}},{"cell_type":"code","source":"describeNumCat = df.describe(include=[\"O\"])\ndescribeNumCat.T.style.background_gradient(cmap='viridis',low=0.2,high=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Value Counting","metadata":{}},{"cell_type":"code","source":"cats = ['brand','category', 'name', 'size'] \nfor col in cats:\n    print(f'''Value count kolom {col}:''')\n    print(df[col].value_counts())\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Graphic Approach","metadata":{}},{"cell_type":"markdown","source":"### Correlation heatmap","metadata":{}},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['rating', 'number_of_reviews', 'love', 'price', 'value_price', 'online_only', 'exclusive', 'limited_edition', 'limited_time_offer']\n\nplt.figure(figsize=(30,20))\nax = sns.heatmap(data = df[features].corr(),cmap='YlGnBu',annot=True)\n\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5,top - 0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis results From Correlation Heatmap**\n- based on the picture above can be seen that the 'price' and 'value_price' features have a correlation of 0.99, then it is necessary to check further whether these two features have the same actual value but different column names only? \n- 'love' and 'number_of_review' features can also be seen to have a fairly high correlation value of 0.74.","metadata":{}},{"cell_type":"markdown","source":"### Scatter plot","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\n_ = plt.scatter(x=df['price'], y=df['value_price'], edgecolors=\"#000000\", linewidths=0.5)\n_ = ax.set(xlabel=\"price\", ylabel=\"value_price\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\n_ = plt.scatter(x=df['love'], y=df['number_of_reviews'], edgecolors=\"#000000\", linewidths=0.5)\n_ = ax.set(xlabel=\"love\", ylabel=\"number_of_reviews\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boxplot","metadata":{}},{"cell_type":"code","source":"features = ['number_of_reviews', 'love', 'price', 'value_price']\nplt.figure(figsize=(20, 8))\nfor i in range(0, len(features)):\n    plt.subplot(1, 7, i+1)\n    sns.boxplot(y=df[features[i]],color='green',orient='v')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis Variable Dependent \"value_price\"","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\n\nplt.subplot(1,2,1)\nplt.title('Sale Price Distribution Plot')\nsns.distplot(df.value_price)\n\nplt.subplot(1,2,2)\nplt.title('Sale Price Spread')\nsns.boxplot(y=df.value_price)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.value_price.describe(percentiles = [0.25,0.50,0.75,0.85,0.90,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GET SKEWNESS \nprint(f\"Skewness Co-efficient: {round(df.value_price.skew(), 3)}\")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), dpi=300)\n\n# HISTOGRAM \nfrom scipy import stats\nsns.distplot(df['value_price'] , fit=stats.norm, ax=ax1)\nax1.set_title('Histogram')\n\n# PROBABILITY / QQ PLOT\nstats.probplot(df['value_price'], plot=ax2)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to analyze the column 'value_price', because the target variable is numeric then look at the histogram whether distributed normally or not. in the column, in the 'value_price' column, we can see a positive skewed because the tail of the distribution is to the right of the most value. That is, most distributions are in low value. So, the target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed. We will apply log transformation to the feature to make the distribution close to gaussian. We will apply log(1+x) transformation to avoid 0 values (if present)","metadata":{}},{"cell_type":"code","source":"df[\"value_price\"] = np.log1p(df[\"value_price\"])\n\n# GET SKEWNESS \nprint(f\"Skewness Co-efficient: {round(df.value_price.skew(), 3)}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), dpi=300)\n\n# HISTOGRAM \nfrom scipy import stats\nsns.distplot(df['value_price'] , fit=stats.norm, ax=ax1)\nax1.set_title('Histogram')\n# PROBABILITY / QQ PLOT\nstats.probplot(df['value_price'], plot=ax2)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After transformation the skewness has reduced from 3.143 to 0.31, and the plot now looks close to the normal distribution and the probability plot can confirm the same.","metadata":{}},{"cell_type":"markdown","source":"### Price and Value_price Check Similarity ","metadata":{}},{"cell_type":"code","source":"for index, row in df.iterrows():\n    if row['price']!=row['value_price']:\n        print(index, row['price'], row['value_price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can be seen that indeed these two tables have many different values therefore it is then decided to keep these two features.","metadata":{}},{"cell_type":"markdown","source":"### What Brand Got the Highest Number of Reviews","metadata":{}},{"cell_type":"code","source":"bestBrandReviews = df.groupby([\"brand\"]).head()\nbestBrandReviews = bestBrandReviews.sort_values('number_of_reviews', ascending=False)\nbestBrandReviews.head(10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Most Popular Product Based on Rating","metadata":{}},{"cell_type":"code","source":"rating_products = pd.DataFrame(round(df.groupby('brand')['rating'].mean(),2))\nmost_rating = rating_products.sort_values('rating', ascending=False)\nmost_rating.head(10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Brand **Four Sigmatic** which is the most popular brand is not the brand that is the highest rated brand, it shows that this brand is only a popular brand but not the most effective brand for buyers. \n- However, **Fable & Mane\t, Aether Beauty, and Montblanc** brands are the highest rated brands with maximum scores, and this shows many who like these brands with all the qualities they have given.\n","metadata":{}},{"cell_type":"markdown","source":"### What Product got the most total Rating","metadata":{}},{"cell_type":"code","source":"popular_products = pd.DataFrame(df.groupby('brand')['rating'].sum())\nmost_popular = popular_products.sort_values('rating', ascending=False)\nmost_popular.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The analysis obtained by SEPHORA COLLECTION brand managed to become the most popular product with a total number of ratings given by consumers, namely 1893.5 rating. but this could be because this brand has a lot of sales.","metadata":{}},{"cell_type":"markdown","source":"### What are the Most Expensive Brands","metadata":{}},{"cell_type":"code","source":"price_products = pd.DataFrame(df.groupby('brand')['price'].mean())\nmost_price = price_products.sort_values('price', ascending=False)\nmost_price.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Brand **Four Sigmatic, Montblanc, and Aether Beauty** which are the highest rated brands can be seen not including brands with an average price of expensive products.  \n- Brands such as **dyson, ReFa, and LightStim** which are the brands with the average price of the most expensive products. brand **ReFa** itself is a brand that falls into the top 10 category with the best rating with a score of 4.83.\n","metadata":{}},{"cell_type":"markdown","source":"### What Product got a lot of Love From Customer","metadata":{}},{"cell_type":"code","source":"love_products = pd.DataFrame(df.groupby('brand')['love'].mean())\nmost_love = love_products.sort_values('love', ascending=False)\nmost_love.head(10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Product got a lot of Reviews From Customer","metadata":{}},{"cell_type":"code","source":"reviews_products = pd.DataFrame(df.groupby('brand')['number_of_reviews'].mean())\nmost_reviews = reviews_products.sort_values('number_of_reviews', ascending=False)\nmost_reviews.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for some brands that have the most love in the previous category can be seen also fall into the category of number_of_reviews the top 10. some of them are **Buxom, stila, NARS, Anastasia Beverly Hills, Makeup Eraser, and Urban Decay**. here it can also be seen that these two variables have correlations that can later be seen at the time of correlation analysis before.","metadata":{}},{"cell_type":"markdown","source":"### Analysis Variable Brand Buxom, stila, and NARS","metadata":{}},{"cell_type":"code","source":"xbrand = df[df['brand']=='Buxom']\nxbrand.head(45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ybrand = df[df['brand']=='stila']\nybrand.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zbrand = df[df['brand']=='NARS']\nzbrand.head(45)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the analysis of the above 3 variables can be seen clearly from the brand **'NARS'** which has good data from the side of **love and number_of_reviews** but does not show any correlation with exclusive whether or not a product. so it can be concluded that the two columns **love and number_of_reviews** do not really affect the value of an item is exclusive what not.","metadata":{}},{"cell_type":"markdown","source":"### What Most Popular Category Based on Rating","metadata":{}},{"cell_type":"code","source":"price_category = pd.DataFrame(df.groupby('category')['rating'].mean())\nmost_price = price_category.sort_values('rating', ascending=False)\nmost_price.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Category With The Highest Income Value","metadata":{}},{"cell_type":"code","source":"price_sorted_category = pd.pivot_table(df,\n              index=['category'],\n              values=['price'],\n              aggfunc=['sum']\n              ).reset_index()\nprice_sorted_category.columns = ['category', 'price']\nprice_sorted_category = price_sorted_category.sort_values(['price'], ascending = False)\nprice_sorted_category = price_sorted_category.head(10)\nprice_sorted_category","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,7))\n_ = sns.barplot(x=\"category\", y=\"price\", data=price_sorted_category,\n                palette=\"nipy_spectral\", ax=ax)\n_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n_ = ax.set(xlabel=\"Category\", ylabel=\"Total Price\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Brand With The Highest Income Value","metadata":{}},{"cell_type":"code","source":"price_sorted_category = pd.pivot_table(df,\n              index=['brand'],\n              values=['price'],\n              aggfunc=['sum']\n              ).reset_index()\nprice_sorted_category.columns = ['brand', 'price']\nprice_sorted_category = price_sorted_category.sort_values(['price'], ascending = False)\nprice_sorted_category = price_sorted_category.head(10)\nprice_sorted_category","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis Variable Brand = 'TOM FORD'","metadata":{}},{"cell_type":"code","source":"brandHighestPrice = df[(df[\"brand\"] == 'TOM FORD')]\nbrandHighestPrice.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Category With The Highest sales from Highest Income Value Brands","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,7))\n_ = sns.barplot(x=\"category\", y=\"price\", data=brandHighestPrice,\n                palette=\"nipy_spectral\", ax=ax)\n_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n_ = ax.set(xlabel=\"Category\", ylabel=\"Total Price\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What Product With The Highest Price","metadata":{}},{"cell_type":"code","source":"price_sorted_category = pd.pivot_table(df,\n              index=['name'],\n              values=['price'],\n              aggfunc=['sum']\n              ).reset_index()\nprice_sorted_category.columns = ['name', 'price']\nprice_sorted_category = price_sorted_category.sort_values(['price'], ascending = False)\nprice_sorted_category = price_sorted_category.head(10)\nprice_sorted_category","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is the Brand With the Most Sales?","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,100),dpi=100)\nplt.xticks(rotation=90)\nplt.title('Brand Counts')\nsns.countplot(y=df['brand'], palette=\"nipy_spectral\");","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"brandbig10 = df.groupby(['brand'])['exclusive'].count().sort_values(ascending=False).reset_index().head(10)\n\nplt.figure(figsize=(18,6), dpi=100)\nplt.subplot(2,2,1)\nplt.ylabel('')\nplt.xlabel('')\nsns.barplot(y=brandbig10['brand'],x=brandbig10['exclusive'], palette='nipy_spectral')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is the Category With the Most Sales?","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,40),dpi=100)\nplt.xticks(rotation=90)\nplt.title('Category Counts')\nsns.countplot(y=df['category'], palette=\"nipy_spectral\");","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorybig10 = df.groupby(['category'])['exclusive'].count().sort_values(ascending=False).reset_index().head(10)\n\nplt.figure(figsize=(18,6), dpi=100)\nplt.subplot(2,2,1)\nplt.ylabel('')\nplt.xlabel('')\nsns.barplot(y=categorybig10['category'],x=categorybig10['exclusive'], palette='nipy_spectral')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is the Rating With the Most Sales? ","metadata":{}},{"cell_type":"code","source":"sns.countplot(df['rating'],palette='nipy_spectral',orient='v')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"code","source":"features = ['number_of_reviews','love','price','value_price']\nplt.figure(figsize=(15, 10))\nfor i in range(0, len(features)):\n    plt.subplot(1, 4, i+1)\n    sns.boxplot(y=df[features[i]],color='green',orient='v')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['number_of_reviews'] = np.log1p(df['number_of_reviews'])\ndf['love'] = np.log1p(df['love'])\ndf['price'] = np.log1p(df['price'])\ndf['value_price'] = np.log1p(df['value_price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nfor i in range(0, len(features)):\n    plt.subplot(1, 4, i+1)\n    sns.boxplot(y=df[features[i]],color='green',orient='v')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MarketingFlags'] = df.MarketingFlags.map({False:0, True:1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['id'],axis=1)\ndf = df.drop(['name'],axis=1)\ndf = df.drop(['URL'],axis=1)\ndf = df.drop(['options'],axis=1)\ndf = df.drop(['details'],axis=1)\ndf = df.drop(['how_to_use'],axis=1)\ndf = df.drop(['ingredients'],axis=1)\ndf = df.drop(['price'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature encoding (one hot encoding)","metadata":{}},{"cell_type":"code","source":" df['rating']=df['rating'].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all the categorical columns\ncat_cols = df.select_dtypes(\"object\").columns\n\n## One-Hot Encoding all the categorical variables but dropping one of the features among them.\ndrop_categ = []\nfor i in cat_cols:\n    drop_categ += [ i+'_'+str(df[i].unique()[-1]) ]\n\n## Create dummy variables (One-Hot Encoding)\ndf = pd.get_dummies(df, columns=cat_cols) \n\n## Drop the last column generated from each categorical feature\ndf.drop(drop_categ, axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import metrics\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop('value_price', axis = 1) \ny = df['value_price']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardization","metadata":{}},{"cell_type":"code","source":"# scaler = RobustScaler() #RobustScaler - StandardScaler\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets print the shapes again \nprint(\"Shape of the X Train :\", X_train.shape)\nprint(\"Shape of the y Train :\", y_train.shape)\nprint(\"Shape of the X test :\", X_test.shape)\nprint(\"Shape of the y test :\", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Build\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score,roc_curve, auc, precision_recall_curve, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"xgb = XGBRegressor()\n\nxgb.fit(X_train, y_train)\ndf_imp = pd.DataFrame(xgb.feature_importances_ , columns = ['Importance'], index=X_train.columns)\ndf_imp = df_imp.sort_values(['Importance'], ascending = False)\n\ndf_imp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGB_model = XGBRegressor()\n\nXGB_model.fit(X_train, y_train)\ny_pred= XGB_model.predict(X_test)\n\nprint(\"Accuracy on Traing set   : \",XGB_model.score(X_train,y_train))\nprint(\"Accuracy on Testing set  : \",XGB_model.score(X_test,y_test))\nprint(\"__________________________________________\")\nprint(\"\\t\\tError Table\")\nprint('Mean Absolute Error      : ', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared  Error      : ', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error  : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R Squared Error          : ', metrics.r2_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"RandomForest = RandomForestRegressor()\nRandomForest.fit(X_train, y_train)\ny_pred= RandomForest.predict(X_test)\n\nprint(\"Accuracy on Traing set   : \",RandomForest.score(X_train,y_train))\nprint(\"Accuracy on Testing set  : \",RandomForest.score(X_test,y_test))\nprint(\"__________________________________________\")\nprint(\"\\t\\tError Table\")\nprint('Mean Absolute Error      : ', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error       : ', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error  : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R Squared Error          : ', metrics.r2_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ridge Regression","metadata":{}},{"cell_type":"code","source":"ridge = Ridge()\nridge.fit(X_train, y_train)\ny_pred= ridge.predict(X_test)\n\nprint(\"Accuracy on Traing set   : \",ridge.score(X_train,y_train))\nprint(\"Accuracy on Testing set  : \",ridge.score(X_test,y_test))\nprint(\"__________________________________________\")\nprint(\"\\t\\tError Table\")\nprint('Mean Absolute Error      : ', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared  Error      : ', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error  : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R Squared Error          : ', metrics.r2_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}