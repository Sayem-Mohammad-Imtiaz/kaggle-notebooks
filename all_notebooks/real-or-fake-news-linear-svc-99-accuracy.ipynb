{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T04:39:11.582614Z","iopub.execute_input":"2021-06-21T04:39:11.583294Z","iopub.status.idle":"2021-06-21T04:39:12.503334Z","shell.execute_reply.started":"2021-06-21T04:39:11.583158Z","shell.execute_reply":"2021-06-21T04:39:12.502108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's get both the data\nfake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\ntrue = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:42:53.805318Z","iopub.execute_input":"2021-06-21T04:42:53.805676Z","iopub.status.idle":"2021-06-21T04:42:56.618685Z","shell.execute_reply.started":"2021-06-21T04:42:53.805646Z","shell.execute_reply":"2021-06-21T04:42:56.617425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fake news\nfake.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:43:43.547951Z","iopub.execute_input":"2021-06-21T04:43:43.548357Z","iopub.status.idle":"2021-06-21T04:43:43.58108Z","shell.execute_reply.started":"2021-06-21T04:43:43.548325Z","shell.execute_reply":"2021-06-21T04:43:43.580275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Real/True news\ntrue.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:44:08.982775Z","iopub.execute_input":"2021-06-21T04:44:08.983274Z","iopub.status.idle":"2021-06-21T04:44:08.996967Z","shell.execute_reply.started":"2021-06-21T04:44:08.98324Z","shell.execute_reply":"2021-06-21T04:44:08.995625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's insert a new column 'Real_or_Fake'.It will help when we combine both the tables in determining \n# Fake or Real news\nfake['Real_or_Fake'] = 'Fake'\ntrue['Real_or_Fake'] = 'Real'","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:47:02.415596Z","iopub.execute_input":"2021-06-21T04:47:02.415949Z","iopub.status.idle":"2021-06-21T04:47:02.423277Z","shell.execute_reply.started":"2021-06-21T04:47:02.415919Z","shell.execute_reply":"2021-06-21T04:47:02.422425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Combining fake and true table into one**","metadata":{}},{"cell_type":"code","source":"news = pd.concat([true,fake],axis=0,ignore_index=True)\n\n# First 5 rows of the news table\nnews.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:49:12.063483Z","iopub.execute_input":"2021-06-21T04:49:12.063893Z","iopub.status.idle":"2021-06-21T04:49:12.098301Z","shell.execute_reply.started":"2021-06-21T04:49:12.063858Z","shell.execute_reply":"2021-06-21T04:49:12.097207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of real and fake news\nprint(news['Real_or_Fake'].value_counts())\n\nsns.countplot(x='Real_or_Fake',data=news)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:50:47.188779Z","iopub.execute_input":"2021-06-21T04:50:47.189306Z","iopub.status.idle":"2021-06-21T04:50:47.40281Z","shell.execute_reply.started":"2021-06-21T04:50:47.189261Z","shell.execute_reply":"2021-06-21T04:50:47.401905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check if there is any null value in text column.\nnews['text'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:52:00.374419Z","iopub.execute_input":"2021-06-21T04:52:00.37483Z","iopub.status.idle":"2021-06-21T04:52:00.406939Z","shell.execute_reply.started":"2021-06-21T04:52:00.374796Z","shell.execute_reply":"2021-06-21T04:52:00.40581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There is no null value.But there might be empty string, we will deal with it later**","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"**URL**","metadata":{}},{"cell_type":"code","source":"# Let's read one news\nnews['text'].iloc[33390]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T04:54:36.56793Z","iopub.execute_input":"2021-06-21T04:54:36.568378Z","iopub.status.idle":"2021-06-21T04:54:36.57545Z","shell.execute_reply.started":"2021-06-21T04:54:36.568341Z","shell.execute_reply":"2021-06-21T04:54:36.574195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There might be a url provided in a news text which will not be useful as they provide no information in the form of text.We have to go to the url to obtain more information.Let's remove any url present in the text for all the news**","metadata":{}},{"cell_type":"code","source":"# Function to remove the url\ndef remove_url(text):\n    text = text.split(' ')\n    text1 = ''\n    for word in text:\n        if ('.com' in word) or ('https' in word) or ('bit.ly' in word):\n            continue\n        else:\n            text1 += (word+' ')\n    return text1\n\n# Lets apply this on news text\nnews['text'] = news['text'].apply(remove_url)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:00:23.666657Z","iopub.execute_input":"2021-06-21T05:00:23.667031Z","iopub.status.idle":"2021-06-21T05:00:31.833128Z","shell.execute_reply.started":"2021-06-21T05:00:23.667Z","shell.execute_reply":"2021-06-21T05:00:31.831545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Empty String**","metadata":{}},{"cell_type":"code","source":"# Let's check if any news text is just an empty string\n\n# empty will hold the index of the empty string text\nempty = []\n\n# for loop to find the empty string\nfor i,title,text,*_ in news.itertuples():\n    if text.isspace() or text=='':\n        empty.append(i)\n        \n# number of rows with empty string as form of news text\nprint(f\"There are total {len(empty)} rows with empty string as news text\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:05:14.056437Z","iopub.execute_input":"2021-06-21T05:05:14.056828Z","iopub.status.idle":"2021-06-21T05:05:14.171209Z","shell.execute_reply.started":"2021-06-21T05:05:14.056784Z","shell.execute_reply":"2021-06-21T05:05:14.169735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's remove these empty strings\nnews.drop(empty,inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:06:24.527132Z","iopub.execute_input":"2021-06-21T05:06:24.527533Z","iopub.status.idle":"2021-06-21T05:06:24.545877Z","shell.execute_reply.started":"2021-06-21T05:06:24.5275Z","shell.execute_reply":"2021-06-21T05:06:24.544924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**HTML tags**","metadata":{}},{"cell_type":"code","source":"pip install beautifulsoup4","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:10:57.747644Z","iopub.execute_input":"2021-06-21T05:10:57.748035Z","iopub.status.idle":"2021-06-21T05:11:06.712354Z","shell.execute_reply.started":"2021-06-21T05:10:57.748004Z","shell.execute_reply":"2021-06-21T05:11:06.710857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's remove any HTML tags present in news text\n# We can use BeautifulSoup to do it\nfrom bs4 import BeautifulSoup\n\n# function to remove the HTML tags\ndef remove_html(text):\n    soup = BeautifulSoup(text)\n    text = soup.get_text()\n    \n    return text\n\n# Let's apply the above function on news text\nnews['text'] = news['text'].apply(remove_html)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:11:17.016112Z","iopub.execute_input":"2021-06-21T05:11:17.016517Z","iopub.status.idle":"2021-06-21T05:11:28.73192Z","shell.execute_reply.started":"2021-06-21T05:11:17.016482Z","shell.execute_reply":"2021-06-21T05:11:28.730724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's read some more news\nnews['text'][0][:50]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:12:02.607462Z","iopub.execute_input":"2021-06-21T05:12:02.60783Z","iopub.status.idle":"2021-06-21T05:12:02.617647Z","shell.execute_reply.started":"2021-06-21T05:12:02.607797Z","shell.execute_reply":"2021-06-21T05:12:02.61645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news['text'][6][:50]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:12:10.184409Z","iopub.execute_input":"2021-06-21T05:12:10.184924Z","iopub.status.idle":"2021-06-21T05:12:10.190961Z","shell.execute_reply.started":"2021-06-21T05:12:10.184891Z","shell.execute_reply":"2021-06-21T05:12:10.189672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WASHINGTON (Reuters) or SEATTLE/WASHINGTON (Reuters) are there at the begining of many news text.\n# Our model might learn that if these words are at the begining of the news text,they must belong to one category and might \n# not try to learn from the text that follows.\n\n# Let's go ahead and remove these words from the news text\n# we will split the text on the basis of (Reuters) and ignore the first part\n\n# function to perform the split\ndef split_news(text):\n    if '(Reuters)' in text:\n        text = text.split('(Reuters)')\n\n        return ' '.join(text[1:])\n    return text\n\n# Applying the above function on the news text\nnews['text'] = news['text'].apply(split_news)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:14:11.710606Z","iopub.execute_input":"2021-06-21T05:14:11.711015Z","iopub.status.idle":"2021-06-21T05:14:11.949476Z","shell.execute_reply.started":"2021-06-21T05:14:11.710981Z","shell.execute_reply":"2021-06-21T05:14:11.948096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Punctuation**","metadata":{}},{"cell_type":"code","source":"# Let's remove the punctuations from the news text\nimport string\n\npunctuations = string.punctuation\n\n# Lets add '\\n','\\n\\n' and ' ' in punctuations\npunctuations += '\\n \\n\\n'\n\n#function to remove the punctuations\ndef remove_punct(text):\n    text = text.split(' ')\n    text  = [word.lower() for word in text if word not in punctuations]\n    \n    return ' '.join(text)\n\n# applying the above function in news text\nnews['text'] = news['text'].apply(remove_punct)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:20:10.198657Z","iopub.execute_input":"2021-06-21T05:20:10.199095Z","iopub.status.idle":"2021-06-21T05:20:15.081475Z","shell.execute_reply.started":"2021-06-21T05:20:10.19906Z","shell.execute_reply":"2021-06-21T05:20:15.080274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's divide the data into X and y. X will be news text and y would be label- Fake or Real.**","metadata":{}},{"cell_type":"code","source":"X = news['text']\ny = news['Real_or_Fake']\n\n# Lets do the one hot encoding to convet y\nencoded_y = pd.get_dummies(y,drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:27:50.469744Z","iopub.execute_input":"2021-06-21T05:27:50.470204Z","iopub.status.idle":"2021-06-21T05:27:50.48346Z","shell.execute_reply.started":"2021-06-21T05:27:50.470171Z","shell.execute_reply":"2021-06-21T05:27:50.48216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1 in encoded_y means Real news and 0 means Fake news**","metadata":{}},{"cell_type":"code","source":"# Libraries to split the data into train and test data,create maodel and evaluating the matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score,classification_report","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:50:17.656246Z","iopub.execute_input":"2021-06-21T05:50:17.656593Z","iopub.status.idle":"2021-06-21T05:50:17.661971Z","shell.execute_reply.started":"2021-06-21T05:50:17.656548Z","shell.execute_reply":"2021-06-21T05:50:17.660979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets split the data into train and test data. We will use 25% of the data as test data\nX_train,X_test,y_train,y_test = train_test_split(X,encoded_y.values.reshape(-1,),test_size=0.25,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:50:30.939772Z","iopub.execute_input":"2021-06-21T05:50:30.940125Z","iopub.status.idle":"2021-06-21T05:50:30.953004Z","shell.execute_reply.started":"2021-06-21T05:50:30.940096Z","shell.execute_reply":"2021-06-21T05:50:30.952069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Naive_bayes model**","metadata":{}},{"cell_type":"code","source":"# creating a naive model\npipeline_naive = Pipeline([\n    ('vector',TfidfVectorizer(stop_words='english')),\n    ('classifier',MultinomialNB())\n])\n\n# training the model\npipeline_naive.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:50:33.733984Z","iopub.execute_input":"2021-06-21T05:50:33.734559Z","iopub.status.idle":"2021-06-21T05:50:48.380236Z","shell.execute_reply.started":"2021-06-21T05:50:33.734525Z","shell.execute_reply":"2021-06-21T05:50:48.379509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction\npredict_naive = pipeline_naive.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:50:48.381536Z","iopub.execute_input":"2021-06-21T05:50:48.381973Z","iopub.status.idle":"2021-06-21T05:50:52.984074Z","shell.execute_reply.started":"2021-06-21T05:50:48.381944Z","shell.execute_reply":"2021-06-21T05:50:52.982825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the naive model\nacc_naive = accuracy_score(y_test,predict_naive)\nprint(f'Naive model has {acc_naive.round(2)*100}% accuracy')\nprint('\\n')\nprint(classification_report(y_test,predict_naive))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:58:49.373102Z","iopub.execute_input":"2021-06-21T05:58:49.373506Z","iopub.status.idle":"2021-06-21T05:58:49.401707Z","shell.execute_reply.started":"2021-06-21T05:58:49.373471Z","shell.execute_reply":"2021-06-21T05:58:49.400588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression Model**","metadata":{}},{"cell_type":"code","source":"# creating a logistic model\npipeline_logistic = Pipeline([\n    ('vector',TfidfVectorizer(stop_words='english')),\n    ('classifier',LogisticRegression())\n])\n\n# training the model\npipeline_logistic.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:51:57.845387Z","iopub.execute_input":"2021-06-21T05:51:57.845765Z","iopub.status.idle":"2021-06-21T05:52:18.367059Z","shell.execute_reply.started":"2021-06-21T05:51:57.845733Z","shell.execute_reply":"2021-06-21T05:52:18.366018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction\npredict_logistic = pipeline_logistic.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:52:33.279051Z","iopub.execute_input":"2021-06-21T05:52:33.279588Z","iopub.status.idle":"2021-06-21T05:52:37.970707Z","shell.execute_reply.started":"2021-06-21T05:52:33.279523Z","shell.execute_reply":"2021-06-21T05:52:37.969735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the logistic model\nacc_logistic = accuracy_score(y_test,predict_logistic)\nprint(f'LogisticRegression model has {acc_logistic.round(2)*100}% accuracy')\nprint('\\n')\nprint(classification_report(y_test,predict_logistic))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:59:01.974219Z","iopub.execute_input":"2021-06-21T05:59:01.97481Z","iopub.status.idle":"2021-06-21T05:59:02.002723Z","shell.execute_reply.started":"2021-06-21T05:59:01.974774Z","shell.execute_reply":"2021-06-21T05:59:02.001407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LinearSVC model**","metadata":{}},{"cell_type":"code","source":"# creating a LinearSVC model\npipeline_svc = Pipeline([\n    ('vector',TfidfVectorizer(stop_words='english')),\n    ('classifier',LinearSVC())\n])\n\n# training the model\npipeline_svc.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:55:32.993935Z","iopub.execute_input":"2021-06-21T05:55:32.994287Z","iopub.status.idle":"2021-06-21T05:55:48.286679Z","shell.execute_reply.started":"2021-06-21T05:55:32.994257Z","shell.execute_reply":"2021-06-21T05:55:48.28539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction\npredict_svc = pipeline_svc.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:56:30.59222Z","iopub.execute_input":"2021-06-21T05:56:30.592619Z","iopub.status.idle":"2021-06-21T05:56:35.325362Z","shell.execute_reply.started":"2021-06-21T05:56:30.592584Z","shell.execute_reply":"2021-06-21T05:56:35.324058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the LinearSVC model\nacc_svc = accuracy_score(y_test,predict_svc)\nprint(f'LinearSVC model has {acc_svc.round(2)*100}% accuracy')\nprint('\\n')\nprint(classification_report(y_test,predict_svc))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:59:15.887095Z","iopub.execute_input":"2021-06-21T05:59:15.88745Z","iopub.status.idle":"2021-06-21T05:59:15.917268Z","shell.execute_reply.started":"2021-06-21T05:59:15.887416Z","shell.execute_reply":"2021-06-21T05:59:15.91606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's visualize the accuracy of all the three models\nmodels = {\n    'Naive':acc_naive,\n    'Logistic':acc_logistic,\n    'SVC':acc_svc\n}\n\nsns.set_style('darkgrid')\nplt.plot(models.keys(),models.values(),marker='*',color='blue',markeredgecolor='red',markeredgewidth=4)\nplt.xlabel('Models')\nplt.ylabel('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-06-21T06:03:09.342341Z","iopub.execute_input":"2021-06-21T06:03:09.342751Z","iopub.status.idle":"2021-06-21T06:03:09.485579Z","shell.execute_reply.started":"2021-06-21T06:03:09.342718Z","shell.execute_reply":"2021-06-21T06:03:09.484147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LinearSVC model performed better as compared to others with 99% accuracy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}