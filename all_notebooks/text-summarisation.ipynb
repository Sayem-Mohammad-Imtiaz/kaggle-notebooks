{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text Summarising of the articles","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Text-Summarization\n**Automatic summarization** is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.\n\nAutomatic data summarization is part of the real machine learning and data mining. The main idea of summarization is to find a subset of data which contains the \"information\" of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.\n\nThere are two general approaches to automatic summarization: Extraction and Abstraction. \n1. *Extractive Summarization*: These methods rely on extracting several parts, such as phrases and sentences, from a piece of text and stack them together to create a summary. Therefore, identifying the right sentences for summarization is of utmost importance in an extractive method.\n2. *Abstractive Summarization*: These methods use advanced NLP techniques to generate an entirely new summary. Some parts of this summary may not even appear in the original text. Such a summary might include verbal innovations. \nResearch to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization.\n\nIn this Jupyter notebook, TextRank algorithm for extractive text summarization is implemented using Google's PageRank search algorithm to generate corelations among sentences.\n\n### Libraries Used\n- [Numpy](http://www.numpy.org)\n- [Pandas](https://pandas.pydata.org/)\n- [Natural Language Toolkit](https://www.nltk.org/)\n\n### Algorithms and Concepts\n- TextRank\n- [PageRank](https://en.wikipedia.org/wiki/PageRank)\n- [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the required Libraries\nimport nltk\n# nltk.download('punkt') # one time execution\nimport re\n#nltk.download('stopwords') # one time execution\nimport matplotlib.pyplot as plt\n\nfrom nltk.tokenize import sent_tokenize\n\nfrom nltk.corpus import stopwords\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport networkx as nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences.\nWe will be using the pre-trained Wikipedia 2014 + Gigaword 5 GloVe vectors available [here](http://www.kaggle.com/mohitkundu1/glove-word-embeddings). Heads up â€“ the size of these word embeddings is 331 MB.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract word vectors\nword_embeddings = {}\nfile = open('../input/glove-word-embeddings/glove.6B.100d.txt', encoding='utf-8')\nfor line in file:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nfile.close()\nlen(word_embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the above file is big it will take some time to create the word embeddings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the file\ndf = pd.read_excel('../input/medicine-descrptions/TASK.xlsx')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now see the names of the columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change the name of the column and we can delete the fisrt row as it will of no use to us.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns = {'Unnamed: 1' : 'Introduction' }, inplace=True)\n# Deleting the first row\ndf.drop(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now this looks good. Let us now convert the dataset into a dictionary as it would be easy to use.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the DataFrame into a dictionary\ntext_dictionary = {}\nfor i in range(1,len(df['TEST DATASET'])):\n    text_dictionary[i] = df['Introduction'][i]\n    \nprint(text_dictionary[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are 1000 such description of the different medicines. The task is to give summarised form of these description.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to remove stopwords\ndef remove_stopwords(sen):\n    stop_words = stopwords.words('english')\n    \n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to make vectors out of the sentences\ndef sentence_vector_func (sentences_cleaned) : \n    sentence_vector = []\n    for i in sentences_cleaned:\n        if len(i) != 0:\n            v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n        else:\n            v = np.zeros((100,))\n        sentence_vector.append(v)\n    \n    return (sentence_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to get the summary of the articles\n# NOTE - Remove '#' infront of print statement for displaying the contents at different stages of the text summarisation process\ndef summary_text (test_text, n = 5):\n    sentences = []\n    \n    # tokenising the text \n    sentences.append(sent_tokenize(test_text))\n    # print(sentences)\n    sentences = [y for x in sentences for y in x] # flatten list\n    # print(sentences)\n    \n    # remove punctuations, numbers and special characters\n    clean_sentences = pd.Series(sentences).str.replace(\"[^a-z A-Z 0-9]\", \" \")\n\n    # make alphabets lowercase\n    clean_sentences = [s.lower() for s in clean_sentences]\n    #print(clean_sentences)\n\n    \n    # remove stopwords from the sentences\n    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\n    #print(clean_sentences)\n    \n    sentence_vectors = sentence_vector_func(clean_sentences)\n    \n    # similarity matrix\n    sim_mat = np.zeros([len(sentences), len(sentences)])\n    #print(sim_mat)\n    \n    # Finding the similarities between the sentences \n    for i in range(len(sentences)):\n        for j in range(len(sentences)):\n            if i != j:\n                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n    \n    \n    nx_graph = nx.from_numpy_array(sim_mat)\n    scores = nx.pagerank(nx_graph)\n    #print(scores)\n    \n    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)))\n    # Extract sentences as the summary\n    summarised_string = ''\n    for i in range(n):\n        \n        try:\n            summarised_string = summarised_string + str(ranked_sentences[i][1])            \n        except IndexError:\n            print (\"Summary Not Available\")\n    \n    return (summarised_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Kindly let me know in how many sentences you want the summary - \")\nx = 3\n\nsummary_dictionary = {}\n\nfor key in text_dictionary:\n    \n    para = text_dictionary[key]    \n    summary = summary_text(para,x)\n    summary_dictionary[key] = summary\n    if key>0 and key<=10 :\n        print(\"Summary of the article - \",key)\n        print(summary)\n        print('='*50)    \n    \nprint (\"*\"*10,\"The process has been completed successfully\",\"*\"*10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"As there are 1000 diiferent type of medicines's descriptions, it would not be a good idea to display all the summarised version. Hence only first 9 summarised text are displayed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table = pd.DataFrame(list(summary_dictionary.items()),columns = ['TEST DATASET','Summary'])\ndata_table = pd.DataFrame(list(text_dictionary.items()),columns = ['TEST DATASET','Introduction'])\n# Combining the findings into the table\nresult  = pd.concat([data_table , summary_table['Summary']], axis = 1 , sort = False)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving it to a file (remove the '#' to save)\nresult.to_csv(\"Summary_File.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}