{"cells":[{"metadata":{},"cell_type":"markdown","source":"### BoomBikes Bike Sharing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Business Case:**\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\nA US bike-sharing provider `BoomBikes` has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\nThe company wants to know:\n\t- Which variables are significant in predicting the demand for shared bikes.\n\t- How well those variables describe the bike demands\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppres warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import required libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nimport statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step - 1. Reading, understandig and visualising data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1.1 Reading and Understanding data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading data set\ndf = pd.read_csv('../input/boom-bike-dataset/bike_sharing_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting insights of dataframe\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# getting descriptive insights of dataframe\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no Null values in the column and majority of data are having numerical values except date column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for any duplicate entries\ndf.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no duplicate entries found in the dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Insights:\n-  `instant` column is a record index which does not have any significance in out analysis. So we will drop the column  \n- The varibles `casual` and `registered` are summed up to get `cnt` which is our target variable. Also during prediction we wll not be having these data, so we will drop these two variables which we are not going to use in the model.\n- We are going to use `weekday` varible which is derived from `dteday`, so we will not be using `dteday` and will drop it.\n- `temp` and `atemp` are directly correlated among each other. We will use `temp` and drop `atemp`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns instance, dteday, casual, registered and atemp\ndf.drop(['instant', 'dteday','casual','registered','atemp'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After droppping the variables checking the columns abnd rows in the dataframe\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identify categorical variables\ncat_vars = ['season','yr','mnth','holiday','weekday', 'workingday','weathersit']\n\n# identify numeric variables\nnum_vars = ['temp', 'hum','windspeed','cnt']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# convert dtype of categorical variables\ndf[cat_vars] = df[cat_vars].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# get insights of numeric variable\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# get the insights of categorical variables\ndf.describe(include=['category'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Disctionary:**<br>\nBelow categorical columns are having following catecteristics and can be maped with respective values:\n- `season` column is having four seasons as (1:spring, 2:summer, 3:fall, 4:winter)\n- `mnth` column is having 12 categorical values denoting for months Jan to Dec\n- `weathersit` is having for categorical values (1: Clear_FewClouds, 2: Mist_Cloudy, 3: LightSnow_LightRain, 4: HeavyRain_IcePallets)\n- `weekday` column having 7 varibles ( 0 to 6 ) denoting (0: Sun, 1: Mon 2: Tue, 3: Wed, 4: Thu, 5: Fri, 6: Sat) \n\n`yr`, `holiday`, `workingday` are having binary values. So we will not map these columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# maped the season column according to descripttions\ndf['season'] = df['season'].map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\n\n# maped the weekday column according to descriptin\ndf['weekday'] = df['weekday'].map({0: 'Sun', 1: 'Mon', 2: 'Tue', 3: 'Wed', 4: 'Thu', 5: 'Fri', 6: 'Sat'})\n\n\n# maped mnth column values (1 to 12 ) as (jan to dec) respectively\ndf['mnth'] = df['mnth'].map({1:'jan', 2:'feb', 3:'mar', 4:'apr', 5: 'may', 6: 'jun', 7: 'jul', 8: 'aug', 9: 'sep', 10: 'oct',\n                             11: 'nov', 12:'dec'})\n\n#  maped weathersit column\ndf['weathersit'] = df['weathersit'].map({1: 'Clear_FewClouds', 2: 'Mist_Cloudy', 3: 'LightSnow_LightRain', 4: 'HeavyRain_IcePallets'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step - 2. Data  Visualisation","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Check the data info before proceeding for analysis\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 Univariate analaysis","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# visualise the pattern of demand (target variable - 'cnt') over period of two years\nplt.figure(figsize=(20,5))\nplt.plot(df.cnt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n- We can observe that there was growth over the period and recently we could see there is reduction in demand","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualising numerical varibles\n\n# selecting numerical variables\nvar = df.select_dtypes(exclude = 'category').columns\n\n# Box plot\ncol = 2\nrow = len(var)//col+1\n\nplt.figure(figsize=(12,8))\nplt.rc('font', size=12)\nfor i in list(enumerate(var)):\n    plt.subplot(row, col, i[0]+1)\n    sns.boxplot(df[i[1]])    \nplt.tight_layout()   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n- From the above plots it could be observed that `hum` and `windspeed` are having few outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get percentage outlier for hum and windspeed\n\n# function to get outlier percentage\ndef percentage_outlier(x):\n    iqr = df[x].quantile(0.75)-df[x].quantile(0.25)\n    HL = df[x].quantile(0.75)+iqr*1.5\n    LL = df[x].quantile(0.25)-iqr*1.5\n    per_outlier = ((df[x]<LL).sum()+(df[x]>HL).sum())/len(df[x])*100\n    per_outlier = round(per_outlier,2)\n    return(per_outlier)\n\nprint('Percentage of outlier (hum): ', percentage_outlier('hum'))\nprint('Percentage of outlier (windspeed): ', percentage_outlier('windspeed'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\n- Percentage of outlier for `hum` and `windspeed` are `0.27` and `1.78` respectively.\n- As these percentage is low, and we can see there is no significant abnormal outlier value. So we will leave it as it is.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # Visulalising Categorical Variables using pie chart\n\ndf_piplot=df.select_dtypes(include='category')\nplt.figure(figsize=(18,16))\nplt.suptitle('pie distribution of categorical features', fontsize=20)\nfor i in range(1,df_piplot.shape[1]+1):\n    plt.subplot(3,3,i)\n    f=plt.gca()\n    f.set_title(df_piplot.columns.values[i-1])\n    values=df_piplot.iloc[:,i-1].value_counts(normalize=True).values\n    index=df_piplot.iloc[:,i-1].value_counts(normalize=True).index\n    plt.pie(values,labels=index,autopct='%1.0f%%')\n# plt.tight_layout(pad = 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Visulalising Categorical Variables\n# # selecting categorical variables\n# var = df.select_dtypes(include='category').columns\n\n# # Box plot\n# col = 3\n# row = len(var)//col+1\n\n# plt.figure(figsize=(12,12))\n# # plt.rc('font', size=12)\n# for i in list(enumerate(var)):\n#     plt.subplot(row, col, i[0]+1)\n#     sns.countplot(df[i[1]])\n#     plt.xticks(rotation = 90)\n# plt.tight_layout(pad = 1.0)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- Seasons:   We could see business was operating similar days in all four seasons.\n- Yr:   Number of days operation in both the year are almost same.\n- Month: We could see business was operating similar days in all 12 months.\n- Holiday: Business was operating in 3% days of holiday\n- weekdays: We could see business was operating similar percentage in all weekdays.\n- Workingday: Bisuness was operating in 68% in workign days and 32% in nonworking days.\n- Weathersit: From the above analaysis it is being observed that there is no data for 4th category of `weathersit i.e Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog`.May be the company is not operating on those days or there was no demand of bike.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2.2 Bi-Variate analysis","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# pairplot for continuous data type\nsns.pairplot(df.select_dtypes(['int64','float64']), diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**<br>\nFrom the above pairplot we could observe that, `temp` has highest positive correlation with target variable `cnt`.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# look at the correaltion between continous varibales using heat map\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could infer follwing observation:\n> - A positive correalation observed between `cnt` and `temp` (0.63)\n> - A Negative correlation observed for `cnt` with `hum` and `windspeed` (-0.099 and -0.24)","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Box plot for categorical variables\ncol = 3\nrow = len(cat_vars)//col+1\n\nplt.figure(figsize=(15,12))\nfor i in list(enumerate(cat_vars)):\n    plt.subplot(row,col,i[0]+1)\n    sns.boxplot(x = i[1], y = 'cnt', data = df)\n    plt.xticks(rotation = 90)\nplt.tight_layout(pad = 1)    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**<br>\nQ. From your analysis of the categorical variables from the dataset, what could you infer about their effect on the dependent variable?\n\nAnswer:\n- The demad of bike is less in the month of `spring` when compared with other seasons\n- The demand bike increased in the year 2019 when compared with year 2018.\n- Month Jun to Sep is the period when bike demand is high. The Month Jan is the lowest demand month.\n- Bike demand is less in holidays in comparison to not being holiday.\n- The demand of bike is almost similar throughout the weekdays.\n- There is no significant change in bike demand with workign day and non working day.\n- The bike demand is high when weather is `clear and Few clouds` however demand is less in case of `Lightsnow and light rainfall`. We do not have any dat for `Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog` , so we can not derive any conclusion. May be the company is not operating on those days or there is no demand of bike.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Step - 3. Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Creating of dummy variables","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# get dummy variables for season, weekday, mnth and weathersit\ndummy_vars = pd.get_dummies(df[['season','weekday','mnth','weathersit']],drop_first=True)\n\n# concat the dummy df with original df\ndf = pd.concat([df,dummy_vars], axis = 1)\n\n# drop season column\ndf.drop(['season','weekday','mnth','weathersit'], axis=1, inplace=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# check data frame\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Check datafrmae\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"yr, holiday and workign day columns are showing datatype 'categorical' but these data  are 0 and 1. So we will convert these data type to `uint8`","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Convert categorical columns to numeric \ndf[['yr','holiday','workingday']]= df[['yr','holiday','workingday']].astype('uint8')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Splitting data into test and train set ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train test dataset\ndf_train, df_test = train_test_split(df, train_size = 0.7, random_state = 10 )\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Scaling of data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- During EDA we could observe that there is different range of data in the data set. So it becomes important to scale the data. \n- Here we will be using Min-Max scaling (normalisation) to scale both training and tesing dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Training Data scaling**","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Scaling of train set\n\n# instantiate an object\nscaler = MinMaxScaler()\n\n# fit and transform on training data\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing data scaling**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check test dataset before scaling\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform test dataset \ndf_test[num_vars] = scaler.transform(df_test[num_vars])\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Split data set into X and y sets (for both train and test set)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X and y data dataframe for train set\ny_train = df_train.pop('cnt')\nX_train = df_train\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X and y data dataframe for test set\ny_test = df_test.pop('cnt')\nX_test = df_test\n\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step - 4. Data Modeling and Evaluation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Recusive feature elemination (RFE)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking variables for for X_train columns\nX_train.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Running RFE to select 15 number of varibles\n# Create object\nlm = LinearRegression()\n# fit model\nlm.fit(X_train, y_train)\n# run RFE\nrfe = RFE(lm, 15)\nrfe = rfe.fit(X_train, y_train)\n\n# Select columns\ncol = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Building model using statsmodel, for the detailed statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_train_rfe with RFE selected variables\nX_train_rfe = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# create function for stats linear model \ndef sm_linearmodel(X_train_sm):\n    #Add constant\n    X_train_sm = sm.add_constant(X_train_sm)\n\n    # create a fitted model (1st model)\n    lm = sm.OLS(y_train,X_train_sm).fit()\n    return lm","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Function to calculate VIF\n# calculate VIF\ndef vif_calc(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'],2)\n    vif = vif.sort_values(by='VIF', ascending = False)\n    return vif","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Create 1st stats model and look for summary and VIF\nlm_1 = sm_linearmodel(X_train_rfe)\nprint(lm_1.summary())\n\n# Calculate VIF\nprint(vif_calc(X_train_rfe))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop to remove P value variables >0.05 in bstep mannen and update model\n\npvalue = lm_1.pvalues\nwhile(max(pvalue)>0.05):\n    maxp_var = pvalue[pvalue == pvalue.max()].index\n    print('Removed variable:' , maxp_var[0], '    P value: ', round(max(pvalue),3))\n    \n    # drop variable with high p value\n    X_train_rfe = X_train_rfe.drop(maxp_var, axis = 1)\n    lm_1 = sm_linearmodel(X_train_rfe)\n    pvalue = lm_1.pvalues\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- Two step model upadation hapenned in the above step.\n- loop-1: highest pvalue in the model; `mnth_may : 0.054`. As the pvalue is >0.05 it is insignificant for the model, so `mnth_may` is dropped and model updated.\n- loop-2: highest pvalue in the mdoel; `mnth_aug : 0.056`. As the pvalue is >0.05 it is insignificant for the model, so `mnth_aug` is dropped and model updated.\n- pvalues for all the varibles are < 0.05 so we will look for summary and VIF of model `lm_1`.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Look for sumamry of model\nprint(lm_1.summary())\n\n# Calculate VIF\nprint(vif_calc(X_train_rfe))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- R2 : `0.843` and R2- adj : `0.839`. All the variables are having pvalue<0.05. Now we will look into VIF.\n- VIF: `temp` is having highest VIF (19.22), but during EDA we could see `temp` has a high correlation with `cnt` and also its a important varible based on business understanding. So we will keep that varible and look for highest variable which is >5.\n-  We could found `hum` as next highetst VIF (12.09) which is > 0.5. We will drop the variable and update the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Drop variable and update model","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# drop varible having high VIF\nX_train_new = X_train_rfe.drop(['hum'],axis = 1)\n\n# Create stats model and look for summary\nlm_2 = sm_linearmodel(X_train_new)\nprint(lm_2.summary())\n\n# Calculate VIF\nprint(vif_calc(X_train_new))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- After dropping the variable `hum` there is no significant change in R-squared or adj. R2-squared. So decissionto drop the varibale is correct.\n- `R-squared: 0.836` and `Adj R-squared : 0.832`. All the variables are having pvalue < 0.05. So we will look into VIF.\n- VIF: `temp` is having high VIF. But as explained earlier we will look for next highest VIF `season_fall (6.8)` which is >5. We will drop `season_fall` and update the model.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# drop varible having high VIF\nX_train_new = X_train_new.drop(['season_fall'],axis = 1)\n\n# Create stats model and look for summary\nlm_3 = sm_linearmodel(X_train_new)\nprint(lm_3.summary())\n\n# Calculate VIF\nprint(vif_calc(X_train_new))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- After dropping the variable `season_fall` and updating model, we could see `R-squared: 0.827` and `Adj R-squared : 0.824`. there is no significant change in R-squared or adj. R2-squared. So decission to drop the varibale is correct.\n- All the variables are having pvalue < 0.05. All VIF is also < 5. \n- So we will 1st look into the higher pvalue varible and drop it to see the effect on R-squared.\n- `mnth_mar` is having highest pvalue `0.019`. We will drop the variable, update the model and see the any impact in R-squared.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# drop varible having high VIF\nX_train_new = X_train_new.drop(['mnth_mar'],axis = 1)\n\n# Create stats model and look for summary\nlm_4 = sm_linearmodel(X_train_new)\nprint(lm_4.summary())\n\n# Calculate VIF\nprint(vif_calc(X_train_new))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- After dropping the variable `mnth_mar` and updating model, we could see `R-squared: 0.825` and `Adj R-squared : 0.822`. There is no significant change in R-squared or adj. R2-squared. So decission to drop the variable is correct.\n- All the variables are having pvalue < 0.05. All VIF is also < 5. \n- We could go with model lm_4 but we will try to reduce further independent variable.\n- So we will 1st look into the higher pvalue varible and drop it to see the effect on R-squared.\n- `mnth_oct` is having highest pvalue `0.002`. We will drop the variable, update the model and see the any impact in R-squared.\nThere is no significant change in R2 for lm_3 and lm_4 however we could reduce one independent variable. So we will choose lm_4","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# drop varible having high VIF\nX_train_new = X_train_new.drop(['mnth_oct'],axis = 1)\n\n# Create stats model and look for summary\nlm_5 = sm_linearmodel(X_train_new)\nprint(lm_5.summary())\n\n# Calculate VIF\nprint(vif_calc(X_train_new))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- After dropping the variable `mnth_oct` and updating model, we could see `R-squared: 0.822` and `Adj R-squared : 0.819`. There is no significant change in R-squared or adj. \n- All the variables are having pvalue alomost zero and all VIF is also < 5. \n- There is no significant change in Adj R-squared for `lm_4` and `lm_5` however we could reduce one independent variable. So we will choose `lm_5` as our final model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Final Model and its variables:**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now the model looks goos with the Above variable. Lets list down the final varibles","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# List down final model varibales and its coefficients\n\n# assign final model to lm_final\nlm_final = lm_5\n\n# list down and check variables of final model\nvar_final = list(lm_final.params.index)\nvar_final.remove('const')\nprint('Final Selected Variables:', var_final)\n\n# Print the coefficents of final varible\nprint('\\033[1m{:10s}\\033[0m'.format('\\nCoefficent for the variables are:'))\nprint(round(lm_final.params,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The final varibles are: `'yr','holiday', 'temp', 'windspeed', 'season_summer', 'season_winter', 'mnth_sep', 'weathersit_Mist_Cloudy', 'weathersit_LightSnow_LightRain'`**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Model Evaluation on train set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Residual analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select final variables from the test dataset\nX_train_res = X_train[var_final]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add constant\nX_train_res = sm.add_constant(X_train_res)\n\n# predict train set\ny_train_pred = lm_final.predict(X_train_res)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# distrubition plot for residue\nres = y_train - y_train_pred\nsns.distplot(res)\nplt.title('Error terms')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- The distribution plot of error term shows the normal distribution with mean at Zero.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error terms train set\nc = [i for i in range(1,len(y_train)+1,1)]\nfig = plt.figure(figsize=(8,5))\nplt.scatter(y_train,res)\nfig.suptitle('Error Terms', fontsize=16)              # Plot heading \nplt.xlabel('Y_train_pred', fontsize=14)                      # X-label\nplt.ylabel('Residual', fontsize=14)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- It seems like the corresponding residual plot is reasonably random. \n- Also the error terms satisfies to have reasonably constant variance (homoscedasticity) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Prediction on test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check dataframe for the test set\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# select final variables from X_test\nX_test_sm = X_test[var_final]\nX_test_sm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add constant\nX_test_sm = sm.add_constant(X_test_sm)\nX_test_sm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test dataset\ny_test_pred = lm_final.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Evaluate the model on test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get R-Squared fro test dataset\nr2_test = r2_score(y_true = y_test, y_pred = y_test_pred)\nprint('R-Squared for Test dataset: ', round(r2_test,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adj. R-Squared for test dataset\nN= len(X_test)          # sample size\np =len(var_final)     # Number of independent variable\nr2_test_adj = round((1-((1-r2_test)*(N-1)/(N-p-1))),3)\nprint('Adj. R-Squared for Test dataset: ', round(r2_test_adj,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Sqare Error\nmse = mean_squared_error(y_test, y_test_pred)\nprint('Mean_Squared_Error :' ,round(mse,4))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"res_test = y_test - y_test_pred\nplt.title('Error Terms', fontsize=16) \nsns.distplot(res_test)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The distribution plot of error term shows the normal distribution with mean at Zero.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error terms\nc = [i for i in range(1,len(y_test)+1,1)]\nfig = plt.figure(figsize=(8,5))\nplt.scatter(y_test,res_test)\nfig.suptitle('Error Terms', fontsize=16)              # Plot heading \nplt.xlabel('Y_test_pred', fontsize=14)                      # X-label\nplt.ylabel('Residual', fontsize=14)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It seems like the corresponding residual plot is reasonably random.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_test_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_test_pred', fontsize = 16)      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Insights:**\n- We can colude that the model `lm_5` fit isn't by chance, and has descent predictive power.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Step-5: Conclusions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print R Squared and adj. R Squared\nprint('R- Sqaured train: ', round(lm_final.rsquared,2), '  Adj. R-Squared train:', round(lm_final.rsquared_adj,3) )\nprint('R- Sqaured test : ', round(r2_test,2), '  Adj. R-Squared test :', round(r2_test_adj,3))\n\n# Print the coefficents of final varible\nprint('\\033[1m{:10s}\\033[0m'.format('\\nCoefficent for the variables are:'))\nprint(round(lm_final.params,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The equation of our best fitted line is:\n\n$ cnt = 0.097 + (0.256  \\times  yr) + (0.097  \\times  holiday) + (0.552 \\times temp) - (0.135 \\times windspeed) + (0.092 \\times SeasonSummer) + (0.143 \\times SeasonWinter) + (0.093 \\times MonthSep) - (0.085 \\times WeathersitMistCloudy) - (0.264 \\times weathersitLightSnowLightRain) $","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- From R-Sqaured and adj R-Sqaured value of both train and test dataset we could conclude that the above variables can well explain more than 80% of bike demand.\n- Coeffiencients of the variables explains the factors effecting the bike demand\n\n- Based on final model top three features contributing significantly towards explaining the demand are:\n    1. Temperature (0.552)\n    2. weathersit : Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds (-0.264)\n    3. year (0.256)\n\n\n- **So it recomended to give these variables utmost importance while planning to achieve maximum demand.**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}