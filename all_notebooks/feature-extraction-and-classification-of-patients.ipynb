{"cells":[{"metadata":{"_uuid":"6c86352ed5db3f810c55e22aff2ac5ab0d46da77"},"cell_type":"markdown","source":"In this notebook my goal is to try to find the model that classifies patients with the greatest precision, subdividing them by class, using the attributes that describe the characteristics of each patient.\nTherefore, I will proceed by separating the predictors which are the numerical attributes, from the target which is the class to which the patient belongs."},{"metadata":{"_uuid":"0af8fb5c1d70a91f9a429d649b199a3148f8cc33"},"cell_type":"markdown","source":"The contents of this notebook will be:\n- Exploratory Data Analysis\n- Feature Extraction (PCA,LDA)\n- Train_test_split and cross-validation methods\n- Multinomial Logistic Regression\n- Decision Tree\n- K-Nearest Neighbors\n- Learning curve\n- Confusion Matrix\n- Accuracy, Precision, Recall\n"},{"metadata":{"trusted":false,"_uuid":"56b748afd040a1faa356f92108b17ea569eaaf06"},"cell_type":"code","source":"import pandas as pd # It helps me to manipulate sequential and tabular data\nimport numpy as np # Linear algebra\nimport seaborn as sns # It helps me to plot high level graphical interface \nimport sklearn #This library provides a great number of classification algorithms\nimport matplotlib.pyplot as plt # This module permits me to plot 2D grphics\n% matplotlib inline \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d17178c0ef78b7506ae782cb2dbac185765326f7"},"cell_type":"markdown","source":"#  Exploratory Data Analysis"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"4bbc5f2a68c62cb744a2a7131da6b0e99989247b"},"cell_type":"code","source":"df=pd.read_csv('../input/column_3C_weka.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec2462c1803666e4a670fcf8c038849c3f5322f5"},"cell_type":"markdown","source":"Initially it is possible to observe that the dataset is composed of 310 entries and for each entry we have 7 columns, 6 of which represent the biomechanical characteristics of the patient and a categorical that is the class with which the patient has been labeled. It is very interesting to note that there are no missing data for all patients."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"aa6af8f867f91da959e41c593e15011431ef33a5"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5b6e6a712e147287d18548db4e3f1287b5c1502"},"cell_type":"markdown","source":"With describe() function I obtain a summary of statistical measures"},{"metadata":{"trusted":false,"_uuid":"9bf953610cbf19514c84fc35564b530d70ecb012"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21ba5ffc137e1f245baafe860e1f2e16ce321a40"},"cell_type":"markdown","source":"I separate the predictor from response"},{"metadata":{"trusted":false,"_uuid":"032dfec2d8cf844809707af82675888ef54e988c"},"cell_type":"code","source":"X=df.drop(['class'],axis=1) #Predictors\n\nY=df['class'].values #Response\n#print(Y)\nX.head()\n#print(len(X))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1990ca338e1b87a53e67211b0c3af4f9b90502f3"},"cell_type":"markdown","source":"Let's now look at these numerical results by graphing them through box-plots:"},{"metadata":{"trusted":false,"_uuid":"7e6f41f0cf7820bacb35423336bb49af85610501"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.boxplot(data=X,orient='v')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47b62f7321a93985063fd0f5f1bddf4978a14842"},"cell_type":"markdown","source":"In all distributions there are outliers, in particular the maximum outlier of the \"degree_spondylolisthesis\" attribute that is very far from the other attributes is obvious to the eye. By analyzing the relative values of that patient better, I noticed quite strange values compared to the other patients in the same category, so thinking it was a transcription error during the collection of the dataset I decided to eliminate it."},{"metadata":{"trusted":false,"_uuid":"c4047f44f8d5aa8a8c564ee0e328b06a6dba57ad"},"cell_type":"code","source":"x=df[df['degree_spondylolisthesis']>100]\nx.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c5f5f6ce85031c30d1ff449c094bf8a81f2fdca4"},"cell_type":"code","source":"x=df[df['degree_spondylolisthesis']<400]\nX=x.drop(['class'],axis=1) #Predictors\nY=x['class'].values #Response\nX.head()\n#Now I have 309 record instead than 310","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7da182a00f27a141f31d9cae4510e4d127d335c9"},"cell_type":"markdown","source":"Deleting the anomalous value gives a better view of the graphical representation:"},{"metadata":{"trusted":false,"_uuid":"9fcfdeeea4b70db15c9116ebae7d05662eee6518"},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.boxplot(data=X,orient='v')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ceca7a90dda89c3372cf89be8f059160cddc28b"},"cell_type":"markdown","source":"From the distance between each quartile and the median it is possible to notice how the attribute 'degree_spondylolisthesis' has a clear positive asymmetric distribution (because the median is smaller than average) with respect to the other attribute groups, moreover it is possible to observe how the standard deviation of this attribute is very high and this indicates that the data accumulate in areas far from the expected value."},{"metadata":{"_uuid":"6a4c66a38a88298c75c3a4fe9e579ab462487618"},"cell_type":"markdown","source":"From the following graph we can also see how the values of the biomechanical variables for each class vary."},{"metadata":{"trusted":false,"_uuid":"4abf84d13c3fa8079915eec7286b472f8911be04"},"cell_type":"code","source":"#Create boxplot for each variable\nplt.figure(figsize=(20,10))\nfor id, var in enumerate(X):\n    plt.subplot(2,3,id+1)\n    sns.boxplot(x='class', y=var, data=x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54637b04375b30a4e3f59d17156e546ccd8e7295"},"cell_type":"markdown","source":"The distribution of patients by class tells us that we have 60 (19.4%) patients diagnosed with Hernia, 149 (48.2%) patients with Spondylolysis and 100 (32.3%) patients with a spinal column Normal;"},{"metadata":{"trusted":false,"_uuid":"5af178e0b043c094177b846a0767d475d07984f3"},"cell_type":"code","source":"plt.figure(figsize=(7,4))\nsns.countplot('class', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d65d0cb8ac0e1e1f53b948bbb82a6aab0e0126ed"},"cell_type":"markdown","source":"Putting aside the classes we try to see now the correlation between the various variables, to get an idea also on the link between one variable and another."},{"metadata":{"trusted":false,"_uuid":"5b3f193fc6a5bd6fb7ad2c5f969d644bd5fbcdd2"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6)) \nsns.heatmap(X.corr(), annot=True, fmt=\".2f\",linewidths=.5,ax=ax)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab4b381c24a820848003d5f8c1e7c0c12178c7e5"},"cell_type":"markdown","source":"The following pairplot shows us graphically the mathematical model that binds the biomechanical variables:"},{"metadata":{"trusted":false,"_uuid":"13a802a89d29b100efa2b1e13c571ca38ad351cd"},"cell_type":"code","source":"g = sns.pairplot(df, hue='class')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60690ec98a45e5fda4aecfc33535a316817fa839"},"cell_type":"markdown","source":"# Principal Component Analysis"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b0bc3728b3dab39d657563a2e158e54c1360cca3"},"cell_type":"code","source":"#Standardize the data \nfrom sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)\nscaled_df = pd.DataFrame(X_std, columns=['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle','sacral_slope','pelvic_radius','degree_spondylolisthesis'])\n#print(np.std(X_std))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7db2d4715c590703bcf32d327584c58e4b50307a"},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(14, 6))\nax1.set_title('Before standardization')\nsns.kdeplot(df['pelvic_incidence'], ax=ax1)\nsns.kdeplot(df['pelvic_tilt'], ax=ax1)\nsns.kdeplot(df['lumbar_lordosis_angle'], ax=ax1)\nsns.kdeplot(df['sacral_slope'], ax=ax1)\nsns.kdeplot(df['pelvic_radius'], ax=ax1)\nsns.kdeplot(df['degree_spondylolisthesis'], ax=ax1)\nax1.set_ylim(0,0.05)\nax2.set_title('After standardization')\nsns.kdeplot(scaled_df['pelvic_incidence'], ax=ax2)\nsns.kdeplot(scaled_df['pelvic_tilt'], ax=ax2)\nsns.kdeplot(scaled_df['lumbar_lordosis_angle'], ax=ax2)\nsns.kdeplot(scaled_df['sacral_slope'], ax=ax2)\nsns.kdeplot(scaled_df['pelvic_radius'], ax=ax2)\nsns.kdeplot(scaled_df['degree_spondylolisthesis'], ax=ax2)\nax2.set_ylim(0,0.6)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"51fcbbe7cdc21447a82f6c9e245771bc2b0656bb"},"cell_type":"code","source":"cov_mat=np.cov(X_std.T) # with np.cov I compute the covariance matrix of the standardized data\neigen_vals,eigen_vecs=np.linalg.eig(cov_mat)# with linalg.eig I compute eigenvalues and eigenvectorscalcolo eigen_vals\nprint('\\nEigenvalues \\n%s' % eigen_vals)\nprint('\\nEigenvectors \\n%s' % eigen_vecs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63d2e25589231e6c53ebeaa1f5eb80ccb39b91cf"},"cell_type":"code","source":"tot=sum(eigen_vals)\nvar_exp=[(i/tot)*100 for i in sorted(eigen_vals,reverse=True)]\ncum_var_exp=np.cumsum(var_exp)\n#print(len(var_exp))\nplt.figure(figsize=(10,5))\nplt.bar(range(6),var_exp,alpha=0.5,align='center',label='individual explained variance')\nplt.step(range(6),cum_var_exp,where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.tight_layout()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63ce8f1280e5a272ba670aeb5535a49011539d77"},"cell_type":"code","source":"print(\"Variance explained by every single main component:\")\nprint(var_exp)\nprint(\"\\n\")\nprint(\"Total variance explained:\")\nprint(cum_var_exp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"160780387bc243133b30668a3905cc4c21d8433a"},"cell_type":"code","source":"#I do the list of the (eigenvalue,eigenvector) tuples\neigen_pairs=[(np.abs(eigen_vals[i]),eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n# I sort the tuples (eigenvectors,eigenvalues) in discending order\neigen_pairs.sort(key=lambda x:x[0],reverse=True)\n\nprint('Eigenvalues in ordine discendente:')\nfor i in eigen_pairs:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ce0bd78c124b5d529b4639b6e6aec675151d495f"},"cell_type":"code","source":"#I choose the first Principal Component because I want to plot data in 2 dimensions\nmatrix_w=np.hstack((eigen_pairs[0][1][:,np.newaxis],\n                   eigen_pairs[1][1][:,np.newaxis],\n                   ))\nprint('Matrix W:\\n',matrix_w)\nprint(np.shape(matrix_w))\nprint(np.shape(X_std))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"92eeef623858a7ad2fccfa354c78a87303647cd8"},"cell_type":"code","source":"#I transform the original 309x6 matrix in 309x2 data matrix\nX_std_pca=X_std.dot(matrix_w)# (319*6)*(6*2)= (310*2)\n#print(X_std_pca)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a12eae94e80b399779ed670497817b3b624b0344"},"cell_type":"markdown","source":"The plot of the data using the first two principal component is the follow:"},{"metadata":{"trusted":false,"_uuid":"95f8e98618c5964acebb5b1917dd85421266a9d0"},"cell_type":"code","source":"colors=['blue','green','orange']\nmarkers=['s','^','o']\nplt.figure(figsize=(6,4))\nfor lab,col,m in zip(np.unique(Y),colors,markers):\n    plt.scatter(X_std_pca[Y==lab,0],X_std_pca[Y==lab,1]*-1,label=lab,c=col,marker=m,alpha=0.8)\n    \nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend(loc='upper left')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5a79a7e6d41e1d7ca2be008ca890f973c1553be9"},"cell_type":"code","source":"#I implement PCA using sklearn\nfrom sklearn.decomposition import PCA as sklearnPCA\npca = sklearnPCA(n_components=2)\nT=pca.fit_transform(X_std)#Fit the model with X and apply the dimensionality reduction on X.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"811b359c8d87599840ee1c711de82940e3db6933"},"cell_type":"code","source":"import math\n\ndef get_important_features(transformed_features, components_, columns):\n    \"\"\"\n    This function will return the most \"important\" \n    features so we can determine which have the most\n    effect on multi-dimensional scaling\n    \"\"\"\n    num_columns = len(X.columns)\n\n    # Scale the principal components by the max value in\n    # the transformed set belonging to that component\n    xvector = components_[0] * max(transformed_features[:,0])\n    yvector = components_[1] * max(transformed_features[:,1])\n\n    # Sort each column by it's length. These are your *original*\n    # columns, not the principal components.\n    important_features = { columns[i] :round( math.sqrt(xvector[i]**2 + yvector[i]**2),2) for i in range(num_columns) }\n    important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n    print (\"Features for importance:\\n\\n\",important_features)\n\nget_important_features(T, pca.components_, X.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"80e3a54f74dd5a5af659420bf6661621c3d92734"},"cell_type":"code","source":"def draw_vectors(transformed_features, components_, columns):\n    \"\"\"\n    This funtion will project your *original* features\n    onto your principal component feature-space, so that you can\n    visualize how \"important\" each one was in the\n    multi-dimensional scaling\n    \"\"\"\n\n    num_columns = len(X.columns)\n\n    # Scale the principal components by the max value in\n    # the transformed set belonging to that component\n    xvector = components_[0] * max(transformed_features[:,0])\n    yvector = components_[1] * max(transformed_features[:,1])\n\n    ax = plt.axes()\n    \n\n    for i in range(num_columns):\n    # Use an arrow to project each original feature as a\n    # labeled vector on your principal component axes\n        plt.arrow(0, 0, xvector[i], yvector[i], color='black', width=0.0005, head_width=0.02, alpha=0.75)\n        plt.text(xvector[i]*1.2, yvector[i]*1.2, list(columns)[i], color='black', alpha=0.9)\n\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ff2abb33eb6d4e3ab9ef0beef6b3f607e9b1033"},"cell_type":"code","source":"plt.figure(figsize=(14,7))\nax = draw_vectors(T, pca.components_, X.columns.values)\nT_df = pd.DataFrame(T)\nT_df.columns = ['component1', 'component2']\nT_df=T_df.values\ncolors=['blue','green','orange']\nmarkers=['s','^','o']\n\nfor lab,col,m in zip(np.unique(Y),colors,markers):\n    plt.scatter(T_df[Y==lab,0],T_df[Y==lab,1],label=lab,c=col,marker=m,alpha=0.6)\n\nplt.xlabel('Principle Component 1')\nplt.ylabel('Principle Component 2')\n\nplt.show()\n#print(T_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79394d28b65d29c6d82ce2cf28fe73c0aa3120d5"},"cell_type":"markdown","source":"In the following plot we have that the ligthing points represent the projected after applying PCA, the dark spots in cross shapes represent the original data"},{"metadata":{"trusted":false,"_uuid":"3b24c596331fac436f96b20e5dd98f271bb047ec"},"cell_type":"code","source":"X_new = pca.inverse_transform(T)\ncolors=['blue','green','orange']\nmarkers=['s','^','o']\nplt.figure(figsize=(14,7))\nplt.scatter(T[:, 0], T[:, 1], alpha=0.2,c='black',marker='x')\nfor lab,col,m in zip(np.unique(Y),colors,markers):\n    plt.scatter(X_new[Y==lab,0],X_new[Y==lab,1],label=lab,c=col,marker=m,alpha=0.9)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0548cf92ea4e442078d98e1c8a2304a3ee0e8c8"},"cell_type":"markdown","source":"# Linear Discriminant Analysis"},{"metadata":{"trusted":true,"_uuid":"71127a1ab2fa85584628371fc5321a96992ada7f"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nenc=LabelEncoder()\nlabel_encoder=enc.fit(Y)\nEnc_y=label_encoder.transform(Y)+1\nlabel_dict={1:'Hernia',2:'Normal',3:'Spondylolisthesis'}\nnp.set_printoptions(precision=4)\nmean_vecs=[]\nfor cl in range(1,4):\n    mean_vecs.append(np.mean(X[Enc_y==cl],axis=0))\n    print('Mean vector class %s:\\n%s\\n'%(label_dict[cl],mean_vecs[cl-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af9b2694e51d905f2eaf750886b6bea4cd562aae"},"cell_type":"code","source":"d=6\nS_W=np.zeros((d,d))#scatter matrix is the same of covariance matrix; the cov matrix is normalized version of the scatter matrix\nfor cl,mv in zip(range(1,4),mean_vecs):\n    class_sc_mat=np.zeros((d,d)) \n    for row in X_std[Enc_y==cl]:\n        row,mv=row.reshape(d,1),mv.values.reshape(d,1)\n        class_sc_mat+=(row-mv).dot((row-mv).T) \n    S_W+=class_sc_mat \nprint('Within-class scatter matrix: %sx%s' % (S_W.shape[0],S_W.shape[1]))\nprint('within-class scatter matrix:\\n',S_W)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"38c72b0e1e56c1899c85e03b914f5cb2eb3feace"},"cell_type":"code","source":"#Compute the between-class scatter matrix\nmean_overall=np.mean(X_std,axis=0)\nd=6\nS_B=np.zeros((d,d))#between class scatter matrix\nfor i,mean_vec in enumerate(mean_vecs):\n    n=X_std[Enc_y==i+1,:].shape[0]\n    mean_vec=mean_vec.reshape(d,1) \n    mean_overall=mean_overall.reshape(d,1)\n    S_B+=n*(mean_vec-mean_overall).dot((mean_vec-mean_overall).T)\nprint('Between-class scatter matrix: %sx%s' % (S_B.shape[0],S_B.shape[1]))\nprint('Between-class scatter matrix:\\n',S_B)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2cbd6dc2807a9a3cb612c82b4924d91864b47d12"},"cell_type":"code","source":"eigen_vals,eigen_vecs=np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b286fe153e6f979e677247b2a9bfeebfb8004d8a"},"cell_type":"code","source":"#I do the list of the (eigenvalue,eigenvector) tuples\neigen_pairs=[(np.abs(eigen_vals[i]),eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n# I sort the (eigenvectors,eigenvalues) tuples in descending order\neigen_pairs=sorted(eigen_pairs,key=lambda x:x[0],reverse=True)\n#stampo\nprint('Eigenvalues in ordine discendente:')\nfor i in eigen_pairs:\n    print(i[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9cb71ff80afb78fe5f18182b29a7f7950cbd6967"},"cell_type":"code","source":"print('Discriminability ratio:\\n')\neigv_sum=sum(eigen_vals)\nfor i,j in enumerate(eigen_pairs):\n    print('Linear discriminant {0:}:{1:.2%}'.format(i+1,(j[0]/eigv_sum).real))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3706ac74d173b1c5f83551cb9d4010f2e22617ab"},"cell_type":"code","source":"tot=sum(eigen_vals.real)\ndiscr=[(i/tot) for i in sorted(eigen_vals.real,reverse=True)]\ncum_discr=np.cumsum(discr)\n\nplt.figure(figsize=(10,6))\nplt.bar(range(1,7),discr,alpha=0.5,align='center',label='individual discriminability')\nplt.step(range(1,7),cum_discr,where='mid',label='cumulative discriminability')\nplt.ylabel('discriminability ratio')\nplt.xlabel('Linear discriminant')\nplt.legend(loc='upper right')\nplt.tight_layout()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b4c20f6cdf2734605ad30fbf02f5b4b7dcef16f1"},"cell_type":"code","source":" print('Linear discriminant ',cum_discr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bf36736b447beaae0926e7639ac607a0823ee957"},"cell_type":"code","source":"w=np.hstack((eigen_pairs[0][1][:,np.newaxis],\n            eigen_pairs[1][1][:,np.newaxis],\n            #eigen_pairs[2][1][:,np.newaxis]\n            ))\nprint('Matrix W:\\n',w)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70fa958379bd2f1bde857b72ecd1954514db79b8"},"cell_type":"markdown","source":"The following scatter-plot represents the data using the first two linear discriminants. We can observe that with LDA we have a better separability of classes than the PCA."},{"metadata":{"trusted":false,"_uuid":"02e56063e3171f5cb444a945c1cb11b7c41c6cbc"},"cell_type":"code","source":"X_std_lda=X_std.dot(w.real)\ncolors=['blue','green','orange']\nmarkers=['s','^','o']\nplt.figure(figsize=(10,6))\nfor lab,col,m in zip(np.unique(Y),colors,markers):\n    plt.scatter(X_std_lda[Y==lab,0]*-1,X_std_lda[Y==lab,1],label=lab,c=col,marker=m,alpha=0.8)\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.legend(loc='upper right')\nplt.tight_layout()\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c430f076e370d26a6e906f6683a5d990b10ff53b"},"cell_type":"code","source":"#Implementation of LDA using sklearn library\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nsklearn_lda=LDA(n_components=2)\nX_lda_sklearn=sklearn_lda.fit_transform(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3416f986829151c0e06decb53c26094485db31d9"},"cell_type":"code","source":"def plot_scikit_lda(X,title):\n    plt.figure(figsize=(10,6))\n    for label,marker,color in zip(\n    np.unique(Y),('s','^','o'),('blue','green','orange')):\n        plt.scatter(x=X[:,0][Y==label],\n                   y=X[:,1][Y==label],\n                    label=label,\n                   marker=marker,\n                   color=color,\n                   alpha=0.8)\n    plt.xlabel('LD1')\n    plt.ylabel('LD2')\n    leg= plt.legend(loc='upper right', fancybox=True)\n    \n   # plt.title(title)\n    \n    plt.tick_params(axis=\"both\",which=\"both\",bottom=\"off\",top=\"off\",labelbottom=\"on\",left=\"off\",right=\"off\",labelleft=\"on\")\n    \n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)\n    \n   # plt.grid()\n    plt.tight_layout\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4d29dc299638ba1015a719e849d68eafe1d5dd35"},"cell_type":"code","source":"plot_scikit_lda(X_lda_sklearn, title='Default LDA via scikit-learn')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0e6a419e1a1401c8d262be2247cf62b969f76f5"},"cell_type":"markdown","source":"****## Train_test_split method"},{"metadata":{"trusted":false,"_uuid":"fd3719ffd3e82568a59f6aedf5b2495be002544b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,StratifiedKFold\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)\nprint('X_train dim:', x_train.shape)\nprint('X_test dim:',x_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a7b1abfc361fe7b052947f24487c229b7a7e4f5b"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be86e0c5616bbfde3621ad9a938f546d2780587f"},"cell_type":"markdown","source":"With the following I represent a DataFrame,that contains train_score and test_score of different classifier, using train_test-split method"},{"metadata":{"trusted":false,"_uuid":"a8d63c831678aa7725ec456a08d30691207364d1"},"cell_type":"code","source":"\nmodels=[LogisticRegression(multi_class='multinomial',solver='newton-cg'),LogisticRegression(),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=5),\n        KNeighborsClassifier(n_neighbors=3),KNeighborsClassifier(n_neighbors=8),KNeighborsClassifier(n_neighbors=16)]\nclassifier_comparison=pd.DataFrame(columns=['Classificator','train_score','test_score'])\nfor i in range(len(models)):\n    models[i].fit(x_train,y_train)\n    if i==0:\n        classifier_comparison.loc[i,'Classificator']='MultinomialLogisticRegression'\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)\n    elif i==1:\n        classifier_comparison.loc[i,'Classificator']='LogisticRegression'\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)\n    elif i==2:\n        dtree=models[i]\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=3)'\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)\n    elif i==3:\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=5)'\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)\n    elif i==4:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=3)'\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)\n    elif i==5:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=8)'\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)\n    elif i==6:\n        classifier_comparison.loc[i,'Classificatore']='KNeignborsClassifier (k=16)'\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)  \n    else:\n        classifier_comparison.loc[i,'Classificator']=models[i].__class__.__name__\n        classifier_comparison.loc[i,'train_score']=models[i].score(x_train,y_train)\n        classifier_comparison.loc[i,'test_score']=models[i].score(x_test,y_test)  \nclassifier_comparison\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37597f79e13d7246350335c27ead3f6dcf655de4"},"cell_type":"markdown","source":"# K-fold cross-validation method"},{"metadata":{"_uuid":"731d461f0a385bc0cb954a4e616037e77ca43024"},"cell_type":"markdown","source":"With the following I represent a DataFrame,that contains train_score and test_score of different classifier, using train_test-split method."},{"metadata":{"trusted":false,"_uuid":"6bf238d2ceb2f34154785ba3f9ed14a59999b8e6"},"cell_type":"code","source":"models=[LogisticRegression(multi_class='multinomial',solver='newton-cg'),LogisticRegression(),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=5),KNeighborsClassifier(n_neighbors=3),\n        KNeighborsClassifier(n_neighbors=8),KNeighborsClassifier(n_neighbors=16)]\nkfold=StratifiedKFold(n_splits=10,random_state=42)\nclassifier_comparison=pd.DataFrame(columns=['Classificator','train_score','test_score'])\nfor i, model in enumerate(models):\n    classifier=model\n    cv_result=cross_validate(model,X,Y,cv=kfold,scoring='accuracy')\n    if i==0:\n        classifier_comparison.loc[i,'Classificator']='MultinomialLogisticRegression'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==1:\n        classifier_comparison.loc[i,'Classificator']='LogisticRegression'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()    \n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==2:\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=3)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==3:\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=5)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==4:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=3)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()  \n    elif i==5:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=8)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean() \n    elif i==6:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=16)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()  \n    \n    else:\n        classifier_comparison.loc[i,'Classificator']=model.__class__.__name__\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']= cv_result['test_score'].mean()\nclassifier_comparison\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6171b176a31449594c55094132526eb33eb1044f"},"cell_type":"code","source":"#complexity of the model using K-NN\nneig = np.arange(1, 20)\ntrain_accuracy = []\ntest_accuracy = []\ntrain = np.array(X)\nlabels = np.array(Y)\n# Loop over different value of k to know for which k I have the best accuracy\nfor i, k in enumerate(neig):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    test_sum=0\n    train_sum=0\n    for train_index, test_index in kfold.split(train,labels):\n        X_train, X_test = [train[i] for i in train_index],[train[j] for j in test_index]\n        y_train, y_test = [labels[i] for i in train_index],[labels[j] for j in test_index]\n        knn.fit(X_train,y_train)\n        test_sum+= knn.score(X_test,y_test)\n        train_sum+=knn.score(X_train,y_train)\n    #train accuracy\n    train_accuracy.append((train_sum/10))\n    # test accuracy\n    test_accuracy.append((test_sum/10))\n\n# Plot\nplt.figure(figsize=[8,4])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afa936399620ae8753d6b85b702557ccc53e2155"},"cell_type":"markdown","source":"I check the model's performance on the data on which I applied the PCA"},{"metadata":{"trusted":false,"_uuid":"3c7e32aba4a452fd7ae6b7ed0415833b5a8b50c3"},"cell_type":"code","source":"models=[LogisticRegression(multi_class='multinomial',solver='newton-cg'),LogisticRegression(),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=5),KNeighborsClassifier(n_neighbors=3),\n        KNeighborsClassifier(n_neighbors=8),KNeighborsClassifier(n_neighbors=16)]\nkfold=StratifiedKFold(n_splits=10,random_state=42)\nclassifier_comparison=pd.DataFrame(columns=['Classificator','train_score','test_score'])\nfor i, model in enumerate(models):\n    classifier=model\n    cv_result=cross_validate(model,X_std_pca,Y,cv=kfold,scoring='accuracy')\n    if i==0:\n        classifier_comparison.loc[i,'Classificator']='MultinomialLogisticRegression'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==1:\n        classifier_comparison.loc[i,'Classificator']='LogisticRegression'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()    \n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==2:\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=3)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==3:\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=5)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==4:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=3)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==5:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=8)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean() \n    elif i==6:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=16)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()  \n    \n    else:\n        classifier_comparison.loc[i,'Classificator']=model.__class__.__name__\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']= cv_result['test_score'].mean()\nclassifier_comparison\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cacf090feca3299b45b454f4ed5965ef714902bf"},"cell_type":"markdown","source":"I check the model's performance on the data on which I applied the LDA"},{"metadata":{"trusted":false,"_uuid":"066ad6c10e73958eb12a0b6fb0d209588228ae9c"},"cell_type":"code","source":"models=[LogisticRegression(multi_class='multinomial',solver='newton-cg'),LogisticRegression(),DecisionTreeClassifier(max_depth=3),DecisionTreeClassifier(max_depth=5),KNeighborsClassifier(n_neighbors=3),\n        KNeighborsClassifier(n_neighbors=8),KNeighborsClassifier(n_neighbors=16)]\nkfold=StratifiedKFold(n_splits=10,random_state=42)\nclassifier_comparison=pd.DataFrame(columns=['Classificator','train_score','test_score'])\nfor i, model in enumerate(models):\n    classifier=model\n    cv_result=cross_validate(model,X_lda_sklearn,Y,cv=kfold,scoring='accuracy')\n    if i==0:\n        classifier_comparison.loc[i,'Classificator']='MultinomialLogisticRegression'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==1:\n        classifier_comparison.loc[i,'Classificator']='LogisticRegression'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()    \n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==2:\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=3)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==3:\n        classifier_comparison.loc[i,'Classificator']='DecisionTreeClassifier (max_depth=5)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()\n    elif i==4:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=3)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()  \n    elif i==5:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=8)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean() \n    elif i==6:\n        classifier_comparison.loc[i,'Classificator']='KNeignborsClassifier (k=16)'\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']=cv_result['test_score'].mean()  \n    \n    else:\n        classifier_comparison.loc[i,'Classificator']=model.__class__.__name__\n        classifier_comparison.loc[i,'train_score']=cv_result['train_score'].mean()\n        classifier_comparison.loc[i,'test_score']= cv_result['test_score'].mean()\nclassifier_comparison\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14ae1a25f79fd15aa2e6755854608695e57dbbce"},"cell_type":"markdown","source":"By comparing the model results after applying the PCA and after applying the LDA, it can be seen that using the LDA results are much better than using PCA. In particular the results that I obtain with after I applied LDA are very similar to the results that I obtain using the original dataset without using feature extraction techniques."},{"metadata":{"_uuid":"614df5b8526826c85fe447b10480a19a5cfddd91"},"cell_type":"markdown","source":"The following is the graphical representation of Decision-Tree with max depth equal to three. In particular from this representation we can see that the variable 'degree_spondylolisthesis' split the dataset in two subset. In particular for 'degree_spondylolistesis'<=16.079 the patients tend to belong principally to the classes are Hernia and Normal instead for values of  'degree_spondylolistesis'>=16.079 the patients principally belong to the class Spondilolystesis.Thanks to the Gini index is possible to analyze the impurity of each node of the tree; moreover it is possible to observe for each class, what is the number of patients that belong to each class."},{"metadata":{"trusted":false,"_uuid":"3c49c5d8612dd8bedea5fcd8cd5dd419af194703"},"cell_type":"code","source":"from sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ntrain = np.array(X)\nlabels = np.array(Y)\ndtree=DecisionTreeClassifier(max_depth=3)\ntest_sum=0\ntrain_sum=0\nfor train_index, test_index in kfold.split(train,labels):\n    \n    X_train, X_test = [train[i] for i in train_index],[train[j] for j in test_index]\n    y_train, y_test = [labels[i] for i in train_index],[labels[j] for j in test_index]\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    dtree.fit(X_train,y_train)\n    test_sum+= dtree.score(X_test,y_test)\n    train_sum+=dtree.score(X_train,y_train)\n\nprint(\"train score: \",train_sum/10)\nprint(\"test score: \",test_sum/10)\n\nlabels=[]\nfor i in range(0,6):\n    labels.append(df.columns[i])\n\ndot_data = StringIO()\nexport_graphviz(dtree, out_file=dot_data,\n                feature_names=labels,\n                class_names=['Hernia', 'Normal', 'Spondylolisthesis'],\n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5abab34468299f5463fb991d3d4a8c3ce6c4d276"},"cell_type":"markdown","source":"# Learning curve"},{"metadata":{"trusted":true,"_uuid":"dc692b0e858635126922245c9967b8264b457290"},"cell_type":"code","source":"from sklearn.utils import shuffle\nX_shuf, Y_shuf = shuffle(X, Y)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71364fe3882e2aa2b9909d497fcbed8a23022678"},"cell_type":"markdown","source":"- Learning curve using Multinomial Logistic Regression classifier"},{"metadata":{"trusted":false,"_uuid":"2bdbe39916da947a8ae6cec67b31cf4b9f93e1f0"},"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\ncv=StratifiedKFold(n_splits=10)\npipe_lr=make_pipeline(StandardScaler(),LogisticRegression(multi_class='multinomial',solver='newton-cg',random_state=42))\ntrain_sizes,train_scores,test_scores= learning_curve(estimator=pipe_lr,X=X_shuf,y=Y_shuf,train_sizes=np.linspace(0.1,1.0,10),cv=cv,n_jobs=1)\ntrain_mean=np.mean(train_scores,axis=1)\ntrain_std=np.std(train_scores,axis=1)\ntest_mean=np.mean(test_scores,axis=1)\ntest_std=np.std(test_scores,axis=1)\nplt.plot(train_sizes,train_mean,color='blue',marker='o',markersize=5,label='training accuracy')\nplt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.2,color='blue')\nplt.plot(train_sizes,test_mean,color='black',linestyle='--',marker='s',markersize=5,label='validation accuracy')\nplt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.2,color='black')\nplt.grid()\nplt.xlabel('Number of training sample')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.65,1.0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcde67ff52fe9abac6c9b1f84db8a5fa0e65c148"},"cell_type":"markdown","source":"From the above curve, we observe that our model works really well already when we evaluate only 150 samples and we can see how it tends to increase accuracy as the size of the dataset."},{"metadata":{"_uuid":"965401acc683dfcd81519fab4757d06be42008f4"},"cell_type":"markdown","source":"- Learning curve using Decision-Tree classifier with max depth equal to 3"},{"metadata":{"trusted":false,"_uuid":"fb9b131e77021fabbf39a00ca0d23bd9c60654cf"},"cell_type":"code","source":"pipe_lr1=make_pipeline(StandardScaler(),DecisionTreeClassifier(max_depth=3,random_state=1))\ntrain_sizes,train_scores,test_scores= learning_curve(estimator=pipe_lr1,X=X_shuf,y=Y_shuf,train_sizes=np.linspace(0.1,1.0,10),cv=cv,n_jobs=1)\ntrain_mean=np.mean(train_scores,axis=1)\ntrain_std=np.std(train_scores,axis=1)\ntest_mean=np.mean(test_scores,axis=1)\ntest_std=np.std(test_scores,axis=1)\nplt.plot(train_sizes,train_mean,color='blue',marker='o',markersize=5,label='training accuracy')\nplt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.2,color='blue')\nplt.plot(train_sizes,test_mean,color='black',linestyle='--',marker='s',markersize=5,label='validation accuracy')\nplt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.2,color='black')\nplt.grid()\nplt.xlabel('Number of training sample')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.65,1.0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e42b5a5fe86187316117663080f7aad947894fc"},"cell_type":"markdown","source":"The above learning curve, in which we use the Decision Tree classifier with depth 3, shows us how this classification technique initially suffers from overfitting seen the marked gap between the two accuracy curves; in particular, we note how this gap tends to get thinner with the increase in the number of samples taken into consideration, until reaching an accuracy of almost 85% when we have considered more than 250 samples."},{"metadata":{"_uuid":"6cc694e78df88fe3bfde6139aea48534fae14515"},"cell_type":"markdown","source":"- Learning curve using K-NN classifier with k=8"},{"metadata":{"trusted":false,"_uuid":"a41697cfe67d5d2dd151fdf71d9d135022ce1aa7"},"cell_type":"code","source":"pipe_lr2=make_pipeline(StandardScaler(),KNeighborsClassifier(n_neighbors=8))\ntrain_sizes,train_scores,test_scores= learning_curve(estimator=pipe_lr2,X=X_shuf,y=Y_shuf,train_sizes=np.linspace(0.1,1.0,10),cv=cv,n_jobs=1)\ntrain_mean=np.mean(train_scores,axis=1)\ntrain_std=np.std(train_scores,axis=1)\ntest_mean=np.mean(test_scores,axis=1)\ntest_std=np.std(test_scores,axis=1)\nplt.plot(train_sizes,train_mean,color='blue',marker='o',markersize=5,label='training accuracy')\nplt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.2,color='blue')\nplt.plot(train_sizes,test_mean,color='black',linestyle='--',marker='s',markersize=5,label='validation accuracy')\nplt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.2,color='black')\nplt.grid()\nplt.xlabel('Number of training sample')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.65,1.0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae498d2cdf8c84533af34144702cbfc38124399d"},"cell_type":"markdown","source":"The above learning curve, shows us that the K-Nearest-Neighbors classifier achieves its greater accuracy when we considered about 150 samples and then lost accuracy. As the number of data taken into consideration increases, it seems that the accuracy of the training set and the test set tend to converge again. In this case with a larger dataset we could have removed some doubt."},{"metadata":{"_uuid":"6b15db9fbff3a25a98220ae8d0d58bd408a9434c"},"cell_type":"markdown","source":"# # Confusion Matrix, Precision, Recall"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"75268da4c160cf174a7d324023005503ec277c2c"},"cell_type":"markdown","source":"Previously I evaluated my models using Accuracy metric.\nNow to measure the performance of the model I use Precision and Recall that I will derive from Confusion Matrix"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2e61c8af38d6ad19b3e04ad2fbfab5b575e81fd4"},"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(cm, classes,normalize=False,cmap=plt.cm.Blues):\n    \n    #This function prints and plots the confusion matrix.\n   #Normalization can be applied by setting `normalize=True`.\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    \n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3c8158b7b0a3a280a964f51d74add845a32bbb2c"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(LogisticRegression(multi_class='multinomial',solver='newton-cg'), X, Y, cv=cv)\nfrom sklearn.model_selection import cross_val_predict\ny_predL = cross_val_predict(LogisticRegression(multi_class='multinomial',solver='newton-cg'), X, Y, cv=cv)\nconf_matL = confusion_matrix(Y, y_predL)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3f4bc8fe10c81e3d2e87159b2a5028aff42e3c21"},"cell_type":"code","source":"class_names=['Hernia','Normal','Spondylolidthesis']\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(conf_matL, classes=class_names )\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(conf_matL, classes=class_names, normalize=True )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f8e4dcc0a546ef92adb1d589ad9fcb2663fb0cda"},"cell_type":"code","source":"recall=np.diag(conf_matL)/np.sum(conf_matL,axis=1)\nprecision=np.diag(conf_matL)/np.sum(conf_matL,axis=0)\ncomparison=pd.DataFrame(index=['Hernia','Normal','Spondylolisthesis'],columns=['Recall %','Precision %'])\ncomparison['Recall %']=recall*100\ncomparison['Precision %']=precision*100\ncomparison","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bcc72aa7899126da4734cfda92886667f0783d1f"},"cell_type":"code","source":"scores = cross_val_score(DecisionTreeClassifier(max_depth=3), X, Y, cv=cv)\ny_predD = cross_val_predict(DecisionTreeClassifier(max_depth=3), X, Y, cv=cv)\nconf_matD = confusion_matrix(Y, y_predD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3581579425136a5d5094360d3fb7f6df888c5e23"},"cell_type":"code","source":"plt.figure()\nplot_confusion_matrix(conf_matD, classes=class_names )\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(conf_matD, classes=class_names, normalize=True )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"36b32e5ec6157263e9cf47f9107ff3239bde0951"},"cell_type":"code","source":"recall=np.diag(conf_matD)/np.sum(conf_matD,axis=1)\nprecision=np.diag(conf_matD)/np.sum(conf_matD,axis=0)\n#print(pd.DataFrame(recall,columns=['Recall']))\n#print(pd.DataFrame(precision,columns=['Precision']))\ncomparison=pd.DataFrame(index=['Hernia','Normal','Spondylolisthesis'],columns=['Recall %','Precision %'])\ncomparison['Recall %']=recall*100\ncomparison['Precision %']=precision*100\ncomparison","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c095aa446fd9570076c4345ec16cede086148e80"},"cell_type":"code","source":"scores = cross_val_score(KNeighborsClassifier(n_neighbors=8), X, Y, cv=cv)\ny_predK = cross_val_predict(KNeighborsClassifier(n_neighbors=8), X, Y, cv=cv)\nconf_matK = confusion_matrix(Y, y_predK)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b9dafaca141124523f731034ae3c14b009a3b19c"},"cell_type":"code","source":"plt.figure()\nplot_confusion_matrix(conf_matK, classes=class_names )\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(conf_matK, classes=class_names, normalize=True )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"58e8999d5829c9a50ac8055466ef8bb21faa234b"},"cell_type":"code","source":"recall=np.diag(conf_matK)/np.sum(conf_matK,axis=1)\nprecision=np.diag(conf_matK)/np.sum(conf_matK,axis=0)\n#print(pd.DataFrame(recall,columns=['Recall']))\n#print(pd.DataFrame(precision,columns=['Precision']))\ncomparison=pd.DataFrame(index=['Hernia','Normal','Spondylolisthesis'],columns=['Recall %','Precision %'])\ncomparison['Recall %']=recall*100\ncomparison['Precision %']=precision*100\ncomparison","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd6d92a5dd4314a3a4c069e4d8c4e6d524a64753"},"cell_type":"markdown","source":"Through the techniques of feature extraction used to reduce the dimensionality of the data, and being that my goal is to represent the data through two main components, we obtain a discrete result using the PCA while we have a very good result using the LDA.\nFurthermore, it can be observed, how the results obtained using only the first two components of the LDA are very close to the results obtained using the original dataset, without having applied feature extraction techniques. Considering that there is not much difference in the results, it is convenient to reduce the dimensionality of the data through the LDA, both because it allows to improve the computational efficiency of the learning algorithm and because sometimes it allows to improve the performance during the prediction phase."},{"metadata":{"_uuid":"e65c21f1c97e6e5df22affe1b9f65e2b26e5fcdf"},"cell_type":"markdown","source":"However, I have found that patients labeled with the \"Hernia\" and \"Normal\" classes have similar characteristics and there is no clear division between the two classes, while patients labeled with the \"Spondylolisthesis\" class are more separate than the other two classes.\nIn general, for the analyzed dataset, the techniques used are quite efficient, in fact in most cases the accuracy is around 85%."},{"metadata":{"_uuid":"5125f9c69ff0671dd95043fea5a766915e487cab"},"cell_type":"markdown","source":"The generated models will then be able to accurately predict with the patient class for new records that will be added to the dataset, provided they have the same structure with the same attributes and classes."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}