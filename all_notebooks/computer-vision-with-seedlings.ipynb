{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Computer vision with seedlings\n\nSeedlings are heavily competing for nutrients and water and often this phase is crucial for a plant to prevail against all other seedlings. Consequently it can be of advantage in agriculture to automatically detect wild and cultivated plants. By eliminating wild plant in the early growth state of the plant one might be able to reduce crop losses in the end. Let's explore how computer vision and machine learning can help to solve this task. For this purpose we can use [this research image dataset](https://vision.eng.au.dk/plant-seedlings-dataset/) with 12 different common species in Danish agriculture of cultivated and wild plants.\n\n<img src=\"https://cdn.pixabay.com/photo/2014/01/31/23/25/cress-255938_1280.jpg\" width=\"600px\">\n\n\nIn a previous notebook we already analysed that this dataset has an unconscious target leakage. It leads to a model that learns from the size of the stones in the background to predict the plant species. In this notebook we will bypass this behaviour by extracting plant information only and thereby removing the pebble stones background.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Loading packages\n\nAs usual we need to load some packages to get started:","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport random\n\nfrom PIL import Image\nfrom imageio import imread\n\nimport seaborn as sns\nsns.set_style(\"dark\")\nsns.set()\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CyclicLR\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\n\nfrom skimage.morphology import closing, disk, opening\n\nimport cv2\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom os import listdir\nfrom skimage.segmentation import mark_boundaries\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\ntorch.backends.cudnn.deterministic=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training=False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data\n\nLet's take a look at our input folder:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that I have added pretrained neural networks for solving this task.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = \"../input/v2-plant-seedlings-dataset/nonsegmentedv2/\"\nOUTPUT_PATH = \"segmented_seedlings\"\nMODEL_PATH = \"../input/seedlingsmodel/segmented_seedlings\"\nLOSSES_PATH = \"../input/seedlingsmodel/\"\nsubfolders = listdir(base_path)\nsubfolders","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting image paths and target species","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"All target species have different subfolders. Consequently we can extract the target as well as the path for each image by collecting them for each subfolder. After doing so we end up with a pandas dataframe that holds the path, the species as well as the width and height of all images:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"total_images = 0\nfor folder in subfolders:\n    total_images += len(listdir(base_path + folder))\n\nplantstate = pd.DataFrame(index=np.arange(0, total_images), columns=[\"width\", \"height\", \"species\"])\n\nk = 0\nall_images = []\nfor m in range(len(subfolders)):\n    folder = subfolders[m]\n    \n    images = listdir(base_path + folder)\n    all_images.extend(images)\n    n_images = len(images)\n    \n    for n in range(0, n_images):\n        image = imread(base_path + folder + \"/\" + images[n])\n        plantstate.loc[k, \"width\"] = image.shape[1]\n        plantstate.loc[k, \"height\"] = image.shape[0]\n        plantstate.loc[k, \"species\"] = folder\n        plantstate.loc[k, \"image_name\"] = images[n]\n        k+=1\n\nplantstate.width = plantstate.width.astype(np.int)\nplantstate.height = plantstate.height.astype(np.int)\nplantstate.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The images can have different widths and heights but most of the time they are quadratic. We have already found this insight in the previous part of the analysis [\"Train as you fight with seedlings\"](https://www.kaggle.com/allunia/train-as-you-fight-with-seedlings) that covers the unconscious target leakage. \n\n## Preparing the targets\n\nNow we need to turn the string of the species to numerical values using label encoding:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"encoder = LabelEncoder()\nlabels = encoder.fit_transform(plantstate.species.values)\nplantstate[\"target\"] = labels\nNUM_CLASSES = plantstate.target.nunique()\nplantstate.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Peek at the data ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(4,3,figsize=(20,25))\n\nfor m in range(4):\n    for n in range(3):\n        folder = subfolders[m+n*4]\n        files = listdir(base_path + folder + \"/\")\n        image = imread(base_path + folder + \"/\" + files[0])\n        ax[m,n].imshow(image)\n        ax[m,n].grid(False)\n        ax[m,n].set_title(folder + \"/\" + str(m+n*4+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see clearly by the size of the stones that the **images of the plants were taken with different distances the the ground**. This procedure is **coupled to the growth state of the plant but also to the species** themselves. For example, the most images of Scentless Mayweed were taken at an early growth state where the camera was close to the stones. In contrast many images of black grass and maize were taken with a high distance to the ground to cover all parts of a single plant. **This leads to a unconscious target leakage as one can infer the the species type by looking at the size of the stones in the background**. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# How can we bypass the target leakage?\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can **get rid of the leakage by removing the pebble stones**. Is it important to scale down large plants of later growth states to small ones? In my opinion it's not. For each species we have several different kind of plants that are related to its growth state. Maize for example might look completely different at early growth states compared to later ones. This is tricky as we allow the appearance of our species to vary a lot within one class. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## About RGB, HSV and LAB\n\n### RGB\n\n* The RGB is an additive color space.\n* Creating any color by mixing the three primitive additives Red, Green and Blue.\n* A pixel with 3 channels contains information of (red, green, blue) light with values between 0 and 255. \n* Given all values equal to 0 maps to black, all values 255 maps to white.\n\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/03/RGB_farbwuerfel.jpg\" width=\"600px\"> \n<a href=\"http://creativecommons.org/licenses/by-sa/3.0/\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=13754\">Link</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## HSV\n\n* **Colors of each hue** follow the radius on the circle. It's values have an angular dimension starting with the red primary at 0°, passing the green primary at 120° and the blue primary at 240° and finally ending up with red again at 360°.\n* The **vertical axis describes the gray scale** ranging from black (0) to white (1). \n* The **horizontal axis covers the saturation** of the color that can be reduced by tinting with white from 1 to 0.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f1/HSV_cone.jpg\" width=\"300px\"> \n\nImage created by <a href=\"https://en.wikipedia.org/wiki/User:Wapcaplet\" class=\"extiw\" title=\"en:User:Wapcaplet\">Wapcaplet</a> - From en wiki, <a href=\"http://creativecommons.org/licenses/by-sa/3.0/\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=308191\">Link</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## LAB\n\n* The LAB colorspace consists of three values describing the lightness (L) as well as values A and B that describe a colorful plane. \n* Lightness has values from black (0) to white (100)\n* A holds colors from green to red. (−128 to 127)\n* B holds colors from blue to yellow. (−128 to 127)\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7d/CIELAB_color_space_front_view.png\" width=\"300px\"> \n\nImage created by <a href=\"//commons.wikimedia.org/wiki/User:Farbenprofi\" title=\"User:Farbenprofi\">Holger Everding</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=38366969\">Link</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Extracting the pebble stones \nFor our eyes it's easy to detect the plant by its green color but given RGB numerical values it's not. If we take a look at individual color channels we can see that many pebble stones have almost the same value as the plant. This problem could be even more worse for other species than Fat Hen. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"example_path = base_path + \"Sugar beet/27.png\" ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,ax = plt.subplots(3,3,figsize=(20,17))\n\ntitles = [[\"Red\", \"Green\", \"Blue\"],\n         [\"Hue\", \"Saturation\", \"Value\"],\n         [\"\\n lightness from black to white\", \"\\n A - from green to red\", \"\\n B - from blue to yellow\"]]\npil_image = Image.open(example_path)\nnp_image = np.array(pil_image)\nimage_hvs = cv2.cvtColor(np_image, cv2.COLOR_BGR2HSV)\nimage_lab = cv2.cvtColor(np_image, cv2.COLOR_BGR2LAB)\n\nfor n in range(3):\n    ax[0,n].imshow(np_image[:,:,n], cmap=\"RdYlGn\")\n    ax[0,n].grid(False)\n    ax[0,n].set_title(\"Sugar beet/27\" + \" - \" + titles[0][n]);\n    ax[1,n].imshow(image_hvs[:,:,n], cmap=\"RdYlGn\")\n    ax[1,n].set_title(\"Sugar beet/27\" + \" - \" + titles[1][n]);\n    ax[2,n].imshow(image_lab[:,:,n], cmap=\"RdYlGn\")\n    ax[2,n].set_title(\"Sugar beet/27\" + \" - \" + titles[2][n]);\nplt.savefig(\"Colorspace\", dpi=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the LAB colorspace the situation looks best! :-) Let's use the A-Channel to segment the plants. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Segmentation with thresholds\n\nWe have seen that the first channel of the LAB space that decribes the colors from green to red is sufficient to extract the plant information. This is good as we only need one threshold to compute a mask that allows us to perform the segmentation. After applying this concept we can see that there are still some stone speckels left. We can get rid of them by using a technique called [opening](https://en.wikipedia.org/wiki/Opening_%28morphology%29).\n\nThe method first sets a pixel ij to the minimum of its neighborhood pixels (in our case given by a disk with radius R centered at the pixel ij) followed by the opposite operation to the maximum of its neighborhood. This way we remove all small speckles and then restore the boundary values of the plants that have been set to the minimum as well. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"my_threshold = 121\nmy_radius = 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the mask by thresholding the A-Channel and fine-tune the mask by an opening:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mask(image, threshold, radius):\n    mask = np.where(image < threshold, 1, 0)\n    selem = disk(radius)\n    mask = closing(mask, selem)\n    return mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Multiply all channels by the mask to set the pebble stones to zero while retaining the plants:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def segment_plant(np_image, threshold, radius):\n    image_lab = cv2.cvtColor(np_image, cv2.COLOR_BGR2LAB)\n    mask = get_mask(image_lab[:,:,1], threshold, radius)\n    masked_image = np_image.copy()\n    for n in range(3):\n        masked_image[:,:,n] = np_image[:,:,n] * mask\n    return masked_image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here is how the result looks like:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nsns.distplot(image_lab[:,:,1].flatten(), ax=ax[0], kde=False)\nmask = get_mask(image_lab[:,:,1], my_threshold, my_radius)\nax[1].imshow(mask);\nax[2].imshow(segment_plant(np_image, my_threshold, my_radius))\nax[0].grid(False)\nax[1].grid(False)\nax[2].grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that a threshold of 120 is already sufficient to solve this task. Furthermore it seems to be a good choice for the other species as well:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4,6,figsize=(20,14))\n\nfor m in range(6):\n    folder = subfolders[m]\n    files = listdir(base_path + folder + \"/\")\n    image = np.array(Image.open(base_path + folder + \"/\" + files[0]))\n    ax[0,m].imshow(image)\n    ax[1,m].imshow(segment_plant(image, my_threshold, my_radius))\n    ax[0,m].grid(False)\n    ax[1,m].grid(False)\n    ax[0,m].set_title(folder + \"/\" + files[0])\n    \n    folder = subfolders[m+6]\n    files = listdir(base_path + folder + \"/\")\n    image = np.array(Image.open(base_path + folder + \"/\" + files[0]))\n    ax[2,m].imshow(image)\n    ax[3,m].imshow(segment_plant(image, my_threshold, my_radius))\n    ax[2,m].grid(False)\n    ax[3,m].grid(False)\n    ax[2,m].set_title(folder + \"/\" + files[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building up the workflow\n\nOk, let's start with the modelling part. As we need to perform a custom data preprocessing step I would like to work with pytorch. In my opinion it seems to offers a nice way to include your own preprocessing operations. \n\n## Preprocessing\n\n### Plant segmentation\n\nWe only need to write our segmentation as a callable class. This way we don't need to pass our parameters every time and we can include our method in a sequence of pytorch transform operations:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class SegmentPlant(object):\n    \n    def __call__(self, image):\n        np_image = np.array(image)\n        image = segment_plant(np_image, my_threshold, my_radius)\n        pil_image = Image.fromarray(image)\n        return pil_image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Further preprocessing steps\n\n* Besides the plant segmentation we need to resize our images to a common size. \n* As we have a few images that are not quadratic we also need to perform a crop around the image center where we usually expect the plant to be located. \n* In addition we should add further data augmentation techniques during training of our network. This includes random rotations, flips as well as random zooming.\n* The final step is given by an appropriate normalization of the images that suites to the pretrained model of our choice. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class RandomZoom(object):\n    \n    def __call__(self, image):\n        zoom_factor = np.random.uniform(0.7, 1.2)\n        height = image.size[0]\n        width = image.size[1]\n        new_size = (np.int(zoom_factor*height), np.int(zoom_factor*width))\n        return transforms.Resize(new_size)(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_transform(key=\"train\", plot=False):\n    train_sequence = [RandomZoom(),\n        transforms.Resize(size=256),\n            transforms.CenterCrop(224),\n            SegmentPlant(),\n            transforms.RandomAffine(30),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip()]\n    val_sequence = [transforms.Resize(size=256),\n            transforms.CenterCrop(224),\n            SegmentPlant()]\n    if plot==False:\n        train_sequence.extend([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        val_sequence.extend([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n        \n    data_transforms = {'train': transforms.Compose(train_sequence),'val': transforms.Compose(val_sequence)}\n    return data_transforms[key]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the preprocessed images without normalization:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,6,figsize=(20,11))\n\ntrain_transform = my_transform(key=\"train\", plot=True)\nval_transform = my_transform(key=\"val\", plot=True)\n\nfor m in range(6):\n    folder = subfolders[m]\n    files = listdir(base_path + folder + \"/\")\n    image = Image.open(base_path + folder + \"/\" + files[0])\n    ax[0,m].imshow(image)\n    transformed_img = train_transform(image)\n    ax[1,m].imshow(transformed_img)\n    ax[2,m].imshow(val_transform(image))\n    ax[0,m].grid(False)\n    ax[1,m].grid(False)\n    ax[2,m].grid(False)\n    ax[0,m].set_title(folder + \"/\" + files[0])\n    ax[1,m].set_title(\"Preprocessing for train\")\n    ax[2,m].set_title(\"Preprocessing for val\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks as expected! There is just resizing and segmentation for the validation data and further image augmentations like random zoom, rotations and cropping for the training data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dataset and Dataloader\n\nPytorch makes it really easy to prepare and load your data by writing your own dataset. In our case we use a pandas dataframe to extract the target and to load the related image given its name. Furthermore we pass our transform methods to transform with respect to \"val\" and \"train\":","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class SeedlingsDataset(Dataset):\n    \n    def __init__(self, root_dir, df, transform=None):\n        self.root_dir = root_dir\n        self.states = df\n        self.transform=transform\n      \n    def __len__(self):\n        return len(self.states)\n        \n    def __getitem__(self, idx):\n        image_path = self.root_dir + self.states.species.values[idx] + \"/\" \n        image_path += self.states.image_name.values[idx]\n        image = Image.open(image_path)\n        image = image.convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n         \n        target = self.states.target.values[idx]\n        return {\"image\": image, \"label\": target}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation strategy\n\nAs our dataset is small we like to go with a traditional split of our data into: 60% training data,20 % dev data and 20% test data. As we found class imbalance in our target distribution, we should stratify the split on our target data. This way we ensure that we have similar distributions in train, dev and test data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain the training data by splitting into 60% train and 40% for the next split\ntrain_idx, sub_test_idx = train_test_split(plantstate.index.values,\n                                           test_size=0.4,\n                                           random_state=2019,\n                                           stratify=plantstate.target.values)\n\n# Split the residual 40% into two parts (each 20% of the original data): \ndev_idx, test_idx = train_test_split(sub_test_idx,\n                                     test_size=0.5,\n                                     random_state=2019,\n                                     stratify=plantstate.loc[sub_test_idx, \"target\"].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to setup our datasets for each data chunk and pytorch dataloaders:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = plantstate.loc[train_idx].copy()\ndev_df = plantstate.loc[dev_idx].copy()\ntest_df = plantstate.loc[test_idx].copy()\n\ntrain_dataset = SeedlingsDataset(base_path, train_df, transform=my_transform(key=\"train\"))\ndev_dataset = SeedlingsDataset(base_path, dev_df, transform=my_transform(key=\"val\"))\ntest_dataset = SeedlingsDataset(base_path, test_df, transform=my_transform(key=\"val\"))\n\nimage_datasets = {\"train\": train_dataset, \"dev\": dev_dataset, \"test\": test_dataset}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"dev\", \"test\"]}\n\nprint(len(train_dataset), len(dev_dataset), len(test_dataset))\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ndev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n\ndataloaders = {\"train\": train_dataloader, \"dev\": dev_dataloader, \"test\": test_dataloader}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transfer learning\n\nWe will use a model that was already trained to classify a broad range of daily objects on the imagenet dataset to solve the new task to classify plant species. This method is called transfer learning and it provides many advantages over training from skretch: \n\n* The first layers of convolutional neural networks often extract basic features like edges, textures and abstract patters. Only the very depth layers extract information that can be related to parts and objects. If you like have a nice introduction to it, take a look at [feature visualizations](https://distill.pub/2017/feature-visualization/) of [Christopher Colahs Blog](https://colah.github.io/about.html).  \n* By only removing the last fully connected layer and building some new densely connected layers we still have enough parameters to train a new task on. Usually the last fully connected layers contain a large proportion of trainable parameters due to their high connectiviy. \n* As we have already trained parameters for the feature extraction we may be able to bypass the high demand for training data of neural networks that we would need to prevent overfitting. \n\nLet's go! :-)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def train_loop(model, criterion, optimizer, scheduler=None, num_epochs = 10, lam=0.0):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    loss_dict = {\"train\": [], \"dev\": [], \"test\": []}\n    lam_tensor = torch.tensor(lam, device=device)\n    \n    running_loss_dict = {\"train\": [], \"dev\": [], \"test\": []}\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        for phase in [\"train\", \"dev\", \"test\"]:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n            \n            tk0 = tqdm(dataloaders[phase], total=int(len(dataloaders[phase])))\n\n            counter = 0\n            for bi, d in enumerate(tk0):\n                inputs = d[\"image\"]\n                labels = d[\"label\"]\n                inputs = inputs.to(device, dtype=torch.float)\n                labels = labels.to(device, dtype=torch.long)\n                \n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                \n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        \n                        #l2_reg = torch.tensor(0., device=device)\n                        #for param in model.parameters():\n                            #l2_reg = lam_tensor * torch.norm(param)\n                        \n                        #loss += l2_reg\n            \n                        optimizer.step()\n                        \n                        \n                        \n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                if phase == 'train':\n                    if scheduler is not None:\n                        scheduler.step()\n                \n                counter += 1\n                tk0.set_postfix({'loss': running_loss / (counter * dataloaders[phase].batch_size),\n                                 'accuracy': running_corrects.double() / (counter*dataloaders[phase].batch_size)})\n                running_loss_dict[phase].append(running_loss / (counter * dataloaders[phase].batch_size))\n                \n            epoch_loss = running_loss / dataset_sizes[phase]\n            loss_dict[phase].append(epoch_loss)\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            # deep copy the model\n            if phase == 'dev' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n        print()\n    \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))              \n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, loss_dict, running_loss_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have added some pretrained models for pytorch. I have decided to start simple by working with resnet18 but other models may be more performant either to solve this task with respect to a desired evaluation metric or with respect to speed performance during inference.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input/pretrained-pytorch-models/\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the GPU is available we like to use it:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On top of resnet as a feature extractor it makes sense to add some more fully connected layers. Due to their high connectivity they hold a large portion of trainable parameters that allow the network to adapt to the new task of plant species recognition:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.resnet18(pretrained=False)\nif run_training:\n    model.load_state_dict(torch.load(\"../input/pretrained-pytorch-models/resnet18-5c106cde.pth\"))\nnum_features = model.fc.in_features\nprint(num_features)\n\nmodel.fc = nn.Sequential(\n    nn.Linear(num_features, 512),\n    nn.ReLU(),\n    nn.BatchNorm1d(512),\n    nn.Dropout(0.5),\n    \n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.BatchNorm1d(256),\n    nn.Dropout(0.5),\n    \n    nn.Linear(256, NUM_CLASSES))\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n\nmodel.apply(init_weights)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of target plant species shows a class imbalance. A simple approach to start with is by setting class weights that make seldom classes more relevant for the loss (and gradients) than common classes.   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = compute_class_weight(y=train_df.target.values, class_weight=\"balanced\", classes=train_df.target.unique())    \nclass_weights = torch.FloatTensor(weights)\nif device.type==\"cuda\":\n    class_weights = class_weights.cuda()\nprint(class_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In our case we can use a so called weighted cross entropy loss. The species with higher weights contribute more to our measurement of prediction errors. By minimizing this loss function we enforce our model to learn the appropriate parameters to solve our species classification task:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.SGD(model.fc.parameters(), lr=0.09, momentum=0.9)\nscheduler = CyclicLR(optimizer, base_lr=0.01, max_lr=0.09)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"if run_training:\n    model, loss_dict, running_loss_dict = train_loop(model, criterion, optimizer, scheduler=scheduler, num_epochs = 10)\n    \n    if device == \"cpu\":\n        OUTPUT_PATH += \".pth\"\n    else:\n        OUTPUT_PATH += \"_cuda.pth\"\n        \n    torch.save(model.state_dict(), OUTPUT_PATH)\n    \n    losses_df = pd.DataFrame(loss_dict[\"train\"],columns=[\"train\"])\n    losses_df.loc[:, \"dev\"] = loss_dict[\"dev\"]\n    losses_df.loc[:, \"test\"] = loss_dict[\"test\"]\n    losses_df.to_csv(\"losses_segmented_seedlings.csv\", index=False)\n    \n    running_losses_df = pd.DataFrame(running_loss_dict[\"train\"], columns=[\"train\"])\n    running_losses_df.loc[0:len(running_loss_dict[\"dev\"])-1, \"dev\"] = running_loss_dict[\"dev\"]\n    running_losses_df.loc[0:len(running_loss_dict[\"test\"])-1, \"test\"] = running_loss_dict[\"test\"]\n    running_losses_df.to_csv(\"running_losses_segmented_seedlings.csv\", index=False)\nelse:\n    if device == \"cpu\":\n        MODEL_PATH += \".pth\"\n    else:\n        MODEL_PATH += \"_cuda.pth\"\n    model.load_state_dict(torch.load(MODEL_PATH))\n    model.eval()\n    \n    losses_df = pd.read_csv(LOSSES_PATH + \"losses_segmented_seedlings.csv\")\n    running_losses_df = pd.read_csv(LOSSES_PATH + \"running_losses_segmented_seedlings.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results - How good can we classify plant species?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Loss convergence\n\nAfter training, we should take a look at the train, dev and test losses of our splitted dataset:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,1,figsize=(20,15))\n\nax[0].plot(running_losses_df[\"train\"], '-o', label=\"train\")\nax[0].set_xlabel(\"Step\")\nax[0].set_ylabel(\"Weighted x-entropy\")\nax[0].set_title(\"Loss change over steps\")\nax[0].legend();\n\nax[1].plot(running_losses_df[\"dev\"], '-o', label=\"dev\", color=\"orange\")\nax[1].set_xlabel(\"Step\")\nax[1].set_ylabel(\"Weighted x-entropy\")\nax[1].set_title(\"Loss change over steps\")\nax[1].legend();\n\nax[2].plot(running_losses_df[\"test\"], '-o', label=\"test\", color=\"mediumseagreen\")\nax[2].set_xlabel(\"Step\")\nax[2].set_ylabel(\"Weighted x-entropy\")\nax[2].set_title(\"Loss change over steps\")\nax[2].legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Each step belongs to one computed batch in this plot. \n* We can clearly see how the training loss converges and that it shows jumps after each epoch.\n* The same can be observed for the dev and test data.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(losses_df[\"train\"], '-o', label=\"train\")\nplt.plot(losses_df[\"dev\"], '-o', label=\"dev\")\nplt.plot(losses_df[\"test\"], '-o', label=\"test\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Weighted x-entropy\")\nplt.title(\"Loss change over epochs\");\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Looking at the loss per epoch, we can see that the model learns to make better predictions at least until epoch 5. After then the dev and test losses show a slight increase again. This could indicate overfitting but due to the fact that both losses decrease again it's not obvious and clear. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Predictions and confusion\n\nLet's take a look at some individual predictions and on the confusion matrix:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dev_predictions = pd.DataFrame(index = np.arange(0, dataset_sizes[\"dev\"]), columns = [\"true\", \"predicted\"])\ntest_predictions = pd.DataFrame(index = np.arange(0, dataset_sizes[\"test\"]), columns = [\"true\", \"predicted\"])\n\nplt.ion()\n\ndef imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\ndef evaluate_model(model, predictions_df, key):\n    was_training = model.training\n    model.eval()\n\n    with torch.no_grad():\n        for i, data in enumerate(dataloaders[key]):\n            inputs = data[\"image\"].to(device)\n            labels = data[\"label\"].to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"true\"] = data[\"label\"].numpy().astype(np.int)\n            predictions_df.loc[i*BATCH_SIZE:(i+1)*BATCH_SIZE-1, \"predicted\"] = preds.cpu().numpy().astype(np.int)\n    predictions_df = predictions_df.dropna()\n    return predictions_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks fine. Let's go into more details!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Dev data","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nspecies_map = {0: \"Black-grass\",\n               1: \"Charlock\",\n               2: \"Cleavers\",\n               3: \"Common Chickweed\",\n               4: \"Common wheat\",\n               5: \"Fat Hen\",\n               6: \"Loose Silky-bent\",\n               7: \"Maize\",\n               8: \"Scentless Mayweed\",\n               9: \"Shepherd's Purse\",\n               10: \"Small-flowered Cranesbill\",\n               11: \"Sugar beet\"}\n\ndev_predictions = evaluate_model(model, dev_predictions, \"dev\")\ndev_predictions.loc[:,\"true\"] = dev_predictions.loc[:, \"true\"].astype(np.int)\ndev_predictions.loc[:, \"predicted\"] = dev_predictions.loc[:, \"predicted\"].astype(np.int)\ndev_predictions.loc[:, \"true species\"] = dev_predictions.loc[:, \"true\"].map(species_map)\ndev_predictions.loc[:, \"predicted species\"] = dev_predictions.loc[:, \"predicted\"].map(species_map)\ndev_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(dev_predictions.true.values, dev_predictions.predicted.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We were able to classify 87 % of the classes in the dev data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Test data","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test_predictions = evaluate_model(model, test_predictions, \"test\")\ntest_predictions.loc[:,\"true\"] = test_predictions.loc[:, \"true\"].astype(np.int)\ntest_predictions.loc[:, \"predicted\"] = test_predictions.loc[:, \"predicted\"].astype(np.int)\ntest_predictions.loc[:, \"true species\"] = test_predictions.loc[:, \"true\"].map(species_map)\ntest_predictions.loc[:, \"predicted species\"] = test_predictions.loc[:, \"predicted\"].map(species_map)\ntest_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(test_predictions.true.values, test_predictions.predicted.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With close to 85 % this accuracy of the test data is similar to the dev data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Single accuracies and confusion matrix\n\nTo improve the model and the workflow we need to take a look at single acurracy scores and on the confusion matrix:","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"single_accuracies = []\nnames = []\n\nfor key, val in species_map.items():\n    y_pred = dev_predictions[dev_predictions.true==key].predicted.values\n    y_true = dev_predictions[dev_predictions.true==key].true.values\n    \n    single_accuracies.append(np.int(accuracy_score(y_true,y_pred)*100))\n    names.append(val)\n\nplt.figure(figsize=(20,5))\nsns.barplot(x=names, y=single_accuracies)\nplt.xticks(rotation=90);\nplt.ylabel(\"%\")\nplt.title(\"Single accuracies of plant species classes \\n belonging to the dev data\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* We failed to predict black-grass correctly. Furthermore we observe lower accuracies for common wheat and Shepherd's Purse. \n* Luckily we also observe a lot of plants with high accuracy scores > 95 %. \n* To significantly improve our model we need to find out why black-grass was misclassified that often and why some other plants show low scores.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dev_confusion = confusion_matrix(dev_predictions[\"true species\"].values, dev_predictions[\"predicted species\"].values)\ntest_confusion = confusion_matrix(test_predictions[\"true species\"].values, test_predictions[\"predicted species\"].values)\n\nfig, ax = plt.subplots(1,2,figsize=(25,10))\nsns.heatmap(dev_confusion, annot=True, cmap=\"Oranges\", square=True, cbar=False, linewidths=1, ax=ax[0]);\nsns.heatmap(test_confusion, annot=True, cmap=\"Greens\", square=True, cbar=False, linewidths=1, ax=ax[1]);\nax[0].set_title(\"Confusion matrix of Dev-data\");\nax[0].set_xticklabels([name for val, name in species_map.items()], rotation=90)\nax[0].set_yticklabels([name for val, name in species_map.items()], rotation=45)\nax[0].set_xlabel(\"predicted\")\nax[0].set_ylabel(\"true\")\nax[1].set_title(\"Confusion matrix of Test-data\");\nax[1].set_xticklabels([name for val, name in species_map.items()], rotation=90)\nax[1].set_yticklabels([name for val, name in species_map.items()], rotation=45);\nax[1].set_xlabel(\"predicted\")\nax[1].set_ylabel(\"true\");\nplt.savefig(\"Confusion\", dpi=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* Most of the time black-grass is misclassified as loose-silky-bent.\n* All other classes do not show a major class that occurs as a misclassification counterpart. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ignoring the black-grass/loose-silky-bent classes, how good is the score?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(dev_predictions[dev_predictions.true.isin([6,0])==False].true.values,\n               dev_predictions[dev_predictions.true.isin([6,0])==False].predicted.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Why is black-grass misclassified and can we solve the problem easily?\n\nLet's take a look again at the plants and how they are prepared for training:","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4,6,figsize=(20,14))\n\nfor m in range(6):\n    folder = subfolders[m]\n    files = listdir(base_path + folder + \"/\")\n    image = np.array(Image.open(base_path + folder + \"/\" + files[0]))\n    ax[0,m].imshow(image)\n    ax[1,m].imshow(segment_plant(image, my_threshold, my_radius))\n    ax[0,m].grid(False)\n    ax[1,m].grid(False)\n    ax[0,m].set_title(folder + \"/\" + files[0])\n    \n    folder = subfolders[m+6]\n    files = listdir(base_path + folder + \"/\")\n    image = np.array(Image.open(base_path + folder + \"/\" + files[0]))\n    ax[2,m].imshow(image)\n    ax[3,m].imshow(segment_plant(image, my_threshold, my_radius))\n    ax[2,m].grid(False)\n    ax[3,m].grid(False)\n    ax[2,m].set_title(folder + \"/\" + files[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights on How-To-Improve\n\n* Black-grass, losse-silky bent and common wheat look very similar after segmentation.\n* As we used random zoom as image augmentation technique it might occur that the classification challenge had become even harder.The size of the plants could be an important feature that we have destroyed this way. Perhaps black-grass always looks a bit more narrow than loose-silky-bent. Then we could improve by removing the zoom effects in image augmentations. \n* Furthermore we have several artifacts on the leave edges that are related to image closing and plant segmentation. To improve the classification for all plants, this step has to be reworked.\n* We should add Gaussian blurr as image augmentation technique as the image resolution differs from image to image and from growth state to growth state.\n\nAfter improving the model by adjusting image preprocessing and augmentation one should play with model complexity and the chosen number of iterations as well as the learning rate. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nThe analysis showed how deep learning can be of great help to recognize plant species. It covered the whole cycle from data exploration, preprocessing, feature engineering, machine learning modelling and result analysis. The workflow acts as a baseline that can be improved iteratively by repeating the different steps and adjusting single aspects that are likely to improve the results. \n\nIf you like to build upon it - just fork! ;-)\n\nHappy kaggling! ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}