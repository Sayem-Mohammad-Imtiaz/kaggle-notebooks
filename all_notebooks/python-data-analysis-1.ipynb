{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f3f61d04-2b0d-7d96-6298-512201416171"},"source":"I want to find evergreen topics vs topics that are shortlived"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99aea969-98f3-396a-59b7-1931592ca1ea"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d7e440e-1ba4-fd82-eafb-1e21fda67403"},"outputs":[],"source":"# Imports\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\ndata=pd.read_csv(\"../input/HN_posts_year_to_Sep_26_2016.csv\")\n\n# We will be converting the created_at column to hour, minutes etc\ndata['created_at']=data['created_at'].map(lambda x:datetime.strptime(x,'%m/%d/%Y %H:%M'))\ndata['hour']=data['created_at'].map(lambda x:x.hour)\ndata['year']=data['created_at'].map(lambda x:x.year)\ndata['month']=data['created_at'].map(lambda x:x.month)\ndata['title']=data['title'].map(lambda x:x.lower())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbdaf299-b4a3-e114-f551-cdca97546cc8"},"outputs":[],"source":"### Simple Plots\n# Plot for Hourly distribution. We will loop for plotting it for 12 months\n\nplt.figure()\nmonthLabel=['Jan','Feb','Mar','Apr','May','June','July','Aug','Sep','Oct','Nov','Dec']\nfor monthVal in range(1,13):\n    data[data['month']==monthVal].groupby(['hour'])['id'].count().plot(figsize=(10,10),label=monthLabel[monthVal-1])\nplt.xlabel(\"Hour of the day\")\nplt.ylabel(\"Number of Posts\")\nplt.legend()\nplt.show()\n# We can see that there is definitely a trend\n# But how does it relate to our problem statement?\n# I think that topics initiated during the peak hours is likely to get more traction that topics initiated during off hours\n# To verify this, let us plot comments. It should follow the same pattern\n\nplt.figure()\nfor monthVal in range(1,13):\n    data[data['month']==monthVal].groupby(['hour'])['num_comments'].sum().plot(figsize=(10,10),label=monthLabel[monthVal-1])\nplt.xlabel(\"Hour of the day\")\nplt.ylabel(\"Number of Comments\")\nplt.legend()\nplt.show()\n\n# I think that topics initiated during the peak hours is likely to get more traction that topics initiated during off hours\n# To verify this, let us plot comments. It should follow the same pattern\n\nplt.figure()\nfor monthVal in range(1,13):\n    data[data['month']==monthVal].groupby(['hour'])['num_points'].sum().plot(figsize=(10,10),label=monthLabel[monthVal-1])\nplt.xlabel(\"Hour of the day\")\nplt.ylabel(\"Number of Upvotes\")\nplt.legend()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bd72bc1-d26c-0c1e-40fd-f3a50ad92328"},"outputs":[],"source":"# Translating each row into a TOPIC\n# This will be challenging\n\n# COUNT VECTORIZER\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\ntitleCounts = count_vect.fit_transform(data['title'])\ntitleCounts.shape\n# 80076 features\n\n# The number of unique words created is 80076. Not that helpful\n# We need to do a lot of cleaning before this\n# Removing StopWords\n# Lemmatization\n# Stemming\n\n# We need to do some straight away replacements\ncharsToBeReplaced=['.', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']\ndef replaceText(Text,CharsToBeReplaced):\n    for x in CharsToBeReplaced:\n        Text=Text.replace(x,\"\")\n    return Text\n\ndata['CleanedTitle']=data['title'].map(lambda x:replaceText(x,charsToBeReplaced))\n\n\n# Let us use NLTK to remove the Stop Words and Stem and Lemmatize the data\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nporter_stemmer = PorterStemmer()\ndata['CleanedTitle']=data['CleanedTitle'].map(lambda z: ' '.join([wordnet_lemmatizer.lemmatize(porter_stemmer.stem(x)) for x in z.split(' ') if x not in stop_words]))\n\n# We will now work on this data set\ntitleCounts=count_vect.fit_transform(data['CleanedTitle'])\n# 71623 features. Not much reduction, but something is better than nothing\n#from sklearn.feature_extraction.text import TfidfTransformer\n#transformer=TfidfTransformer(smooth_idf=False)\n#tfidf = transformer.fit_transform(titleCounts)\n#tfidf.toarray() \n\n# This caused some memory error, so let us try something else\n# Let us analyse at a month level at least"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66e37bca-7d4c-8f88-f99b-75999e7cf6d4"},"outputs":[],"source":"# We are choosing a smaller data set to avoid memory issues\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# We will now be running this for all the months and then take a weighted average\nTotalCoefficientsWeight=pd.DataFrame()\n\n#transformer=TfidfTransformer(smooth_idf=False)\n#titleCounts=count_vect.fit_transform(data[(data['month']==1) & (data['year']==2016)]['CleanedTitle'])\n#tfidf = transformer.fit_transform(titleCounts)\ndef analysisMonthYear(Month,Year,ngramLength,evaluationColumn):\n    if len(data[(data['month']==Month) & (data['year']==Year)]['CleanedTitle']) >0:\n        vectorizer=TfidfVectorizer(max_df=0.95, min_df=2,max_features=100,stop_words='english',ngram_range=(1,ngramLength))\n        tfidf_vect=vectorizer.fit_transform(data[(data['month']==Month) & (data['year']==Year)]['CleanedTitle'])\n        topFeatures=vectorizer.get_feature_names()\n        # This was for debugging\n        #print(\"Year is \" + str(Year) + \" Month is \" + str(Month))\n        # This is very time consuming, so i am commenting it for now\n        #topFeaturesCount=[sum([1 if f1 in text1.split(' ') else 0 for text1 in data['CleanedTitle']]) for f1 in topFeatures]\n\n        # Now we can plot the frequency of posts having these words\n        # e.g. object\n\n        # We will now be creating a matrix X, that consists of the word vectors( only the important features)\n        # The correponding Y matrix will be the respective counts and we will then optimize it\n        # We might have to select fewer features in the beginning to check our logic. LEt us being with 100 features\n        # So the matrix will be numberOfRows X 100\n    \n        # Running the count vectorizer\n        from sklearn.feature_extraction.text import CountVectorizer\n        tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,max_features=100,vocabulary=topFeatures,ngram_range=(1,ngramLength))\n        tf = tf_vectorizer.fit_transform(data[(data['month']==Month) & (data['year']==Year)]['CleanedTitle'])\n\n        # Now that we have got our features, let us perform some linear algebra functions to extract the important relations\n        #AX=Y\n\n        # Y is the comment count for each row of MONTH=1 and YEAR=2016\n        # A is the TF matrix\n        # Let us find the X, which will tell us the weightage of each word\n        from scipy.sparse.linalg import lsqr\n        x = lsqr(tf, data[(data['month']==Month) & (data['year']==Year)][evaluationColumn])\n\n        # Since we have an asymmetric matrix, we will have to take help of LEast Squares Solution\n\n        # The following is now the final coefficients\n        FinalCoefficients=[[topFeatures[i],x[0][i]] for i in range(0,len(topFeatures))]\n        FinalCoefficients=pd.DataFrame(FinalCoefficients)\n        FinalCoefficients.columns=['Word','Weight']\n        return FinalCoefficients\n    else:\n        return pd.DataFrame(columns=['Word','Weight'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3927db6c-3f1e-873d-5823-25e4ae772b23"},"outputs":[],"source":"# Temporary Script to create a wordcloud\n# Now we will create a wordCloud of the top 30 words\nfrom wordcloud import WordCloud\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport matplotlib \nmatplotlib.rc('xtick', labelsize=20) \n# We will now be creating a weighted list\nFinalCoefficients=pd.DataFrame()\nfor year in data['year'].unique():\n    for month in data['month'].unique():\n        global FinalCoefficients\n        tempCoefficients=analysisMonthYear(month,year,1,'num_comments')\n        FinalCoefficients=FinalCoefficients.append(tempCoefficients)\n\n# Sum up across months\ndf=pd.DataFrame(columns=['Word','Values'])\ndf['Values']=FinalCoefficients.groupby(['Word'])['Weight'].sum().order(ascending=0)[1:30].values\ndf['Word']=FinalCoefficients.groupby(['Word'])['Weight'].sum().order(ascending=0)[1:30].index.values\n\n# Create the word cloud\nwordcloud = WordCloud().generate_from_frequencies([(row['Word'],row['Values']) for index, row in df.iterrows()])\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.suptitle(\"Important words in the data by Weight based on Num Comments\",fontsize=20)\nplt.show()\n\n# WordCloud is just fancy, we want a bar chart\nfig=plt.figure(figsize=(20,10))\ny_pos = np.arange(len(df['Word']))\nplt.bar(y_pos,df['Values'])\nplt.xticks(y_pos, df['Word'],rotation=90)\nplt.suptitle(\"Important words in the data by Weight based on Num Comments\",fontsize=20)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3007ed9b-0814-5ab4-aaeb-5b5161073c41"},"outputs":[],"source":"## ANALYSIS 2 ###\n# We will now create a timeline of the most important words\ndf=pd.DataFrame(columns=['Word','Values'])\ndf['Values']=FinalCoefficients.groupby(['Word'])['Weight'].count().order(ascending=0)[1:30].values\ndf['Word']=FinalCoefficients.groupby(['Word'])['Weight'].count().order(ascending=0)[1:30].index.values\n\n# We will now create a plot based on its presence in each month's list of most important words\nplt.figure(figsize=(20,10))\ny_pos = np.arange(len(df['Word']))\nplt.bar(y_pos,df['Values'])\nplt.xticks(y_pos, df['Word'],rotation=90)\nplt.suptitle(\"Important words in the data by count based on Num comments\",fontsize=20)\nplt.show()\n\n# We can see that there is a lot of difference in the words that came here and the last bar plot\n# This is because our TFID and LS on the matrix is being done over a month. We need to do it over the entire data set\n# to get the complete picture\n\n# So we can conclude that the above chart conains words which attract maximum responses"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd81902b-83ee-f932-6bf5-0674d90b7811"},"outputs":[],"source":"# Let us now do the analysis for upvotes\n# We will now be creating a weighted list\nFinalCoefficients=pd.DataFrame()\nfor year in data['year'].unique():\n    for month in data['month'].unique():\n        global FinalCoefficients\n        tempCoefficients=analysisMonthYear(month,year,1,'num_points')\n        FinalCoefficients=FinalCoefficients.append(tempCoefficients)\n\n# Sum up across months\ndf=pd.DataFrame(columns=['Word','Values'])\ndf['Values']=FinalCoefficients.groupby(['Word'])['Weight'].count().order(ascending=0)[1:30].values\ndf['Word']=FinalCoefficients.groupby(['Word'])['Weight'].count().order(ascending=0)[1:30].index.values\n\n\n# WordCloud is just fancy, we want a bar chart\nplt.figure(figsize=(20,10))\ny_pos = np.arange(len(df['Word']))\nplt.bar(y_pos,df['Values'])\nplt.xticks(y_pos, df['Word'],rotation=90)\nplt.suptitle(\"Important words in the data by count based on Num Upvotes\",fontsize=20)\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}