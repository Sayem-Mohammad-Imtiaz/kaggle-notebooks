{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting whether a person will default on their premium\n\nImporting necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Wrangling \nimport numpy as np\nimport pandas as pd \n\n# Data Visualisation \n%matplotlib inline\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Machine Learning \nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron \nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read data into dataframes "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train (1).csv')\ntest_data = pd.read_csv('../input/test (1).csv')\ncombine = [train_data, test_data]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data.describe(percentiles = [.08, .07, .06])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference**\n\n* 93% of the people have paid their premiums. \n* The age of people is very varied between 21 and 103"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 6))\nsns.heatmap(train_data.corr(), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Wrangling "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in combine: \n    dataset['age'] = dataset['age_in_days']//365\n    dataset.drop(['age_in_days'], axis = 1, inplace = True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[['sourcing_channel', 'target']].groupby('sourcing_channel', as_index = False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's try and fill the missing values\n\n#### Application Under writing score"},{"metadata":{},"cell_type":"markdown","source":"We might need to make income groups to understand the relations better "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['IncomeBands'] = pd.cut(train_data['Income'], 5)\ntrain_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's standardize our data by using a standard scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = MinMaxScaler()\nscaler = scaler.fit(train_data[['Income']])\nx_scaled = scaler.transform(train_data[['Income']])\nx_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(scaler.mean_)\nprint(scaler.scale_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['scaled_income'] = x_scaled\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['IncomeBands'] = pd.cut(train_data['scaled_income'], 5)\ntrain_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's try and deal with outlier values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['Income'].mean())\nprint(train_data['Income'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train_data['Income'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_bound = 0.95\nlower_bound = 0.1\nres = train_data['Income'].quantile([lower_bound, upper_bound])\nprint(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we can collect all the values in this range and let go of the other ones. "},{"metadata":{"trusted":true},"cell_type":"code","source":"true_index = (train_data['Income'] < res.loc[upper_bound])\ntrue_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"false_index = ~true_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_outlier_data = train_data[true_index].copy()\nno_outlier_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make groups for the new income range"},{"metadata":{"trusted":true},"cell_type":"code","source":"no_outlier_data['IncomeBands'] = pd.cut(no_outlier_data['Income'], 5)\nno_outlier_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset.loc[ dataset['Income'] <= 23603.99, 'Income'] = 0\n    dataset.loc[(dataset['Income'] > 23603.99) & (dataset['Income'] <= 109232.0), 'Income'] = 1\n    dataset.loc[(dataset['Income'] > 109232.0) & (dataset['Income'] <= 194434.0), 'Income'] = 2\n    dataset.loc[(dataset['Income'] > 194434.0) & (dataset['Income'] <= 279636.0), 'Income'] = 3\n    dataset.loc[(dataset['Income'] > 279636.0) & (dataset['Income'] <= 364838.0), 'Income'] = 4\n    dataset.loc[(dataset['Income'] > 364838.0) & (dataset['Income'] <= 450040.0), 'Income'] = 5\n    dataset.loc[ dataset['Income'] > 450040.0, 'Income'] = 6\n    \ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.loc[false_index, 'Income'] = 5\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(['IncomeBands', 'scaled_income'], axis = 1, inplace = True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's also make groups for Age**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['AgeBands'] = pd.cut(train_data['age'], 5)\ntrain_data[['AgeBands', 'target']].groupby('AgeBands', as_index = False).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in combine:    \n    dataset.loc[ dataset['age'] <= 37.4, 'age'] = 0\n    dataset.loc[(dataset['age'] > 37.4) & (dataset['age'] <= 53.8), 'age'] = 1\n    dataset.loc[(dataset['age'] > 53.8) & (dataset['age'] <= 70.2), 'age'] = 2\n    dataset.loc[(dataset['age'] > 70.2) & (dataset['age'] <= 86.6), 'age'] = 3\n    dataset.loc[ dataset['age'] > 86.6, 'age'] = 4\ntrain_data.drop('AgeBands', axis = 1, inplace = True)\ncombine = [train_data, test_data]\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[['age', 'application_underwriting_score']].groupby('age').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['PremBand'] = pd.cut(train_data['no_of_premiums_paid'], 5)\ntrain_data[['PremBand', 'application_underwriting_score']].groupby('PremBand').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['application_underwriting_score'].mean())\nprint(train_data['application_underwriting_score'].std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data[train_data['sourcing_channel'] == 'A']['application_underwriting_score'].std())\ntrain_data[['sourcing_channel', 'target']].groupby('sourcing_channel', as_index = False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(train_data[train_data['sourcing_channel'] == 'C']['application_underwriting_score'].std())\ntrain_data[['sourcing_channel', 'application_underwriting_score']].groupby('sourcing_channel', as_index = False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[['residence_area_type', 'application_underwriting_score']].groupby('residence_area_type', as_index = False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can set the values of underwriting score on the basis of the sourcing channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = [train_data, test_data]\nfor dataset in combine: \n    mask1 = dataset['application_underwriting_score'].isnull()\n    for source in ['A', 'B', 'C', 'D', 'E']:\n        mask2 = (dataset['sourcing_channel'] == source)\n        source_mean = dataset[dataset['sourcing_channel'] == source]['application_underwriting_score'].mean()\n        dataset.loc[mask1 & mask2, 'application_underwriting_score'] = source_mean\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['application_underwriting_score'].isnull()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test_data[test_data['Count_3-6_months_late'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Add  a new variable 'late premium' for late premiums"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'Count_3-6_months_late', data = train_data, hue = 'target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'Count_6-12_months_late', data = train_data, hue = 'target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset['late_premium'] = 0.0\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = [train_data, test_data]\nfor dataset in combine:\n        dataset.loc[(dataset['Count_3-6_months_late'].isnull()),  'late_premium'] = np.NaN\n        dataset.loc[(dataset['Count_3-6_months_late'].notnull()), 'late_premium'] = dataset['Count_3-6_months_late'] + dataset['Count_6-12_months_late'] + dataset['Count_more_than_12_months_late']\ntrain_data.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['target'].corr(train_data['late_premium'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15, 6))\nsns.heatmap(test_data.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x = 'perc_premium_paid_by_cash_credit', y = 'late_premium', data = train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = 'late_premium', data = train_data, hue = 'target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If it's more than 7 then the loan is never sanctioned. So, let's set those values first. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[['late_premium', 'target']].groupby('late_premium').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for dataset in [train_data]:\ntrain_data.loc[(train_data['target'] == 0) & (train_data['late_premium'].isnull()),'late_premium'] = 7\ntrain_data.loc[(train_data['target'] == 1) & (train_data['late_premium'].isnull()),'late_premium'] = 2\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.isnull().sum())\nprint('\\n')\nprint(test_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing the late_premium value in the test data "},{"metadata":{"trusted":true},"cell_type":"code","source":"guess_prem = np.zeros(5)\nfor dataset in [test_data]:\n    for i in range(1, 6):\n        guess_df = dataset[(dataset['Income'] == i)]['late_premium'].dropna()\n\n        # age_mean = guess_df.mean()\n        # age_std = guess_df.std()\n        # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n        premium_guess = guess_df.median()\n        guess_prem[i - 1] = int(premium_guess) \n\n    for j in range(1, 6):\n        dataset.loc[(dataset.late_premium.isnull()) & (dataset.Income == j), 'late_premium'] = guess_prem[j - 1] + 1\n\n    dataset['late_premium'] = dataset['late_premium'].astype(int)\n\ntest_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(['Count_3-6_months_late', 'Count_6-12_months_late', 'Count_more_than_12_months_late'], axis = 1, inplace = True)\ntest_data.drop(['Count_3-6_months_late', 'Count_6-12_months_late', 'Count_more_than_12_months_late'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conversion to numerical data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting Area Type and sourcing channel to Ordinal Variables\ncombine = [train_data, test_data]\nfor dataset in combine: \n    dataset['residence_area_type'] = dataset['residence_area_type'].map( {'Urban' : 1, 'Rural' : 0} )\n    dataset['sourcing_channel'] = dataset['sourcing_channel'].map( {'A' : 0, 'B' : 1, 'C' : 2, 'D' : 3, 'E' : 4} )\ntrain_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further conversions"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['application_underwriting_score'] = train_data['application_underwriting_score']/100\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's also work on no of premiums paid"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_bound = 0.95\nres = train_data['no_of_premiums_paid'].quantile([upper_bound])\nprint(res)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_index = train_data['no_of_premiums_paid'] < res.loc[upper_bound]\nfalse_index = ~true_index\ntrue_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['PremBand'] = pd.cut(train_data[true_index]['no_of_premiums_paid'], 4)\ntrain_data[['PremBand', 'application_underwriting_score']].groupby('PremBand').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine = [train_data, test_data]\n# for dataset in combine: \n#     dataset.loc[ dataset['no_of_premiums_paid'] <= 6.25, 'no_of_premiums_paid'] = 0\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 6.25) & (dataset['no_of_premiums_paid'] <= 10.5), 'no_of_premiums_paid'] = 1\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 10.50) & (dataset['no_of_premiums_paid'] <= 14.75), 'no_of_premiums_paid'] = 2\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 14.75) & (dataset['no_of_premiums_paid'] <= 19.0), 'no_of_premiums_paid'] = 3\n#     dataset.loc[ dataset['no_of_premiums_paid'] > 19.0, 'no_of_premiums_paid'] = 4\n    \n# train_data.drop('PremBand', axis = 1, inplace = True)\n# train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We also need to convert the premium column"},{"metadata":{"trusted":true},"cell_type":"code","source":"upper_bound = 0.90\nres = train_data['premium'].quantile([upper_bound])\nprint(res)\ntrue_index = train_data['premium'] < res.loc[upper_bound]\nfalse_index = ~true_index\ntrue_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['PremBand'] = pd.cut(train_data[true_index]['premium'], 4)\ntrain_data[['PremBand', 'target']].groupby('PremBand').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = [train_data]\nfor dataset in combine: \n    dataset.loc[ dataset['premium'] <= 5925.0, 'premium'] = 0\n    dataset.loc[(dataset['premium'] > 5925.00) & (dataset['premium'] <= 10650.0), 'premium'] = 1\n    dataset.loc[(dataset['premium'] > 10650.0) & (dataset['premium'] <= 15375.0), 'premium'] = 2\n    dataset.loc[(dataset['premium'] > 15375.0) & (dataset['premium'] <= 201200.0), 'premium'] = 3\n    dataset.loc[ dataset['premium'] > 201200.0, 'premium'] = 4\ntrain_data.drop('PremBand', axis = 1, inplace = True)\ntrain_data.head()\ncombine = [train_data, test_data]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally convert percentage premium paid"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['PremBand'] = pd.cut(train_data['perc_premium_paid_by_cash_credit'], 4)\ntrain_data[['PremBand', 'target']].groupby('PremBand').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset.loc[ dataset['perc_premium_paid_by_cash_credit'] <= 0.25, 'perc_premium_paid_by_cash_credit'] = 0\n    dataset.loc[(dataset['perc_premium_paid_by_cash_credit'] > 0.25) & (dataset['perc_premium_paid_by_cash_credit'] <= 0.5), 'perc_premium_paid_by_cash_credit'] = 1\n    dataset.loc[(dataset['perc_premium_paid_by_cash_credit'] > 0.5) & (dataset['perc_premium_paid_by_cash_credit'] <= 0.75), 'perc_premium_paid_by_cash_credit'] = 2\n    dataset.loc[ dataset['perc_premium_paid_by_cash_credit'] > 0.75, 'perc_premium_paid_by_cash_credit'] = 3\ntrain_data.drop('PremBand', axis = 1, inplace = True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[['perc_premium_paid_by_cash_credit', 'late_premium']] = train_data[['perc_premium_paid_by_cash_credit', 'late_premium']].astype(int)\ntest_data[['perc_premium_paid_by_cash_credit']] = test_data[['perc_premium_paid_by_cash_credit']].astype(int)\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building our models"},{"metadata":{},"cell_type":"markdown","source":"Let's make the data splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_data.drop(['id', 'target', 'premium', 'perc_premium_paid_by_cash_credit'], axis = 1).copy()\ny_train = train_data['target']\nX_test = test_data.drop(['id', 'perc_premium_paid_by_cash_credit'], axis = 1).copy()\nprint(X_train.shape, y_train.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Oversampling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nprint('Number of positive and negative reviews:\\n',y_train.value_counts())\nsm = SMOTE(random_state=0,ratio=1.0)\nX_train_res,y_train_res = sm.fit_sample(X_train,y_train)\nprint('Shape after oversampling\\n',X_train_res.shape) \nprint('Equal 1s and 0s \\n', np.bincount(y_train_res))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Without Oversampling\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ncm = confusion_matrix(logreg.predict(X_train),y_train)\nprint(cm)\n\nprint(classification_report(logreg.predict(X_train),y_train))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With Oversampling\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train_res, y_train_res)\ny_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train_res, y_train_res) * 100, 2)\nacc_log\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ncm = confusion_matrix(logreg.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(logreg.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_data = pd.DataFrame(train_data.columns.delete(0))\ncoeff_data.columns = ['Feature']\ncoeff_data['Correlation'] = pd.Series(logreg.coef_[0])\ncoeff_data.sort_values(by = 'Correlation', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes ~ Without oversampling\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes ~ With oversampling\n\ngaussian = GaussianNB()\ngaussian.fit(X_train_res, y_train_res)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_gaussian)\n\ncm = confusion_matrix(gaussian.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(gaussian.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k - Nearest Neighbours"},{"metadata":{"trusted":true},"cell_type":"code","source":"# without oversampling\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with oversampling\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train_res, y_train_res)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train_res, y_train_res) * 100, 2)\n\ncm = confusion_matrix(knn.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(knn.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Perceptron Algorithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nacc_perceptron","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perceptron - with oversampling\n\nperceptron = Perceptron()\nperceptron.fit(X_train_res, y_train_res)\ny_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train_res, y_train_res) * 100, 2)\nacc_perceptron\n\ncm = confusion_matrix(perceptron.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(perceptron.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stochastic Gradient Descent - with oversampling\n\nsgd = SGDClassifier()\nsgd.fit(X_train_res, y_train_res)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_sgd)\n\ncm = confusion_matrix(sgd.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(sgd.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(max_depth = 7)\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree - oversampling\n\ndecision_tree = DecisionTreeClassifier(max_depth = 7)\ndecision_tree.fit(X_train_res, y_train_res)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_decision_tree)\n\n\ncm = confusion_matrix(decision_tree.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(decision_tree.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators = 10)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest - oversampling\n\nrandom_forest = RandomForestClassifier(n_estimators = 10)\nrandom_forest.fit(X_train_res, y_train_res)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train_res, y_train_res)\nacc_random_forest = round(random_forest.score(X_train_res, y_train_res) * 100, 2)\nprint(acc_random_forest)\n\ncm = confusion_matrix(random_forest.predict(X_train_res),y_train_res)\nprint(cm)\n\nprint(classification_report(random_forest.predict(X_train_res),y_train_res))\n\ntnr = np.round(cm[0][0]/(cm[0][0] + cm[1][0]) * 100,3)\ntpr = np.round(cm[1][1]/(cm[1][1] + cm[0][1]) * 100,3)\nfpr = np.round(cm[1][0] / (cm[1][0] + cm[0][0]) * 100,3)\nprint('TPR = ',tpr,'%')\nprint('TNR = ',tnr,'%')\nprint('FPR = ',fpr,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning ~ KNN"},{"metadata":{},"cell_type":"markdown","source":"\n#### Run it when you have time\n\nfrom sklearn.model_selection import GridSearchCV\nknn = KNeighborsClassifier()\np = list(range(1,100,3))\nparameters = {'n_neighbors':p}\nclf = GridSearchCV(knn,param_grid = parameters, scoring = 'roc_auc', cv=10, return_train_score = True)\nclf.fit(X_train_res,y_train_res)\n\ntrain_auc_error = [1 - x for x in clf.cv_reslts_['mean_train_score']]\ntrain_auc_std = np.std(train_auc_error)\n\ntest_auc_error = [1 - x for x in clf.cv_results_['mean_test_score']]\ntest_auc_std = np.std(test_auc_error)\n\nplt.plot(p,train_auc_error,label = 'Train AUC',color = 'orange')\nplt.gca().fill_between(parameters,train_auc_error-train_auc_std,train_auc_error+train_auc_std,color= 'orange')\n\nplt.plot(p,test_auc_error,label = 'Test AUC', color = 'darkblue')\nplt.gca().fill_between(parameters,test_auc_error-test_auc_std,test_auc_std+test_auc_std,color= 'darkbue')\n\nplt.xlabel('K: Hyperparameter')\nplt.ylabel('Errors')\nplt.legend(loc = 'lower right')\nplt.show()\n\noptimal_k = clf.best_params_.get('n_neighbors')\n\nprint('optimal k is ', optimal_k)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_values = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\npred_values.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"id\": test_data[\"id\"],\n        \"target\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}