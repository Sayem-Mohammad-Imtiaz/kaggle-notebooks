{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hello thereüôè, this notebook is contains information of Every Machine learning Probelm(Beginner to Advance).\nI Collected codes & thoery from \"Approaching(Almost) Any Machine Learning Problem\" and added some important thoery stuff from internet. \n\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/ml.jpg?raw=true'>\n\n### Contents\n\n1. Supervised vs Unsupervised learning\n2. Cross-validation\n3. Evaluation metrics\n4. Approaching categorical variables\n5. Feature engineering\n6. Feature selction \n7. Hyperparameter optimization\n8. Approaching image classification & segmentation\n9. Approaching text classification/regression\n10. Approaching ensembling and stacking\n11. Approaching reproducible code & model serving\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1 Supervised Learning vs Unsupervised Learning \n**Introduction**\n> ‚ÄúWhat‚Äôs the difference between supervised learning and unsupervised learning?‚Äù\n\nThis is actually among the first things you should learn when you‚Äôre embarking on your machine learning journey. We cannot simply jump into the model building phase if we don‚Äôt understand where algorithms like linear regression, logistic regression, clustering, neural networks, etc. fall under.\n\n#### Supervised Vs Unsupervised\n<img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Supervised-Vs-Unsupervised.jpg'>\n\nIf we don‚Äôt know what the objective of the machine learning algorithm is, we will fail in our endeavor to build an accurate model. This is where the idea of supervised learning and unsupervised learning comes in.\n\nI will discuss these two concepts using examples and also answer the big question ‚Äì how to decide when to use supervised learning or unsupervised learning?\n \nLet‚Äôs begin by taking a look at Supervised Learning.\n \n#### What is Supervised Learning?\n\n<img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/A-typical-supervised-learning-algorithm.png'>\n\nIn supervised learning, the computer is taught by example. It learns from past data and applies the learning to present data to predict future events. In this case, both input and desired output data provide help to the prediction of future events.\n\nFor accurate predictions, the input data is labeled or tagged as the right answer.\n\n##### Supervised Learning\n\nSupervised Machine Learning Categorisation\nIt is important to remember that all supervised learning algorithms are essentially complex algorithms, categorized as either classification or regression models.\n\n1) **Classification Models** ‚Äì Classification models are used for problems where the output variable can be categorized, such as ‚ÄúYes‚Äù or ‚ÄúNo‚Äù, or ‚ÄúPass‚Äù or ‚ÄúFail.‚Äù Classification Models are used to predict the category of the data. Real-life examples include spam detection, sentiment analysis, scorecard prediction of exams, etc.\n\n2) **Regression Models** ‚Äì Regression models are used for problems where the output variable is a real value such as a unique number, dollars, salary, weight or pressure, for example. It is most often used to predict numerical values based on previous data observations. Some of the more familiar regression algorithms include linear regression, logistic regression, polynomial regression, and ridge regression.\n\n<img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/regression-vs-classification-in-machine-learning.png'>\n\nThere are some very practical applications of supervised learning algorithms in real life, including:\n\nText categorization\nFace Detection\nSignature recognition\nCustomer discovery\nSpam detection\nWeather forecasting\nPredicting housing prices based on the prevailing market price\nStock price predictions, among others\n \n\n#### What is Unsupervised Learning?\n\nUnsupervised learning, on the other hand, is the method that trains machines to use data that is neither classified nor labeled. It means no training data can be provided and the machine is made to learn by itself. The machine must be able to classify the data without any prior information about the data.\n\nThe idea is to expose the machines to large volumes of varying data and allow it to learn from that data to provide insights that were previously unknown and to identify hidden patterns. As such, there aren‚Äôt necessarily defined outcomes from unsupervised learning algorithms. Rather, it determines what is different or interesting from the given dataset.\n\nThe machine needs to be programmed to learn by itself. The computer needs to understand and provide insights from both structured and unstructured data. Here‚Äôs an accurate illustration of unsupervised learning:\n\n<img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Unsupervised-Learning.png'>\n \n\n#### Unsupervised Machine Learning Categorization\n1) **Clustering** is one of the most common unsupervised learning methods. The method of clustering involves organizing unlabelled data into similar groups called clusters. Thus, a cluster is a collection of similar data items. The primary goal here is to find similarities in the data points and group similar data points into a cluster.\n\n2) **Anomaly detection** is the method of identifying rare items, events or observations which differ significantly from the majority of the data. We generally look for anomalies or outliers in data because they are suspicious. Anomaly detection is often utilized in bank fraud and medical error detection.\n\n \n\n#### Applications of Unsupervised Learning Algorithms\n\nSome practical applications of unsupervised learning algorithms include:\n\nFraud detection\nMalware detection\nIdentification of human errors during data entry\nConducting accurate basket analysis, etc.\n \n\n##### When Should you Choose Supervised Learning vs. Unsupervised Learning?\n\nIn manufacturing, a large number of factors affect which machine learning approach is best for any given task. And, since every machine learning problem is different, deciding on which technique to use is a complex process.\n\nIn general, a good strategy for honing in on the right machine learning approach is to:\n\n* **Evaluate the data.** Is it labeled/unlabelled? Is there available expert knowledge to support additional labeling? This will help to determine whether a supervised, unsupervised, semi-supervised or reinforced learning approach should be used\n* **Define the goal.** Is the problem recurring, defined one? Or, will the algorithm be expected to predict new problems?\n* **Review available algorithms** that may suit the problem with regards to dimensionality (number of features, attributes or characteristics). Candidate algorithms should be suited to the overall volume of data and its structure\n* **Study successful applications** of the algorithm type on similar problems\n \n\n**End Notes**\nSupervised learning and unsupervised learning are key concepts in the field of machine learning. A proper understanding of the basics is very important before you jump into the pool of different machine learning algorithms.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Most of the time, it's also possible to convert a supervised dataset to unsupervised to see how they look like when plotted.\nFor example, let's take a look at the dataset in below image. Image shows that **MINIST** dataset which is a very popular dataset of handwritten digits, and it is a supervised problem in which you are given the images of the numbers and the correct label associated with them.\n\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/MNIST.png?raw=true'>\n\nThis dataset can easily be converted to an upsupervised setting for basic visualization.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If we do a `t-Distributed Stochastic Neighbour Embedding(t-SNE)` decomposition of this dataset, we can see that we can separte the images to some extent just by doing with two components on the image pixels. This is shown on this Figure below\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/tsne.png?raw=true'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import some usefull libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# scikit-learn to get data and perform t-SNE\nfrom sklearn import datasets\nfrom sklearn import manifold\n\n%matplotlib inline\n#Graphics in retina format are more sharp and Legible\n%config inlinebackend.figure_format = 'retina'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = datasets.fetch_openml(\n    'mnist_784', \n    version=1,\n    return_X_y=True\n)\n\npixel_values, targets = data\ntargets = targets.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pixel_values.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an array of pixel values and another array of target. Since target are the string type, we convert them to integers.\n\n`pixel_values` is a 2-dimemsional arrya of shape 70000x784. There are 70000 different images, each of size 28x28 pixles. Falttening 28x28 gives 784 data points.\n\nWe can visualize the samples in this dataset by reshaping them to their original shape and them plotting them using matplotlib.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"single_image = pixel_values[1, :].reshape(28, 28)\n\nplt.imshow(single_image, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_image = pixel_values[2, :].reshape(28, 28)\n\nplt.imshow(single_image, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_image = pixel_values[7, :].reshape(28, 28)\n\nplt.imshow(single_image, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most important step comes after we have grabbed the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = manifold.TSNE(n_components=2, random_state=42)\n\ntransformed_data = tsne.fit_transform(pixel_values[:3000, :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This Step creates the t-SNE transformation of the data. We use only two components as we can visualize them well in a two-dimensional setting. The `transformed_data`, in this case, in array of shape 3000x2 (3000 rows and 2 columns). A data like this can be converted to pandas dataframe by calling *_pd.DataFrame_* on the array","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_df = pd.DataFrame(\n    np.column_stack((transformed_data, targets[:3000])),\n    columns=[\"X\",\"Y\",\"Targets\"]\n)\n\ntsne_df.loc[:, \"Targets\"] = tsne_df.Targets.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can plot it using seaborn and matplotlib.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = sns.FacetGrid(tsne_df, hue=\"Targets\", size=8)\n\ngrid.map(plt.scatter,\"X\",\"Y\").add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is one way of visualizing upsupervied datasets. We can also do **k-means clustering** on the same dataset and see how it performs in an unsupervied setting. One question that aries all the time is how to find the optimal number of clusters in k-means clustering. Well, there is no right answer. You have to find the number by cross-validation.\n\n**MINST** is supervised classification problem, and we converted it to an unsupervied problem only to check if it gives any kind of good results and it is would be even better if we use classification algorithms. What are they and how to use them? Let's look at them in next topic.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2. Cross-validation\n\nWe did not build any models in the previous topic. The reason for that is simple. Before creating any king of machine learning model, we must known what cross-validation is and how to choose the best cross-validation depending on your dataset.\n\nSo, **what is cross-validation**, and why should we care about it?\n\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/cross-validation.png?raw=true'>\n\nWe can find multiple definations as to what cross-validation is. Mine is a one-liner:\n*cross-validation is a step in the process of building a machine learning model which helps us ensure that our models fit the data accurately and also ensure that we do not overfit.* But this leads to another term: **overfitting.**\n\nTo explain overfitting, i think it's best if we look at a dataset. There is a **red wine-quality dataset** which is quite famous. This dataset has 11 different attributes that decide the quality of red wine.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's see how this data look like.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/taruntiwarihp/dataSets/master/winequality-red.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10) # This dataset looks something like this: ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can treat this problem either as a classification problem or as a regression problem since wine quality is nothing but a real number between 0 and 10. For simplicity, let's choose classification. This dataset, however, consists of only six types of quality values. We will thus map all quality values from 0 to 5.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.quality.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a mapping dictionary that maps the  quality values from 0 to 5.\nquality_mapping = {\n    3: 0,\n    4: 1,\n    5: 2,\n    6: 3,\n    7: 4,\n    8: 5\n}\n\n# you can use the map function of pandas with any dictionary to covert the values in a given column to values in the dictionary\ndf.loc[:,\"quality\"] = df.quality.map(quality_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we look at this data and consider it a classification problem, a lot of algorithms come to our mind that we can apply to it, probably, we can use neural networks. But it would be a bit of a stretch if we dive into NN from the beginning. SO, let's start with something simple that we can visualize too: **decision trees.**\n\nBefore we begin to understand what overfitting is, let's divide the data into two parts. This dataset has 1500 samples. We keep 1000 samples for training and 599 as a seprate set.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use sample with frac=1 to shuffle the dataframe \n# we reset the indices since they change after \n# shuffling the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# top 1000 rows are selected for training \ndf_train = df.head(1000)\n\n# botton 599 values are selected for testing/validation\ndf_test = df.tail(599)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now train a decision tree model on the training set. For the decision tree model, I am going to use scikit-learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import from scikit-learn\nfrom sklearn import tree\nfrom sklearn import metrics\n\n# initialize decision tree classifier class with a max_depth of 3\nclf = tree.DecisionTreeClassifier(max_depth=3)\n\n# choose the columns you want to train on \n# these are the features for model\ncols = ['fixed acidity',\n        'volatile acidity',\n        'citric acid',\n        'residual sugar',\n        'chlorides',\n        'free sulfur dioxide',\n        'total sulfur dioxide',\n        'density',\n        'pH',\n        'sulphates',\n        'alcohol'\n]\n\n# train the mode on the provided features and mapped quality from before\nclf.fit(df_train[cols], df_train.quality)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**-> *I have used a `max_depth` of 3 for the decision tree classifier. I have left all other parameters of this model to its default value.*\n\nNow, we test the accuracy of model on the training set and the test set:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate prediction on the training set \ntrain_predictions =  clf.predict(df_train[cols])\n\n# generate prediction on the test set\ntest_predictions = clf.predict(df_test[cols])\n\n# calculate the accuracy of prediction on training dataset\ntrain_accuracy = metrics.accuracy_score(\n    df_train.quality, train_predictions\n)\n\n# calculate the accuracy of prediction on test dataset\ntest_accuracy = metrics.accuracy_score(\n    df_test.quality, test_predictions\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training and test accuracies are found to be 59.9% and 54.90%. Now we increase the `max_depth` to 7 and repeat the process. This gives training accuracy of 76.6% and test accuracy 57.3%. Here, we have used accuracy, mainly because it is the most straightforward metric. It might not be the best metric for this problem. What about we calculate these accuracies for different values of max_depth and make a plot? ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import  scikit-learn tree and metric\nfrom sklearn import tree\nfrom sklearn import metrics\n\n# import matplotlib and seaborn for plotting\nimport matplotlib \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# this is our global size of label text on the plots\nmatplotlib.rc('xtick',labelsize=20)\nmatplotlib.rc('ytick',labelsize=20)\n\n# This line ensures that the plot is displayed inside the notebook\n%matplotlib inline\n\n# initalize lists to store accuracies for training and test data\n# we start with 50% accuracy\ntrain_accuracies = [0.5]\ntest_accuracies = [0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterate over a few depth values\nfor depth in range(1, 25):\n    # init the model\n    clf = tree.DecisionTreeClassifier(max_depth=depth)\n    \n    # fit the model on given feature\n    clf.fit(df_train[cols],df_train.quality)\n    \n    # create training and test predictions\n    train_predictions =  clf.predict(df_train[cols])\n    test_predictions = clf.predict(df_test[cols])\n\n    # calculate training and test accuracies\n    train_accuracy = metrics.accuracy_score(\n        df_train.quality, train_predictions\n    )\n\n    test_accuracy = metrics.accuracy_score(\n        df_test.quality, test_predictions\n    )\n    \n    # append accuracies\n    train_accuracies.append(train_accuracy)\n    test_accuracies.append(test_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create two plots using matplotlib and seaborn \nplt.figure(figsize=(10,5))\nsns.set_style(\"whitegrid\")\nplt.plot(train_accuracies,label=\"Train Accuracy\")\nplt.plot(test_accuracies,label=\"Test Accuracy\")\nplt.legend(loc=\"upper left\", prop={'size':15})\nplt.xticks(range(0, 26, 5))\nplt.xlabel(\"max_depth\",size=20)\nplt.ylabel(\"accuracy\",size=20)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the best score for test data is obtained when `max_depth` has a value of 7. As we keep increasing the value of this parameter, test accuracy remains the same or gets worse, but the training accuracy keeos increasing, It means that our simple decision tree model keeps learning about training data better and better with an increase in max_depth, but the performance on test data does not improve at all.\n\n*This is called overfitting*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Whenever we train a neural network, we must moniter loss during the training time for both training and test set. If we have a very large network for a dataset which is quite small (i.e. very less number of samples), we will observe that the loss for both training and test set will dicrease as we keep training, However, at some point, test loss will reach its minima, and after that, it will start increasing even though training loss decreases further. We must stop training where the validation loss reaches its minimum value.\n\n*This is the most common explanation of overfitting.*\n\n **Occam's razor** in simple words states that one should not try to complicate things that can be solved in a much simpler manner. In general, whenever your model does not obey Occam's razor, it is probably overfitting.\n \n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/overfitting.png?raw=true'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now come back to cross-validation\n\nWhile explaining about overfitting, I decided to divide the data into two parts. I trained the model on one part and checked its performance on the other part. Well, this is also a kind of cross-validation commonly known as **hold-out set.** We use this kind of (cross-) validation when we have a large amount of data and model inference is a time-consuming process.\n\nHowever, there are a few types of cross-validation techniques which are the most popular and wideky used.\n\nThese include:\n    1. k-fold cross-validation.\n    2. stratified k-fold cross-validation\n    3. hold-out based validation\n    4. leave-one-out cross-validation\n    5. group k-fold cross-validation\n    \nCross-validation is dividing training data into a few parts. We train the module on some of these parts and test on the remaining parts. See in this image.\n\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/k-fold-cross-validation-1024x416.png?raw=true'>\n\nWhen you get a dataset to build machine learning models, you separate them into **two different sets: training and validation**. Many people also split it into a third set and call it a **test set.** We can devide the data into k different sets which are exculsize of each other. This is known as **k-fold cross-validation**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas and model_selection module of scikit-learn\nimport pandas as pd\nfrom sklearn import model_selection\n\nif __name__ == \"__main__\":\n    # Training data is in a csv file called train.csv\n    df = pd.read_csv(\"../input/novartis-data/Train.csv\")\n    \n    # we create a new  column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.KFold(n_splits=5)\n    \n    # fill the new kfold column\n    for fold, (trn_, val_) in enumerate (kf.split(X=df)):\n        df.loc[val_, 'kfold'] = fold\n        \n    # save the new csv with kflod column\n    df.to_csv(\"train_folds.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MULTIPLE_OFFENSE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next important type of cross-validation is **stratified k-fold**. If you have a skewed dataset for binary classification with 90% positive samples and only 10% negative samples, you don't want to use random k-fold cross-validation. Using simple k-fold corss-validation for dataset like this can result in folds with all negative samples. In these cases, we perfer using stratified k-fold cross-validation.\nStratified k-fold cross-validation keeps the ratio of labels in each fold constant. So, in each fold, you will have the same 90% positive and 10% negative samples. Thus, whatever metric you choose to evaluate, it will give similar results across all folds.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas and model_selection module of scikit-learn\nimport pandas as pd\nfrom sklearn import model_selection\n\nif __name__ == \"__main__\":\n    # Training data is in a csv file called train.csv\n    df = pd.read_csv(\"../input/novartis-data/Train.csv\")\n    \n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    df = df.sample(frac=1).reset_index(drop=True)\n    \n    # fetch target\n    y = df.MULTIPLE_OFFENSE.values\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    \n    # fill the newkfold column\n    for f, (t_, v_) in enumerate(kf.split(X=df,y=y)):\n        df.loc[v_, 'kfold'] = f\n        \n    # save the new csv with kfold column\n    df.to_csv(\"train_sffolds.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = pd.read_csv(\"train_folds.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sf = pd.read_csv(\"train_sffolds.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(kf['MULTIPLE_OFFENSE'], kf['kfold'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(sf['MULTIPLE_OFFENSE'], sf['kfold'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In many cases, we have to deal with small datasets and creating big validation sets means losing a lot of data for model to learn. In those cases, we can opt for a type of k-fold cross-validation where k=N, where N is the number of samples in the dataset. This means that in all folds of training, we will be training on all data samples except 1. The number of folds for this type of cross-validation is the same as the number of samples that we have in the dataset.\n\nNow we can move to regression. The good thing about regression problems is that we can use all the cross-validation techniques mentioned above for regression problems except for stratified k-fold. That is we cannot use stratified k-fold directly, but there are ways to change the problem a bit so that we can use stratified k-fold for regression problems. Mostly, simple k-fold cross-validation works for any regression problem. However, if you see that the distribution of targets is not consistent, you can use stratified k-fold.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To use **stratified k-fold for a regression problem**, we have first to divide the targets into bins, and then we can use stratified k-fold in the same way as for classification problems. There are several choices for selecting the appropriate number of bins. If you have a lot of samples( > 10k, > 100k), then you don't need to care about the number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of samples, you can use a simple rule like **Sturge's Rule** to calculate the appropriate number of bins.\n\nSturge's rule:\n>      Number of Bins = 1 log2(N)\n\nWhere N is the number of samples you have in your dataset.\n                                                                                   \nLet's make a simple regression dataset and try to apply stratified k-fold as shown in the following python snippet.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stratified k-fold for regression\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn import model_selection\n\ndef create_folds(data):\n    # we create a new column called kfold and fill it with -1\n    data['kfold'] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # calculate the number of bins by Sturge's rule \n    # I take the floor of the value, you can also just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    \n    # fill the new kfold column note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\",axis=1)\n    # return dataframe with folds\n    \n    return data\n\nif __name__ == \"__main__\":\n    # we create a sample dataset with 15000 samples and 100 features and 1 target\n    X, y = datasets.make_regression(\n        n_samples=15000, n_features=100, n_targets=1\n    )\n    \n    # create a dataframe out of our numpy array\n    df = pd.DataFrame(\n        X,\n        columns=[f\"f_{i}\" for i in range(X.shape[1])]\n    )\n    df.loc[:,\"target\"] = y\n    \n    # create folds\n    df = create_folds(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross-validation is the first and most essential step when it comes to building machine learning models. If you want to do feature engineering, split your data first. If you're going to build models, split your data first. If you have a *good cross-validation scheme in which validation data is representative of training and real-world data*, you will be able to build a good machine learning model which is highly generalizable.\n\nThe type of cross-validation presented in this topic can be applies to almost any machine learning problem, Still, you must keep in mind that cross-validation also depends a lot on the data any you might need to adopt new forms of cross-validation depending on your problem and data.\n\nFor example,let's say we have a problem  in which we would like to build a model to detect skin cancer from skin images of patients. Our task is to build a binary classifier which takes an input image and predicts the probabililty for it being benign or malignant.\n\nIn these king of datasets, you might have multiple images for the sample patient in the training dataset. So, to build a good cross-validation system here, you must have stratified k-folds, but you must also make sure that patients in training data do not appear in validation data. Fortunately, scikit-learn offers a type of cross-validation known as ***GroupKFold***. Here the patients can be considered as groups. But unfortunatley, there is no way to combine ***GrouoKFold*** with ***StratifiedKFold*** in scikit-learn.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Evaluation metrics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"When it comes to machine learning problems, you  will encounter a lot of different types of metrics in the real world. Sometimes, people even end up creating metrics that suit the business problem. We will see some of the most common metrics that you can use when starting with your very first few projects.\n\nAt the start of the topics, we introduced supervised and unsupervised learning. Althrough there are some kinds of metrics that you can use for unsupervised learning, we will only focus on supervised. THe reason for this is because supervised problems are in abundance compared to un-supervised, and evaluation of unsupervised methods is quite subjective.\n\nIf we talk about classification problems, the most common metrics used are:\n- Accuracy\n- Precision(P)\n- Recall(R)\n- F1 score(F1)\n- Area under the ROC (Receiver Operating Characteristic) curve or simply AUC(AUC)\n- Log Loss\n- Precision at k (P@k)\n- Average precision at k(AP@k)\n- Mean average precision at k(MAP@k)\n\nWhen it comes to regression, the most commonly used evaluatio metrics are:\n- Mean absolute error(MAE)\n- Mean squared error(MSE)\n- Root mean squared error(RMSE)\n- Root mean squared logarithmic error(RMSLE)\n- Mean percentage error(MPE)\n- Mean absolute percentage error (MAPE)\n- R2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To learn more about these metrics, let's start with a simple problem. Suppose we have a **binary classification problem**, i.e. a problem in which there are only two targets. Let's suppose it's problem of classifying chest x-ray images. There are chest x-ray images with no problem, and some of the chest x-ray images have collapsed ling which is also known as pneumothorax. So, our task is to build a classifier that given a chest x-ray image can detect if it has pneumothorax.\n\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/pneumothorax.jpg?raw=true'>\n\nWe also assume that we have an equal number of pneumothorax and non-pneumothorax chest x-ray images: let's say 100 each. Thus, we have 100 positive samples and 100 negative samples with a total of 200 images.\n\n\n**Accuracy:** It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For the problem described above, if you build a model that classified 90 images accurately, your accuracy is 90% or 0.90. If only 83 images are classified correctly, the accuracy of your model is 83% or 0.83. Simple.\n\nPython code for calculating accuracy is also quite simple.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy \ndef accuracy(y_true, y_pred):\n    \"\"\"\n    Function to calculate accuracy\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: accuracy score\n    \"\"\"\n    # initialize a simple counter for correct prediction\n    correct_counter = 0\n    # loop over all elements of y_true and y_pred \"together\"\n    for yt, yp in zip(y_true, y_pred):\n        if yt == yp:\n            # if prediction is equal to truth, increase the counter\n            correct_counter += 1\n    \n    # return accuracy\n    # which is correct predictions over the number of samples\n    return correct_counter / len(y_true)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also caculate accuracy using scikit-learn and accuracy function\nfrom sklearn import metrics\nl1 = [0,1,1,1,0,0,0,1]\nl2 = [0,1,0,1,0,1,0,0]\nprint(metrics.accuracy_score(l1,l2))\nprint(accuracy(l1,l2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's say we change the dataset a bit such that there are 180 chest x-ray images which do not have pneumothorax and only 20 with pneumothorax. Even in this case, we will create the training and validation set with the same ratio of positive to negative(pneumothorax to non-pneumothorax) targets. In each set, we have 90 non-pneumothorax and 10 pneumothorax images. If you say that all images in the validation set are non-pneumothorax, what would your accuracy be? Let's see; you classified 90% of the images correctly. So, your accuracy is 90%.\n\nBut look at it one more time .\n\nYou didn't even build a model and got an accuracy of 90%. That seems kind of useless. If we look carefully, we will see that the dataset is skewed i.e., the number of samples in one class outnumber the number of samples in other class by a lot. In these kinds of cases, it is not advisable to use accuracy as an evaluation metric as it is not representative of the data. So, you might get high accuracy, but your model will probably not perform that will when it comes to real-world samples, any you won't be able to explain to your manager why.\n\nIn these cases, it's better to look at other metrics such as **precision.**\n\nBefore learning about precision, we need to know a few terms. Here we have assumed that chest x-ray images with pneumothorax are positive class (1) and withour pneumothorax are negative class(0)\n\n1. True Positive(TP)\n2. True Negative(TN)\n3. False Positive(FP)\n4. False Negative(FN)\n\nLet's look at implementations of these, one at a time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def true_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate True Positives\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of true positives\n    \"\"\"\n    # initialize\n    tp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 1:\n            tp += 1\n    return tp\n\ndef true_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate True Negative\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of true negatives\n    \"\"\"\n    # initialize\n    tn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 0:\n            tn +=1\n    return tn\n\ndef false_positive(y_true, y_pred):\n    \"\"\"\n    Function to calculate False Positive\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of False positives\n    \"\"\"\n    # initialize\n    fp = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 0 and yp == 1:\n            fp += 1\n    return fp\n\ndef false_negative(y_true, y_pred):\n    \"\"\"\n    Function to calculate False Negative\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: number of False negatives\n    \"\"\"\n    # initialize\n    fn = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 0:\n            fn += 1\n    return fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(true_positive(l1,l2))\nprint(false_positive(l1,l2))\nprint(false_negative(l1,l2))\nprint(true_negative(l1,l2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we have to define accuracy using the terms described above, we can write:\n> Accuracy Score = (TP + TN) / (TP + TN + FP + FN)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy_v2(y_true, y_pred):\n    \"\"\"\n    Function to calculate accuracy using tp/tn/fp/fn\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: accuracy score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    tn = true_negative(y_true,y_pred)\n    accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n    return accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy(l1,l2))\nprint(accuracy_v2(l1,l2))\nprint(metrics.accuracy_score(l1,l2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can move to other important metrics.\n\nFirst one is precison. **Precision** is defined as:\n> Precision = TP / (TP + FP)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: precision score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fp = false_positive(y_true, y_pred)\n    precision = tp/ (tp + fp)\n    return precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precision(l1,l2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This seems  Fine\n\nNext, we come to recall. **Recall** is defined as:\n> Recall = TP / (TP + FN)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate recall\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: recall score\n    \"\"\"\n    tp = true_positive(y_true, y_pred)\n    fn = false_negative(y_true, y_pred)\n    recall  = tp / (tp + fn)\n    return recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall(l1,l2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And that matches our calculated values!\n\nFor a \"good\" model, our precision and recall values should be high. We see that in the above examples, the recall is quite high. However, precision is very low! Our model produces quite a lot of false positive but less false negatives. Fewer false negative are good in tis type of problem because you don't want to say that patients do not have pneumothorax when they do. That is going to be more harmful. But we do have a lot of false positives, and that's not good either.\n\nMost of the models predict a probability, and when we predict, we usually choose this threshold to be 0.5. This threshold is not always ideal, and depending on this threshod, your values of precision and recall can change drastically. If for every threshold we choose, we calculate the precision and recall values, we can create a plot between these sets of values. This plot or curve is known as the precision-recall curve.\n\nBefore looking into the precision-recall curve, let's assume two lists.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, \n          1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n\ny_pred = [0.02638412, 0.11114267, 0.31620708,\n         0.0490937, 0.0191491, 0.17554844, \n         0.15952202, 0.03819563, 0.11639273, \n         0.079377, 0.08584789, 0.39095342, \n         0.27259048, 0.03447096, 0.04644807, \n         0.03543574, 0.18521942, 0.05934905, \n         0.61977213, 0.33056815]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, y_true is our targets, and y_pred is the probability values for a sample being assigned a value of 1. So, now, we look at probabilites in prediction instead of the predicted value (which is most of the time calculated with a threshold at 0.5).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precisions = []\nrecalls = []\n# how we assumed these thresholds is a long story\nthresholds = [0.0490937, 0.05934905, 0.079377, \n              0.08584789, 0.11114267, 0.11639273, \n              0.15952202, 0.17554844, 0.18521942, \n              0.27259048, 0.31620708, 0.33056815,\n              0.39095342, 0.61977213]\n\n# for every threshold, calculate predictions in library and append calculated precisions and recalls to their respective lists\nfor i in thresholds:\n    temp_prediction = [1 if x >= i else 0 for x in y_pred]\n    p = precision(y_true, temp_prediction)\n    r = recall(y_true, temp_prediction)\n    precisions.append(p)\n    recalls.append(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"precisions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recalls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nplt.plot(recalls, precisions)\nplt.xlabel(\"Recall\", fontsize = 15)\nplt.ylabel(\"Precision\", fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"F1 score is a metric that combines both precision and recall. It is defined as simple weighted average(harmonic mean) of precision and recall. If we donate precision using P and recall using R, we can represent the F1 score as:\n \n > F1 = 2PR / (P + R)\n    \nA little bit of mathematics will lead you to the following equation of F1 based on TP, FP and FN.\n \n > F1 = 2TP / (2TP + FP + FN)\n   \nA python implementation is simple because we have already implemented these:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1(y_true, y_pred):\n    \"\"\"\n    Function to calculate f1 score\n    :param y_true: list of true values \n    :param y_pred: list of predicted values\n    :return: f1 score\n    \"\"\"\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    \n    score = 2 * p * r / (p + r)\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1(l1,l2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nmetrics.f1_score(l1, l2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of looking at precision and recall individually, you can also just look at **F1** score. Same as for precision,recall and accuracy, F1 score also ranges from 0 to 1, and a prefect prediction model has an F1 of 1.\n\nThen there are other crucial terms that we should know about.\n\nThe first one is **TPR or True Positive Rate**, which is the same as recall.\n\n> TRP = TP / (TP + FN)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# TPR or True Positive Rate\n\ndef tpr (y_true, y_pred):\n    \"\"\"\n    Function to calculate tpr\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: tpr/recall\n    \"\"\"\n    return recall(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TPR or recall is also known as **sensitivity.**\n\nAnd **FPR or False Positive Rate**, which is defined as:\n \n> FPR = FP / (TN + FP)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# FPR or False Positive Rate\n\ndef fpr (y_true, y_pred):\n    \"\"\"\n    Function to calculate fpr\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: fpr\n    \"\"\"\n    fp = false_positive(y_true, y_pred)\n    tn = true_negative(y_true, y_pred)\n    return fp / (tn + fp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And **1 - FPR** is known as **specificity or True Negative Rate or TNR**\n\nLet's calculate only two values, though: TPR and FPR.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# empty lists to store tpr\n# and fpr values\ntpr_list = []\nfpr_list = []\n\n# actual targets\ny_true = [0, 0, 0, 0, 1, 0, 1,\n         0, 0, 1, 0, 1, 0, 0, 1]\n\n# predicted probabilities of sample being 1\ny_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.5, \n         0.9, 0.5, 0.3, 0.66, 0.3, 0.2, \n         0.85, 0.15, 0.99]\n\n# handmade thresholds\nthresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,\n              0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n\n# loop over all thresholds\nfor thresh in thresholds:\n    # calculate predictions for a given threshold\n    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n    # calculate tpr\n    temp_tpr = tpr(y_true, temp_pred)\n    # calculate fpr\n    temp_fpr = fpr(y_true, temp_pred)\n    #append tpr and fpr to lists\n    tpr_list.append(temp_tpr)\n    fpr_list.append(temp_fpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data={'Thresholds':thresholds, 'TPR':tpr_list, 'FPR':fpr_list}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7, 7))\nplt.fill_between(fpr_list, tpr_list, alpha=0.4)\nplt.xlim(0, 1.0)\nplt.ylim(0, 1.0)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR',fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This curve is also known as the **Receiver Operating Characteristic(ROC)**. And if we calculate the area under this ROC cure, we are calculating  another metric which is used very often when you have a dataset which has skewed binary targets.\n\nThis metric is known as the **Area Under ROC Curve** or **Area Under Curve** or just simply **AUC**. There are many ways to calculate the area under the ROC curve. For this particular purpose, we will stick to the fantastic implementation by scikit-learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = [0, 0, 0, 0, 1, 0, 1, \n         0, 0, 1, 0, 1, 0, 0, 1]\ny_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n         0.9, 0.5, 0.3, 0.66, 0.3, 0.2, \n         0.85, 0.15, 0.99]\nmetrics.roc_auc_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After calculating probabilities and AUC, you would want to make predictions on the test set. Depending on the problem and use-case, you might want to either have probabilities or actual classes. If you want to have probabilities, it's effortless. You already have them. If you want to have classes, you need to select a threshold. In the case of binary classification, you can do something like the following.\n\n> Prediction = Probability >= Threshold\n \nWhich means, that *predictions* is a new list which contains only binary variablrs. An item in *prediction* is 1 if the probability is greater than or equal to a given *threshold* else the value is 0.\n\nAnd guess what, you can use the ROC curve to choose this threshold! The ROC curve will tell you how the threshold impacts false positive rate and true positive rate and thus, in turn, false positives and true positive. You should choose the threshold that is best suited for your problem and datasets.\n\nFor example, If you don't want to have too many false positives, you should have a high threshold value. This will, however, also give you a lot more false negatives. Observe the trade-off and select the best threshold. Let's see how these thresholds impact true positive and false positive values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# empty lists to store ture positive and false positive values\ntp_list = []\nfp_list = []\n\n# actual targets\ny_true = [0, 0, 0, 0, 1, 0, 1, \n         0, 0, 1, 0, 1, 0, 0, 1]\n\n# predicted probabilities of a sample being 1\ny_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n         0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n          0.85, 0.15, 0.99]\n\n# some handmade thresholds \nthresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, \n             0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n\n# loop over all thresholds\nfor thresh in thresholds:\n    # calculate predictions for a given threshold\n    temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n    # calculate tp\n    temp_tp = true_positive(y_true, temp_pred)\n    # calculate fp\n    temp_fp = false_positive(y_true, temp_pred)\n    tp_list.append(float(temp_tp))\n    fp_list.append(float(temp_fp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data={'Thresholds':thresholds, 'TP':tp_list, 'FP':fp_list}) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src='https://github.com/taruntiwarihp/raw_images/blob/master/threshold.png?raw=true'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another important metric you should learn after learning AUC is **log loss**. In case of a binary classification problem, we define log loss as:\n\n> Log Loss = - 1.0 * (target * log(prediction) + (1 - target) * log(1- prediction))\n\nFor multiple samples in the dataset, the log-loss over all samples is a mere average of all individual log losses. One thing to remember is that log loss penalized(Bad results) quite high for an incorrect or a far-off prediction, i.e. log loss punishes you for being very sure and very wrong.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_loss(y_true, y_proba):\n    \"\"\"\n    Function to calculate log loss\n    :param y_true: list of true values\n    :param y_proba: list of probabilities for 1\n    :return: overall log loss\n    \"\"\"\n    # define an epsilon value \n    # this can also be an input\n    # this value is used to clip probabilities \n    epsilon = 1e-15\n    # initialize empty list to store \n    # individual losses\n    loss=[]\n    # loop over all true and predicted probability values\n    for yt, yp in zip(y_true, y_proba):\n        # adjust probability\n        # 0 gets converted to 1e-15\n        # 1 gets converted to 1-1e-15\n        # Why? Think about it!\n        yp = np.clip(yp, epsilon, 1 - epsilon)\n        # calculate loss for one sample \n        temp_loss = - 1.0 * (\n            yt * np.log(yp)\n            + (1 - yt) * np.log(1 - yp)\n        )\n        # add to loss list\n        loss.append(temp_loss)\n    # return mean loss over all samples\n    return np.mean(loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = [0, 0, 0, 0, 1, 0, 1, \n         0, 0, 1, 0, 1, 0, 0, 1]\n\ny_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n          0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n          0.85, 0.15, 0.99]\n\nlog_loss(y_true, y_proba)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\nmetrics.log_loss(y_true, y_proba)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the metrics that we discussed until now can be converted to a multi-class version. The idea is quite simple. Let's take precision and recall. We can calculate precision and recall for each class in a **multi-classification** problem.\n\nThere are three different ways to calculate this which might get confusiong from time to time. Let's assume we are interested in precision first. We know that precision depends on true positive and false positives.\n\n1. **Macro averaged precision:** Calculate precision for the all classes individually and then average them.\n2. **Micro averaged precision:** Calculate class wise true positive and false positive and then use that to calculate overall precision.\n3. **Weighted precision:** Same as macro but in this case, it is weighted average depending on the number of items in each class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def macro_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate macro averaged precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: macro precision score\n    \"\"\"\n    # find the number of classes by taking lenght of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize precision to 0 \n    precision = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        \n        # calculate false positive for current class\n        fp = false_positive(temp_true, temp_pred)\n        \n        # calculate precision for current class\n        temp_precision= tp / (tp + fp)\n        \n        # keep adding precision for all classes\n        precision += temp_precision\n        \n    # calculate and retrun average precision over all classes\n    precision /= num_classes\n    return precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def micro_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate micro averaged precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: micro precision score\n    \"\"\"\n    # find the number of classes by taking length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize tp and fp to 0\n    tp = 0\n    fp = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class and update overall tp\n        tp += true_positive(temp_true, temp_pred)\n        \n        # calculate false positive for current class and update overall fp\n        fp += false_positive(temp_true, temp_pred)\n        \n    # calculate precision for current class\n    precision= tp / (tp + fp)\n    return precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ndef weighted_precision(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted averaged precision\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: weighted precision score\n    \"\"\"\n    # find the number of classes by taking lenght of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1: 15, 2: 21}\n    class_counts = Counter(y_true)\n    \n    # initialize precision to 0 \n    precision = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate tp and fp for class\n        tp = true_positive(temp_true, temp_pred)\n        fp = false_positive(temp_true, temp_pred)\n        \n        # calculate precision for current class\n        temp_precision= tp / (tp + fp)\n        \n        # multiply precision with count of samples in class\n        weighted_precision = class_counts[class_] * temp_precision\n        \n        # add to overall precision\n        precision += weighted_precision\n        \n    # calculate overall precision by dividing by total number of samples\n    overall_precision = precision / len(y_true)\n    \n    return overall_precision","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(macro_precision(y_true, y_pred))\n\nprint(metrics.precision_score(y_true, y_pred, average=\"macro\"))\n\nprint(micro_precision(y_true, y_pred))\n\nprint(metrics.precision_score(y_true, y_pred, average=\"micro\"))\n\nprint(weighted_precision(y_true, y_pred))\n\nprint(metrics.precision_score(y_true, y_pred, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, we can implement the **recall metric for multi-class**. Precision and recall depend on true positive, false positive and false negative while F1 depends on precision and recall.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def macro_recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate macro averaged recall\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: macro recall score\n    \"\"\"\n    # find the number of classes by taking lenght of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize recall to 0 \n    recall = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class\n        tp = true_positive(temp_true, temp_pred)\n        \n        # calculate false negative for current class\n        fn = false_negative(temp_true, temp_pred)\n        \n        # calculate recall for current class\n        temp_recall= tp / (tp + fn)\n        \n        # keep adding recall for all classes\n        recall += temp_recall\n        \n    # calculate and retrun average recall over all classes\n    recall /= num_classes\n    return recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def micro_recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate micro averaged recall\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: micro recall score\n    \"\"\"\n    # find the number of classes by taking length of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # initialize tp and fn to 0\n    tp = 0\n    fn = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate true positive for current class and update overall tp\n        tp += true_positive(temp_true, temp_pred)\n        \n        # calculate false negative for current class and update overall fp\n        fn += false_negative(temp_true, temp_pred)\n        \n    # calculate recall for current class\n    recall = tp / (tp + fn)\n    return recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ndef weighted_recall(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted averaged recall score\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: weighted recall score\n    \"\"\"\n    # find the number of classes by taking lenght of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1: 15, 2: 21}\n    class_counts = Counter(y_true)\n    \n    # initialize recall to 0 \n    recall = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate tp and fn for class\n        tp = true_positive(temp_true, temp_pred)\n        fn = false_negative(temp_true, temp_pred)\n        \n        # calculate recall for current class\n        temp_recall = tp / (tp + fn)\n        \n        # multiply recall with count of samples in class\n        weighted_recall = class_counts[class_] * temp_recall\n        \n        # add to overall recall\n        recall += weighted_recall\n        \n    # calculate overall recall by dividing by total number of samples\n    overall_recall = recall / len(y_true)\n    \n    return overall_recall","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(macro_recall(y_true, y_pred))\n\nprint(metrics.recall_score(y_true, y_pred, average=\"macro\"))\n\nprint(micro_recall(y_true, y_pred))\n\nprint(metrics.recall_score(y_true, y_pred, average=\"micro\"))\n\nprint(weighted_recall(y_true, y_pred))\n\nprint(metrics.recall_score(y_true, y_pred, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Implementation for f1 using macro and micro is left as an excercise for the reader and one version of F1 for multi-class, i.e., weighted average is implemented here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_f1(y_true, y_pred):\n    \"\"\"\n    Function to calculate weighted averaged f1 score\n    :param y_true: list of true values\n    :param y_pred: list of predicted values\n    :return: weighted f1 score\n    \"\"\"\n    # find the number of classes by taking lenght of unique values in true list\n    num_classes = len(np.unique(y_true))\n    \n    # create class:sample count dictionary\n    # it looks something like this:\n    # {0: 20, 1: 15, 2: 21}\n    class_counts = Counter(y_true)\n    \n    # initialize f1 to 0 \n    f1 = 0\n    \n    # loop over all classes\n    for class_ in range(num_classes):\n        \n        # all classes except current are considered negative\n        temp_true = [1 if p == class_ else 0 for p in y_true]\n        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n        \n        # calculate precision and recall for class\n        p = precision(temp_true, temp_pred)\n        r = recall(temp_true, temp_pred)\n        \n        # calculate f1 of class\n        if p + r != 0:\n            temp_f1 = 2 * p * r / (p + r)\n        else:\n            temp_f1 = 0\n        \n        # multiply f1 with count of samples in class\n        weighted_f1 = class_counts[class_] * temp_f1\n        \n        # add to f1 precision\n        f1 += weighted_f1\n        \n    # calculate overall f1 by dividing by total number of samples\n    overall_f1 = f1 / len(y_true)\n    \n    return overall_f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\nprint(weighted_f1(y_true, y_pred))\n\nprint(metrics.f1_score(y_true, y_pred, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, we have precision, recall and F1 implemented for multi-class problems. You can similarly convert AUC and log loss to multi-class formats too. This format of conversion is known as **one-vs-all**. I'm not going to implement them here as the implementation is quite similar to what we have already discussed.\n\nIn binary or multi-class classification, it is also quite popular to take a look at **confusion matrix**. \n\nWe see that the confusion matrix is made up od TP, FP, FN and TN. These are the only values we need to calculate precision, recall, F1 score and AUC. Sometimes, people also prefer calling FP as **Type-I error** and FN as **Type-II Error.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://github.com/taruntiwarihp/raw_images/blob/master/confusion-matrix.png?raw=true'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can also expand the binary classification matrix to a multi-class confusion matrix. How would that look like? If we have N classes, it will be a matrix of size NxN. For every class, we calculate the total number of samples that went to the class in concern and other classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n\n# some targets\ny_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n\n# some predictions\ny_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n\n# get confusion matrix from sklearn\ncm = metrics.confusion_matrix(y_true, y_pred)\n\n# plot using matplotlib and seaborn\nplt.figure(figsize=(10,10))\ncmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0,\n                            as_cmap=True)\nsns.set(font_scale=2.5)\nsns.heatmap(cm, annot=True, cmap=cmap, cbar=False)\nplt.ylabel(\"Actual Labels\", fontsize=20)\nplt.xlabel(\"Predicted Label\", fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, until now, we have tackled metrics for binary and multi-class classification. Then comes another type of classification problem called **multi-label classification**. In multi-label classification, each sample can have one or more classes associated with it. One simple examplr of this type of problem would be a task in which you are asked to predict different objects in a given image.\n\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/objects.jpg?raw=true'>\n\nNote tgat this dataset's objective is something different but let's not go there. Let's assume that the aim is only to predict if an object is present in an image or not. we have a table, flower-plot, window, but we don't have other objects such as computer, bet, tv, etc. So, one image can have multiple targets associated with it. This type of problem is the multi-label classification problem.\n\nThe metrics for this type of classificagtion problem are a bit different. Some suitable and most common metrics are:\n\n1. Precision at k (P@K)\n2. Average precision at k (AP@K)\n3. Mean average precision at k (MAP@K)\n4. Log Loss\n\nLet's start with **precision at k or P@K**. One must not confuse this precision with the precision discussed earlier. If you have a list of original classes for a given sample and list of predicted classes for the same, precision is defined as the number of hits in the predicted list considering only top-k predictions, divided by k.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates precision at k\n    for a single sample \n    :param y_true: list of values, actual classes.\n    :param y_pred: list of values, predicted classes.\n    :return: precision at a given value k\n    \"\"\"\n    # if k is 0, return 0. we should never have this as k is always >= 1\n    if k == 0:\n        return 0\n    # we are interested only in top-k predictions\n    y_pred = y_pred[:k]\n    # convert predictions to set\n    pred_set = set(y_pred)\n    # convert actual values to set\n    true_set = set(y_true)\n    # find common values\n    common_values = pred_set.intersection(true_set)\n    # return length of common values over k\n    return len(common_values) / len(y_pred[:k])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have **average precision at k or AP@K**. AP@K is calculated using P@K. For example, if we have to calculate AP@3, we calculate P@1, P@2 and P@3 and then divide the sum by 3.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def apk(y_true, y_pred, k):\n    \"\"\"\n    This function calculates average precision at k for single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :return: average precision at a given value k\n    \"\"\"\n    # initialize p@k list of values\n    pk_values = []\n    # loop over all k. from 1 to k + 1\n    for i in range(1, k + 1):\n        # calculate p@i and append to list\n        pk_values.append(pk(y_true, y_pred, i))\n    \n    # if we have no values in the list, return 0\n    if len(pk_values) == 0:\n        return 0\n    # else, we return the sum of list over length of list\n    return sum(pk_values) / len(pk_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = [\n    [1, 2, 3],\n    [0, 2],\n    [1],\n    [2, 3],\n    [1, 0],\n    []\n]\ny_pred = [\n    [0, 1, 2],\n    [1],\n    [0, 2, 3],\n    [2, 3, 4, 0],\n    [0, 1, 2],\n    [0]\n]\nfor i in range(len(y_true)):\n    for j in range(1, 4):\n        print(\n            f\"\"\"\n            y_true={y_true[i]},\n            y_pred={y_pred[i]},\n            AP@{j}={apk(y_true[i], y_pred[i], k=j)}\n            \"\"\"\n            )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how we can calculate AP@k which is per sample. In Machine learning, we are interested in all samples, and that's why we have **mean average precision at k or MAP@k**. MAP@k is just an average of AP@k and can be calculated easily by the following python code.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mapk(y_true, y_pred, k):\n    \"\"\"This function calculates mean avg precision at k for a single sample\n    :param y_true: list of values, actual classes\n    :param y_pred: list of values, predicted classes\n    :return: mean av precision at a given value k\n    \"\"\"\n    # initialize empty list for apk values\n    apk_values = []\n    # loop over all samples\n    for i in range(len(y_true)):\n        # store apk values for every sample\n        apk_values.append(\n            apk(y_true[i], y_pred[i], k=k)\n        )\n    # return mean of apk values list\n    return sum(apk_values) / len(apk_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can calculate MAP@k for k=1, 2, 3 and 4 for the same list of lists.\n\ny_true = [\n    [1, 2, 3],\n    [0, 2],\n    [1],\n    [2, 3],\n    [1, 0],\n    []\n]\ny_pred = [\n    [0, 1, 2],\n    [1],\n    [0, 2, 3],\n    [2, 3, 4, 0],\n    [0, 1, 2],\n    [0]\n]\n\nprint(mapk(y_true, y_pred, k=1))\n\nprint(mapk(y_true, y_pred, k=2))\n\nprint(mapk(y_true, y_pred, k=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P@k, AP@k and MAP@k all range from 0 to 1 with 1 being the best.\n\nPlease note that sometimes you might see different implementations of P@k and AP@k on the internet. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from:\n# https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\ndef apk(actual, predicted, k=10):\n    \"\"\"\n    Computes the average precision at k.\n    This function computes the AP at k between two lists of imtes.\n    Paramerters\n    ----------\n    actual : list\n             A list of elements that are to be predicted (order doesn't matter)\n    predicted : list\n                A list of predicted elements (order does matter)\n    k : int, optional\n        The maximum number of predicted elements\n    Returns\n    -------\n    score : double\n            The average precision at k over the input lists\n    \"\"\"\n    if len(predicted)>k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i,p in enumerate(predicted):\n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    if not actual:\n        return 0.0\n\n    return score / min(len(actual), k)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This implementation is another version of AP@k where order matters and we weigh the predictions. This implementation will have slightly different results from what i have presented.\n\nNow, we come to **log loss for multi-label classification**. This is quite easy. You can convert the targets to binary format and then use a log loss for each columns. In the end, you can take the average of log loss in each column. This is also known as mean column-wise log loss. Of course, there are other ways you can implement this, any you should explore it  as you come across it.\n\nWe have now reached a stage where we can say that we now know all binary, multi-class and multi-label classification metrics, and now we can more to regression metrics.\n\nThe mot common metric in regression is error. **Error** is simple and very easy to understand.\n\n> Error = True Value - Predicted Value\n\n**Absolute error** is just absolute of the above.\n\n> Absolute Error = Abs(True Value - Predicted Value)\n\nThen we have **mean absolute error(MAE)**. It's just mean of all absolute errors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_absolute_error(y_true, y_pred):\n    \"\"\"\n    This function calculates me\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean absolute error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    # loop over all samples in the true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate absolute error and add to error\n        error += np.abs(yt - yp)\n    #return mean error\n    return error / len(y_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, we have squared error and **mean squared error (MSE)**.\n\n> Squared Error = (True Value - Predicted Values)2\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_squared_error(y_true, y_pred):\n    \"\"\"\n    This function calculates mse\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real number, predicted values\n    :return: mean squared error\n    \"\"\"\n    # initialize error at 0\n    error = 0 \n    # loop over all samples in the true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate squared error\n        # and add to error\n        error += (yt - yp) ** 2\n    # return mean error\n    return error / len(y_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MSE and **RMSE (root mean squared error)** are the most popular metrics used evaluating regression models.\n\n> RMSE = SQRT (MSE) \n\nAnother type of error in same class is **squared logarithmic error.** Some people call it **SLE**, and when we take mean of this error across all samples, it is known as **MSLE(mean squared logarithmic error)** and implemented as follows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_squared_log_error (y_true, y_pred):\n    \"\"\"\n    This function calculates msle \n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean squared logarithmic error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    # loop over all samples in true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate squared log error and add to error\n        error += (np.log(1 + yt) - np.log(1 + yp)) ** 2\n    # return mean error \n    return error / len(y_true) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Root mean squared logarithmic error** is just a square root of this. It is also known as **RMSLE**.\n\nThen we have the precentage error:\n\n> Percentage Error = ((True Value - Predicted Value) / True Value ) * 100\n\nSame can be converted to mean percentage error for all samples.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_percentage_error(y_true, y_pred):\n    \"\"\"\n    This function calculates mpe\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean percentage error\n    \"\"\"\n    # initialize error at 0\n    error = 0 \n    \n    # loop over all samples in true and predicted list\n    for yt, yp in zip(y_true, y_pred):\n        # calculate percentage error and add to error \n        error += (yt - yp) / yt\n        \n    # return mean percentage error\n    return error / len(y_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And an absolute version of the same (and more common version) is known as **mean absolute percentage error or MAPE**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_abs_percentage_error(y_true, y_pred):\n    \"\"\"\n    This function calculates MAPE\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: mean absolute percentage error\n    \"\"\"\n    # initialize error at 0\n    error = 0\n    # loop over all samples in true and predicted list \n    for yt, yp in zip (y_true, y_pred):\n        # calculate percentage error and add to error\n        error += np.abs(yt - yp) / yt\n    # return mean percentage error\n    return error / len(y_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best thing about regression is that there are only a few most popular metrics that can be applied to almost every regression problem. And it is much easier to understand when we compare it to classification metrics.\n\nLet's talk about another regression metric known as **R2 (R-squared)**, also known as the **coefficient of determination**.\n\nIn simple words, R-squared says how good you models fits the data. R-squared closer to 1.0 says that the model fits the data quite well, wheras closer 0 means that model isn't that good. R-squared can also be negative when the model just makes absurd predictions.\n\nThe formula for R-squared is shown below, but as always a python implementation makes things more clear.\n\n<img src = 'https://github.com/taruntiwarihp/raw_images/blob/master/r2.png?raw=true'>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def r2(y_true, y_pred):\n    \"\"\"\n    This function calculates r-squared score\n    :param y_true: list of real numbers, true values\n    :param y_pred: list of real numbers, predicted values\n    :return: r2 score\n    \"\"\"\n    # calculate the mean value of true values\n    mean_true_value = np.mea(y_true)\n    \n    # initialize numerator with 0\n    numerator = 0\n    \n    # initialize denominator with 0\n    denominator = 0\n    \n    # loop over all true and predicted values\n    for yt, yp in zip(y_true, y_pred):\n        # update numerator \n        numerator += (yt - yp) ** 2\n        # update denominator \n        denominator += (yt - mean_true_value) ** 2\n    # calculate the ratio\n    ratio = numerator / denominator\n    # return 1 - ratio\n    retrun (1 - ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Note*** => *I have implemented these metrics in the most straightforward manner, and that means they are not efficient enough. You can make most of them in a very efficient way by properly using numpy. For example, take a look at the implementation of mean absolute error without any loops.*\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mae_np(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I could have implemented all the metrics this way but to learn it's etter to look at low-level implementation. Once you learn the low-level implementation in pure python, and without using a lot of numpy, you can easily converted it to numpy and make it much faster.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Lists of all metrics**\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/classification.PNG?raw=true'>\n<img src='https://github.com/taruntiwarihp/raw_images/blob/master/clus&reg.PNG?raw=true'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **This is what i did for you guys (specially for beginners), It took around 4-5 days or working and crunching important inforation from the internet. I hope you understand it, Please Upvote any share this notebook to beginners.**\n\n***Note*** => *Rest of the part i will cover in next notebooks.*\n\n**Reference** - Abhishek Thakur","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"All Books(PDF) of DataScience & Machine learning is available [here](https://github.com/taruntiwarihp/BookYouNeed)\nFork Thik repository at your account.\n\n\nThank You :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}