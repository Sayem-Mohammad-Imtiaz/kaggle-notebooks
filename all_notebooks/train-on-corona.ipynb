{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D,MaxPool2D \nfrom keras.layers.normalization import BatchNormalization\nimport numpy as np\nnp.random.seed(42)\nfrom keras.models import load_model\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nimport imageio \nimport seaborn\nimport shutil\nseaborn.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pretraining"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = str(3000)\nall_dict = \"../input/tb\" + dataset\npre_test_dict = \"../input/tb500val\"\npre_data_gen = ImageDataGenerator(rescale=1.0/255)\n\npre_train_gen = pre_data_gen.flow_from_directory(directory = all_dict, classes=[\"NORMAL\", \"TUBERCULOSIS\"],class_mode=\"binary\",target_size=(256, 256), shuffle = True)\npre_val_gen = pre_data_gen.flow_from_directory(directory = pre_test_dict, classes=[\"NORMAL\", \"TUBERCULOSIS\"], class_mode=\"binary\",target_size=(256, 256), shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        keras.metrics.AUC(name=\"auc\"),\n\n]\n\nAlexNet = Sequential()\n\n#1st Convolutional Layer\nAlexNet.add(Conv2D(filters=96, input_shape=(256,256,3), kernel_size=(11,11), strides=(4,4), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\nAlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#2nd Convolutional Layer\nAlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\nAlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#3rd Convolutional Layer\nAlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n\n#4th Convolutional Layer\nAlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n\n#5th Convolutional Layer\nAlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\nAlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#Passing it to a Fully Connected layer\nAlexNet.add(Flatten())\n# 1st Fully Connected Layer\nAlexNet.add(Dense(4096, input_shape=(32,32,3,)))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nAlexNet.add(Dropout(0.5))\n\n#2nd Fully Connected Layer\nAlexNet.add(Dense(4096))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n#Add Dropout\nAlexNet.add(Dropout(0.5))\n\n#3rd Fully Connected Layer\nAlexNet.add(Dense(1000))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('relu'))\n#Add Dropout\nAlexNet.add(Dropout(0.5))\n\n#Output Layer\nAlexNet.add(Dense(1))\nAlexNet.add(BatchNormalization())\nAlexNet.add(Activation('sigmoid'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = tf.keras.callbacks.History()\nopt = keras.optimizers.Adadelta(learning_rate= 1e-3)\nAlexNet.compile(optimizer=opt,loss='binary_crossentropy',metrics=metrics)\ncsv_logger = tf.keras.callbacks.CSVLogger('training_pretrained_TB' + dataset +\".csv\", append=True)\nreduceLR = tf.keras.callbacks.ReduceLROnPlateau(mintor=\"val_loss\",min_lr=1e-6, factor=1e-2)\n\n\n\ncheckpoint_filepath = \"./pretrained\" + dataset + \".h5\"\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\ncallbacks_list = [csv_logger,history]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\n\nAlexNet.compile(optimizer=opt,loss='binary_crossentropy',metrics=metrics)\nhistory = AlexNet.fit(x = pre_train_gen,epochs = 5, validation_data = pre_val_gen, batch_size = batch_size,callbacks = callbacks_list, shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AlexNet.save( \"alexnet_pretrainedTB\" + dataset + \".h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finetunning****"},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 30\nmodel = \"1000\"\n\n\ndataset = \"dbcovid\" + str(n)\n\ntrain_dict = os.path.join(\"../input/\",dataset)\ntest_dict = \"../input/dbcovidval150\"\n\ndata_gen = ImageDataGenerator(rescale=1.0/255)\n\ntrain_gen = data_gen.flow_from_directory(directory = train_dict,classes = [\"NORMAL\", \"COVID-19\"], class_mode=\"binary\",target_size=(256, 256))\nval_gen = data_gen.flow_from_directory(directory = test_dict,classes = [\"Normal\", \"COVID-19\"], class_mode=\"binary\", target_size=(256, 256))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TL_file = tf.keras.models.load_model(\"./alexnet_pretrainedTB1000.h5\")\nTL_file.summary()\n\nlast_index = -15\n    \nlast_conv_layer_output = TL_file.layers[last_index].output\ndense1 = Dense(1000)(last_conv_layer_output)\nBN1 = BatchNormalization(name=\"bn1\")(dense1)\nact1 = Activation('relu', name =\"act1\")(BN1)\ndropout1 = Dropout(0.5)(act1)\ndense2 = Dense(1)(dropout1)\nBN2 = BatchNormalization(name=\"bn2\")(dense2)\nact2 = Activation('sigmoid', name =\"act2\" )(BN2)\n\n\n\n\nTL = Model(TL_file.inputs, act2)\n\nfor i in TL.layers[:-7] :\n    i.trainable = False\n\n    \nTL.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nhistory_TL = tf.keras.callbacks.History()\nhistory_baseline = tf.keras.callbacks.History()\n\n\n\n\nreduceLR = tf.keras.callbacks.ReduceLROnPlateau(mintor=\"val_loss\",min_lr=1e-6, factor=1e-2)\ncsv_logger_TL = tf.keras.callbacks.CSVLogger(dataset + model +' training_TL.csv', append=True)\ncsv_logger_baseline = tf.keras.callbacks.CSVLogger(dataset + model + ' training_baseline.csv', append=True)\n\n\ncallbacks_TL = [ csv_logger_TL ,history_TL ]\ncallbacks_baseline = [csv_logger_baseline,history_baseline ]\n\n\n\n\nopt_TL = keras.optimizers.Adagrad(learning_rate=1e-3)\nopt_BL = keras.optimizers.Adagrad(learning_rate=1e-3)\n\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n    return lr\n\nlr_metric_TL = get_lr_metric(opt_TL)\nlr_metric_BL = get_lr_metric(opt_BL)\n\nmetrics_TL = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        keras.metrics.AUC(name=\"auc\"),\n        lr_metric_TL\n    \n]\n\nmetrics_BL = [\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        keras.metrics.AUC(name=\"auc\"),\n        lr_metric_BL\n    \n]\n\n\n\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TL.compile(optimizer=opt_TL,loss='binary_crossentropy',metrics=metrics_TL)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TL_history = TL.fit(x = train_gen, validation_data = val_gen, epochs = 10 , batch_size = batch_size, callbacks= callbacks_TL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline = Sequential()\n\n#1st Convolutional Layer\nbaseline.add(Conv2D(filters=96, input_shape=(256,256,3), kernel_size=(11,11), strides=(4,4), padding='same'))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\nbaseline.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#2nd Convolutional Layer\nbaseline.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\nbaseline.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#3rd Convolutional Layer\nbaseline.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\n\n#4th Convolutional Layer\nbaseline.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\n\n#5th Convolutional Layer\nbaseline.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\nbaseline.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n#Passing it to a Fully Connected layer\nbaseline.add(Flatten())\n# 1st Fully Connected Layer\nbaseline.add(Dense(4096, input_shape=(32,32,3,)))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nbaseline.add(Dropout(0.5))\n\n#2nd Fully Connected Layer\nbaseline.add(Dense(4096))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\n#Add Dropout\nbaseline.add(Dropout(0.5))\n\n#3rd Fully Connected Layer\nbaseline.add(Dense(1000))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('relu'))\n#Add Dropout\nbaseline.add(Dropout(0.5))\n\n#Output Layer\nbaseline.add(Dense(1))\nbaseline.add(BatchNormalization())\nbaseline.add(Activation('sigmoid'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline.compile(optimizer=opt_BL,loss='binary_crossentropy',metrics=metrics_BL)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BL_history = baseline.fit(x = train_gen, validation_data = val_gen, epochs = 10 , batch_size = batch_size, callbacks= callbacks_baseline)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history = baseline_history\n\nplt.plot(plot_history.history['accuracy'])\nplt.plot(plot_history .history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history = TL_history\n\nplt.plot(TL_history.history['accuracy'])\nplt.plot(TL_history .history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non_pretrained_model = Sequential()\n# non_pretrained_model.add(Conv2D(32, (3, 3), input_shape=(224,224,3)))\n# non_pretrained_model.add(Activation('relu'))\n# non_pretrained_model.add(MaxPooling2D(pool_size=(2, 2)))\n\n# non_pretrained_model.add(Conv2D(32, (3, 3)))\n# non_pretrained_model.add(Activation('relu'))\n# non_pretrained_model.add(MaxPooling2D(pool_size=(2, 2)))\n\n# non_pretrained_model.add(Conv2D(64, (3, 3)))\n# non_pretrained_model.add(Activation('relu'))\n# non_pretrained_model.add(MaxPooling2D(pool_size=(2, 2)))\n\n# non_pretrained_model.add(Conv2D(64, (3, 3)))\n# non_pretrained_model.add(Activation('relu'))\n# non_pretrained_model.add(MaxPooling2D(pool_size=(2, 2)))\n\n\n\n# non_pretrained_model.add(Flatten())\n# non_pretrained_model.add(Dense(128))\n# non_pretrained_model.add(Activation('relu'))\n# non_pretrained_model.add(Dropout(0.5))\n# non_pretrained_model.add(Dense(1))\n# non_pretrained_model.add(Activation('sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# opt2 = keras.optimizers.Adam(learning_rate=1e-2)\n\n# non_pretrained_model.compile(optimizer=opt2,loss='binary_crossentropy',metrics=metrics)\n# batch_size = 128\n# non_pretrained_model.fit(x = train_gen, validation_data = val_gen,epochs = 50 , batch_size = batch_size, shuffle= True, callbacks = callbacks_scratch)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}