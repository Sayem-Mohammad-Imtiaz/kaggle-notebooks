{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport xgboost\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see if we have any null/NaN values in dataset\nprint(train.isnull().sum())\n\n#Plotting it using seaborn heatmap\nsns.heatmap(train.isnull(), yticklabels=False, cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preprocessing\n \n#converting to lower case\ntrain[\"review\"] = train[\"review\"].apply(lambda x: x.lower()) \n#removing everything instead of 0-9, a-z, A-Z\ntrain[\"review\"] = train[\"review\"].apply(lambda x: re.sub(\"[^0-9a-zA-Z]\",\" \", x))\n#removing html tags\nclean_html = re.compile('<.*?>')\ntrain[\"review\"] = train[\"review\"].apply(lambda x: re.sub(clean_html, \"\", x))\n#lemmatizing\ntrain[\"review\"] = train[\"review\"].apply(lambda x: lemmatizer.lemmatize(x))\n#removing extra white spaces\ntrain[\"review\"] = train[\"review\"].apply(lambda x: re.sub(\" +\",\" \", x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(use_idf = True,\n                             lowercase = True, \n                             strip_accents='ascii',\n                             stop_words=stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = vectorizer.fit_transform(train[\"review\"])\n\nlabel_encoding = {\n    \"positive\": 1,\n    \"negative\": 0\n}\n\nlabel_decoding = {\n    1: \"positive\",\n    0: \"negative\"\n}\n\ny = train[\"sentiment\"].map(label_encoding).to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### First I would like to go with MultinomialNB as it works very good for textual data."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB()\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(X_test)\n\nprint(\"Accuracy : {} %\".format(round(accuracy_score(y_test, y_pred)*100, 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Okay so the base model that was the MultinomialNB gives us an accuracy of ~ 87%\n#### Let's try another classifier, I'm going to use RandomForestClassifier as it works like a charm for any problem!"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf = RandomForestClassifier()\n\nclf_rf.fit(X_train, y_train)\n\ny_pred_rf = clf_rf.predict(X_test)\n\nprint(\"Accuracy : {} %\".format(round(accuracy_score(y_test, y_pred_rf)*100, 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's do some hyperparameter tuning and see if we can improve the accuracy further"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting hyper-parameters in RandomForestClassifier\n\n#Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n#Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n#Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n#Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n#Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n#Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n#Creating a random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using RandomizedSearchCV for selecting best hyper-parameters\n#On each iteration, the algorithm will choose a difference combination of the features\n#the benefit of a random search is that we are not trying every combination, \n#but selecting at random to sample a wide range of values.\n\n#n_iter = number of different combinations to try\n#cv = number of folds to use for cross validation\n\nrandomizedSearch = RandomizedSearchCV(estimator=clf_rf,\n                                     param_distributions=random_grid,\n                                     n_iter=10,\n                                     n_jobs=-1,\n                                     cv=3,\n                                     verbose=2,\n                                     random_state=42)\n\nrandomizedSearch.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Best parameters after performing RandomizedSearchCV\nprint(randomizedSearch.best_params_)\n\n#Getting the best estimator after performing RandomizedSearchCV\nrandomSearchModel = randomizedSearch.best_estimator_\n\ny_pred = randomSearchModel.predeict(X_test, y_test)\n\nprint(\"Accuracy : {} %\".format(round(accuracy_score(y_test, y_pred)*100, 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RandomForest was taking a long time when training, so i skipped it.\n\n#### Now I'm trying XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = xgboost.XGBClassifier()\n\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xgb = xgb.predict(X_test)\n\nprint(\"Accuracy : {} %\".format(round(accuracy_score(y_test, y_pred_xgb)*100, 4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n              \"max_depth\"        : [3, 4, 5, 6, 8, 10, 12, 15],\n              \"min_child_weight\" : [1, 3, 5, 7],\n              \"gamma\"            : [0.0, 0.1, 0.2 , 0.3, 0.4],\n              \"colsample_bytree\" : [0.3, 0.4, 0.5 , 0.7]}\n\nrandomXgb = RandomizedSearchCV(estimator=xgb,\n                               param_distributions=param_grid,\n                               n_iter=20,\n                               n_jobs=-1,\n                               verbose=2,\n                               cv=3,\n                               random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finalXGB = randomXgb.estimator\n\nprint(randomXgb.get_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xgb = finalXGB.predict(X_test)\n\nprint(\"Accuracy : {} %\".format(round(accuracy_score(y_test, y_pred_xgb)*100, 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So at last, MultinomialNB gave us highest accuracy of ~87%, so we will go with that!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}