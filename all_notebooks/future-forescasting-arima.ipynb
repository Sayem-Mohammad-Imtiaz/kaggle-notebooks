{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename inimport matplotlib.pyplot as plt\nfrom datetime import datetime\n%matplotlib inline filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom datetime import datetime\n%matplotlib inline\n\n# Allow several prints in one cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m')\ndata = pd.read_csv('../input/air-passengers/AirPassengers.csv',  index_col='Month',  parse_dates = ['Month'], date_parser = dateparse)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = data['#Passengers']\nts.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking month wise\nts[datetime(1949,1,1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check Data Year Wise\nts['1949']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Checking the stationarity**\n\nAssume we have time series data, for eg. stock prices (I'm highly innovative, i know) . What is stationary process?\n\nMean = constant over all intervals.\nVariance = constant over all intervals."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stationarity is defined using very strict criterion. However, for practical purposes we can assume the series to be stationary if it has constant statistical properties over time, ie. the following:\n\nConstant mean\nConstant variance\nAn autocovariance that does not depend on time.\nPlotting Rolling Statistics: We can plot the moving average or moving variance and see if it varies with time. By moving average/variance I mean that at any instant ‘t’, we’ll take the average/variance of the last year, i.e. last 12 months. But again this is more of a visual technique.\n\nDickey-Fuller Test: This is one of the statistical tests for checking stationarity. Here the null hypothesis is that the TS is non-stationary. The test results comprise of a Test Statistic and some Critical Values for difference confidence levels. If the Test Statistic is less than the Critical Value, we can reject the null hypothesis and say that the series is stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=12).mean()\n    rolstd = timeseries.rolling(window=12).std()\n\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    \n    print(dfoutput)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore, the time series is not stationary.\n\nLets understand what is making a TS non-stationary. There are 2 major reasons behind non-stationaruty of a TS:\n\n    **Trend – varying mean over time. For eg, in this case we saw that on average, the number of passengers was growing over time.\n    Seasonality – variations at specific time-frames. eg people might have a tendency to buy cars in a particular month because of pay increment or festivals.\n**\nThe underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented on this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back.\n\nTo eliminate trend we will apply a transformation that makes the values more or less same, ie. we use log\nOne way to eliminate trend (that doesn't always work)"},{"metadata":{},"cell_type":"markdown","source":"**One way to eliminate trend (that doesn't always work)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_log = np.log(ts)\nplt.plot(ts_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will deal with the noise by taking rolling mean i.e. smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_smooth = ts_log.rolling(window = 12).mean()\nplt.plot(ts_smooth, color = 'red')\nplt.plot(ts_log)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Case 1: No subtraction\nno_sub_ts = ts_smooth\nno_sub_ts.dropna(inplace = True)\n\n# Case 2: Yes subtraction\nsub_ts = ts_log - ts_smooth\nsub_ts.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform Dickey-Fuller on both\ntest_stationarity(no_sub_ts)\ntest_stationarity(sub_ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exponential weighted average instead of rolling average**"},{"metadata":{"trusted":true},"cell_type":"code","source":"expwighted_avg = ts_log.ewm(halflife=12).mean()\nplt.plot(ts_log)\nplt.plot(expwighted_avg, color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_ts_diff = ts_log-expwighted_avg\ntest_stationarity(exp_ts_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSince test statistic is lower than 1% critical value, it means that we are 99% sure that we have a stationary series.\n\nOther ways of eliminating trend and seasonality,\n\nDifferencing\nDecomposition\nDifferencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_ts_diff = ts_log-expwighted_avg\ntest_stationarity(exp_ts_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nSince test statistic is lower than 1% critical value, it means that we are 99% sure that we have a stationary series.\n\nOther ways of eliminating trend and seasonality,\n\n    Differencing\n    Decomposition\n\nDifferencing"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_diff = ts_log - ts_log.shift()\nts_diff.dropna(inplace = True)\ntest_stationarity(ts_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_diff_exp = ts_diff-ts_diff.ewm(halflife = 12).mean()\nts_diff_exp.dropna(inplace = True)\ntest_stationarity(ts_diff_exp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! We got confidence of 99%.\n\nDecomposition"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts_log)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(ts_log, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_log_decompose = residual\nts_log_decompose.dropna(inplace=True)\ntest_stationarity(ts_log_decompose)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Forecasting\nLets make model on the TS after differencing as it is a very popular technique. Also, its relatively easier to add noise and seasonality back into predicted residuals in this case. Having performed the trend and seasonality estimation techniques, there can be two situations:\n\nA strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as white noise. But this is very rare.\nA series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data.\nLet me give you a brief introduction to ARIMA. I won’t go into the technical details but you should understand these concepts in detail if you wish to apply them more effectively. ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n\nNumber of AR (Auto-Regressive) terms (p): AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\nNumber of MA (Moving Average) terms (q): MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\nNumber of Differences (d): These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.\nSelecting p, q, and d values\n\nAutocorrelation Function (ACF): It is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\nPartial Autocorrelation Function (PACF): This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_acf = acf(ts_diff, nlags=20)\nlag_pacf = pacf(ts_diff, nlags=20, method='ols')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**p = 2, q = 2\n\nNow AR, MA & ARIMA models for the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AR model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(ts_log, order=(2, 1, 0))  \nresults_AR = model.fit(disp=-1)  \nplt.plot(ts_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MA model\n\nmodel = ARIMA(ts_log, order=(0, 1, 2))  \nresults_AR = model.fit(disp=-1)  \nplt.plot(ts_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ARIMA model\n\nmodel = ARIMA(ts_log, order=(2, 1, 2))  \nresults_AR = model.fit(disp=-1)  \nplt.plot(ts_diff)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_diff)**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.Series(results_AR.fittedvalues, copy = True)\npreds_cumsum = preds.cumsum()\npreds_cumsum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_log = pd.Series(ts_log.ix[0], index=ts_log.index)\npreds_log = preds_log.add(preds_cumsum,fill_value=0)\npreds_log.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ARIMA = np.exp(preds_log)\nplt.plot(ts)\nplt.plot(preds_ARIMA)\nplt.title('RMSE: %.4f'% np.sqrt(sum((preds_ARIMA-ts)**2)/len(ts)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}