{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Use pandas to inspect tables on rossman dataset\n\nWe're starting from Sanyam Bhutani's Kaggle dataset made of fastai lesson 6v3 data (rossmann competition with all additional data). "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read all tables\nPATH=('../input/fastai-v3-lesson-6-rossmandataset/')\ntable_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']\ntables = [pd.read_csv(PATH+f'{fname}.csv', low_memory=False) for fname in table_names]\ntrain, store, store_states, state_names, googletrend, weather, test = tables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create a diagram showing possible tables relationships\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grab the list of fields for each table\ndicts = [{'src':n, 'field':df.columns} for n,df in zip(table_names,tables)]\npd.DataFrame(dicts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STEP1**: We've constructed a DataFrame (AKA table) using a list of dictionaries. This is useful because:\n+ we can ensure that all the data are \"aligned\" (ie: same number of columns and data type)\n+ each column is aligned using the dictionary key, so we can mix different type of values and deal with missing values\n+ we have column names our final *table*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample: mixing missing values\npd.DataFrame(\n    [{'src':n, 'field':df.columns} for n,df in zip(table_names,tables)] \n  + [{'src':'What???','note':'this should be removed'}]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STEP2**: flatten the `field` column, transforming a row containing a list of columns into an equivalent number of rows.\n\nNOTE: again a great suggestion from Simon: the `explode` (aka `flatten`) operator is not even mentioned in [Wes McKinney](https://www.oreilly.com/library/view/python-for-data/9781449323592/) official pandas book ;-) "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(dicts).explode('field')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **STEP3**: inline create dummy column. This *functional / immutable* approach is very compact and ensure that no changes will be applied to the original data. \n\nYou can obtain the same result writing, but to do so you'll need to write this in multiple lines and assigning intermediate result to variable (AKA the *mutable way*).\n```\ndf['isPresent']=True\ndf['myOtherField']=df.src + \":\" + df.field\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.assign(isPresent=True, myOtherField=lambda r: r.src + \":\" + r.field).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **STEP4:** group-by and unstack. This operation is similar to Excel's \"pivot table\".\n\n**IMPORTANT**: to prevent having hierarchical indices on the column we've selected the `['isPresent']`column. this means that we're acqually *unstacking* a Series, not a DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.assign(isPresent=True).groupby(['field','src'])['isPresent'].count().unstack().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP5: filter and sort `fields` in order to have more frequent first.\nWe're going to skip the fields where count is less than two because we cannot make any \"join\" on them.\nWe obtain this in \"two steps\":\n1. Compute the field order that we want: in this case we group by filed name and sort descending by count; moreover we'll filter out all the fields with count less than 2.\n2. Reindex the resulting DataFrame with the index found in the previous step.\n\nNOTE: Computing the field order, to be more clear, we've used the *immutable* rename operator."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute field order\nfield_by_count = (df.groupby(['field']).count() # Group by field and take the count\n                    .rename(columns={'src':'src_count'}) # Rename 'src' column \n                    ['src_count'] # Transform DataFrame into a Series\n                    .loc[lambda x: x>=2] # Filter our all\n                    .sort_values(ascending=False) # Sort values\n                 )\nfield_by_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**VERY IMPORTANT**: notice that `field_by_count` is **a Series, not a DataFrame**.\nI've preferred to work with a Series because we're going to focus on a single field (src_count) to apply our business logic (keep if greater than 1 and sort descending).\nThis *transformation from DataFrame to Series* happens when we've selected the field `['src_count']`.\n\n**FILTERING**: we've usewd the `iloc[function]` filter syntax in order to enforce an *immutable* approach. Another option to do the same could be: \n```\nfield_by_count = field_by_count[field_by_count>=2]\n```"},{"metadata":{},"cell_type":"markdown","source":"**NOTE:** On the previous cell I've shown a \"general\" approach that involves group-by and computations.\nIf we're interested in counting values only, pandas offer the Series method `.value_counts` that does the same thing in a more compact form."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute field order ()\nfield_by_count = (df['field'].value_counts() # Short form to say: give me a Series with the count of field.\n                    .loc[lambda x: x>=2] # Filter our all\n                    .sort_values(ascending=False) # Sort values\n                 )\nfield_by_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reorder columns\ndf.assign(isPresent=True).groupby(['field','src'])['isPresent'].count().unstack().reindex(field_by_count.index).head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP6: transform into boolean\nIn order to be more clear we can convert to boolean the result of count (count is always 1 if we group by `src` and `field` because we don't have fields with duplicate name)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform into boolean\ndf.assign(isPresent=True).groupby(['field','src'])['isPresent'].count().astype(bool).unstack().reindex(field_by_count.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP7: color in gray missing values in order to be more readable\n\nNOTE: the `style` step should always be the last one, and it's returning value is no more a DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final command with explantion\n(\n    df.assign(isPresent=True) # Add \"isPresent\" placeholder\n      .groupby(['field','src']).count() # Groupby\n      ['isPresent'] # Convert to series taking only this field\n      .astype(bool) # convert type\n      .unstack() # \"Pivot\" with respect to 'src'\n      .reindex(field_by_count.index) # Reoder and filder\n      .style.highlight_null(null_color='gray') # Change style of output\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# astore the intermediate relationship table\nrels = df.assign(isPresent=True).groupby(['field','src'])['isPresent'].count().astype(bool).unstack().reindex(field_by_count.index)\nrels.style.highlight_null(null_color='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Restacking again shows us the non null relations :-)\nrels.stack()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract unique realtionships\nrels_edges = rels.stack().reset_index().groupby('level_0')['src'].apply(tuple).unique()\nrels_edges","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remember the fields that made the join\ntt = (rels.stack() # Make the stack\n          .reset_index() # reset all comumn index\n          .rename(columns={'level_0':'field'}) # rename due to reset index\n          .groupby('field') # group by field and \n          ['src'] # transform into Series\n          .apply(tuple) # transform the 'src' that is a list of rows into a tuple in order to be indexed\n          .reset_index() # reset index in order to be able to proceeed\n          .groupby('src')['field'] # Regroup result by src\n          .apply(list) # Transform to list (this was a iterable)\n          .apply(lambda x: x[0] + ('...' if len(x)>1 else '')) # transform the list into a string with the first field and ellipsis\n     )\ntt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rels_edges = list(tt.index)\nrels_desc = tt.values\npd.DataFrame({'desc':rels_desc,'edges':rels_edges}) # Just to display!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMPORTANT**: we apply `tuple` and not `list` because tuple are immutable and hashable, so we can use `unique` operator to filter out duplicates."},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport numpy as np\n\nrels_edge_pairs = [list(itertools.combinations(re, 2)) for re in rels_edges]\nrels_edge_pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"edges_df = pd.DataFrame({'desc':rels_desc,'pairs':rels_edge_pairs}).explode('pairs')\nedges_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\n\n\ndot = graphviz.Digraph(comment='Tables relationships')\n\nfor c in rels.columns: dot.node(c)\nfor i,r in edges_df.iterrows():\n    if (r.desc=='file'): continue # Skip joining by file!\n    if (r.pairs == ('train','test')) or (r.pairs == ('test','train')): continue # Skip train / test rels\n    dot.edge(r.pairs[0],r.pairs[1],label=r.desc)\n        \ndisplay(dot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE**: this is a very nahive way of showing possible relationships, based only on the field name. This is the reason why I've filtered out the \"file\" field."},{"metadata":{},"cell_type":"markdown","source":"## External libraries: missingno to have an overview about missing values\nThnx to [Simon Grest](https://www.kaggle.com/simongrest) for this great suggestion: this library let's you quick figure out what are the columns with missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\nfor i,n in enumerate(table_names):\n    df = tables[i]\n    msno.matrix(df.sample(min(len(df),250)));\n    plt.title(n);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"googletrend","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot with pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot by week\ngoogletrend['date'] = googletrend['week'].apply(lambda x: x.split(' - ')[0])\ngoogletrend['date'] = pd.to_datetime(googletrend['date'])\n(googletrend.set_index('date')\n           .groupby('file') # Group by \"file/store\"\n           .trend #  Diveide into series\n           .plot(figsize=(20,10), title='Trend by week') # Plot the group content\n);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot by month\n(\n    googletrend.set_index('date') # Set date time index: thi is needed to time-resample\n               .groupby('file') # group values by file\n               .resample('M').mean() # Resample and take the mean for each period  \n               .swaplevel() # Swap index from (file,date) -> (date,file)\n               ['trend'] # Transform into a serie to avoid multiple level indices\n               .unstack() # Pivot and put \"file\" into columns\n               .plot(figsize=(20,10)) # Plot each column into a separate line\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}