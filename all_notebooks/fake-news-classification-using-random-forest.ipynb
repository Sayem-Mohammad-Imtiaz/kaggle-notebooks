{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Basic Imports"},{"metadata":{},"cell_type":"markdown","source":"Here, we import the basic packages like **pandas** and **numpy** for I/O and Linear Algebra. Link to the documentation of the packages can be found here:\n* [numpy](https://numpy.org/doc/stable/)\n* [pandas](https://pandas.pydata.org/docs/reference/index.html#api)\n* [os](https://docs.python.org/3/library/os.html)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Initialising the Fake News Dataset into a DataFrame**"},{"metadata":{},"cell_type":"markdown","source":"I tried Training using the full corpus but even Kaggle Kernels ran out of memory, so I've shortened both the corpus to only have 5000 instances of each class"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"fakedataset = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")\nfake = fakedataset[:5000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Initialising the Real News Dataset into a DataFrame**"},{"metadata":{"trusted":true},"cell_type":"code","source":"realdataset = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nreal = realdataset[:5000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Processing"},{"metadata":{},"cell_type":"markdown","source":"**Adding Classes to the Datasets**"},{"metadata":{},"cell_type":"markdown","source":"The dataset doesn't have any classes associated with it, so to enable Supervised Learning I've given a class of 1 to real news and 0 to fake news"},{"metadata":{"trusted":true},"cell_type":"code","source":"real[\"class\"] = 1\nfake[\"class\"] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Concatenating Text and Title into a Single Column (to improve performance)**"},{"metadata":{},"cell_type":"markdown","source":"Just Predicting with the Title/Headlines wouldn't give good results so, I've Concatenated both (Title and Text) into a single column of the individual dataframes. Also, I haven't tried to exploit the relationship between subjects so I've dropped those columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"real[\"text\"] = real[\"title\"] + \" \" + real[\"text\"]\nfake[\"text\"] = fake[\"title\"] + \" \" + fake[\"text\"]\n\nreal.drop([\"subject\", \"date\", \"title\"], axis = 1)\nfake.drop([\"subject\", \"date\", \"title\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a Single DataFrame**"},{"metadata":{},"cell_type":"markdown","source":"Joined both dataframes to just have a single dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = real.append(fake, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del real, fake","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**!! Download stopwords and punkt from nltk**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Performing basic pre-processing on the Corpus**"},{"metadata":{},"cell_type":"markdown","source":"Here, I've used the [Porter Stemmer](https://www.nltk.org/howto/stem.html) from the nltk module to stem the words. Also, I've converted everything into lowercase and removed stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nstopwords = nltk.corpus.stopwords.words('english')\nstemmer = nltk.PorterStemmer()\n\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count/(len(text) - text.count(\" \")), 3)*100\n\ndataset['body_len'] = dataset['text'].apply(lambda x: len(x) - x.count(\" \"))\ndataset['punct%'] = dataset['text'].apply(lambda x: count_punct(x))\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [stemmer.stem(word) for word in tokens if word not in stopwords]\n    return text\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Train, Test and Split Datasets**"},{"metadata":{},"cell_type":"markdown","source":"Here, I've used the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn to create random train and test subsets of the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX=dataset[['text', 'body_len', 'punct%']]\ny=dataset['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorising the Corpus using TfidfVectorizer**"},{"metadata":{},"cell_type":"markdown","source":"We, use the [TfidfVectoriser](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) from sklearn module to transform the matrix into a tf-idf representation. This is representation is commonly used in document classification and information retrieval "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['text'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['text'])\ntfidf_test = tfidf_vect_fit.transform(X_test['text'])\n\nX_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)\n\nX_train_vect.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing Necessary Packages from sklearn**"},{"metadata":{},"cell_type":"markdown","source":"We use the following functions for building our model:\n* [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) : A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n* [precision_recall_fscore_support](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) : To compute precision, recall, F-measure and support for each class\n* [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) : To compute the accuracy classification score\n* [matplotlib](https://matplotlib.org/api/index.html)\n* [seaborn](https://seaborn.pydata.org/api.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import accuracy_score as acs\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training The Model"},{"metadata":{},"cell_type":"markdown","source":"We build a RandomForestClassifier instance and then train it on our training subset and then evaluate it over our test subset. And then make a confusion matrix using the [heatmap function](https://seaborn.pydata.org/generated/seaborn.heatmap.html#seaborn.heatmap) from seaborn"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n\nrf_model = rf.fit(X_train_vect, y_train)\n\ny_pred = rf_model.predict(X_test_vect)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label=1, average='binary')\nprint('Precision: {} / Recall: {} / F1-Score: {} / Accuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), round(acs(y_test,y_pred), 3)))\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nclass_label = [0, 1]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}