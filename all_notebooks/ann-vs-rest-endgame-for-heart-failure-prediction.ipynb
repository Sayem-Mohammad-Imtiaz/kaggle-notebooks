{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a><h1 style='background:#6daa9f; border:3; color:white'><center> Machine Learning For Heart Failure Prediction: ANN Endgame </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"<center><img \nsrc=\"https://d1nakyqvxb9v71.cloudfront.net/wp-content/uploads/2020/01/heart-health-tips-animation-thumbnail.gif\" width=\"900\" height=\"900\"></img></center>\n\n<br>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a><h1 style='background:#7ad16d; border:0; color:black'><center> Table of contents </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"1. [Introduction](#1)\n1. [Data cleaning, exploration and preprocessing](#2)\n1. [Basic model building](#3)\n1. [Comparison: ANN vs rest](#4)\n1. [Acknowledgements](#5)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a><h1 style='background:#7ad16d; border:0; color:black'><center> Introduction </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"Cardiovascular disease (CVD) is the most common cause of morbidity and mortality among men and women globally. Heart failure is a commong CVD condition. The Heart Foundation defines Heart failure as \"A condition where your heart isnâ€™t pumping as well as it should be.\" The signs and symptoms of heart failure commonly include shortness of breath, excessive tiredness and leg swelling. \n\nCommon causes of heart failure:\n    1. Coronary artery disease\n    2. Myocardial infraction (heart attack)\n    3. High blood pressure\n    4. Arterial fibrillation\n    5. Cardiomyopathy\n    6. Valvular heart disease \n    7. Infections "},{"metadata":{},"cell_type":"markdown","source":"> **Objective**: In this notebook, I will build a series of classifier model and compare that with ANN."},{"metadata":{},"cell_type":"markdown","source":"**Variables in the dataset:**\n\n* **Age**: Age of the patient\n* **Anaemia**: If the patient had the haemoglobin below the normal range\n* **Creatinine_phosphokinase**: The level of the creatine phosphokinase in the blood in mcg/L\n* **Diabetes**: If the patient was diabetic\n* **Ejection_fraction**: Ejection fraction is a measurement of how much blood the left ventricle pumps out with each contraction\n* **High_blood_pressure**: If the patient had hypertension\n* **Platelets**: Platelet count of blood in kiloplatelets/mL\n* **Serum_creatinine**: The level of serum creatinine in the blood in mg/dL\n* **Serum_sodium**: The level of serum sodium in the blood in mEq/L\n* **Sex**: The sex of the patient\n* **Smoking**: If the patient smokes actively or ever did in past\n* **Time**: It is the time of the patient's follow-up visit for the disease in months\n* **Death_event**: If the patient deceased during the follow-up period"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a><h1 style='background:#7ad16d; border:0; color:black'><center> Data cleaning, exploration and preprocessing  </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"# Loading the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom keras.layers import Dense, BatchNormalization, Dropout, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading data\ndata = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prevalence of outcome event\nsns.set_theme(context='poster')\nplt.figure(figsize=(10,7))\nplt.title('Disease status \\n (Survived (0), Death (1))', fontsize=20)\ncols= [\"#7cd16d\",\"#eb2009\"]\nsns.countplot(x= data[\"DEATH_EVENT\"], palette= cols)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding missing values\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation between the variables in the study\ndata.corr().style.background_gradient(cmap='Spectral').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What the data tell us:\n<br>\n    1. Serum creatiine (r=0.29) is postively and sodium (-0.20) is negatively correlated with risk of death.\n    1. Interestingly lifestyle factors such as smoking (-0.01) and diabetes (0.00) were either not correlated or weakly correlated with risk of deaths. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Age distribution\nsns.set_theme(context='poster')\nplt.figure(figsize=(20,20))\nplt.title('Distribution of age', color=\"Green\",fontsize=40)\nDays_of_week=sns.countplot(x=data['age'])\nDays_of_week.set_xticklabels(Days_of_week.get_xticklabels(), rotation=40, ha=\"right\",fontsize=20)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxen and swarm plot of some non binary features.\nfeature = [\"age\",\"creatinine_phosphokinase\",\"ejection_fraction\",\"platelets\",\"serum_creatinine\",\"serum_sodium\", \"time\"]\nfor i in feature:\n    plt.figure(figsize=(8,8))\n    sns.swarmplot(x=data[\"DEATH_EVENT\"], y=data[i], color=\"black\", alpha=0.5)\n    sns.violinplot(x=data[\"DEATH_EVENT\"], y=data[i], palette=cols)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What the data tell us:\n<br>\n    1. Outlier observations are detected for the variables above.\n    1. This might be due to measurement error or due to some factors unique to the study population."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_theme(context='poster')\nplt.figure(figsize=(15,10))\nplt.title('Kernel density plot for age based on follow up (time)', color=\"Green\",fontsize=30)\nsns.kdeplot(x=data[\"time\"], y=data[\"age\"], hue =data[\"DEATH_EVENT\"], palette=cols)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{},"cell_type":"markdown","source":"The major steps invovled in preprocessing:\n<br>\n    1. Outlier detection and correction.\n    1. If necessary feature engineering for the dependent and independent variables.\n    1. Dividing the dataset for training and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining the target X and Y variable\nX=data.drop([\"DEATH_EVENT\"],axis=1)\ny=data[\"DEATH_EVENT\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Standard scaler features of the dataset\ncol_names = list(X.columns)\ns_scaler = preprocessing.StandardScaler()\nX_df= s_scaler.fit_transform(X)\nX_df = pd.DataFrame(X_df, columns=col_names)   \nX_df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Examining the scaled features\nsns.set_theme(context='poster')\nplt.figure(figsize=(20,15))\nplt.title('Examining the scaled features (of columns)', color=\"Green\",fontsize=30)\n#colours =[\"#774571\",\"#b398af\",\"#f1f1f1\" ,\"#afcdc7\", \"#6daa9f\"]\nsns.violinplot(data = X_df,palette = 'Set2')\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spliting test and training sets\nX_train, X_test, y_train,y_test = train_test_split(X_df,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a><h1 style='background:#7ad16d; border:0; color:black'><center> Basic model building </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"We build our model using artificial nural network,which involves the following steps:\n1. Initialising the ANN\n1. Defining the added layers\n1. Compiling the ANN\n1. Train the ANN"},{"metadata":{},"cell_type":"markdown","source":"Following building the ANN model we will compare the results with similar models build using:\n    1. Catboost\n    1. Random Forest\n    1. Xgboost\n    1. Logistic regression\n    1. KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Initialising the NN\nmodel = Sequential()\n\n# 2. layers\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# 3. Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# 4. Train the ANN\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 500, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_accuracy = np.mean(history.history['val_accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_accuracy', val_accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we show the testing results as well as the classification report and the confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting from the test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)\nnp.set_printoptions()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac_ann = accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print the classification test results\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a><h1 style='background:#7ad16d; border:0; color:black'><center> Comparison: ANN vs rest </center></h1>\n"},{"metadata":{},"cell_type":"markdown","source":"**1. Logistic regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix & accuracy score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nmodel = LogisticRegression()\n\n#Fit the model\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nmylist = []\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\n# accuracy score\nacc_logreg = accuracy_score(y_test, y_pred)\n\nmylist.append(acc_logreg)\nprint(cm)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Random forrest classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the optimum number of n_estimators\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#Figure\nsns.set_theme(context='poster')\nplt.figure(figsize=(15,10))\nplt.title('Number of estimators', color=\"Green\",fontsize=30)\nplt.plot(list(range(10,30)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the RandomForest Classifier on the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 15, criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set results\ny_pred = classifier.predict(X_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nacc_randomforest = accuracy_score(y_test, y_pred)\nmylist.append(acc_randomforest)\nprint(cm)\nprint(acc_randomforest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Xboost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,30,1):\n    classifier = XGBClassifier(n_estimators = estimators, max_depth=12, subsample=0.7)\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    list1.append(accuracy_score(y_test,y_pred))\n##Figure\nsns.set_theme(context='poster')\nplt.figure(figsize=(15,10))\nplt.title('Number of estimators', color=\"Green\",fontsize=30)\nplt.plot(list(range(10,30,1)), list1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 10, max_depth=12, subsample=0.7)\nclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_xgboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_xgboost)\nprint(cm)\nprint(ac_xgboost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Catboost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac_catboost = accuracy_score(y_test, y_pred)\nmylist.append(ac_catboost)\nprint(cm)\nprint(ac_catboost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix for prediction results \ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = 'magma', annot = True, annot_kws = {'size':15})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Summary of all model classifiers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest','xgboost','catboost'],\n    'Score': [acc_logreg, \n              acc_randomforest, ac_ann, ac_xgboost,ac_catboost\n              ]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Figure for all the classifier models\nplt.rcParams['figure.figsize']=15,8 \nsns.set_style(\"darkgrid\")\nax = sns.barplot(x=models.Model, y=models.Score, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 25)\nplt.xticks(fontsize = 12, horizontalalignment = 'center', rotation = 8)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a><h1 style='background:#7ad16d; border:0; color:black'><center> Acknowledgements </center></h1>"},{"metadata":{},"cell_type":"markdown","source":"The model I have developed has slightly underperformed compared to other models developed by fellow Kaggler [@karnikakapoor](https://www.kaggle.com/karnikakapoor/heart-failure-prediction-ann) and [@midouazerty](https://www.kaggle.com/midouazerty/heart-disease-using-8-machine-learning-algorithms). I will continuousuly updating and training the model - hoping to get a better score in subsequent iteraions."},{"metadata":{},"cell_type":"markdown","source":"> Last not the least, I would like to thank the fellow Kaggler [@karnikakapoor](https://www.kaggle.com/karnikakapoor/heart-failure-prediction-ann) and [@midouazerty](https://www.kaggle.com/midouazerty/heart-disease-using-8-machine-learning-algorithms) for providing the template for building a ANN model. I further compared the ANN approach with other traditional and ML based models using the template provided by [@midouazerty](https://www.kaggle.com/midouazerty/heart-disease-using-8-machine-learning-algorithms). "},{"metadata":{},"cell_type":"markdown","source":"Despite the model's results here, key driver's of cardiovascular health lies indisputably in our lifestyle habits (e.g. sedentary habits, consumption of junk and energy dense food, smoking etc.), changes in these habits are essential for reducing the risk of cardiovascular events (e.g. HF)."},{"metadata":{},"cell_type":"markdown","source":"<center><img \nsrc=\"https://cdn.dribbble.com/users/1277402/screenshots/4180449/heartwalk.gif\" width=\"900\" height=\"900\"></img></center>\n\n<br>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}