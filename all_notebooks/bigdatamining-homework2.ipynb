{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nclass DGIM(object):\n    def __init__(self, filepath, windowsize=1000, autoUpdate=True):\n        '''\n        Parameters:\n            filepath: the path of the stream file\n            windowsize: the length of the current window\n            autoUpdate: wheter update the count after each coming bit if True\n        '''\n        import math\n        self.filepath = filepath\n        self.windowsize = windowsize\n        self.buckets = []\n        self.counts = [0] * int(2*math.log(self.windowsize))\n        self.cnt1bits = 0             # The number of bit 1 in the current window\n        self.curTs = 0                # The current relevant timestamp equaling to the real timestamp mod windowsize\n        self.autoUpdate = autoUpdate  # Update cnt1bits after each coming bit if True\n        \n    def pushDown(self):\n        '''\n        If the last bit of the oldest bucket equals to the current relevant timestamp, namely out of the window, remove this bucket.\n        '''\n        if self.buckets == []:\n            return\n        if self.buckets[-1][0] == self.curTs-1:\n            self.counts[self.buckets[-1][1]] -= 1\n            self.buckets.pop()\n            \n    def checkBuckets(self):\n        '''\n        Check the buckets of each size from the newest to the oldest, merging 2 buckets if 3 of them have the same size.\n        '''\n        for i in range(len(self.counts)):\n            if self.counts[i] == 3:\n                self.counts[i] = 1\n                self.counts[i+1] += 1\n                self.buckets[i+1][1] += 1\n                self.buckets.pop(i+2)\n        \n    def forward(self):\n        '''\n        Receive a new bit from the stream and build a new bucket if it's bit 1.\n        '''\n        c = self.file.read(1)\n        if c == '\\t':\n            c = self.file.read(1)\n        if c == '':\n            return -1\n        self.curTs = (self.curTs + 1) % self.windowsize\n        self.pushDown()\n        if c == '1':\n            self.counts[0] += 1\n            self.buckets.insert(0,[self.curTs-1, 0])\n        self.checkBuckets()\n        \n    def update(self):\n        '''\n        Count the number of bit 1 in the current window via the buckets\n        '''\n        self.cnt1bits = 0\n        for i in range(len(self.counts)-1):\n            if self.counts[i+1] == 0:\n                self.cnt1bits += 2**(i-1) * self.counts[i]\n                break\n            else:\n                self.cnt1bits += 2**i * self.counts[i]\n    \n    def work(self):\n        '''\n        Traverse the whole stream file.\n        '''\n        self.file = open(self.filepath, 'r')\n        while(True):\n            if self.forward() == -1:\n                break\n            if self.autoUpdate:\n                self.update()\n        self.file.close()\n    \n    def autoUpdateEnable(self):\n        '''\n        Switch on the auto-update.\n        '''\n        self.autoUpdate = True\n    \n    def autoUpdateDisable(self):\n        '''\n        Switch off the auto-update.\n        '''\n        self.autoUpdate = False\n    \n    def query(self):\n        if not self.autoUpdate:\n            self.update()\n        return self.cnt1bits\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = DGIM(\"../input/coding2/stream_data.txt\", 1000)\n%time d.work()\nd.query()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = DGIM(\"../input/coding2/stream_data.txt\", 1000)\nd.autoUpdateDisable()\n%time d.work()\nd.query()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nclass plainCount(object):\n    def __init__(self, filepath, windowsize=1000):\n        '''\n        Parameters:\n            filepath: the path of the stream file\n            windowsize: the length of the current window\n        '''\n        self.filepath = filepath\n        self.windowsize = windowsize\n        self.buffer = []\n        self.cnt1bits = 0\n        \n    def forward(self):\n        '''\n        Receive a new bit from the stream and count if it's bit 1. Discard the oldest bit.\n        '''\n        c = self.file.read(1)\n        if c == '\\t':\n            c = self.file.read(1)\n        if c == '':\n            return -1\n        self.buffer.append(c)\n        self.cnt1bits += (c == '1')\n        if len(self.buffer) > self.windowsize:\n            lc = self.buffer.pop(0)\n            self.cnt1bits -= (lc == '1')\n\n    def work(self):\n        '''\n        Traverse the whole stream file.\n        '''\n        self.file = open(self.filepath, 'r')\n        while(True):\n            if self.forward() == -1:\n                break\n        self.file.close()\n    \n    def query(self):\n        return self.cnt1bits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = plainCount(\"../input/coding2/stream_data.txt\", 1000)\n%time p.work()\np.query()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Cost Comparison**\n\n|        | DGIM | DGIM* | Plain |  \n| :----: | :--: | :---: | :---: |  \n| Time/ms  | 247 | 92.6 | 41.7 |  \n| Space/bits | 8 | 8 | 1000 |  \n\nIn this table, the \"Time\" row indicates how long the algorithm finishs processing the whole \"stream_data.txt\" file, and the \"Space\" row indicates how many bits are used to store the counting info. DGIM* disables the auto-update of counts after each coming bit, which is acceptable because we are not querying all the time. It is obvious that DGIM requires less space at the cost of time performance, and the cost can be limited if auto-update is disabled.\n\n**Accuracy Comparison**\n\n|        | DGIM | Plain |  \n| :----: | :--: | :---: |  \n| Counts of 1  | 508  | 391 |  \n| Error Rate | 29.9% | 0% | \n\nIn this table, the \"Counts of 1\" row indicates the number of '1' in the last 1000 characters of the \"stream_data.txt\" file, and DGIM keeps it error rate below 50% as the theoretical proof.","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom functools import partial\nfrom tqdm import tqdm\nfrom sklearn.metrics import jaccard_score\nclass LocSenHash(object):\n    def __init__(self):\n        return\n        \n    def load(self, filename):\n        self.docs = np.loadtxt(filename, dtype=np.uint32, delimiter=',', skiprows=1)[:, 1:]\n        self.docNum, self.shingleNum = self.docs.shape\n    \n    def hashReset(self, prime, mod):\n        self.mod = mod\n        self.hashFunc = []\n        for i in range(self.sigNum):\n            a = np.random.randint(1, prime)\n            b = np.random.randint(1, prime)\n            self.hashFunc.append(partial(self.sigHash, a=a, b=b, prime=prime, mod=mod))\n    \n    def sigHash(self, x, a, b, prime, mod):\n        return ((a * x + b) % prime) % mod\n\n    def getSignature(self, prime, mod, sigNum):\n        self.sigNum = sigNum\n        self.hashReset(prime, mod)\n        self.hashTable = np.full((self.sigNum, self.docNum), self.mod, dtype=np.uint32)\n        for i in tqdm(range(self.shingleNum)):\n            for j, hashFunc in enumerate(self.hashFunc):\n                hashValue = hashFunc(i)\n                nzi = np.nonzero(self.docs[:, i])\n                self.hashTable[j, nzi] = np.minimum(self.hashTable[j, nzi], hashValue)\n    \n    def lsHash(self, bandNum, rowNum, prime, mod):\n        self.bandNum = bandNum\n        self.rowNum = rowNum\n        self.lshResult = []\n        a = np.random.randint(1, prime)\n        b = np.random.randint(1, prime)\n        for i in tqdm(range(self.bandNum)):\n            temp = {}\n            hashValue = []\n            for j in range(self.docNum):\n                hashValue.append(hash(self.hashTable[i*self.rowNum:(i+1)*self.rowNum, j].tobytes()))\n            for j, v in enumerate(hashValue):\n                if v in temp.keys():\n                    temp[v].append(j)\n                else:\n                    temp[v] = [j]\n            self.lshResult.append(temp)\n    \n    def getSimDoc(self, doci, num):\n        self.simCount = {}\n        for j in tqdm(range(len(self.lshResult))):\n            for key, value in self.lshResult[j].items():\n                if doci in value:\n                    for i in value:\n                        if i in self.simCount.keys():\n                            self.simCount[i] += 1\n                        else:\n                            self.simCount[i] = 1\n        self.similarity = sorted(self.simCount.items(), key=lambda item: item[1], reverse=True)\n        self.simDoc = []\n        for i in range(1, num+1):\n            self.simDoc.append(self.similarity[i][0])\n    \n    def checkSim(self):\n        print(\"Similar Doc\\t\\tJaccard Score\\n\")\n        for doc in self.simDoc:\n            print(\"{}\\t\\t\\t\\t{}\".format(doc, jaccard_score(self.docs[0,:], self.docs[doc, :])))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:49:36.8835Z","iopub.execute_input":"2021-05-20T15:49:36.88386Z","iopub.status.idle":"2021-05-20T15:49:38.059033Z","shell.execute_reply.started":"2021-05-20T15:49:36.883829Z","shell.execute_reply":"2021-05-20T15:49:38.057771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = LocSenHash()\nl.load(\"../input/coding2/docs_for_lsh.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:49:39.652531Z","iopub.execute_input":"2021-05-20T15:49:39.653019Z","iopub.status.idle":"2021-05-20T15:52:22.846119Z","shell.execute_reply.started":"2021-05-20T15:49:39.652986Z","shell.execute_reply":"2021-05-20T15:52:22.845036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l.getSignature(2**31-1, 2**17-1, 100)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:52:22.848071Z","iopub.execute_input":"2021-05-20T15:52:22.848508Z","iopub.status.idle":"2021-05-20T16:03:23.257247Z","shell.execute_reply.started":"2021-05-20T15:52:22.848448Z","shell.execute_reply":"2021-05-20T16:03:23.256023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l.lsHash(10, 10, 2**31-1, 2**17-1)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:04:00.560561Z","iopub.execute_input":"2021-05-20T16:04:00.560976Z","iopub.status.idle":"2021-05-20T16:04:23.590583Z","shell.execute_reply.started":"2021-05-20T16:04:00.560943Z","shell.execute_reply":"2021-05-20T16:04:23.589667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"l.getSimDoc(0, 30)\nl.checkSim()","metadata":{},"execution_count":null,"outputs":[]}]}