{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport scipy.stats as stats\nimport sys,os,glob\nfrom functools import partial\nimport cv2\n\nimport torchvision\nfrom torchvision import datasets, transforms\n\nimport torch\nfrom torch.utils.data import Dataset,DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport torch.nn as nn\nfrom sys import float_info\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,MultiplicativeNoise,ColorJitter,\n    Transpose, ShiftScaleRotate, Blur,GaussianBlur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise,GaussNoise,IAAAffine, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomSizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\nfrom albumentations.pytorch import ToTensorV2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAGS = {}\nFLAGS['batch_size'] = 64\nFLAGS['num_workers'] = 4\nFLAGS['learning_rate'] = 0.01\nFLAGS['num_cores'] = 1\nFLAGS['num_epochs'] = 2\nFLAGS['img_size'] = (256,256)\n\nROOT = \"../input/cassava-leaf-disease-merged/\"\ntrainpath = ROOT + 'train/'\ndf = pd.read_csv(\"../input/cassava-leaf-disease-merged/merged.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nclass Model(nn.Module):\n    def __init__(self, ovnum = 10):\n        super(Model, self).__init__()\n        self.vgg = torchvision.models.vgg16_bn(pretrained=False).features[0:17]\n        weight = self.vgg[0].weight\n        self.vgg[0] = nn.Conv2d(5, 64, kernel_size=(7, 7), stride=(1, 1), padding=(1, 1), bias=False)\n        self.vgg[0].weight = torch.nn.Parameter(torch.cat([weight,weight[:,:2,:,:]],dim=1))\n        #self.mid = nn.Sequential(nn.Conv2d(256,512, 3, 1, 1,bias=False),nn.BatchNorm2d(512),nn.ReLU())\n        self.out =   nn.Sequential(nn.Conv2d(256,5, 1, 1, 0,bias=False),nn.Softmax2d())\n        self.ovout = nn.Sequential(nn.Conv2d(256,ovnum, 1, 1, 0,bias=False),nn.Softmax2d())\n    def forward(self, x):\n        x = self.vgg(x)\n        #x = self.mid(x)\n        x_out = self.out(x)\n        x_out_oc = self.ovout(x)\n        y = F.interpolate(x_out, size=FLAGS['img_size'], mode=\"bilinear\", align_corners=False)\n        y_ov = F.interpolate(x_out_oc, size=FLAGS['img_size'], mode=\"bilinear\", align_corners=False)\n        return y, y_ov","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(ovnum= 10)\nmodel.load_state_dict(torch.load('../input/training-iic-segmentations-cassava/model_epoch0.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_transforms():\n    return Compose([            \n            #Transpose(p=0.5),\n            #HorizontalFlip(p=0.5),\n            #VerticalFlip(p=0.5),\n            #RandomSizedCrop(min_max_height=((16, 24)),height=28,width=28,p=0.20),\n            #MultiplicativeNoise(p=1.0,multiplier=(0.8,1.2),elementwise=False),\n            #GaussianBlur(p=1.0,blur_limit=(3,7)),\n            #Cutout(num_holes=8, max_h_size=8, max_w_size=8, p=1.0),\n            ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.125,p=1.0),\n        ], p=1.)\n          \ndef tensor_transform():\n    return Compose([ToTensorV2()], p=1.)\n\ndef random_affine(img,device,min_rot=None, max_rot=None, min_shear=None,max_shear=None, min_scale=None, max_scale=None):\n    assert (len(img.shape) == 3)\n    a = np.radians(np.random.rand() * (max_rot - min_rot) + min_rot)\n    shear = np.radians(np.random.rand() * (max_shear - min_shear) + min_shear)\n    scale = np.random.rand() * (max_scale - min_scale) + min_scale\n    affine1_to_2 = np.array([[np.cos(a) * scale, - np.sin(a + shear) * scale, 0.],\n                           [np.sin(a) * scale, np.cos(a + shear) * scale, 0.],\n                           [0., 0., 1.]], dtype=np.float32)  # 3x3\n\n    affine2_to_1 = np.linalg.inv(affine1_to_2).astype(np.float32)\n\n    affine1_to_2, affine2_to_1 = affine1_to_2[:2, :], affine2_to_1[:2, :]  # 2x3\n    affine1_to_2, affine2_to_1 = torch.from_numpy(affine1_to_2).to(device), torch.from_numpy(affine2_to_1).to(device)\n\n    img = perform_affine_tf(img.unsqueeze(dim=0), affine1_to_2.unsqueeze(dim=0))\n    img = img.squeeze(dim=0)\n\n    return img, affine1_to_2, affine2_to_1\n\ndef perform_affine_tf(data, tf_matrices):\n  # expects 4D tensor, we preserve gradients if there are any\n\n    n_i, k, h, w = data.shape\n    n_i2, r, c = tf_matrices.shape\n    assert (n_i == n_i2)\n    assert (r == 2 and c == 3)\n\n    grid = F.affine_grid(tf_matrices, data.shape)  # output should be same size\n    data_tf = F.grid_sample(data, grid,\n                          padding_mode=\"zeros\")  # this can ONLY do bilinear\n\n    return data_tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class torchdatasets_forIIC(Dataset):\n    def __init__(self,df,transform,noisetransform,device='cpu'):\n        self.df = df\n        self.transform = transform\n        self.noisetransform = noisetransform\n        self.device = device\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path_list = self.df.image_id.values\n        path = trainpath + path_list[idx]\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, FLAGS['img_size'])\n        #image = tfcrop()(image=image)[\"image\"]\n        assert(image.shape == (*FLAGS['img_size'],3))\n        org_image=np.array(image, dtype='float32')/255.0        \n        proc_image=np.array(image,dtype=np.float32)\n\n        #change orignal image to sobel image \n        img_gray = cv2.cvtColor(proc_image, cv2.COLOR_RGB2GRAY)\n        dx = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=3)\n        dy = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=3)\n        xmax = np.max(abs(dx))\n        ymax = np.max(abs(dy))\n        sobel_img_x = np.float32(dx[:,:,None]/xmax)\n        sobel_img_y = np.float32(dy[:,:,None]/ymax)\n        data_target = np.concatenate([org_image,sobel_img_x,sobel_img_y],axis=2)\n        data_target = self.transform(image=data_target)[\"image\"]\n        \n        #change augment image to sobel image \n        augimage = self.noisetransform(image=proc_image)[\"image\"]\n        aug_gray = cv2.cvtColor(augimage, cv2.COLOR_RGB2GRAY)\n        dx_tf = cv2.Sobel(aug_gray, cv2.CV_64F, 1, 0, ksize=3)\n        dy_tf = cv2.Sobel(aug_gray, cv2.CV_64F, 0, 1, ksize=3) \n        sobel_img_x_tf = np.float32(dx_tf[:,:,None]/xmax)\n        sobel_img_y_tf = np.float32(dy_tf[:,:,None]/ymax)\n        org_image_tf = self.noisetransform(image=org_image)[\"image\"]\n        data_other = np.concatenate([org_image_tf,sobel_img_x_tf,sobel_img_y_tf],axis=2)\n        data_other = self.transform(image=data_other)[\"image\"]\n        data_other,affine1_to_2,affine2_to_1=random_affine(data_other, min_rot=-30, max_rot=30, min_shear=-10,\n                                                         max_shear=10, min_scale=0.8, max_scale=1.2, device=self.device)\n        return affine2_to_1, data_target, data_other","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noisetransform = get_train_transforms()\ntransform = tensor_transform()\ntest_ds = torchdatasets_forIIC(df=df,transform=transform,noisetransform=noisetransform,device='cpu')\ntest_loader = torch.utils.data.DataLoader(test_ds, batch_size=4,\n                          shuffle=False, num_workers=4,drop_last=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = 4\nrow = 3\nplt.figure(figsize=(col*5,row*5/0.90))\nnum = 0 \nmodel.eval()\n\nfor affine2_to_1,data,gdata in test_loader:\n    if num==col*row:\n        break\n    #print(data.shape,gdata.shape)\n    A =model(data)[0].detach().numpy()\n    for j in range(4):\n        if num==col*row:\n            break\n        image = A[j]\n        newimg = np.zeros(image[0][0].shape)\n        for i in range(5):\n            piece = image[i]\n            piece[piece>0.95]=i\n            newimg = newimg + piece\n        num += 1\n        plt.subplot(row, col, num)\n        plt.imshow(data[j][:3].permute(1,2,0))\n        #plt.show()\n        num += 1\n        plt.subplot(row, col, num)\n        plt.imshow(newimg)#.transpose(1,2,0))\n        #plt.show()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}