{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Webserver Log File Analysis Template\n\nInitial steps at creating a pipeline for log file analysis for finding insights on the website's traffic, users, locations, search engine crawlers, referring sites, consumed content, performance, and anything else that can be gleaned. \n\nThis first step is the prototype of a process of convering a log file to an efficient format on disk (Apache Parquet), and then to read it into an efficient DataFrame with optimized datatypes. \n\nIn this example we convert a 3.3 GB text file to a 258 MB `parquet` file, which is later read into a 342 MB DataFrame. The total time can vary between three to five minutes, depending on the system used. This notebook ran the full process in 204 seconds. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install advertools","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.display.max_columns = None\nimport re\nimport os\nimport time\nfrom tqdm import tqdm\n\nfrom dataset_utilities import value_counts_plus\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample lines from the log file"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lsSh /kaggle/input/web-server-access-logs/access.log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head -n 4 /kaggle/input/web-server-access-logs/access.log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset \n\nThe dataset was downloaded from Harvard's Dataverse and contains logs from an Iranian ecommerce site (zanbil.ir): \n\n\nZaker, Farzin, 2019, \"Online Shopping Store - Web Server Logs\", \nhttps://doi.org/10.7910/DVN/3QBYB5, Harvard Dataverse, V1"},{"metadata":{},"cell_type":"markdown","source":"# Log Format \nThis approach assumes the common log format and/or the combined one, which are two of the most commonly used. Eventually other formats can be incorporated. We start with the below regular express taken from: \n\n[Regular Expressions Cookbook](https://learning.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch07s12.html)  \nby Jan Goyvaerts, Steven Levithan  \nPublisher: O'Reilly Media, Inc.\nRelease Date: August 2012\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is a minor bug in this regex, it misses the last field. I'll fix this soon. \n\ncommon_regex = '^(?P<client>\\S+) \\S+ (?P<userid>\\S+) \\[(?P<datetime>[^\\]]+)\\] \"(?P<method>[A-Z]+) (?P<request>[^ \"]+)? HTTP/[0-9.]+\" (?P<status>[0-9]{3}) (?P<size>[0-9]+|-)'\ncombined_regex = '^(?P<client>\\S+) \\S+ (?P<userid>\\S+) \\[(?P<datetime>[^\\]]+)\\] \"(?P<method>[A-Z]+) (?P<request>[^ \"]+)? HTTP/[0-9.]+\" (?P<status>[0-9]{3}) (?P<size>[0-9]+|-) \"(?P<referrer>[^\"]*)\" \"(?P<useragent>[^\"]*)'\ncolumns = ['client', 'userid', 'datetime', 'method', 'request', 'status', 'size', 'referer', 'user_agent']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Approach\n\n* Loop through the lines of the input log file one by one. This ensures minimal memory consumption. \n* For each line, check it against the regular expression, and process it: \n  * Match: append the matched line to a `parsed_lines` list\n  * No match: append the non-matching line to the `errors_file` for later analysis\n* Once `parsed_lines` reaches 250,000 elements, convert the list to a DataFrame and save it to a `parquet` file in the `output_dir`. Clear the list. This also ensures minimal memory usage, and the 250k can be tweaked if necessary.\n* Read all the files of the `output_dir` with `read_parquet` into a pandas DataFrame. This function handles reading all the files and combines them. \n* Optimize the columns by using more efficient data types, most notably the pandas categorical type.\n* Write the DataFrame to a single file, for more convenient handling, and with the more efficient datatypes. This results in even faster reading.\n* Delete the files in `output_dir`.\n* Read in the final file with `read_parquet`.\n* Start analyzing.\n"},{"metadata":{},"cell_type":"markdown","source":"> ## Create a destinatoin directory where output files will be stored\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%mkdir parquet_dir","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The `logs_to_df` function"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport re\nimport pandas as pd\n\n\ndef logs_to_df(logfile, output_dir, errors_file):\n    with open(logfile) as source_file:\n        linenumber = 0\n        parsed_lines = []\n        for line in tqdm(source_file):\n            try:\n                log_line = re.findall(combined_regex, line)[0]\n                parsed_lines.append(log_line)\n            except Exception as e:\n                with open(errors_file, 'at') as errfile:\n                    print((line, str(e)), file=errfile)\n                continue\n            linenumber += 1\n            if linenumber % 250_000 == 0:\n                df = pd.DataFrame(parsed_lines, columns=columns)\n                df.to_parquet(f'{output_dir}/file_{linenumber}.parquet')\n                parsed_lines.clear()\n        else:\n            df = pd.DataFrame(parsed_lines, columns=columns)\n            df.to_parquet(f'{output_dir}/file_{linenumber}.parquet')\n            parsed_lines.clear()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Times will vary from system to system, and I will use the approximate values, so when you read this, you will likely see slightly different numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"%time logs_to_df(logfile='/kaggle/input/web-server-access-logs/access.log', output_dir='parquet_dir/', errors_file='errors.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The whole process just described took around 2.5 minutes. \n\nActually we are now ready to start analysis, as we have the parquet files that can be read. But we will optimize them even more. \n\nChecking the number of resulting parsing errors:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wc errors.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time logs_df = pd.read_parquet('parquet_dir/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the whole directory takes about nine seconds. We now check the size of the resulting directory on disk:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!du -sh parquet_dir/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"257 ÷ 3,300 = 0.07. \n\nThe resulting file is 7% the size of the original. \n\nLet's see how much memory it takes: "},{"metadata":{"trusted":true},"cell_type":"code","source":"logs_df.info(show_counts=True, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"711 MB. We now remove the files in `parquet_dir` and optimize the datatypes and use more efficient ones. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%rm -r parquet_dir/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logs_df['client'] = logs_df['client'].astype('category')\ndel logs_df['userid']\nlogs_df['datetime'] = pd.to_datetime(logs_df['datetime'], format='%d/%b/%Y:%H:%M:%S %z')\nlogs_df['method'] = logs_df['method'].astype('category')\nlogs_df['status'] = logs_df['status'].astype('int16')\nlogs_df['size'] = logs_df['size'].astype('int32')\nlogs_df['referer'] = logs_df['referer'].astype('category')\nlogs_df['user_agent'] = logs_df['user_agent'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logs_df.info(verbose=True, show_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The file was reduced further from 711 to 342 MB. (342 ÷ 711 = 0.48 of the original size)\n\nWe now save it to a single file, and read again."},{"metadata":{"trusted":true},"cell_type":"code","source":"%time logs_df.to_parquet('logs_df.parquet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lshS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time logs_df = pd.read_parquet('logs_df.parquet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now reading the file took almost half the previous time. Sorry again for the imprecise numbers!\n\nWe are now ready to start analyzing."},{"metadata":{"trusted":true},"cell_type":"code","source":"logs_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logs_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Page/response size analysis - Clustering pages by size\n\nHow many \"small\" pages do we have? And how many \"large\"? What about medium size pages? \nHow do we even define those labels? \n\nSummarizing a large number of values is not always easy. One way to do it is by using KMeans clustering on a list of numbers. We specify the number of clusters, and the algorithm finds the a set of `K` (number) points, where each is the mean of a group of points. Hence \"K-Means\".\n\nWe can also get other statistics for each cluster (min, max, std, and so on), then visualize those centers, and see how many points we have in that cluster. We can do this interactively, changing the number of clusters to see the optimal number based on the application. In this case, we are stil exploring, so this is basically allowing us to identify pages with a small, medium, large, etc. size, without us giving those values explicitly. We are discovering them this way.\n\nFirst instantiate the model with the desired number of clusters: "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nk = 5\nkmeans = KMeans(k)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the model to the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time kmeans.fit(logs_df[['size']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the `cluster_centers_` These are the means of the groups/clusters of points that were discovered based on the given `k`."},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(kmeans.cluster_centers_.round(0).flatten())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a table to get other statistics about each cluster, using the `labels_` attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df = logs_df.groupby(kmeans.labels_)['size'].describe().sort_values('mean').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df.style.background_gradient(subset=['count'], cmap='cividis').format({'mean': '{:,.0f}', 'count': '{:,.0f}'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems we have eight million responses in the first (low) cluster. The average response size in bytes is 3,576 for this group, and we can also see the minimum, maximum, and other statistics for this, and other clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter(cluster_df, \n                 x='mean', y='count',\n                 size=[5]*len(cluster_df),\n                 size_max=15,\n                 log_y=True,\n                 hover_data=['min', 'max', 'std'],\n                 title=f'<b>Page distribution by response size ({k} clusters).</b><br>Points represent the average page size for a cluster of pages.',\n                 labels={'mean': 'Average page size (bytes)',\n                         'count': \"Number of pages in cluster\"}, \n                )\nfig.data[0].hovertemplate = '<b>Average page size (bytes): %{x:,.0f}</b><br><br>Number of pages in cluster: %{y:,.0f}<br><br>min: %{customdata[0]:,.0f}<br>max: %{customdata[1]:,.0f}<br>std: %{customdata[2]:,.0f}<extra></extra>'\nfor minimum in cluster_df['min']:\n    fig.add_vline(x=minimum, line={'width': 1})\nfig.layout.font.size = 14\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking how variable the page size is, for the same page, taking the home page as an example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"logs_df[logs_df['request'].eq(\"/\")][['request', 'size']].value_counts().reset_index().rename(columns={0: 'count'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the home page was returned with 2,298 different sizes, many of which were zero."},{"metadata":{},"cell_type":"markdown","source":"# Analyzing Organic Search Traffic\n\nReferer traffic that is from search engines can provide valuable information on that traffic. \nMost importantly, the query parameters of the URLs of those referers contain that information. \n\nWe can easily filter for those URLs, and then parse their different elements to get some information on the type of traffic they are sending:"},{"metadata":{"trusted":true},"cell_type":"code","source":"goog_organic = logs_df[logs_df['referer'].str.contains('google\\.com/search')]['referer']\ngoog_organic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import advertools as adv\ngoog_url_df = adv.url_to_df(goog_organic)\ngoog_url_df = goog_url_df.rename(columns={col: col.replace('query_', '') for col in goog_url_df.columns})\ngoog_url_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sort the parameters by the frequency of their use: "},{"metadata":{"trusted":true},"cell_type":"code","source":"(goog_url_df.iloc[:, 7:]\n .notna()\n .mean()\n .sort_values(ascending=False)[:30]\n .to_frame()\n .rename(columns={0: '%'})\n .style.format('{:.1%}'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ipywidgets as widgets\n\ndef count_column_values(col_name, show_top=10, sort_others=False):\n    series = goog_url_df[col_name].str.split('@@').explode()\n    print(f'Times {series.name} was provided: {series.count():,}')\n    print(f'Number of sessions: {len(goog_url_df):,}')\n    print(f'Number of unique {series.name}s: {series.nunique():,}')\n#     print(f'{series.name}s per page: {series.count()/len(goog_url_df):.2f}')\n    display(value_counts_plus(series, sort_others=sort_others, dropna=False, show_top=show_top))\n\nwidgets.interact(count_column_values,\n                 col_name=goog_url_df.select_dtypes('object').columns,\n                 show_top=widgets.IntSlider(min=1, max=50, value=10));\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}