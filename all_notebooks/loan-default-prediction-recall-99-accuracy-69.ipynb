{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/loan-default-prediction/Default_Fin.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Predicted variable distribution\ndf['Defaulted?'].value_counts().plot(kind='bar')\nplt.title('Figure 1: Not default vs Default')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicted variable distribution per employment type\nsns.barplot(x='Employed', y='Defaulted?', data=df)\nplt.title('Default distribution by Employment type')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Bank Balance distribution per default status:\nplt.figure(figsize=(12,6))\nplt.hist(df[df['Defaulted?']==1]['Bank Balance'], histtype='step', bins=100, label='Defaulted')\nplt.hist(df[df['Defaulted?']==0]['Bank Balance'], histtype='step', bins=100, label='Not Defaulted')\nplt.title('Figure 3: Bank Balance distribution by Default status')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Annual Salary distribution per default status:\nplt.figure(figsize=(12,6))\nplt.hist(df[df['Defaulted?']==1]['Annual Salary'], histtype='step', bins=100, label='Defaulted')\nplt.hist(df[df['Defaulted?']==0]['Annual Salary'], histtype='step', bins=100, label='Not Defaulted')\nplt.title('Figure 4: Annual Salary distribution by Default status')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Annual Salary versus Bank balance per default type\nplt.figure(figsize=(18,8))\nsns.scatterplot(x='Annual Salary', y='Bank Balance', hue='Defaulted?', data=df, size='Employed', style='Employed')\nplt.title('Figure 5: Annual Salary versus Bank balance per default types')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EDA Conclusions: \n1. From Figure 5, we can see that most defaulted cases appears in the upper part of the scatter plots. The upper part represent customers whose bank balance is relatively large compared to their annual salary, meaning they may have taken on larger debt in proportion with their salary than they should. This is consistent with a reality-based observation: if one take on debt more than they can afford, they are more likely to default.\n2. In Figure 5, the bigger markers - representing unemployed customer - gather in the left side of the chart. This indicates that most unemployed customers have a lower range of annual salary. Despite the lower salary range, their bank balance distribution is similar to the employed group. Therefore we can deduce that, compared to the employed group, there is a higher proportion of unemployed customers borrowing more than they should. Therefore, statistically unemployed customer is more likely to default. This hypothesis is also supported by Figure 2, where we can see the Unemployed customer group has a higher default mean of IQR.\n\nOverall we can see that a tree-based algorithm or support vector machine will do well in this problem.","metadata":{}},{"cell_type":"markdown","source":"# 2. Model Selection","metadata":{}},{"cell_type":"code","source":"##Split data:\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(columns=['Index', 'Defaulted?'])\ny = df['Defaulted?']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As tree-based model is non-parameter, it's not neccessary to do any scaling. The varibles are also coded in the right (numerical) format, so I'll go ahead and perform cross validation with hyper parameter tuning. \n\nBut before doing hyperparameter tuning, I'm curious if random forest will be better than gradient boosted tree. I also want to use a cost-sensitve SVM model as a baseline against these 2 tree based model, so I'll compare both of them by plotting the learning curve to see which one is more likely to overfit.","metadata":{}},{"cell_type":"markdown","source":"**Plot learning curve of Gradient Boosting Classifier and Random Forest Classifier, against SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.model_selection import learning_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None, scoring='recall',\n                        n_jobs=None, train_sizes=[0.2, 0.4, 0.6, 0.8]):\n    \"\"\"\n    Generate the test and training learning curve\n\n    Parameters\n    ----------\n    estimator : estimator instance\n\n    title : strTitle for the chart.\n\n    X :Training vector\n\n    y : Target relative to ``X`` for classification or regression;\n\n    axes : Axes object that is used for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        \n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the trainingsets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.(default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    axes.set_title(title)\n    if ylim is not None:\n        axes.set_ylim(*ylim)\n    axes.set_xlabel(\"Training examples\")\n    axes.set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs,\n                       train_sizes=train_sizes, return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plot learning curve\n    axes.grid()\n    axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes.legend(loc=\"best\")\n\n    return plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define 3 types of models:\ngbc = GradientBoostingClassifier()\nrfc = RandomForestClassifier()\nsvc = make_pipeline(StandardScaler(), SVC(class_weight={0:1, 1:100})) ##The class_weight parameters is a must as we are telling \n##the model to focus on training class 1 by giving it higher weight.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compare learning curve with recall as the metrics**","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n\n#GBC\ntitle = \"Learning Curves (Gradient Boosting Classifier)\"\nestimator = gbc\nplot_learning_curve(estimator, title, X_train, y_train, axes=axes[0], ylim=(0.2, 1.01),\n                    cv=5)\n\n#RFC\ntitle = r\"Learning Curves (Random Forest Classifier)\"\nestimator = rfc\nplot_learning_curve(estimator, title, X_train, y_train, axes=axes[1], ylim=(0.2, 1.01),\n                    cv=5)\n\n#SVC\ntitle = r\"Learning Curves (Support Vector Classifier)\"\nestimator = svc\nplot_learning_curve(estimator, title, X_train, y_train, axes=axes[2], ylim=(0, 1.01),\n                    cv=5)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compare learning curve using accuracy as the metrics**","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n\ntitle = \"Learning Curves (Gradient Boosting Classifier)\"\nestimator = gbc\nplot_learning_curve(estimator, title, X_train, y_train, axes=axes[0], ylim=(0.7, 1.01),\n                    cv=5, scoring='accuracy')\n\ntitle = r\"Learning Curves (Random Forest Classifier)\"\nestimator = rfc\nplot_learning_curve(estimator, title, X_train, y_train, axes=axes[1], ylim=(0.7, 1.01),\n                    cv=5, scoring='accuracy' )\n\ntitle = r\"Learning Curves (Support Vector Classifier)\"\nestimator = svc\nplot_learning_curve(estimator, title, X_train, y_train, axes=axes[2], ylim=(0.6, 1.01),\n                    cv=5, scoring='accuracy' )\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With larger training examples, the gap between training score and cross-validation score of GBC becomes smaller, while RFC's stay almost the same. As our training size is quite large (7000), I'll prefer the Gradient Boosting Classifer. \n\nNoted that the Support Vector Classifier does not overfit, which is quite normal as this model only relies on some vector machine.\n\nConsidering a default prediction excercise, recall is a more important metrics (busines-wide). I'll definitely go for a cost-sensitive support vector classifier. \n","metadata":{}},{"cell_type":"markdown","source":"# 3. Hyper-parameter tuning","metadata":{}},{"cell_type":"markdown","source":"**Hyper-parameter tuning for Gradient Boosting Classifer**","metadata":{}},{"cell_type":"markdown","source":"Support Vector Classifier should be the optimal solution if 'recall' is the main metrics. However, since I'm so invested in the Trees, I'll tune the GBC to see if its recalls can improve. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define parameteres grid\n#There are only 3 variables (employed, bank balance and annual salary), hence max_features is not that meaningful to tune. \n#For max_depth, I still want to tune it as it's a relatively important hyper-parameters for ensemble tree model. \nparams = {\n   'learning_rate' : [0.1, 0.3, 0.5, 0.7],\n    'n_estimators': [100, 200, 300, 400, 500],\n    'subsample': [0.4, 0.6, 0.8],\n    'max_depth': [2, 3],\n    'min_samples_leaf' :[10, 50, 100],\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Perform Randomize search on training set, using 2 scoring metrics\nclf = RandomizedSearchCV(gbc, param_distributions=params, n_iter=20, scoring =['accuracy', 'recall'], cv=5, \n                         refit='recall', random_state=10, return_train_score=True)\nresults = clf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot Random Search Results\ndef plot_random_search_results(results):\n    '''\n    Input:\n    \n    results: A randomized search cv object, after fitting with data\n    \n    Return: Chart visualising the train and test score of the Randomized search process.\n    '''\n    plt.figure(figsize=(12,5))\n    plt.plot(results.cv_results_['mean_train_accuracy'], label = 'Mean Train Accuracy')\n    plt.plot(results.cv_results_['mean_test_accuracy'], label = 'Cross-Val Accuracy')\n    plt.plot(results.cv_results_['mean_train_recall'], label = 'Mean Train Recall')\n    plt.plot(results.cv_results_['mean_test_recall'], label = 'Cross-Val Recall')\n\n\n    plt.annotate('Model with best recall', xy=(results.best_index_, 0.58), xytext=(results.best_index_ + 0.05, 0.7),\n            arrowprops=dict(facecolor='blue', shrink=0.15, width=2))\n\n    plt.title('Train and Test Score in each interations')\n    plt.xlabel('Score')\n    plt.ylabel('RandomSearch Iterations')\n    plt.legend(loc='best')\n    return plt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_random_search_results(results)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#I'm supposed to tune the parameter for SVC, however it took a lot of time to tune while current result I got is already above 95%, so I decided to skip this step.","metadata":{}},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"markdown","source":"**Evaluate with Gradient Boosting Trees**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, plot_roc_curve\n\ny_pred = results.best_estimator_.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy score of best model on the test data is: %f' %accuracy)\nrecall = recall_score(y_test, y_pred)\nprint('Recall score of best model on the test data is: %f' %recall)\n\n#Print AUC Curve\nfig, ax = plt.subplots(1, 1, figsize=(10,6))\nplot_roc_curve(results.best_estimator_, X_test, y_test, ax = ax) \nplt.title('ROC of best GBC model')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Evaluate with Support Vector Machine**","metadata":{}},{"cell_type":"code","source":"svc.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_svc = svc.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred_svc)\nprint('Accuracy score of best model on the test data is: %f' %accuracy)\nrecall = recall_score(y_test, y_pred_svc)\nprint('Recall score of best model on the test data is: %f' %recall)\n\n#Print AUC Curve\nfig, ax = plt.subplots(1, 1, figsize=(10,6))\nplot_roc_curve(svc, X_test, y_test, ax = ax) \nplt.title('ROC of SVC model')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"We can see that while tree-based model perform better on average if accuracy is the main metrics; SVC is the best choice in this case where recall is used to score. \nThis makes me wonder, if we combine both of this model, we will get a best of both world results. ","metadata":{}}]}