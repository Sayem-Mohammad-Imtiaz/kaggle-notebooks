{"cells":[{"metadata":{"_uuid":"7c66e88d582c7a0e56396054a8d51906a6fe1749"},"cell_type":"markdown","source":"# Pima indianas Diabetes Database\n\nHello, today i am going to  estimate the liklihood of person to be diabetic, this is my first MachineLearning real application away from the theory.\n \n"},{"metadata":{"_uuid":"dbd847bbd0377d2506a4df122578c26ed6e0624f"},"cell_type":"markdown","source":"## Prepare , load the data \n\nfirst things first, import the modules needed"},{"metadata":{"trusted":true,"_uuid":"ab394d483f877119e59458c0421648644e79dcee"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n# disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7258199f40e72c346b3e93298efc828da45a16d8"},"cell_type":"code","source":"filename = '../input/diabetes.csv'\ndata=pd.read_csv(filename)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcae7a838cd297f673d748daec8fbfa8a9a6858c"},"cell_type":"markdown","source":"let's take a very quick look at our data"},{"metadata":{"trusted":true,"_uuid":"6bfbd2fb0cdc7eecfbcdf42f6183afb41e3d738b"},"cell_type":"code","source":"print(data.columns) # to know all the features(variables) we got in our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f15bc34b41afdcbd6b34c461d8719e6030ff49c2"},"cell_type":"code","source":"print(data.head()) #the first 5 rows","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33a2f7955d8c54beba2a9fe1370b04143ee71f90"},"cell_type":"markdown","source":" ## First things first: analysing the outcome\n \n by the outcome we mean whether the person is diabetic or no (1 = yes; 0 = no)\n\ndescriptive statistics summary :\n\n \n "},{"metadata":{"trusted":true,"_uuid":"0d7881a24f1d97d508af09e97637b3c964f572ee"},"cell_type":"code","source":"data['Outcome'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3da9c2a77babd0a0fcde5f087a67f4e790fd18f4"},"cell_type":"markdown","source":"now the histogram of the outcome : \n"},{"metadata":{"trusted":true,"_uuid":"7fb506897dad4bc395402124c6d712622b7b575c"},"cell_type":"code","source":"data['Outcome'].hist(figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04080068fb1cea191d60a689fc2043d15e0dd619"},"cell_type":"markdown","source":"we can easily see that  the Non-Diabetic persons is more than the diabetics by what it seems the half\nthat's mean accuracy is no more fine way to mesure how well our models doing.\n"},{"metadata":{"_uuid":"955a00dab388e10e0009c3830cfdba17b3b90e42"},"cell_type":"markdown","source":"#  ' Diabetes ' feature Relationships !!!!\n\n**Diabetes** love Aged,ppl with high level of Glucose  \" based from what I learned from high School\", \nSo let's start with them, by that i mean there relation with Diabetes.\n\n"},{"metadata":{"trusted":true,"_uuid":"baed329a56d5dee22319fabc14aa5f0c1878f5d5"},"cell_type":"code","source":"\n#borrowed from my friend Ayoub Benaissa.\ndef plot_diabetic_per_feature(data, feature):\n    grouped_by_Outcome = data[feature].groupby(data[\"Outcome\"])\n    diabetic_per_feature = pd.DataFrame({\"Sick\": grouped_by_Outcome.get_group(1),\n                                        \"Not Sick\": grouped_by_Outcome.get_group(0),\n                                        })\n    hist = diabetic_per_feature.plot.hist(bins=60, alpha=0.6)\n    hist.set_xlabel(feature)\n    plt.show()\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8171d79f3cce7c69081d85cd8ee28bd01b88ed"},"cell_type":"markdown","source":"let's start with the age :"},{"metadata":{"trusted":true,"_uuid":"753519b61d0e93f0c1e8f55f7b5daedc99264849"},"cell_type":"code","source":"plot_diabetic_per_feature(data, \"Age\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72e20483a500d9a764db3dbf0c81436adcf55c20"},"cell_type":"markdown","source":"*Intersting*, as u see, The older the persons,The higher the number of diabetics.\nofc that's based from the dataset ' my doctor friend told me there's 2 types of Diabetes, so maybe this data concern the one who target the olders\"\n"},{"metadata":{"trusted":true,"_uuid":"37631ea4af6acdb725d75e0a2e463365be960f6e"},"cell_type":"code","source":"plot_diabetic_per_feature(data, \"Glucose\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c00c4b768b81a4a3d8d9365147ab968469b85900"},"cell_type":"markdown","source":"Same ... the more Glucose you have to more likley to have Diabetes, i guess that will be pretty obvious for you  if you know bit of biology.\nWe notice the odd 0 information, let's confirm: \n\n"},{"metadata":{"trusted":true,"_uuid":"be75d8e7c8023d1ae0bb7ad4161cb4767fe51c1a"},"cell_type":"code","source":"print(data[\"Glucose\"].min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10d9700b06290d55656de5c0f084ff9abc632d8b"},"cell_type":"markdown","source":"that true, I don't think that a person can have 0 Glucose and still alive, so i'll delete it later + It also may effect the accuracy of the model.\n\n\nWhat about the **BMI** ( it's basically the weight of the person with kg) :\n"},{"metadata":{"trusted":true,"_uuid":"1113bb66e59683bc2498db312fc9537d577e1ca0"},"cell_type":"code","source":"plot_diabetic_per_feature(data, \"BMI\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b2ccf5579855f04a3edb2e67a629712e585e123"},"cell_type":"markdown","source":"very reasonable results, same notes from the 2 above, we can also notice the 0 odd info so we'll delete it late for the same reasons."},{"metadata":{"_uuid":"7729af4a6a718160a10bd1eb3d60bdbed198b214"},"cell_type":"markdown","source":"# Think beyond the box\nlet's think beyond the box, how is that? \n    with :\n* Correlation matrix\n* Scatter plots between the most correlated variables\n"},{"metadata":{"_uuid":"ccfb8c551b86ad62e591e14923ca3184cacc2ab6"},"cell_type":"markdown","source":"**Correlation matrix  : **\n"},{"metadata":{"trusted":true,"_uuid":"229b0cb31c5eff3df9de84d1eca803c05d87aa84"},"cell_type":"code","source":"import seaborn as sns #the librery we'll use for the job xD\n\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, cbar=True, annot=True, square=True, vmax=.8);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58e1e0a8244d238e610a253a2aa9dca22f8e0ad0"},"cell_type":"markdown","source":"Alright, let's break this matrix down to some few notes : \n* Glucose, Age and  BMI are the most Correlated features with the 'Outcome'\n* Bloodpressure, SkinThikness have tiny Correlation with the outcome, hummm !\n* check how the SkinThikness and BMI Correlated, make me think of rolling it out since mose of the fat ppl tends to have thicc skin\n* Age with Pregnancies are the most Correlated features\n* Insulin with Glucuse ' BIOLOGY  :) \"\n* DiabetesPedigreeFunction bit Correlated with most of them ' I am not sure with feature really mean\"\n* finnaly SkinThikness with Insulin, that's odd !"},{"metadata":{"_uuid":"29a9e66cb22529392f44429ec2edbb43bfa4d3a0"},"cell_type":"markdown","source":"### Scatter plots between 'Outcome' and correlated variables"},{"metadata":{"trusted":true,"_uuid":"0927210d408764f20878b40a43232d78d23c27dd"},"cell_type":"code","source":"sns.set()\ncols = ['Pregnancies','Glucose','BloodPressure','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\nsns.pairplot(data[cols], size = 2.5)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5d630112ffbdb1dc0b1ac86a1e5d44bf7be0b29"},"cell_type":"markdown","source":"opss! that's a huge amount of graphs to analyse but let's carry on :\n* bloodpressure and age tend to have a relation and that's kinda obviose since most of aged ppl have bloodpressure\n* Glucose and insulin have very strong relation makes me think about deleting the inslin futuere\n* in pregnancy/age we notice some kind of a liniar line in the right bottom \n\nthat's it, no need to repeat the past notes ' even tho i did '\n\n"},{"metadata":{"_uuid":"b15f59352dc5ece90cee59eabb624de5e29a1b67"},"cell_type":"markdown","source":"# Outliers \nOutliers is also something that we should be aware of. Why? Because outliers can markedly affect our models.\n\nlet's see what we can do :"},{"metadata":{"trusted":true,"_uuid":"9a8bdbeac7815fdfad04e7739cc9c5b58bcf2bda"},"cell_type":"code","source":"data.min()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddcde78bf7707a752cd300923bce29b6ff3e9a3c"},"cell_type":"markdown","source":"It's okay to have 0 Pregnancies, for the rest, we'll delete the rows containing the 0 values:\n"},{"metadata":{"trusted":true,"_uuid":"63e2b043c9f38a678c5a2a63c05d0804d655672f"},"cell_type":"code","source":"data = data.drop(data[data['Glucose'] == 0].index)\ndata = data.drop(data[data['SkinThickness'] == 0].index) # even it will be deleted xD\ndata = data.drop(data[data['BloodPressure'] == 0].index) #same\ndata = data.drop(data[data['BMI'] == 0].index)\ndata = data.drop(data[data['Insulin'] == 0].index)\n\nprint(data.min()) # let's check\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9fedf43b8dd08be5983b55158342a049415b2fa"},"cell_type":"markdown","source":"*excellent*, NEXT !!"},{"metadata":{"_uuid":"77e96df489c07a80bf1595b113a5d549585300df"},"cell_type":"markdown","source":"# Preparing the data/Selecting a model\n\nsince our data is only about 700 row of data, I'll go with k-fold cross validation (better that test/train in accuracy, but take more  computation time)"},{"metadata":{"trusted":true,"_uuid":"1847335fb9e7b59b4749f558268c2296038795d0"},"cell_type":"code","source":"#just the libreries we need\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, make_scorer,f1_score, precision_score, recall_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#preparing the data\ncols = ['Pregnancies','Glucose','DiabetesPedigreeFunction','Insulin','BMI','Age']\n\nY=data['Outcome']\n#rescaledX = StandardScaler().fit_transform(data[cols])\n#X=pd.DataFrame(data = rescaledX, columns= cols)\nX=data[cols]\n\n# I deleted BloodPressure and Skinthikness\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y, random_state = 25, test_size = 0.2)\n\n\n\n\nsvm1 = svm.SVC(kernel='linear')\nsvm2 = svm.SVC(kernel='rbf') \nlr = LogisticRegression()\nrf = RandomForestClassifier()\nknn=KNeighborsClassifier()\nmodels = {\"Logistic Regression\": lr,\"Random Forest\": rf, \"svm linear\": svm1 , \"svm rbf\": svm2,\"KNeighborsClassifier\": knn }\nl=[]\nfor model in models:\n    l.append(make_pipeline(Imputer(),  models[model]))\n#Finally get the cross-validation scores\ni=0\n\nfor Classifier in l:    \n    accuracy = cross_val_score(Classifier,X_train,Y_train,scoring='accuracy',cv=10)\n    print(\"===\", [*models][i] , \"===\")\n    print(\"accuracy = \",accuracy)\n    print(\"accuracy.mean = \", accuracy.mean())\n    print(\"accuracy.variance = \", accuracy.var())\n    i=i+1\n    print(\"\")\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29f7b76eb4a795553a18b4a2bfdbafbc26813bee"},"cell_type":"markdown","source":"our outcome (1 and 0) isn't balanced, but not very much so I think it's okay to use accuracy to defince the best models then compare them by F1 score\n\nnotes:\n* I split the data into test/train sets , I did the cv on the train test then I'll pick the best models and test them in the test set ( overfitting avoiding      level:99999 )  and compare them by f1 score / recall / precision\n* Logistic Regression got the higher accuracy mean and  low variance \n* Random Forest and  svm linear  both seems to be good models\n* svm rbf get around 0 variance but I don't actually care since it has pretty low accuracy.mean (checking its table all the values are less than 0.7) \n\nI'll go with  Logistic Regression,  Random Forest and  svm linear:"},{"metadata":{"trusted":true,"_uuid":"c96b82300b40a2ea4cc171fa12a0c175771e6c39"},"cell_type":"code","source":"\nlr = LogisticRegression()\nlr.fit(X_train,Y_train)\npredictions = lr.predict(X_test)\nsns.heatmap(confusion_matrix(Y_test, predictions), annot=True, cmap=\"YlGn\")\nplt.title(' LogisticRegression ')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nprint(\"the f1 score for Logistic Regression is :\",(f1_score(Y_test, predictions, average=\"macro\")))\nprint(\"the precision score is :\",(precision_score(Y_test, predictions, average=\"macro\")))\nprint(\"the recall score is :\",(recall_score(Y_test, predictions, average=\"macro\")))   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"251f0e3b9871b209579ed2e38ef5d21bedb890e6"},"cell_type":"code","source":"\nrf = RandomForestClassifier()\nrf.fit(X_train,Y_train)\npredictions = rf.predict(X_test)\nsns.heatmap(confusion_matrix(Y_test, predictions), annot=True, cmap=\"YlGn\")\nplt.title(' Random Forest Classifier ')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nprint(\"the f1 score for Random Forest Classifier is :\",(f1_score(Y_test, predictions, average=\"macro\")))\nprint(\"the precision score is :\",(precision_score(Y_test, predictions, average=\"macro\")))\nprint(\"the recall score is :\",(recall_score(Y_test, predictions, average=\"macro\")))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fac8ad8731a11e78b59de5a1574791416e8daf66"},"cell_type":"code","source":"svm = svm.SVC(kernel='linear')\nsvm.fit(X_train,Y_train)\npredictions = svm.predict(X_test)\nsns.heatmap(confusion_matrix(Y_test, predictions), annot=True, cmap=\"YlGn\")\nplt.title(' SVM kernel(linear) ')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nprint(\"the f1 score for SVM linear is :\",(f1_score(Y_test, predictions, average=\"macro\")))\nprint(\"the precision score is :\",(precision_score(Y_test, predictions, average=\"macro\")))\nprint(\"the recall score is :\",(recall_score(Y_test, predictions, average=\"macro\")))   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47a268c5f71e57df03e748f220dca17077c8868b"},"cell_type":"markdown","source":"Random Forest and support vector machine (linear) both have pretty good f1 score but I'll go with the **Random forest** since it have  higher **fscore** and lower ** false positive values**, and it can be more tuned later.\n\n\n**thanks for reading  and to the next version/kernels **\n\n\n"},{"metadata":{"trusted":true,"_uuid":"68ec576f077a892b13fc756c862ad402e4387401"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcf5ed62236ddcb7e55d47c0bed2b67bf3f87c45"},"cell_type":"code","source":"\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bde3a9e78f6c219e7737bd1690e57d7bf8001993"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}