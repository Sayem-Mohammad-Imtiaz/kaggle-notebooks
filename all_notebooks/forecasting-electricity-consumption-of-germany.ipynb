{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Forecasting hourly electricity consumption of Germany"},{"metadata":{},"cell_type":"markdown","source":"Electricity grid and market have become increasingly challenging to operate and maintain in the recent years. In particular, one of the main responsibilities of transmission system operators and aggregators consists in maintaining balance between production and consumption. With the development and spread of renewable energy sources, production has become more intermittent which requires even more effort to maintain the balance.\n\nOne of the underlying task of maintaining grid balance, it to forecast the consumption. In this analysis, we train and test a few models to forecast total german load, on an hourly basis, with a lead time of 24 hours.\n\nThe data was retrieved from [ENTSO-E Transparency Platform](https://transparency.entsoe.eu/), which provides access to electricity generation, transportation, and consumption data for the pan-European market.\n\nThis notebook follows a structure similar to [this nice tutorial](https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost) from [Rob Mulla](https://www.kaggle.com/robikscube). The main adaptations are the following :\n* it applies to German load instead of PJM data covering US east region,\n* it includes additional features such as holidays and lag features,\n* a linear model, and a random forest are used as baselines in addition to the XGB model,\n* the final XGB model is finetuned with some grid search CV."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\n\nimport holidays\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (\n    StandardScaler, OneHotEncoder, FunctionTransformer\n)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import (\n    train_test_split, KFold, GridSearchCV, ParameterGrid,\n)\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor, DMatrix, plot_importance\nfrom xgboost import cv as xgb_cv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{},"cell_type":"markdown","source":"We will work with consumption data ranging from Jan 2015 to Jan 2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"STUDY_START_DATE = pd.Timestamp(\"2015-01-01 00:00\", tz=\"utc\")\nSTUDY_END_DATE = pd.Timestamp(\"2020-01-31 23:00\", tz=\"utc\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The German load data is originally available with 15-min resolution. We have resampled it on an hourly basis for this analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"de_load = pd.read_csv(\"../input/western-europe-power-consumption/de.csv\")\nde_load = de_load.drop(columns=\"end\").set_index(\"start\")\nde_load.index = pd.to_datetime(de_load.index)\nde_load.index.name = \"time\"\nde_load = de_load.groupby(pd.Grouper(freq=\"h\")).mean()\nde_load = de_load.loc[\n    (de_load.index >= STUDY_START_DATE) & (de_load.index <= STUDY_END_DATE), :\n]\nde_load.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define our test set as the last 12 months of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_test(df, split_time):\n    df_train = df.loc[df.index < split_time]\n    df_test = df.loc[df.index >= split_time]\n    return df_train, df_test\n\ndf_train, df_test = split_train_test(\n    de_load, pd.Timestamp(\"2019-02-01\", tz=\"utc\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our training data covers roughly 20% of the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = df_train[\"load\"].plot(figsize=(12, 4), color=\"tab:blue\")\n_ = df_test[\"load\"].plot(ax=ax, color=\"tab:orange\", ylabel=\"MW\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{},"cell_type":"markdown","source":"There are no missing observations in our training data (there actually were a few missing observations on 15-min granularity, but we took care of these with hourly aggregation when loading the data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[df_train[\"load\"].isna(), :].index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create features for training"},{"metadata":{},"cell_type":"markdown","source":"The following features are used for training our forecast models :\n* time features: month, weekday and hour\n* national holiday features, as a boolean time series\n* lag features: load data with a lag values ranging from 24 to 48 hours"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_time_features(df):\n    cet_index = df.index.tz_convert(\"CET\")\n    df[\"month\"] = cet_index.month\n    df[\"weekday\"] = cet_index.weekday\n    df[\"hour\"] = cet_index.hour\n    return df\n\ndef add_holiday_features(df):\n    de_holidays = holidays.Germany()\n    cet_dates = pd.Series(df.index.tz_convert(\"CET\"), index=df.index)\n    df[\"holiday\"] = cet_dates.apply(lambda d: d in de_holidays)\n    df[\"holiday\"] = df[\"holiday\"].astype(int)\n    return df\n\ndef add_lag_features(df, col=\"load\"):\n    for n_hours in range(24, 49):\n        shifted_col = df[col].shift(n_hours, \"h\")\n        shifted_col = shifted_col.loc[df.index.min(): df.index.max()]\n        label = f\"{col}_lag_{n_hours}\"\n        df[label] = np.nan\n        df.loc[shifted_col.index, label] = shifted_col\n    return df\n\ndef add_all_features(df, target_col=\"load\"):\n    df = df.copy()\n    df = add_time_features(df)\n    df = add_holiday_features(df)\n    df = add_lag_features(df, col=target_col)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The lag features introduce a few missing values which we will move out of the analysis. The features of our training set are then the following :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = add_all_features(df_train).dropna()\ndf_test = add_all_features(df_test).dropna()\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then separate target values from features into distinct data frames."},{"metadata":{"trusted":true},"cell_type":"code","source":"target_col = \"load\"\nX_train = df_train.drop(columns=target_col)\ny_train = df_train.loc[:, target_col]\nX_test = df_test.drop(columns=target_col)\ny_test = df_test.loc[:, target_col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preparation pipeline"},{"metadata":{},"cell_type":"markdown","source":"We'll use the following data preparation pipeline to apply one-hot encoders on categorical feratures (time features), and a standard scaler on numerical features (lag features)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_prep_pipeline(df):\n    cat_features = [\"month\", \"weekday\", \"hour\"]  # categorical features\n    bool_features = [\"holiday\"]  # boolean features\n    num_features = [c for c in df.columns\n                    if c.startswith(\"load_lag\")]  # numerical features\n    prep_pipeline = ColumnTransformer([\n        (\"cat\", OneHotEncoder(), cat_features),\n        (\"bool\", FunctionTransformer(), bool_features),  # identity\n        (\"num\", StandardScaler(), num_features),\n    ])\n    prep_pipeline = prep_pipeline.fit(df)\n    \n    feature_names = []\n    one_hot_tf = prep_pipeline.transformers_[0][1]\n    for i, cat_feature in enumerate(cat_features):\n        categories = one_hot_tf.categories_[i]\n        cat_names = [f\"{cat_feature}_{c}\" for c in categories]\n        feature_names += cat_names\n    feature_names += (bool_features + num_features)\n    \n    return feature_names, prep_pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then fit pipeline on training data, and apply it on training and test sets"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"feature_names, prep_pipeline = fit_prep_pipeline(X_train)\n\nX_train_prep = prep_pipeline.transform(X_train)\nX_train_prep = pd.DataFrame(X_train_prep, columns=feature_names, index=df_train.index)\nX_test_prep = prep_pipeline.transform(X_test)\nX_test_prep = pd.DataFrame(X_test_prep, columns=feature_names, index=df_test.index)\n\nX_train_prep.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training regression models"},{"metadata":{},"cell_type":"markdown","source":"Three models will be trained for our prediction task : a simple linear models with L1 and L2 regularisation, a random forest, and gradient boosting model (based on XGBoost library)."},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_model = SGDRegressor(penalty=\"elasticnet\", tol=10, random_state=42)\nrf_model = RandomForestRegressor(\n    n_estimators=100, criterion='mse', min_samples_leaf=0.001, random_state=42\n)\nxgb_model = XGBRegressor(n_estimators=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Learning curves"},{"metadata":{},"cell_type":"markdown","source":"The behaviour and performance of the previous models can be represented with learning curves. These are showing the models performance evolution based on experience (the amount of training data that is fed to the algorithm)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def compute_learning_curves(model, X, y, curve_step, verbose=False):\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, shuffle=False\n    )\n    n_train_obs = X_train.shape[0]\n    n_iter = math.ceil(n_train_obs / curve_step)\n    train_errors, val_errors, steps = [], [], []\n    for i in range(n_iter):\n        n_obs = (i+1) * curve_step\n        n_obs = min(n_obs, n_train_obs)\n        model.fit(X_train[:n_obs], y_train[:n_obs])\n        y_train_predict = model.predict(X_train[:n_obs])\n        y_val_predict = model.predict(X_val)\n        train_mse = mean_squared_error(y_train[:n_obs], y_train_predict)\n        val_mse = mean_squared_error(y_val, y_val_predict)\n        train_errors.append(train_mse)\n        val_errors.append(val_mse)\n        steps.append(n_obs)\n        if verbose:\n            msg = \"Iteration {0}/{1}: train_rmse={2:.2f}, val_rmse={3:.2f}\".format(\n                i+1, n_iter, np.sqrt(train_mse), np.sqrt(val_mse)\n            )\n            print(msg)\n    return steps, train_errors, val_errors\n\ndef plot_learning_curves(steps, train_errors, val_errors, ax=None, title=\"\"):\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=(6, 4))\n    train_rmse = np.sqrt(train_errors)\n    val_rmse = np.sqrt(val_errors)\n    ax.plot(steps, train_rmse, color=\"tab:blue\",\n            marker=\".\", label=\"training\")\n    ax.plot(steps, val_rmse, color=\"tab:orange\",\n            marker=\".\", label=\"validation\")\n    ylim = (0.8*np.median(train_rmse),\n            1.5*np.median(val_rmse))\n    ax.set_ylim(ylim)\n    ax.set_xlabel(\"Number of observations\")\n    ax.set_ylabel(\"RMSE (MW)\")\n    ax.set_title(title)\n    ax.legend()\n    ax.grid()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linear model achieves a validation RMSE of ±3230MW. The training RMSE is ±3100MW which is relatively close."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"steps, train_mse, val_mse = compute_learning_curves(\n    lin_model, X_train_prep, y_train, 500, verbose=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(steps, train_mse, val_mse, title=\"Linear model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is interesting to note that linear model performance significantly improves once we feed it the first year data (up to ±8760 hours), but does not further improve significantly as we feed it the next years' data.\n\nLet's now see what performance we achieve with the random forest. Validation error drops to ±2360MW while training error lies around 1730MW, which would indicate that the model overfits the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_steps, rf_train_mse, rf_val_mse = compute_learning_curves(\n    rf_model, X_train_prep, y_train, 500, verbose=True\n)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plot_learning_curves(rf_steps, rf_train_mse, rf_val_mse, title=\"Random forest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost model pushes RMSE even further to 2050MW with training RMSE of only 260MW. This indicates once again that our model is overfitting the training data. We will try to handle the overfitting later on at model fine-tuning step."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"xgb_steps, xgb_train_mse, xgb_val_mse = compute_learning_curves(\n    xgb_model, X_train_prep, y_train, 500, verbose=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(xgb_steps, xgb_train_mse, xgb_val_mse, title=\"XGB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features' importance"},{"metadata":{},"cell_type":"markdown","source":"Let's estimate feature importance based on XGB model trained on the whole training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model.fit(X_train_prep, y=y_train)\n_, ax = plt.subplots(1, 1, figsize=(6, 6))\n_ = plot_importance(xgb_model, ax=ax, max_num_features=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on this model, all the lag features have higher importance than time and calendar features. The two most important features are lags H-24 and H-48, i.e. the consumption yesterday and the day before at the same hour."},{"metadata":{},"cell_type":"markdown","source":"## Fine-tuning the model"},{"metadata":{},"cell_type":"markdown","source":"In this part, we will start from the XGB model that we previously trained, and try to optimize its hyperparameters. To do so, we'll use a straightforward grid search approach (grid searh CV)."},{"metadata":{},"cell_type":"markdown","source":"### Grid search cross validation"},{"metadata":{},"cell_type":"markdown","source":"The following function applies grid search CV with XGB models. In particular, it uses the parameter `early_stopping_rounds` to interrupt training when validation error stops improving for $n$ iterations where $n$ is the parameter's value.\n\nEarly stopping is a way to prevent overfitting and reduce computation time. We will also try to reduce overfitting by optimizing XGB hyperparameters `eta` (learning rate) and `max_depth`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_grid_search_cv(\n    params_grid, X, y, nfold,\n    num_boost_round=1000, early_stopping_rounds=10,\n):\n    params_grid = ParameterGrid(params_grid)\n    search_results = []\n    print(f\"Grid search CV : nfold={nfold}, \" +\n          f\"numb_boost_round={num_boost_round}, \" +\n          f\"early_stopping_round={early_stopping_rounds}\")\n    for params in params_grid:\n        print(f\"\\t{params}\")\n        cv_df = xgb_cv(\n            params=params, dtrain=DMatrix(X, y), nfold=nfold,\n            num_boost_round=num_boost_round,\n            early_stopping_rounds=early_stopping_rounds,\n            shuffle=False, metrics={\"rmse\"},\n        )\n        cv_results = params.copy()\n        cv_results[\"train-rmse-mean\"] = cv_df[\"train-rmse-mean\"].min()\n        cv_results[\"test-rmse-mean\"] = cv_df[\"test-rmse-mean\"].min()\n        search_results.append(cv_results)\n    return pd.DataFrame(search_results)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code runs a 4-fold cross-validation on a hyperparameter grid of 18 possible combinations. So it will train a total of 72 models."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"params_grid = dict(\n    eta = [0.05, 0.1, 0.3],\n    max_depth = [2, 4, 6],\n    min_child_weight = [5, 1]\n)\nxgb_search_scores = xgb_grid_search_cv(\n    params_grid, X_train_prep, y_train, nfold=4, early_stopping_rounds=10\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results of the grid search are the following :"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"xgb_search_scores.sort_values(by=\"test-rmse-mean\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training our final model"},{"metadata":{},"cell_type":"markdown","source":"Based on previous result, we can train our final model on the whole training set"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"final_model = XGBRegressor(\n    n_estimators=1000, learning_rate=0.05, max_depth=6, min_child_weight=5\n)\nfinal_model.fit(\n    X_train_prep, y_train, early_stopping_rounds=10,\n    eval_set=[(X_train_prep, y_train), (X_test_prep, y_test)],\n    verbose=False,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions on test set"},{"metadata":{},"cell_type":"markdown","source":"Our final XGB model achieves RMSE score of ±1740MW on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model.best_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's group predicted and actual test data into a data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_predictions_df(model, X, y):\n    y_pred = model.predict(X)\n    df = pd.DataFrame(dict(actual=y, prediction=y_pred), index=X.index)\n    df[\"squared_error\"] =  (df[\"actual\"] - df[\"prediction\"])**2\n    return df\n\npred_df = compute_predictions_df(\n    final_model, X_test_prep, y_test\n)\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing actual and predicted curves on the test set :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_predictions(pred_df, start=None, end=None):\n    _, ax = plt.subplots(1, 1, figsize=(12, 5))\n    start = start or pred_df.index.min()\n    end = end or pred_df.index.max()\n    pred_df.loc[\n        (pred_df.index >= start) & (pred_df.index <= end),\n        [\"actual\", \"prediction\"]\n    ].plot.line(ax=ax)\n    ax.set_title(\"Predictions on test set\")\n    ax.set_ylabel(\"MW\")\n    ax.grid()\n\nplot_predictions(pred_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Special time intervals"},{"metadata":{},"cell_type":"markdown","source":"The intervals \n* 15th Apr. 2019 – 6th May 2019, and\n* 16th Dec. 2019 – 6th Jan 2020\n\nseem slightly irregular compared to the rest. This is most likely due to holiday periods. Let's zoom on these periods  to check how our predictions perform. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2019-04-15\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2019-05-06\", tz=\"utc\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance drops slightly around 21st of April and on May 2nd. For the 2nd of May, this could be explained by the fact that 1st of May is a national holiday in Germany, and that the model is using lag features to estimate May 2nd volume using May 1st without properly adapting scale (as May 2nd is not a holiday).\n\nAppart from these, our model does a reasonable job at predicting volumes."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2019-12-16\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2020-01-06\", tz=\"utc\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we observe that our model is slightly overestimating consumption during Christmas period and New Year's Eve."},{"metadata":{},"cell_type":"markdown","source":"### Best and worst prediction days"},{"metadata":{},"cell_type":"markdown","source":"The days with worst prediction performance are the following :"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_pred_df = pred_df.groupby(pd.Grouper(freq=\"D\")).mean()\ndaily_pred_df.sort_values(by=\"squared_error\", ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find back the Labour Day's and end of year's periods in the above list. We also observe poor performance on the 20th of June :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2019-06-19\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2019-06-22\", tz=\"utc\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model is largely overestimating consumption on 20th of June. This date is actually a *regional* holiday in Germany (Fronleichnam) which are currently not taken into account by our model. These only and happen in specific states, but still have an impact on the load at country level.\n\nLooking at the best predictions, we have the following days :"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_pred_df.sort_values(by=\"squared_error\", ascending=True).head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions were particularly accurate on 23rd and 25th of January 2020, as we can see from the graph below."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_predictions(pred_df,\n                 start=pd.Timestamp(\"2020-01-23\", tz=\"utc\"),\n                 end=pd.Timestamp(\"2020-01-26\", tz=\"utc\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion and next steps"},{"metadata":{},"cell_type":"markdown","source":"Gradient boosting model achieves reasonable performance (RMSE of ±1740MW) at load forecasting at country level. The most relevant features for this task are lag features, especially at H-24 and H-48. Week-ends and holidays have less impact, but are still relevant to use.\n\nTo further improve the model, additional features could be taken into account such as\n* additional lag features (beyond H-48),\n* regional holidays,\n* temperature data."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}