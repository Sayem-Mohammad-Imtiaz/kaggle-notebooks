{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Model Building Pipeline: Data Analysis\n\nIn this following notebook, we will go through the Data Analysis step in the Machine Learning model building pipeline. There will be a notebook for each one of the Machine Learning Pipeline steps:\n\n1. [Data Analysis](https://www.kaggle.com/rkb0023/exploratory-data-analysis-house-rent-prediction)\n2. Feature Engineering\n3. [Model Building](https://www.kaggle.com/rkb0023/model-building-house-rent-prediction)\n\n**This is the notebook for step 2: Feature Engineering**\n\nThe dataset can be found in [iNeuron](https://challenge-ineuron.in/mlchallenge.php#) ML Challenge 2.\n\n<hr>\n\n## Predicting Rent Price of Houses\n\nThe aim of the project is to build a machine learning model to predict the rent price of homes based on different explanatory variables describing aspects of residential houses. \n\n\n<hr>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## House Prices dataset: Feature Engineering\n\nIn the following cells, we will engineer / pre-process the variables of the House Rent Dataset from iNeuron. We will engineer the variables so that we tackle:\n\n1. Missing values\n2. Temporal variables\n3. Non-Gaussian distributed variables\n4. Categorical variables: remove rare labels\n5. Categorical variables: convert strings to numbers\n5. Standarise the values of the variables to the same range\n\nLet's go ahead and load the dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# to handle datasets\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to divide train and test set\nfrom sklearn.model_selection import train_test_split\n\n# feature scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.simplefilter(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/houserentpredictiondataset/houseRent/housing_train.csv')\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Getting appropriate data types. For example baths have a dtype of float, and it contains some decimal values. But it is a quantitative variable. So transforming it to remove decimal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['baths'] = np.ceil(data['baths'])\ndata['baths'] = data['baths'].astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing values\n\n### Categorical variables","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# make a list of the categorical variables that contain missing values\ncat_var_na = ['laundry_options', 'parking_options']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_missing_cat(data, var, modeof):\n    return data.groupby(modeof)[var].transform(\n        lambda x: x.fillna(x.mode()[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"laundry_options\"] = impute_missing_cat(data, \"laundry_options\", \"type\")\ndata[\"parking_options\"] = impute_missing_cat(data, \"parking_options\", \"type\")\ndata = data.dropna(subset=[\"state\", \"description\"],axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numerical variables\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a list with the numerical variables that contain missing values\nnum_var_na = ['lat', 'long']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_missing_num(data, var, meanof):\n    return data.groupby(meanof)[var].transform(\n        lambda x: x.fillna(x.mode()[0]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8cd4872-67c6-4c82-a1a3-807d08efb658","_execution_state":"idle","_uuid":"56c268a7217e85301d2847ceeca410d62e781b89","trusted":true},"cell_type":"code","source":"data[\"lat\"] = impute_missing_num(data, \"lat\", \"region\")\ndata[\"long\"] = impute_missing_num(data, \"long\", \"region\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9193270b-8e7b-4a24-b769-d6eb661a1ae7","_execution_state":"idle","_uuid":"465570fcce8944ee31b1709ec6954d03cbf32ff8"},"cell_type":"markdown","source":"Is there any remaining missing value ? ","execution_count":null},{"metadata":{"_cell_guid":"0adf05cf-ce60-4169-805c-ca776e60e85a","_execution_state":"idle","_uuid":"b091fa2ebef19425019e2e550410d0376b9e9fac","trusted":true},"cell_type":"code","source":"#Check remaining missing values if any \nall_data_na = (data.isnull().sum() / len(data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In EDA, we decided to remove outliers according to the upper and lower bound of its interquartile range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"price_upper, price_lower = 2400, 1\nsqfeet_upper, sqfeet_lower = 1762, 1\nbeds_upper, beds_lower = 3, 1\nbaths_upper, baths_lower = 3, 1\n\ndata = data[(data['price'] <= price_upper) & (data['price'] >= price_lower)]\ndata = data[(data['sqfeet'] <= sqfeet_upper) & (data['sqfeet'] >= sqfeet_lower)]\ndata = data[(data['beds'] <= beds_upper) & (data['beds'] >= beds_lower)]\ndata = data[(data['baths'] <= baths_upper) & (data['baths'] >= baths_lower)]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting More Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['premium_house'] = np.where((data['baths']>=data['beds'])&(data['beds']>1),1,0)\ndata['pets_allowed'] = np.where((data['cats_allowed']==1)&data['dogs_allowed']==1,1,0)\ndata['beds_per_sqfeet'] = data['beds'] / data['sqfeet']\ndata['baths_per_beds'] = data['baths'] / data['beds']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring *'description'* column","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"data.description[82226].lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[x in data.description[82226].lower() for x in ['pool', 'swimming','wi-fi','fireplace','grilling','gym','fence', 'court']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get some intriguing new features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['has_pool'] = data['description'].apply(lambda x: 1 if 'pool' in x.lower() or 'swimming' in x.lower() else 0)\ndata['has_grill'] = data['description'].apply(lambda x: 1 if 'grill' in x.lower() or 'grilling' in x.lower() else 0)\ndata['has_fireplace'] = data['description'].apply(lambda x: 1 if 'fireplace' in x.lower() or 'fire pits' in x.lower() else 0)\ndata['gym_nearby'] = data['description'].apply(lambda x: 1 if 'gym' in x.lower() or 'fitness' in x.lower() else 0)\ndata['school/clg_nearby'] = data['description'].apply(lambda x: 1 if 'school' in x.lower() or 'college' in x.lower() else 0)\ndata['wifi_facilities'] = data['description'].apply(lambda x: 1 if 'wifi' in x.lower() or 'wi-fi' in x.lower() else 0)\ndata['valet_service'] = data['description'].apply(lambda x: 1 if 'valet' in x.lower() else 0)\ndata['shopping_nearby'] = data['description'].apply(lambda x: 1 if 'shopping' in x.lower() else 0)\ndata['sports_playground'] = data['description'].apply(lambda x: 1 if 'sport' in x.lower()  or 'sports' in x.lower() \n                                                      or 'tennis' in x.lower() or 'soccer' in x.lower() \n                                                      or 'soccers' in x.lower() or 'court' in x.lower() else 0)\ndata['dining_nearby'] = data['description'].apply(lambda x: 1 if 'dining' in x.lower() else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in ['has_pool', 'has_grill', 'has_fireplace', 'gym_nearby',\n       'school/clg_nearby', 'wifi_facilities', 'valet_service',\n       'shopping_nearby', 'sports_playground', 'dining_nearby']:\n    print(data[var].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical variable transformation\n\nWe will log transform the positive numerical variables in order to get a more Gaussian-like distribution. This tends to help Linear machine learning models. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in ['price','sqfeet','baths_per_beds','beds_per_sqfeet']:\n    data[var] = np.log(data[var])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check that data set does not contain null values in the engineered variables\n[var for var in ['price','sqfeet','baths_per_beds','beds_per_sqfeet'] if data[var].isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical variables\n\n### Removing rare labels\n\nFirst, we will group those categories within variables that are present in less than 1% of the observations. That is, all values of categorical variables that are shared by less than 1% of houses, well be replaced by the string \"Rare\".\n\nTo learn more about how to handle categorical variables visit our course [Feature Engineering for Machine Learning](https://www.udemy.com/feature-engineering-for-machine-learning/?couponCode=UDEMY2018) in Udemy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's capture the categorical variables in a list\n\ncat_vars = ['region', 'type', 'laundry_options', 'parking_options', 'state']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_ls = {\n    'region': \n        ['denver', 'fayetteville', 'jacksonville', 'omaha / council bluffs', 'rochester'],\n     'type': \n        ['apartment', 'condo', 'duplex', 'house', 'manufactured', 'townhouse'],\n     'laundry_options': \n        ['laundry in bldg', 'laundry on site', 'w/d hookups', 'w/d in unit'],  \n     'parking_options': \n        ['attached garage', 'carport', 'detached garage', 'off-street parking', 'street parking'],\n     'state': \n        ['al', 'ar', 'az', 'ca', 'co', 'ct', 'fl', 'ga', 'ia', 'id', 'il', 'in', 'ks', 'ky', 'la', \n        'ma', 'md', 'mi', 'mn', 'ms', 'nc', 'nd', 'ne', 'nj', 'nm', 'nv', 'ny', 'oh']\n}\n\n\nfor var in cat_vars:\n    data[var] = np.where(data[var].isin(\n        frequent_ls[var]), data[var], 'Rare')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding of categorical variables\n\nNext, we need to transform the strings of the categorical variables into numbers. We will do it so that we capture the monotonic relationship between the label and the target.\n\nTo learn more about how to encode categorical variables visit our course [Feature Engineering for Machine Learning](https://www.udemy.com/feature-engineering-for-machine-learning/?couponCode=UDEMY2018) in Udemy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will assign discrete values to the strings of the variables,\n# so that the smaller value corresponds to the category that shows the smaller\n# mean house sale price\n\n\ndef replace_categories(data, var, target):\n\n    # order the categories in a variable from that with the lowest\n    # house sale price, to that with the highest\n    ordered_labels = data.groupby([var])[target].mean().sort_values().index\n\n    # create a dictionary of ordered categories to integer values\n    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n\n    # use the dictionary to replace the categorical strings by integers\n    data[var] = data[var].map(ordinal_label)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for var in cat_vars:\n    replace_categories(data, var, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check absence of na in the train set\n[var for var in data.columns if data[var].isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"features = ['region', 'price', 'type', 'sqfeet', 'smoking_allowed', 'wheelchair_access', \n            'electric_vehicle_charge', 'comes_furnished', 'laundry_options', 'parking_options','lat', 'long', \n            'premium_house', 'pets_allowed', 'beds_per_sqfeet', 'baths_per_beds', 'has_pool', 'has_grill', \n            'has_fireplace', 'gym_nearby', 'school/clg_nearby', 'wifi_facilities', 'valet_service', \n            'shopping_nearby', 'sports_playground', 'dining_nearby']\n\ndata_final = data[features].copy()\ndata_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in features:\n    data_final[feature] = data_final[feature].astype(np.float64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Heatmap","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = data.corr()\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\nfig, ax = plt.subplots(figsize=(25,25)) \n\nsns.heatmap(corr_matrix, \n            annot=True, \n            square=True,\n            fmt='.2g',\n            mask=mask,\n            ax=ax).set(\n    title = 'Feature Correlation', xlabel = 'Columns', ylabel = 'Columns')\n\nax.set_yticklabels(corr_matrix.columns, rotation = 0)\nax.set_xticklabels(corr_matrix.columns)\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_final.to_csv('data_cleaned.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}