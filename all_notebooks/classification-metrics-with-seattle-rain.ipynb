{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py","mimetype":"text/x-python","name":"python","version":"3.6.3","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"bd507c4cd296098d41513fc665e277e1840c4645","_cell_guid":"c73d2379-ae64-40be-ac7b-abc8387eab57"},"source":"# Classification Metrics with Seattle Rain\n\nPredicting whether or not it will rain in Seattle tomorrow, based on whether or not it rained in Seattle yesterday, is a classic classification problem (moreover, a classic [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) simulation problem, but I digress...).\n\nTo measure how well we do on classification tasks like this one we take advantage of a well-developed variety of *classification metrics*. These compliment the regression metrics I covered in [a previous notebook](https://www.kaggle.com/residentmario/model-fit-metrics/).","cell_type":"markdown"},{"metadata":{"_uuid":"f29376c8b884e3f42fcd5616acb096117f90c9c2","_cell_guid":"c13c7661-56d0-41ce-b4bc-9add94e70a34"},"outputs":[],"execution_count":null,"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndf = pd.read_csv(\"../input/seattleWeather_1948-2017.csv\")\ndf.head()"},{"metadata":{"_uuid":"559440ad1d61eb6598a1d014beb7fd1739067857","_cell_guid":"8c234d1d-4d84-4506-8ca9-c1b59af76bc3"},"outputs":[],"execution_count":null,"cell_type":"code","source":"df = df.dropna()\nX = df.loc[:, ['PRCP', 'TMAX', 'TMIN']].shift(-1).iloc[:-1].values\ny = df.iloc[:-1, -1:].values.astype('int')\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(X, y)\ny_hat = clf.predict(X)"},{"metadata":{"_uuid":"7b7eb63365756d3161661dc06e01adcfa4753b83","_cell_guid":"c0641482-b7aa-4ecb-aab3-9f68efe4ab42"},"source":"## Accuracy score\n\nThe accuracy of a binary classifier is nothing less and nothing more than the proportion of predictions which are made correctly. In this case, our model was accurate ~70% of the time.","cell_type":"markdown"},{"metadata":{"_uuid":"d1f3b509668177d3d83d15e4abdd17913a2063ed","_cell_guid":"6b086f0f-62ff-41f1-88d7-48af7af831d8"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y, y_hat)"},{"metadata":{"_uuid":"c4b21e3d0e28c7e2afba0285c857c2dc24315db3","_cell_guid":"33ff59aa-0d32-40d4-8b39-d85739f6cf09"},"source":"Accuracy is usually the end result, and what the model end user will see and care about. However, it's not very useful for diagnosing a classifier because it doesn't tell us *where* the model is making errors. Answering this \"where\" question is an essential part of model-building. Hence accuracy is a start, but not an end.","cell_type":"markdown"},{"metadata":{"_uuid":"7c3920d2bbf74b7d6f65a9e4da4534b920d997dc","_cell_guid":"0169c351-f3a7-4a20-9a1e-f09ca82a6c20"},"source":"## Confusion matrix\n\nThe confusion matrix steps beyond the accuracy score by providing us with information about where we are committing errors, in the form of a conditional distribution table.","cell_type":"markdown"},{"metadata":{"_uuid":"4e7d5e1533811facdce0d274fe1a5f51b1e69752","_cell_guid":"8b4b72ff-b6fe-49a8-956b-8532365c0ab8"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y, y_hat)"},{"metadata":{"_uuid":"a1f63fac9131a1a417b1760b1979c1cc1f5aa14f","_cell_guid":"5ab0a530-1e96-4667-b5e9-05e462f91509"},"source":"It's usually convenient to plot the confusion matrix in a `seaborn` `heatmap`:","cell_type":"markdown"},{"metadata":{"_uuid":"bd4cb7781f4705dc1910cdf67be18dbab287c6ab","_cell_guid":"5dd12065-56fc-4bb8-a5f8-154d9a114ca9"},"outputs":[],"execution_count":null,"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(confusion_matrix(y, y_hat) / len(y), cmap='Blues', annot=True)"},{"metadata":{"_uuid":"896518dcb20034bfba2515a59aa3de98b63a6bb2","_cell_guid":"72661830-db37-4cb4-b93c-d838c89d4742"},"source":"The 0,0 and 1,1 cells are correct classifications, and their sum is the classifier accuracy. These are also known as the *true positive* and *true negative* rates. The 1,0 cell is *false positive rate*: times when the classifier generates a false positive (in our case, predicting that it will rain when it doesn't). This occurs in 13% of all cases. Meanwhile, 17% of all cases is *false negative rate* (predicting it will not rain when it does) in the 0,1 cell.\n\nIn a multiclass setting, this plot will expand out with more entries, but the story will remain the same: results on the diagonal are good, results off the diagonal are bad, and finding where those off-diagonal results tend to occur will help us understand where our errors are and how we can improve on them.\n\nIn this case we see that our false negative rate is significantly higher (relative to our true negative rate) than our false positive rate: 0.17 is a bigger with respect to 0.26 than 0.13 is to 0.44. Perhaps (at least with more complicated data!) we should investigate what errors we are making in those misclassifications.","cell_type":"markdown"},{"metadata":{"_uuid":"bd1030176fa7f31f37654db7f9ce49f95e49b469","_cell_guid":"86590d7f-1d74-49b5-938c-fc33d4267655"},"source":"## Hamming loss\n\nHamming loss or [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) is a pretty cool concept taken from information theory. In information theory, the Hamming distance between two pieces of information is the number of bits of the recieved signal that we would need to change in order to match the signal sent. Another thing to think about it is as the distance between a word and a misspelling of that word, e.g. \"animus\" and \"animos\" would have a Hamming loss of 1, since we have to make one correction to match the two words.\n\nIn classification tasks, however, Hamming loss simplifies down to $1 - \\text{Accuracy}$, as the following demonstrates (recall that our prediction accuracy was 70%).","cell_type":"markdown"},{"metadata":{"_uuid":"3a7c6e306a122d6d7d9554a68c87e962050c86ac","_cell_guid":"641b4752-bd42-45f1-bfea-22a632386028"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import hamming_loss\n\nhamming_loss(y, y_hat)"},{"metadata":{"_uuid":"c7f39f74db16c498054c26a7c9786602876962eb","_cell_guid":"fb6cceea-772f-4e3a-82ae-846d51ea7e21"},"source":"## Precision and recall; F-score\n\n**Precision** and **recall** are two metrics which are often tied together by a computed fit statistic called the **F-score**.\n\nPrecision and recall originate from the field of information retrieval. In information retrieval, we are tasked with providing users with records which are relevant to tehir search query, and not records which are irrelevant. Suppose, for example, that we are running a search engine. Our search engine returns 30 pages, of which 20 are relevant and 10 are irrelevant. Our search engine also fails to return 40 other relevant pages.\n\nIn this toy example, precision is the percentage of results we've returned which are relevant: 20/30, or 2/3. Recall, meanwhile, the \"completeness\" of the information we've returned with respect to all possible relevant results: 20/60, or 1/3.\n\nIn math:\n\n$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$\n$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$\n\nHere's a picture that Wikipedia provides for the concepts:\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/440px-Precisionrecall.svg.png)\n\nIf you read any literature on search, you will learn that finding the right balance between precision and recall is an important part of search. Lightweight users care less about recall and more about precision, because they are mainly interested in finding a few hits that satisfy their lightweight needs. An example of a search fitting this profile might be \"Can dogs eat chocolate?\". Power users meanwhile prioritize recall; they can tolerate a few false positives, so long as they are able to thoroughly research \"General Zhukov's role in the Stalingrad counterattack\".\n\nAt the same time, precision and recall are fundamentally a trade-off. Returning *all* of the relevant results naturally requires returning at least *some* edge cases that turn out to be useless, and returning *only* relevant results naturally requires excluding *some* edge cases that turn out to be useful.","cell_type":"markdown"},{"metadata":{"_uuid":"eb427756a03d007c6c17ef53a957b261c70a75cf","_cell_guid":"aa846d05-030d-46a6-9cf9-003ff852c3ea"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n\nprint(precision_score(y, y_hat))\nprint(recall_score(y, y_hat))"},{"metadata":{"_uuid":"deedaafe4125be94c32ad1eb5e064a1cb6bdfb2d","_cell_guid":"3731c86d-612e-4c60-944d-7e96cec1ec50"},"source":"`sklearn` includes a utility for visualizing the trade-off between these two values, the `precision_recall_curve`.","cell_type":"markdown"},{"metadata":{"_uuid":"887b4470331660ea4be4b7f3eefc126b418e3271","_cell_guid":"77e22f55-b90a-41af-9de8-86bd37604a50"},"outputs":[],"execution_count":null,"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nprecision, recall, _ = precision_recall_curve(y, y_hat)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nax.step(recall, precision, color='steelblue',\n         where='post')\nax.fill_between(recall, precision, step='post', color='lightgray')\nplt.suptitle('Precision-Recall Tradeoff for Seattle Rain Prediction')\nplt.xlabel('Recall')\nplt.ylabel('Precision')"},{"metadata":{"_uuid":"7de694dd935fe11afcd65ae3a4a4433348e95c41","_cell_guid":"009f8b1c-4b9f-4694-9b29-1de7a72498ed"},"source":"In the classification context, this trade-off between precision and recall is often encoded using the **F-score**. An F-score is a fit statistic, scored between 0 and 1, which measures model fitness as a weighted [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of precision and recall. Which of these two measures matters more is controlled by the $\\beta$ hyperparameter. A $\\beta$ of 0 considers only precision (and so is equivalent to the `precision_score`, a $\\beta$ of 1 is known as an **F1 score** and considers an even balance of the two, while F1 scores greater than 1 (to infinity) are weighted more heavily towards recall.\n\nWhat should your choice of $\\beta$ be? The F1 score is worth pointing out (and even has its own `sklearn` implementation, `f1_score`) because it is a reasonable default. Choosing a specific ratio of precision to recall is usually a domain decision. We should favor a higher $\\beta$ if recall is more important for our model end users, and a lower $\\beta$ if precision is more important to our model end users.\n\nKeep in mind that it just so happens that F1, precision, and recall are often very targeted at the model end user (e.g. they correspond well with the classification model *as a product*). If these measures are not meaningful to us or our product, then we shouldn't be using them at all.","cell_type":"markdown"},{"metadata":{"_uuid":"4d6e76a1c7cfe13b33f27815553bced976127642","_cell_guid":"6db2ace7-6b42-413f-b3a0-286a22a64ca5"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import fbeta_score\n\nfbeta_score(y, y_hat, beta=1)"},{"metadata":{"_uuid":"13dd7112007c965de65349aa27c32cb52babffb9","_cell_guid":"9f277e1f-3a3f-4b37-9182-12a82af5ea6b"},"source":"## ROC curve\n\nThe **ROC curve** is the most common graphic way of examining the performance of a model. It stands for \"Reciever Operator Characteristic\" (dunno why), and it plots the performance of the model on two measures: the *true positive rate* and the *false positive rate*. Here is what a ROC curve looks like:","cell_type":"markdown"},{"metadata":{"_uuid":"b2074470005b8238db9f92b6a40bf9ce7b177ed9","_cell_guid":"0a050f0b-a19d-4bae-9ef5-29e66f7b7d66"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y, y_hat)\n\nfig, ax = plt.subplots(1, figsize=(12, 6))\nplt.plot(fpr, tpr, color='darkorange', label='Model Performace')\nplt.plot([0, 1], [0, 1], color='gray', label='Random Performace')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Seattle Rain ROC Curve')\nplt.legend(loc=\"lower right\")"},{"metadata":{"_uuid":"ef532a9d212a239b2cf31d34448d9524426a0726","_cell_guid":"64e1bc59-de6c-4bc0-91ac-d49c3ec31d6f","collapsed":true},"source":"The ROC curve is a nice and heavily utilized classification metric because it does a good job measuring how much harder getting incrementally more correct responses is for the model.\n\nTo get the first few true positives, the model does not have to work very hard, because we can pick up records which we have a very high confidence in. The model's confidence in each of the results that we collect afterwards drops steadily; picking up more true positives also involves making some errors, and hence, picking up more false positives. The last few results will naturally be the hardest to squeeze out; they will involve making the largest number of errors (picking up the most false positives).\n\nA model which picks up only true positives reliably will have a very high slope on as much of the line as possible, probably only leveling out on the last few records. Hence, when it comes to the ROC curve, the best model is one which sticks as close to the top of the chart as possible.\n\nBy looking a different points on the ROC curve, we can diagnose how much confidence the model has in different parts of the data. For example, if the ROC curve for the first quartile is extremely steep, then we see that the model \"clears\" the first 25% of true positives very easily. Seeing where the model begins to \"fall off\" is often very informative, because it's those records that we can often make most progress accuracy on (and hardly ever more marginal records further down the curve!).","cell_type":"markdown"},{"metadata":{"_uuid":"410ea7a50f2b10eab54668efbb518ca3ed9c855d","_cell_guid":"02f6a946-30cc-474e-aed6-2c347481597c","collapsed":true},"source":"## AUC\n\nThe ROC curve can be summarized using a single metric: the \"area under the curve\", or AUC. AUC is an oft-used metric of choice for classification model parameter tuning. It is no more and no less than the area under the curve above (an integral, or perhaps a trapazoidal approximation thereof).\n\nA higher AUC is better: more area under the curve, and hence, less input to the false positive rate. This metric is implemented in `sklearn` thusly:","cell_type":"markdown"},{"metadata":{"_uuid":"aad36cb4fe98dc3cfb1c0a441e0cc51b7cbfd706","_cell_guid":"13033cf8-abe8-4118-a6d5-541859db992f"},"outputs":[],"execution_count":null,"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y, y_hat)"},{"metadata":{"_uuid":"83521cd397ac756a6806f77062b8a797575d9e55","_cell_guid":"af925851-0ddb-4830-8d78-bd7fea206c5b","collapsed":true},"source":"# Conclusion\n\nThese metrics are the basics of measuring classification performace. There are other more advanced measurements, like the Gini coefficient and log-loss, which I will cover later in other notebooks.\n\nHopefully you found this material informative!","cell_type":"markdown"}],"nbformat_minor":1}