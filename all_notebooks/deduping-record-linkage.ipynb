{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Deduplication & Record Linkage. "},{"metadata":{},"cell_type":"markdown","source":"# This notebook shows how to use TD IDF, FUZZY to both dedupe and match records at scale besides K Nearest Neighbour algorithm as an alternative closeness measure "},{"metadata":{},"cell_type":"markdown","source":"\nData in the real world is messy. Dealing with messy data sets is painful and burns through time which could be spent analysing the data itself.\n\n![https://www.acronis.com/en-us/articles/deduplication/](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTc_jlg2hrSRYqdenJdv7p_4Xo6Uj-qqCPpx4ANHI2hNkA8TJQPJQ&s)\n\n- **Deduplication**. Aligning similar categories or entities in a data set (for example, we may need to combine ‘D J Trump’, ‘D. Trump’ and ‘Donald Trump’ into the same entity).\n- **Record Linkage**. Joining data sets on a particular entity (for example, joining records of ‘D J Trump’ to a URL of his Wikipedia page)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Important Talk by: presented at PyBay2018 \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nHTML('<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/McsTWXeURhA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**Record Deduplication**, or more generally, Record Linkage is the task of finding which records refer to the same entity, like a person or a company. It's used mainly when there isn't a unique identifier in records like Social Security Number for US citizens\n[Dedupe.io](https://dedupe.io)"},{"metadata":{},"cell_type":"markdown","source":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Purple\">If you find this kernel useful or interesting, please don't forget to upvote the kernel =)\n\n</body>\n</html>\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Import libs"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read in Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/sec-edgar-companies-list/\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root = '../input/sec-edgar-companies-list/'\n\ndata = pd.read_csv(root + 'sec__edgar_company_info.csv',encoding='latin')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Glimpse of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Size of data ',data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.select_dtypes('object').apply(pd.Series.nunique, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FuzzyWuzzy\n\nIn computer science, fuzzy string matching is the technique of finding strings that match a pattern approximately (rather than exactly). In another word, fuzzy string matching is a type of search that will find matches even when users misspell words or enter only partial words for the search. It is also known as approximate string matching.\n\n\n- Fuzzywuzzy is a Python library uses **Levenshtein Distance** to calculate the differences between sequences in a simple-to-use package.\n- Instalation: !pip install fuzzywuzzy, import: from fuzzywuzzy import fuzz, from fuzzywuzzy import process\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fuzzywuzzy\nfrom fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ratio** , compares the entire string similarity, in order."},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.ratio('ZZ GLOBAL LLC', 'ZZLL INFORMATION TECHNOLOGY, INC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is telling us that the 'ZZ GLOBAL LLC' and 'ZZLL INFORMATION TECHNOLOGY, INC' pair are about **36%** the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.ratio('ZZ GLOBAL LLC', 'ZZX, LLC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is telling us that the 'ZZ GLOBAL LLC' and 'ZZX, LLC' pair are about **57%** the same."},{"metadata":{},"cell_type":"markdown","source":"**partial_ratio** , compares partial string similarity."},{"metadata":{},"cell_type":"markdown","source":"- We are still using the same data pairs."},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.partial_ratio('ZZ GLOBAL LLC', 'ZZLL INFORMATION TECHNOLOGY, INC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.partial_ratio('ZZ GLOBAL LLC', 'ZZX, LLC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**token_sort_ratio** , ignores word order."},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_sort_ratio('ZZ GLOBAL LLC', 'ZZLL INFORMATION TECHNOLOGY, INC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_sort_ratio('ZZ GLOBAL LLC', 'ZZX, LLC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**token_set_ratio** , ignores duplicated words. It is similar with token sort ratio, but a little bit more flexible."},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_set_ratio('ZZ GLOBAL LLC', 'ZZLL INFORMATION TECHNOLOGY, INC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fuzz.token_set_ratio('ZZ GLOBAL LLC', 'ZZX, LLC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF & N-Grams"},{"metadata":{},"cell_type":"markdown","source":"**TF-IDF** is a method to generate features from text by multiplying the frequency of a term (usually a word) in a document (the Term Frequency, or TF) by the importance (the Inverse Document Frequency or IDF) of the same term in an entire corpus. This last term weights less important words (e.g. the, it, and etc) down, and words that don’t occur frequently up. IDF is calculated as:\n\n\n\n<html>\n<body>\n\n<p><font size=\"4\" color=\"Purple\">IDF(t) = log_e(Total number of documents / Number of documents with term t in it) \n\n</body>\n</html>"},{"metadata":{},"cell_type":"markdown","source":"### N-Grams  & De-Duplication\n\nWhile the terms in **TF-IDF** are usually words, this is not a necessity. In our case using words as terms wouldn’t help us much, as most company names only contain one or two words. This is why we will use n-grams: sequences of N contiguous items, in this case characters. The following function cleans a string and generates all n-grams in this string:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ftfy # amazing text cleaning for decode issues..","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom ftfy import fix_text\n\ndef ngrams(string, n=3):\n    string = fix_text(string) # fix text\n    string = string.encode(\"ascii\", errors=\"ignore\").decode() #remove non ascii chars\n    string = string.lower()\n    chars_to_remove = [\")\",\"(\",\".\",\"|\",\"[\",\"]\",\"{\",\"}\",\"'\"]\n    rx = '[' + re.escape(''.join(chars_to_remove)) + ']'\n    string = re.sub(rx, '', string)\n    string = string.replace('&', 'and')\n    string = string.replace(',', ' ')\n    string = string.replace('-', ' ')\n    string = string.title() # normalise case - capital at start of each word\n    string = re.sub(' +',' ',string).strip() # get rid of multiple spaces and replace with a single\n    string = ' '+ string +' ' # pad names for ngrams...\n    string = re.sub(r'[,-./]|\\sBD',r'', string)\n    ngrams = zip(*[string[i:] for i in range(n)])\n    return [''.join(ngram) for ngram in ngrams]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('All 3-grams in \"McDonalds\":')\nngrams('McDonalds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The code to generate the matrix of TF-IDF values for each is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ncompany_names = data['Company Name'].unique()\nvectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\ntf_idf_matrix = vectorizer.fit_transform(company_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resulting matrix is very sparse as most terms in the corpus will not appear in most company names. Scikit-learn deals with this nicely by returning a sparse CSR matrix.\n\nYou can see the first row (**“!J INC”**) contains three terms for the columns 11, 16196, and 15541."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( tf_idf_matrix.shape, tf_idf_matrix[5] )\n# Check if this makes sense:\n\nngrams('#1 PAINTBALL CORP')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The last term (**‘ORP’**) has a relatively low value, **0.22892**, which makes sense as this term will appear often in the corpus, thus receiving a lower IDF weight."},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = time.time()\nprint(process.extractOne('Ministry of Justice', company_names[0:999])) #org names is our list of organization names\nt = time.time()-t1\nprint(\"SELFTIMED:\", t)\nprint(\"Estimated hours to complete for 1000 rows of  dataset:\", (t*len(company_names[0:999]))/60/60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Record linkage and a different approach\n> In the below section we will see how this is achieved and also use the K Nearest Neighbour algorithm as an alternative closeness measure.\nThe dataset we would like to join on is a set of ‘clean’ organization names created by the Office for National Statistics (ONS):"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/1014/1*k45HFixH1Q-qxxH1i2rsxQ.png)"},{"metadata":{},"cell_type":"markdown","source":"As can be shown in the code below, the only difference in this approach is to transform the messy data set using the tdif matrix which has been learned on the clean data set.\n\nThe **‘getNearestN’** then uses Scikit’s implementation of K Nearest Neighbours to find the closest matches in the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"##################\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nroot2 = '../input/gov-names/'\nclean_org_names = pd.read_excel(root2 + 'Gov Orgs ONS.xlsx')\nclean_org_names = clean_org_names.iloc[:, 0:6]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\norg_name_clean = clean_org_names['Institutions'].unique()\n\nprint('Vecorizing the data - this could take a few minutes for large datasets...')\nvectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)\ntfidf = vectorizer.fit_transform(org_name_clean)\nprint('Vecorizing completed...')\n\nfrom sklearn.neighbors import NearestNeighbors\nnbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\n\norg_column = 'Company Name' #column to match against in the messy data\nunique_org = set(data[org_column].values) # set used for increased performance\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###matching query:\ndef getNearestN(query):\n    queryTFIDF_ = vectorizer.transform(query)\n    distances, indices = nbrs.kneighbors(queryTFIDF_)\n    return distances, indices\n\nimport time\nt1 = time.time()\nprint('getting nearest n...')\ndistances, indices = getNearestN(unique_org)\nt = time.time()-t1\nprint(\"COMPLETED IN:\", t)\n\nunique_org = list(unique_org) #need to convert back to a list\nprint('finding matches...')\nmatches = []\nfor i,j in enumerate(indices):\n    temp = [round(distances[i][0],2), clean_org_names.values[j][0][0],unique_org[i]]\n    matches.append(temp)\n\nprint('Building data frame...')  \nmatches = pd.DataFrame(matches, columns=['Match confidence (lower is better)','Matched name','Origional name'])\nprint('Done') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matches.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding close matches through getNearestN"},{"metadata":{"trusted":true},"cell_type":"code","source":"matches.sort_values('Match confidence (lower is better)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In summary, tf-idf can be a highly effective and highly performant way of cleaning, deduping and matching data when dealing with larger record counts."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**References**\n\nhttp://towardsdatascience.com/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49,\n\nhttps://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536,\n\nhttps://bergvca.github.io/2017/10/14/super-fast-string-matching.html?source=post_page-----84f2bfd0c536---------------------- "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n<html>\n<body>\n\n<p><font size=\"5\" color=\"Red\">If you like my kernel please consider upvoting it</font></p>\n<p><font size=\"4\" color=\"Green\">Don't hesitate to give your suggestions in the comment section</font></p>\n\n</body>\n</html>\n"},{"metadata":{},"cell_type":"markdown","source":"# Final"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}