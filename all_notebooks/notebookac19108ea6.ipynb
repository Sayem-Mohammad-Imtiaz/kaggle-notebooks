{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importando os dados\nbase = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col = 0)\noriginal = base\nbase.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# substituindo quantitativos pela media \n\nareaMedia = base['area'].mean()\nbase.update(base['area'].fillna(areaMedia))\n\nquartosMedia = base['rooms'].mean()\nbase.update(base['rooms'].fillna(quartosMedia))\n\nbanheiroMedia = base['bathroom'].mean()\nbase.update(base['bathroom'].fillna(banheiroMedia))\n\nimpostoMedia = base['property tax (R$)'].mean()\nbase.update(base['property tax (R$)'].fillna(impostoMedia))\n\nvalorMedia = base['rent amount (R$)'].mean()\nbase.update(base['rent amount (R$)'].fillna(valorMedia))\n\nseguroMedia = base['fire insurance (R$)'].mean()\nbase.update(base['fire insurance (R$)'].fillna(seguroMedia))\n\ntaxasMedia = base['hoa (R$)'].mean()\nbase.update(base['hoa (R$)'].fillna(taxasMedia))\n\ntotalMedia = base['total (R$)'].mean()\nbase.update(base['total (R$)'].fillna(totalMedia))\n\ntotalMedia = base['parking spaces'].mean()\nbase.update(base['parking spaces'].fillna(totalMedia))\n\n# verificando dados nulos/faltantes \nbase.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verificando dados qualitativos mais frequentes\n\nprint(base['city'].value_counts())\nprint('\\n')\nprint(base['animal'].value_counts())\nprint('\\n')\nprint(base['furniture'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# substituindo dados qualitativos faltantes pelos mais frequentes\nbase['furniture'] = base['furniture'].fillna('not furnished')\nbase['animal'] = base['animal'].fillna('acept')\nbase['city'] = base['city'].fillna('São Paulo')\n\n#excluindo os dados faltantes do andar \n#base = base.dropna(subset=['floor'])\n\n# verificando dados nulos/faltantes \nbase.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# arrumando valores de hoa (R$) total(R$)\n\nnovo = base['hoa (R$)'] < (20000)\nbase = base[novo]\nnovo = base['total (R$)'] < (20000)\nbase = base[novo]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Arrumando valores de rent amount (R$) e fire insurance (R$)\nnovo = base['rent amount (R$)'] < (17500)\nbase = base[novo]\nnovo = base['fire insurance (R$)'] < (300)\nbase = base[novo]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# arrumando valores de property tax (R$)\n\nnovo = np.abs(stats.zscore(base['property tax (R$)'])) < 3\nbase = base[novo]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# arrumando valores de area\n\nnovo = base['area'] < (5000)\nbase = base[novo]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# arrumando valores de area\n\narea = np.abs(stats.zscore(base['area'])) < 3\nbase = base[area]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature engineering\n\n# criando uma nova variavel que será a soma das taxas/impostos/seguro\n\nbase['sumtaxs'] = base['fire insurance (R$)']+ base['hoa (R$)'] + base['property tax (R$)']\nbase[['sumtaxs','fire insurance (R$)', 'hoa (R$)', 'property tax (R$)']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# notamos que a coluna 'rooms' foi considerada a menos importante.\n# exclindo a coluna rooms\n\n#base = base.drop(['rooms'], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# utilizando o metrodo MinMax para normalizar os valores entre 0 e 1\ndados_quantitativos = ['area','bathroom','hoa (R$)','rent amount (R$)','property tax (R$)','fire insurance (R$)','parking spaces','total (R$)','sumtaxs','rooms']\nscaler = MinMaxScaler()\ndata = base[dados_quantitativos]\nscaler.fit(data)\n\ndata_scaled = scaler.transform(data)\ndata_scaled = pd.DataFrame(data_scaled)\ndata_scaled.columns = dados_quantitativos\ndata_scaled.index = base.index\n\nbase = base.drop(dados_quantitativos, axis=1)\nbase = pd.concat([base, data_scaled], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trocando as colunas 'animals', 'furniture' por valores 0 e 1\n\nbase['furniture'].replace(to_replace='furnished', value=1, inplace=True)\nbase['furniture'].replace(to_replace='not furnished', value=0, inplace=True)\n\nbase['animal'].replace(to_replace='acept', value=1, inplace=True)\nbase['animal'].replace(to_replace='not acept', value=0, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# OneHot Encoding\nfrom sklearn.preprocessing import OneHotEncoder\nbase = base[base['city'].notna()]\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(base[['city']])\nenc_df = pd.DataFrame(enc.transform(base[['city']]).toarray(), columns=enc.get_feature_names(['city']))\nbase.reset_index(drop=True, inplace=True)\nenc_df.reset_index(drop=True, inplace=True)\nbase = base.join(enc_df)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# excluindo as colunas 'city' e 'floor'\ndel base['floor']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del base['city']\nbase","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pegamos o dataset df e separamos em x (entrada) e y (saida), numa separação 70% treino e 30% validação\nfrom keras import *\nfrom tensorflow.keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import plot_model\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf = base\n\n# Normalizamos os dados de df em uma escala de [0, 1]\n# Estou fazendo isto aqui pois temos que \"desnormalizar\" na hora de gerar os gráficos de R²\ncolumn_names = base.columns\nscaler = MinMaxScaler()\nscaler.fit(df)\ndf = scaler.transform(df)\ndf = pd.DataFrame(df)\ndf.columns = column_names\n\n# Pegamos o dataset df e separamos em x (entrada) e y (saida), numa separação 70% treino e 30% validação\ninput_dim = df.shape[1] - 1\nx = df.drop(columns='total (R$)')\ny = df['total (R$)']\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.30, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NEURONIOS_CAMADA_INICIAL = 6\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [3]\nUSAR_DROPOUT = False\nDROPOUT_VALUE = 0.2\nTIPO_REGULARIZADOR = 'l1'\nFN_ATIVACAO = 'tanh'\n\n# #####################################################################################\n# Definição da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermediárias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# Última camada da RNA (1 saída)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n#plot_model(model, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\n\nCALLBACKS = [] # Definição dos callbacks a serem utilizados. Isso aqui é opcional, mas pode ajudar: https://keras.io/api/callbacks/early_stopping/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 300\n#OPTIMIZER = 'adam' # 'adam' é o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\nOPTIMIZER = keras.optimizers.SGD(learning_rate=0.001)\n# Compilação do modelo + Definição da Função de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = pyplot.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R² = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}