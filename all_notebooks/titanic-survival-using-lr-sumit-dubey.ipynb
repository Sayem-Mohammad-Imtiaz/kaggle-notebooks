{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Titanic Disaster Survival Using Logistic Regression</h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"**Load the Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"../input/train.csv\")\ntest_data=pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**View the data using head function which returns top  rows**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explaining Dataset**\n\nsurvival : Survival 0 = No, 1 = Yes <br>\npclass : Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd <br>\nsex : Sex <br>\nAge : Age in years <br>\nsibsp : Number of siblings / spouses aboard the Titanic parch # of parents / children aboard the Titanic <br>\nticket : Ticket number fare Passenger fare cabin Cabin number <br>\nembarked : Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton <br>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Import Seaborn for visually analysing the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Find out how many survived vs Died using countplot method of seaboarn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Survived',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Male vs Female Survival**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Survived',hue='Sex',data=train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**See age group of passengeres travelled **<br>\nNote: We will use displot method to see the histogram. However some records does not have age hence the method will throw an error. In order to avoid that we will use dropna method to eliminate null values from graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_data['Age'].dropna())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fill the missing values**<br> we will fill the missing values for age. In order to fill missing values we use fillna method.<br> For now we will fill the missing age by taking average of all age "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Age']=train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age']=test_data['Age'].fillna(test_data['Age'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can verify that no more null data exist** <br> we will examine data by isnull mehtod which will return nothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['Age'].isnull()]\n#test_data[train_data['Age'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Alternatively we will visualise the null value using heatmap**<br>\nwe will use heatmap method by passing only records which are null. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_data.isnull())\n#sns.heatmap(test_data.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see cabin column has a number of null values, as such we can not use it for prediction. Hence we will drop it**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop('Cabin',axis=1,inplace=True)\ntest_data.drop('Cabin',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()\n#test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preaparing Data for Model**<br>\nNo we will require to convert all non-numerical columns to numeric. Please note this is required for feeding data into model. Lets see which columns are non numeric info describe method"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see, Name, Sex, Ticket and Embarked are non-numerical.It seems name and ticket number are not useful for Machine Learning Prediction hence we will eventually drop it. For Now we would convert Embarked and Sex Columns to dummies numerical values******"},{"metadata":{"trusted":true},"cell_type":"code","source":"sex=pd.get_dummies(train_data['Sex'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Sex_m']=sex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(['Name','Sex','Ticket','Embarked','Sex'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sex=pd.get_dummies(test_data['Sex'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['Sex_m']=sex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.drop(['Name','Sex','Ticket','Embarked','Sex'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data.head()\ntest_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_m']\ntarget='Survived'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_data[test_data.isnull()==True]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building Model using Logestic Regression**"},{"metadata":{},"cell_type":"markdown","source":"**Build the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_model=LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_model.fit(train_data[features],train_data[target])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=l_model.predict(test_data[features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**See how our model is performing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived':predict})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Precision is fine considering Model Selected and Available Data. Accuracy can be increased by further using more features (which we dropped earlier) and/or  by using other model**\n\nNote: <br>\nPrecision : Precision is the ratio of correctly predicted positive observations to the total predicted positive observations <br>\nRecall : Recall is the ratio of correctly predicted positive observations to the all observations in actual class\nF1 score - F1 Score is the weighted average of Precision and Recall.\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}