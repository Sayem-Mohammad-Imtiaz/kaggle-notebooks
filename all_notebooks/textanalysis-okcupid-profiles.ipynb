{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objective","metadata":{}},{"cell_type":"markdown","source":"This OkCupid profiles dataset has text data as essays. Every user needs to fill the essay with respect to different dimensions. We found that User did not fill all the essays. But User has filled at least one of the essays. So we decided to combine the all essays with respect to every user and named it as eassy. \n\nNow we want to study the user's different segments or intrests. For this, We want to categories all the essays into various topics using different approaches. These topics will help us to understand the user's interest behavious and this will help us to provide recommendation to every user.\n\nWe are going to applied following steps to achieve our objective:\n* Combine columns essay0 to eassy9 as essay\n* Preprocessing the essay text\n* Perform EDA\n\t1. Wods frequency distribution\n\t2. Part of speech tagging\n* Identify user's intrest behavious using topic modeling","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfrom IPython.display import display\nfrom bokeh.io import output_notebook\nfrom bokeh.models import Label\nfrom bokeh.plotting import figure, output_file, show\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom textblob import TextBlob\nfrom tqdm import tqdm\nimport ast\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport re\nimport scipy.stats as stats\nfrom scipy.sparse import csr_matrix, csc_matrix\nfrom scipy import sparse\nimport seaborn as sb\nimport spacy\nimport string\noutput_notebook()\npd.options.mode.chained_assignment = None","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"full_df = pd.read_csv(\"../input/okcupid-profiles/okcupid_profiles.csv\")\nfull_df['essay'] = full_df[full_df.columns[21:]].apply(lambda x: ' '.join(x.astype(str)), axis=1)\nraw_data = full_df[[\"essay\"]]\nraw_data[\"essay\"] = raw_data[\"essay\"].astype(str)\nraw_data.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computation Challenge\nFor entire text data, we are facing computation challenges from preprocessing to every topic modeling techique. So we have decided to perform our analysis using sampling. \n\nOur solution approach as Sampling:\n*  Random 10% sampling from main dataset. [Iteration-1]\n*  Selective sampling using every categorical dimentions [Iteration-2]","metadata":{}},{"cell_type":"markdown","source":"### Random Sampling [Iteration-1]","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"raw_data = raw_data.sample(frac=0.10)\nraw_data[\"essay\"] = raw_data[\"essay\"].astype(str)\nraw_data.info()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"For preprocessing, we are cleaning text using below criteria.\n1.\tApply lower casing\n2.\tRemove puctualtions\n3.\tRemove numbers\n4.\tRemove stop words\n5.\tRemove rare words\n6.\tRemove frequent words\n7.\tApply lemmatization","metadata":{}},{"cell_type":"code","source":"class TextPreprocessor:\n    APOSTROPHE = '\\u2019'\n    EMOTICONS_REGEX = r'[\\U0001f600-\\U0001f64f]+'\n    DINGBATS_REGEX = r'[\\U00002702-\\U000027b0]+'\n    TRANSPORT_AND_MAP_REGEX = r'[\\U0001f680-\\U0001f6c0]+'\n    ENCLOSED_CHARS_REGEX = r'[\\U000024c2-\\U0001f251]+'\n    MISC_REGEX = r'[\\U000000a9-\\U0001f999]'\n\n    def make_lowercase(self, data_frame, column_name):\n        data_frame[column_name] = data_frame[column_name].str.lower()\n        #data_frame[column_name] = data_frame[column_name].apply(lambda texts: \n        print('make_lowercase applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_punctuation(self, data_frame, column_name):\n        PUNCT_TO_REMOVE = string.punctuation\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: text.translate(str.maketrans('', '', PUNCT_TO_REMOVE)))\n        print('remove_punctuation applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_stop_words(self, data_frame, column_name):\n        STOPWORDS = set(stopwords.words('english'))\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([word for word in str(text).split() if word not in STOPWORDS]))\n        print('remove_stop_words applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_frequent_words(self, data_frame, column_name):\n        cnt = Counter()\n        for text in data_frame[column_name].values:\n            for word in text.split():\n                cnt[word] += 1\n        FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n        print('remove_frequent_words applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def remove_rare_words(self, data_frame, column_name, max_rare_words_count=10):\n        cnt = Counter()\n        for text in data_frame[column_name].values:\n            for word in text.split():\n                cnt[word] += 1\n        RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-max_rare_words_count - 1:-1]])\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n        print('remove_rare_words applied')\n        #data_frame.head(5)\n        return data_frame\n\n    def stem_words(self, data_frame, column_name):\n        stemmer = PorterStemmer()\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n        print('stem_words applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def lemmatize_words(self, data_frame, column_name):\n        lemmatizer = WordNetLemmatizer()\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n        print('lemmatize_words applied')\n        # print(data_frame[column_name])\n        return data_frame\n    \n    def remove_numbers(self, data_frame, column_name):\n        number_pattern = r'\\d+'\n        data_frame[column_name] = data_frame[column_name].apply(\n            lambda text: re.sub(pattern=number_pattern, repl=\" \", string=text))\n        print('remove_numbers applied')\n        return data_frame\n\n\n    def lemmatize_words_v2(self, data_frame, column_name):\n        lemmatizer = WordNetLemmatizer()\n        wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n        # pos_tagged_text = nltk.pos_tag(text.split())\n        data_frame[column_name] = data_frame[column_name].apply(lambda text: \" \".join(\n            [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in\n             nltk.pos_tag(text.split())]))\n        print('lemmatize_words_v2 applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n    def tokenize(self, data_frame, column_name):\n        data_frame[column_name] = data_frame[column_name].apply(lambda text: nltk.tokenize.word_tokenize(text))\n        print('tokenize applied')\n        #print(data_frame[column_name])\n        return data_frame\n\n\n    def clean_text(self, data_frame, column_name):     \n        data_frame_local = self.make_lowercase(data_frame, column_name)\n        data_frame_local = self.remove_punctuation(data_frame_local, column_name)\n        data_frame_local = self.remove_numbers(data_frame_local, column_name)\n        data_frame_local = self.remove_stop_words(data_frame_local, column_name)\n        data_frame_local = self.remove_rare_words(data_frame_local, column_name)\n        data_frame_local = self.remove_frequent_words(data_frame_local, column_name)\n        data_frame_local = self.lemmatize_words_v2(data_frame_local, column_name)\n        #data_frame_local = self.tokenize(data_frame_local, column_name)\n        return data_frame_local\n\n\n    def remove_emojis(data):\n        result = []\n        for word in data:\n            match = []\n            match += re.findall(EMOTICONS_REGEX, word)\n            match += re.findall(ENCLOSED_CHARS_REGEX, word)\n            match += re.findall(DINGBATS_REGEX, word)\n            match += re.findall(TRANSPORT_AND_MAP_REGEX, word)\n            match += re.findall(MISC_REGEX, word)\n            if not match == []:\n                for item in match:\n                    word = word.replace(item, '')\n            result.append(word)\n        return result\n\n\n    def remove_empty_strings(data):\n        return [word for word in data if word != '']","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TextPreprocessor\nWe have developed the TextPreprocessor module to apply all the preprocessing logic in one go.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"text_processor = TextPreprocessor()\nreindexed_data = text_processor.clean_text(raw_data, 'essay')\nreindexed_data.head(5)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Re-indexing the data frame\nCurrent data frame is randomly sampled. So indexing is not correct. We need to re-indexed the dataset.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"reindexed_data[\"essay\"] = reindexed_data[\"essay\"].astype(str)\nreindexed_data.reset_index(drop=True, inplace=True)\nreindexed_data_values = reindexed_data[\"essay\"]\nreindexed_data.head(5)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nWe are going to apply some basic eploratory analysis. We are performing below analysis.\n*   Words frequency distribution\n*   Part of speech tagging","metadata":{}},{"cell_type":"code","source":"# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Words frequency distribution\nHere we can evaluate the top 50 words and their frequency.","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=50, count_vectorizer=count_vectorizer, text_data=reindexed_data[\"essay\"])\n\nfig, ax = plt.subplots(figsize=(20,8))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in OkCupid dataset (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can seet the top 50 word's distribution. Work, Movie, food and book etc are most fequent words user used in their profile.","metadata":{}},{"cell_type":"markdown","source":"### Words statistics","metadata":{}},{"cell_type":"code","source":"tagged_essays = [TextBlob(reindexed_data[\"essay\"][i]).pos_tags for i in range(reindexed_data[\"essay\"].shape[0])]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tagged_essays_df = pd.DataFrame({'tags':tagged_essays})\n\nword_counts = [] \npos_counts = {}\n\nfor eassy in tagged_essays_df[u'tags']:\n    word_counts.append(len(eassy))\n    for tag in eassy:\n        if tag[1] in pos_counts:\n            pos_counts[tag[1]] += 1\n        else:\n            pos_counts[tag[1]] = 1\n            \nprint('Total number of words: ', np.sum(word_counts))\nprint('Mean number of words per eassy: ', np.mean(word_counts))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on this analysis, we have **1.1 million** words and on average **190 words** every eassy contains.","metadata":{}},{"cell_type":"markdown","source":"#### Words distribution","metadata":{}},{"cell_type":"code","source":"y = stats.norm.pdf(np.linspace(0,14,50), np.mean(word_counts), np.std(word_counts))\n\nfig, ax = plt.subplots(figsize=(18,8))\nax.hist(word_counts, bins=range(1,14), density=True);\nax.plot(np.linspace(0,14,50), y, 'r--', linewidth=1);\nax.set_title('Eassy word lengths');\nax.set_xticks(range(1,14));\nax.set_xlabel('Number of words');\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We did not get any distribution in this sample.","metadata":{}},{"cell_type":"markdown","source":"#### Part of Speech tagging for eassay","metadata":{}},{"cell_type":"code","source":"pos_sorted_types = sorted(pos_counts, key=pos_counts.__getitem__, reverse=True)\npos_sorted_counts = sorted(pos_counts.values(), reverse=True)\n\nfig, ax = plt.subplots(figsize=(18,8))\nax.bar(range(len(pos_counts)), pos_sorted_counts);\nax.set_xticks(range(len(pos_counts)));\nax.set_xticklabels(pos_sorted_types);\nax.set_title('Part-of-Speech Tagging for Eassy Corpus');\nax.set_xlabel('Type of Word');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using POS tagging, our corpus has top 3 section as noun (NN), Adjectives (JJ) and Adverbs (RB).","metadata":{}},{"cell_type":"markdown","source":"# Topic Modeling\nFor our OkCupid text corpus dataset, We are going to evaluate clustering algorithim for categorising the user interest behavious into topics. We are using here Random Sample 10% of actual dataset. Since we have limitation on computation. First iteration we will evaluate this random sample and second iteration we will use selective random sample. ","metadata":{}},{"cell_type":"markdown","source":"#### Preprocessing\nWe have sampled corpus data. we need to extract features. So we are using SKLearn's CountVectorizer object to get document-term-matrix. This matrix will be **n x K** dimention where **K** is the number of distinct words with respect to **n** essay.","metadata":{}},{"cell_type":"code","source":"small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\nsmall_text_sample = reindexed_data[\"essay\"]\n\nprint('Essay before vectorization: \\n{}'.format(small_text_sample[10]))\n\nsmall_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n\nprint('Essay after vectorization: \\n{}'.format(small_document_term_matrix[10]))","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on above document-term-matrix, we can see that we have very high-rank and sparse data. Based on this we are selecting Latent Semantic Analysis or Latent Dirichilet Allocation. These two algorithm are using our document-term matrix and will get output as **n x N** topic matrix. Here N is the number of topic categories and n is the number of essay in our sample. here we are providing 10 initial value for N.","metadata":{}},{"cell_type":"code","source":"# Number of topic categories declare\nN = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Latent Semantic Analysis\nThis is the very effective way to truncated singular value decomposition of a high-rank and sparse document-term matrix. This will preserved largest singular values.","metadata":{}},{"cell_type":"code","source":"lsa_model = TruncatedSVD(n_components=N)\nlsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define helper functions\ndef get_keys(topic_matrix):\n    '''\n    returns an integer list of predicted topic \n    categories for a given topic matrix\n    '''\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys\n\ndef keys_to_counts(keys):\n    '''\n    returns a tuple of topic categories and their \n    accompanying magnitudes for a given list of keys\n    '''\n    count_pairs = Counter(keys).items()\n    categories = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories, counts)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lsa_keys = get_keys(lsa_topic_matrix)\nlsa_categories, lsa_counts = keys_to_counts(lsa_keys)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define helper functions\ndef get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n    '''\n    returns a list of n_topic strings, where each string contains the n most common \n    words in a predicted category, in order\n    '''\n    top_word_indices = []\n    for topic in range(N):\n        temp_vector_sum = 0\n        for i in range(len(keys)):\n            if keys[i] == topic:\n                temp_vector_sum += document_term_matrix[i]\n        temp_vector_sum = sparse.csr_matrix(temp_vector_sum)\n        temp_vector_sum = temp_vector_sum.toarray()\n        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n        top_word_indices.append(top_n_word_indices)   \n    top_words = []\n    for topic in top_word_indices:\n        topic_words = []\n        for index in topic:\n            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n            temp_word_vector[:,index] = 1\n            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n        top_words.append(\" \".join(topic_words))         \n    return top_words","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_n_words_lsa = get_top_n_words(20, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lsa)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have predicted the topic category for sample essays. Each topic category are sharing top 50 words for topic intuation.","metadata":{}},{"cell_type":"markdown","source":"#### Topic eassy fequency distribution\nHere we are visulizing the topic category frequency distribution for sampled essays.","metadata":{}},{"cell_type":"code","source":"top_3_words = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lsa_categories, lsa_counts);\nax.set_xticks(lsa_categories);\nax.set_xticklabels(labels);\nax.set_ylabel('Number of essay');\nax.set_title('LSA topic counts');\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on LSA, Majority of our users belongs to topic_0. This leads us as a concern for recommendation. But this does not provide clusting view. Here we are using dimensionality-reduction technique called t-SNE for better explanation of topic category.","metadata":{}},{"cell_type":"code","source":"tsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keys = lsa_keys\ntwo_dim_vectors = tsne_lsa_vectors\n\n\nmean_topic_vectors = []\nfor t in range(N):\n    articles_in_that_topic = []\n    for i in range(len(keys)):\n        if keys[i] == t:\n            articles_in_that_topic.append(two_dim_vectors[i])    \n    #-------------------------------\n#     temp_vector_sum = sparse.csr_matrix(temp_vector_sum)\n#     temp_vector_sum = temp_vector_sum.toarray()\n    #-------------------------------\n    \n    if len(articles_in_that_topic) == 0:\n        print(f'length = {len(articles_in_that_topic)}')\n        print('skip')\n        continue\n    print(f'length = {len(articles_in_that_topic)}')\n    articles_in_that_topic = np.vstack(articles_in_that_topic)\n    mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n    print(mean_article_in_that_topic)\n    print(type(mean_article_in_that_topic))\n    print(mean_article_in_that_topic.ndim)\n    mean_topic_vectors.append(mean_article_in_that_topic)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define helper functions\ndef get_mean_topic_vectors(keys, two_dim_vectors):\n    '''\n    returns a list of centroid vectors from each predicted topic category\n    '''\n    mean_topic_vectors = []\n    for t in range(N):\n        articles_in_that_topic = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles_in_that_topic.append(two_dim_vectors[i])    \n        if len(articles_in_that_topic) == 0:\n#             print(f'length = {len(articles_in_that_topic)}')\n#             print('skip')\n            mean_topic_vectors.append(np.array([0.0 , 0.0]))\n            continue\n        articles_in_that_topic = np.vstack(articles_in_that_topic)\n        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n        \n        mean_topic_vectors.append(mean_article_in_that_topic)\n    return mean_topic_vectors","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colormap = np.array([\n    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\ncolormap = colormap[:N]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_3_words_lsa = get_top_n_words(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\nlsa_mean_topic_vectors = get_mean_topic_vectors(lsa_keys, tsne_lsa_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LSA Topics\".format(N), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_keys])\n\nfor t in range(N):\n    label = Label(x=lsa_mean_topic_vectors[t][0], y=lsa_mean_topic_vectors[t][1], \n                  text=top_3_words_lsa[t], text_color=colormap[t])\n    plot.add_layout(label)\n    \nshow(plot)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here There is no clear sepration of topics. Here we can see the similarity between LSA topic frequency distribution and t-SNE cluster. But we are failed to explained cluster clearly.","metadata":{}},{"cell_type":"markdown","source":"### Latent Dirichilet Allocation\nNow we will repeat this process for LDA. This is a generative probalilistic process.","metadata":{}},{"cell_type":"code","source":"lda_model = LatentDirichletAllocation(n_components=N, learning_method='online', \n                                          random_state=0, verbose=0)\nlda_topic_matrix = lda_model.fit_transform(small_document_term_matrix)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_keys = get_keys(lda_topic_matrix)\nlda_categories, lda_counts = keys_to_counts(lda_keys)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_n_words_lda = get_top_n_words(20, lda_keys, small_document_term_matrix, small_count_vectorizer)\n\nfor i in range(len(top_n_words_lda)):\n    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have predicted the topic category for sample essays. Each topic category are sharing top 10 words for topic intuation.\n","metadata":{}},{"cell_type":"markdown","source":"#### Topic eassy fequency distribution\nHere we are visulizing the topic category frequency distribution for sampled essays.","metadata":{}},{"cell_type":"code","source":"top_3_words = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlabels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lda_categories]\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(lda_categories, lda_counts);\nax.set_xticks(lda_categories);\nax.set_xticklabels(labels);\nax.set_title('LDA topic counts');\nax.set_ylabel('Number of essays');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on LDA, Majority of our users belongs to topic_0 and secondly belogs to topic 2. This leads us as a concern for recommendation. But this does not provide clusting view. Here we are using dimensionality-reduction technique called t-SNE for better explanation of topic category. ","metadata":{}},{"cell_type":"code","source":"tsne_lda_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lda_vectors = tsne_lda_model.fit_transform(lda_topic_matrix)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_3_words_lda = get_top_n_words(3, lda_keys, small_document_term_matrix, small_count_vectorizer)\nlda_mean_topic_vectors = get_mean_topic_vectors(lda_keys, tsne_lda_vectors)\n\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(N), plot_width=700, plot_height=700)\nplot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n\nfor t in range(N):\n    label = Label(x=lda_mean_topic_vectors[t][0], y=lda_mean_topic_vectors[t][1], \n                  text=top_3_words_lda[t], text_color=colormap[t])\n    plot.add_layout(label)\n\nshow(plot)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on above clustering graph, We are unable to explane the topic separation clearly. Now we can conclude that random sampling is not providing the clear prospective. ","metadata":{}}]}