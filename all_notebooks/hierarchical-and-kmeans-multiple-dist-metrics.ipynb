{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport pylab\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nfrom scipy.stats import kurtosis, skew\nimport warnings\nfrom sklearn.metrics import matthews_corrcoef\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Context\nThere are many industries where understanding how things group together is beneficial. For example, retailers want to understand the similarities among their customers to direct advertisement campaigns, and botanists classify plants based on their shared similar characteristics. One way to group objects is to use clustering algorithms. We are going to explore the usefulness of unsupervised clustering algorithms to help doctors understand which treatments might work with their patients.\n\n# Content\nWe are going to cluster anonymized data of patients who have been diagnosed with heart disease. Patients with similar characteristics might respond to the same treatments, and doctors could benefit from learning about the treatment outcomes of patients like those they are treating. The data we are analyzing comes from the V.A. Medical Center in Long Beach, CA. To download the data, visit here.\n\nBefore running any analysis, it is essential to get an idea of what the data look like. The clustering algorithms we will use require numeric data—we'll check that all the data are numeric.'''","metadata":{}},{"cell_type":"markdown","source":"# Preproccessing\n\n* age - Age of patient\n* sex - Gender of patient\n* cp - chest pain type\n* trestbps - Resting blood pressure (in mm Hg on admission to the hospital)\n* chol - Serum cholesterol in mg/dl\n* fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n* restecg - Resting electrocardiographic results\n* thalach - Maximum heart rate achieved\n* exang - Exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest\n* slope - The slope of the peak exercise ST segment\n\nThere are 4 columns representing boolean feature: sex, fbs, resrecg, exang\nAnd 1 column for categorial feature: cp\n\nThe standard k-means algorithm isn't directly applicable to categorical data, for various reasons. The sample space for categorical data is discrete, and doesn't have a natural origin. A Euclidean distance function on such a space isn't really meaningful. \n*As someone put it, \"The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs.\"* \n\nAs long as K-Means does not go well with catergorial/boolean data, theese columns will be deleted.\nMight try alternative algorithm to solve the problem later.","metadata":{}},{"cell_type":"markdown","source":"**Read data, get rid of categorical features**","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-patients/heart_disease_patients.csv', delimiter=',', nrows = 1000)\ndf.dataframeName = 'heart_disease_patients.csv'\nnew_df = df.drop(['id','sex','fbs','restecg','exang','cp','slope'],axis=1).dropna()\nnew_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fist try analyzis","metadata":{}},{"cell_type":"code","source":"def plotPerColumnDistribution(df):\n    # добавить ассиметрию и эксцесс\n    colors = {0:'powderblue',1:'lightsalmon',2:'darkcyan',3:'mediumorchid',4:'plum', 5:'black'}\n    plt.figure(figsize=(20,20))\n    for i,x in enumerate(df.columns):\n        label=f'''Mean: {round(df[x].mean(),2)} \n        Std: {round(df[x].std(),2)}\n        Min: {round(df[x].min(),2)}\n        Q1: {round(df[x].quantile(0.25))}\n        Q2:{round(df[x].quantile(0.5),2)} \n        Q3:{round(df[x].quantile(0.75),2)} \n        Max:{round(df[x].max(),2)}\n        Kurtosis: {round(kurtosis(df[x]),2)}\n        Simmetry: {round(skew(df[x]),2)}\n        '''\n        plt.subplot(3,2,i+1)\n        #plt.figure(figsize=(10,7))\n        plt.title(x.upper(),fontsize=15)\n        sns.distplot(df[x], color=colors[i],label=label)\n        plt.legend(fontsize=15)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPerColumnDistribution(new_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you look at the way the scales change on the graphs, you can see that scaling data to one dimension is absolutely necessary. It is worth noting that\nfeatures differ significantly from\ndirection of skewness, therefore we shoud try\nstandardization as well.","metadata":{}},{"cell_type":"markdown","source":"## Correlation","metadata":{}},{"cell_type":"code","source":"def pair_corr(df):\n    bin_cols = ['sex', 'fbs','exang']\n    corr = pd.DataFrame(columns=df.columns, index=df.columns)\n    for x in corr.columns:\n        for i in corr.index:\n            if x in bin_cols and i in bin_cols:\n                corr[x][i]= matthews_corrcoef(df[x], df[i])\n            elif ((x in bin_cols) and (i not in bin_cols)) or ((x not in bin_cols) and (i  in bin_cols)):\n                corr[x][i] = 0\n            else:\n                corr[x][i] = np.corrcoef(df[x],df[i])[0,1]\n    return corr\n        \n    \ndef r_ij(pair_corr):\n    coeff =[[],[],[],[],[]]\n    for  i in range(0,5):\n        for j in range(0,5):\n            alg_= np.delete(np.delete(pair_corr, i, axis=1), j, axis = 0)#algebraic complement\n            r_ij = ((-1)**(i+j))*np.linalg.det(alg_)\n            coeff[i].append(r_ij)\n    return coeff\n\ndef get_chast_corr(df, coeff):\n    chast_corr = [[],[],[],[],[]]\n    for i in range(0,5):\n        for j in range(0,5):\n            if i == j:\n                r_ij = 1\n            else:\n                r_ij = - coeff[i][j] / (coeff[i][i]*coeff[j][j])**(1/2)\n            chast_corr[i].append(r_ij)\n    return pd.DataFrame(data=chast_corr,\n           index= df.columns,\n          columns= df.columns)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### For future analyzis: glance at correlation of boolean features alone. As we see, it is barely significant.","metadata":{}},{"cell_type":"code","source":"pcorrbin = pair_corr(df).loc[['sex','fbs','exang']][['sex','fbs','exang']]\nsns.heatmap(pcorrbin.astype(float), vmax=1, square=True,annot=True, cmap='BuPu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation between numeric features","metadata":{}},{"cell_type":"code","source":"num_corr = new_df.corr()\nplt.figure(figsize=(8,8))\nplt.title('Correlation between numeric')\nsns.heatmap(num_corr.astype(float), vmax=1, square=True,annot=True, cmap='BuPu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Pearson method was used to calculate the correlation coefficient. No strong\ndependences between the features were found. A weak linear dependence occures\nbetween thalach (maximum heart rate) and age (age).\n\nWith increasing age\nto some extent, the rate of maximum heartbeat decreases. Also, with\nan increase in the maximum heart rate, ST decreases relatively often.","metadata":{}},{"cell_type":"markdown","source":"### Multicollinearity check\n##### Let's check for a false correlation. The coefficient of partial correlation.\n\nThe partial correlation coefficient evaluates the tightness of dependence between two variables with fixed values of the others.\nPartial coefficients are slightly weaker than ordinary pearson coefficients. This shows that the multicollinearity of features is minimal.\n","metadata":{}},{"cell_type":"code","source":"num_corr = new_df.corr()\ncoeff_num = r_ij(num_corr.to_numpy(dtype = 'float')) #partial correlation coefficient \nplt.figure(figsize=(8,8))\nplt.title('Partial correlation between numeric')\nsns.heatmap(get_chast_corr(num_corr, coeff_num), vmax=1, square=True,annot=True,cmap='BuPu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Anomalies detection\n\nBox-plot detects 2 and more anomalies among features:\nchol, trestbps, oldpeak.\n###### Anomalies are not extreme\n","metadata":{}},{"cell_type":"code","source":"colors = {0:'powderblue',1:'lightsalmon',2:'darkcyan',3:'mediumorchid',4:'plum', 5:'black'}\nplt.figure(figsize=(15,15))\nfor i, column in enumerate(new_df.columns):\n    plt.subplot(3,2,i+1)\n    sns.boxplot(new_df[column],color=colors[i]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling \n* The goal of normalization is to convert the original set to the range [0..1] \n* The goal of standardization is to convert the original set to a new one with a mean\n  value of 0 and a standard deviation of 1.","metadata":{}},{"cell_type":"code","source":"standartizer = StandardScaler()\nnormalizer = MinMaxScaler()\nXsscaled = standartizer.fit_transform(new_df)\nXnscaled = normalizer.fit_transform(new_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Choosing a number of clusters for K-Means","metadata":{}},{"cell_type":"markdown","source":"# Which distance metric for K-Means is better?\n\nOften in medical research, in addition to the most popular distance -\nthe Euclidean distance - the cosine distance is used. It takes into account the angle between\nfeature vectors in a multidimensional space. \n\nI performed an analysis\nof the average and single method, both are not indicative and give no\nclue about the cluster structure of the data. Take a look at it if you need. However, I  decided  at the stage of hierarchical\nclustering not to consider the near neighbor method (single), since it is absolutely not\nindicative for any distance metric. ","metadata":{}},{"cell_type":"markdown","source":"## Hierarchy clustering","metadata":{}},{"cell_type":"code","source":"import time\nimport scipy.cluster.hierarchy as sch\n\nstart = time.time()\n\nmetrics = ['euclidean', 'cosine']\nmethods = ['single','average']\n\nfor method in methods:\n    for metric in metrics:    \n        mergings = sch.linkage(Xnscaled, metric=metric,method=method)\n        plt.figure(figsize=(20,5))\n        plt.subplot(1,2,1)\n        sch.dendrogram(mergings,\n                   leaf_rotation=90,\n                   leaf_font_size=10,\n                   )\n        plt.title(f'{metric} distance with {method} method on normalized data')\n\n        mergings = sch.linkage(Xsscaled, metric=metric,method=method)\n        plt.subplot(1,2,2)\n        sch.dendrogram(mergings,\n                   leaf_rotation=90,\n                   leaf_font_size=10,\n                   )\n        plt.title(f'{metric} distance with {method} method on standardized data')\n        plt.show() \n\n\nend = time.time()\nprint(f'Time elapsed: {end - start}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, in most cases, the average distance is not applicable. Only cosine distance on standartized data shows promise (5 clusters)","metadata":{}},{"cell_type":"markdown","source":"Below I test how\nthe Euclidean and Cosine distances behave on Hierarchical Clustering with further neighbor (complete) method\n. Although it would be interesting to consider Mahalanobis and Minkowski distance as well. I haven't found any Kmeans implementations using the built-in python libraries with these distances. If there are some, let me know!","metadata":{}},{"cell_type":"code","source":"start = time.time()\n\nmetrics = ['euclidean', 'cosine']\nmethods = ['complete']\n\n\nfor method in methods:\n    for metric in metrics:    \n        mergings = sch.linkage(Xnscaled, metric=metric,method=method)\n        plt.figure(figsize=(20,5))\n        plt.subplot(1,2,1)\n        sch.dendrogram(mergings,\n                   leaf_rotation=90,\n                   leaf_font_size=10,\n                   )\n        plt.title(f'{metric} distance with {method} method on normalized data')\n\n        mergings = sch.linkage(Xsscaled, metric=metric,method=method)\n        plt.subplot(1,2,2)\n        sch.dendrogram(mergings,\n                   leaf_rotation=90,\n                   leaf_font_size=10,\n                   )\n        plt.title(f'{metric} distance with {method} method on standardized data')\n        plt.show() \n\n\nend = time.time()\nprint(f'Time elapsed: {end - start}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Euclidean distance shows the same number of clusters for\nboth normalized and standardized data. It is worth noting that the\nleftmost \"orange\" cluster is too small and was attached last in the case\nof standardized data, means no good. Most likely outliers grouped there somehow. We better presume 4 clusters.\n\nThe cosine distance returns 2 and 12 clusters, respectively. However, within the study field, two clusters are too obvious and unproven, and 12 clusters, therefore 12 different\ntreatment methods are probably too risky. The cosine distance on\nstandardized data may tend to place similar objects in\ndifferent groups.\n\nAll 4 dendrograms indicate the presence of a cluster structure in the data.","metadata":{}},{"cell_type":"markdown","source":"##### If for some reason you need to devide data by 5 clusters better standartize it and use cosine distance metric. With other options it shows no promise.","metadata":{}},{"cell_type":"markdown","source":"## Elbow mothod\n\nThe \"Elbow\" method involves repeated execution of the algorithm\nwith an increase in the number of clusters, clustering score is calculated as a function of the number\nof clusters and presented on the graph. The score is a form of the ratio of the intracluster distance to\nthe intercluster distance. The optimal number of clusters is determined\nby the location of the \" elbow bend”, i.e. when adding a new cluster does not\nsignificantly improve the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\ndef wss_calculation(K, data, dist, meth):\n    WSS = []\n    for i in range(K):\n        cluster = AgglomerativeClustering(n_clusters= i+1, affinity=dist, linkage=meth)  \n        cluster.fit_predict(data)\n        # cluster index\n        label = cluster.labels_\n        wss = []\n        for j in range(i+1):\n            # extract each cluster according to its index\n            idx = [t for t, e in enumerate(label) if e == j]\n            cluster = data[idx,]\n            # calculate the WSS:\n            cluster_mean = cluster.mean(axis=0)\n            distance = np.sum(np.abs(cluster - cluster_mean)**2,axis=-1)\n            wss.append(sum(distance))\n        WSS.append(sum(wss))\n    return WSS\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = ['euclidean','cosine']\nmethods = ['complete','average']\n\n\nfor method in methods:\n    for metric in metrics:  \n            if (metric == 'euclidean' and method=='average'):\n                break\n            \n            plt.figure(figsize=(20,5))\n            \n\n            WSS=wss_calculation(12, Xnscaled, metric, method)\n            cluster_range = range(1, 13)\n\n            plt.subplot(1,2,1)\n            plt.grid(True)\n            plt.title(f'{metric} distance with {method} method on normalzied data')\n            plt.xlabel('Number of cluster (k)')\n            plt.ylabel('Total intra-cluster variation')\n            plt.plot(cluster_range, WSS, marker = \"x\")\n\n\n\n            WSS=wss_calculation(12, Xsscaled, metric, method)\n            cluster_range = range(1, 13)\n\n            plt.subplot(1,2,2)\n            plt.grid(True)\n            plt.title(f'{metric} distance with {method} method on standartized data')\n            plt.xlabel('Number of cluster (k)')\n            plt.ylabel('Total intra-cluster variation')\n            plt.plot(cluster_range, WSS, marker = \"x\")\n\n            plt.show()\n\nplt.figure(figsize=(20,5))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The elbow for the Euclidean distance and normalized data bends at a point\nrather equal to 6 clusters - after 6, the slope becomes more gentle and does not fluctuate. For standardized data, it is more difficult to choose, but intuitively, from 4 to 6 clusters. For cosine distance and normalized data, the elbow bend is not obvious: 2 or 5 clusters. For standardized data: also 2 or 5 clusters.\n","metadata":{}},{"cell_type":"markdown","source":"## Silhouette method\n\nThe \"silhouette\" coefficient is calculated using the average intra\n-cluster distance (a) and the average distance to the nearest cluster (b) for each sample.\nThe silhouette is calculated as (b - a) / max (a, b). b is the distance between a and the nearest\ncluster that a does not belong to. \n\nYou can calculate the average silhouette value for all samples and use it as\na metric to estimate the number of clusters where the optimal number is at\nthe peak of the average silhouette.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nmetrics = ['euclidean','cosine']\ndata =[Xnscaled, Xsscaled]\nfor metric in metrics:\n    for i in (0,1):\n        plt.figure(figsize=(10,5))\n        hc_silhouette = []\n        hc_scores = []\n        for j in range(2,12):\n            cluster = AgglomerativeClustering(n_clusters= j, affinity=metric, linkage='complete')  \n            y_hc = cluster.fit_predict(data[i])\n            silhouette = silhouette_score(data[i], y_hc)\n            hc_silhouette.append(silhouette)\n\n        plt.subplot(2,1,i+1)\n        if i==0:\n            plt.title(f'{metric.upper()}. The silhouette coefficient for normalized data ')\n        else: \n            plt.title(f'{metric.upper()}. The silhouette coefficient for standartized data ')\n        \n        plt.grid(True)\n        plt.xlabel(\"Number of clusters\",fontsize=14)\n        plt.ylabel(\"Silhouette score\",fontsize=15)\n        plt.plot([i for i in range(2,12)],hc_silhouette, marker = 'x')\n\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Euclidean distance, the silhouette method shows 4 clusters on\nnormalized data and 2 on standardized data.\nFor the cosine distance, the silhouette method shows 2 clusters in both cases.","metadata":{}},{"cell_type":"markdown","source":"# Summary:\nA decision based on all graphs considered apparentlty doesn't exist in this case. \nThe graphs do not give an unambiguous answer and contradict each other.\n\nNotes on the cosine distance:\n\nThe elbow and silhouette graphs for the cosine distance\non both data types can be interpreted in favor of 2 clusters.\nBut look at the dendrogram for standardized data,\nit is noticeable that the two clusters are poor-quality separation, since at the penultimate\nstage of the union, the distance between the two clusters is barely seen.\n\nStandardized data:\nFor standardized data, has a Euclidean distance there has a greater potential to divide the sample by more than\n2 clusters, if you do not take into account the results\nof the silhouette. It is quite possible that in medicine, clusters' size differ significantly. \nGiven the specifics, it will be interesting to try\nto divide the standardized sample into 4 clusters in the hope that the small\norange cluster (displayed on the dendrogram) will successfully join somewhere\n. The dendrogram for the cosine distance looks less\nattractive.\n\n#### For standardized data, we choose the Euclidean distance and 4 clusters. For normalized - cosine distance and 4 clusters, so we could compare results.","metadata":{}},{"cell_type":"markdown","source":"# K-Means","metadata":{}},{"cell_type":"markdown","source":"### Standartized data  - cosine distance","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n#function for grouping clusters in 2d-features dimension\ndef createseq(points, y_hc, cluster,i,j):\n    a = np.array([x[i] for x in points])[np.where(y_hc==cluster)]\n    b = np.array([x[j] for x in points])[np.where(y_hc==cluster)]\n    return np.vstack((a,b))\n\ndef Kmeans1(data, NUM):\n    km = KMeans(init='k-means++', n_clusters=NUM, n_init=12)\n    km.fit_transform(data)\n    y_km = km.labels_\n    ccolors = {0:'powderblue',1:'lightsalmon',2:'darkcyan',3:'mediumorchid',4:'red', 5:'black'}\n    plt.figure(figsize=(20,20))\n    count = 0\n    for i in range(0,5):\n        for j in range(i,5):\n            if i != j:\n                count+=1\n                for c in range(0,NUM):\n                    cluster = c\n                    seq = createseq(data, y_km, cluster,i,j)\n                    means = (seq[0].mean(),seq[1].mean())\n                    plt.subplot(5,2,count)\n                    plt.scatter(seq[0], seq[1],  \n                                c=ccolors[c], \n                                label = f'Cреднее: ({means[0].astype(float).round(3)},{means[1].astype(float).round(3)})') \n                    plt.xlabel(f'{new_df.columns[i]}')\n                    plt.ylabel(f'{new_df.columns[j]}')\n                    plt.legend()\n        \n                #plt.title(new_df.columns[i].upper()+'-'+new_df.columns[j].upper())    \n    plt.show()\n    return(y_km)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clust_eu_stand = Kmeans1(Xsscaled, 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalized data  - euclidean distance","metadata":{}},{"cell_type":"code","source":"from nltk.cluster.kmeans import KMeansClusterer\nfrom nltk.cluster.util import cosine_distance\n\ndef Kmeans2(data, NUM_CLUSTERS):\n    km = KMeansClusterer(NUM_CLUSTERS, distance=cosine_distance, repeats=14)\n    assigned_clusters = km.cluster(data, assign_clusters=True)\n    y_km = np.array(assigned_clusters)\n    ccolors = {0:'powderblue',1:'lightsalmon',2:'darkcyan',3:'mediumorchid',4:'red', 5:'black'}\n    count = 0\n    plt.figure(figsize=(20,20))\n    for i in range(0,5):\n        for j in range(i,5):\n            if i != j:\n                count+=1\n                for c in range(0, NUM_CLUSTERS):\n                    cluster = c\n                    seq = createseq(data, y_km, cluster,i,j)\n                    means = (seq[0].mean(),seq[1].mean())\n                    plt.subplot(5,2,count)\n                    plt.scatter(seq[0], seq[1],  \n                                c=ccolors[c], \n                                label = f'Cреднее: ({means[0].astype(float).round(3)},{means[1].astype(float).round(3)})')\n                    plt.xlabel(f'{new_df.columns[i]}')\n                    plt.ylabel(f'{new_df.columns[j]}')\n                    plt.legend()  \n    plt.show()\n    return(y_km)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clust_cos_norm = Kmeans2(Xnscaled,4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3D diagram (age-chol-thalach)","metadata":{}},{"cell_type":"code","source":"new_df['cluster_norm'] = clust_cos_norm\nnew_df['cluster_stand'] = clust_eu_stand\n\n\n#project the users feature vector in 3 dimensions\nfig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\n\nax.scatter(new_df.iloc[:,0], new_df.iloc[:,3], new_df.iloc[:,4], c=new_df['cluster_norm'].to_numpy(), cmap='viridis', s=20)\n_ = plt.title('Clusters')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As long as data is multi (5) dimensional, 3D and 2D charts might seem chaotic, but it doesn't mean clustering is not appropriate","metadata":{}},{"cell_type":"markdown","source":"## Mean values per cluster","metadata":{}},{"cell_type":"markdown","source":"### Standartized data  - cosine distance","metadata":{}},{"cell_type":"code","source":"means = pd.DataFrame(columns=['Cluster1','Cluster2', 'Cluster3', 'Cluster4'], index=new_df.columns[:-2])\nfor i in range(0,4):\n    df1 = new_df[new_df['cluster_stand'] == i]\n    for x in df1.columns:\n        if x not in ['cluster_norm','cluster_stand']:\n            means.iloc[:,i][x] = round(df1[x].mean(),3)\nmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalized data  - euclidean distance","metadata":{}},{"cell_type":"code","source":"means = pd.DataFrame(columns=['Cluster1','Cluster2', 'Cluster3', 'Cluster4'], index=new_df.columns[:-2])\nfor i in range(0,4):\n    df1 = new_df[new_df['cluster_norm'] == i]\n    for x in df1.columns:\n        if x not in ['cluster_norm','cluster_stand']:\n            means.iloc[:,i][x] = round(df1[x].mean(),3)\nmeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing results\n### Rand index","metadata":{}},{"cell_type":"code","source":"from scipy.special import comb\n\ndef rand_index_score(clusters1, clusters2):\n\n    tp_plus_fp = comb(np.bincount(clusters1), 2).sum()\n    tp_plus_fn = comb(np.bincount(clusters2), 2).sum()\n    A = np.c_[(clusters1, clusters2)]\n    tp = sum(comb(np.bincount(A[A[:, 0] == i, 1]), 2).sum()\n             for i in set(clusters1))\n    fp = tp_plus_fp - tp\n    fn = tp_plus_fn - tp\n    tn = comb(len(A), 2) - tp - fp - fn\n    return (tp + tn) / (tp + fp + fn + tn)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rand_index_score(clust_cos_norm, clust_eu_stand)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results were similar by 76%. I leave the interpretation to the specialists in the subject area.","metadata":{}},{"cell_type":"markdown","source":"Both algorithms showed reasonable results and did not differ too much from\neach other. According to the Rend index, clustering is identical in the case of using different\nscaling methods by almost 80%. The division into 4 clusters looks reasonable and\ncan be considered in further research.","metadata":{}}]}