{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-07T15:54:36.554931Z","iopub.execute_input":"2021-06-07T15:54:36.555361Z","iopub.status.idle":"2021-06-07T15:54:36.570241Z","shell.execute_reply.started":"2021-06-07T15:54:36.555321Z","shell.execute_reply":"2021-06-07T15:54:36.569263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/twitter-airline-sentiment/Tweets.csv')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:36.572538Z","iopub.execute_input":"2021-06-07T15:54:36.574689Z","iopub.status.idle":"2021-06-07T15:54:36.713333Z","shell.execute_reply.started":"2021-06-07T15:54:36.574651Z","shell.execute_reply":"2021-06-07T15:54:36.712402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WE need text and sentiment for analysis, hence clean the data**","metadata":{}},{"cell_type":"code","source":"import re\n\ndf.text=df.text.apply(lambda x : x.lower())\ndf.text = df.text.apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndf.text.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:36.717748Z","iopub.execute_input":"2021-06-07T15:54:36.719848Z","iopub.status.idle":"2021-06-07T15:54:36.850746Z","shell.execute_reply.started":"2021-06-07T15:54:36.719805Z","shell.execute_reply":"2021-06-07T15:54:36.849728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf.airline_sentiment=df.airline_sentiment.map({'neutral' : 1, 'positive':1,'negative':0})\ndf.airline_sentiment[0:10]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:36.852731Z","iopub.execute_input":"2021-06-07T15:54:36.853071Z","iopub.status.idle":"2021-06-07T15:54:36.864112Z","shell.execute_reply.started":"2021-06-07T15:54:36.853036Z","shell.execute_reply":"2021-06-07T15:54:36.862977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\ndef cleanText(text):\n    \n    lemma = WordNetLemmatizer()\n    stp = stopwords.words('english')\n    \n    # This means remove everything except alphabetical and numerical characters\n    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n    \n    text = text.lower()\n    \n    # This mean split sentences by words (\"I am good\" => [\"I\",\"am\",\"good\"])\n    text = nltk.word_tokenize(text)\n    \n    # Lemmatizers convert words to their base form using dictionaries (going => go, bees => be , dog => dog)\n    text = [lemma.lemmatize(word) for word in text]\n    \n    # We should remove stopwords, stopwords are the words that has no special meaning such as I,You,Me,Was\n    text = [word for word in text if word not in stp]\n    \n    # Everything is ready, now we just need join the elements of lists ([\"feel\",\"good\"] => \"feel good\")\n    text = \" \".join(text)\n    \n    return text\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:36.86598Z","iopub.execute_input":"2021-06-07T15:54:36.866438Z","iopub.status.idle":"2021-06-07T15:54:36.873361Z","shell.execute_reply.started":"2021-06-07T15:54:36.8664Z","shell.execute_reply":"2021-06-07T15:54:36.872573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_text=[]\nfor w in df.text:\n    clean_text.append(cleanText(w))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:36.874726Z","iopub.execute_input":"2021-06-07T15:54:36.875334Z","iopub.status.idle":"2021-06-07T15:54:43.285555Z","shell.execute_reply.started":"2021-06-07T15:54:36.875294Z","shell.execute_reply":"2021-06-07T15:54:43.284709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=np.array(clean_text)\ny=df.airline_sentiment\ny=np.array(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:43.286818Z","iopub.execute_input":"2021-06-07T15:54:43.28733Z","iopub.status.idle":"2021-06-07T15:54:43.30209Z","shell.execute_reply.started":"2021-06-07T15:54:43.287292Z","shell.execute_reply":"2021-06-07T15:54:43.301324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x.shape,y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:43.304688Z","iopub.execute_input":"2021-06-07T15:54:43.305035Z","iopub.status.idle":"2021-06-07T15:54:43.30989Z","shell.execute_reply.started":"2021-06-07T15:54:43.304998Z","shell.execute_reply":"2021-06-07T15:54:43.308725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#y = df[['pos_neg','pos_neu','neu_neg']]\n\n#train_vectors = vectorizer.fit_transform(x).toarray()\nX_train, X_test , Y_train, Y_test = train_test_split(x,y, test_size=0.2, random_state=50)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:43.311858Z","iopub.execute_input":"2021-06-07T15:54:43.312264Z","iopub.status.idle":"2021-06-07T15:54:43.325268Z","shell.execute_reply.started":"2021-06-07T15:54:43.312229Z","shell.execute_reply":"2021-06-07T15:54:43.324477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(X_train)\ntest_vectors = vectorizer.transform(X_test)\nprint(train_vectors.shape, test_vectors.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:43.326485Z","iopub.execute_input":"2021-06-07T15:54:43.326826Z","iopub.status.idle":"2021-06-07T15:54:43.593421Z","shell.execute_reply.started":"2021-06-07T15:54:43.326792Z","shell.execute_reply":"2021-06-07T15:54:43.592438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(train_vectors, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:43.594714Z","iopub.execute_input":"2021-06-07T15:54:43.595274Z","iopub.status.idle":"2021-06-07T15:54:43.605144Z","shell.execute_reply.started":"2021-06-07T15:54:43.595235Z","shell.execute_reply":"2021-06-07T15:54:43.604206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from  sklearn.metrics  import accuracy_score\npredicted = clf.predict(test_vectors)\nprint(accuracy_score(Y_test,predicted))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:43.606774Z","iopub.execute_input":"2021-06-07T15:54:43.607248Z","iopub.status.idle":"2021-06-07T15:54:43.61395Z","shell.execute_reply.started":"2021-06-07T15:54:43.607209Z","shell.execute_reply":"2021-06-07T15:54:43.612998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nVocab_size= 500\noov_tok='<OOV>'\n\ntokenizer=Tokenizer(num_words=Vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index=tokenizer.word_index\n\nx_train_sequnces=tokenizer.texts_to_sequences(X_train)\n\ntokenizer.fit_on_texts(X_test)\nx_test_sequnces=tokenizer.texts_to_sequences(X_test)\nx_test_sequnces[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:43.615606Z","iopub.execute_input":"2021-06-07T15:54:43.615955Z","iopub.status.idle":"2021-06-07T15:54:44.073381Z","shell.execute_reply.started":"2021-06-07T15:54:43.615921Z","shell.execute_reply":"2021-06-07T15:54:44.072593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 100\ntrunc_type='post'\npadding_type='post'\nx_train_padded=pad_sequences(x_train_sequnces,maxlen=max_length,truncating=trunc_type,padding=padding_type)\n\nx_test_padded=pad_sequences(x_test_sequnces,maxlen=max_length,truncating=trunc_type,padding=padding_type)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:44.074641Z","iopub.execute_input":"2021-06-07T15:54:44.074976Z","iopub.status.idle":"2021-06-07T15:54:44.180751Z","shell.execute_reply.started":"2021-06-07T15:54:44.07494Z","shell.execute_reply":"2021-06-07T15:54:44.179891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel=tf.keras.Sequential([\n    tf.keras.layers.Embedding(1000,15,input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')    \n])\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:44.182038Z","iopub.execute_input":"2021-06-07T15:54:44.182396Z","iopub.status.idle":"2021-06-07T15:54:44.223729Z","shell.execute_reply.started":"2021-06-07T15:54:44.182361Z","shell.execute_reply":"2021-06-07T15:54:44.222708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(x_train_padded,Y_train,epochs=20,validation_data=(x_test_padded,Y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:54:44.225182Z","iopub.execute_input":"2021-06-07T15:54:44.225528Z","iopub.status.idle":"2021-06-07T15:55:05.935484Z","shell.execute_reply.started":"2021-06-07T15:54:44.225491Z","shell.execute_reply":"2021-06-07T15:55:05.934675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range=range(1,len(acc)+1)\n\nplt.plot(epochs_range, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs_range, val_acc, 'b' ,label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs_range, loss,'bo', label='Training Loss')\nplt.plot(epochs_range, val_loss,'b', label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:55:05.936781Z","iopub.execute_input":"2021-06-07T15:55:05.937106Z","iopub.status.idle":"2021-06-07T15:55:06.218738Z","shell.execute_reply.started":"2021-06-07T15:55:05.937073Z","shell.execute_reply":"2021-06-07T15:55:06.217885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Need to resduce over fitting**","metadata":{}},{"cell_type":"code","source":"embedding_dim=128\nmodel_multiple_bidi_lstm = tf.keras.Sequential([\n    tf.keras.layers.Embedding(3000, embedding_dim, input_length=x_train_padded.shape[1]),\n    tf.keras.layers.SpatialDropout1D(0.4),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,dropout=0.2,recurrent_dropout=0.2)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel_multiple_bidi_lstm.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:55:06.220071Z","iopub.execute_input":"2021-06-07T15:55:06.220421Z","iopub.status.idle":"2021-06-07T15:55:06.464856Z","shell.execute_reply.started":"2021-06-07T15:55:06.220384Z","shell.execute_reply":"2021-06-07T15:55:06.463936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_multiple_bidi_lstm.compile(optimizer='RMSprop',loss='binary_crossentropy',metrics=['accuracy'])\nhistory=model_multiple_bidi_lstm.fit(x_train_padded,Y_train,epochs=20,steps_per_epoch=100,validation_data=(x_test_padded,Y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T15:55:06.466303Z","iopub.execute_input":"2021-06-07T15:55:06.466899Z","iopub.status.idle":"2021-06-07T16:22:37.070916Z","shell.execute_reply.started":"2021-06-07T15:55:06.466857Z","shell.execute_reply":"2021-06-07T16:22:37.070115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range=range(1,len(acc)+1)\n\nplt.plot(epochs_range, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs_range, val_acc, 'b' ,label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs_range, loss,'bo', label='Training Loss')\nplt.plot(epochs_range, val_loss,'b', label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:22:37.072727Z","iopub.execute_input":"2021-06-07T16:22:37.072983Z","iopub.status.idle":"2021-06-07T16:22:37.392256Z","shell.execute_reply.started":"2021-06-07T16:22:37.072956Z","shell.execute_reply":"2021-06-07T16:22:37.391338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}