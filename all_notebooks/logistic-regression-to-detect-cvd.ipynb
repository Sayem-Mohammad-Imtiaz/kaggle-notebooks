{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Logistic regression for cardiovascular disease detection\nUsing a logistic regression to detect CVD results in a model with 72% accuracy on the test set (0.71 and 0.73 on F1 scores for the classes, which is quite good)!\n\nThis is still in somewhat draft status as I wait to hear more about the codebook from the dataset uploader. I don't want to make any false assumptions in my interpretation.\n\nAny feedback/questions are welcome.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Step 1: EDA\nKey things I'm looking for:\n- data types:  we have a mix of categorical (inc. binary) and continuous variables, we need keep that in mind when preprocessing the data\n- class imbalance: the data is approximately balanced, so we won't need to worry about balancing the dataset\n- outliers: for the continous variables, we want to keep an eye on whether there are outliers","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/cardio-vascular-disease-detection/cardio_train.csv', delimiter=';')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I create some data visualizations to get a better sense of the data and next steps:\n- Loop through the categorical variables here and create \"incidence\" charts to help see the incidence (%) of cardiovascular disease by category. This tells me that certain categories definitely do have higher risk. \n- Loop through the continuous variables to get a sense of their distribution and the target. This suggests presence of outliers in most of the continuous variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_incidence(feature):\n    cats = set(data[feature].values)\n    \n    xs = range(0, len(cats))\n    ys_bar=[]\n    ys_line = []\n    \n    for cat in cats:\n        ys_bar.append(data[data[feature] == cat].shape[0])\n        ys_line.append(data[(data[feature] == cat) & (data.cardio == 1)].shape[0]/data.shape[0] * 100)\n    \n    fig, ax = plt.subplots()\n    \n    ax.bar(xs, ys_bar, color='grey')\n\n    ax2 = ax.twinx()\n    ax2.plot(xs, ys_line, color='teal')\n    \n    ax.set_xticks(xs)\n    ax.set_xticklabels(cats, rotation=90)\n    ax.set_xlabel(feature)\n    \n    ax.set_ylabel('Frequency (n)')\n    ax2.set_ylabel('Incidence (%)')\n    \n    fig.suptitle(f\"Cardio incidence by {feature}\")\n    \n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n\n# first I transform the age variable to years instead of days\ndata.age = data.age.apply(lambda x: x / 365)\n\ncat_vars = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in cat_vars:\n    plot_incidence(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in cont_vars:\n    _ = sns.boxplot(x='cardio', y=var, data=data)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Data Processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# i've decided to engineer the bmi feature and remove the height and weight \n# height and weight in general aren't as informative as their ratio to another \n# i then code the bmi based on the standard categories\ndata['bmi'] = round(data.weight/data.height * 100, 2)\ndata['bmi_cat'] = pd.cut(data.bmi, pd.IntervalIndex.from_tuples([(0, 18.5), (18.5, 25), (25, 30), (30, 1000)]))\ncat_vars.append('bmi_cat')\n\ncont_vars.remove('height')\ncont_vars.remove('weight')\n\n# i also want to make age a categorical variable\ndata.age = data.age.apply(lambda x: round(x))\ndata['age_cat'] = pd.qcut(data.age, q=10, duplicates='drop', labels=[x for x in range(0, 10)])\n\ncat_vars.append('age_cat')\ncont_vars.remove('age')\n\n# list for my final variables\nfinal_vars = []\nfinal_vars.extend(cont_vars)\nfinal_vars.extend(cat_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test set\nx_train, x_test, y_train, y_test = train_test_split(data[final_vars], data.cardio, test_size=.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting up a pipeline to transform categorical variables to one hot encoded variables and to scale continuous variables\nct = ColumnTransformer(transformers=[('onehot', OneHotEncoder(), cat_vars), ('scaler', StandardScaler(), cont_vars)])\n\n# note that we only fit it using data that will be used to fit the model: x_train\nct.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transforming x_train and x_test according to pipeline\nx_train = ct.transform(x_train)\nx_test = ct.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Training the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nlr = LogisticRegressionCV(cv=5, penalty='l1', solver='liblinear', refit=True, random_state=42)\n\nstart = time.time()\nlr.fit(x_train, y_train.values)\nend = time.time()\nprint(f\"logistic regression fit in {(end - start) /60} mins\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Evaluating the Model\nBased on the classification report, we have a strong model that does not appear to overfit the problem (as the performance on the train and test are quite similar). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nfor key, value in {'TRAIN': [x_train, y_train], 'TEST': [x_test, y_test]}.items():\n    preds = lr.predict(value[0])\n    print(f\"{key} RESULTS\\n\\n{classification_report(preds, value[1])}\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Interpreting the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = []\nfeature_names.extend(cont_vars)\n\nfor cat in cat_vars:\n    for val in set(data[cat].values):\n        feature_names.append(f\"{cat}_{val}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_coefs = {feature: coefficient for feature, coefficient in zip(feature_names, lr.coef_[0])} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_df = pd.Series(feature_coefs).to_frame()\nfeature_df = feature_df.reset_index()\n\nfeature_df.rename(columns={'index': 'feature', 0: 'log_prob'}, inplace=True)\n\nfeature_df['odds'] = feature_df.log_prob.apply(np.exp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# manually changing the odds ratio for age_cat_8 because it was stretching the plot too much\nfeature_df.at[28, 'odds'] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting\nys = [y for y in range(0, 30)]\nxs = feature_df.odds.values\ncs = []\n\nfor x in xs:\n    if x < 1:\n        cs.append('blue')\n    elif x == 1:\n        \n        cs.append('grey')\n    else:\n        cs.append('orange')\n    \nfig = plt.figure(figsize=(8, 8))\n_ = plt.scatter(xs, ys,s=30,color=cs)\n\nplt.yticks(ticks=ys, labels=feature_df.feature.values)\nplt.xlabel('Odds Ratio')\nplt.ylabel('Feature')\nplt.title('Odds Ratios for Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My preliminary interpretation is:\n- risk increases with age, with particularly high risk at age 61-62 (age_cat_8)\n- risk is also higher for gender 2 (I assume this is male)\n- risk is highest for the underweight BMI, interestingly (BMI 0-18.5%)\n- risk is highest with above normal glucose readings (interestingly not with the well above normal readings)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}