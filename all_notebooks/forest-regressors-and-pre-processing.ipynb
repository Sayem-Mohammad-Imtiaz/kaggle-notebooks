{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/family-income-and-expenditure/Family Income and Expenditure.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing the columns using .head() function"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_data = dataset[dataset.isnull().any(axis=1)]\nprint(null_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our dataset has total 7536 rows which have missing values, so we will have to deal with them respectively. We shall try replacing these missing values by mean and median, and then removing the rows, and try to find the effect on the model. Then we shall draw our conclusions for the dataset. First we shall replace these missing values using mean of the dataset.\nIn the first method we shall be replacing the NAN values using mean of the column."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.fillna(dataset.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall be dropping some columns like: Household Head Occupation, Household Head Class of Worker, Type of Roof, Type of Walls, Toilet Facilities, Main Source of Water Supply.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop(['Household Head Occupation', 'Household Head Class of Worker', 'Type of Roof', 'Type of Walls', 'Toilet Facilities', 'Main Source of Water Supply'], axis= 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have filled the NA and missing values, we shall begin with creating dummy varibales for Region, Main Source of Income, Household Head Sex, Household Head Marital Status, Household Head Highest Grade Completed, Household Head Job or Business Indicator, Type of Household, Type of Building/House and Tenure Status. We shall be using pandas.get_dummies() for that."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.get_dummies(dataset, columns=['Region','Main Source of Income','Household Head Sex', 'Household Head Marital Status', 'Household Head Highest Grade Completed', 'Household Head Job or Business Indicator', 'Type of Household', 'Type of Building/House', 'Tenure Status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = dataset['Total Household Income']\ndataset = dataset.drop(['Total Household Income'], axis = 1)\nx = dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating test and train set, with random_state set at 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we shall be scaling the dataset, and then compare the models accuracy on grounds of scaled and not scaled. The first model we shall be using is Random Forest Regression regression using 10 trees only."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 100, random_state=0)\nregressor.fit(xTrain, yTrain)\npredictedWithoutScaling = regressor.predict(xTest) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we shall compare the predicted and true labels using R2Score and Mean Square Error."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_squared_error\nr2score = r2_score(yTest, predictedWithoutScaling)\nmse = mean_squared_error(yTest, predictedWithoutScaling)\nprint('R2 Score using Random Forest without scaling using mean to fill NA values: ',r2score)\nprint('Mean Squared Error using Random Forest without scaling using mean to fill NA values: ',mse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see that the model predicted not so good without Scaling, we shall now scale the data using MinMaxScaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(xTrain)\nxTrain = scaler.transform(xTrain)\nxTest = scaler.transform(xTest)\nregressor.fit(xTrain, yTrain)\npredictedWithScaling = regressor.predict(xTest) \nr2score2 = r2_score(yTest, predictedWithScaling)\nmse2 = mean_squared_error(yTest, predictedWithScaling)\nprint('R2 Score using Random Forest without scaling using mean to fill NA values: ',r2score2)\nprint('Mean Squared Error using Random Forest without scaling using mean to fill NA values: ',mse2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer that the Random Forest Regressor perfroms same, with or without scaling, So we shall look into another Regressor. Now we shall use Gradient boosted decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nboostedRegressor = GradientBoostingRegressor( loss ='ls', learning_rate = 0.1, n_estimators= 200)\nboostedRegressor.fit(xTrain, yTrain)\nboostedPredicted = boostedRegressor.predict(xTest)\nr2score3 = r2_score(yTest, boostedPredicted)\nmse3 = mean_squared_error(yTest, boostedPredicted)\nprint('R2 Score using Gradient Boosted Forest without scaling using mean to fill NA values: ',r2score3)\nprint('Mean Squared Error using Gradient Boosted Forest without scaling using mean to fill NA values: ',mse3)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}