{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport jieba\nimport pathlib\nfrom os import path \n\nimport numpy as np\nimport pandas as pd\n\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\n\nfrom multiprocessing import Pool","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:47:39.421047Z","iopub.execute_input":"2021-06-25T08:47:39.421592Z","iopub.status.idle":"2021-06-25T08:47:39.42671Z","shell.execute_reply.started":"2021-06-25T08:47:39.421542Z","shell.execute_reply":"2021-06-25T08:47:39.425659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. 配置等相关","metadata":{}},{"cell_type":"markdown","source":"### 0.1 基本路径、训练参数等配置","metadata":{}},{"cell_type":"code","source":"# 路径配置\n# ===================路径相关==================\n# 输入相关base路径\nin_base_dir = '/kaggle/input'\ntrain_data_path = path.join(in_base_dir, 'text-summary', 'train.csv')\ntest_data_path =  path.join(in_base_dir, 'text-summary', 'test.csv')\n\nstopwords_path = path.join(in_base_dir, 'english-and-chinese-stopwords', 'stopwords.txt')\n\n# 输出相关base路径\nout_base_dir = '/kaggle/working'\nmodel_dir = path.join(out_base_dir, 'model')\ndata_dir =  path.join(out_base_dir, 'data')\nresult_dir = path.join(out_base_dir, 'results')\n\nfor path_dir in [model_dir, data_dir, result_dir]:\n    if not path.exists(path_dir):\n        os.mkdir(path_dir)\n\n\n### 处理后的数据输出路径\ntrain_seg_path = path.join(data_dir, 'train_seg_data.csv')\ntest_seg_path = path.join(data_dir, 'test_seg_data.csv')\n\n\nmerged_seg_path = path.join(data_dir, 'merged_train_test_seg_data.csv')  #合并train/test，构造用于训练词向量的数据\n\n\nsave_w2v_model_path = path.join(model_dir, 'word2vector', 'word2vec.model') # 词向量模型路径\nembedding_matrix_path = path.join(model_dir, 'word2vector', 'embedding_matrix') # 词向量矩阵路径\n\n\nvocab_index_to_key_path = path.join(model_dir, 'word2vector', 'vocab_index_to_key.json')# 词向量词汇表路径\nvocab_key_to_index_path = path.join(model_dir, 'word2vector', 'vocab_key_to_index.json')\n\n\ntrain_x_seg_path = path.join(data_dir, 'train_x_seg_data.csv')# train/test数据与标签的路径\ntrain_y_seg_path = path.join(data_dir, 'train_y_seg_data.csv')\ntest_x_seg_path = path.join(data_dir, 'test_x_seg_data.csv')\ntest_y_seg_path = path.join(data_dir, 'test_y_seg_data.csv')\n\n\ntrain_x_pad_path = path.join(data_dir, 'train_x_pad_data.csv')# train/test数据与标签，pad处理后的路径\ntrain_y_pad_path = path.join(data_dir, 'train_y_pad_data.csv')\ntest_x_pad_path = path.join(data_dir, 'test_x_pad_data.csv')\ntest_y_pad_path = path.join(data_dir, 'test_y_pad_data.csv')\n\n\ntrain_x_path = path.join(data_dir, 'train_X')# train/set数据与标签，转换成索引形式后路径\ntrain_y_path = path.join(data_dir, 'train_Y')\ntest_x_path = path.join(data_dir, 'test_X')\ntest_y_path = path.join(data_dir, 'test_Y')\n\n\ndefault_checkpoint_dir = model_dir # 训练结果文件保持路径\n# test_save_dir = path.join(results_dir, 'demotest')\n\n# ===================参数相关==================\n# 词向量配置\nembedding_dim = 100\nword2vec_train_epochs = 5\n\n# 训练参数配置\nbatch_size = 8\nepochs = 10\n\nmax_enc_len = 400\nmax_dec_len = 100\n\nvocab_size = 30000\n\nbeams_size = batch_size\n\n# samples\nsample_total = 82871\n\n# 多进程\ncpu_cores = 4 * 2","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:47:39.428564Z","iopub.execute_input":"2021-06-25T08:47:39.428872Z","iopub.status.idle":"2021-06-25T08:47:39.449022Z","shell.execute_reply.started":"2021-06-25T08:47:39.428844Z","shell.execute_reply":"2021-06-25T08:47:39.447951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 0.2 开启模块设置","metadata":{}},{"cell_type":"markdown","source":"# 1. Vocab类，处理vocab等相关内容","metadata":{}},{"cell_type":"code","source":"def load_embedding_matrix(file_path=embedding_matrix_path, max_vocab_size=102400):\n    embedding_matrix = np.load(file_path + '.npy')\n    flag_matrix = np.zeros_like(embedding_matrix[:Vocab.MASKS_COUNT])\n    return np.concatenate([flag_matrix, embedding_matrix])[: max_vocab_size]\n\n\ndef load_word2vec_model():\n\n    return Word2Vec.load(save_w2v_model_path)\n\n\nclass Vocab:\n\n    PAD_TOKEN = '<PAD>'\n    UNKNOWN_TOKEN = '<UNK>'\n    START_DECODING = '<START>'\n    STOP_DECODING = '<STOP>'\n\n    MASKS = [PAD_TOKEN, UNKNOWN_TOKEN, START_DECODING, STOP_DECODING]\n    MASKS_COUNT = len(MASKS)\n\n    PAD_TOKEN_INDEX = MASKS.index(PAD_TOKEN)\n    UNKNOWN_TOKEN_INDEX = MASKS.index(UNKNOWN_TOKEN)\n    START_DECODING_INDEX = MASKS.index(START_DECODING)\n    STOP_DECODING_INDEX = MASKS.index(STOP_DECODING)\n\n    def __init__(self, vocab_file=vocab_key_to_index_path, vocab_max_size=None):\n\n        self.word2index, self.index2word = self.load_vocab(vocab_file, vocab_max_size)\n        self.count = len(self.word2index)\n\n    @staticmethod\n    def load_vocab(file_path, vocab_max_size=None):\n\n        word2index = {mask: index for index, mask in enumerate(Vocab.MASKS)}\n        index2word = {index: mask for index, mask in enumerate(Vocab.MASKS)}\n\n        vocab_dict = list(json.load(fp=open(file_path, 'r', encoding='utf-8')).items())[:-4]\n        vocab_dict = vocab_dict if vocab_max_size is None else vocab_dict[: vocab_max_size]\n\n        for word, index in vocab_dict:\n            word2index[word] = index + Vocab.MASKS_COUNT\n            index2word[index + Vocab.MASKS_COUNT] = word\n\n        return word2index, index2word\n\n    def word_to_index(self, word):\n\n        return self.word2index[word] if word in self.word2index else self.word2index[self.UNKNOWN_TOKEN]\n\n    def index_to_word(self, word_index):\n\n        assert word_index in self.index2word, f'word index [{word_index}] not found in vocab'\n\n        return self.index2word[word_index]\n\n    def size(self):\n        return self.count","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:47:39.472524Z","iopub.execute_input":"2021-06-25T08:47:39.472847Z","iopub.status.idle":"2021-06-25T08:47:39.487523Z","shell.execute_reply.started":"2021-06-25T08:47:39.472818Z","shell.execute_reply":"2021-06-25T08:47:39.486527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. 数据预处理，构造数据集","metadata":{}},{"cell_type":"markdown","source":"### 2.1 数据预处理集数据保存相关组件","metadata":{}},{"cell_type":"code","source":"# 数据预处理\ndef transform_data(sentence, vocab):\n\n    word_list = sentence.split()\n    # 按照vocab的index进行转换\n    # 遇到位置此次就填充unk的索引\n    idx = [vocab.word2index[word] if word in vocab.word2index else vocab.UNKNOWN_TOKEN_INDEX for word in word_list]\n\n    return idx\n\ndef pad_proc(sentence, max_len, vocab):\n    \"\"\"\n    填充字段\n    < start > < end > < pad > < unk >\n    :param sentence:\n    :param x_max_len:\n    :param vocab:\n    :return:\n    \"\"\"\n    # 0. 按照空格分词\n    word_list = sentence.strip().split()\n\n    # 1. 截取最大长度的词\n    word_list = word_list[:max_len]\n    # 2. 填充<unk>\n    sentence = [word if word in vocab else Vocab.UNKNOWN_TOKEN for word in word_list]\n    # 3. 填充<start>和<end>\n    sentence = [Vocab.START_DECODING] + sentence + [Vocab.STOP_DECODING]\n    # 4. 长度对齐\n    sentence = sentence + [Vocab.PAD_TOKEN] * (max_len - len(word_list))\n\n    return ' '.join(sentence)\n\n\ndef get_max_len(dataframe):\n    \"\"\"\n    获取合适的最大长度\n    :param dataframe: 带统计的数据， train_df['Question']\n    :return:\n    \"\"\"\n    max_lens = dataframe.apply(lambda x: x.count(' ') + 1)\n\n    return int(np.mean(max_lens) + 2 * np.std(max_lens))\n\n\ndef load_stopwords(file_path):\n\n    with open(file_path, 'r', encoding='utf-8') as f:\n        stopwords = f.readlines()\n    f.close()\n\n    stopwords = (word.strip() for word in stopwords)\n    return stopwords\n\n\ndef clean_sent(sent):\n    \"\"\"\n      :param sent: strings\n      :return: 去除非中文字符\n      \"\"\"\n    sent = re.sub(r'[^\\u4e00-\\u9fa5]', '', sent)\n    return sent\n\ndef seg_words(sent):\n    # 分词\n    word_generator = jieba.cut(sent)\n\n    # 过滤条件1\n    remove_words = ['|', '[', ']', '语音', '图片', '你好', '您好']\n    word_generator = (word for word in word_generator if word and word not in remove_words)\n\n    # 过滤条件2\n    stop_words = load_stopwords(file_path=stopwords_path)\n    word_generator = (word for word in word_generator if word and word not in stop_words)\n\n    return ' '.join(word_generator)\n\ndef sentence_proc(sentence):\n\n    # 将原对话拆分为若干个句子\n    sent_generator = sentence.split('|')\n\n    # 每个句子分别进行分处理\n    # 1. 去除非中文符号\n    sent_generator = (clean_sent(sent) for sent in sent_generator)\n\n    # 2. 分词处理\n    sent_generator = (seg_words(sent) for sent in sent_generator)\n\n    # 重新组合成处理后的句子\n    return ' '.join(sent_generator)\n\ndef save_to_csv(dataframe, save_path=path.join(data_dir, 'result.csv'), index=False):\n\n    assert isinstance(dataframe, pd.DataFrame) or isinstance(dataframe, pd.Series), 'Error type .'\n    dataframe.to_csv(save_path, encoding='utf_8_sig', index=index)\n\ndef sentences_proc(dataframe):\n\n    col_list = ['Brand', 'Model', 'Question', 'Dialogue', 'Report']\n\n    for col in col_list:\n        if col in dataframe.columns:\n            dataframe[col] = dataframe[col].apply(sentence_proc, )\n\n    return dataframe\n    \ndef multi_process_csv(dataframe, func):\n\n    # 数据切分\n    data_split = np.array_split(dataframe, cpu_cores)\n\n    # 并发处理\n    with Pool(processes=cpu_cores) as pool:\n        dataframe = pd.concat(pool.map(func, data_split))\n\n    pool.close()\n    pool.join()\n\n    return dataframe\n    \n\ndef pre_process(csv_file_path):\n\n    # 0. 数据读取\n    dataframe = pd.read_csv(csv_file_path)\n    print(f\"data size: {len(dataframe)}\")\n\n    # 1. 空值、重复值处理\n    dataframe.dropna(subset=['Report'], inplace=True)\n    dataframe.fillna('', inplace=True)\n    dataframe.drop_duplicates(keep='first', inplace=True)\n\n    # 2. 句子处理\n    dataframe = multi_process_csv(dataframe, func=sentences_proc)\n\n    return dataframe\n\n\ndef get_processed_data(train_data_path, test_data_path, is_to_save):\n    \"\"\"\n    train data , test data 预处理\n    :param train_data_path: 训练数据路径\n    :param test_data_path: 测试数据路径\n    :return: dataframe, 处理后数据\n    \"\"\"\n    train_df = pre_process(train_data_path)\n    test_df = pre_process(test_data_path)\n\n    # 保存预处理后的数据\n    if is_to_save:\n        save_to_csv(train_df, save_path=train_seg_path)\n        save_to_csv(test_df, save_path=test_seg_path)\n\n    return train_df, test_df\n\n\ndef get_merged_data(train_df, test_df, is_to_save=True):\n    merged_df = pd.concat([train_df, test_df])[['Question', 'Dialogue', 'Report']]\n    merged_df['merged'] = merged_df.apply(lambda x: ' '.join(x), axis=1)\n\n    if is_to_save:\n        save_to_csv(merged_df['merged'], save_path=merged_seg_path)\n\n    return merged_df['merged']\n\ndef get_train_test_split(train_df, test_df, w2v_model):\n\n    train_df['X'] = train_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n    train_df['X'].to_csv(train_x_seg_path, index=None, header=False)\n\n    test_df['X'] = test_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n    test_df['X'].to_csv(test_y_seg_path, index=None, header=False)\n\n    # 标签为Report列\n    train_df['Report'].to_csv(train_y_seg_path, index=None, header=False)\n    test_df['Report'].to_csv(test_y_seg_path, index=None, header=False)\n\n    # 填充开始、结束符号，未知词用oov，长度填充\n    vocab = w2v_model.wv.index_to_key\n\n    # 训练集和测试集的X处理\n    x_max_len = max(get_max_len(train_df['X']), get_max_len(test_df['X']))\n\n    train_df['X'] = train_df['X'].apply(lambda x: pad_proc(x, x_max_len, vocab))\n    test_df['X'] = test_df['X'].apply(lambda x: pad_proc(x, x_max_len, vocab))\n\n    # 训练集和测试集的Y的处理\n    train_y_max_len = get_max_len(train_df['Report'])\n    train_df['Y'] = train_df['Report'].apply(lambda x: pad_proc(x, train_y_max_len, vocab))\n\n    test_y_max_len = get_max_len(train_df['Report'])\n    test_df['Y'] = test_df['Report'].apply(lambda x: pad_proc(x, test_y_max_len, vocab))\n\n    # 保存oov处理后的数据\n    train_df['X'].to_csv(train_x_pad_path, index=False, header=False)\n    train_df['Y'].to_csv(train_y_pad_path, index=False, header=False)\n\n    test_df['X'].to_csv(test_x_pad_path, index=False, header=False)\n    test_df['Y'].to_csv(test_y_pad_path, index=False, header=False)\n\n    # oov和pad处理后的数据，词向量重新训练\n    print('start retrain word2vec model')\n    w2v_model.build_vocab(LineSentence(train_x_pad_path), update=True)\n    w2v_model.train(LineSentence(train_x_pad_path), epochs=word2vec_train_epochs, total_examples=w2v_model.corpus_count)\n\n    w2v_model.build_vocab(LineSentence(train_y_pad_path), update=True)\n    w2v_model.train(LineSentence(train_y_pad_path), epochs=word2vec_train_epochs, total_examples=w2v_model.corpus_count)\n\n    w2v_model.build_vocab(LineSentence(test_x_pad_path), update=True)\n    w2v_model.train(LineSentence(test_x_pad_path), epochs=word2vec_train_epochs, total_examples=w2v_model.corpus_count)\n\n    # 重新保存词向量\n    if not path.exists(path.dirname(save_w2v_model_path)):\n        os.mkdir(path.dirname(save_w2v_model_path))\n    w2v_model.save(save_w2v_model_path)\n    print('finish retrain word2vec model .')\n\n    # 更新vocab\n    vocab = w2v_model.wv.index_to_key\n    print(f'final w2v_model has vocabulary length: {len(vocab)}')\n\n    # 保存到本地\n    vocab_key_to_index = w2v_model.wv.key_to_index\n    vocab_index_to_key = {index: key for key, index in vocab_key_to_index.items()}\n    json.dump(vocab_key_to_index, fp=(open(vocab_key_to_index_path, 'w', encoding='utf-8')), ensure_ascii=False)\n    json.dump(vocab_index_to_key, fp=(open(vocab_index_to_key_path, 'w', encoding='utf-8')), ensure_ascii=False)\n\n    # 保存词向量矩阵\n    embedding_matrix = w2v_model.wv.vectors\n    np.save(embedding_matrix_path, embedding_matrix)\n\n    # 数据集转换，将词转换成索引: [<start> 方向基 ...] -> [2, 403, ...]\n    vocab = Vocab()\n    train_idx_x = train_df['X'].apply(lambda x: transform_data(x, vocab))\n    train_idx_y = train_df['Y'].apply(lambda x: transform_data(x, vocab))\n\n    test_idx_x = train_df['X'].apply(lambda x: transform_data(x, vocab))\n    test_idx_y = train_df['Y'].apply(lambda x: transform_data(x, vocab))\n\n    # 数据转换成numpy数组\n    train_x = np.array(train_idx_x.tolist())\n    train_y = np.array(train_idx_y.tolist())\n\n    test_x = np.array(test_idx_x.tolist())\n    test_y = np.array(test_idx_y.tolist())\n\n    # 数据保存\n    np.save(train_x_path, train_x)\n    np.save(train_y_path, train_y)\n\n    np.save(test_x_path, test_x)\n    np.save(test_y_path, test_y)\n\n    return train_x, train_y, test_x, test_y\n\ndef train_word2vec(file_path=merged_seg_path):\n\n    # 训练词向量\n    model = Word2Vec(\n        LineSentence(source=file_path),\n        vector_size=embedding_dim,\n        sg=1,\n        workers=cpu_cores,\n        window=5,\n        min_count=5,\n        epochs=word2vec_train_epochs,\n    )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:47:39.489011Z","iopub.execute_input":"2021-06-25T08:47:39.489458Z","iopub.status.idle":"2021-06-25T08:47:39.536488Z","shell.execute_reply.started":"2021-06-25T08:47:39.489421Z","shell.execute_reply":"2021-06-25T08:47:39.535617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 数据预处理及数据构造调度函数","metadata":{}},{"cell_type":"code","source":"def build_data(train_data_path, test_data_path):\n\n    # 1. train/test data 预处理, 并保存处理后的文件到本地\n    train_df, test_df = get_processed_data(train_data_path, test_data_path, is_to_save=True)\n    # print(train_df.shape, test_df.shape)\n\n    # 2. 构造train/test用于word2vec训练的数据\n    merged_df = get_merged_data(train_df, test_df, is_to_save=True)\n\n    # 3. 训练词向量\n    w2v_model = train_word2vec(file_path=merged_seg_path)\n    print(w2v_model)\n\n    # 4. 构造训练与测试的的X, y\n    get_train_test_split(train_df, test_df, w2v_model)\n\nif __name__ == '__main__':\n    # 构造数据集\n    if flag_build_data:\n        build_data(train_data_path, test_data_path)\n        print('数据集、词向量等构造完成')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T08:47:39.537693Z","iopub.execute_input":"2021-06-25T08:47:39.538338Z","iopub.status.idle":"2021-06-25T09:09:12.103153Z","shell.execute_reply.started":"2021-06-25T08:47:39.538298Z","shell.execute_reply":"2021-06-25T09:09:12.102065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/model/word2vector/\n!echo \"=======\"\n!ls /kaggle/working/data\n","metadata":{"execution":{"iopub.status.busy":"2021-06-25T09:09:12.104724Z","iopub.execute_input":"2021-06-25T09:09:12.105143Z","iopub.status.idle":"2021-06-25T09:09:14.363573Z","shell.execute_reply.started":"2021-06-25T09:09:12.105095Z","shell.execute_reply":"2021-06-25T09:09:14.362329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/notebook3682a557ea/data\n# !ls /kaggle/input/notebook3682a557ea/data","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:04:55.116697Z","iopub.execute_input":"2021-06-25T10:04:55.117255Z","iopub.status.idle":"2021-06-25T10:04:55.864275Z","shell.execute_reply.started":"2021-06-25T10:04:55.117218Z","shell.execute_reply":"2021-06-25T10:04:55.863147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}