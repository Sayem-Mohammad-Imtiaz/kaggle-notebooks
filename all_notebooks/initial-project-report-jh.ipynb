{"cells":[{"metadata":{"id":"Owt8VShvFbwd"},"cell_type":"markdown","source":"# GRAD-E1326: Python Programming for Data Scientists\n## Ph.D. Hannah BÃ©chara\n### Ji Yoon Han & Mariana G. Carrillo \n\n**Initial Project report: Tweet Sentiment Analysis**\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"9-M2beadFbwj","trusted":false},"cell_type":"code","source":"#Importing libraries for sentiment analysis \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk #Natural Language Processing Package \nimport os #functions for interacting with the operating system\n# import spacy #Models for NLP\n# import torch #also for NLP\nfrom tqdm.notebook import tqdm \n# from transformers import BertTokenizer\n# from torch.utils.data import TensorDataset\n# import transformers #contains pretrained models to perform tasks on texts\n# from transformers import BertForSequenceClassification\n# from wordcloud import WordCloud #For nice wordclouds\n# import tensorflow as tf #Package to develop train models \n# from tensorflow.keras.preprocessing import text \n# from tensorflow.keras.preprocessing import sequence\n# from tensorflow.keras.layers import *\n# from tensorflow.keras.models import Model, Sequential\n# from tensorflow.keras.utils import to_categorical\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem import WordNetLemmatizer\nimport time #for handling dates and times\nimport re \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n# from tensorflow.keras.metrics import AUC\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom nltk.tokenize import TweetTokenizer\n","execution_count":null,"outputs":[]},{"metadata":{"id":"VslZrpiwFbwm"},"cell_type":"markdown","source":"Loading data"},{"metadata":{"id":"XvUqtnTWGLIH","outputId":"7a370c26-7f58-4fcc-b5ad-2bc45a2421f2","trusted":false},"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","execution_count":null,"outputs":[]},{"metadata":{"id":"cfTM2Sn0Gvdy","outputId":"d0c3136a-ca19-442c-f746-6120e6ee8712","trusted":false},"cell_type":"code","source":"# Loading and cleaning data (for Google Colab)\nfrom google.colab import files \nuploaded = files.upload()","execution_count":null,"outputs":[]},{"metadata":{"id":"zMzQaK6PJaCJ","trusted":false},"cell_type":"code","source":"import io\n\ntrain_data = pd.read_csv(io.BytesIO(uploaded['Corona_NLP_train.csv']), encoding = 'latin-1')\ntest_data = pd.read_csv(io.BytesIO(uploaded['Corona_NLP_test.csv']), encoding = 'latin-1')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"id":"Almfwh5OFbwm","trusted":false},"cell_type":"code","source":"# # Loading and cleaning data\n# train_data = pd.read_csv('Corona_NLP_train.csv', encoding='latin-1')\n# test_data = pd.read_csv('Corona_NLP_test.csv', encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"id":"zqfjuZqhFbwp"},"cell_type":"markdown","source":"**Data cleaning**"},{"metadata":{"id":"IjWS-wCWFbwp","outputId":"7b6e2a57-02d2-40c7-d2b8-4827202aca9c","trusted":false},"cell_type":"code","source":"# #Clean Tweets --> TRAIN DATA (these functions run locally, not on jupiter notebooks)\n\n# !pip3 install tweet-preprocessor #processing library \n# # !pip3 install preprocessor\n# import preprocessor as p \n\n# # def preprocess_tweet(row):\n# #     text = row['OriginalTweet']\n# #     text = p.clean(text)\n# #     return text\n\n# train_data['OriginalTweet'] = train_data.apply(preprocess_tweet, axis=1) #remove urls, hashtags, mentions, etc.\n\n# train_data.count() #check tweets\n\n# train_data = train_data.dropna() #drop NAs\n# train_data = train_data.drop_duplicates() #drop duplicates\n\n# train_data.count() #check tweets again\n\n# from gensim.parsing.preprocessing import remove_stopwords #removing stopwords\n\n# def stopword_removal(row):\n#     text = row['OriginalTweet']\n#     text = remove_stopwords(text)\n#     return text\n\n# test_data['OriginalTweet'] = test_data.apply(stopword_removal, axis=1)\n\n# test_data['OriginalTweet'] = test_data['OriginalTweet'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ') #make lowercase, remove punctuation\n\n# test_data.head() #check it worked","execution_count":null,"outputs":[]},{"metadata":{"id":"pO4TVXyISzwc","outputId":"1fe6daaa-2cee-4cd5-db6c-bef4ad4bf79c","trusted":false},"cell_type":"code","source":"!python --version","execution_count":null,"outputs":[]},{"metadata":{"id":"uUt8dJvbSVaa","outputId":"f3adb187-55a6-4603-f8fe-a61264fece4e","trusted":false},"cell_type":"code","source":"!pip3 install tweet-preprocessor\nimport preprocessor as p\n\n# p.clean('test test https://allisonkoh.github.io/')","execution_count":null,"outputs":[]},{"metadata":{"id":"HxfKOKw0Qc4V","outputId":"c068b8ed-e4b3-4aa4-89c5-00d5c96dcfe1","trusted":false},"cell_type":"code","source":"#Clean Tweets --> TRAIN DATA\n\ndef preprocess_tweet(row):\n    text = row['OriginalTweet']\n    text = p.clean(text)\n    return text\n\ntrain_data['OriginalTweet'] = train_data.apply(preprocess_tweet, axis=1) #remove urls, hashtags, mentions, etc.\n\ntrain_data.count() #check tweets\n\ntrain_data = train_data.dropna() #drop NAs\ntrain_data = train_data.drop_duplicates() #drop duplicates\n\ntrain_data.count() #check tweets again\n\nfrom gensim.parsing.preprocessing import remove_stopwords #removing stopwords\n\ndef stopword_removal(row):\n    text = row['OriginalTweet']\n    text = remove_stopwords(text)\n    return text\n\ntrain_data['OriginalTweet'] = train_data.apply(stopword_removal, axis=1)\n\ntrain_data['OriginalTweet'] = train_data['OriginalTweet'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ') #make lowercase, remove punctuation\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"htv3lry0Fbwu","outputId":"1dca7e5d-38ae-4e87-a543-60b1d0a76eca","trusted":false},"cell_type":"code","source":"#Clean Tweets --> TEST DATA\n\ndef preprocess_tweet(row):\n    text = row['OriginalTweet']\n    text = p.clean(text)\n    return text\n\ntest_data['OriginalTweet'] = test_data.apply(preprocess_tweet, axis=1) #remove urls, hashtags, mentions, etc.\n\ntest_data.count() #check tweets\n\ntest_data = test_data.dropna() #drop NAs\ntest_data = test_data.drop_duplicates() #drop duplicates\n\ntest_data.count() #check tweets again\n\nfrom gensim.parsing.preprocessing import remove_stopwords #removing stopwords\n\ndef stopword_removal(row):\n    text = row['OriginalTweet']\n    text = remove_stopwords(text)\n    return text\n\ntest_data['OriginalTweet'] = test_data.apply(stopword_removal, axis=1)\n\ntest_data['OriginalTweet'] = test_data['OriginalTweet'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ') #make lowercase, remove punctuation\n\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"JAlpQcF2Fbww"},"cell_type":"markdown","source":"Data preview"},{"metadata":{"id":"nuommRd7Fbwx","outputId":"abf744b3-9842-4902-c8b7-8bcd03dae0de","trusted":false},"cell_type":"code","source":"#Train data \n\ntrain_data.head(5) #preview\ntrain_data.describe() #descriptive statistics\n","execution_count":null,"outputs":[]},{"metadata":{"id":"secKE6rGFbw0","outputId":"d24c1d3a-d210-454d-f2c6-f1df353e5a4f","trusted":false},"cell_type":"code","source":"#Test data\n\ntest_data.head(5) #preview\ntest_data.describe() #descriptive statistics\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jNUSn4cWFbw3"},"cell_type":"markdown","source":"### Exploratory Data analysis"},{"metadata":{"id":"pQ1lK0FVFbw3"},"cell_type":"markdown","source":"#### Test data"},{"metadata":{"id":"LVh04J51Fbw4","outputId":"c98f97bf-d1a1-47e7-f312-36b0d49341ca","trusted":false},"cell_type":"code","source":"#Create histogram --> Distribution TEST DATA\n#Can we also make this a function?\nplt.figure(figsize=(12,6)) #specifying the size of the figure\nsns.set_palette(\"Spectral\") #color palette\nsns.countplot(x='Sentiment', data=test_data, order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], )\nplt.xlabel('Sentiment(tag)')\nplt.ylabel('Count of tweets')\nplt.suptitle('Histogram of tweet distribution per sentiment classification (Test data)')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"v2IryxTIFbw6","outputId":"07ec99bb-a909-41b0-f434-3494862e20c1","trusted":false},"cell_type":"code","source":"#Distribution of tweet counts \ntest_data.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='line', figsize=(12, 6))\nplt.title('Tweets on Coronavirus March 2020 (Test data)')\nplt.ylabel('Tweet Count')","execution_count":null,"outputs":[]},{"metadata":{"id":"hHUdac22Fbw-","outputId":"a825eb84-3bb9-44f6-f903-3c49ced0a316","trusted":false},"cell_type":"code","source":"#Attempt to create wordcloud - test data\ndef wordcloud2(test_data):\n    stopwords = set(STOPWORDS)\n    stopwords.add(\"https\")\n    stopwords.add(\"00A0\")\n    stopwords.add(\"00BD\")\n    stopwords.add(\"00B8\")\n    stopwords.add(\"ed\")\n    wordcloud2 = WordCloud(background_color=\"white\",stopwords=stopwords).generate(\" \".join([i for i in test_data['OriginalTweet'].str.upper()]))\n    plt.imshow(wordcloud2)\n    plt.axis(\"off\")\n    plt.title(\"Most common words, test data\")\n    figsize=(12, 6)\n\nwordcloud2(test_data)  ","execution_count":null,"outputs":[]},{"metadata":{"id":"YJV2b0BQFbxB"},"cell_type":"markdown","source":"#### Train data"},{"metadata":{"id":"CHn_HAuSFbxC","outputId":"a8045da5-dafa-4149-82bc-6481dd67878a","trusted":false},"cell_type":"code","source":"#Create histogram --> Distribution TRAIN DATA\nplt.figure(figsize=(12,6))\nsns.set_palette(\"Spectral\")\nsns.countplot(x='Sentiment', data=train_data, order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], )\nplt.xlabel('Sentiment (tag)')\nplt.ylabel('Count of tweets')\nplt.suptitle('Histogram of tweet distribution per sentiment classification (Train data)')","execution_count":null,"outputs":[]},{"metadata":{"id":"ygSOjXryFbxE","outputId":"4803d799-2d57-49c4-cd97-1361f35997c8","trusted":false},"cell_type":"code","source":"#Distribution of tweet counts \ntrain_data.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='line', figsize=(12, 6))\nplt.title('Tweets on Coronavirus, March 2020, (Train data)')\nplt.ylabel('Tweet Count')","execution_count":null,"outputs":[]},{"metadata":{"id":"9qooFxqQFbxK","outputId":"e9d0dcc1-ee40-4e35-a409-95e98c74b9d3","trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n#Attempt to create wordcloud\ndef wordcloud1(training_data):\n    stopwords = set(STOPWORDS)\n    stopwords.add(\"https\")\n    stopwords.add(\"00A0\")\n    stopwords.add(\"00BD\")\n    stopwords.add(\"00B8\")\n    stopwords.add(\"ed\")\n    wordcloud1 = WordCloud(background_color=\"white\",stopwords=stopwords).generate(\" \".join([i for i in train_data['OriginalTweet'].str.upper()]))\n    plt.imshow(wordcloud1)\n    plt.axis(\"off\")\n    plt.title(\"Most common words, training data\")\n    figsize=(12, 6)\n\nwordcloud1(train_data)  ","execution_count":null,"outputs":[]},{"metadata":{"id":"nqCYwJS4ar7j","outputId":"7e18a0b3-4b3e-43db-86f6-6a28ff3a65bf","trusted":false},"cell_type":"code","source":"\n# Import nltk / stopwords\nimport nltk\nnltk.download('stopwords')\n\n# Define stopwords \nstop_words = stopwords.words('english') #defining var to remove stopwords in the process_tweet function \n\n# Define function for cleaning tweets \ndef clean_tweet(tweet):\n    tweet = re.sub(r'http\\S+', ' ', tweet) #removing urls\n    tweet = re.sub(r'<.*?>', ' ', tweet)  # removing html tags    \n    tweet = re.sub(r'\\d+', ' ', tweet) #removing digits\n    tweet = re.sub(r'#\\w+', ' ', tweet)    #removing hashtags\n    tweet = re.sub(r'@\\w+', ' ', tweet) #removing mentions\n    tweet = tweet.split() #removing stop words\n    tweet = \" \".join([word for word in tweet if not word in stop_words])\n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"id":"f9B2WuuheWIF","outputId":"ee976a3b-562d-48ab-8c75-cebd6654ecac","trusted":false},"cell_type":"code","source":"# Clean tweets from train data by creating a new column in the train_data df\ntrain_data['CleanTweet'] = train_data['OriginalTweet'].apply(lambda x: clean_tweet(x))\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"k5q6B1vbeWin","outputId":"4197e49d-38ba-4eda-d516-669291699a3b","trusted":false},"cell_type":"code","source":"# Clean tweets from test data by creating a new column in the test_data df\ntest_data['CleanTweet'] = test_data['OriginalTweet'].apply(lambda x: clean_tweet(x))\ntest_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"BTWsYHbsFbxQ"},"cell_type":"markdown","source":"### Tokenization"},{"metadata":{"id":"KqRjZR8wgVRU","outputId":"a07d8c7a-1b15-43c4-d106-3bf131ca3c65","trusted":true},"cell_type":"code","source":"# Comparing different tokenizers (https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7)\n# NOTE: Once the tweets are clean, the tokenizers might not be that different.\nimport nltk\nnltk.download('punkt')\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import WordPunctTokenizer \n# from nltk.tokenize import RegrexTokenizer\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.tokenize import RegexpTokenizer\n\n# Sample clean tweets \ncompare_list = train_data['CleanTweet'].head(10)\n\n# Code for using each tokenizer \n\n## separate words using spaces and punctuations\n\nword_tokens = []\nfor sent in compare_list:\n    print(word_tokenize(sent))\n    word_tokens.append(word_tokenize(sent))\n\n## split all punctuations into separate tokens\n\npunct_tokenizer = WordPunctTokenizer()\n\npunct_tokens = []\nfor sent in compare_list:\n    print(punct_tokenizer.tokenize(sent))\n    punct_tokens.append(punct_tokenizer.tokenize(sent))\n\n## tweet tokenizer \n\ntweet_tokenizer = TweetTokenizer()\n\ntweet_tokens = []\nfor sent in compare_list:\n    print(tweet_tokenizer.tokenize(sent))\n    tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n\n## Match on whitespace\n\nspace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n\nspace_tokens = []\nfor sent in compare_list:\n    \n    print(space_tokenizer.tokenize(sent))\n    space_tokens.append(space_tokenizer.tokenize(sent))\n\n\n# Compare tokenizers \n\ntokenizers = {'word_tokenize': word_tokens,\n             'WordPunctTokenize':punct_tokens,\n            #  'RegrexTokenizer for matching':match_tokens,\n             'RegrexTokenizer for white space': space_tokens,\n             'TweetTokenizer': tweet_tokens }\n\ndf_comparing_tokenizers = pd.DataFrame.from_dict(tokenizers)\n\ndf_comparing_tokenizers.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"sTee8fbGjA66","trusted":false},"cell_type":"code","source":"# Writing a function once you choose a tokenizer (pseudo code below; insert your tokenizer of choice)\n\n# def tokenize_tweet(tweet):\n#   tweet = word_tokenize(tweet) # replace word_tokenize with your tokenizer of choice if you want to use a different one.\n#   return tweet\n\n# test_data['TokenizedTweet'] = test_data['CleanTweet'].apply(lambda x: tokenize_tweet(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"e-HWbLrZFbxT"},"cell_type":"markdown","source":"### References \n* Matplotlib.org. 2020. Pyplot Tutorial â Matplotlib 3.3.2 Documentation. [online] Available at: <https://matplotlib.org/tutorials/introductory/pyplot.html> [Accessed 20 October 2020].\n* Kaggle.com. 2020. Sentiment Prediction. [online] Available at: <https://www.kaggle.com/shahraizanwar/covid19-tweets-sentiment-prediction-rnn-85-acc> [Accessed 18 October 2020].\n* ***Towards Data Science. How to Tokenize Tweets with Python https://towardsdatascience.com/an-introduction-to-tweettokenizer-for-processing-tweets-9879389f8fe7***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}