{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport datetime\n# if using a Jupyter notebook, inlcude:\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Read the data into a DataFrame and display some contents\nfilepath = \"../input/renfe.csv\"\ndata = pd.read_csv(filepath)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are\", data.shape[0], \"rows in this DataFrame\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_prices = data['price'].isnull().sum()\nprint(\"There are\", empty_prices, \"rows without prices that we will drop\")\ndata.dropna(subset=['price'], inplace=True)\nprint(\"After dropping empty prices, we have\", data.shape[0], \"rows\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set the index to the first column \"Unnamed: 0\", then rename the column to \"Id\"\nprint(\"Set the index to the first column named\", data.columns[0], \"then rename it to 'Id'\")\ndata.set_index(data.columns[0], inplace=True)\ndata.index.name='Id'\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The inset_date column does appear to be useful to me, I'll drop this column for now. I'll keep it in another kernel \\\n      to see if it makes a difference with error.\")\ndata.drop(['insert_date'], axis=1, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the data into target and training dataframes/series\ny = data['price']\nX = data.drop(['price'], axis=1)\ndisplay(y.head())\ndisplay(X.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#start_date and end_date have useful date and time information that I want to split into separate columns\n#refer to the link below on how this is done\n#https://stackoverflow.com/questions/35491274/pandas-split-column-of-lists-into-multiple-columns\n#We should \n#print(\"Split start_date's values into a list of date and time\")\n#temp_start_date = X['start_date'].str.split()\n#print(temp_start_date.head())\n#print(\"Create a two column DataFrame with the above\")\n#start_date_dataframe = pd.DataFrame(temp_start_date.values.tolist(), index=temp_start_date.index, columns=['start_date','start_time'])\n#start_date_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#start_date and end_date have useful information that I want to split up and convert to day of the week and hour of the day\nstart_date_day = pd.to_datetime(X['start_date']).dt.day_name()\nstart_date_time = pd.to_datetime(X['start_date']).dt.hour\n\ndisplay(start_date_day.head())\ndisplay(start_date_time.head())\nprint(type(start_date_day), type(start_date_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Rename these Series to prevent overlap when joining to DataFrame X\nstart_date_day.rename('start_date_day', inplace=True)\nstart_date_time.rename('start_date_time', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#join the above Series to DataFrame X\nX = X.join([start_date_day,start_date_time])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Want to calculate the duration of the ride by taking the difference of end_date and start_date\nduration = pd.to_datetime(X['end_date']) - pd.to_datetime(X['start_date'])\nprint(type(duration))\ndisplay(duration.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duration.name = 'duration'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Join the duration Series to X\nX = X.join([duration])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(['start_date','end_date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Follow examples by splitting the data into training and validation datasets before One Hot Encoding\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#need to OneHotEncode the string columns\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=30, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"OH_X_valid['duration'] = OH_X_valid['duration'].dt.seconds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OH_X_train['duration'] = OH_X_train['duration'].dt.seconds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}