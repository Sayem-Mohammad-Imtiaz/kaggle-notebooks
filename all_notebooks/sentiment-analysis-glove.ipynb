{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer #word stemmer class\nlemma = WordNetLemmatizer()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\nfrom keras.layers.embeddings import Embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = dict()\nf = open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_total = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ndisplay(df_total.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(tweet):\n    tweet = tweet.lower() # Convert to lowercase\n    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet) # Remove words with non-ASCII characters\n    words = tweet.split()\n    words = filter(lambda x: x[0]!= '@' , tweet.split()) # Remove user tags\n    words = [word for word in words if word not in set(stopwords.words('english'))] # Remove stop words\n    tweet = \" \".join(words)\n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total['preprocessedTweet'] = df_total.tweet.apply(preprocess_text)\ndisplay(df_total.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = df_total.preprocessedTweet.apply(lambda x: len(x.split())).max()\n\nt = Tokenizer()\nt.fit_on_texts(df_total.preprocessedTweet)\nvocab_size = len(t.word_index) + 1\nencoded_tweets = t.texts_to_sequences(df_total.preprocessedTweet)\npadded_tweets = pad_sequences(encoded_tweets, maxlen=max_length, padding='post')\n\nvocab_size = len(t.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, 50))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(padded_tweets, df_total.label, test_size=0.2, stratify=df_total.label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove = Sequential()\nmodel_glove.add(Embedding(vocab_size, 50, input_length=max_length, weights=[embedding_matrix], trainable=True))\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fit train data\nmodel_glove.fit(x_train, y_train, epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model_glove.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr, rc, thresholds = precision_recall_curve(y_test, y_pred)\nplt.plot(thresholds, pr[1:])\nplt.plot(thresholds, rc[1:])\nplt.show()\ncrossover_index = np.max(np.where(pr == rc))\ncrossover_cutoff = thresholds[crossover_index]\ncrossover_recall = rc[crossover_index]\nprint(\"Crossover at {0:.2f} with recall {1:.2f}\".format(crossover_cutoff, crossover_recall))\nprint(classification_report(y_test, y_pred > crossover_cutoff))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}