{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report\nimport pandas_profiling as pp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/health-care-data-set-on-heart-attack-possibility/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Profiling Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\ndf.info()\n\npp.ProfileReport(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['target']\nX = df.drop('target', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"**For this task I used 5 most popular classification models. Performance estimator = f1_score.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\nclfs = {\n    'LogisticR': LogisticRegression(),\n    'SGD': SGDClassifier(penalty='elasticnet', alpha=0.005),\n    'Random Forest': RandomForestClassifier(n_estimators=1000),\n    'SVC': LinearSVC(C=1, loss='hinge', max_iter=10000),\n    'KNN': KNeighborsClassifier(n_neighbors=3),\n    'GNB': GaussianNB()\n}\n\n# Training & Testing loop\n\nfor i, (name, clf) in enumerate(clfs.items()):\n    if name == 'LogisticR':\n        log = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == 'SGD':\n        sgd = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"Random Forest\":\n        randomforest = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"SVC\":\n        svc = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"GNB\":\n        gnb = pd.Series(clf.fit(X_train, y_train).predict(X_test))\n    elif name == \"KNN\":\n        knn = pd.Series(clf.fit(X_train, y_train).predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = pd.concat([log, sgd, randomforest, svc, gnb, knn], axis=1, keys=['log','sgd','rf','svc','gnb','knn'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfor i in preds.columns:\n    print('F1_score of %s model is %f' % (i,f1_score(y_test,preds[i])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Apparently, the model with the best performance is SGD.**\n**Let's try it with polynomial features.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2)\n\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.fit_transform(X_test)\n\ncross_val_score(SGDClassifier(penalty='elasticnet', alpha=0.0001), X_test_poly, y_test, cv=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Okay, so the CV score of polynomial features ges lower as we add new dimensions. Logically, the relationships between features are more linear-like than quadratic or cubic.**"},{"metadata":{},"cell_type":"markdown","source":"## Plotting learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(GaussianNB(), \n                                                        X, \n                                                        y,\n                                                        # Number of folds in cross-validation\n                                                        cv=10,\n                                                        # Evaluation metric\n                                                        scoring='f1',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 30))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"F1 Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can observe, that F1 score stabilizes as the model gets more samples to train on.**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\ny_scores = gnb.predict_proba(X_train)[:,1]\n#For SGDClassifier, use decision_function.\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n\ndef plot_prc (precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(loc='center left')\n    plt.ylim([0,1])\n\nplot_prc(precisions, recalls, thresholds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And this is how we can shift the decision boundry to play with the precision/recall proportion.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\ny_pred = (gnb.predict_proba(X_test)[:,1] >= 0.1).astype(bool) \n\ny_pred2 = (gnb.predict_proba(X_test)[:,1] >= 0.9).astype(bool) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('If we set the threshold to 0.1, then we get a recall score of %s' % recall_score(y_test, y_pred))\nprint('If we set the threshold to 0.9, then we get a recall score of %s' % recall_score(y_test, y_pred2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}