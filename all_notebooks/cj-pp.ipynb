{"cells":[{"metadata":{"_uuid":"eced6028d3f730e0c64b4b3a1d708451379e14b6"},"cell_type":"markdown","source":"https://www-shanelynn-ie.cdn.ampproject.org/v/s/www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/amp/?usqp=mq331AQCCAE%3D&amp_js_v=0.1#referrer=https%3A%2F%2Fwww.google.com&amp_tf=From%20%251%24s"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import libraries to build Word2Vec model, and load Newsgroups data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport sys\nimport re\nfrom gensim.models import Word2Vec\nfrom gensim.models.phrases import Phraser, Phrases\nTEXT_DATA_DIR = '../input/20_newsgroups/20_newsgroups'\nprint(os.listdir(\"../input/20_newsgroups/20_newsgroups\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2189f75e20300fcee868cff62aa6c064d0e66963"},"cell_type":"code","source":"# Newsgroups data is split between many files and folders.\n# Directory stucture 20_newsgroup/<newsgroup label>/<post ID>\n\ntexts = []         # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []        # list of label ids\nlabel_text = []    # list of label texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0e7be3737a892a3d2a2a382a388c0dec83d5a03"},"cell_type":"code","source":"# Go through each directory\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\n    path = os.path.join(TEXT_DATA_DIR, name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            # News groups posts are named as numbers, with no extensions.\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                f = open(fpath, encoding='latin-1')\n                t = f.read()\n                i = t.find('\\n\\n')  # skip header in file (starts with two newlines.)\n                if 0 < i:\n                    t = t[i:]\n                texts.append(t)\n                f.close()\n                labels.append(label_id)\n                label_text.append(name)\n\nprint('Found %s texts.' % len(texts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a1960e855c3eacacc0a70c98d1e67e169d021ff"},"cell_type":"code","source":"# Cleaning data - remove punctuation from every newsgroup text\nsentences = []\n# Go through each text in turn\nfor ii in range(len(texts)):\n    sentences = [re.sub(pattern=r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]', \n                        repl='', \n                        string=x\n                       ).strip().split(' ') for x in texts[ii].split('\\n') \n                      if not x.endswith('writes:')]\n    sentences = [x for x in sentences if x != ['']]\n    texts[ii] = sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48e7a61bb15bb59ec7c0fffdb63b8a3129791399"},"cell_type":"code","source":"print(texts[6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dea06b2a4a461c0edd86e4ef5a6c89babd15ae5a"},"cell_type":"code","source":"# concatenate all sentences from all texts into a single list of sentences\nall_sentences = []\nfor text in texts:\n    all_sentences += text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ff179f9a0e2c5b8bfe3390fc307361f015b68d3"},"cell_type":"markdown","source":"Phrase Detection\nCommonly occurring multiword expressions (bigrams / trigrams) in text carry different meaning to the words occurring singularly. For example, the words ‘new’ and ‘York’ expressed singularly are inherently different to the utterance ‘New York’. Detecting frequently co-occuring words and combining them can enhance word vector accuracy.\n\nA ‘Phraser‘ from Gensim can detect frequently occurring bigrams easily, and apply a transform to data to create pairs, i.e. ‘New York’ -> ‘New_York’. Pre-processing text input to account for such bigrams can improve the accuracy and usefulness of the resulting word vectors. Ultimately, instead of training vectors for ‘new’ and ‘york’ separately, a new vector for ‘New_York’ is created.\n\nThe gensim.models.phrases module provides everything required in a simple form:"},{"metadata":{"trusted":true,"_uuid":"63b51637ff5a72491d0415d1d5603e1216a90450"},"cell_type":"code","source":"# Phrase Detection\n# Give some common terms that can be ignored in phrase detection\n# For example, 'state_of_affairs' will be detected because 'of' is provided here: \ncommon_terms = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n# Create the relevant phrases from the list of sentences:\nphrases = Phrases(all_sentences, common_terms=common_terms)\n# The Phraser object is used from now on to transform sentences\nbigram = Phraser(phrases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ee3d90309927d354cd865328e37aa252d90fc9d"},"cell_type":"code","source":"len(all_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"529aeb9039a6d049281e0b021a7beafef62a42a7"},"cell_type":"code","source":"print(bigram[all_sentences[5676]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb778024c1997a28600345c25e33e09e420b489c"},"cell_type":"code","source":"# Applying the Phraser to transform our sentences is simply\nall_sentences = list(bigram[all_sentences])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b68d9a653f431172b65e6206831956eca8f7d4"},"cell_type":"markdown","source":"Creating the Word Embeddings using Word2Vec"},{"metadata":{"trusted":true,"_uuid":"9f0eb148d50bfc98958e01d43d286a9694e9ce7b"},"cell_type":"code","source":"model = Word2Vec(all_sentences, \n                 min_count=3,   # Ignore words that appear less than this\n                 size=200,      # Dimensionality of word embeddings\n                 workers=2,     # Number of processors (parallelisation)\n                 window=5,      # Context window for words during training\n                 iter=30)       # Number of epochs training over corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1c967f4607cbe06c8ca8feb4226028a60a5a7c1"},"cell_type":"code","source":"model.vector_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1393e1bbdab8ac5142076ee9d51717e6842c24ba"},"cell_type":"code","source":"len(model.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87c2bdd4df06c48e501eea6c8dd93617b7b4caf1"},"cell_type":"code","source":"model.most_similar('New_York')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}