{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis of Airline users using thier tweets."},{"metadata":{},"cell_type":"markdown","source":"**Breakdown of this notebook:**\n\n1. Loading the dataset: Load the data and import the libraries.\n2. Data Preprocessing:\n     - Analysing missing data. \n     - Removing redundant columns.\n3. Visualising and counting sentiments of tweets for each airline.\n4. Wordcloud plots for **positive** and **negative** tweets to visualise most frequent words for each.\n5. Analysing the reasons for **negative tweets** for each airline.\n6. Visualising negative tweet-sentiment relationship with dates.\n7. Predicting the tweet sentiments with tweet text data with:\n      - SVM(Support Vector Machine)\n      - Decision Tree Classifier\n      - Random Forest Classifier\n8. Calculating accuracies, plotting the confusion matrix and comparing the models."},{"metadata":{},"cell_type":"markdown","source":"### References:- \nI learnt a lot from this blog which shows you how to handle nlp data and implement data preprocessing and explanatory visualization for better understanding.\n\nhttps://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/"},{"metadata":{},"cell_type":"markdown","source":"### Importing the libraries and loading the data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #data visualisation\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint(\"All libraries are imported\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df= pd.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"The first step should be to check the shape of the dataframe and then check the number of null values in each column.\n\nIn this way we can get an idea of the redundant columns in the data frame depending on which columns have the highest number of null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the dataframe is\",df.shape)\nprint(\"The number of nulls in each column are \\n\", df.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get a better idea, lets calculate the percentage of nulls or NA values in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Percentage null or na values in df\")\n((df.isnull() | df.isna()).sum() * 100 / df.index.size).round(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here **tweet_coord , airline_sentiment_gold, negativereason_gold**  have more than 90% missing data. It will be better to delete these columns as they will not provide any constructive information.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['tweet_coord']\ndel df['airline_sentiment_gold']\ndel df['negativereason_gold']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Airline sentiments for each airline\n"},{"metadata":{},"cell_type":"markdown","source":"- Firstly lets calculate the total number of tweets for each airline\n- Then, we are going to get the barplots for each airline with respect to sentiments of tweets (positive,negative or neutral).\n- This will give us a clear idea about the airline sentiments and airlines relationship. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of tweets for each airline \\n \",df.groupby('airline')['airline_sentiment'].count().sort_values(ascending=False))\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nplt.figure(1,figsize=(15,15))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    new_df=df[df['airline']==i]\n    count=new_df['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    plt.bar(Index,count, color=['blue', 'green', 'red'])\n    plt.xticks(Index,['negative','neutral','positive'])\n    plt.ylabel('Mood Count')\n    plt.xlabel('Mood')\n    plt.title('Count of Moods of '+i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - United, US Airways, American substantially get negative reactions.\n - Tweets for Virgin America are the most balanced."},{"metadata":{},"cell_type":"markdown","source":"### Most used words in Positive and Negative tweets "},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The goal is to firstly get an idea of the most frequent words in negative tweets.\n- Get idea about most frequent words in positive tweets."},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud for Negative sentiments of tweets"},{"metadata":{},"cell_type":"markdown","source":"Wordcloud is a great tool for visualizing nlp data. The larger the words in the wordcloud image , the more is the frequency of that word in our text data."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df=df[df['airline_sentiment']=='negative']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud for positive reasons"},{"metadata":{},"cell_type":"markdown","source":"The code for getting positive sentiments is completely same with the one for negative sentiments. Just replace negative with positive in the first line."},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df=df[df['airline_sentiment']=='positive']\nwords = ' '.join(new_df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(15,15))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets try and calculate the highest frequency words in postive sentimental tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate highest frequency words in positive tweets\ndef freq(str): \n  \n    # break the string into list of words  \n    str = str.split()          \n    str2 = [] \n  \n    # loop till string values present in list str \n    for i in str:              \n  \n        # checking for the duplicacy \n        if i not in str2: \n  \n            # insert value in str2 \n            str2.append(i)  \n              \n    for i in range(0, len(str2)): \n        if(str.count(str2[i])>50): \n            print('Frequency of', str2[i], 'is :', str.count(str2[i]))\n        \nprint(freq(cleaned_word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Words like **Thanks**, **best**, **customer** , **love**, **flying** , **good** are understandably present in the **most frequent** words of positive tweets. \n* However, other than these, most of the words are stop words and need to be filtered. We will do so later.\n* Lets try and visualize the reasons for negative tweets first !!"},{"metadata":{},"cell_type":"markdown","source":"### What are the reasons for negative sentimental tweets for each airline ?"},{"metadata":{},"cell_type":"markdown","source":"#### We will explore the **negative reason** column of our dataframe to extract conclusions about negative sentiments in the tweets by the customers "},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the number of negative reasons\ndf['negativereason'].nunique()\n\n\nNR_Count=dict(df['negativereason'].value_counts(sort=False))\ndef NR_Count(Airline):\n    if Airline=='All':\n        a=df\n    else:\n        a=df[df['airline']==Airline]\n    count=dict(a['negativereason'].value_counts())\n    Unique_reason=list(df['negativereason'].unique())\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame\n\n\n\ndef plot_reason(Airline):\n    \n    a=NR_Count(Airline)\n    count=a['count']\n    Index = range(1,(len(a)+1))\n    plt.bar(Index,count, color=['blue','yellow','red','orange','black','brown','gray','cyan','purple','green'])\n    plt.xticks(Index,a['Reasons'],rotation=90)\n    plt.ylabel('Count')\n    plt.xlabel('Reason')\n    plt.title('Count of Reasons for '+Airline)\n    \nplot_reason('All')  \nplt.figure(2,figsize=(15,15))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    plot_reason(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Customer Service Issue** is the main neagtive reason for US Airways,United,American,Southwest,Virgin America\n- **Late Flight** is the main negative reason for Delta  \n- Interestingly, Virgin America has the least count of negative reasons (all less than 60)\n- Contrastingly to Virgin America, airlines like US Airways,United,American have more than 500 negative reasons (Late flight, Customer Service Issue)"},{"metadata":{},"cell_type":"markdown","source":"### Is there a relationship between negative sentiments and date ?"},{"metadata":{},"cell_type":"markdown","source":"Our dataframe has data from **2015-02-17** to **2015-02-24**\n\nIt will be interesting to see if the date has any effect on the sentiments of the tweets(*especially negative !*). We can draw various conclusions by visualizing this."},{"metadata":{"trusted":true},"cell_type":"code","source":"date = df.reset_index()\n#convert the Date column to pandas datetime\ndate.tweet_created = pd.to_datetime(date.tweet_created)\n#Reduce the dates in the date column to only the date and no time stamp using the 'dt.date' method\ndate.tweet_created = date.tweet_created.dt.date\ndate.tweet_created.head()\ndf = date\nday_df = df.groupby(['tweet_created','airline','airline_sentiment']).size()\n# day_df = day_df.reset_index()\nday_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows the sentiments of tweets for each date from **2015-02-17** to **2015-02-24** for every airline in our dataframe.\n\nOur next step will be to plot this and get better visualization for negative tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"day_df = day_df.loc(axis=0)[:,:,'negative']\n\n#groupby and plot data\nax2 = day_df.groupby(['tweet_created','airline']).sum().unstack().plot(kind = 'bar', color=['blue', 'green', 'red','yellow','orange','purple'], figsize = (15,8), rot = 70)\nlabels = ['American','Delta','Southwest','US Airways','United','Virgin America']\nax2.legend(labels = labels)\nax2.set_xlabel('Date')\nax2.set_ylabel('Negative Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Interestingly, **American** has a sudden upsurge in negative sentimental tweets on **2015-02-23**, which reduced to half the very next day **2015-02-24**. (*I hope American is doing better these days and resolved their Customer Service Issue as we saw before*)\n- **Virgin America** has the least number of negative tweets throughout the weekly data that we have. It should be noted that the total number of tweets for **Virgin America** was also significantly less as compared to the rest airlines, and hence the least negative tweets.\n- The negative tweets for all the rest airlines is slightly skewed towards the end of the week !"},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing the tweet text data"},{"metadata":{},"cell_type":"markdown","source":"Now, we will clean the tweet text data and apply classification algorithms on it"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tweet_to_words(tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['clean_tweet']=df['text'].apply(lambda x: tweet_to_words(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The data is split in the standard 80,20 ratio.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = train_test_split(df,test_size=0.2,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\ntest_features=v.transform(test_clean_tweet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediciting sentiments from tweet text data "},{"metadata":{},"cell_type":"markdown","source":"- SVM(Support Vector Machine)\n- Decision Tree Classifier\n- Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"Classifiers = [\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['airline_sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['airline_sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['airline_sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))\n    print(classification_report(pred,test['airline_sentiment']))\n    cm=confusion_matrix(pred , test['airline_sentiment'])\n    plt.figure()\n    \n    plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Reds)\n    plt.xticks(range(3), ['Negative', 'Neutral','Positive'], fontsize=16,color='blue')\n    plt.yticks(range(3), ['Negative', 'Neutral','Positive'], fontsize=16,color='blue')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we you can see above we have plotted the **Confusion Matrix** for predicted sentiments and actual sentiments (negative,neutral and positive)\n- **Random Forest Classifier** gives us the best accuracy score, precision scores according to the classification report.\n- The confusion matrix shows the TP,TN,FP,FN for all the 3 sentiments(negative,neutral and positive)\n  Here also **Random Forest Classifier** gives **better** results than the **Decision Tree Classifier** and **SVM**.\n  \n  "},{"metadata":{"trusted":true},"cell_type":"code","source":"Index = [1,2,3]\nplt.figure(figsize=(15,10))\nplt.bar(Index,Accuracy,color=['orange','red','blue'])\nplt.xticks(Index, Model,rotation=45)\nplt.ylabel('Accuracy')\nplt.xlabel('Model')\nplt.title('Accuracies of Models')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thanks for stay tuned...."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}