{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nfile = '../input/record/record_fix.csv'\n\n# tentukan lokasi file, nama file, dan inisialisasi csv\nf = open(file, 'r')\nreader = csv.reader(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopword = set(stopwords.words('indonesian'))\n\n#start process_tweet\ndef processTweet(tweet):\n    # process the tweets\n    #Convert to lower case\n    tweet = tweet.lower()\n    #Convert www.* or https?://* to URL\n    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',tweet)\n    #Convert @username to AT_USER\n    tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n    #Remove additional white spaces\n    tweet = re.sub('[\\s]+', ' ', tweet)\n    #Replace #word with word\n    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n    #trim\n    tweet = tweet.strip('\\'\"')\n    return tweet\n#end\n\n#start replaceTwoOrMore\ndef replaceTwoOrMore(s):\n    #look for 2 or more repetitions of character and replace with the character itself\n    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n    return pattern.sub(r\"\\1\\1\", s)\n#end\n\n#start getfeatureVector\ndef getFeatureVector(tweet):\n    featureVector = []\n    #split tweet into words\n    words = tweet.split()\n    for w in words:\n        #replace two or more with two occurrences\n        w = replaceTwoOrMore(w)\n        #strip punctuation\n        w = w.strip('\\'\"?,.')\n        #check if the word stats with an alphabet\n        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n        #ignore if it is a stop word\n        if(w in stopword or val is None):\n            continue\n        else:\n            featureVector.append(w.lower())\n    return featureVector\n#end","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nf = open(file, 'r')\nreader = csv.reader(f)\nline = f.readline()\n\nprint(\"Case Folding : \")\nprint()\n\ncase_folding = []\nfor row in reader:\n    cf = processTweet(row[0])\n    featureVector = getFeatureVector(cf)\n    #print(featureVector)\n    case_folding.append(featureVector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import array as arr\nimport csv\n\n#Read the tweets one by one and process it\ninpTweets = csv.reader(open('../input/data-training/testSentimen.csv', 'r'), delimiter=',', quotechar='|')\ntweets = []\nfeatureList = []\nfor row in inpTweets:\n    sentiment = row[0]\n    tweet = row[1]\n    processedTweet = processTweet(tweet)\n    featureVector = getFeatureVector(processedTweet)\n    tweets.append((featureVector, sentiment))\n    featureList = featureList + featureVector\n#end loop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#start extract_features\ndef extract_features(tweet):\n    tweet_words = set(tweet)\n    features = {}\n    for word in featureList:\n        features['contains(%s)' % word] = (word in tweet_words)\n    return features\n#end\npos = 0\nneg = 0\nneu = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk.classify\n# Remove featureList duplicates\nfeatureList = list(set(featureList))\n\n# Generate the training set\ntraining_set = nltk.classify.util.apply_features(extract_features, tweets)\n\n# Train the Naive Bayes classifier\nNBClassifier = nltk.NaiveBayesClassifier.train(training_set)\n\nf = open(file, 'r')\nline = f.readline()\n\n\n# Test the classifier\n#testTweet = 'baru saja donor darah, tangan saya masih sakit'\ntestTweet = 'ujaran'\nprocessedTestTweet = processTweet(testTweet)\nsentiment = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet)))\nprint (\"testTweet = %s, sentiment = %s\\n\" % (testTweet, sentiment))\nx = 0\n\nprint(\"ANALISIS SENTIMEN\")\nprint()\nsentimen=[]\nwhile line:\n    processedTweet = processTweet(line)\n    featureVector = getFeatureVector(processedTweet)\n    line = f.readline()\n    testTweet = processedTweet\n    processedTestTweet = processTweet(testTweet)\n    sentiment = NBClassifier.classify(extract_features(getFeatureVector(processedTestTweet)))\n    sentimen.append(sentiment)\n    print (\"testTweet = %s, sentiment = %s\\n\" % (featureVector, sentiment))\n    \n    myData=featureVector\n    x = x + 1\n    if sentiment == 'positive':\n        pos = 1 + pos\n    elif sentiment == 'negative':\n        neg = 1 + neg\n    elif sentiment == 'neutral':\n        neu = 1 + neu\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Jumlah Sentiment Positive : ')\nprint(pos)\nprint('Jumlah Sentiment Negative : ')\nprint(neg)\nprint('Jumlah Sentiment Neutral :')\nprint(neu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint('Analisis Sentimen')\nlabels = 'Positive', 'Negative', 'Neutral'\nsizes = [pos, neg, neu]\ncolors = ['steelblue', 'slategray', 'gray']\nexplode = (0.2, 0.1, 0.1)  # explode 1st slice\n \n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\n \nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}