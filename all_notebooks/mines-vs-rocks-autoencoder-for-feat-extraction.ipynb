{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup\n\nLoading the libraries to use","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras import Input\nfrom keras import layers\nfrom keras import Model\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:18:11.474527Z","iopub.execute_input":"2021-06-15T21:18:11.475253Z","iopub.status.idle":"2021-06-15T21:18:18.755379Z","shell.execute_reply.started":"2021-06-15T21:18:11.475142Z","shell.execute_reply":"2021-06-15T21:18:18.754385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Description\nThe [dataset](https://medium.com/r/?url=http%3A%2F%2Farchive.ics.uci.edu%2Fml%2Fdatasets%2Fconnectionist%2Bbench%2B%28sonar%2C%2Bmines%2Bvs.%2Brocks%29) of this notebook contains patterns obtained by bouncing sonar signals off a metal cylinder and rocks at various angles and under various conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. Each pattern is a set of 60 frequency bins scaled to a range from 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time.","metadata":{}},{"cell_type":"code","source":"sonar = pd.read_csv('../input/mines-vs-rocks/sonar.all-data.csv', header=None)\nsonar.head()\nplt.figure(figsize=(8,5))\ncorr = sonar.corr()\nsns.heatmap(corr, cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:18:18.756747Z","iopub.execute_input":"2021-06-15T21:18:18.757031Z","iopub.status.idle":"2021-06-15T21:18:19.433859Z","shell.execute_reply.started":"2021-06-15T21:18:18.75698Z","shell.execute_reply":"2021-06-15T21:18:19.432965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As it can be seen in the previous plot, there is some correlation between variables, this information can be compressed with the use of an autoencoder. Now let's make a sample representation of the power spectral density of two of the samples included in the dataset.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(sonar[sonar[60] == 'R'].values[0][:-1], label='Rock', color='lightblue')\nplt.plot(sonar[sonar[60] == 'M'].values[0][:-1], label='Metal', color='dodgerblue', linestyle='--')\nplt.legend()\nplt.title('Example of both classes')\nplt.xlabel('Frequency bin')\nplt.ylabel('Power spectral density (normalized)')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:18:19.435598Z","iopub.execute_input":"2021-06-15T21:18:19.435875Z","iopub.status.idle":"2021-06-15T21:18:19.651454Z","shell.execute_reply.started":"2021-06-15T21:18:19.435847Z","shell.execute_reply":"2021-06-15T21:18:19.650452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at the categories distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(3,3))\nsns.catplot(x=sonar[60], kind='count', data=sonar, palette='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:18:19.653091Z","iopub.execute_input":"2021-06-15T21:18:19.653378Z","iopub.status.idle":"2021-06-15T21:18:19.818088Z","shell.execute_reply.started":"2021-06-15T21:18:19.653342Z","shell.execute_reply":"2021-06-15T21:18:19.816979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Splitting\nLet's conform the inputs, target variable and split the test and train dataset for further processing.","metadata":{}},{"cell_type":"code","source":"X = sonar.loc[:,:59]\ny = sonar.loc[:,60]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=1)\n\nn_inputs = X.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:18:19.819706Z","iopub.execute_input":"2021-06-15T21:18:19.820114Z","iopub.status.idle":"2021-06-15T21:18:19.832121Z","shell.execute_reply.started":"2021-06-15T21:18:19.82008Z","shell.execute_reply":"2021-06-15T21:18:19.831088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modelling the Autoencoder\nBelow is the model of the autoencoder that we'll use to compress the information from the dataset.","metadata":{}},{"cell_type":"code","source":"# Model AutoEncoder\n\ninputs = Input(shape=(n_inputs,))\nencoded = layers.Dense(n_inputs/2)(inputs)\nencoded = BatchNormalization()(encoded)\nencoded = LeakyReLU()(encoded)\nencoded = layers.Dense(n_inputs/3)(encoded)\nencoded = BatchNormalization()(encoded)\nencoded = LeakyReLU()(encoded)\n\nbottleneck = layers.Dense(n_inputs/6)(encoded)\n\ndecoded = layers.Dense(n_inputs/3)(bottleneck)\ndecoded = BatchNormalization()(decoded)\ndecoded = LeakyReLU()(decoded)\ndecoded = layers.Dense(n_inputs/2)(decoded)\ndecoded = BatchNormalization()(decoded)\ndecoded = LeakyReLU()(decoded)\noutputs = layers.Dense(n_inputs, activation='sigmoid')(decoded)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:18:19.835321Z","iopub.execute_input":"2021-06-15T21:18:19.835696Z","iopub.status.idle":"2021-06-15T21:18:19.998769Z","shell.execute_reply.started":"2021-06-15T21:18:19.835665Z","shell.execute_reply":"2021-06-15T21:18:19.997947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To train the model we'll use a callback to stop the training in case the validation set (in this case training) can not achieve a further improvement. ","metadata":{}},{"cell_type":"code","source":"callback = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the Model\nhistory = model.fit(\n    X_train, X_train, epochs=10000, batch_size=30,\n    verbose=0, validation_data=(X_test,X_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:18:19.999849Z","iopub.execute_input":"2021-06-15T21:18:20.000183Z","iopub.status.idle":"2021-06-15T21:27:40.872695Z","shell.execute_reply.started":"2021-06-15T21:18:20.000152Z","shell.execute_reply":"2021-06-15T21:27:40.871847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Results Comparison\nLet's see how the compressed representation of data compares to the non-compressed one, using a baseline model for comparison.","metadata":{"execution":{"iopub.status.busy":"2021-06-15T22:19:08.47055Z","iopub.execute_input":"2021-06-15T22:19:08.471037Z","iopub.status.idle":"2021-06-15T22:19:08.479536Z","shell.execute_reply.started":"2021-06-15T22:19:08.470982Z","shell.execute_reply":"2021-06-15T22:19:08.478226Z"}}},{"cell_type":"code","source":"encoder = Model(inputs=inputs, outputs=bottleneck)\nX_train_encode = encoder.predict(X_train)\nX_test_encode = encoder.predict(X_test)\n\nacc = list()\ntraining = [(X_train, X_test),\n            (X_train_encode, X_test_encode)]\n\nfor x_train, x_test in training:\n        model = LogisticRegression()\n        model.fit(x_train, y_train)\n        yhat = model.predict(x_test)\n\n        acc.append(accuracy_score(y_test, yhat))\n                   \nprint(f\"60-dimensions model accuracy: {acc[0]}\\n\"\n      f\"10-dimensions model accuracy: {acc[1]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:27:40.874381Z","iopub.execute_input":"2021-06-15T21:27:40.874768Z","iopub.status.idle":"2021-06-15T21:27:41.099333Z","shell.execute_reply.started":"2021-06-15T21:27:40.874729Z","shell.execute_reply":"2021-06-15T21:27:41.098331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 A note about training \nAs can be seen, the prediction is the same using the compressed data and the non compressed one. Due to the stochastic initialization of neural networks, sometimes you have to repeat this process several times until a good performance is reached. \n\nFor the following part where the encoder is applied to a neural net for classification purposes, a fine tuned encoder is used, which has been previoulsy trained (through several iterations) until an optimum value is reached. Results were always compared between the 60-dimensions and 10-dimensions accuracy with the baseline model.","metadata":{}},{"cell_type":"markdown","source":"# 6. Plotting the loss curve\nNext, we'll plot the loss curves to see how the model improves through epochs. After experimenting with different epoch amounts, it's been determined that the best results needs a pretty high epoch number, as the used before. Notice that despite using the EarlyStopping, the model is not halting when the curve remains almost flat.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.title('Loss')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:27:41.100978Z","iopub.execute_input":"2021-06-15T21:27:41.101287Z","iopub.status.idle":"2021-06-15T21:27:41.2733Z","shell.execute_reply.started":"2021-06-15T21:27:41.10126Z","shell.execute_reply":"2021-06-15T21:27:41.272407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, the model is saved for later use, in the next [notebook](https://www.kaggle.com/augustodenevreze/mines-vs-rocks-encoder-deep-learning-clf).","metadata":{}},{"cell_type":"code","source":"encoder = Model(inputs=inputs, outputs=bottleneck)\n# encoder.save('../input/mines-vs-rocks/encoder.h5')","metadata":{"execution":{"iopub.status.busy":"2021-06-15T21:27:41.274658Z","iopub.execute_input":"2021-06-15T21:27:41.274923Z","iopub.status.idle":"2021-06-15T21:27:41.282864Z","shell.execute_reply.started":"2021-06-15T21:27:41.27489Z","shell.execute_reply":"2021-06-15T21:27:41.282033Z"},"trusted":true},"execution_count":null,"outputs":[]}]}