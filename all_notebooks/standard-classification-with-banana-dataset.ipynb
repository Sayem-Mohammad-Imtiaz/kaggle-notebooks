{"cells":[{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[],"metadata":{"_uuid":"f763aa48515675ec9925c733ad683e39b33d6ee2","_cell_guid":"e7906e7e-6afd-490c-a6a2-7c87ae3c4d53"},"execution_count":2},{"source":"**Importing Neccessary Packages and reading the csv file and printing the head of the csv file.**","cell_type":"markdown","metadata":{}},{"source":"open_file = pd.read_csv(\"../input/banana.csv\",sep=\",\")\nprint(open_file.head())\nprint(open_file.shape)","cell_type":"code","outputs":[],"metadata":{},"execution_count":4},{"source":"**Checking whether any column in the dataset contains NaN values**","cell_type":"markdown","metadata":{}},{"source":"print(open_file.isnull().values.any())","cell_type":"code","outputs":[],"metadata":{},"execution_count":5},{"source":"**Computing the Basic Statistics(Descriptive) of the features in the dataset.**","cell_type":"markdown","metadata":{}},{"source":"print(open_file.describe())","cell_type":"code","outputs":[],"metadata":{},"execution_count":6},{"source":"**Now, Using Matplotlib library plotting the two features in the Scatter plot**","cell_type":"markdown","metadata":{}},{"source":"import matplotlib.pyplot as plt\nplt.scatter(open_file['At1'],open_file['At2'])\nplt.show()","cell_type":"code","outputs":[],"metadata":{},"execution_count":7},{"source":"**Now, Subsetting the dataset by removing the \"Class\" feature the from the original dataset.**","cell_type":"markdown","metadata":{}},{"source":"file = open_file[['At1','At2']]\nprint(file.head())","cell_type":"code","outputs":[],"metadata":{},"execution_count":8},{"source":"**Now, Plotting the two features in the correlation matrix**","cell_type":"markdown","metadata":{}},{"source":"correlation = file.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlation, vmin=-1, vmax=1)\nfig.colorbar(cax)\nnames=[\"At1\",\"At2\"]\nticks = np.arange(0,2,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\nplt.show()","cell_type":"code","outputs":[],"metadata":{},"execution_count":9},{"source":"**Now using the sklearn library, we import train_test_test from cross validation and split the original dataset into training and test dataset(70,30)**","cell_type":"markdown","metadata":{}},{"source":"from sklearn.cross_validation import train_test_split\ntrain,test = train_test_split(open_file,test_size=0.3)\nfeatures_train = train[['At1','At2']]\nfeatures_test = test[['At1','At2']]\nlabels_train = train.Class\nlabels_test = test.Class\nprint(labels_test.head())\nprint(train.shape)\nprint(test.shape)","cell_type":"code","outputs":[],"metadata":{},"execution_count":10},{"source":"**Now, we can use our machine learning algorithms to play around with training and test data set. We begin with \"Naive Bayes - Gaussian\" classifier.**","cell_type":"markdown","metadata":{}},{"source":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\ntraining = clf.fit(features_train,labels_train)\npredictions = clf.predict(features_test)\nprint(predictions)\nprint(\"Accuracy:\",clf.score(features_test,labels_test))","cell_type":"code","outputs":[],"metadata":{},"execution_count":11},{"source":"**Now, Using our second classifier as \"Support Vector Machine\" with kernel=\"rbf\".**","cell_type":"markdown","metadata":{}},{"source":"from sklearn import svm\nclf = svm.SVC(kernel='rbf')\ntraining = clf.fit(features_train,labels_train)\npredictions = clf.predict(features_test)\nprint(predictions)\nprint(\"Accuracy:\",clf.score(features_test,labels_test))","cell_type":"code","outputs":[],"metadata":{},"execution_count":12},{"source":"**\"Support Vector Machine\" with kernel=\"linear\".**","cell_type":"markdown","metadata":{}},{"source":"from sklearn import svm\nclf = svm.SVC(kernel='linear')\ntraining = clf.fit(features_train,labels_train)\npredictions = clf.predict(features_test)\nprint(predictions)\nprint(\"Accuracy:\",clf.score(features_test,labels_test))","cell_type":"code","outputs":[],"metadata":{},"execution_count":13},{"source":"**\"Now, Using our third classifier as \"DecisionTreeClassifier\".**","cell_type":"markdown","metadata":{}},{"source":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\ntraining = clf.fit(features_train,labels_train)\npredictions = clf.predict(features_test)\nprint(predictions)\nprint(\"Accuracy:\",clf.score(features_test,labels_test))","cell_type":"code","outputs":[],"metadata":{},"execution_count":14},{"source":"**Now, Using our fourth classifier as \"KNeighborsClassifier\"**","cell_type":"markdown","metadata":{}},{"source":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier()\ntraining = clf.fit(features_train,labels_train)\npredictions = clf.predict(features_test)\nprint(predictions)\nprint(\"Accuracy:\",clf.score(features_test,labels_test))","cell_type":"code","outputs":[],"metadata":{},"execution_count":15},{"source":"**We can try to use another classifier - \"BaggingClassifier\"\nA Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.**","cell_type":"markdown","metadata":{}},{"source":"from sklearn.ensemble import BaggingClassifier\nclf = BaggingClassifier(n_estimators=100, random_state=7)\nboosted = clf.fit(features_train,labels_train)\nprediction = clf.score(features_test,labels_test)\nprint(\"Accuracy:\",prediction)","cell_type":"code","outputs":[],"metadata":{},"execution_count":16},{"source":"**\"Now, we can try to predict the accuracy using the RandomForestClassifier\"**","cell_type":"markdown","metadata":{}},{"source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, random_state=7)\nboosted = clf.fit(features_train,labels_train)\nprediction = clf.score(features_test,labels_test)\nprint(\"Accuracy:\",prediction)","cell_type":"code","outputs":[],"metadata":{},"execution_count":17},{"source":"**So, with this, we come to know that SVM with 'rbf' kernel outperforms other algorithms, furthermore we can increase the accuracy by tuning the hyperparameter of the used algorithms**","cell_type":"markdown","metadata":{}},{"source":"from sklearn import svm\nclf = svm.SVC(kernel='rbf',C=10,gamma='auto')\ntraining = clf.fit(features_train,labels_train)\npredictions = clf.predict(features_test)\nprint(predictions)\nprint(\"Accuracy:\",clf.score(features_test,labels_test))","cell_type":"code","outputs":[],"metadata":{},"execution_count":25}],"metadata":{"language_info":{"file_extension":".py","pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1}