{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Term-Project\n\n## Abhimanyu Joshi\n\n### UTA ID : 1001796557\n\n<B>Aim: </B>\n<br>\nThe aim of this project is to build a text classifier that would predict the rating for a review based on the text available for the review. I have used several classifiers and compared there performances as well as built a review-prediction model."},{"metadata":{},"cell_type":"markdown","source":"## Data collection\n\n<font size=4>The datasets from './boardgamegeek-reviews/bgg-13m-reviews.csv' contains six columns(index, user, rating, comment, ID, name). But only two columns will be used for this task which is rating and comment. After this two columns are chosen, there are still some NaNs in the datasets.</font>"},{"metadata":{},"cell_type":"markdown","source":"<B>Imports for the project.</B>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport string\nimport nltk\nimport random\nimport matplotlib.mlab as mlab\nimport math\nfrom nltk.corpus import stopwords\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nimport sklearn.metrics as metrics\nimport tensorflow as tf\nimport os\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data\n<br>\n<B>Loading up the boardgamegeek-reviews dataset.</B> I removed the unwanted columns to increase the efficiency."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_frame = pd.read_csv('../input/boardgamegeek-reviews/bgg-15m-reviews.csv', index_col=0)\ndata_frame.drop(data_frame.columns[4], axis=1, inplace=True)\ndata_frame.drop(data_frame.columns[3], axis=1, inplace=True)\ndata_frame.drop(data_frame.columns[0], axis=1, inplace=True)\nprint(data_frame.head())\ndata_frame.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a large number of rows which contains NaN as an entry and are required to be removed to increase the efficiency."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_frame.dropna(subset = [\"comment\"], inplace=True)\n\nreduced = data_frame\nreduced.head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since it is impossible to host the complete dataset on pythonanywhere and also computing the values for approximately 3 Million entries was a very time and resource extensive job, so I have trimmed down the dataset to <B>100,000</B> entries which is much more easy to process and upload for the project."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = 100000\nsample_frame = reduced.sample(n=sample_size)\nsample_frame = shuffle(sample_frame)\nsample_frame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Filtering\n\nAs it's understandable that the data has shrunked <B>from 3-Million to merely 100,000 entries</B> but I would say that it's still a good data size to work with. Also, since there were <B>less than 10 </B> entries with <B>Rating=0</B> so it was a good option to filter them out and have a more even spread of data among the rest of the remaing ratings $[1-10]$.\n<br>\nAlso here we can see the break down of frequencies of each rating."},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced[\"rating\"] = reduced[\"rating\"].round(0).astype(int)\nreduced.groupby([\"rating\"]).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(reduced.rating)\nplt.xlabel('Ratings')\nplt.ylabel('Count')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_frame[\"rating\"] = sample_frame[\"rating\"].round(0).astype(int)\nsample_frame.groupby([\"rating\"]).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there is a concentration of ratings in range of $['6-8']$ which makes a perfect sense but also makes our job a little bit harder to train a classifier on such a heavily biased data. But since these are the raw figures from the original cleaned data (removed NaN) so I tried to maintain this ratio while tranning my classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(sample_frame.rating)\nplt.xlabel('Ratings')\nplt.ylabel('Count')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-processing\n\n### Filtered Data\nThere were <B>15.8-Million</B> rows of data in the original dataset out of which nearly <B>12-Million</B> were removed due to NaN values. After that we had a dataset of approx <B>3-Million rows and 5 columns</B> which has been shrunk down to a sample of size <B>100,000 rows and 2 columns $['rating','comment']$</B>\n\n### StopWords\nStopwords are the most commonly appearing words like punctuations (a, an, the etc.) which barely generate any value for text-classification, so we used nltk to find and remove the stop words from our data.\n\n### General pre-processing methods\nThe first step is to uniform text format to lower-case, then we used tokenization which is the task of chopping words up into pieces, called tokens and during this process I also removed the punctuations from the data.<B> For Stemming </B> I used Porter Stemmer which very well for the English Language. It helps to reduce complexity while retaining the basic meaning of the words.\n\n<B>Here I also generated two numpy arrays from the data to again reduce the size and for further use in the project.</B>"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(sample_frame.comment)\nnp.random.shuffle(x)\ny = np.array(sample_frame.rating)\nnp.random.shuffle(y)\n\ntext_data = []\nfor index, text in tqdm(enumerate(x)):\n    # lower case\n    text = text.lower()\n    # tokenize\n    words = word_tokenize(text)\n    # topwords\n    words = [w for w in words if w not in stopwords.words('english')]\n    # remove punctuation\n    words = [w for w in words if w not in string.punctuation]\n    # Stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    text_data.append(words)\n\nx = np.array(text_data, dtype=object)\nnp.save('x_new.npy', x)\nnp.save('y_new.npy', y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tf-idf\n\nThe dataset is first devided into train data and test data with a test size of 20%, then I vectorized the data to be used for Tf-idf to assign a weight to each word in oreder to help us assess the importance of a word in relation to a document or a document to a corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nprint('load data')\n\nx = np.load('x_new.npy', allow_pickle=True)\ny = np.load('y_new.npy', allow_pickle=True)\n\nfor i, d in tqdm(enumerate(x)):\n    sentence = ' '.join(x[i])\n    x[i] = sentence\n   \n    \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n\n\ncount_vect = CountVectorizer(min_df=0.001, max_df=0.5, max_features=1500)\nX_train_counts = count_vect.fit_transform(x_train)\nX_test_counts = count_vect.fit_transform(x_test)\n\ncount_vect = TfidfTransformer()\ntf_transformer = TfidfTransformer().fit(X_train_counts)\nx_train = tf_transformer.transform(X_train_counts)\n\nx_train = x_train.toarray()\nprint(x_train.shape)\nprint(y_train.shape)\n\ntf_transformer = TfidfTransformer().fit(X_test_counts)\nx_test = tf_transformer.transform(X_test_counts)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision tree\n\nI used $sklearn$ to import DecisionTreeClassifier method with <B>criterion:</B> “gini” for the Gini impurity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decision_tree(train_x, train_y, test_x, test_y):\n    print('...Decision_tree...')\n    clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=2, min_samples_split=20, min_samples_leaf=5).fit(train_x, train_y)\n    predict_y = clf.predict(test_x)\n    print('Decision tree accuracy: ', metrics.accuracy_score(test_y, predict_y))\n\n\ndecision_tree(x_train, y_train, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive-Bayes\n\n<B>MultinomialNB</B> function is loaded from $sklearn$ . The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bayes_fit_and_predicted(train_x, train_y, test_x, test_y):\n    print('...Bayes...')\n    clf = MultinomialNB(alpha=1.0).fit(train_x, train_y)\n    predict_y = clf.predict(test_x)\n    print('Bayes accuracy: ', metrics.accuracy_score(test_y, predict_y))\n    \nbayes_fit_and_predicted(x_train, y_train, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM\n\n<B>SVM</B> or Support Vector Machine is a set of supervised learning methods used for classification, regression and outliers detection. The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input."},{"metadata":{"trusted":true},"cell_type":"code","source":"def svm_fit_and_predicted(train_x, train_y, test_x, test_y, C=1.0):\n    print('...SVM...')\n    clf = LinearSVC(C=1.0, penalty='l2').fit(train_x, train_y)\n    predict_y = clf.predict(test_x)\n    print('SVM accuracy: ', metrics.accuracy_score(test_y, predict_y))\n\nsvm_fit_and_predicted(x_train, y_train, x_test, y_test)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN using keras\n\nKeras is a higher level library which operates over either TensorFlow or Theano, and is intended to stream-line the process of building deep learning networks. \n\nI have less experience working with neural-nets so the accuracies are pretty poor but it's always a good experience to learn something new and try to implement it in a project. Here I built a sequential model using Keras <B>Convolutional, Activation, Max-pooling and dense</B> layers with <B> relu and sigmoid </B> activation functions with $Adam$ optimizer and $crossentropy_loss$."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers\n\n\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n\nMAX_SEQUENCE_LENGTH = 100\nMAX_NUM_WORDS = 500\nEMBEDDING_DIM = 50\nVALIDATION_SPLIT = 0.2\n\nprint('load data')\n\nx = np.load('x_new.npy', allow_pickle=True)\ny = np.load('y_new.npy', allow_pickle=True)\n\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(x_train)\nx_train = tokenizer.texts_to_sequences(x_train)\n# tokenizer.fit_on_texts(x_test)\n# x_test = tokenizer.texts_to_sequences(x_test)\n\nword_index = tokenizer.word_index\n\n\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n#x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n\nnum_words = min(MAX_NUM_WORDS, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=num_words\n                                    , output_dim= EMBEDDING_DIM\n                                    ,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)\n                                    ,mask_zero=True\n                                    , input_length=MAX_SEQUENCE_LENGTH))\n\n\n# inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n\n# x = tf.keraslayers.Embedding(num_words, EMBEDDING_DIM)(inputs)\n\nmodel.add(layers.Conv1D(128, 5, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(10, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\n\n#predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n\n#model = tf.keras.Model(inputs, predictions)\n\n# model.add(tf.keras.layers.Dropout(0.5))\n# model.add(tf.keras.layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3))\n# #model.add(tf.keras.layers.Conv1D(128, 7, padding='valid', activation='relu', strides=3))\n# model.add(tf.keras.layers.GlobalMaxPooling1D())\n# #model.add(tf.keras.layers.Flatten())\n# model.add(tf.keras.layers.Dense(128, activation='relu'))\n# model.add(tf.keras.layers.Dropout(0.5))\n# model.add(tf.keras.layers.Dense(1, activation='sigmoid', name=\"predictions\"))\n\n#model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n#print(x_train[0,:])\n\n\nmodel.fit(x_train, y_train, batch_size=100, epochs=3, validation_split=VALIDATION_SPLIT)\n\n# model save\nmodel.save('cnn.h5')\n\nx_test = tokenizer.texts_to_sequences(x_test)\nx_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n\nresults = model.evaluate(x_test, y_test)\nprint('Test loss:', results[0])\nprint('Test accuracy:', results[1])\n\n\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    epochs=5,\n                    verbose=True,\n                    validation_data=(x_test, y_test),\n                    batch_size=50)\nloss, accuracy = model.evaluate(x_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(x_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Embedding(input_dim=num_words, \n                           output_dim=EMBEDDING_DIM, \n                           input_length=MAX_SEQUENCE_LENGTH))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(10, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n    model = tf.keras.models.Sequential()\n    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(10, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = dict(num_filters=[32, 64, 128],\n                  kernel_size=[3, 5, 7],\n                  vocab_size=[5000], \n                  embedding_dim=[50],\n                  maxlen=[100])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nepochs = 20\nembedding_dim = 50\nmaxlen = 100\noutput_file = 'output.txt'\n\n\nprint('load data')\n\nx = np.load('x_new.npy', allow_pickle=True)\ny = np.load('y_new.npy', allow_pickle=True)\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n\n\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(x_train)\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nx_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\nx_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n\n\n\nparam_grid = dict(num_filters=[32, 64, 128],\n                  kernel_size=[3, 5, 7],\n                  vocab_size=[vocab_size],\n                  embedding_dim=[embedding_dim],\n                  maxlen=[maxlen])\n\nmodel = tf.keras.models.Sequential()\nmodel = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model,\n                        epochs=epochs, batch_size=10,\n                        verbose=False)\n\n\n\ngrid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n                              cv=2, verbose=1, n_iter=2)\ngrid_result = grid.fit(x_train, y_train)\n\n# Evaluate testing set\ntest_accuracy = grid.score(x_test, y_test)\n\n\n\ns = ('Running {} data set\\nBest Accuracy : '\n             '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\noutput_string = s.format(\n    source,\n    grid_result.best_score_,\n    grid_result.best_params_,\n    test_accuracy)\nprint(output_string)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\n### Decision tree accuracy:  0.2557\n\n### Bayes accuracy:  0.24015\n\n### SVM accuracy:  0.22535\n\n<B> the experiment with cnn failed miserably <B>"},{"metadata":{},"cell_type":"markdown","source":"## Chalanges:\n\nFirst and the biggest challenge was the size of data, I have tried to compute the whole 15 million rows of data and failed due to system memory crash or google colab max_ram_limit crash. \n\nWorking on local machine was a bit hard since my mac doesn't have GPU support, just trying again and again with different parameter for neural-net trainning was tough.\n\nWorking with a small sample of data seemed to have affected the accuracy pretty badly.\n\nTrying to come-up with a good cnn learning model is still tough for me, surely need to study more on that."},{"metadata":{},"cell_type":"markdown","source":"## References:\n\n<a>https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f</a>\n\n<a>https://realpython.com/python-keras-text-classification/</a>\n<a>https://www.kaggle.com/jvanelteren/boardgamegeek-reviews/notebooks</a>\n\n<a>https://www.tensorflow.org/api_docs/python/tf/keras/</a>\n    \n    "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}