{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Salary Prediction Project of US Baseball Major League Players with Four Different Models\n\nIn this project the below described data will be used to predict the salaries of baseball players. The data retrieved from \"https://www.kaggle.com\"\n    \n    \n### Description\n    \n#### Context\n\nThis dataset is part of the R-package ISLR and is used in the related book by G. James et al. (2013) \"An Introduction to Statistical Learning with applications in R\" to demonstrate how Ridge regression and the LASSO are performed using R.\n\n#### Content\nThis dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n#### Format\n\nA data frame with 322 observations of major league players on the following 20 variables.\n\n- AtBat Number of times at bat in 1986\n- Hits Number of hits in 1986\n- HmRun Number of home runs in 1986\n- Runs Number of runs in 1986\n- RBI Number of runs batted in in 1986\n- Walks Number of walks in 1986\n- Years Number of years in the major leagues\n- CAtBat Number of times at bat during his career\n- CHits Number of hits during his career\n- CHmRun Number of home runs during his career\n- CRuns Number of runs during his career\n- CRBI Number of runs batted in during his career\n- CWalks Number of walks during his career\n- League A factor with levels A and N indicating player’s league at the end of 1986\n- Division A factor with levels E and W indicating player’s division at the end of 1986\n- PutOuts Number of put outs in 1986\n- Assists Number of assists in 1986\n- Errors Number of errors in 1986\n- Salary 1987 annual salary on opening day in thousands of dollars\n- NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987\n\nAcknowledgements\nPlease cite/acknowledge: Games, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York.\n   \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore')\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, ElasticNet, Lasso, LassoCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import RobustScaler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading data\n\ndf = pd.read_csv(\"../input/hitters/Hitters.csv\")  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understanding Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detecting missing values \n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Salary variable has 59 missing values","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#For visualizing missing values I need to install below package\n# When you are working with anaconda you may need this installation\n\nconda install -c conda-forge/label/cf202003 missingno\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing missing values\n\nimport missingno as msno\nmsno.bar(df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation values more than 0.5 between features (Because of >0.5 I can only see the values above 0.5)\n\ncorrelation_matrix = df.corr().round(2)\nfiltre=np.abs(correlation_matrix['Salary'])>0.50\ncorr_features=correlation_matrix.columns[filtre].tolist()\nsns.clustermap(df[corr_features].corr(),annot=True,fmt=\".2f\")\nplt.title('Correlation btw features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Even though there are very high correlation between some of the variables I will not do anything. Normally this problem should be solved.\n# Here I will delete missing values\n\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values('Salary', ascending = False).head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I have 3 categoric variables\n\ndf['League'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['NewLeague'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Division'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming nominal variables with one hot encoding method. Normally label encoding variable can be applied for dummy variables. One hot encoding is appropriate for the nominal variables have 3 or more categories \n\ndf = pd.get_dummies(df, columns = ['League', 'Division', 'NewLeague'], drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For detecting outliers I will use LocalOutlierFactor. I will use default values of 20 and 'auto'.\n\nclf=LocalOutlierFactor(n_neighbors=20, contamination='auto')\nclf.fit_predict(df)\ndf_scores=clf.negative_outlier_factor_\ndf_scores= np.sort(df_scores)\ndf_scores[0:20]","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"?LocalOutlierFactor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will take the 5th value as  threshold while the values after fift values decreasing closely\n# However at first I will visualize this situation regarding outliers\n\nsns.boxplot(df_scores);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold=np.sort(df_scores)[5]\nprint(threshold)\ndf = df.loc[df_scores > threshold]\ndf = df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardization\n# I will make some operations in the below rows.\n# Salary is my dependent variable, others are dummy variables. At first I will drop them from my independent variable set (X)\n#At last I will combine all of the independent variables\n\ndf_X=df.drop(['Salary','League_N','Division_W','NewLeague_N'], axis=1)\ndf_X.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaled_cols=StandardScaler().fit_transform(df_X)\n\n\n\nscaled_cols=pd.DataFrame(scaled_cols, columns=df_X.columns)\nscaled_cols.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df=df.loc[:, \"League_N\":\"NewLeague_N\"]\ncat_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Salary=pd.DataFrame(df['Salary'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([Salary,scaled_cols, cat_df], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dependent variable y = Salary, independents variables x = the variables without salary\n\ny = df['Salary']\nX = df.drop('Salary', axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will evaluate our model results cccording to mean value of predicted variable (y) \n\ny.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MODELING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test separation process and determining train and test size\n#Test size will be %20 of the data and random state will be 46 for all of the models in order to compare the models\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg = LinearRegression()\nmodel = linreg.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndf_linreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_linreg_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Prediction value (rmse) for linear regression model is 382.00085575367274. y.mean value is 538.2316872586872\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridreg = Ridge()\nmodel = ridreg.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ndf_ridreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_ridreg_rmse ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lasreg = Lasso()\nmodel = lasreg.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndf_lasreg_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_lasreg_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"enet = ElasticNet()\nmodel = enet.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndf_enet_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_enet_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Four models' Root Mean Squared Errors (RMSE) \n\ndef compML(df, y, alg):\n    model = alg().fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    model_name = alg.__name__\n    print(model_name, \"Model RMSE:\", RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [LinearRegression, Ridge, Lasso, ElasticNet] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    compML(df, 'Salary', model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression Model Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper parameter optimization with cross validation function.\n# We will try to tune the model by assigning new alpha values.\n# Default alpha value is 1.0 in Ridge regression. We will try different values.\n# The best fit alpha value or parameter will be employed in the final model\n\nalpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\nridreg_cv = RidgeCV(alphas = alpha, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nridreg_cv.fit(X_train, y_train)\nridreg_cv.alpha_\n\n#Final Model \n\nridreg_tuned = Ridge(alpha = ridreg_cv.alpha_).fit(X_train,y_train)\ny_pred = ridreg_tuned.predict(X_test)\ndf_ridge_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_ridge_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression Model Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper parameter optimization with cross validation function.\n# We will try to tune the model by assigning new alpha values.\n# Default alpha value is 1.0 in Lasso regression. We will try different values.\n# The best fit alpha value or parameter will be employed in the final model\n\nalpha = [0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]\nlasso_cv = LassoCV(alphas = alpha, cv = 10, normalize = True)\nlasso_cv.fit(X_train, y_train)\nlasso_cv.alpha_\n\n# Final Model \n\nlasso_tuned = Lasso(alpha = lasso_cv.alpha_).fit(X_train,y_train)\ny_pred = lasso_tuned.predict(X_test)\ndf_lasso_tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\ndf_lasso_tuned_rmse","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"?Lasso","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net Regression Regression Model Tuning","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true},"cell_type":"code","source":"?ElasticNet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper parameter optimization with cross validation function.\n# We will try to tune the model by assigning new alpha values.\n# Default alpha value is 1.0 and default l1_ratio is 0.5 in ElesticNet regression. We will try different values.\n# The best fit  values or parameters will be employed in the final model\n\n\nenet_params = {\"l1_ratio\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n              \"alpha\":[0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]}\nenet = ElasticNet()\nenet_model = enet.fit(X_train,y_train)\nenet_cv = GridSearchCV(enet_model, enet_params, cv = 10).fit(X, y)\nenet_cv.best_params_\n\n#Final Model \n\nenet_tuned = ElasticNet(**enet_cv.best_params_).fit(X_train,y_train)\ny_pred = enet_tuned.predict(X_test)\ndf_enet_tuned_rmse = np.sqrt(mean_squared_error(y_test,y_pred))\ndf_enet_tuned_rmse ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparable Results of Four Basic and Tuned Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nComparableResults_df =pd.DataFrame({\"LINEAR\":[df_linreg_rmse],\"RIDGE\":[df_ridreg_rmse],\"RIDGE TUNED\":[df_ridge_tuned_rmse],\n                             \"LASSO\":[df_lasreg_rmse],\"LASSO TUNED\":[df_lasso_tuned_rmse], \n                             \"ELASTIC NET\":[df_enet_rmse], \"ELASTIC NET TUNED\":[df_enet_tuned_rmse]})\n\nComparableResults_df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n## Result\n\nIn this project, four different linear regression models were employed to predict salary of any US Major Baseball League player. By using Linear, Ridge, Lasso, and ElesticNet Regression Machine Learning Models the root mean squared errors (RMSE) values were calculated. The RMSE is a measure of the average deviation of the estimates from the observed values. Then, the RMSE values were tried to be deacreased with the help of hyperparameter optimizations. As result the lowest RMSE value (295.98) obtained from the tuned ElesticNet Regression model.  According to analyses and predictions results, tuned ElesticNet Regression model is the best model to predict a US Baseball Major League player's salary.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}