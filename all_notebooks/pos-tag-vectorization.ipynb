{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8fca88eb-f29c-7a90-450e-3728264ed4ef"},"source":"This is an advanced example of model evaluation with the addition of POS tag vectorization (based on an other column in the data set), using FeatureUnion in a pipeline and GridSearchCV with cross-validation splitting strategy (5 folds to make it faster)\n\nThere is also an example of extracting most informative features for the prediction\n\nSee full version in:\nhttps://github.com/KaterynaD/TweetsAutorshipAttributionModelsEvaluation/blob/master/POS%2Btag%2Bvectorization.ipynb"},{"cell_type":"markdown","metadata":{"_cell_guid":"2aec6ce1-6356-fe55-6311-290133e7cc2f"},"source":"I use Bernoulli Naive Bayes classifier only because on these data all classifiers provides more or less the same results.\nSee https://github.com/KaterynaD/TweetsAutorshipAttributionModelsEvaluation/blob/master/Autorship%2Battribution%2Bmodels%2Bevaluation.ipynb"},{"cell_type":"markdown","metadata":{"_cell_guid":"7e03e1d2-d985-8b63-788a-3d24c0d372c8"},"source":"From the achieved results point of view: char and stemmed word vectorizeres provides good results. The other combinations (with or without POS tag vectorizer) may or may not provide slightly better or worse results.\nFor most pairs of authors I get 90 - 96% accuracy. I experimented with 500 - 2000 rows rows data sets per author"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98bf6902-e841-d45d-c152-b284a2cb97f7"},"outputs":[],"source":"import pandas as pd\nfrom pandas import Series,DataFrame\nimport numpy as np"},{"cell_type":"markdown","metadata":{"_cell_guid":"a5c57671-4b08-1294-725b-dca48b44c08e"},"source":"For sparsity reasons I pre-process data before analysis:\n\n 1. removing re-tweets\n 2. removing short messages (less then 4 words)\n 3. replacing @ with REF\n 4. replacing any url with URL\n 5. replacing any date with DATE\n 6. replacing any time with TIME\n 7. replace digits with NUM"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f869d452-ce74-1b8a-ff8d-966094301055"},"outputs":[],"source":"#data\n#KimKardashianTweets data\ndf_kk=pd.read_csv('../input/KimKardashianTweets.csv')\nlen(df_kk)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f36a9490-9e83-b02d-dbb1-f7bed3a01087"},"outputs":[],"source":"#HillaryClintonTweets data\ndf_hc=pd.read_csv('../input/HillaryClintonTweets.csv')\nlen(df_hc)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3d1f509-0786-1b91-6da8-ff1401966c75"},"outputs":[],"source":"author1='KimKardashian'\nauthor2='HillaryClinton'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c63e5ad-4c6b-4c84-6101-ce8c5ebec8b7"},"outputs":[],"source":"import random\n#2000 random sample rows for KK\nrows = random.sample(list(df_kk.index), 2000)\ndf_kk = df_kk.ix[rows]\n#2000 random sample rows for HC\nrows = random.sample(list(df_hc.index), 2000)\ndf_hc = df_hc.ix[rows]\n#join back together\ndf=df_kk.append(df_hc,ignore_index=True)\nlen(df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cdafc088-6afd-9ddb-e817-90a8df22b80a"},"outputs":[],"source":"#data pre-processing\ndf.drop(df[df.retweet==True].index, inplace=True)\ndf['num_of_words'] = df[\"text\"].str.split().apply(len)\ndf.drop(df[df.num_of_words<4].index, inplace=True)\ndf[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\ndf[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\nlen(df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bde72f51-1bd6-6288-9308-9c41451a81fe"},"source":"POS tag vectorizer\n\nI am going to convert the text column in the data set to the string of POS tag and replace the usual POS tag 2-3 chars abbreviations to 1 char abbrevations (e.g. 'NNP' -> 'N', 'NNPS' -> 'O') in order to use CountVectorizer with 'char' analyzer to get the most informative POS tag combinations per author"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0a5fa55-24a4-06b1-cbd9-6f028adb3082"},"outputs":[],"source":"#POS tag 2-3 chars abbrivation mapping to 1 char abbrevations\n#http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\npos_code_map={'CC':'A','CD':'B','DT':'C','EX':'D','FW':'E','IN':'F','JJ':'G','JJR':'H','JJS':'I','LS':'J','MD':'K','NN':'L','NNS':'M',\n'NNP':'N','NNPS':'O','PDT':'P','POS':'Q','PRP':'R','PRP$':'S','RB':'T','RBR':'U','RBS':'V','RP':'W','SYM':'X','TO':'Y','UH':'Z',\n'VB':'1','VBD':'2','VBG':'3','VBN':'4','VBP':'5','VBZ':'6','WDT':'7','WP':'8','WP$':'9','WRB':'@'}\n#Python 2 code_pos_map={v: k for k, v in pos_code_map.iteritems()}\ncode_pos_map = {v: k for k, v in  pos_code_map.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0e421f8-b154-2fd7-b30c-6886b824448a"},"outputs":[],"source":"#abbrivation converters\ndef convert(tag):\n    try:\n        code=pos_code_map[tag]\n    except:\n        code='?'\n    return code\ndef inv_convert(code):\n    try:\n        tag=code_pos_map[code]\n    except:\n        tag='?'\n    return tag"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e372d99-9b13-35e2-7ca4-828121f50195"},"outputs":[],"source":"#POS tag converting\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk import pos_tag, word_tokenize\ndef pos_tags(text):\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text_processed=tokenizer.tokenize(text)\n    return \"\".join(convert(tag) for (word, tag) in nltk.pos_tag(text_processed))\ndef text_pos_inv_convert(text):\n    return \"-\".join(inv_convert(c.upper()) for c in text)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0c36d01-b33c-bf3b-945e-1a9f04ac9ea9"},"outputs":[],"source":"#a new column for pos tags\ndf['text_pos']=df.apply(lambda x: pos_tags(x['text']), axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2df7613d-706f-5b91-b187-9fc381da58b6"},"source":"Here is how a sequence of pos tags looks like to be used in CountVectorizer()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27cba4c7-016b-6a5d-e0e0-8ef99cf1fd9f"},"outputs":[],"source":"df.ix[:,['author','text','text_pos']].head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e72b1902-a00f-7f7d-84b1-d5fc966c01d7"},"source":"Now let's look if there are unique combinations of POS tags per author"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8138df69-ef36-f475-6d92-6ef33d80f67b"},"outputs":[],"source":"df_features=pd.DataFrame()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5a10d40-3300-cbcc-0a52-086077a5aedc"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\nfor a in df.author.unique():\n    v = CountVectorizer(analyzer='char',ngram_range=(3, 3))\n    ngrams = v.fit_transform(df[df['author'] == a]['text_pos'])\n    df_t=pd.DataFrame(\n    {'Feature': v.get_feature_names(),\n     'Count': list(ngrams.sum(axis=0).flat),\n     'Author': a\n    })\n    #\n    df_features=df_features.append(df_t,ignore_index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"26ddde95-970d-d2a5-0442-edc2572ea1b1"},"source":"Let's convert the 1 char abbrivated pos tag sequence back to the common known 2-3 chars abbreviations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b177cb9-ca4f-0a26-2265-d69f3af1d9dc"},"outputs":[],"source":"df_features['Feature_POS']=df_features.apply(lambda x: text_pos_inv_convert(x['Feature']), axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"91da68ed-99a5-2ea6-2c7d-4945379e93f2"},"source":"There are indeed a lot of unique POS tags combinations per author"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ae2c931-014d-f487-f91c-2a1c6ddfb493"},"outputs":[],"source":"df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author2].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e42c994c-a712-23e9-13d4-6b65d3b9c923"},"outputs":[],"source":"df_features[~df_features.Feature.isin(df_features[df_features['Author'] != author1].Feature)].sort_values('Count', ascending=False).ix[:,['Author','Count','Feature','Feature_POS']].head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fee6d389-87c7-c91a-7d15-a731e60b698c"},"source":"To avoid overfitting let's hold out a part of the available data as a test set twt_test (X), author_test (Y)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26c500fc-e60a-1e2a-2bdf-552683c8ff6f"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\ntwt_train, twt_test, author_train, author_test = train_test_split(df.ix[:,['text','text_pos']], df['author'], test_size=0.4, random_state=42)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ced49f0b-28d3-9520-8745-b5105e42fe24"},"source":"The function will be used as tokenizer in the evaluation. As I discovered using stop words does not improve the model so I removed item 2 (removing stop words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc98d9a4-0812-298e-9b6a-cad2430cb27a"},"outputs":[],"source":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\ndef text_process(text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Tokenizes and removes punctuation\n    3. Stems\n    4. Returns a list of the cleaned text\n    \"\"\"\n\n    # tokenizing\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text_processed=tokenizer.tokenize(text)\n    \n    \n    # steming\n    porter_stemmer = PorterStemmer()\n    \n    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n    \n\n    return text_processed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8595bf6-02be-9466-d87d-c9d2f55637f3"},"outputs":[],"source":"ScoreSummaryByModelParams = list()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2cb1a68-c983-f233-0386-d255732f6cdd"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV"},{"cell_type":"markdown","metadata":{"_cell_guid":"fddd6e02-6426-973d-5c76-82fd59f9b90a"},"source":"ItemSelector and TextAndTextCodedExtractor classes is used in a pipeline to get a proper column (text or text_pos) from a data set to be used in a vectorizer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24c1d70e-6037-6a4d-b801-b146a54125c6"},"outputs":[],"source":"from sklearn.base import BaseEstimator, TransformerMixin\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[self.key]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee7c1c11-bec7-e84d-8b23-0b759fde12f1"},"outputs":[],"source":"class TextAndTextCodedExtractor(BaseEstimator, TransformerMixin):\n    \"\"\"Extract the text & text_pos from a tweet in a single pass.\n    \"\"\"\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, tweets):\n        features=tweets.ix[:,['text_pos','text']].to_records(index=False)\n\n        return features"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a759e25-8d77-4858-b53f-e22a47c1c3c0"},"source":"ModelParamsEvaluation function receives as parameters 2 parts of its pipeline: f_union, which is a pipeline itself with different combinations of vectorizers and a model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8640846e-7b14-c508-8d38-0028b8b0da29"},"outputs":[],"source":"def ModelParamsEvaluation (f_union,model,params,comment):\n    pipeline = Pipeline([\n    # Extract the text & text_coded\n    ('textandtextcoded', TextAndTextCodedExtractor()),\n\n    # Use FeatureUnion to combine the features from text and text_coded\n    ('union', f_union, ),\n\n    # Use a  classifier on the combined features\n    ('clf', model),\n    ])\n    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1, cv=5)\n    grid_search.fit(twt_train, author_train)\n    #best score\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(params.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])    \n "},{"cell_type":"markdown","metadata":{"_cell_guid":"f183ff49-2f9c-91dd-d4d7-75ece7051b1b"},"source":"First I examine the model with only 1 vectorizer We do not need FeatureUnion in this case but I use it just to keep the pattern of all experiments"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e37f5ac1-9ab6-d9db-b66f-e6b19beb6bc6"},"outputs":[],"source":"f1_union=FeatureUnion(\n        transformer_list=[\n              # Pipeline for pulling char features  from the text\n            ('char', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='char')),\n            ])),               \n\n        ],\n    )"},{"cell_type":"markdown","metadata":{"_cell_guid":"9295519b-198d-4da5-3518-e7f1820b1ca5"},"source":"'char' analyzer provides a perfect result by itself"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b89abf2-7a47-22d2-fafe-0d23114a3527"},"outputs":[],"source":"from sklearn.naive_bayes import BernoulliNB\np = {\n    'union__char__tfidf__max_df': (0.5, 0.75, 1.0),\n    'union__char__tfidf__ngram_range': ((2, 2), (3, 3)), \n    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n\nModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9ae5dc1-004f-f3bf-de9b-3bb4aae559e2"},"outputs":[],"source":"f1_union=FeatureUnion(\n        transformer_list=[\n            # Pipeline for pulling word features from the text\n            ('word', Pipeline([\n            ('selector', ItemSelector(key='text')),\n            ('tfidf',    TfidfVectorizer(analyzer='word')),\n            ])),              \n\n        ],\n    )"},{"cell_type":"markdown","metadata":{"_cell_guid":"f075f87a-af84-eb22-c81f-bd9a898237d8"},"source":"'word' analyzer is worse for these 2 authors but for other pairs (AdamSavage - ScottKelly) it provides better results the the char analyzer. As you can see it is not recommended using stop words"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a673a2c-5b33-b335-ec44-94183e0e235f"},"outputs":[],"source":"p = {\n    'union__word__tfidf__max_df': (0.5, 0.75, 1.0),\n    'union__word__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3)), \n    'union__word__tfidf__stop_words': (None, 'english'),\n    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n\nModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, word')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26549327-1c32-af52-93ab-1519f69d38aa"},"outputs":[],"source":"f1_union=FeatureUnion(\n        transformer_list=[\n            # Pipeline for pulling word features from the text\n            ('text', Pipeline([\n            ('selector', ItemSelector(key='text')),\n            ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process)),\n            ])),              \n\n        ],\n    )"},{"cell_type":"markdown","metadata":{"_cell_guid":"c2c5f34b-07e3-89e8-f335-b6c74bbee290"},"source":"'stemmed word' analyzer is better then the just 'word' analyzer but still worse then the 'char' for these 2 authors\nBut for other pairs (AdamSavage - ScottKelly) it provides better results the the char analyzer and worse then the just 'word'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c71277b3-07c0-4bb6-349f-6704fc2a95d1"},"outputs":[],"source":"p = {\n    'union__text__tfidf__max_df': (0.5, 0.75, 1.0),\n    'union__text__tfidf__ngram_range': ((1, 1),(2, 2), (3, 3)), \n    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n\nModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, stemmed words, no stop words')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7ea4693-2bc6-77f6-474a-d5a38c1dd047"},"outputs":[],"source":"f1_union=FeatureUnion(\n        transformer_list=[\n            # Pipeline for pulling pos tag features  from the text_pos\n            ('text_pos', Pipeline([\n            ('selector', ItemSelector(key='text_pos')),\n            ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n            ])),                  \n\n        ],\n    )"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a77be8c-df67-2297-f288-8c34a3e5e0b7"},"source":"POS tag vectorizer is not very selective. Let's see how it words in the combinations"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"afe22ba2-9c1d-1a18-f3a6-6dac1e31bdc1"},"outputs":[],"source":"p = {\n    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\n\nModelParamsEvaluation(f1_union,BernoulliNB(),p,'Bernoulli Naive Bayes, POS tags')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"799dabec-0b2a-22e3-b400-30d9dca4d67a"},"outputs":[],"source":"df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\ndf_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\ndf_ScoreSummaryByModelParams"},{"cell_type":"markdown","metadata":{"_cell_guid":"301545d1-e050-70ea-9d7a-b99f947be95c"},"source":"With small variations 'char and stemmed word' combination provides the best result for most of analyzed authors pairs"},{"cell_type":"markdown","metadata":{"_cell_guid":"d52b5cbd-d829-3799-8c7d-9993f8b1993e"},"source":"Using POS tag vectorizer does not improve the score dramatically. Its impact is more visible only for AdamSavage - ScottKelly pair"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9a7a6fb-e571-ddda-7eb4-b521c0d1054a"},"outputs":[],"source":"f3_union=FeatureUnion(\n        transformer_list=[\n             # Pipeline for pulling word stemmed features from the text\n            ('text', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n            ])),\n                    \n            # Pipeline for pulling char features  from the text\n            ('char', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n            ])),\n                    \n            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n            ('text_pos', Pipeline([\n                ('selector', ItemSelector(key='text_pos')),\n                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n            ])),                  \n\n        ],\n\n    )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf4e00c7-6719-d1ff-94e0-07bdbe1ce92c"},"outputs":[],"source":"p = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\nModelParamsEvaluation(f3_union,BernoulliNB(),p,'Bernoulli Naive Bayes, char + stemmed word + POS tags')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c470acb6-4506-8643-8c55-98a053701d54"},"outputs":[],"source":"df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\ndf_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\ndf_ScoreSummaryByModelParams"},{"cell_type":"markdown","metadata":{"_cell_guid":"9d35a811-d911-c360-c04e-f026140ca61c"},"source":"Now let's run prediction and review the results\nPredictionEvaluation function combines the scores from several methods for comparizon, ModelRun function runs the prediction for different models and most_informative_feature_for_binary_classification is used to get most informative features from a model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"047ef1b5-7865-4ea6-b988-1d59e871c0d9"},"outputs":[],"source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\nfrom scipy import interp"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2353c4d6-1f32-2842-5795-b74a24177ce1"},"outputs":[],"source":"ScoreSummaryByVector = list()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35c6997a-b2b6-4098-3daf-e0455acba7ec"},"outputs":[],"source":"def PredictionEvaluation(author_test_b,author_predictions_b,comment):\n    Precision=precision_score(author_test_b,author_predictions_b)\n    print ('Precision: %0.3f' % (Precision))\n    Accuracy=accuracy_score(author_test_b,author_predictions_b)\n    print ('Accuracy: %0.3f' % (Accuracy))\n    Recall=recall_score(author_test_b,author_predictions_b)\n    print ('Recall: %0.3f' % (Recall))\n    F1=f1_score(author_test_b,author_predictions_b)\n    print ('F1: %0.3f' % (F1))\n    print ('Confussion matrix:')\n    print (confusion_matrix(author_test_b,author_predictions_b))\n    ROC_AUC=roc_auc_score(author_test_b,author_predictions_b)\n    print ('ROC-AUC: %0.3f' % (ROC_AUC))\n    ScoreSummaryByVector.append([Precision,Accuracy,Recall,F1,ROC_AUC,comment])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"643996df-8c9c-b3fb-f8ef-733605490484"},"outputs":[],"source":"def ModelRun (f_union,model):\n    pipeline = Pipeline([\n    # Extract the text & text_coded\n    ('textandtextcoded', TextAndTextCodedExtractor()),\n\n    # Use FeatureUnion to combine the features from text and text_coded\n    ('union', f_union, ),\n\n    # Use a  classifier on the combined features\n    ('clf', model),\n    ])\n    pipeline.fit(twt_train, author_train)\n    author_predicted = pipeline.predict(twt_test)\n    \n    feature_names=list()\n    for p in (pipeline.get_params()['union'].transformer_list):\n        fn=(p[0],pipeline.get_params()['union'].get_params()[p[0]].get_params()['tfidf'].get_feature_names())\n        feature_names.append(fn)\n    df_fn=pd.DataFrame()\n    for fn in feature_names:\n        df_fn= df_fn.append(pd.DataFrame(\n        {'FeatureType': fn[0],\n         'Feature': fn[1]\n        }),\n        ignore_index=True)    \n    \n    from sklearn.preprocessing import LabelBinarizer\n    lb = LabelBinarizer()\n    author_test_b = lb.fit_transform(author_test.values)\n    author_predicted_b  = lb.fit_transform(author_predicted)\n    return (df_fn,pipeline.get_params()['clf'],author_predicted,author_predicted_b, author_test_b)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"606536e0-af92-f273-f91d-c198254027a9"},"outputs":[],"source":"def most_informative_feature_for_binary_classification(feature_names, classifier):\n    class_labels = classifier.classes_\n\n    topnvalues_class0 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values))\n    topnvalues_class1 = sorted(zip(classifier.coef_[0], feature_names['Feature'].values, feature_names['FeatureType'].values), reverse=True)\n\n    topn_df_class0=pd.DataFrame(topnvalues_class0, columns=['Coef','Feature','FeatureType'])\n    topn_df_class0['Author']=class_labels[0]\n    \n    topn_df_class1=pd.DataFrame(topnvalues_class1, columns=['Coef','Feature','FeatureType'])\n    topn_df_class1['Author']=class_labels[1]    \n    \n    topn_df=topn_df_class0.append(topn_df_class1)\n    \n        \n    return topn_df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"706edfd0-c35b-053a-9ade-4d11364a955d"},"outputs":[],"source":"f2_union=FeatureUnion(\n        transformer_list=[\n            # Pipeline for pulling char features  from the text\n            ('char', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n            ])),\n            # Pipeline for pulling stememd word features from the text\n            ('text', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',    TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n            ])),        \n\n        ],\n\n    )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d535eb5d-89fe-7cbf-7e3b-08c78bea98fc"},"outputs":[],"source":"(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f2_union,BernoulliNB(alpha=0.0001))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9ad224f-496b-5428-096e-bbe99625bafb"},"outputs":[],"source":"PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c335b34a-2feb-07b5-7f37-dd7da31f7f39","collapsed":true},"outputs":[],"source":"f3_union=FeatureUnion(\n        transformer_list=[\n             # Pipeline for pulling word stemmed features from the text\n            ('text', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n            ])),\n                    \n            # Pipeline for pulling char features  from the text\n            ('char', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n            ])),\n                    \n            # Pipeline for pulling flexible pattern features  from the text_coded with POS tags\n            ('text_pos', Pipeline([\n                ('selector', ItemSelector(key='text_pos')),\n                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n            ])),                  \n\n        ],\n\n    )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a388605f-3ffb-b145-cfd3-0f7a582d9cf1","collapsed":true},"outputs":[],"source":"(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f3_union,BernoulliNB(alpha=0.0001))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2a80587-9705-15f0-26af-5ac72e1e65c3"},"outputs":[],"source":"PredictionEvaluation(author_predicted_b, author_test_b,'char+stemmed word+POS tag')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1e9553b-4303-62ef-a69d-29457b684c1f"},"outputs":[],"source":"f4_union=FeatureUnion(\n        transformer_list=[\n\n            # Pipeline for pulling word features from the text\n            ('word', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',    TfidfVectorizer(analyzer='word',stop_words=None,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n            ])),\n                    \n             # Pipeline for pulling word features after word_processing from the text\n            ('text', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='word',tokenizer= text_process,ngram_range=(1, 1),max_df=0.5,max_features=None)),\n            ])),\n                    \n            # Pipeline for pulling char features  from the text\n            ('char', Pipeline([\n                ('selector', ItemSelector(key='text')),\n                ('tfidf',     TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=5000)),\n            ])),\n                    \n            # Pipeline for pulling flexible pattern features  from the text_coded\n            ('text_pos', Pipeline([\n                ('selector', ItemSelector(key='text_pos')),\n                ('tfidf',    TfidfVectorizer(analyzer='char',ngram_range=(3, 3),max_df=0.5,max_features=None)),\n            ])),                  \n\n        ],\n\n    )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f186b6b-a4c3-914b-bef6-6c7285ac8c4f"},"outputs":[],"source":"(feature_names,clf,author_predicted,author_predicted_b, author_test_b)=ModelRun(f4_union,BernoulliNB(alpha=0.0001))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"520ee778-a615-5897-d4a5-49f0649bb63a"},"outputs":[],"source":"PredictionEvaluation(author_predicted_b, author_test_b,'char+word+stemmed word+POS tag')"},{"cell_type":"markdown","metadata":{"_cell_guid":"2ac3d61a-76c5-51c2-a00d-16192ef79e18"},"source":"Here is the summary per model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25b6becd-11af-d800-a6c6-16ca5ea294f9"},"outputs":[],"source":"df_ScoreSummaryByVector=DataFrame(ScoreSummaryByVector,columns=['Precision','Accuracy','Recall','F1','ROC-AUC','Vector'])\ndf_ScoreSummaryByVector.sort_values(['F1'],ascending=False,inplace=True)\ndf_ScoreSummaryByVector"},{"cell_type":"markdown","metadata":{"_cell_guid":"0db3600e-e44e-adb8-c7ff-f0d05caa4421"},"source":"Now let's review teh most informative features for the last prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"524f1571-1edd-2f53-365b-b0cb2a1fd93e"},"outputs":[],"source":"TopFeatures_df=most_informative_feature_for_binary_classification(feature_names, clf)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b4aa56a-766a-b9a8-7aa1-fa3f1578843b"},"outputs":[],"source":"df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\ndf1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\ndf1.reset_index(inplace=True)\ndf2=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\ndf2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\ndf2.reset_index(inplace=True)\ndf3=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\ndf3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\ndf3.reset_index(inplace=True)\ndf4=TopFeatures_df.loc[((TopFeatures_df['Author']==author2) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\ndf4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\ndf4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\ndf4.reset_index(inplace=True)\ndf_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\ndf_kk_top_features.drop('index', axis=1, inplace=True)\ndf_kk_top_features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"836ffb4f-1e19-ec7a-f886-f808fca79909"},"outputs":[],"source":"df1=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='char')),['Author','Coef','Feature']].head(10)\ndf1.rename(columns={'Coef':'CoefChar','Feature':'Char'}, inplace=True)\ndf1.reset_index(inplace=True)\ndf2=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='word')),['Coef','Feature']].head(10)\ndf2.rename(columns={'Coef':'CoefWord','Feature':'Word'}, inplace=True)\ndf2.reset_index(inplace=True)\ndf3=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text')),['Coef','Feature']].head(10)\ndf3.rename(columns={'Coef':'CoefText','Feature':'Text'}, inplace=True)\ndf3.reset_index(inplace=True)\ndf4=TopFeatures_df.loc[((TopFeatures_df['Author']==author1) & (TopFeatures_df['FeatureType']=='text_pos')),['Coef','Feature']].head(10)\ndf4.rename(columns={'Coef':'CoefTextPOS','Feature':'TextPOS'}, inplace=True)\ndf4['TextPOS']=df4.apply(lambda x: text_pos_inv_convert(x['TextPOS']), axis=1)\ndf4.reset_index(inplace=True)\ndf_kk_top_features = pd.concat([df1,df2,df3,df4],axis=1)\ndf_kk_top_features.drop('index', axis=1, inplace=True)\ndf_kk_top_features"},{"cell_type":"markdown","metadata":{"_cell_guid":"5e9fdb11-299e-a00c-8f82-a0ce795ca900"},"source":"And let's take a look what was predicted wrongly"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"714e7fd0-a3df-a24f-1b0d-ca231dbe6d2b"},"outputs":[],"source":"author_predicted=pd.DataFrame(author_predicted,columns=['predicted'])\ndf_wrong_result = pd.concat([twt_test.reset_index(),author_test.reset_index(),author_predicted], axis=1)\ndf_wrong_result.drop('index', axis=1, inplace=True)\ndf_wrong_result.drop('text_pos', axis=1, inplace=True)\ndf_wrong_result=df_wrong_result[df_wrong_result['author']!=df_wrong_result['predicted']]\ndf_wrong_result.head(10)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}