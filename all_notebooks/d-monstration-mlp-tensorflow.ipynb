{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Démonstration #\n\n\nDans cette démonstration notre modèle va prédire les annulations de réservation d'hôtels avec un classificateur binaire.\n\nC'est à dire qu'il va déterminer si oui ou non une réservation a été annulé."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup plotting\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nhotel = pd.read_csv('../input/dl-course-data/hotel.csv')\n\n# En X on a notre jeu de données avec nos features et notre cible est la colonne 'is canceled'\nX = hotel.copy()\ny = X.pop('is_canceled')\n\nX['arrival_date_month'] = \\\n    X['arrival_date_month'].map(\n        {'January':1, 'February': 2, 'March':3,\n         'April':4, 'May':5, 'June':6, 'July':7,\n         'August':8, 'September':9, 'October':10,\n         'November':11, 'December':12}\n    )\n\nfeatures_num = [\n    \"lead_time\", \"arrival_date_week_number\",\n    \"arrival_date_day_of_month\", \"stays_in_weekend_nights\",\n    \"stays_in_week_nights\", \"adults\", \"children\", \"babies\",\n    \"is_repeated_guest\", \"previous_cancellations\",\n    \"previous_bookings_not_canceled\", \"required_car_parking_spaces\",\n    \"total_of_special_requests\", \"adr\",\n]\nfeatures_cat = [\n    \"hotel\", \"arrival_date_month\", \"meal\",\n    \"market_segment\", \"distribution_channel\",\n    \"reserved_room_type\", \"deposit_type\", \"customer_type\",\n]\n\ntransformer_num = make_pipeline(\n    SimpleImputer(strategy=\"constant\"), # there are a few missing values\n    StandardScaler(),\n)\ntransformer_cat = make_pipeline(\n    SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n    OneHotEncoder(handle_unknown='ignore'),\n)\n\npreprocessor = make_column_transformer(\n    (transformer_num, features_num),\n    (transformer_cat, features_cat),\n)\n\nX_train, X_valid, y_train, y_valid = \\\n    train_test_split(X, y, stratify=y, train_size=0.75)\n\nX_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\ninput_shape = [X_train.shape[1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous allons créer un réseau de neurone tel que le schéma ci-dessous"},{"metadata":{},"cell_type":"markdown","source":"<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/V04o59Z.png\" width=\"400\" alt=\"Diagram of network architecture: BatchNorm, Dense, BatchNorm, Dropout, Dense, BatchNorm, Dropout, Dense.\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Diagram of a binary classifier.</center></figcaption>\n</figure>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(256, activation='relu', input_shape=[33]),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(256, activation='relu'), \n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid'),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- La fonction Dropout supprime un pourcentage de neurone de la couche suivante \n- La fonction BatchNormalization permet d'aider si l'apprentissage est lent ou instable (exemple : avant d'utiliser des données ont les nettoies, de cette même manière on normalise notre réseaux de neurone avant de passer les données dedans, on peut dire que le BatchNormalization est un peu une bonne pratique)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Total params : ce sont les poids du notre réseaux de neurone\n\n- Trainable params : ce sont l'ensemble des paramètre qui peuvent être modifier par Tensorflow automatiquement lors de l'apprentissage dans le but de minimiser l'erreur que l'on va définir"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- optimizer : c'est le type d'optimizer qui permet de réduire l'erreur, l'adam est une variante de la descente de gradient stochastique mais qui adapte le learning rate en temps réel \n- loss : fonction d'erreur, qui est içi la crossentropy binaire\n- metric : c'est la précision de notre entrainement"},{"metadata":{"trusted":true},"cell_type":"code","source":"# En pratique plus on fais d'itération (d'epoch) et plus l'erreur est censé diminuer, \n# La fonction early stop permet d'arrêter l'apprentissage au moment ou l'erreur commence de nouveau à augmenter cela évite donc de continuer a utiliser de la puissance de calcul mais surtout d'éviter le surapprentissage\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=200,\n    callbacks=[early_stopping],\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit que grâce à la fonction EarlyStop, l'apprentissage s'est stopper à 27 epoch"},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- En orange on a la courbe qui correspond au jeu de test \n- En bleu la courbe qui correspond au jeu d'apprentissage"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}