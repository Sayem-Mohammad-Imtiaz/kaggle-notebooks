{"cells":[{"metadata":{"id":"yK3QU41RIvtC"},"cell_type":"markdown","source":"# Introduction\n\nThis Notebook is **mainly for educational purposes**. According to a dataset to classify Short Message Service (SMS) as spam or not, the goal will be to evaluate Bidirectional Encoder Representations from Transformers (BERT) with other Machine Learning (ML) classification algorithms.\n\nAmong these algorithms, four ML classification algorithms will be compared:\n\n1.   **DistilBERT**;\n2.   **K-Nearest Neighbors (KNN)**;\n3.   **Multinomial Naive Bayes**;\n4.   **Support Vector Model (SVM)**.\n\nWe had the opportunity to test those algorithms with [another dataset](https://www.kaggle.com/rememberyou/comparison-of-bert-and-other-ml-classification-ii) containing SMS samples as well."},{"metadata":{"id":"Wl_RfuKkIny3"},"cell_type":"markdown","source":"# Objectives\n\nTo compare these algorithms, we will:\n\n*   **Do Feature Engineering**: create the features according to the raw data.\n*   **Analyze and understand the data** made available.\n*   **Pre-process these data according to the algorithm**: for instance, some of these algorithms only work with numerical values.\n*   **Do Fine-Tuning**: optimize the training parameters of the ML algorithm.\n*   **Compare the results** obtained.\n*   **Apply the Ockham's razor principle**: take the best and/or simplest algorithm if there is no significant difference."},{"metadata":{"id":"sWcL3g9cLsdy"},"cell_type":"markdown","source":"# Installing and Importing Packages\n"},{"metadata":{"id":"szz9D70YL40b"},"cell_type":"markdown","source":"Using the `pip` Python package manager, let's install all the necessary packages for this Notebook:"},{"metadata":{"id":"NINiOAdexPbm","outputId":"73f6822e-c946-4d6d-80fe-4b0a58c834e7","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!python -m pip install matplotlib\n!python -m pip install nltk\n!python -m pip install numpy\n!python -m pip install pandas\n!python -m pip install seaborn\n!python -m pip install sklearn\n!python -m pip install tensorflow\n!python -m pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"id":"4MJjnDxpMMvi"},"cell_type":"markdown","source":"The necessary packages being installed, let's already import most of the packages for this Notebook:"},{"metadata":{"id":"lbQisjvxvHpc","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import re\n\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn import feature_extraction\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,\n                             recall_score)\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom transformers import TFTrainer, TFTrainingArguments","execution_count":null,"outputs":[]},{"metadata":{"id":"Mp1idvCeNDhY"},"cell_type":"markdown","source":"With the NLTK's data downloader, we will install the following corpora and trained models:\n*   `punkt`: Punkt Tokenizer Models.\n*   `stopwords`: Stopwords Corpus.\n*   `wordnet`: WordNet-InfoContent."},{"metadata":{"id":"cWZ4HShxMklu","outputId":"9aad5150-6109-46a3-9001-28c02474c4c5","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"nltk.download(\"punkt\")\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ruPujWCKRqLP"},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"id":"TBt5S7krNxWx"},"cell_type":"markdown","source":"For this Notebook, the [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) dataset has been chosen. The main reasons for such a choice is that it contains a lot of data, and its columns are suitable for the comparison of the ML algorithms we want to make.\n\nIn this section we will load the dataset and apply minor preprocessing to the columns and values to make it easier to use."},{"metadata":{"id":"Ph3hBzFIM3rZ"},"cell_type":"markdown","source":"## Loading"},{"metadata":{"id":"gBEEGmXpR1qG"},"cell_type":"markdown","source":"Let's start by loading our dataset and looking at the columns available to us:"},{"metadata":{"id":"GT_AQYGjMFiV","outputId":"7e6cdcd3-ddf3-48ee-9ba5-f97496ad90a9","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\n    \"../input/sms-spam-collection-dataset/spam.csv\", encoding=\"latin-1\"\n)\ndf.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"HCUX4zibSAHb"},"cell_type":"markdown","source":"After loading this dataset, you can directly see some modifications to be made:\n\n*   **three** \"Unnamed\" **columns can be deleted**;\n*   **the spam column** (`v1`) a**nd the SMS content column** (`v2`) **can be renamed** to be more explicit;\n*  **the content of the spam column** (`v1`) **can be binarized** for better processing ease for ML algorithms.\n\n"},{"metadata":{"id":"4lJfm4q6MkLk"},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"id":"9wQPVCTGT4_J"},"cell_type":"markdown","source":"In order to have a better ease of use, a first pre-processing of these data would be to apply the modifications mentioned above:"},{"metadata":{"id":"tJO54S0NTt_u","outputId":"c4026bfc-eee4-47e0-fce4-9e3ca4fcc4f4","trusted":true},"cell_type":"code","source":"df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], inplace=True, axis=1)\ndf.rename({\"v1\": \"is_spam\", \"v2\": \"content\"}, axis=1, inplace=True)\ndf[\"is_spam\"].replace({\"ham\": 0, \"spam\": 1}, inplace=True)\ndf.head(n=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"EfAawYruMZhN"},"cell_type":"markdown","source":"We can see that the dataset has become much clearer."},{"metadata":{"id":"30OFzk-5Xy69"},"cell_type":"markdown","source":"Finally, to get a better idea on the amount of data made available, we can look at the shape of the DataFrame that defines the dataset:"},{"metadata":{"id":"4DgxCyIXX1zc","outputId":"632678ca-9a2f-457e-9b97-b7dab636ccd1","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"37eUY45EY-t8"},"cell_type":"markdown","source":"So we have a **dataset that contains 5572 rows and 2 columns**."},{"metadata":{"id":"35OzGfN8JkW-"},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"id":"EflZ3h7AR3bS"},"cell_type":"markdown","source":"By Feature Engineering, we **refer to the creation of the features according to the raw data**. It is on the basis of these features that the training of the classification ML models will be done.\n\nAmong these features, we will create these:\n\n*   `nwords`: feature that will **contain the number of words** in an SMS.\n*   `message_len`: feature that will **contain the number of characters** in an SMS message.\n*   `nupperchars`: feature that will **contain the number of uppercase characters** in an SMS.\n*   `nupperwords`: feature that will **contain the number of uppercase words** in an SMS.\n*   `is_free_o_win`: feature that will **contain 1 if the SMS contains the words \"free\" and \"win\"; 0 otherwise**.\n*   `is_url`: feature that will **contain 1 if the SMS contains a URL; 0 otherwise**.\n\nThis translates as follows:"},{"metadata":{"id":"uTw_ZwdIJnPf","outputId":"8c532082-7ef4-42c1-b917-ca4c13a8ca6f","trusted":true},"cell_type":"code","source":"df[\"nwords\"] = df[\"content\"].apply(lambda s: len(re.findall(r\"\\w+\", s)))\ndf[\"message_len\"] = df[\"content\"].apply(len)\ndf[\"nupperchars\"] = df[\"content\"].apply(\n    lambda s: sum(1 for c in s if c.isupper())\n)\ndf[\"nupperwords\"] = df[\"content\"].apply(\n    lambda s: len(re.findall(r\"\\b[A-Z][A-Z]+\\b\", s))\n)\ndf[\"is_free_or_win\"] = df[\"content\"].apply(\n    lambda s: int(\"free\" in s.lower() or \"win\" in s.lower())\n)\ndf[\"is_url\"] = df[\"content\"].apply(\n    lambda s: 1\n    if re.search(\n        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n        s,\n    )\n    else 0\n)\ndf.head(n=25)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q4ZA7VrlWuNn"},"cell_type":"markdown","source":"We can see that the columns corresponding to our features have been created."},{"metadata":{"id":"48hixwSVNllK"},"cell_type":"markdown","source":"# Analyze and Understanding Data"},{"metadata":{"id":"PGZxaKRUN8Ov"},"cell_type":"markdown","source":"Above all, it is **important to analyze and understand the data** made available. Indeed, once a better understanding of the dataset is achieved, **we will be able to create the necessary features** for the dataset.\n\nThe dataset being loaded, we will analyze the following seven aspects:\n\n1.   **the SMS distribution**;\n2.   **the word frequency in spam and ham SMS**;\n3.   **the length of spam SMS compared to ham SMS**;\n4.   **the number of words in spam SMS compared to ham SMS**;\n5.   **the number of uppercase words in spam SMS compared to ham SMS**;\n6.   **the number of uppercase characters in spam SMS compared to ham SMS**;\n7.   **the content of the words \"free\" or \"win\" in the SMS**;\n8.   **the content of a URL in the SMS**."},{"metadata":{"id":"WHMnkcGzNRup"},"cell_type":"markdown","source":"## SMS Distribution"},{"metadata":{"id":"x1UksbmqVp7h"},"cell_type":"markdown","source":"Now that we know a bit more about the organization of the dataset, it is good to know the percentage of spam SMS and ham SMS:"},{"metadata":{"id":"e6cejq6udLPf","outputId":"dd993a71-c41b-4974-bea3-051a741216ce","trusted":true},"cell_type":"code","source":"n_sms = pd.value_counts(df[\"is_spam\"], sort=True)\nn_sms.plot(kind=\"pie\", labels=[\"ham\", \"spam\"], autopct=\"%1.0f%%\")\n\nplt.title(\"SMS Distribution\")\nplt.ylabel(\"\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"AznCUWRqW8-S"},"cell_type":"markdown","source":"Above, **87% of these SMS are ham and 13% of them are spam**."},{"metadata":{"id":"xsPMul0VuxPP"},"cell_type":"markdown","source":"## Word Frequency"},{"metadata":{"id":"RGD9xFL7dtlg"},"cell_type":"markdown","source":"Let's start by creating two DataFrames: \n\n1.   `df1`: will contain the words and their frequency in the SMS ham.\n2.   `df2`: will contain the words and their frequency in the SMS spam.\n"},{"metadata":{"id":"NNGr79hQdyh_","trusted":true},"cell_type":"code","source":"from collections import Counter\n\ndf1 = pd.DataFrame.from_dict(\n    Counter(\" \".join(df[df['is_spam'] == 0][\"content\"]).split()).most_common(20)\n)\ndf1 = df1.rename(columns={0: \"word_in_ham\", 1 : \"frequency\"})\n                 \ndf2 = pd.DataFrame.from_dict(\n    Counter(\" \".join(df[df['is_spam'] == 1][\"content\"]).split()).most_common(20)\n)\ndf2 = df2.rename(columns={0: \"word_in_spam\", 1 : \"frequency\"})","execution_count":null,"outputs":[]},{"metadata":{"id":"Hf5AtzGE0e6T"},"cell_type":"markdown","source":"Now that the DataFrames have been created, let's sketch their corresponding graphs in order to look at their respective word frequencies:"},{"metadata":{"id":"Ie9-Mp0Ud1Jh","outputId":"1f3b924c-6153-40c9-8daa-f60904537383","trusted":true},"cell_type":"code","source":"df1.plot.bar(legend=False)\nplt.xticks(np.arange(len(df1[\"word_in_ham\"])), df1[\"word_in_ham\"])\nplt.title(\"Word Frequency in Ham SMS.\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"j5UEmXB6el8o","outputId":"9b70d53a-e4cf-4b71-e860-c3ac9c1050ba","trusted":true},"cell_type":"code","source":"df2.plot.bar(legend=False, color=\"orange\")\nplt.xticks(np.arange(len(df2[\"word_in_spam\"])), df2[\"word_in_spam\"])\nplt.title(\"Word Frequency in Spam SMS.\")\nplt.xlabel(\"Word\")\nplt.ylabel(\"Frequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"paENXjeaeudW"},"cell_type":"markdown","source":"After sketching, we can see that stop words are the most frequent words in both spam and ham SMS."},{"metadata":{"id":"snvxvKngenrG"},"cell_type":"markdown","source":"## Length"},{"metadata":{"id":"t-t5O66gXbZU"},"cell_type":"markdown","source":"Let's see if the length has an influence on SMS spam or ham:"},{"metadata":{"id":"vYXogCs3epms","outputId":"acb49594-fd77-4643-9f0f-524cfc040f7d","trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"message_len\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(-50, 250),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"message_len\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Length\",\n    ylabel=\"Density\",\n    title=\"Length of SMS.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QF3pJuJVesRm"},"cell_type":"markdown","source":"With this plot, we notice two things:\n1.   In general, **spam** messages are **longer** than **ham** messages (that is normal due to the number of words).\n2.   **Spam** messages have around **150 characters**.\n\n"},{"metadata":{"id":"KGUZ15h1egoM"},"cell_type":"markdown","source":"## Number of Words"},{"metadata":{"id":"cMb-6jTnXzZw"},"cell_type":"markdown","source":"Let's see if the number of words has an influence on SMS spam or ham:"},{"metadata":{"id":"jWfYVhXlejuZ","outputId":"8047e47d-4eb5-41ee-d3b6-b44e395a1dfe","trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"nwords\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(-10, 50),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"nwords\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Words\",\n    ylabel=\"Density\",\n    title=\"Number of Words in SMS.\",\n)\nax.legend(loc=\"upper right\")","execution_count":null,"outputs":[]},{"metadata":{"id":"n9WDSW9mel39"},"cell_type":"markdown","source":"With this plot, we can notice that **spam** SMS have more words than **ham** SMS.\n\n**Spam** SMS seem to have around **30 words**, where **ham** SMS seem to have around **10 words** to **25 words** and more."},{"metadata":{"id":"72eR58M5e0fk"},"cell_type":"markdown","source":"## Number of Uppercased Words"},{"metadata":{"id":"BdP5-KINX3Cp"},"cell_type":"markdown","source":"Let's see if the number of uppercased words has an influence on SMS spam or ham:"},{"metadata":{"id":"VZ5mVmote2xR","outputId":"2c25e700-52cb-45a5-8ef5-bf305212328a","trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10, 4))\nsns.kdeplot(\n    df.loc[df.is_spam == 0, \"nupperwords\"],\n    shade=True,\n    label=\"Ham\",\n    clip=(0, 35),\n)\nsns.kdeplot(df.loc[df.is_spam == 1, \"nupperwords\"], shade=True, label=\"Spam\")\nax.set(\n    xlabel=\"Uppercased Words\",\n    ylabel=\"Density\",\n    title=\"Number of Uppercased Words.\",\n)\nax.legend(loc=\"upper right\")","execution_count":null,"outputs":[]},{"metadata":{"id":"eJJ4sBkae4pE"},"cell_type":"markdown","source":"With this plot, we can notice that there is a small pattern with the number of **uppercased words**. The **density is lower** which is normal due to the fact that there is **less spam** messages than **ham** messages.\n\nWe can also notice that the number of **uppercased words** is around **zero** for the **ham** messages."},{"metadata":{"id":"hj_Gpg9veurM"},"cell_type":"markdown","source":"## Number of Uppercased Characters"},{"metadata":{"id":"v58eYQJWX5gc"},"cell_type":"markdown","source":"Let's see if the number of uppercased characters has an influence on SMS spam or ham:"},{"metadata":{"id":"btOLE-1eewb-","outputId":"d1533334-2c7c-409e-a870-ca6ac7ef069d","trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10, 5))\nax = sns.scatterplot(x=\"message_len\", y=\"nupperchars\", hue=\"is_spam\", data=df)\nax.set(\n    xlabel=\"Characters\",\n    ylabel=\"Uppercase Characters\",\n    title=\"Number of Uppercased Characters in SMS.\",\n)\nax.legend(loc=\"upper right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"WpCD1LHBeyFQ"},"cell_type":"markdown","source":"With this plot we can notice two things:\n1.   **Spam** messages are clustered together based on their length. But we can also see that some **spam** messages have more uppercased characters that others.\n2.   There is a linear pattern for **ham** messages that contains more uppercased character than others. \n\n"},{"metadata":{"id":"jlxkWgoQe6pM"},"cell_type":"markdown","source":"## Contains \"free\" or \"win\""},{"metadata":{"id":"o-wG2b-RX8PP"},"cell_type":"markdown","source":"Let's see if the \"free\" and \"win\" words has an influence on SMS spam or ham:"},{"metadata":{"id":"7agMWiOMe8CR","outputId":"5332f4fe-2a56-41a7-9566-5645ddb25b12","trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10, 4))\ngrouped_data = (\n    df.groupby(\"is_spam\")[\"is_free_or_win\"]\n    .value_counts(normalize=True)\n    .rename(\"Percentage of Group\")\n    .reset_index()\n)\nprint(grouped_data)\n\nax.set(\n    title=\"Distribution of FREE/WIN Words Between Spam and Ham\"\n)\n\nsns.barplot(\n    x=\"is_spam\",\n    y=\"Percentage of Group\",\n    hue=\"is_free_or_win\",\n    data=grouped_data,\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"wJRu280Te9ud"},"cell_type":"markdown","source":"With this plot, we can notice two things:\n1.   There is **36.94% of spam** SMS that contains the words **\"free\"** or **\"win\"**.\n2.   There is only **2.69% of ham** SMS that contains the words **\"free\"** or **\"win\"**.\n\n"},{"metadata":{"id":"lIFz2HqAArHb"},"cell_type":"markdown","source":"## Contains URL"},{"metadata":{"id":"CxxGI363AvIw"},"cell_type":"markdown","source":"Let's see if a URL has an influence on SMS spam or ham:"},{"metadata":{"id":"_YrFwy8tAtUc","outputId":"0aa49ef0-197d-4bc0-ba9a-f451f5f08468","trusted":true},"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10, 4))\ngrouped_data = (\n    df.groupby(\"is_spam\")[\"is_url\"]\n    .value_counts(normalize=True)\n    .rename(\"Percentage of Group\")\n    .reset_index()\n)\nprint(grouped_data)\n\nax.set(\n    title=\"Distribution of URL Between Spam and Ham\"\n)\n\nsns.barplot(\n    x=\"is_spam\",\n    y=\"Percentage of Group\",\n    hue=\"is_url\",\n    data=grouped_data,\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"1x2x-vrCAunq"},"cell_type":"markdown","source":"With this plot, we can notice two things:\n1.   there is **2.55% of spam** that contains a URL;\n2.   there is only **97.45% of spam** that doesn't contains a URL."},{"metadata":{"id":"bJVYtkKaTQ1O"},"cell_type":"markdown","source":"# Preprocessing Data"},{"metadata":{"id":"H3OcvH04bBn6"},"cell_type":"markdown","source":"In ML, preprocessing data is a process of preparing raw data to make them suitable to a ML model. \n\nFrom a semantic point of view, our dataset has some drawbacks for a ML model:\n\n*   **presence of stop words** (e.g., so, is, a);\n*   **presence of punctuations and digits**;\n*   **words are not lemmatized.**"},{"metadata":{"id":"uqfZ3Akwh5qK"},"cell_type":"markdown","source":"Since this dataset has a lot of abbreviations, we will not apply stemming, but only lemmatization.\n\nAs a quick reminder:\n\n*   **Stemming**: NLP algorithm that **cuts the end or the beginning of a word** based on a list of common prefixes that can be found in an inflected word (e.g., `Stemming[change, changing, changes]` ➡️ chang).\n*   **Lemmatization**: NLP algorithm that **looks at the morphological analysis of words** based on detailed dictionaries, in order to relate the shape of a word to its lemma (e.g., `Lemmatization[change, changing, changes]` ➡️ change).\n\nAs a second pre-processing of these data, let's remove the stop words, punctuation and digits from each SMS, without forgetting to apply lemmatization to them:"},{"metadata":{"id":"Pf5I1cu2vmS2","outputId":"22a56535-87ca-4cdd-dcc2-c18a44ebfcfc","trusted":true},"cell_type":"code","source":"from nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: re.sub(r\"[^a-zA-Z]+\", \" \", row)  \n)\ndf[\"content\"] = df[\"content\"].apply(lambda row: word_tokenize(row))\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: [\n        token for token in row if token not in set(stopwords.words(\"english\"))\n    ]\n)\ndf[\"content\"] = df[\"content\"].apply(\n    lambda row: \" \".join([WordNetLemmatizer().lemmatize(word) for word in row])\n)\ndf.head(n=25)","execution_count":null,"outputs":[]},{"metadata":{"id":"_y-UOW4WnA8o"},"cell_type":"markdown","source":"It would still be possible to speculate on more pre-processing to be done (e.g., finding the original words based on abbreviations), but since a SMS is not a formal message, it may be wise to keep capital letters and abbreviations."},{"metadata":{"id":"CG6enQf-96Ju"},"cell_type":"markdown","source":"# Creation of Training and Testing Datasets"},{"metadata":{"id":"jBqJQSQJ1LWQ"},"cell_type":"markdown","source":"Before being able to train our model, it is necessary to split our dataset into a training and testing dataset:"},{"metadata":{"id":"W1VCLXKnvwyn","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    df[\"content\"], df[\"is_spam\"], stratify=df[\"is_spam\"],test_size=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"2cThvT9a6M0G"},"cell_type":"markdown","source":"Our dataset initially composed of 5572 lines is now split into two smaller datasets according to the following proportions:"},{"metadata":{"id":"1eMrw-QX8ejE","outputId":"d744015b-47e9-40bb-e00d-671a1e70d469","trusted":true},"cell_type":"code","source":"print(f\"Training data: {len(X_train)} (80%)\")\nprint(f\" Testing data: {len(X_test)} (20%)\")","execution_count":null,"outputs":[]},{"metadata":{"id":"wigvv_XELdYQ"},"cell_type":"markdown","source":"**NOTE:** in some use-cases, it can be interesting to split again the training dataset in order to create a validation dataset.  The validation dataset could be useful when we want to stop training a model when a certain precision is reached, to avoid overlearning. In our case, it may be preferable to use the training data set to train the model and achieve better accuracy.\n"},{"metadata":{"id":"MpKFXfjKfhhE"},"cell_type":"markdown","source":"# BERT"},{"metadata":{"id":"n1hatWptm3a4"},"cell_type":"markdown","source":"BERT is a bidirectional transformer pretrained using a combination of Masked Language Modeling (MLM) objective and Next Sentence Prediction (NSP) on a large corpus comprising the Book Corpus and Wikipedia."},{"metadata":{"id":"hEI0YmgN-Elx"},"cell_type":"markdown","source":"## Tokenization"},{"metadata":{"id":"M9XC0DxrgIqN"},"cell_type":"markdown","source":"Tokenization will allow us to feed batches of sequences into the model at the same time, only if these two conditions are met:\n\n1.   **the SMS are padded to the same length**;\n2.   **the SMS are truncated to be not longer model's maximum input length**.\n\nTo do the tokenization of our datasets, we also need to choose a pre-trained model. \nFor this dataset, the basic model (`bert-base-uncased`) will be sufficient:\n"},{"metadata":{"id":"nOrU4w5Nh6Kd","outputId":"1ce9727e-5305-4da7-c65c-95b2157343ba","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\ntokenizer","execution_count":null,"outputs":[]},{"metadata":{"id":"Yz7XZ-rqzOSd"},"cell_type":"markdown","source":"Before we can encode our datasets with BERT, **it is important to decide on a maximum sentence length for padding/truncating to**. This will allow us to have a better speed for training and evaluation.\n\nTo do this, we will perform one tokenization pass of the datasets in order to measure the maximum sentence length:"},{"metadata":{"id":"AKeQkvbxzJvv","outputId":"97609c03-79e1-4c6f-8523-657a8147413e","trusted":true},"cell_type":"code","source":"max_len = 0\nfor row in X_train:\n    max_len = max(max_len, len(tokenizer.encode(row)))\nprint(f\"Max sentence length (train): {max_len}\")\n\nmax_len = 0\nfor row in X_test:\n    max_len = max(max_len, len(tokenizer.encode(row)))\nprint(f\"Max sentence length (test): {max_len}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"xhSlX3_H1gMl"},"cell_type":"markdown","source":"Since we have that the maximum length sentence is 93 for the training dataset and 93 for the testing dataset, **we will take a maximum length of 96 characters for both datasets**."},{"metadata":{"id":"-02nlGCViAZ-"},"cell_type":"markdown","source":"Based on this pre-trained model, the encodings for our training and testing  datasets are generated as follows:"},{"metadata":{"id":"rSSr5974iC_k","outputId":"4c28dc08-1650-4b4a-ca31-bfad5055213b","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_encodings = tokenizer(\n    X_train.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)\ntest_encodings = tokenizer(\n    X_test.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"B380wXJTiUlg"},"cell_type":"markdown","source":"## Transformation of Labels and Encodings"},{"metadata":{"id":"3eqgIfgTiYMF"},"cell_type":"markdown","source":"Before we can Fine-Tuning and training our model, we must batched these encodings to a `TensorSliceDataset` object, so that each key in the batch encoding corresponds to a hyper-parameters named according to the model we are going to train:"},{"metadata":{"id":"LbH4Bo9ei0b-","outputId":"ce8b698b-0491-41ad-b641-37499507ec57","trusted":true},"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_encodings), y_train)\n)\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(test_encodings), y_test)\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"eBxjHnFri4Dz"},"cell_type":"markdown","source":"We are now ready to Fine-Tuning and training our BERT model!"},{"metadata":{"id":"d_mTYVvVjvzK"},"cell_type":"markdown","source":"## Fine-Tuning and Training"},{"metadata":{"id":"HwXZC6Xgjut3"},"cell_type":"markdown","source":"**Fine-tuning consists of generating embeddings specific to a task**. Since we would like to create embeddings specifically for a classification task, we will have to train our data only for this task. However, for a pre-trained BERT model that is best suited for multiple tasks, fine-tuning will not be possible. It will therefore be necessary to generate the BERT embeddings as features and pass them through an independent classifier (e.g., RandomForest).\n\nUsing the `TFTrainingArguments` class present in the `huggingface/transformers` module, the Fine-Tuning can be done this way:"},{"metadata":{"id":"35WZB4XtkUrH"},"cell_type":"markdown","source":"Following Fine-Tuning and our datasets, the training of the BERT model can be done as follows:"},{"metadata":{"id":"1OccVLKckZO4","trusted":true},"cell_type":"code","source":"training_args = TFTrainingArguments(\n    output_dir=\"/kaggle/working/sms/results/bert\",\n    num_train_epochs=8,\n    per_device_train_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"/kaggle/working/sms/logs/bert\",\n    logging_steps=10,\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"_-Heu-LLlwaV"},"cell_type":"markdown","source":"Following Fine-Tuning and our datasets, the training of the BERT model can be done as follows:"},{"metadata":{"id":"wB8aiRoskc_8","outputId":"5f345d4b-a335-46b5-e496-8c16e621a7e5","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from transformers import TFBertForSequenceClassification\n\nwith training_args.strategy.scope():\n    model = TFBertForSequenceClassification.from_pretrained(\n        \"bert-base-uncased\"\n    )\n\ntrainer = TFTrainer(\n    model=model, args=training_args, train_dataset=train_dataset\n)\ntrainer.train()","execution_count":null,"outputs":[]},{"metadata":{"id":"9MzGoVxTQwc_"},"cell_type":"markdown","source":"**NOTE:** you can ignore the warnings."},{"metadata":{"id":"NZ42EmT8k_XT"},"cell_type":"markdown","source":"This model being trained, let's save it, as well as its configuration to be able to load it directly when needed."},{"metadata":{"id":"ogvoPclHlBvb","outputId":"05e5dd0d-2dad-49cf-bf40-12773aa84e7a","trusted":true},"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/sms/models/bert\")\ntokenizer.save_pretrained(training_args.output_dir)","execution_count":null,"outputs":[]},{"metadata":{"id":"lGG-URtulFNZ"},"cell_type":"markdown","source":"## Measurement of Predictions"},{"metadata":{"id":"MpychjLyurZw"},"cell_type":"markdown","source":"The measurement of SMS predictions present in our test dataset as spam or ham, will allow us to make sure that the model is well trained."},{"metadata":{"id":"ExIxUFIRtiZm"},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"id":"fnOzrnbWlJWI"},"cell_type":"markdown","source":"Our BERT model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:"},{"metadata":{"id":"rpQJiko_hPIk","outputId":"57a7bc56-7922-4850-b1fe-4c65bb97602b","trusted":true},"cell_type":"code","source":"preds, label_ids, metrics = trainer.predict(test_dataset)\npreds[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"gMcHFz89oOhV"},"cell_type":"markdown","source":"It should be noted above that we have what are called **logits** (i.e. \t$\\exists n, n \\in ]-\\infty, \\infty[$).\n\nAnother thing we have to be careful of, is that no additional embeddings have been generated after the predictions:"},{"metadata":{"id":"tXvD_neYE9RR","outputId":"d0dfd1d4-48ba-4f80-9c69-3395ba400fd7","trusted":true},"cell_type":"code","source":"print(f\"Test dataset size: {len(y_test)}\")\nprint(f\" Predictions size: {len(preds)}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"KPsvu7-gPtXN"},"cell_type":"markdown","source":"Here it is the case, we have 13 additional embeddings. Let's make sure to delete them:"},{"metadata":{"id":"b2oXzNcfFBnM","outputId":"53ecfd31-10fa-4988-ba42-a178649381ce","trusted":true},"cell_type":"code","source":"preds = preds[: len(y_test)]\nlen(preds)","execution_count":null,"outputs":[]},{"metadata":{"id":"KnJdXThrL1Q8"},"cell_type":"markdown","source":"The reason for these extra embeddings after training is due to a `huggingface/transformers` bug, which should be fixed in the next releases."},{"metadata":{"id":"mLXhn2lZtloT"},"cell_type":"markdown","source":"### Normalization"},{"metadata":{"id":"deG90DZZpYEL"},"cell_type":"markdown","source":"To get rid of these logits, the vector of raw (non-normalized) predictions generated by the classification model should be passed to a normalization function to convert logits to probabilities. As we use a binary classification, we should use the `sigmoid` function and then the conversion of the probabilities into final predictions is done by taking the label for which the probability is highest.\n\nWith the help of the `argmax` function from `numpy`, we can make a two-shot stone:"},{"metadata":{"id":"vsxw9qO-kQst","outputId":"d7a24154-1551-40f7-b052-b4a31783b011","trusted":true},"cell_type":"code","source":"preds = np.argmax(preds, axis=1)\npreds","execution_count":null,"outputs":[]},{"metadata":{"id":"tlTlQSzDho8h"},"cell_type":"markdown","source":"**NOTE:** in a multi-class classification problem, these logits would be normalized with a `softmax` function."},{"metadata":{"id":"ucW8fXervHE6"},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"id":"QVo2MS9_vUNK"},"cell_type":"markdown","source":"Sketch the confusion matrix will allow us to measures the quality of the classification system:"},{"metadata":{"id":"Z8jjVLhflLzJ","outputId":"307b8601-01c7-43ab-cfb1-0a5d1c1a1d32","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","execution_count":null,"outputs":[]},{"metadata":{"id":"lSvTNNoXvmNB"},"cell_type":"markdown","source":"Above, each row of the confusion matrix corresponds to a real class and each column corresponds to an estimated class.\n\nThrough the matrix of confusion, we have:\n\n*  **966 SMS being ham were well predicted**: True Negative (TN);\n*  **0 ham SMS have been detected as spam** False Positive (FP);\n*  **1 spam SMS have been detected as ham** False Negative (FN);\n*  **148 spam SMS have been detected as spam** True Positive (TP)."},{"metadata":{"id":"EaPAE0Rgv9NA"},"cell_type":"markdown","source":"### Scores"},{"metadata":{"id":"RrdcC1AlwI9L"},"cell_type":"markdown","source":"Let's look at the score obtained by the predictions.\n\nAs a quick reminder:\n\n1.   **Precision:** is the ratio between the True Positives and all the Positives.\n2.   **Recall:** is the measure of our model correctly identifying True Positives.\n3.   **Accuracy:** is the ratio of the total number of correct predictions and the total number of predictions.\n\nWhich gives us:"},{"metadata":{"id":"-N2TGUdllTCn","outputId":"2cf33f8d-5f23-4a07-8e24-f4973b9a25c4","trusted":true},"cell_type":"code","source":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","execution_count":null,"outputs":[]},{"metadata":{"id":"jJRc-5Wsz1X-"},"cell_type":"markdown","source":"# DistilBERT\n"},{"metadata":{"id":"pKRUepz8fAKo"},"cell_type":"markdown","source":"DistilBERT is a distilled version of BERT, which is smaller, faster, cheaper and lighter. This variant should have performance close to BERT."},{"metadata":{"id":"rjh9lQMPgGL4"},"cell_type":"markdown","source":"## Tokenization"},{"metadata":{"id":"-pzjFRRK-lJI"},"cell_type":"markdown","source":"As for BERT, let's tokenize our dataset so that we can feed batches of sequences into the model at the same time.\n\nFor this dataset, the basic model (`distilbert-base-uncased`) will be sufficient:"},{"metadata":{"id":"Jsw_MD3-ABjX","outputId":"621dde79-8e34-4846-9f17-5025fc7cc6e1","trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\ntokenizer","execution_count":null,"outputs":[]},{"metadata":{"id":"x0Mv6aiPACam"},"cell_type":"markdown","source":"As with BERT, we will take a maximum length of 96 characters for both datasets to have a better speed for training and evaluation. \n\nBased on this pre-trained model, the encodings for our training and testing datasets are generated as follows:"},{"metadata":{"id":"oMgIFXtu04jd","outputId":"7549a87a-b874-453e-e71a-867aa0d5caae","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"train_encodings = tokenizer(\n    X_train.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)\ntest_encodings = tokenizer(\n    X_test.tolist(),\n    max_length=96,\n    padding=\"max_length\",\n    truncation=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"6XYxO-HX0T3m"},"cell_type":"markdown","source":"## Transformation of Labels and Encodings"},{"metadata":{"id":"u2Y5BUt-0igQ"},"cell_type":"markdown","source":"Similar to what we saw before, let's associate these codings to a `TensorSliceDataset` object in order to Fine-Tuning and train our model."},{"metadata":{"id":"tU7in8DF0Zkw","outputId":"26a2289d-2770-47eb-adc9-33d24f52f40a","trusted":true},"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_encodings), y_train)\n)\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(test_encodings), y_test)\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"SKlH4-ItEPE6"},"cell_type":"markdown","source":"We are now ready to fine-tuning and training our DistilBERT model!"},{"metadata":{"id":"CfnMf3MN0FMJ"},"cell_type":"markdown","source":"## Fine-Tuning and Training"},{"metadata":{"id":"NvPWLuZWExeU"},"cell_type":"markdown","source":"The fine-tuning for DistilBERT is identical to BERT:"},{"metadata":{"id":"P65UI6ZHv22Z","trusted":true},"cell_type":"code","source":"training_args = TFTrainingArguments(\n    output_dir=\"/kaggle/working/sms/results/distilbert\",\n    num_train_epochs=8,\n    per_device_train_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"/kaggle/working/sms/logs/distilbert\",\n    logging_steps=10,\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"cNtN9bFhHOEw"},"cell_type":"markdown","source":"Following Fine-Tuning and our datasets, the training of the DistilBERT model can be done as follows:"},{"metadata":{"id":"eVUldvZ7Eui5","outputId":"8b0c0498-d6c6-4c46-eb63-1469bc9b1ed4","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from transformers import TFDistilBertForSequenceClassification\n\nwith training_args.strategy.scope():\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\"\n    )\n\ntrainer = TFTrainer(\n    model=model, args=training_args, train_dataset=train_dataset\n)\ntrainer.train()","execution_count":null,"outputs":[]},{"metadata":{"id":"X_-U-RHaSeo2"},"cell_type":"markdown","source":"**NOTE:** you can ignore the warnings."},{"metadata":{"id":"UdbeM503IVHY"},"cell_type":"markdown","source":"Let's also save our DistilBERT model and its configuration in a persistent way:"},{"metadata":{"id":"lUtzyQRwIUDK","outputId":"96a46f7e-044a-41f9-9e33-f50ec2ea4edb","trusted":true},"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/sms/models/distilbert\")\ntokenizer.save_pretrained(training_args.output_dir)","execution_count":null,"outputs":[]},{"metadata":{"id":"peOARCUnIio0"},"cell_type":"markdown","source":"## Measurement of Predictions"},{"metadata":{"id":"FDV_HUur1vrL"},"cell_type":"markdown","source":"As seen previously, let's measure SMS predictions as spam or ham."},{"metadata":{"id":"3PI9FzkP1nIp"},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"id":"cADmFUuv1Esf"},"cell_type":"markdown","source":"Our DistilBERT model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:"},{"metadata":{"id":"hCmlvEsWwgWN","outputId":"70291bc3-49c5-49e0-ce73-878005de0c0f","trusted":true},"cell_type":"code","source":"preds, label_ids, metrics = trainer.predict(test_dataset)\npreds[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"fuEqmT9S1brX"},"cell_type":"markdown","source":"With DistilBERT, we also have logits."},{"metadata":{"id":"EERW8ErdVAHy"},"cell_type":"markdown","source":"Let's see if we also have additional embeddings:"},{"metadata":{"id":"sTlQctvFS50f","outputId":"a90e5314-44c0-41cb-9ed9-49897b70b7b7","trusted":true},"cell_type":"code","source":"print(f\"Test dataset size: {len(y_test)}\")\nprint(f\" Predictions size: {len(preds)}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Zq-76Os9TnQU"},"cell_type":"markdown","source":"Here it is the case, we have 13 additional embeddings. Let's make sure to delete them:"},{"metadata":{"id":"crJSVICdTrj0","outputId":"4d2cf322-b1ec-4600-981d-f14ef72173d9","trusted":true},"cell_type":"code","source":"preds = preds[: len(y_test)]\nlen(preds)","execution_count":null,"outputs":[]},{"metadata":{"id":"C8X7JtXw12F1"},"cell_type":"markdown","source":"### Normalization"},{"metadata":{"id":"F_gFUJKs14WP"},"cell_type":"markdown","source":"Let's convert these logits into probabilities and the latter into final predictions by taking the label for which the probability is highest:"},{"metadata":{"id":"xgz0j4GwVxZv","trusted":true},"cell_type":"code","source":"preds = np.argmax(preds, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"5VOjb-c12NqJ"},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"id":"2MRkugym2Y0B"},"cell_type":"markdown","source":"Using the confusion matrix, measures of the quality of the classification system are given: "},{"metadata":{"id":"lwBTWYWFVrlu","outputId":"5816540e-7913-496e-86cd-1fc1af337754","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","execution_count":null,"outputs":[]},{"metadata":{"id":"dsCBEqpfZ2-x"},"cell_type":"markdown","source":"Above, each row of the confusion matrix corresponds to a real class and each column corresponds to an estimated class.\n\nThrough the matrix of confusion, we have:\n\n*  **964 SMS being ham were well predicted**: True Negative (TN);\n*  **2 ham SMS have been detected as spam** False Positive (FP);\n*  **2 spam SMS have been detected as ham** False Negative (FN);\n*  **147 spam SMS have been detected as spam** True Positive (TP)."},{"metadata":{"id":"ifSMiAlu2yGp"},"cell_type":"markdown","source":"### Scores"},{"metadata":{"id":"R26bcBBz20Kd"},"cell_type":"markdown","source":"Let's look at the score obtained by the predictions:"},{"metadata":{"id":"HcKi7RNLQgi0","outputId":"a0f8ea8e-5a17-4de2-bc75-0ac7b863d15c","trusted":true},"cell_type":"code","source":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","execution_count":null,"outputs":[]},{"metadata":{"id":"IncnoU1c1cfh"},"cell_type":"markdown","source":"# KNN"},{"metadata":{"id":"UB10TS2h7gZT"},"cell_type":"markdown","source":"K-Nearest Neighbors (KNN) is an approach to data classification that estimates how likely a data point is to be a member of one group or the other depending on what group the data points nearest to it are in."},{"metadata":{"id":"Ja27sc_42lYq"},"cell_type":"markdown","source":"## Fine-Tuning and Training"},{"metadata":{"id":"O0YsL8bX7VjS"},"cell_type":"markdown","source":"As we said before, let's use grid search techniques using cross-validation to determine the hyper-parameters of our model and train this model on them:\n\nTo tune the hyper-parameters of the KNN, it is recommended to use grid search techniques using cross-validation (**SEE:** [scikit-learn's documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)) to evaluate the performance of the model on the data at each value.\n\nLet's use this technique to train our model according to the optimal value of the neighbors hyper-parameter:"},{"metadata":{"id":"Nkc-yI8D1eJv","outputId":"a377e413-a322-4227-c2c5-bda8bc7083b2","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", KNeighborsClassifier()),\n        ]\n    ),\n    {\n        \"clf__n_neighbors\": (8, 15, 20, 25, 40, 55),\n    }\n)\nknn.fit(X=X_train, y=y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"JSyrdegO2nyu"},"cell_type":"markdown","source":"## Measurement of Predictions"},{"metadata":{"id":"XiBTAfZL2wQr"},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"id":"xQcJq1H_2xqX"},"cell_type":"markdown","source":"Our KNN model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:"},{"metadata":{"id":"nPSO4EW72r09","outputId":"9e4bb8ba-8c55-4930-d66a-0e1097ec4b08","trusted":true},"cell_type":"code","source":"preds = knn.predict(X_test)\npreds","execution_count":null,"outputs":[]},{"metadata":{"id":"4H3ApYOU24VU"},"cell_type":"markdown","source":"Here, we already have the final predictions given by the logit probabilities."},{"metadata":{"id":"gcWyow9W26k2"},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"id":"nXfG0eWP29Bl"},"cell_type":"markdown","source":"Using the confusion matrix, measures of the quality of the classification system are given:"},{"metadata":{"id":"GlyPmO-P2-8l","outputId":"1782f180-50be-48c0-d7d4-c87de3393f89","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","execution_count":null,"outputs":[]},{"metadata":{"id":"NpLmveMz3BBA"},"cell_type":"markdown","source":"Through the confusion matrix, we have:\n\n*   **964 SMS being ham were well predicted**: True Negative (TN);\n*   **2 ham SMS have been detected as spam**: False Positive (FP);\n*   **46 spam SMS have been detected as ham**: False Negative (FN);\n*   **103 spam SMS have been detected as spam**: True Positive (TP)."},{"metadata":{"id":"ErEcFpoI5C59"},"cell_type":"markdown","source":"### Scores"},{"metadata":{"id":"itdUmPJ27ysw"},"cell_type":"markdown","source":"Let's look at the score obtained by the predictions:"},{"metadata":{"id":"rPJ3Dxxz5EN7","outputId":"7d8105ba-3003-4f24-f6fa-5af2f68c234f","trusted":true},"cell_type":"code","source":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","execution_count":null,"outputs":[]},{"metadata":{"id":"8qtjUoxkSHFz"},"cell_type":"markdown","source":"# Multinomial Naive Bayes Classifier"},{"metadata":{"id":"VVKaT-nHWFsW"},"cell_type":"markdown","source":"As the features of our dataset have discrete frequency counts, we will use the Multinomial type of Naive Bayes Model.\n\nTo detect if a SMS is consider as spam or not, the Multinomial Naive Bayes classifier will use word counts in the content of the SMS with the help of the Bag-of-Words (BoW) method. This method, will elaborate a matrix of rows according to words, where each intersection corresponds to the frequency of occurrence of these words."},{"metadata":{"id":"ci6guS9i6ozU"},"cell_type":"markdown","source":"## Fine-Tuning and Training"},{"metadata":{"id":"uQReBzq5Xx2a"},"cell_type":"markdown","source":"As we said before, let's use grid search techniques using cross-validation to determine the optimal value of the $\\alpha$ hyper-parameter of our model and train this model on this hyper-parameter:"},{"metadata":{"id":"ymZA0NJzVq38","outputId":"8475d2a4-c91f-49f9-c731-5824e744b3c0","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmnbayes = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", MultinomialNB()),\n        ]\n    ),\n    {\n        \"tfidf__use_idf\": (True, False),\n        \"clf__alpha\": (0.1, 1e-2, 1e-3),\n        \"clf__fit_prior\": (True, False),\n    },\n)\nmnbayes.fit(X=X_train, y=y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"A3pjahKUZXar"},"cell_type":"markdown","source":"Out of curiosity, let us look at which were the hyper-parameters to be privileged for the training of the model with respect to our training dataset:"},{"metadata":{"id":"xQcE0mBFZaml","outputId":"7b5d6d62-f24c-45c4-cfe4-7169dadbec7b","trusted":true},"cell_type":"code","source":"mnbayes.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"8lHcr8yPZf3Y"},"cell_type":"markdown","source":"For our training dataset, $\\alpha$ must be equal to $10^{-2}$."},{"metadata":{"id":"fvhdeQjBZi8t","outputId":"a93d09e7-594e-4c23-e639-fcef057c13f9","trusted":true},"cell_type":"code","source":"print(f\"{mnbayes.best_score_ * 100:.3f}%\") ","execution_count":null,"outputs":[]},{"metadata":{"id":"2mYy4zJFZmKJ"},"cell_type":"markdown","source":"The mean cross-validated score is therefore 98.587%"},{"metadata":{"id":"-WLNevsF6CrJ"},"cell_type":"markdown","source":"## Measurement of Predictions"},{"metadata":{"id":"LPfmjBoo6KbE"},"cell_type":"markdown","source":"As seen previously, let's measure SMS predictions as spam or ham."},{"metadata":{"id":"lS6GdxAt5jHf"},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"id":"6IR2W1p-5fTs"},"cell_type":"markdown","source":"Our Multinomial Naive Bayes model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:"},{"metadata":{"id":"2hSNd-ry5Gr0","outputId":"6379bae1-820f-4c31-ab0b-1ca0f178ca8b","trusted":true},"cell_type":"code","source":"preds = mnbayes.predict(X_test)\npreds","execution_count":null,"outputs":[]},{"metadata":{"id":"RFQ8f63n8NoM"},"cell_type":"markdown","source":"Here, we already have the final predictions given by the logit probabilities."},{"metadata":{"id":"6ojK2_g44Nrv"},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"id":"YhhmJ0z48fxz"},"cell_type":"markdown","source":"Using the confusion matrix, measures of the quality of the classification system are given:"},{"metadata":{"id":"PxZy5luk4QfR","outputId":"0a4e9ee3-669c-4c7f-d36f-d7745b73ee26","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","execution_count":null,"outputs":[]},{"metadata":{"id":"kuBaggJPIezF"},"cell_type":"markdown","source":"Through the confusion matrix, we have:\n\n*   **959 SMS being ham were well predicted**: True Negative (TN);\n*   **7 ham SMS have been detected as spam**: False Positive (FP);\n*   **14 spam SMS have been detected as ham**: False Negative (FN);\n*   **135 spam SMS have been detected as spam**: True Positive (TP)."},{"metadata":{"id":"HKS3un7n44FV"},"cell_type":"markdown","source":"### Scores"},{"metadata":{"id":"Zf3LyOkJ45yy"},"cell_type":"markdown","source":"Let's look at the score obtained by the predictions:"},{"metadata":{"id":"h5wa90nK5Blx","outputId":"31326d59-fce2-4923-be25-41747bc80f31","trusted":true},"cell_type":"code","source":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","execution_count":null,"outputs":[]},{"metadata":{"id":"itMgqgLIb9Dd"},"cell_type":"markdown","source":"# SVM"},{"metadata":{"id":"xd5dap1EyVx_"},"cell_type":"markdown","source":"## Fine-Tuning and Training"},{"metadata":{"id":"_cyBjNf5-1dg"},"cell_type":"markdown","source":"As we said before, let's use grid search techniques using cross-validation to determine the hyper-parameters of our model and train this model on them:"},{"metadata":{"id":"8wnUWcHF9RB0","outputId":"f6429747-3f1c-4846-ae8b-c174e2ea4841","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = GridSearchCV(\n    Pipeline(\n        [\n            (\"bow\", CountVectorizer()),\n            (\"tfidf\", TfidfTransformer()),\n            (\"clf\", SVC(gamma=\"auto\", C=1000)),\n        ]\n    ),\n    dict(tfidf=[None, TfidfTransformer()], clf__C=[500, 1000, 1500]),\n)\nsvc.fit(X=X_train, y=y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"4kuzEjZiE0Ap"},"cell_type":"markdown","source":"Out of curiosity, let us look at which were the hyper-parameters to be privileged for the training of the model with respect to our training dataset:"},{"metadata":{"id":"QHhdrqkO-p08","outputId":"50a110bc-4bfa-409c-a3aa-57f08d375586","trusted":true},"cell_type":"code","source":"svc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"id":"6H-CUPDNFtYE"},"cell_type":"markdown","source":"For our training dataset, $C$ must be equal to 1000 and we shouldn't transform the count matrix to a normalized term-frequency (tf) representation or for a term-frequency times inverse document-frequency (tf-idf) representation."},{"metadata":{"id":"V4pnXN5EJPoZ"},"cell_type":"markdown","source":"In addition, we can get the mean cross-validated score of the estimator that was chosen by the search:"},{"metadata":{"id":"04SWIdz0Grw8","outputId":"42a28295-85f4-437c-de88-c6949ac7757b","trusted":true},"cell_type":"code","source":"print(f\"{svc.best_score_ * 100:.3f}%\") ","execution_count":null,"outputs":[]},{"metadata":{"id":"hL89q9PxJtz6"},"cell_type":"markdown","source":"The mean cross-validated score is therefore 98.519%"},{"metadata":{"id":"c0F5JaDXL0-p"},"cell_type":"markdown","source":"## Measurement of Predictions"},{"metadata":{"id":"gKvBL1unMZ7b"},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"id":"_bb4xhJKMcqh"},"cell_type":"markdown","source":"Our SVM model being trained, we can now use it to predict if the SMS in our test dataset are spams or not:"},{"metadata":{"id":"IdIxlaXMMgJV","outputId":"ef43239f-2ee5-4ac1-893d-bc3b2aa3511d","trusted":true},"cell_type":"code","source":"preds = svc.predict(X_test)\npreds","execution_count":null,"outputs":[]},{"metadata":{"id":"nlfNGuF4MjvN"},"cell_type":"markdown","source":"Here, we already have the final predictions given by the logit probabilities."},{"metadata":{"id":"THdXw7-GL523"},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"id":"NIT2ObIKL8Kf"},"cell_type":"markdown","source":"Using the confusion matrix, measures of the quality of the classification system are given:"},{"metadata":{"id":"lDy4SLgKLsGF","outputId":"c206ab20-309d-47bd-fcb4-eb5386a1da69","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 4))\n\nheatmap = sns.heatmap(\n    data=pd.DataFrame(confusion_matrix(y_test, preds)),\n    annot=True,\n    fmt=\"d\",\n    cmap=sns.color_palette(\"Blues\", 50),\n)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize=14)\nheatmap.yaxis.set_ticklabels(\n    heatmap.yaxis.get_ticklabels(), rotation=0, fontsize=14\n)\n\nplt.title(\"Confusion Matrix\")\nplt.ylabel(\"Ground Truth\")\nplt.xlabel(\"Prediction\")","execution_count":null,"outputs":[]},{"metadata":{"id":"RlVnXiVsMB71"},"cell_type":"markdown","source":"Through the confusion matrix, we have:\n\n*   **964 SMS being ham were well predicted**: True Negative (TN);\n*   **2 ham SMS have been detected as spam**: False Positive (FP);\n*   **13 spam SMS have been detected as ham**: False Negative (FN);\n*   **136 spam SMS have been detected as spam**: True Positive (TP)."},{"metadata":{"id":"WYQPuNkBMGVj"},"cell_type":"markdown","source":"### Scores"},{"metadata":{"id":"9KwByejlMMwO"},"cell_type":"markdown","source":"Let's look at the score obtained by the predictions:"},{"metadata":{"id":"eauL7YQaMQlB","outputId":"657da511-739f-487a-d760-922ebf7e8746","trusted":true},"cell_type":"code","source":"print(f\"Precision: {precision_score(y_test, preds) * 100:.3f}%\")\nprint(f\"   Recall: {recall_score(y_test, preds) * 100 :.3f}%\")\nprint(f\" Accuracy: {accuracy_score(y_test, preds) * 100:.3f}%\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ZKghA6chT9bo"},"cell_type":"markdown","source":"# Results"},{"metadata":{"id":"kcPMsM78g0dx"},"cell_type":"markdown","source":"If we summarize the results obtained, here is what we get:"},{"metadata":{"id":"exa125GaWH9-"},"cell_type":"markdown","source":"  | ML Algo                  |                                                                  Accuracy |  Precision |      Recall |\n  | -----------------------: | ------------------------------------------------------------------------: | ---------: | ----------: |\n  | BERT                     |                                                                  **99.910** |**100.000** |  **99.329** |\n  | DistilBERT               |                                                                    99.641 |     98.658 |      98.658 |\n  | KNN                      |                                                                    95.695 |     98.095 |      69.128 |\n  | Multinomial Naive Bayes  |                                                                    98.117 |     95.070 |      90.604 |\n  | SVM                      |                                                                    98.655 |     98.551 |      91.275 |"},{"metadata":{"id":"z-Dukt-aoIVQ"},"cell_type":"markdown","source":"We can see that BERT and DistilBERT are the ML classification algorithms that provide the best results. However, based on the scores, we can see that there is no significant difference in precision and accuracy between these algorithms, it is only a few percent!"},{"metadata":{"id":"IAFIdS1jT75s"},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"id":"QMG8PrnZhjMM"},"cell_type":"markdown","source":"From this Notebook, we started by loading a dataset of Spam SMS and created our features on the raw data using Feature Engineering. \n\nOnce our features were created, we analyzed the data made available on the basis of these features, before being able to do data preprocessing which consisted in removing the presence of stop words, punctuation, digits and lemmatize the words.\n\nIn addition, we learned how to fine-tuning different Machine Learning classification algorithms. To do this, it was useful for fine-tuning some of these algorithms to use search grid techniques using cross-validation to evaluate the performance of the model.\n\nBERT and DistilBERT are to be preferred when we would like to push performance to its maximum and to optimize the avoidance of True Positive misclassification (given by the recall score).\n\nHowever, even these algorithms are the best according to the scores, we can still apply Okhalm's razor principle. Indeed, if these few percent more can be neglected, classical classification algorithms such as Multinomial Naive Bayes and SVM can still be preferred because of their simplicity of understanding and implementation."},{"metadata":{"id":"eaMNTNvOSTHh"},"cell_type":"markdown","source":"# References"},{"metadata":{"id":"Xq6UlA-5SWDN"},"cell_type":"markdown","source":"[BERT Fine-Tuning Tutorial with PyTorch](http://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n\n[Naive Bayes & SVM Spam Filtering](https://www.kaggle.com/pablovargas/naive-bayes-svm-spam-filtering)\n\n[Starter: Neural Net w/ 0.97 ROC-AUC - 99% accuracy](https://www.kaggle.com/mrlucasfischer/starter-neural-net-w-0-97-roc-auc-99-accuracy)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}