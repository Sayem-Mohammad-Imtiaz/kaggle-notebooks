{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"**This notebook gives basic NLP works like clustering data and data visualization.**","metadata":{"_uuid":"1f7a4f233da22c0d31b574a24ee63b6326425c4d","_cell_guid":"2d518942-aa71-5320-baea-0e46170db6d0"}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import Counter\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nimport sys\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.stem.porter import *\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nfrom sklearn import metrics\nimport pandas as pd \nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import jaccard_similarity_score\ncv = CountVectorizer()\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics.pairwise import cosine_similarity\nstop = set(stopwords.words(\"english\"))\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/obama-white-house.csv\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"92e0222484fd28d71b967db8657e5a14eec26120","_cell_guid":"aedc4651-27c4-c5f0-96dc-10f6e1e2f7f3"}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"data = pd.read_csv(\"../input/obama-white-house.csv\",nrows=1000)\n\ndata.head(2)","metadata":{"_uuid":"a89ebf87c91ae908b33d7692024d0a8dae26fb54","_cell_guid":"92791eee-9270-6977-18b4-32e91a56c9cd","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(data['title']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title(\"Title\")","metadata":{"_uuid":"b902bb5b948c348371af0ee0d86c94c9c53d9d60","_cell_guid":"032844f3-bc2d-d9e3-784b-c3bed3584137","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(data['content']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Content')","metadata":{"_uuid":"c36c33ba54b9a213d44073e4b3810a819f3baf24","_cell_guid":"6e4f56d3-5571-ccbd-408f-4abbba1080d6","collapsed":true}},{"cell_type":"markdown","source":"**Data Cleaning**","metadata":{"_uuid":"7f96cbea9a759b8b75de0eda4e4b4c295bc66b56","_cell_guid":"9169b7d1-cb69-81a7-9f17-22009290149e"}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"%%timeit\ndef cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s\ndata['content'] = [cleaning(s) for s in data['content']]\ndata['title'] = [cleaning(s) for s in data['title']]\n\n\n#StopWordsRemove\n\n#data['content'] = data.apply(lambda row: nltk.word_tokenize(row['content']),axis=1)\n#data['title'] = data.apply(lambda row: nltk.word_tokenize(row['title']),axis=1)\n\n#data['content'] = data['content'].apply(lambda x : [item for item in x if item not in stop])\n#data['title'] = data['title'].apply(lambda x : [item for item in x if item not in stop])","metadata":{"_uuid":"ecf809a0f9209b03838c5de4725a6f6192e7141d","_cell_guid":"5febecf4-e199-0bfd-7874-5e371374a589","collapsed":true}},{"cell_type":"markdown","source":"**Tf-idf and Kmeans**","metadata":{"_uuid":"2afbf1656c1f5c29352c980f538b6b5601700e1a","_cell_guid":"878dbd24-23aa-d5ad-72f6-51fab65fb086"}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english',use_idf=True)\nmodel = vectorizer.fit_transform(data['content'].str.upper())\nkm = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)\n\nk=km.fit(model)\nterms = vectorizer.get_feature_names()\norder_centroids = km.cluster_centers_.argsort()[:,::-1]\nfor i in range(5):\n    print(\"cluster of words %d:\" %i)\n    for ind in order_centroids[i,:10]:\n        print(' %s' % terms[ind])\n    print() \n    ","metadata":{"_uuid":"b9b5dbece5cb17468e7fa55d8a489548496cfdca","_cell_guid":"2c67a0a7-2a1e-1e27-31f5-2f91cb96772b","collapsed":true}},{"cell_type":"markdown","source":"**Building corpus from Title and Contents**","metadata":{"_uuid":"1e529fd5f45bd9bbacafb294aa7704b100019106","_cell_guid":"8c7336c0-b6f8-0ba7-1b09-4981ebe8e2bf"}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"def build_corpus(data):\n    \"Creates a list of lists containing words from each sentence\"\n    corpus = []\n    for col in ['title', 'content']:\n        for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(data)        \ncorpus[0:2]","metadata":{"_uuid":"1f88e0c63614e0529b3c4beaed4f570a8a10d640","_cell_guid":"29ce8bfe-9513-fe47-3112-6f5d753fd5a6","collapsed":true}},{"cell_type":"markdown","source":"**Words to Vector**","metadata":{"_uuid":"4a890aad13ed3b4b7e5668a71ce1f603a436eb06","_cell_guid":"cf7dda9a-01e2-92ed-fc45-3ce305cba21d"}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=400, workers=4)","metadata":{"_uuid":"af443954bca837d1d80209adda226d5034e2381e","_cell_guid":"c695e7f9-9cf7-1314-44ab-3f9c28653449","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"model.wv['states']","metadata":{"_uuid":"24ca8e9bcffe2ee6938a7d06a61a3b06a449ab05","_cell_guid":"d4d08a4f-eca7-069c-695c-d7a0c03068aa","collapsed":true}},{"cell_type":"markdown","source":"**Data Visualization**","metadata":{"_uuid":"de46e976ded53e74018ddd67b3dc21798f8c88f6","_cell_guid":"6f23a336-9a21-788e-3441-a0226dc99c02"}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\n\n","metadata":{"_uuid":"6fffa485a33c9a2da51b17dbab358efdbc04a68d","_cell_guid":"77e5647c-5549-edcd-62c5-6f36e7845977","collapsed":true}},{"execution_count":null,"outputs":[],"cell_type":"code","source":"tsne_plot(model)","metadata":{"_uuid":"12b533081c49510bec15be21c68c164b01a2e2d1","_cell_guid":"f07672f6-e75e-0d9c-5322-4dbb00712e3f","collapsed":true}}],"metadata":{"_is_fork":false,"language_info":{"file_extension":".py","version":"3.6.1","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","name":"python"},"_change_revision":0,"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}