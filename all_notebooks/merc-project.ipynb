{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style = \"white\",color_codes=True)\nsns.set(font_scale=1.5)\nfrom IPython.display import display\npd.options.display.max_columns = None\n\nfrom sklearn.model_selection import GridSearchCV #to fine tune Hyperparamters using Grid search\nfrom sklearn.model_selection import RandomizedSearchCV# to seelect the best combination(advance ver of Grid Search)\n\n# importing some ML Algorithms \nfrom sklearn.linear_model import LinearRegression # y=mx+c\nfrom sklearn.tree import DecisionTreeRegressor # Entropy(impurities),Gain. \nfrom sklearn.ensemble import RandomForestRegressor # Average of Many DT's\n\n# Testing Libraries - Scipy Stats Models\nfrom scipy.stats import shapiro # Normality Test 1\nfrom scipy.stats import normaltest # Normality Test 2\nfrom scipy.stats import anderson # Normality Test 3\nfrom statsmodels.graphics.gofplots import qqplot # plotting the Distribution of Y with a Line of dot on a 45 degree Line.\n\n# Model Varification/Validation Libraries\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\n\n\n# Matrices and Reporting Libraries\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.metrics import make_scorer\nfrom statsmodels.tools.eval_measures import rmse\nfrom sklearn.model_selection import learning_curve\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds= pd.read_csv(\"../input/mercedesbenz-greener-manufacturing/train.csv\")\ndt = pd.read_csv(\"../input/mercedesbenz-greener-manufacturing/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alright, Lets do some Data Exploratory Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.columns , dt.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.shape , dt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.select_dtypes(include='float').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.select_dtypes(include='int64').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.select_dtypes(include='object').columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some common Obsevations:\n- we can see that the columns X0 to X8 (excudes X7) are categorical \n- Target column 'y' is floating value. \n- Other columns are off int type.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"we can observe the columns X10 till the end has values 0 and 1. Let us confirm if these values are correct.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# training Data\nnp.unique(ds[ds.columns[10:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testomg Data\nnp.unique(dt[dt.columns[10:]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this confimrs that there are only 0 and 1 from the column with index 10 till end. Which is X10 to X385","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Finding the Null Values. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#training \nds.isnull().sum().unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing\ndt.isnull().sum().unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"confirmed that there are no Nulls. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Lets see how the Target column is distributed in Training Data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds[\"y\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"IQR looks like there might be outliers on the Higher values. But this we will check in the Outliers section. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Plotting the Y - Since this column is Time representation, lets see how the time is Distributed using different Plots. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,((ax1,ax2),(ax3,ax4)) = plt.subplots(nrows=2,ncols=2,figsize=(16,6))\nfig.suptitle(\"Distribution of Vaehicle Testing Times\",fontsize=12)\n\nax1.scatter(range(ds.shape[0]),np.sort(ds[\"y\"].values)) \nax2.plot(ds[\"y\"])\nsns.distplot(ds[\"y\"],kde=True,ax=ax3,bins=70)\nqqplot(ds[\"y\"],line='s',ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations : \n\n- Target Y column is almost normally distributed from the KDE grap we can see that. However, there are spikes intermitently. \n- Quantiles Plot with Slope Line shows that the deviation from the expected time. We can notice more deviation are the starting values and at the end. \n- THe Data is Positively Skewed. So the Outliers might be at the End/last quartile. ANd We can notice a single point which is outlined from the Qualtile slope line.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Normality Test\nLets Perform some Normality Test for proving the Hypothesis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Null Hyp - H0 is that , a sample was drawn from the Normal Distribution. (Accept the H0/Failed To Reject H0)\n- And if P=value > 50% then the Test failed to reject the H0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# test 1 - Shapiro Test \nstat , p = shapiro(ds[\"y\"])\nprint('Stat = %.3f, p = %.3f'%(stat,p))\n\nif p>0.05:\n    print('Sampe looks like Gausian or Sample is drawn from it.(Failed TO reject the H0)')\nelse:\n    print('Reject the H0. THis is not Noemaly Distributed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test 1 - K2 test \nstat , p = normaltest(ds[\"y\"])\nprint('Stat = %.3f, p = %.3f'%(stat,p))\n\nif p>0.05:\n    print('Sampe looks like Gausian or Sample is drawn from it.(Failed TO reject the H0)')\nelse:\n    print('Reject the H0. THis is not Noemaly Distributed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Anderson Test\nst,cv,sl= anderson(ds[\"y\"])\nfor a,b in zip(cv,sl):\n    if st < a:\n        print(\"{}   {}      {}\".format())\n        print(\"{} {} {}(accept H0)\".format(a,b,st))\n    else:\n        print('{} {}% {:.2f}(reject H0)'.format(a,int(b),st))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyzing the Columns in Training and Testing Data\nTHis will help us to compare the training and testing Data. Both should Match. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training\nprint(ds.select_dtypes(include='object').columns)\nprint(ds.select_dtypes(include='float').columns)\nprint(ds.select_dtypes(include='int64').columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing\nprint(dt.select_dtypes(include='object').columns)\nprint(dt.select_dtypes(include='float').columns)\nprint(dt.select_dtypes(include='int64').columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking unique values of each feature/column in Training and Testing\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets Tackkle the Object columns to see if the same values are there in Testing data also, or anything new \nt = ds.select_dtypes(include='object').columns\ntt = dt.select_dtypes(include='object').columns\nfor a,b in zip(t,tt):\n    print('\\nunique values in '+a+' for Training sample are : {}  '.format(ds[a].nunique()))\n    print('unique values in '+b+' for Testing samples are : {}  '.format(dt[b].nunique()))\n    print('values that are available in '+a+' Training but not in '+b+' Testing  : {}'.format(list(set(ds[a]).difference(dt[b]))))\n    print('values that are available in '+b+' Testing but not in '+a+' Training : {}'.format( list(set(dt[b]).difference(ds[a]))))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"observations :\n\n    As you can see the data in training and Testing are not the same. So we need to combine these to DS and get the Dummies out off it. So that we dont miss out any new Data. Any new Data in testing sample will make our ML model confused.\n\ncomparing the testing Target Y with the the categorical Columns so that we can discrimiate the Variance. Columns that are not discrimiatingcan be removed.\n\nNote: We are doing this to the Testing DS and not the Training DS.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in tt:\n    plt.figure(figsize=(16,6))\n    sns.boxplot(x=col,y='y',data=ds)\n    plt.xlabel(col,fontsize=10)\n    plt.ylabel('y',fontsize=10)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n\n- X0,X1 and X2 seems to be well discriminating.Why because the Variance is high and they are good predictors for testing.\n- X3,X4,X5 and X6 doesnt seems to be well discriminating power.These Columns can be removed","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering - To check if Categorical Variables X0 X1, X2 ...X8 has strong relationship with Y- Time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### ANOVA test - Since input is Categorical and Output is Numerical.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ols ('y ~ C(X4)',data=ds).fit()    \nprint('F-Statics: ',model.fvalue)\nprint('p-value: ',model.f_pvalue)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anova_table = sm.stats.anova_lm(model,type=2)\nanova_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"observation : \n- Here in this test alone says p < 0.05 and the H0 can be rejected. \n- THis means there is no significant difference netween the weights.\n- Here the X4 column can be retained. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Testing Anova using the Pair Wise Test since the previous P-value is close to 0.05","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.multicomp import pairwise_tukeyhsd\nm_comp = pairwise_tukeyhsd(endog=ds['y'],groups=ds['X4'],alpha=0.05)\nprint(m_comp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"observations:\n\n    Here the Null hypothesis has failed to reject. That means we are accepting the H0. Which is a diffrent result compared to previous one. THat means there is no difference in the weights . THat also means we can drop the column X4.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Lets test the Anova Test for all the columns. In that wasy we can see weather the column X can be retained or dropped. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = t.values\ncolumns\n\nfor col in columns: \n    model = ols('y ~ '+col, data=ds).fit()\n    print('Column {}, F-stat: {:.2f} , F-Pvalues {:.2f}'.format(col,model.fvalue,model.f_pvalue))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the X4 has P-Value >= 0.05 , That means the H0 has failed to Reject. THat means Sample was drawn from the Gausian distribution. THat also means that there is no Discrimiation. THat means the column is similar. Meaning, it might be dependednt/ related . Related column cannot be considered as the entropy will be high. So we need to drop the column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Lets detecct Outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating the 25% percentile\nQ1 = np.percentile(ds.loc[:,'y'],25) # fancy way of slicing only the Y column. You can also use ds[\"y\"]\n\n# calculating the 50% percentile\nQ2 = np.percentile(ds.loc[:,'y'],50)\n\n# calculating the 75% percentile\nQ3 = np.percentile(ds.loc[:,'y'],75)\n\n# Calculating the outlier using the IQR (Q3-Q1) and 1.5 (left over 1% in normal Distribution curve)\nstep = (Q3-Q1) *3 \n\nprint('Q1 = {}, Q2= {}, Q3={} , Outlier Step = {}'.format(Q1,Q2,Q3,step))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Left_side_outlier = ds[ds['y'] <= (Q1-step)].index  # Int64Index([], dtype='int64') / no outliers on Left side\nRight_side_outlier = ds[ds['y'] > (Q3+step)].index # Int64Index([342, 883, 1459, 3133], dtype='int64') 4 indexs of Outliers on Right side","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_left_outlier = len(Left_side_outlier) # 0\nnum_right_outlier = len(Right_side_outlier) # 4 \n\nprint('Number of the Outliers on the lower Side = {}'.format(num_left_outlier))\nprint('Number of the Outliers on the upper Side = {}'.format(num_right_outlier))\n\nprint('lower outliers = {}%'.format((num_left_outlier/ds.shape[0])*100)) #(0/4209) * 100 - this is like accuracy, Number of errors(outliers) / total number of elements(len of Y column). \nprint('lower outliers = {}%'.format((num_right_outlier/ds.shape[0])*100)) #( 4/4209) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.iloc[Right_side_outlier]['y'].sort_values(ascending= False) # Slicing those outliers Index from the DS to see their Y values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only one Outlier \nds[ds['y'] >= 170]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## converting categorelical Columns to numerical ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###  Approach 1 - One Hot encoding\n- Here we first combine the Training and testing data because if there are any unique alues in testing data then the model will fail to identify or perdict. \n- Then we will get Dummies to convert the categorical columns to dummies.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_data = ds.append(dt,ignore_index=True)\ncombined_data = pd.get_dummies(combined_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds.shape , dt.shape , combined_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting the Feature and target columns of the combined DS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = combined_data[0:len(ds)],combined_data[len(dt):] # train 0-4209 ,  Test 4209-8418","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_feature = train.drop(['y','ID'],axis=1)\ny_target = train['y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_features,X_test_features,Y_train_target,Y_test_target = train_test_split(x_feature,y_target,test_size= 0.25 , random_state=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_features.shape, Y_train_target.shape , X_test_features.shape,Y_test_target.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing  - \n### 1 . Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate\nlin = LinearRegression()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlin.fit(X_train_features,Y_train_target) # Fit the Data\n# Prediction - Training Samples\ny_pred = lin.predict(X_train_features)\n\nprint('\\nTraining Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_train_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_train_target,y_pred))\n\n# Prediction - Testing Samples\ny_pred = lin.predict(X_test_features)\n\nprint('\\nTesting Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_test_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_test_target,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing  - \n### 2 . Random Forest - RF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate\nrf = RandomForestRegressor(n_estimators = 20, #Number of decision trees used by the RF\n                           criterion='mse',# method to calculate the quality of split\n                          max_depth= 4 , # Max depth of Each Decision Tree\n                          max_features='auto', \n#say for example We have couple of DT's with 40 columns each, lets take Max feature =8 . \n#Then we calculate the Entropy to all the 40 columns and take the Mean and Max information Gain. This process is common in DT. \n#But here, in Max feature, we will take random heighest features of 8 to decide the Max info Gain. That’s all the difference. Why we are doing this is to choose the averaging. Net effect of this will give you Unbaisad output.\n\n                           min_samples_split = 0.05\n                           )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrf.fit(X_train_features,Y_train_target) # Fit the Data\n# Prediction - Training Samples\ny_pred = rf.predict(X_train_features)\n\nprint('\\nTraining Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_train_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_train_target,y_pred))\n\n# Prediction - Testing Samples\ny_pred = rf.predict(X_test_features)\n\nprint('\\nTesting Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_test_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_test_target,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation . RF is little better than Linear Regression because here we have R2 = 58 and 57 unlike LR whic has -R2 for testing samples.\n R2 = 0.58. This implies that 58% of the variability of the dependent variable has been accounted for, and the remaining 42% of the variability is still unaccounted for. So R2 should be close to 1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate\nknn = KNeighborsRegressor()\n#fit\nknn.fit(X_train_features,Y_train_target) # Fit the Data\n# Prediction - Training Samples\ny_pred = knn.predict(X_train_features)\n\nprint('\\nTraining Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_train_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_train_target,y_pred))\n\n# Prediction - Testing Samples\ny_pred = rf.predict(X_test_features)\n\nprint('\\nTesting Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_test_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_test_target,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### KNN is almost same as the RF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4. Gradiant Boosting Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate\ngbm = GradientBoostingRegressor(n_estimators=600,\n                               max_depth=8,\n                               min_samples_split=.1,\n                               max_features='sqrt',\n                               learning_rate=0.01,\n                               loss='ls')\n#fit\ngbm.fit(X_train_features,Y_train_target) # Fit the Data\n# Prediction - Training Samples\ny_pred = gbm.predict(X_train_features)\n\nprint('\\nTraining Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_train_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_train_target,y_pred))\n\n# Prediction - Testing Samples\ny_pred = rf.predict(X_test_features)\n\nprint('\\nTesting Score : ')\nprint('Mean Squared Error = %.2f'% mean_squared_error(Y_test_target,y_pred))\nprint('R2 score : %.2f'% r2_score(Y_test_target,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. XGB Regression Model\n- we split the training and Testing data.\n- Load the data in dtrain and dtest , which is the binary representation and is xtremely fast for XGB\n- THen take the mean of the Target column of the training Data. \n- Then Get the Base line Prediction for the Testing Target column.BLP = (Testing target.shape) * (Mean of the Training Target)\n- Find the MAE for the above. mean_absolute_error(test_target,BLP)\n- Ajenda is to reach teh MAE as the Base line MAE. \n- Train the Midel with XGB parameters. Model.\n- Use this Model to Predict the Dtrain and Dtest data and their scores. \n- Compare these scores with the Base Line MAE.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_features,X_test_features,Y_train_target,Y_test_target = train_test_split(x_feature,y_target,test_size= 0.2 , random_state=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating DMatrices for the XGB training. This is a Binary Representation\ndtrain = xgb.DMatrix(X_train_features,label = Y_train_target)\ndtest = xgb.DMatrix(X_test_features,label = Y_test_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_train = Y_train_target.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get prediction on the test set\nbaseline_predictions  = np.ones(Y_test_target.shape)*mean_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae_baseline = mean_absolute_error(Y_test_target,baseline_predictions)\nprint(\"BaseLine Mae is : %.2f\"%mae_baseline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_param = { \"max_depth\" : 8,\n             \"min_child_weight\":1,\n             'eta':.35,\n             'subsample':1,\n             'colsample_bytree':.9,\n             'objective':'reg:squarederror',\n             'eval_metric':'mae',\n             'validate_parameters':1,\n             'verbose_eval':False\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = xgb.train(xgb_param,\n                 dtrain,\n                 num_boost_round=999,\n                 evals=[(dtest,'Test')],\n                 early_stopping_rounds=10\n                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\ny_pred=model.predict(dtrain) # adding the DMetrics Binary Values of Training Data.\n\nprint(\"Training : Metrics...\")\nprint(\"Mean ABS Error MAE :     \",metrics.mean_absolute_error(Y_train_target,y_pred))\nprint(\"Mean sq Error MSE :      \",metrics.mean_squared_error(Y_train_target,y_pred))\n\nprint('Root Mean Sq Error RMSE: ',np.sqrt(metrics.mean_squared_error(Y_train_target,y_pred)))\nprint('r2value      :           ',metrics.r2_score(Y_train_target,y_pred))\n\ny_pred = model.predict(dtest) # adding the DMetrics Binary Values of Testing Data.\n\nprint('\\nTesting : Metrics ... ')\nprint('Mean Abs Error MAE :     ',metrics.mean_absolute_error(Y_test_target,y_pred))\nprint(\"Mean sq Error MSE :      \",metrics.mean_squared_error(Y_test_target,y_pred))\n\nprint('Root Mean Sq Error RMSE: ',np.sqrt(metrics.mean_squared_error(Y_test_target,y_pred)))\nprint('r2value      :           ',metrics.r2_score(Y_test_target,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations : XGB is the best among all for this Model. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}