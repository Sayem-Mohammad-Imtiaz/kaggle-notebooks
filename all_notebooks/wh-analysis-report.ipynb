{"nbformat":4,"cells":[{"execution_count":null,"outputs":[],"source":"##Objective\n#Learn various types of clustering algorithms as available in sklearn, We will use \"World Happiness Report data\" as dataset for clustering algorithms.\n## 1.0 Call libraries\nimport numpy as np # linear algebra & data manipulation\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time                   # To time processes\nimport warnings               # To suppress warnings\nimport matplotlib.pyplot as plt                   # For graphics\nfrom sklearn import cluster, mixture              # For clustering\nfrom sklearn.preprocessing import StandardScaler  # For scaling dataset\nimport os                     # For os related operations\nimport sys                    # For data size","cell_type":"code","metadata":{"_uuid":"6b74386680c81e894e62b987edca681e73d28612","_cell_guid":"f61d8cce-78d8-4c25-bdb7-6f503109f5a3","_kg_hide-input":false,"_kg_hide-output":false}},{"execution_count":null,"outputs":[],"source":"# 2. Read data\nX = pd.read_csv(\"../input/2017.csv\", header=0)\n\n# 3. Explore and scale\nX.columns.values\nX.shape                 # 155 X 12\nX = X.iloc[:, 2: ]      # Ignore Country and Happiness_Rank columns\nX.head(2)\nX.dtypes\nX.info\n\n# 3.1 Normalize dataset for easier parameter selection\n#    Standardize features by removing the mean and scaling to unit variance\n# 3.1.2 Instantiate scaler object\nss = StandardScaler()\n# 3.1.3 Use ot now to 'fit' &  'transform'\nss.fit_transform(X)\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"909f549b369ae12306d8d3a3f83ecd28bda9f4bc","_cell_guid":"0abed170-bf77-4f90-a826-63f5b57dc187"}},{"execution_count":null,"outputs":[],"source":"#### 4. Begin Clustering  (Methods that will be used for clustering analysis)  \n#KMeans\n#Mean Shift\n#Mini Batch K-Means\n#Spectral clustering   \n#DBSCAN\n#Affinity Propagation\n#Birch\n#Gaussian Mixture modeling\n\n# 5.1 How many clusters\n#     NOT all algorithms require this parameter\nn_clusters = 2 ","cell_type":"code","metadata":{"collapsed":true,"_uuid":"75242bc83e3f0f92617aaec065981f5c9310d8ec","_cell_guid":"65cfa631-1021-4867-b062-b6250def875e"}},{"execution_count":null,"outputs":[],"source":"## 5 KMeans\n                                 \n# KMeans algorithm clusters data by trying to separate samples in n groups\n#  of equal variance, minimizing a criterion known as the within-cluster sum-of-squares.                         \n\n# 5.1 Instantiate object\nkm = cluster.KMeans(n_clusters =n_clusters )\n\n# 5.2.1 Fit the object to perform clustering\nkm_result = km.fit_predict(X)\n\n# 5.3 Draw scatter plot of two features, coloyued by clusters\nplt.subplot(4, 2, 1)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=km_result)\nplt.title(\"K-Means\")","cell_type":"code","metadata":{"collapsed":true,"_uuid":"6d1a509592a2b46ecfdc2eb14c44f72b938e92eb","_cell_guid":"4f3a4029-33eb-4d42-a100-823cbfbb8279"}},{"execution_count":null,"outputs":[],"source":"## 6. Mean Shift\n# This clustering aims to discover blobs in a smooth density of samples.\n#   It is a centroid based algorithm, which works by updating candidates\n#    for centroids to be the mean of the points within a given region.\n#     These candidates are then filtered in a post-processing stage to\n#      eliminate near-duplicates to form the final set of centroids.\n# Parameter: bandwidth dictates size of the region to search through. \n\n# 6.1\nbandwidth = 0.1  \n\n# 6.2 No of clusters are NOT predecided\nms = cluster.MeanShift(bandwidth=bandwidth)\n\n# 6.3\nms_result = ms.fit_predict(X)\n\n# 6.4\nplt.subplot(4, 2, 2)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=ms_result)\nplt.title(\"Mean Shift\")","cell_type":"code","metadata":{"collapsed":true,"_uuid":"ae3185c25a0cc25e73ed383c043b3ace092148f3","_cell_guid":"4e5dd80e-5bee-4107-91d9-9b5ea09c6358"}},{"execution_count":null,"outputs":[],"source":"## 7. Mini Batch K-Means\n#  Similar to kmeans but clustering is done in batches to reduce computation time\n\n# 7.1 \ntwo_means = cluster.MiniBatchKMeans(n_clusters=n_clusters)\n\n# 7.2\ntwo_means_result = two_means.fit_predict(X)\n\n# 7.3\nplt.subplot(4, 2, 3)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c= two_means_result)\nplt.title(\"Mini Batch K-Means\")\n","cell_type":"code","metadata":{"collapsed":true,"_uuid":"2441dffe27bff74c8f5f607d8289072c56bda459","_cell_guid":"d964369a-f739-4a9a-804d-a3adba9e8f72"}},{"execution_count":null,"outputs":[],"source":"## 8. Spectral clustering   \n# SpectralClustering does a low-dimension embedding of the affinity matrix\n#  between samples, followed by a KMeans in the low dimensional space. It\n#   is especially efficient if the affinity matrix is sparse.\n#   SpectralClustering requires the number of clusters to be specified.\n#     It works well for a small number of clusters but is not advised when \n#      using many clusters.\n\n# 8.1\nspectral = cluster.SpectralClustering(n_clusters=n_clusters)\n\n# 8.2\nsp_result= spectral.fit_predict(X)\n\n# 8.3\nplt.subplot(4, 2, 4)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=sp_result)\nplt.title(\"Spectral clustering\")","cell_type":"code","metadata":{"collapsed":true,"_uuid":"c01a20ac6e4fa37acba0aef6959cb315f31321c1","_cell_guid":"77a3af04-ca2c-4cc2-a762-87010c883433"}},{"execution_count":null,"outputs":[],"source":"## 9. DBSCAN\n#   The DBSCAN algorithm views clusters as areas of high density separated\n#    by areas of low density. Due to this rather generic view, clusters found\n#     by DBSCAN can be any shape, as opposed to k-means which assumes that\n#      clusters are convex shaped.    \n#    Parameter eps decides the incremental search area within which density\n#     should be same\n\neps = 0.3\n\n# 9.1 No of clusters are NOT predecided\ndbscan = cluster.DBSCAN(eps=eps)\n\n# 9.2\ndb_result= dbscan.fit_predict(X)\n\n# 9.3\nplt.subplot(4, 2, 5)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5], c=db_result)\nplt.title(\"DBSCAN\")","cell_type":"code","metadata":{"collapsed":true,"_uuid":"182715fff707f746431186b7e78a3cc36f79bf05","_cell_guid":"be8bd3c1-29fd-4f29-b4b8-cb48e8c288b5"}},{"execution_count":null,"outputs":[],"source":"# 10. Affinity Propagation   \n# Creates clusters by sending messages between pairs of samples until convergence.\n#  A dataset is then described using a small number of exemplars, which are\n#   identified as those most representative of other samples. The messages sent\n#    between pairs represent the suitability for one sample to be the exemplar\n#     of the other, which is updated in response to the values from other pairs. \n#       Two important parameters are the preference, which controls how many\n#       exemplars are used, and the damping factor which damps the responsibility\n#        and availability messages to avoid numerical oscillations when updating\n#         these messages.\n\ndamping = 0.9\npreference = -200\n\n# 10.1  No of clusters are NOT predecided\naffinity_propagation = cluster.AffinityPropagation(\n        damping=damping, preference=preference)\n\n# 10.2\naffinity_propagation.fit(X)\n\n# 10.3\nap_result = affinity_propagation .predict(X)\n\n# 10.4\nplt.subplot(4, 2, 6)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=ap_result)\nplt.title(\"Affinity Propagation\")","cell_type":"code","metadata":{"collapsed":true,"_uuid":"0a85480dd86333eb11be3f782ca93c732b29c6c6","_cell_guid":"5ad9f174-74ff-4a24-83dd-b13879ad359a"}},{"execution_count":null,"outputs":[],"source":"## 11. Birch\n# The Birch builds a tree called the Characteristic Feature Tree (CFT) for the\n#   given data and clustering is performed as per the nodes of the tree\n\n# 11.1\nbirch = cluster.Birch(n_clusters=n_clusters)\n\n# 11.2\nbirch_result = birch.fit_predict(X)\n\n# 11.3\nplt.subplot(4, 2, 7)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=birch_result)\nplt.title(\"Birch\")","cell_type":"code","metadata":{"collapsed":true,"_uuid":"8a351e50603d935035a43e2a6651249f675e84e4","_cell_guid":"ce6d5a70-1ca9-401a-9145-7d0fbbc98bf3"}},{"execution_count":null,"outputs":[],"source":"# 12. Gaussian Mixture modeling\n#  It treats each dense region as if produced by a gaussian process and then\n#  goes about to find the parameters of the process\n\n# 12.1\ngmm = mixture.GaussianMixture( n_components=n_clusters, covariance_type='full')\n\n# 12.2\ngmm.fit(X)\n\n# 12.3\ngmm_result = gmm.predict(X)\nplt.subplot(4, 2, 8)\nplt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=gmm_result)\nplt.title(\"Gaussian Mixture modeling\")\n#########################################################","cell_type":"code","metadata":{"collapsed":true,"_uuid":"36cac905c28ecc3d737ba8033d8edaa8ab1df648","_cell_guid":"cff03e8b-aa35-48cc-9a5d-6e47ec9459ce","_kg_hide-output":false}},{"execution_count":null,"outputs":[],"source":"","cell_type":"code","metadata":{"collapsed":true,"_uuid":"11372925ee41692100e8ce085c756a9adbd4518a","_cell_guid":"48e71aa8-b096-4c22-8336-314dbcf2f03e"}}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","nbconvert_exporter":"python","name":"python"}},"nbformat_minor":1}