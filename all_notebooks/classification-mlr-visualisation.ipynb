{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_place = pd.read_csv(\"../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\ndata_place.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Details about the data\ndata_place.info()\n#Totally there are 14 variables and we can remove the serial number variable as it is not important","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_place.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now checking the missing value imputation\ndata_place.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can fill the salary missing values as zero because the students who are not placed has givven as missing values\ndata_place[\"salary\"]=data_place[\"salary\"].fillna(0.0)\ndata_place.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA and VISUALISATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_place.drop(\"sl_no\",axis=1,inplace=True)\ndata_place.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_place.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_place.skew()# there is no problem of skewness","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bi-variate and multi-variate analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_place.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GENDER VS SALARY","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"gender\",\"salary\",data=data_place)#we can say that the salary for male is more compared to the female","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BASED ON DIFFERENT CATEGORICAL VARIABLES SEEING THE STATUS OF THE CANDIDATES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(data_place.groupby('status')['gender'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['ssc_b'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['hsc_b'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['hsc_s'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['degree_t'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['workex'].value_counts(normalize=True))\nprint(\"\\n\")\nprint(data_place.groupby('status')['specialisation'].value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NOTE:\n\n1) GENDER: we can say that male students are placed more than females but in non-placed status also male has high ratio\n\n2) SSC_B and HSC_B: we can say that this fetaure may not be important\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# GENDER vs STATUS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"gender\", hue=\"status\", data=data_place)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"gender\",\"salary\",hue=\"status\",data=data_place)\n#male candidates got high paid jobs than females","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"gender\", data=data_place)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have more samples of male than female and male are getting high paid jobs compared to the female","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature: ssc_p (Secondary Education percentage), ssc_b (Board Of Education)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"ssc_p\",\"status\",data=data_place)\n#We can say that on an average if a person takes above 50 percent in ssc he may get placed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"ssc_b\",hue=\"status\",data=data_place)\n# we can say that the students in central board are placed more but the ssc education doesnt much effect the placement","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"ssc_b\", data=data_place)\nplt.show()\n# the students studied in central board are got high paid salaries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(\"ssc_p\", \"salary\", hue=\"ssc_b\", data=data_place)\nplt.show()\n#From this plot we can say that the candidates from central board with\n# ssc percentage with an average of 60 are getting highest paid job","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature: hsc_p (Higher Secondary Education percentage), hsc_b (Board Of Education), hsc_s (Specialization in Higher Secondary Education)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"hsc_p\",\"status\",data=data_place)\n# we can say that on an average if student gets 50 percentage, there is possibility of getting placed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"hsc_b\", hue=\"status\", data=data_place)\nplt.show()\n#In HSC other board students are placed more","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(\"salary\", \"hsc_b\", data=data_place)\n# The salary for central board candidates are high","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"hsc_s\", hue=\"status\", data=data_place)\nplt.show()\n#Arts students placed ratio is low","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"hsc_b\", data=data_place)\nplt.show()\n# Salary for Central board candidates are high","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(\"hsc_p\", \"salary\", hue=\"hsc_b\", data=data_place)\nplt.show()\n# A candidate from HSC central board with 60 percentage are getting highest paid job ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"hsc_s\", data=data_place)\nplt.show()\n#The salary for the science students in HSC is more","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Degree percentage and Degree Specialisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Kernel-Density Plot\nsns.kdeplot(data_place.degree_p[ data_place.status==\"Placed\"])\nsns.kdeplot(data_place.degree_p[ data_place.status==\"Not Placed\"])\nplt.legend([\"Placed\", \"Not Placed\"])\nplt.xlabel(\"Under Graduate Percentage\")\nplt.show()\n# the placed rate will be high if the degree percentage is around 55 percentage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"degree_t\", hue=\"status\", data=data_place)\nplt.show()\n#commerce&Mmt students are more placed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(18,6))\nsns.boxplot(\"salary\", \"degree_t\", data=data_place)\nplt.show()\n# we can say that the students in Sci-tech get high paid jobs but Comm&mgmt students are getting good jobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"workex\",hue=\"status\",data=data_place)\n#So the students with experience are getting placed and their chance of not getting place is less","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"salary\",\"workex\",data=data_place)\n#Workexperinced candidates are getting high paid jobs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EMPLOYABILITY TEST PERCENTAGE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"etest_p\",\"status\",data=data_place)\n#So this feature doesnt effect placement","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(\"etest_p\",\"salary\",data=data_place)\n# so this doesnt effect the salary also","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# POST GRADUATE SPECIALISATION ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"specialisation\",\"salary\",hue=\"status\",data=data_place)\n#mkt and finance students are getting highly paid  jobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(\"specialisation\",hue=\"status\",data=data_place)\n#Market and finance candidates are getting placed more","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MBA percantage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(\"mba_p\",\"status\",data=data_place)\n#this doesnt effect status","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot('mba_p',\"salary\",data=data_place)\n#doesnt effect salary ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FEATURE SELECTION:\n The feature which is not important are \nsalary, hsc_b and ssc_b","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = data_place.drop([\"hsc_b\",\"ssc_b\",\"salary\"],axis=1)\ndata_new.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new[\"gender\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ENCODING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new.loc[data_new['gender']=='M','gender']= 0\n\ndata_new.loc[data_new['gender']=='F','gender']= 1\n\ndata_new[\"gender\"] = data_new[\"gender\"].astype(int)\n\n\ndata_new[\"hsc_s\"] = data_new.hsc_s.map({\"Commerce\":0,\"Science\":1,\"Arts\":2})\ndata_new[\"degree_t\"] = data_new.degree_t.map({\"Comm&Mgmt\":0,\"Sci&Tech\":1, \"Others\":2})\ndata_new[\"workex\"] = data_new.workex.map({\"No\":0, \"Yes\":1})\ndata_new[\"status\"] = data_new.status.map({\"Not Placed\":0, \"Placed\":1})\ndata_new[\"specialisation\"] = data_new.specialisation.map({\"Mkt&HR\":0, \"Mkt&Fin\":1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing which model will be best for this data-set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the necessary libraries\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, fbeta_score, confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#NAIVE_BAYES MODEL\nfrom sklearn.naive_bayes import GaussianNB\n\n#SVC \nfrom sklearn.svm import SVC\n\n#XGBOOST\nfrom xgboost import XGBClassifier\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor=data_new.corr()\nplt.figure(figsize=(14,6))\nsns.heatmap(cor,annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from correlation plot we can remove the hsc_s and degree_t\ndata_new.drop([\"hsc_s\",\"degree_t\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_new.drop(\"status\", axis =1).values#independent variable\n\ny = data_new[\"status\"].values #dependant variable\n#train and test data split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 42)\n\nprint(x_train.shape, x_test.shape)\n#NOTE:\n#.values will store the values in the form of array\n#if you not give x will store the values in series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to feed the random state\nseed = 42\n\n#prepare models\nmodels = []\nmodels.append((\"LR\", LogisticRegression()))\nmodels.append((\"LDA\", LinearDiscriminantAnalysis()))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"CART\", DecisionTreeClassifier()))\nmodels.append((\"NB\", GaussianNB()))\nmodels.append((\"RF\", RandomForestClassifier()))\nmodels.append((\"SVM\", SVC(gamma = 'auto')))\nmodels.append((\"XGB\", XGBClassifier()))\n#appending all the models with their names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings(\"ignore\")# to avoid the warnings in our data-set\nresult = []\nnames = []\nscoring = 'recall'\nseed = 42\n\nfor name, model in models:\n    kfold = KFold(n_splits = 5, random_state =seed)# 5 split of data (value of k)\n    cv_results = cross_val_score(model, x_train, y_train, cv = kfold, scoring = scoring)\n    result.append(cv_results)\n    names.append(name)\n    msg = (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplot results for choosing our algorithm\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,4))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(1,1,1)\nplt.boxplot(result)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#FOR recall the best model is NAIVEBAYES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#precion\nimport warnings \nwarnings.filterwarnings(\"ignore\")# to avoid the warnings in our data-set\nresult1 = []\nnames = []\nscoring = 'precision'\nseed = 42\n\nfor name, model in models:\n    kfold = KFold(n_splits = 5, random_state =seed)# 5 split of data (value of k)\n    cv_results = cross_val_score(model, x_train, y_train, cv = kfold, scoring = scoring)\n    result1.append(cv_results)\n    names.append(name)\n    msg1 = (name, cv_results.mean(), cv_results.std())\n    print(msg1)\n#first one is mean value of a model, next one is the std deviation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplot results for choosing our algorithm\nfig = plt.figure(figsize = (8,4))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(1,1,1)\nplt.boxplot(result1)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#For precision is DECISION-TREE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# default scoring is a accuracy\nimport warnings \nwarnings.filterwarnings(\"ignore\")# to avoid the warnings in our data-set\nresult2 = []\nnames = []\nseed = 42\n\nfor name, model in models:\n    kfold = KFold(n_splits = 5, random_state =seed)# 5 split of data (value of k)\n    cv_results = cross_val_score(model, x_train, y_train, cv = kfold)\n    result2.append(cv_results)\n    names.append(name)\n    msg1 = (name, cv_results.mean(), cv_results.std())\n    print(msg1)\n#first one is mean value of a model, next one is the std deviation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (8,4))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(1,1,1)\nplt.boxplot(result2)\nax.set_xticklabels(names)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#For accuracy the best model is Random-forest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(criterion='entropy')\ndtree.fit(x_train, y_train)\ny_pred = dtree.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Random Forest Algorithm\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating confusion matrix heatmap\n\nconf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ETS Method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import ExtraTreesClassifier\nx2= data_new.drop(\"status\",axis=1)\ny= data_new[\"status\"]\n\nmodel = ExtraTreesClassifier(n_estimators =5, criterion = 'entropy')\n\nmodel.fit(x2,y)\n\nfi = model.feature_importances_\n\nprint(fi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_df = pd.DataFrame({'fi':fi, \"feature\":x2.columns})\nfi_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_df.sort_values([\"fi\"],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x2_col = fi_df[fi_df[\"fi\"]>0.05]\nx2_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now we are going to extract only these features\nx2 = x2[x2_col[\"feature\"]]\nx2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now with these values of x and y we are going to build the model\nfrom sklearn.preprocessing import MinMaxScaler\n\nstd_data = MinMaxScaler()\nstd_data = std_data.fit_transform(x2)\nstd_data = pd.DataFrame(std_data, columns =x2.columns)\nstd_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(std_data,y, test_size = 0.25, random_state = 100)\nmodel1 = RandomForestClassifier().fit(x_train,y_train)\n\ny_pred = model1.predict(x_test)\n\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating confusion matrix heatmap\n\nconf_mat = pd.DataFrame(confusion_matrix(y_test, y_pred))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# HYPER-PARAMETER TUNING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n#no of trees in randomforest\n#n_estimators = [100,200,500]\n\n#no of features to consider at every split\nmax_features = ['auto','sqrt']\n\n#max number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10,110,11)]\n\n#minimum no of samples required at each node\nmin_samples_leaf = [1,2,4]\n\n\nrandom_grid = {'max_features':max_features,'max_depth':max_depth,'min_samples_leaf':min_samples_leaf}\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator=rf,param_distributions = random_grid, n_iter= 100, cv=3,verbose=1,n_jobs=2,random_state=11)\n\nrf_random.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_random.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newmod = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=4, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=200,\n                       n_jobs=None, oob_score=False, random_state=42,\n                       verbose=0, warm_start=False).fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = newmod.predict(x_test)\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_n= data_place\ndata_n.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_n[\"gender\"] = data_n.gender.map({\"M\":0,\"F\":1})\ndata_n[\"ssc_b\"] = data_n.ssc_b.map({\"Others\":0,\"Central\":1})\ndata_n[\"hsc_b\"] = data_n.hsc_b.map({\"Others\":0,\"Central\":1})\ndata_n[\"hsc_s\"] = data_n.hsc_s.map({\"Commerce\":0,\"Science\":1,\"Arts\":2})\ndata_n[\"degree_t\"] = data_n.degree_t.map({\"Comm&Mgmt\":0,\"Sci&Tech\":1, \"Others\":2})\ndata_n[\"workex\"] = data_n.workex.map({\"No\":0, \"Yes\":1})\ndata_n[\"status\"] = data_n.status.map({\"Not Placed\":0, \"Placed\":1})\ndata_n[\"specialisation\"] = data_n.specialisation.map({\"Mkt&HR\":0, \"Mkt&Fin\":1})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cor=data_n.corr()\nplt.figure(figsize=(14,6))\nsns.heatmap(cor,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BASED ON THE CORRELATION AND PREVIOUS STEPS WE HAVE SELECTED THE IMPORTANT FEATURES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seperating Features and Target\nX = data_n[[ 'ssc_p', 'hsc_p', 'hsc_s', 'degree_p',  'workex','etest_p', 'specialisation', 'mba_p',]]\ny = data_n['status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify =y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer, accuracy_score,precision_score\nfrom sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nrandomForestFinalModel = RandomForestClassifier(n_estimators=200,criterion='gini',\n max_depth= 4 ,\n max_features= 'auto',random_state=42)\nrandomForestFinalModel.fit(X_train, y_train)\npredictions_rf = randomForestFinalModel.predict(X_test)\n\nprint(classification_report(y_test,predictions_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = pd.DataFrame(confusion_matrix(y_test, predictions_rf))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN CLASSIFICATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\ndf = pd.read_csv(\"../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"salary\"]=df[\"salary\"].fillna(0.0)\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = df.drop(['status',\"salary\"], axis = 1)\ny1 = df.status\n\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\n\n#placed as 1, not placed as 0\ny1 = encoder.fit_transform(y1)\nX1 = pd.get_dummies(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X1, y1, test_size= 0.3, random_state=41)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors= 5 )\nknn.fit(X_train2, y_train2)\ny_pred2 = knn.predict(X_test2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test2,y_pred2))\n\nprint(classification_report(y_test2, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = pd.DataFrame(confusion_matrix(y_test2, y_pred2))\nfig = plt.figure(figsize=(10,7))\nsns.heatmap(conf_mat, annot=True, annot_kws={\"size\": 16}, fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION:\n\nTHUS WE CAN SAY THAT RANDOM-FOREST WILL BE THE BEST MODEL COMPARED TO OTHER MODELS FOR THIS DATA-SET WHICH GIVES BETTER SOLUTION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MULTIPLE LINEAR REGRESSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SSC_P and HSC_P vs MBA_P","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=dataset.iloc[:,[2,4]].values # X contain columns hsc_p and ssc_p\nY=dataset.iloc[:,12].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let’s check out the coefficients for the predictors:\nregressor.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(Y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The regression equation is \n\nY= 38.56 + 0.13(ssc_p) + 0.225(hsc_p)\n\nRscore is 17%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SSC_P, DEGREE_P vs MBA_P","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=dataset.iloc[:,[2,7]].values\nY1=dataset.iloc[:,12].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X1,Y1,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let’s check out the coefficients for the predictors:\nregressor.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(Y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The regression equation is \n\nY = 39.6580 + 0.123(ssc_p) +0.215(degree_p)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# HSC_P, DEGREE_P VS MBA_P","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X2=dataset.iloc[:,[4,7]].values\nY2=dataset.iloc[:,12].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X2,Y2,test_size=0.2,random_state=0)\n\n#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let’s check out the coefficients for the predictors:\nregressor.coef_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(Y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The regression equation is\nY= 44.04 + 0.138(hsc_p) + 0.225(degree_p)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# HSC_P, SSC_P, DEGREE_P vs MBA_P","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X3=dataset.iloc[:,[2,4,7]].values # X contain columns hsc_p and ssc_p\nY3=dataset.iloc[:,12].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X3,Y3,test_size=0.2,random_state=0)\n\n#fitting multiple linear regression to the training set\nfrom sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(X_train,Y_train)\n\n#predicting the test result\ny_pred=regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r2_score(Y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The regression equation is \n\nY= 0.0814(ssc_p)+ 0.10799(hsc_p)+ 0.17898(degree_p)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION:\n\nThus we can say that the model built with ssc_p and degree_p has good rscore value compared to other models.\n\nFor this data-set,\n\nSo we can say that in order to predict mba percentage, the ssc percentage and degree percentage is enough rather than using\n\nhsc percentage feature","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}