{"cells":[{"metadata":{},"cell_type":"markdown","source":"data source : https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset/data#"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/lower-back-pain-symptoms-dataset/Dataset_spine.csv\")\nprint(data.info())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see my labels are ordered -normal= 1, abnormal= 0-. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(how=\"any\", inplace = True)  # Delete useless raw\ndata.drop([\"Unnamed: 13\"], axis = 1, inplace = True)\ndata.Class_att = [1 if each == \"Normal\" else 0 for each in data.Class_att]\n\n\nresult = data.Class_att.values\nfeatures_data = data.drop([\"Class_att\"], axis = 1)\nfeatures_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **NORMALIZATION**\nFormula = (x -min(x))/(max(x)-min(x)"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = (features_data - np.min(features_data))/(np.max(features_data)-np.min(features_data)).values\nfeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **TRAIN-TEST SPLIT**\nTrain Test Split data==> 80% of data set for Train, 20% of data set for Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, result_train, result_test = train_test_split(features, result, test_size = 2, random_state = 42)\n\n\nfeatures_train = features_train.T\nfeatures_test = features_test.T\nresult_train = result_train.T\nresult_test = result_test.T\n\nprint(\"Changed of Features and Values place.\")\n\n\nprint(\"features_train: \", features_train.shape)\nprint(\"features_test \", features_test.shape)\nprint(\"result_train: \", result_train.shape)\nprint(\"result_test: \", result_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PARAMETER INITALIZE AND SIGMOID FUNCTION**\nTime to start defining functions.First of all I need to initialize my weights and bias, then I will need a sigmoid function.\n\nSigmoid Function : f(x) = 1 / ( 1 + (e ^ -x) Initialize weight = 0.01 for each data Initialize bias = 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_weights_bias(dimension): # dimension = 12\n    weights = np.full((dimension,1), 0.01)\n    bias = 0.0\n    return weights, bias\n\ndef sigmoid(z):\n    result_head = 1/(1+np.exp(-z))\n    return result_head\n\nprint(sigmoid(0)) #test sigmoid(z)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **FORWARD AND BACKWARD PROPAGATION FUNCTION**\nz = bias + px1w1 + px2w2 + ... + pxn*wn loss function = -(1 - y) log(1- y_head) - y log(y_head) cost function = sum(loss value) / train dataset sample count"},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(weights, bias, features_train, result_train):\n    #forward\n    z = np.dot(weights.T,features_train) + bias\n    result_head = sigmoid(z)\n    \n    loss = -result_train*np.log(result_head) - (1-result_train)*np.log(1-result_head)\n    cost = (np.sum(loss))/features_train.shape[1]\n    \n    #backward\n    derivative_weights = (np.dot(features_train,((result_head-result_train).T)))/features_train.shape[1]\n    derivative_bias = np.sum(result_head-result_train)/features_train.shape[1]\n    gradients = {\"derivative_weights\" : derivative_weights, \"derivative_bias\" : derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **UPDATE**\nUpdate weights and bias with backward-forward propagation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(weights, bias, features_train, result_train, learning_rate , number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterations):\n        cost,gradients = forward_backward_propagation(weights, bias, features_train, result_train)\n        cost_list.append(cost)\n        \n        weights = weights - learning_rate*gradients[\"derivative_weights\"]\n        bias = bias - learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 5 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i : %f\" %(i,cost))\n            \n    parameters = {\"weights\" : weights,\"bias\" : bias}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number Of Iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PREDICT**\nPredict function for testing purposes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(weights,bias,features_test):\n    z = sigmoid(np.dot(weights.T,features_test)+bias)\n    result_prediction = np.zeros((1,features_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            result_prediction[0,i] = 0\n        else:\n            result_prediction[0,i] = 1\n            \n    return result_prediction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LOGISTIC REGRESSION**\nMain part. Put it all together."},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(features_train, result_train, features_test, result_test, learning_rate, number_of_iterations):\n    \n    dimension = features_train.shape[0]\n    weights, bias = initialize_weights_bias(dimension)\n    \n    parameters, gradients, cost_list = update(weights, bias, features_train, result_train, learning_rate, number_of_iterations) \n    \n    result_prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"], features_test)\n    \n    print(\"Test accuracy: {}%\".format(100-np.mean(np.abs(result_prediction_test - result_test))*100))\n\nlogistic_regression(features_train, result_train, features_test, result_test, learning_rate = 1, number_of_iterations = 100)  ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}