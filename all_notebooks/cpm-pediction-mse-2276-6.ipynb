{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Loading Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset_path = \"../input/real-time-advertisers-auction/Dataset.csv\"\nascendeum_data = pd.read_csv(dataset_path)\nascendeum_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Analyse Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_orig = ascendeum_data.copy()\nascendeum_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ascendeum_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ascendeum_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ascendeum_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n### Inferences from analysing input data:\n     1. There are 17 columns - 1 date time column, 15 Integer columns and 1 Float column\n     2. Nearly 8 coulmns are having less than 10 unique items, so they can be considered as categorical columns\n     3. There are no missign values in any of the given columns\n     4. On average, there is a 0.069 Revenue generate for 33.67 Impressions\n     5. Target, which is CPM need to calculated"},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### CPM calculation and injesting to the dataset\n\nCPM â€“ cost per Mille. It is Calculated as revenue/impressions * 1000. 'bids' and 'price' are measured in terms of CPM."},{"metadata":{"trusted":true},"cell_type":"code","source":"def CPM(revenue, impressions):\n    return revenue / impressions if impressions else 0\n\nascendeum_data['CPM'] = ascendeum_data.apply(lambda x: CPM(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\nascendeum_data['CPM'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Finding Correlation (HeatMap)"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = ascendeum_data.corr()\nplt.figure(figsize=(18,9))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"RdYlGn\",square=True, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n### Inferences from correlation analysis:\n     1. 'integration_type_id' and 'revenue_share_percent' can be dropped as they have constant values through out\n     2. 'measurable_impressions' and 'total_revenue' can be dropped as they are highly correlated with 'total_impressions'"},{"metadata":{"trusted":true},"cell_type":"code","source":"ascendeum_data= ascendeum_data.drop(['integration_type_id', 'revenue_share_percent', \n                                     'measurable_impressions', 'total_revenue'], axis = 1)\nascendeum_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = ascendeum_data.corr()\nplt.figure(figsize=(14,8))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"RdYlGn\",square=True, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n### Handling Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(ascendeum_data[\"CPM\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove the extremes/outliers from CPM. 95% of the data is within 2 standard deviations."},{"metadata":{"trusted":true},"cell_type":"code","source":"ascendeum_data = ascendeum_data[ascendeum_data['CPM'].between(ascendeum_data['CPM'].quantile(.05), ascendeum_data['CPM'].quantile(.95))]\nsns.boxplot(ascendeum_data[\"CPM\"],color=\"green\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(ascendeum_data[\"CPM\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ascendeum_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ascendeum_data['date'] =  pd.to_datetime(ascendeum_data['date'])\nascendeum_data['weekday'] = ascendeum_data['date'].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Created new column \"dayofweek\" from date to include the effect of date."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = ascendeum_data.CPM\nX = ascendeum_data.drop(['CPM', 'date'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Modelling\n\n### Split Dataset (Train, Validation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Decision Tree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nDTR_model = DecisionTreeRegressor( max_leaf_nodes =1000, random_state=0)\n\nDTR_model.fit(train_X, train_y)\n\nval_predictions = DTR_model.predict(val_X)\nprint(\"MAE:\", mean_absolute_error(val_y, val_predictions))\nprint(\"MSE:\", mean_squared_error(val_y, val_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Iterating through various \"Max leaf nodes\" values for lowest MSE and MAE."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    mse = mean_squared_error(val_y, preds_val)\n    return(mae, mse)\n\nfor max_leaf_nodes in [5, 50, 500, 1000, 2000, 5000]:\n    my_mae, my_mse = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t MAE: %d \\t\\t MSE: %d\" %(max_leaf_nodes, my_mae, my_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(random_state=1)\nforest_model.fit(train_X, train_y)\nfor_preds = forest_model.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, for_preds))\nprint(\"MSE:\", mean_squared_error(val_y, for_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    mse = mean_squared_error(val_y, preds_val)\n    return(mae, mse)\n\nfor max_leaf_nodes in [5, 50, 500, 1000, 2000, 5000]:\n    my_mae, my_mse = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t\\t MAE: %d \\t\\t MSE: %d\" %(max_leaf_nodes, my_mae, my_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 LGBM Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import plot_importance\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor as cbr\n\nmodel_lgb = lgb.LGBMRegressor(num_leaves=41, n_estimators=200)\nmodel_lgb.fit(train_X, train_y)\nlgb_preds = model_lgb.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, lgb_preds))\nprint(\"MSE:\", mean_squared_error(val_y, lgb_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 XGB Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(objective='reg:squarederror')\nmodel_xgb.fit(train_X, train_y)\nxgb_preds = model_xgb.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, xgb_preds))\nprint(\"MSE:\", mean_squared_error(val_y, xgb_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Cat Boost Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cbr = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\nmodel_cbr.fit(train_X, train_y)\ncbr_preds = model_cbr.predict(val_X)\n\nprint(\"MAE:\", mean_absolute_error(val_y, cbr_preds))\nprint(\"MSE:\", mean_squared_error(val_y, cbr_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n### Summary:\nMSE of 5 Regressor models evaluated:\n    1. Decision Tree Regressor : 2537.40\n    2. Random Forest Regressor : 2355.67\n    3. LGBM Regressor          : 2362.56\n    4. XGB Regressor           : 2373.63\n    5. Cat Boost Regressor     : 2347.23\n    \n#### Cat Boost Regressor is considered for further investigation as it is having the lowest MSE."},{"metadata":{},"cell_type":"markdown","source":"### Building ML Pipeline\n\nCreating pipeline. \nSplitting categorical and numerical columns.\nPerforming One Hot encoder on categorical columns.\nModel evaluation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nnumerical_cols = ['geo_id', 'order_id', 'ad_unit_id', 'total_impressions', 'viewable_impressions']\ncategorical_cols = ['site_id', 'ad_type_id', 'device_category_id', 'advertiser_id', 'line_item_type_id', \n                    'os_id','monetization_channel_id', 'weekday']\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),\n                                               ('cat', categorical_transformer, categorical_cols)])\n\nmodel = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\n\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\nmy_pipeline.fit(train_X, train_y)\npreds = my_pipeline.predict(val_X)\n\n# Evaluate the model\nprint('MAE:', mean_absolute_error(val_y, preds))\nprint('MSE:', mean_squared_error(val_y, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross-validation to validate the model for over-fitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmy_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                              ('model', cbr(random_seed=242, verbose=0, early_stopping_rounds=10))])\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\", scores)\n\nprint(\"Average MAE score (across experiments):\", scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nmy_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n                              ('model', cbr(random_seed=242, verbose=0, early_stopping_rounds=10))])\n\n# Multiply by -1 since sklearn calculates *negative* MSE\nscores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n\nprint(\"MSE scores:\", scores)\n\nprint(\"Average MSE score (across experiments):\", scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Predictions and Evaluation"},{"metadata":{},"cell_type":"markdown","source":"#### Defining Hybrid Ensemble Learning Model to increase prediction efficiency"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nfrom sklearn import model_selection\nfrom sklearn.metrics import confusion_matrix\n\nestimators = []\n\nmodel1 = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\nestimators.append(('logistic1', model1))\nmodel2 = RandomForestRegressor(random_state=1)\nestimators.append(('logistic2', model2))\nmodel3 = lgb.LGBMRegressor(num_leaves=41, n_estimators=200)\nestimators.append(('logistic3', model3))\nmodel4 = xgb.XGBRegressor(objective='reg:squarederror')\nestimators.append(('logistic4', model4))\nmodel5 = cbr(random_seed=242, verbose=0, early_stopping_rounds=10)\nestimators.append(('logistic5', model5))\n\n# Defining the ensemble model\nensemble = VotingRegressor(estimators)\nensemble.fit(train_X, train_y)\ny_pred = ensemble.predict(val_X)\n\n# Evaluate the model\nprint('MAE:', mean_absolute_error(val_y, y_pred))\nprint('MSE:', mean_squared_error(val_y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_df= pd.DataFrame({})\n\nboost_df['Actual_CPM']= val_y\n\nboost_df['Pred_LGB_CPM']= lgb_preds\nboost_df['Pred_XGB_CPM']= xgb_preds\nboost_df['Pred_CBR_CPM']= cbr_preds\nboost_df['Pred_DTR_CPM']= val_predictions\nboost_df['Pred_RFR_CPM']= for_preds\nboost_df['Pred_Voting_CPM'] = y_pred\nboost_df.sample(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"revenue_df = pd.DataFrame({'Actual_Impressions': val_X['total_impressions'].values,  'Actual_CPM': val_y, \n                           'Pred_Voting_CPM': boost_df['Pred_Voting_CPM'].values})\n\nrevenue_df['Pred_Revenue'] = revenue_df['Pred_Voting_CPM'] * revenue_df['Actual_Impressions'] / (1000 * 100)\nrevenue_df['Pred_Revenue'] = revenue_df['Pred_Revenue'].clip(lower=0)\nrevenue_df.sample(n=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"revenue_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Questions\n\n## 1. What is the potential revenue range our publisher can make in July?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average revenue of june month:', np.round(X_orig[\"total_revenue\"].mean(),2))\nprint('Predicted approximate revenue for july month:', np.round(revenue_df[\"Pred_Revenue\"].mean(),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n### Solution 1: Approximately our publisher in July can make revenue in the range of 0.05 to 0.07.\n----------------------------------------------------------------------------------------------------------------"},{"metadata":{},"cell_type":"markdown","source":"## 2. What is the reserve prices that he/she can set ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reserve price of june month:', np.round(boost_df[\"Actual_CPM\"].max(),2))\nprint('Predicted approximate Reserve price for july month:',np.round(boost_df[\"Pred_Voting_CPM\"].max(),2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n### Solution 2: Predicted reserve prices one can set in the range of 522.15 to 526.92.\n----------------------------------------------------------------------------------------------------------------"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}