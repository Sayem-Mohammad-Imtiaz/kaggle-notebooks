{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Traffic Flow Predictions with LSTM model\n\n## The goal is to predict traffic flow for multiple steps ahead for all the highways in Belgium"},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"## General Import"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom math import sqrt\n\nimport gc\nimport time\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import geojson\nimport geopandas as gpd\nfrom fiona.crs import from_epsg\nimport os, json\nfrom shapely.geometry import shape, Point, Polygon, MultiPoint\nfrom geopandas.tools import sjoin\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns; sns.set()\n\nfrom IPython.display import Image\n\nimport folium\n\nfrom branca.colormap import  linear\nimport json\nimport branca.colormap as cm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SEED"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import seed\n\n# Reproducability\ndef set_seed(seed=31415):\n    \n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nset_seed(31415)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check files"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Streets Network"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_belgium = gpd.read_file('/kaggle/input/belgium-obu/Belgium_streets.json')\n\nm = folium.Map([50.85045, 4.34878], zoom_start=9, tiles='cartodbpositron')\nfolium.GeoJson(df_belgium).add_to(m)\nm","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# BXL_timeseries_kaggle.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\nnew_table = pd.read_csv('../input/obu-data-preprocessing/Flow_BEL_street_30min.csv')\nnRow, nCol = new_table.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SELECT STREETS BASED ON AVERAGE TRAFFIC FLOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_value = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table_index = new_table.iloc[:,1:]\nALL_STREETS = list(table_index.columns.values)\n\nmean_flow =[]\nnew_street=[]\n\n\nfor street in ALL_STREETS:\n    single_street=table_index[street]\n    mean = np.mean(single_street)\n    mean_flow.append(mean)\n    new_street.append(street)\n    \n    \ndf_mean_flow = pd.DataFrame({'street_index':new_street, 'mean_flow': mean_flow})\nprint('')\nprint(df_mean_flow.head())\nprint('')\n\nSTREETS = df_mean_flow[(df_mean_flow['mean_flow']>= mean_value)] \nSTREETS = STREETS.sort_values(by=['street_index'])\nSTREETS = list(STREETS.street_index)\n\nprint('considering a average traffic flow of ' + str(mean_value)+' per street')\nprint('')\nprint('mean traffic flow '+str(mean_value)+ ' ---> number of street segments: ' + str(len(STREETS)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ADD Auxiliary Temporal Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_table['Datetime'] = pd.to_datetime(new_table['datetime'])\n\nDATAFRAME = new_table\nDATAFRAME = DATAFRAME.drop(['datetime'],axis=1) \nDATAFRAME = DATAFRAME[DATAFRAME.columns.intersection(STREETS)]\n\n# Auxiliary\n\nDATAFRAME['minutes'] = new_table['Datetime'].dt.minute\nDATAFRAME['hour'] = new_table['Datetime'].dt.hour\n\nDATAFRAME['hour_x']=np.sin(DATAFRAME.hour*(2.*np.pi/23))\nDATAFRAME['hour_y']=np.cos(DATAFRAME.hour*(2.*np.pi/23))\n\nDATAFRAME['day'] = new_table['Datetime'].dt.day\n\nDATAFRAME['DayOfWeek'] = new_table['Datetime'].dt.dayofweek\nDATAFRAME['WorkingDays'] = DATAFRAME['DayOfWeek'].apply(lambda y: 2 if y < 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 1 if y == 5 else y)\nDATAFRAME['WorkingDays'] = DATAFRAME['WorkingDays'].apply(lambda y: 0 if y == 6 else y)\n\nDATAFRAME = DATAFRAME.drop(['minutes','hour','day'],axis=1)\n\n# temporal features = 4\nfeat_time = 4\n\nDATAFRAME.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize Traffic Flow at particular time"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"STREETS = [int(float(s)) for s in STREETS]\n\n\ndf_belgium = df_belgium[df_belgium.index.isin(STREETS)]\ndf_belgium['Trucks_Flow'] =  DATAFRAME.iloc[2182,:-4].astype(float).values\n\nnbh_count_colormap = linear.YlOrRd_09.scale(0,200)\n\ncolormap_dept = cm.StepColormap(\n    colors=['#00ae53', '#86dc76', '#daf8aa',\n            '#ffe6a4', '#ff9a61', '#ee0028'],\n    vmin = 0,\n    vmax = 200,\n    index=[0, 20, 50, 80, 110, 150, 180])\n\npolygons = df_belgium\nm = folium.Map([50.85045, 4.34878], zoom_start= 9, tiles='cartodbpositron')\n\nstyle_function = lambda x: {\n    'fillColor': colormap_dept(x['properties']['Trucks_Flow']),\n    'color': colormap_dept(x['properties']['Trucks_Flow']),\n    'weight': 1.5,\n    'fillOpacity': 1\n}\nfolium.GeoJson(polygons,\n    style_function=style_function).add_to(m)\n\n\ncolormap_dept.caption = 'Traffic Flow (N#Trucks/30min) at (not real) 12:00 a.m.'\ncolormap_dept.add_to(m)\n\nm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SPLITTING Training/Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_step = 168*2*2 # 1 WEEK\n\n# ATTENTION: anything you learn and is not known in advance, must be learnt only from training data!\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_aux = MinMaxScaler(feature_range=(0, 1))\n\n# TRAINING --- (scaler/scaler_aux).fit_transform()\n# TESTING --- (scaler/scaler_aux).transform()\n\n# TRAINING SET\nTRAIN = DATAFRAME[: -test_step ]\ntrain_feat = scaler.fit_transform(TRAIN.values[:,:-feat_time])\n\n\n# TESTING SET\nTEST = DATAFRAME[-test_step:]\ntest_feat = scaler.transform(TEST.values[:,:-feat_time])\n\n\n# AUX are known in advance\nAUX = scaler_aux.fit_transform(DATAFRAME.values[:,-feat_time:])\ntrain_aux = AUX[: -test_step ]\ntest_aux = AUX[-test_step:]\n\n\n# concate final results\ntrain_feat = np.hstack([train_feat, train_aux])\ntest_feat = np.hstack([test_feat, test_aux])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inverse_transform(forecasts, scaler):\n    # invert scaling\n    inv_pred = scaler.inverse_transform(forecasts)\n    return inv_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nRow, nCol = DATAFRAME.shape\n\nplt.figure(figsize=(20,10))\nplt.plot(np.mean(TEST.iloc[:,:-feat_time],axis=1))\nplt.title('TESTING SET')\nplt.show()\n\nprint(f'Consider {nRow} instances (rows) and {nCol} streets segments (columns)')\nprint('')\nprint('TRAIN SIZE: '+ str(TRAIN.shape))\nprint('')\nprint('TEST SIZE: '+ str(TEST.shape))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM model - Multivariate Multiple-step ahead Prediction Model"},{"metadata":{},"cell_type":"markdown","source":"## Autoregressive Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/image-lstm/Autoregressive.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATA PREPARATION\n### * {BATCH_SIZE, INPUT_SEQUENCE (OUTPUT), FEATURES_SIZE}"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/image-lstm/DATAPREP.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_data(dataframe, INPUT, OUTPUT, AUX, BATCH):\n    \n    TOTAL = INPUT + OUTPUT\n    \n    dataset_feat = tf.data.Dataset.from_tensor_slices(dataframe)\n    \n    aux = tf.data.Dataset.from_tensor_slices(dataframe[:,-AUX:])\n    \n    dataset_labels = tf.data.Dataset.from_tensor_slices(dataframe)\n\n    # features\n    feat = dataset_feat.window(INPUT,  shift=1,  stride=1,  drop_remainder=True)\n    feat = feat.flat_map(lambda window: window.batch(INPUT))\n    \n    # aux\n    aux = aux.window(OUTPUT,  shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n    aux = aux.flat_map(lambda window: window.batch(OUTPUT))\n    \n    # labels\n    label = dataset_labels.window(OUTPUT, shift=1,  stride=1,  drop_remainder=True).skip(INPUT)\n    label = label.flat_map(lambda window: window.batch(OUTPUT))\n    \n    dataset = tf.data.Dataset.zip(((feat, aux), label))\n    \n    dataset = dataset.batch(BATCH).prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PARAMETERS"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_total_features = len(DATAFRAME.columns) \n\nsize_input = 12\nsize_forecast = 12\nsize_total = size_input + size_forecast\nsize_aux = feat_time\n\nbatch_size = 256\nbatch_train = batch_size\nbatch_test = 1\n\nwindowed_train = prep_data(train_feat, size_input, size_forecast, size_aux, batch_train)\nwindowed_test = prep_data(test_feat, size_input, size_forecast, size_aux, batch_test)\n\nlatent_dim = 150\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/image-lstm/The-structure-of-the-LSTM-unit.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/image-lstm/ECDEC.jpg\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import regularizers\n\nclass FeedBack_LSTM(tf.keras.Model):\n    \n    def __init__(self, units, sz_input, tot_feat):\n        \n        super(FeedBack_LSTM, self).__init__()\n        self.tot_feat = tot_feat\n        self.units = units\n        self.inp = sz_input\n        \n        # encoder\n        self.cell_encoder = tf.keras.layers.LSTMCell(self.units, kernel_initializer='glorot_uniform',\n                                                     recurrent_initializer='glorot_uniform', \n                                                     kernel_regularizer=regularizers.l2(0.001),\n                                                     bias_initializer='zeros') \n        \n        self.encoder = tf.keras.layers.RNN(self.cell_encoder, return_state = True)\n        \n        # decoder\n        self.cell_decoder = tf.keras.layers.LSTMCell(self.units,\n                                                     kernel_initializer='glorot_uniform',\n                                                     recurrent_initializer='glorot_uniform',\n                                                     kernel_regularizer=regularizers.l2(0.001),\n                                                     bias_initializer='zeros') \n        \n        self.decoder = tf.keras.layers.RNN(self.cell_decoder, return_state = True)\n        \n        self.dense_0 = tf.keras.layers.Dense(150, activation='relu',\n                                           kernel_regularizer=regularizers.l2(0.001))\n    \n                \n        self.dense = tf.keras.layers.Dense(self.tot_feat,\n                                           kernel_regularizer=regularizers.l2(0.001))\n        \n        \n        \n    def warmup(self, inp_encoder):\n        \n        out_encoder, *state = self.encoder(inp_encoder)\n        \n        return out_encoder, state\n    \n    \n    \n    def call(self, inputs_enc, inputs_dec):\n        \n        # Use a TensorArray to capture dynamically unrolled outputs.\n        predictions = []\n\n        # Initialize the lstm state\n        context_vector, state_enc = self.warmup(inputs_enc)\n        \n        inputs = tf.dtypes.cast(inputs_dec[:, 0, :], tf.float32)\n        \n#         print(context_vector.shape)\n#         print(inputs.shape)\n        \n        x = tf.concat([inputs, context_vector], -1)\n        \n        out_dec, state = self.cell_decoder(x, states = state_enc, training=True)\n            \n        dense_0 = self.dense_0(out_dec)\n        \n        prediction = self.dense(dense_0)\n        \n        # Insert the first prediction\n        predictions.append(prediction)\n        \n        # Run the rest of the prediction steps\n        for n in range(1, self.inp):\n\n            inputs = tf.dtypes.cast(inputs_dec[:, n, :], tf.float32)\n            \n#             print(out_dec.shape)\n#             print(inputs.shape)\n            \n            x = tf.concat([inputs, out_dec], -1)\n            \n            # Execute one lstm step.\n            out_dec, state = self.cell_decoder(x, states=state, training=True)\n            \n            dense_0 = self.dense_0(out_dec)\n        \n            prediction = self.dense(dense_0)\n        \n            # Add the prediction to the output\n            predictions.append(prediction)\n\n        # predictions.shape => (time, batch, features)\n        predictions = tf.stack(predictions)\n\n        # predictions.shape => (batch, time, features)\n        predictions = tf.transpose(predictions, [1, 0, 2])\n    \n        return predictions\n    \n    \n    def inference(self, inputs_enc, inputs_dec):\n        \n        # Use a TensorArray to capture dynamically unrolled outputs.\n        predictions = []\n\n        # Initialize the lstm state\n        context_vector, state_enc = self.warmup(inputs_enc)\n        \n        inputs = tf.dtypes.cast(inputs_dec[:, 0, :], tf.float32)\n        \n#         print(context_vector.shape)\n#         print(inputs.shape)\n        \n        x = tf.concat([inputs, context_vector], -1)\n        \n        out_dec, state = self.cell_decoder(x, states = state_enc, training=False)\n            \n        dense_0 = self.dense_0(out_dec)\n        \n        prediction = self.dense(dense_0)\n        \n        # Insert the first prediction\n        predictions.append(prediction)\n        \n        # Run the rest of the prediction steps\n        for n in range(1, self.inp):\n\n            inputs = tf.dtypes.cast(inputs_dec[:, n, :], tf.float32)\n            \n#             print(out_dec.shape)\n#             print(inputs.shape)\n            \n            x = tf.concat([inputs, out_dec], -1)\n            \n            # Execute one lstm step.\n            out_dec, state = self.cell_decoder(x, states=state, training=True)\n            \n            dense_0 = self.dense_0(out_dec)\n        \n            prediction = self.dense(dense_0)\n        \n            # Add the prediction to the output\n            predictions.append(prediction)\n\n        # predictions.shape => (time, batch, features)\n        predictions = tf.stack(predictions)\n\n        # predictions.shape => (batch, time, features)\n        predictions = tf.transpose(predictions, [1, 0, 2])\n    \n        return predictions\n    \n    \n\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define LSTM for training\n\nthe batch size for training the lstm model is 32. (different from testing as we will see below)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the input for encoder\nencoder_inputs = tf.keras.Input(shape=(size_input, n_total_features), name = 'enc_inputs')\nencoder_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the input for decoder\ndecoder_inputs = tf.keras.Input(shape=(size_input, size_aux), name='aux_inputs')\ndecoder_inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FEEDBACK_lstm = FeedBack_LSTM(latent_dim, size_input, n_total_features)\nFEEDBACK_lstm(encoder_inputs, decoder_inputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAIN MODEL"},{"metadata":{},"cell_type":"markdown","source":"### optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n\n# comment for now\n# checkpoint_dir = './training_checkpoints'\n# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n# checkpoint = tf.train.Checkpoint(optimizer = optimizer, lstm = lstm )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### loss function"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_object = tf.keras.losses.MeanAbsoluteError()\n\ndef loss_function(real, pred):\n    \n    loss_ = loss_object(real, pred)\n\n    return tf.reduce_mean(loss_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - *@tf.function decorator* - to speed-up training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n@tf.function\ndef train_step(inp_enc, inp_dec, targ):\n\n    loss = 0\n\n    with tf.GradientTape() as tape:\n        \n        predictions = FEEDBACK_lstm(inp_enc, inp_dec)\n\n        loss += loss_function(targ, predictions)\n   \n    batch_loss = loss \n    \n    variables = FEEDBACK_lstm.trainable_variables \n\n    gradients = tape.gradient(loss, variables)\n\n    optimizer.apply_gradients(zip(gradients, variables))\n    \n    return batch_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 200\n\nsteps_per_epoch = len(TRAIN) // batch_size\n\n# Keep results for plotting\ntrain_loss_results = []\ntrain_rmse_accuracy_results = []\n\nprint('')\nprint('TRAINING')\nprint('')\n\nfor epoch in range(EPOCHS):\n    \n    start = time.time()\n    \n    epoch_loss_avg = tf.keras.metrics.Mean()\n\n    total_loss = 0\n\n    for (batch, (inp, targ)) in enumerate(windowed_train.take(steps_per_epoch)):\n        \n        inp_enc = inp[0] \n        inp_dec = inp[1]\n\n        batch_loss = train_step(inp_enc, inp_dec, targ)\n\n        # Track progress\n        epoch_loss_avg.update_state(batch_loss)  # Add current batch loss\n        \n    # End epoch\n    train_loss_results.append(epoch_loss_avg.result())\n    \n    \n\n    if epoch % 10 == 0:\n        print(\"Epoch {}: Loss MAE: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n\n          \n          \nprint('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Training Progress"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, sharex=True, figsize=(12, 8))\nfig.suptitle('Training Metrics')\n\naxes.set_ylabel(\"Loss (MAE)\", fontsize=14)\naxes.plot(train_loss_results)\naxes.set_xlabel(\"Epoch\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEST and UPDATE MODEL"},{"metadata":{},"cell_type":"markdown","source":"### Define LSTM for prediction\n\nwe define the same lstm model for prediction: the only difference here is the size, batch_test = 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# input for LSTM\ninputs_test = tf.keras.Input(shape=(size_input, n_total_features), name='inputs')\nprint(' INPUT SHAPE for LSTM: { batch size, input sequence, features size}')\ninputs_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_forecasts(targets, forecasts, n_seq):\n    \n    list_rmse = []\n    list_mae = []\n    \n    for i in range(n_seq):\n        true = np.vstack([target[i] for target in targets])\n        predicted = np.vstack([forecast[i] for forecast in forecasts])\n        \n        rmse = np.sqrt((np.square(true - predicted)).mean(axis=0))\n        mae = np.absolute(true - predicted).mean(axis=0)\n        \n        list_rmse.append(rmse)\n        list_mae.append(mae)\n        \n    list_rmse = np.vstack(list_rmse)\n    list_mae = np.vstack(list_mae)\n    \n    return list_rmse, list_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecasts = []\ntargets = []\n\nrmse_list = []\nmae_list = []\n\n    \nfor (step, (inp, targ)) in enumerate(windowed_test):\n    \n        inp_enc = inp[0]\n        inp_dec = inp[1]\n\n        pred  = FEEDBACK_lstm.inference(inp_enc, inp_dec)\n        \n        truth = inverse_transform(targ[0][:,:-feat_time],  scaler)\n        pred = inverse_transform(pred[0][:,:-feat_time],  scaler)\n        \n        forecasts.append(pred)\n        targets.append(truth)\n        \n        rmse, mae = evaluate_forecasts(targets, forecasts, 12)\n           \n        rmse_list.append(rmse)\n        mae_list.append(mae)\n           \n        plt.plot(np.sum(pred, axis=1), label='Prediction') \n        plt.plot(np.sum(truth, axis=1), label='Truth') \n#         plt.ylim(-1, 150)\n        plt.title('Average Prediction on all highways in Belgium')\n        plt.legend()\n        plt.show()\n        \n        print('* Time step '+str(step))\n        print('* Prediction Accuracy (MAE) '+ str(np.absolute(truth - pred).mean()))\n        print('----')\n        print('* After prediction UPDATE model with new streets observations')\n        \n        new_instance = test_feat[step,:].reshape(1,-1)\n    \n        train_feat = np.vstack([train_feat, new_instance])\n    \n        windowed_new = prep_data(train_feat, size_input, size_forecast, size_aux, batch_train)\n\n        update_steps_per_epoch = len(train_feat)//batch_train\n        \n        UPDATE = 2\n        \n        for epoch in range(UPDATE):\n            \n            # resetting the hidden state at the start of every epoch if state_train = True\n#             lstm.reset_states()\n            \n            for (batch, (inp_new, targ_new)) in enumerate(windowed_new.take(update_steps_per_epoch)):\n            \n                inp_enc = inp_new[0] \n                inp_dec = inp_new[1]\n\n                batch_loss = train_step(inp_enc, inp_dec, targ_new)\n\n                # Track progress\n                epoch_loss_avg.update_state(batch_loss)  # Add current batch loss\n                \n            # End epoch\n            train_loss_results.append(epoch_loss_avg.result())\n\n\n            if epoch % UPDATE == 0:\n                print(\"UPDATE - Epoch {}: Loss MAE: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE_MEAN = np.mean(rmse_list,axis=0).mean(axis=1)\nRMSE_STD =  np.std(rmse_list,axis=0).std(axis=1)\n\nfor i in range(len(RMSE_MEAN)):\n    print('t+'+str(i+1)+' RMSE MEAN ' +str(np.round(RMSE_MEAN[i],3))+' +- '+str(np.round(RMSE_STD[i],3)))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE_MEAN = np.mean(mae_list,axis=0).mean(axis=1)\nMAE_STD =  np.std(mae_list,axis=0).std(axis=1)\n\nfor i in range(len(MAE_MEAN)):\n    print('t+'+str(i+1)+' MAE MEAN ' +str(np.round(MAE_MEAN[i],3))+' +- '+str(np.round(MAE_STD[i],3)))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\n# Saving the objects:\nwith open('save_predictions_results.pkl', 'wb') as f: \n    pickle.dump([rmse_list, mae_list], f)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}