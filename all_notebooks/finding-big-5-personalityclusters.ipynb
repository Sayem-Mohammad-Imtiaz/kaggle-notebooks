{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport pycountry\nos.popen('cd ../input/big-five-personality-test/IPIP-FFM-data-8Nov2018; ls').read()\npath = r'../input/big-five-personality-test/IPIP-FFM-data-8Nov2018/data-final.csv'\ndf_full = pd.read_csv(path, sep='\\t')\npd.options.display.max_columns = 999\ndf_full.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full=df_full[df_full.IPC==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC,LinearSVC,SVR\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import PassiveAggressiveClassifier,Perceptron,LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier,MLPRegressor,BernoulliRBM\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.svm import LinearSVR,SVC\nfrom sklearn.utils import check_array\n\n\nclass StackingEstimator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **fit_params):\n        self.estimator.fit(X, y, **fit_params)\n        return self\n    def transform(self, X):\n        X = check_array(X)\n        X_transformed = np.copy(X)\n        # add class probabilities as a synthetic feature\n        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n\n        # add class prodiction as a synthetic feature\n        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n\n        return X_transformed\n    \nClassifiers = [\n               #Perceptron(n_jobs=-1),\n               #SVR(kernel='rbf',C=1.0, epsilon=0.2),\n               #CalibratedClassifierCV(LinearDiscriminantAnalysis(), cv=4, method='sigmoid'),    \n               #OneVsRestClassifier( SVC(    C=50,kernel='rbf',gamma=1.4, coef0=1,cache_size=3000,)),\n               #KNeighborsClassifier(10),\n               #DecisionTreeClassifier(),\n               RandomForestClassifier(n_estimators=200),\n               ExtraTreesClassifier(n_estimators=250,random_state=0), \n               #OneVsRestClassifier(ExtraTreesClassifier(n_estimators=10)) , \n               #MLPClassifier(alpha=0.510,activation='logistic'),\n               #LinearDiscriminantAnalysis(),\n               #OneVsRestClassifier(GaussianNB()),\n               #AdaBoostClassifier(),\n               #GaussianNB(),\n               #QuadraticDiscriminantAnalysis(),\n               #SGDClassifier(average=True,max_iter=100),\n               XGBClassifier(max_depth=5, base_score=0.005),\n               #LogisticRegression(C=1.0,multi_class='multinomial',penalty='l2', solver='saga',n_jobs=-1),\n               #LabelPropagation(n_jobs=-1),\n               #LinearSVC(),\n               #MultinomialNB(alpha=.01),    \n               #    make_pipeline(\n               #     StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n               #     StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=3, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7)),\n               #     AdaBoostClassifier()\n               # ),\n\n              ]\n\n\nimport numpy as np\nfrom numpy.linalg import norm, svd\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import sparse_random_matrix\n#svd = TruncatedSVD(n_components=int(disc.shape[1]-1), n_iter=7, random_state=42)\n\n\ndef robustSVD(X,n_comp,lmbda=.01, tol=1e-3, maxiter=100, verbose=True):\n    \n    svd = TruncatedSVD(n_components=n_comp, n_iter=10, random_state=42)\n    \"\"\"\n    Inexact Augmented Lagrange Multiplier\n    \"\"\"\n    Y = X\n    norm_two = norm(Y.ravel(), 2)\n    norm_inf = norm(Y.ravel(), np.inf) / lmbda\n    dual_norm = np.max([norm_two, norm_inf])\n    Y = Y / dual_norm\n    A = np.zeros(Y.shape)\n    E = np.zeros(Y.shape)\n    dnorm = norm(X, 'fro')\n    mu = 1.25 / norm_two\n    rho = 1.5\n    sv = 10.\n    n = Y.shape[0]\n    itr = 0\n    while True:\n        Eraw = X - A + (1 / mu) * Y\n        Eupdate = np.maximum(Eraw - lmbda / mu, 0) + np.minimum(Eraw + lmbda / mu, 0)\n        #U, S, V = svd(X - Eupdate + (1 / mu) * Y, full_matrices=False)\n        X_n=svd.fit_transform(X - Eupdate + (1 / mu) * Y)\n        S=svd.singular_values_\n        if itr ==0:\n            pd.DataFrame(svd.explained_variance_ratio_*100).plot()\n        svp = (S > 1 / mu).shape[0]\n        print(S.sum())\n        if svp < sv:\n            sv = np.min([svp + 1, n])\n        else:\n            sv = np.min([svp + round(.05 * n), n])\n        #Aupdate = np.dot(np.dot(U[:, :svp], np.diag(S[:svp] - 1 / mu)), V[:svp, :])\n        Aupdate = svd.inverse_transform(X_n)\n        A = Aupdate\n        E = Eupdate\n        Z = X - A - E\n        Y = Y + mu * Z\n        mu = np.min([mu * rho, mu * 1e7])\n        itr += 1\n        \n        if ((norm(Z, 'fro') / dnorm) < tol) or (itr >= maxiter):\n            u=svd.components_\n            pd.DataFrame(svd.explained_variance_ratio_*100).plot()\n            pd.DataFrame(A[:33,:20]).plot()\n            pd.DataFrame(E[:33,:20]).plot()\n            break\n    if verbose:\n        print(\"Finished at iteration %d\" % (itr))  \n    return A, E, X_n, S\n# the description explains nothing... bizar ?\n### so its clear that police reports don't explain the cause of the accident.\nfrom sklearn.model_selection import train_test_split\n\n\ndef classy(mtrain,mtest,mkolom,veld,idvld,thres,probtrigger):\n    mtrain=mtrain[mkolom+[veld]]\n    mtest=mtest[mkolom]\n    from sklearn import preprocessing\n    from sklearn.metrics import roc_curve, auc,recall_score,precision_score,average_precision_score\n    \n\n    print('train',mtrain.shape,'test',mtest.shape)\n    # Label Encoding - Target \n    print (\"Label Encode strings... \")\n    from sklearn.preprocessing import LabelEncoder\n    #encode train veld\n    lb = LabelEncoder()\n    label = lb.fit_transform(mtrain[veld])\n    #encode all categorial fields\n    def label_encoding(col):\n        le = LabelEncoder()\n        le.fit(list(mtrain[col].values) + list(mtest[col].values))\n        #print(mtrain[col])\n        mtrain[col] = le.transform(mtrain[col].astype(str))\n        mtest[col] = le.transform(mtest[col].astype(str))\n    features=mtrain.columns\n    num_cols = mtrain._get_numeric_data().columns \n    cat_cols = list(set(features) - set(num_cols))\n    for col in cat_cols:\n        print(col)\n        label_encoding(col)\n        \n    e_=mtrain[mkolom].append(mtest[mkolom])\n    #scale\n    from sklearn.preprocessing import MinMaxScaler\n    mms = MinMaxScaler()\n    e_ = mms.fit_transform(e_)\n    #svd\n    svdcol=mtrain.shape[1]-35  #number of  components for SVD   \n    from sklearn.decomposition import TruncatedSVD\n    from scipy.sparse.linalg import svds, eigs\n    #from scipy.sparse import vstack\n    #e_, s, vt = svds(e_, k=svdcol)  #vstack for sparse matrix\n    #e_,e1_,u_,s_=robustSVD(e_,svdcol)\n    e_,s,vt =np.linalg.svd(e_,full_matrices=False)\n    vt=pd.DataFrame(vt,index=mkolom)\n    #print(vt)\n    featx=pd.DataFrame(s,columns=['sing'])\n    featx['descr']=''\n    featx['perc']=featx['sing']/featx ['sing'].sum()*100\n    \n    for xi in vt.columns:\n        featx.iat[xi,1]=list(vt.sort_values(xi,ascending=False)[:3].index)\n    print(featx)\n    #svd = TruncatedSVD(n_components=svdcol, n_iter=7, random_state=42)\n    #e_=svd.fit_transform(e_)  #sparse\n    #A_,er_,e_,s_=robustSVD(e_.values,svdcol+1)  #robust\n    #e_=svd.fit_transform(e_)\n    print('SVD',e_.shape)#,e_)\n    \n    #\n    #find most relevant features\n    clf = ExtraTreesClassifier(n_estimators=100)\n    model = SelectFromModel(clf, prefit=True,threshold =(thres)/100)    \n    #clf = clf.fit( e_[:len(mtrain)], label)\n    #New_features = model.transform( e_[:len(mtrain)])\n    #Test_features= model.transform(e_[-len(mtest):])\n    #New_features= e_[:len(mtrain)]\n    #Test_features=e_[-len(mtest):]\n \n    pd.DataFrame(e_[:len(mtrain)]).plot.scatter(x=0,y=1,c=mtrain[veld]+1,title='Train SVD classes')\n    pd.DataFrame(np.concatenate((e_[-len(mtest):],e_[:len(mtrain)]))).plot.scatter(x=0,y=1,c=[1 for x in range(len(mtest))]+[2 for x in range(len(mtrain))],colormap='viridis',title='Train-2 versus Test-1 SVD')\n    print('Model with threshold',thres/100,e_[:len(mtrain)].shape,e_[-len(mtest):].shape,e_.shape)\n    print('____________________________________________________')\n    \n    Model = []\n    Accuracy = []\n    for clf in Classifiers:\n        fitI=clf.fit(e_[:len(mtrain)],label)\n        pred=fitI.predict(e_[:len(mtrain)])\n\n        #print(pred_prob)\n        Model.append(clf.__class__.__name__)\n        Accuracy.append( (mtrain[veld]==pred).mean() )\n        #print('Accuracy of '+clf.__class__.__name__ +' is '+str(accuracy_score(train[veld],pred)))\n        #prediction of test\n        predicty=lb.inverse_transform(fitI.predict(e_[-len(mtest):]))\n        print(predicty)\n        sub = pd.DataFrame({idvld: mtest[idvld],veld: predicty})\n        #sub.plot(x=idvld,kind='kde',title=clf.__class__.__name__ +str((label==pred).mean()) +'prcnt') \n        #score = average_precision_score(label==label, pred==label)  # works only binary\n        #print('train score: {:.6f}'.format(score))\n        feat=pd.DataFrame(featx.descr)\n        feat['importance']=clf.feature_importances_\n        feat=feat.sort_values(by=['importance'],ascending=False)\n        print(feat.head())\n        feat.plot.barh(x=0,y='importance')\n        sub2=pd.DataFrame(pred,columns=[veld])\n        if veld in mtest.columns:\n            print( clf.__class__.__name__ +str((label==pred).mean() )+' accuracy versus unknown',(sub[veld]==mtest[veld]).mean() )\n        else:\n            print( clf.__class__.__name__ +str((label==pred).mean() ) )\n        klassnaam=clf.__class__.__name__+\".csv\"\n        sub.to_csv(klassnaam, index=False)\n        if probtrigger:\n            pred_prob=fitI.predict_proba(Test_features)\n            sub=pd.DataFrame(pred_prob)\n    return sub\n#kolom=[x for x in X_train.columns if x not in ['Fatal','Date Of Stop','Model','Time Of Stop','Description','Geolocation','Location','Latitude','Longitude']]\nkolom=[x for  x in df_full.reset_index().columns if x not in ['testelapse']]\nsubx=classy(df_full.iloc[:,:].sample(1000).fillna(0).reset_index(),df_full.iloc[:,:].sample(200).fillna(0).reset_index(),kolom,'testelapse','index',0.3,False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def kluster(data,grbvar,label,nummercl,level):\n    '''nummercl < ncol'''\n\n\n    from sklearn.cluster import KMeans\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.metrics import confusion_matrix\n    import matplotlib.pyplot as plt\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.neighbors import KNeighborsClassifier,NeighborhoodComponentsAnalysis\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE,Isomap,SpectralEmbedding,spectral_embedding,LocallyLinearEmbedding,MDS #limit number of records to 100000\n    ytrain=data[label]\n    if True:\n        from category_encoders.cat_boost import CatBoostEncoder\n        CBE_encoder = CatBoostEncoder()\n        cols=[ci for ci in data.columns if ci not in ['index',label]]\n        coltype=data.dtypes\n        featured=[ci for ci in cols]\n        ytrain=data[label]\n        data = CBE_encoder.fit_transform(data.drop(label,axis=1), ytrain)\n        data[label]=ytrain\n\n    clusters = [PCA(n_components=nummercl,random_state=0,whiten=True),\n                TruncatedSVD(n_components=nummercl, n_iter=7, random_state=42),\n                FastICA(n_components=nummercl,random_state=0),\n                NMF(n_components=nummercl,random_state=0),\n                Isomap(n_components=nummercl),\n                LocallyLinearEmbedding(n_components=nummercl),\n                #SpectralEmbedding(n_components=nummercl),\n                #MDS(n_components=nummercl),\n                TSNE(n_components=3,random_state=0),\n                UMAP(n_neighbors=nummercl,n_components=10, min_dist=0.3,metric='minkowski'),\n                ] \n    clunaam=['PCA','tSVD','ICA','NMF','Iso','LLE','Spectr','MDS','tSNE','UMAP']\n    \n    grbdata=data.groupby(grbvar).mean()\n    simdata = cosine_similarity(grbdata.fillna(0))\n    if len(grbdata)<3:\n        simdata=data#.drop(grbvar,axis=1)\n        simdata=simdata.dot(simdata.T)\n        from sklearn import preprocessing\n        simdata = preprocessing.MinMaxScaler().fit_transform(simdata)\n\n    for cli in clusters:\n        print(cli)\n        clunm=clunaam[clusters.index(cli)] #find naam\n        if clunm=='NMF':\n            simdata=simdata-simdata.min()+1\n        svddata = cli.fit_transform(simdata)\n\n        km = KMeans(n_clusters=nummercl, random_state=0)\n        km.fit_transform(svddata)\n        cluster_labels = km.labels_\n        clulabel='Clu'+clunm+str(level)\n        cluster_labels = pd.DataFrame(cluster_labels, columns=[clulabel])\n        print(cluster_labels.head())\n        pd.DataFrame(svddata).plot.scatter(x=0,y=1,c=cluster_labels[clulabel].values,colormap='viridis')\n        print(clunm,cluster_labels.mean())\n        plt.show()\n\n        clusdata=pd.concat([pd.DataFrame(grbdata.reset_index()[grbvar]), cluster_labels], axis=1)\n        if len(grbdata)<3: \n            data['Clu'+clunm+str(level)]=cluster_labels.values\n            \n        else:\n            data=data.merge(clusdata,how='left',left_on=grbvar,right_on=grbvar)\n        \n        print('Correlation\\n',confusion_matrix ( data[label],data[clulabel]))\n            \n    return data\ntrain2=kluster(df_full.iloc[:,:].sample(10000).fillna(0).reset_index(),'country','index',5,1)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}