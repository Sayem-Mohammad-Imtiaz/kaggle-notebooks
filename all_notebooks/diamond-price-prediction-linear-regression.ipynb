{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Problem Statement 1: Linear Regression: Diamond price prediction.¶**\nYou are hired by a company Gem Stones co ltd, which is a cubic zirconia manufacturer. You are provided with the dataset containing the prices and other attributes of almost 27,000 cubic zirconia (which is an inexpensive diamond alternative with many of the same qualities as a diamond). The company is earning different profits on different prize slots. You have to help the company in predicting the price for the stone on the bases of the details given in the dataset so it can distinguish between higher profitable stones and lower profitable stones so as to have better profit share. Also, provide them with the best 5 attributes that are most important."},{"metadata":{},"cell_type":"markdown","source":"Data Dictionary:\nVariable Name : Description\n\nCarat:Carat weight of the cubic zirconia.\nCut: Describe the cut quality of the cubic zirconia. Quality is increasing order Fair, Good, Very Good,Premium,Ideal.\nColor: Colour of the cubic zirconia.With D being the best and J the worst.\nClarity :cubic zirconia Clarity refers to the absence of the Inclusions and Blemishes. (In order from Best to Worst, FL = 1. flawless, I3= level 3 inclusions) FL, IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\nDepth : The Height of a cubic zirconia, measured from the Culet to the table, divided by its average Girdle Diameter.\nTable : The Width of the cubic zirconia's Table expressed as a Percentage of its Average Diameter.\nPrice : the Price of the cubic zirconia.\nX : Length of the cubic zirconia in mm.\nY : Width of the cubic zirconia in mm.\nZ : Height of the cubic zirconia in mm."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np   \nimport pandas as pd    \nimport seaborn as sns\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt   \nimport matplotlib.style\n%matplotlib inline \nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport math","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.1. Read the data and do exploratory data analysis. Describe the data briefly. (Check the null values, Data types, shape, EDA). Perform Univariate and Bivariate Analysis.¶"},{"metadata":{},"cell_type":"markdown","source":"**Importing data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the CSV file into pandas dataframe\ndf = pd.read_csv('../input/gemstone-price-prediction/cubic_zirconia.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check top few records to get a feel of the data structure\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes  # Checking the type of the dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape  # Checking the shape of the dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the info data types column wise\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation-1: \n1. <b>.The data set contains 26967 row, 11 columns .\n1. <b>.In the given data set there are 2 Integer type features,6 Float type features.\n   <b> 3 Object type features. Where 'price' is the target variable and all other are predector variable.\n1. <b>The first column is an index (\"Unnamed: 0\")as this only serial no, we can remove it.\n1. <b>Exept depth, in all the column non null count is 26967."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the serial no column as it is useless for the model\nlr_df = df.drop('Unnamed: 0', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the summary statistics of the dataset\nlr_df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the summary statistics of the  object variable.\nlr_df.describe(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the values which are equal to zero.\nprint(\"Number of rows with x == 0: {} \".format((lr_df.x==0).sum()))\nprint(\"Number of rows with y == 0: {} \".format((lr_df.y==0).sum()))\nprint(\"Number of rows with z == 0: {} \".format((lr_df.z==0).sum()))\nprint(\"Number of rows with depth == 0: {} \".format((lr_df.depth==0).sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping dimentionless diamonds\nlr_df = lr_df.drop(lr_df[lr_df[\"x\"]==0].index)\nlr_df = lr_df.drop(lr_df[lr_df[\"y\"]==0].index)\nlr_df = lr_df.drop(lr_df[lr_df[\"z\"]==0].index)\nlr_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation-2:**\n.On the given data set the the mean and median values does not have much differenc.\n.We can observe Min value of \"x\", \"y\", & \"z\" are zero this indicates that they are faulty values. As we   know dimensionless or 2-dimensional diamonds are not possible. So we need to filter out those as it    clearly faulty data entries.\n.There are three object data type 'cut', 'color' and 'clarity'."},{"metadata":{},"cell_type":"markdown","source":"### Performing EDA :  We will follow the below mentioned steps to perform EDA\n\n#Step 1 :Checking & Removing duplicates.\n\n#Step 2: Checking and treating Missing value.\n\n#Step 3: Outlier Treatment.\n\n#Step 4: Univariate  Analysis.\n\n#Step 5: Bivariate Analysis."},{"metadata":{},"cell_type":"markdown","source":"### EDA-Step-1: Checking for duplicate records in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"dups = lr_df.duplicated()\nprint('Number of duplicate rows = %d' % (dups.sum()))\nprint(lr_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Before',lr_df.shape)\nlr_df.drop_duplicates(inplace=True) \nprint('After',lr_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dups = lr_df.duplicated()\nprint('Number of duplicate rows = %d' % (dups.sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA-Step 2:  Checking Missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are there any missing values ?\nlr_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation-3: \nwe can observe there are 697 missing value in the depth column.\nMissing value treatment will be done in section 1.2."},{"metadata":{},"cell_type":"markdown","source":"### EDA-Step 3 : Outlier Checks."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['carat','depth', 'table', 'x', 'y', 'z',\n       'price' ]\nfor i in cols:\n    sns.boxplot(lr_df[i],whis=1.5)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Outlier treatment :\ndef remove_outlier(col):\n    sorted(col)\n    Q1,Q3=np.percentile(col,[25,75])\n    IQR=Q3-Q1\n    lower_range= Q1-(1.5 * IQR)\n    upper_range= Q3+(1.5 * IQR)\n    return lower_range, upper_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in lr_df[cols].columns:\n    lr,ur=remove_outlier(lr_df[column])\n    lr_df[column]=np.where(lr_df[column]>ur,ur,lr_df[column])\n    lr_df[column]=np.where(lr_df[column]<lr,lr,lr_df[column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['carat','depth', 'table', 'x', 'y', 'z',\n       'price' ]\nfor i in cols:\n    sns.boxplot(lr_df[i],whis=1.5)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EDA-Step 4 : Univariate Analysis.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.hist(figsize=(20,30));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.skew()   # to measure the skeweness of every attribute.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation-4: :\n (1).There is significant amount of outlier present in some variable.\n (2) We can see that the distribution of some quantitative features like \"carat\" and the target feature \"price\" are heavily \"right-skewed\"."},{"metadata":{},"cell_type":"markdown","source":"**EDA-Step 5 : Bivariate Analysis.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\ndf_attr = ( lr_df[cols])\nsns.pairplot(df_attr, diag_kind='kde')  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Get the Correlation Heatmap**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(lr_df.corr(),annot=True,fmt='.2f',cmap='rainbow',mask=np.triu(lr_df.corr(),+1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How each feature affects the price of diamonds.¶**"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = lr_df.corr()\ncorrelations[\"price\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation-5: \nIt can be inferred that most features correlate with the price of Diamond. The notable exception is \"depth\" which has a negligible correlation (<1%)."},{"metadata":{},"cell_type":"markdown","source":"### EDA for Categorical variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA for categorical columns 'CUT'.\nsns.catplot('cut', data=lr_df, kind='count',aspect=2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='cut', y='price', kind='box', data=lr_df, aspect=2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"bservation on 'CUT': The Premium Cut on Diamonds are the most Expensive, followed by Very Good Cut."},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA for categorical columns 'Color'.\nsns.catplot('color', kind='count', data=lr_df, aspect=2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='color', y='price', data=lr_df, aspect =2.5, kind='box')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EDA for categorical columns 'Clarity'.\nsns.catplot('clarity', data=lr_df, kind='count',aspect=2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='clarity', y='price', data=lr_df, aspect =2.5, kind='box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation on 'clarity': The Diamonds clarity with VS1 & VS2  are the most Expensive.**"},{"metadata":{},"cell_type":"markdown","source":"### The inferences drawn from the above Exploratory Data analysis: \n\n#### Observation-1: \n(1).'Price' is the target variable while all others are the predictors.\n(2).The data set contains 26967 row, 11 column.\n(3).In the given data set there are 2 Integer type features,6 Float type features.\n    3 Object type features. Where 'price' is the target variable and all other are predector variable.\n(4)The first column is an index (\"Unnamed: 0\")as this only serial no, we can remove it."},{"metadata":{},"cell_type":"markdown","source":"**Observation-2:**\n(1).On the given data set the the mean and median values does not have much differenc. (2).We can observe Min value of \"x\", \"y\", \"z\" are zero this indicates that they are faulty values. As we know dimensionless or 2-dimensional diamonds are not possible. So we have filter out those as it clearly faulty data entries. (3).There are three object data type 'cut', 'color' and 'clarity'."},{"metadata":{},"cell_type":"markdown","source":"**Observation-3:**\nwe can observe there are 697 missing value in the depth column. There are some duplicate row present. (33 duplicate rows out of 26958). which is nearly 0.12 % of the total data. So on this case we have dropped the duplicated row."},{"metadata":{},"cell_type":"markdown","source":"#### Observation-4: :\nThere are significant amount of outlier present in some variable,the features with datapoint that are far from the rest of dataset which will affect the outcome of our regression model. So we have treat the outliar.\nWe can see that the distribution of some quantitative features like \"carat\" and the target feature \"price\" are heavily \"right-skewed\"."},{"metadata":{},"cell_type":"markdown","source":"#### Observation-5: \nIt looks like most features do correlate with the price of Diamond. The notable exception is \"depth\" which has a negligble correlation (~1%).\nObservation on 'CUT': The Premium Cut on Diamonds are the most Expensive, followed by Very Good Cut."},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Impute null values if present, also check for the values which are equal to zero. Do they have any meaning or do we need to change them or drop them? Do you think scaling is necessary in this case?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we have checked there Are some missing values.\nlr_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputing missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets replace the missing values with median value. \nlr_df = lr_df.fillna(lr_df.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for the values which are equal to zero."},{"metadata":{},"cell_type":"markdown","source":"In Qs.1.1 we have alrady check for 'Zero' value.\nand  we can observe there are some amount of 'Zero' value present on the data set on variable 'x', 'y','z'.\n\nThis indicates that they are faulty values. \n### As we know dimensionless or 2-dimensional diamonds are not possible. So we have filter out those as it clearly faulty data entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.shape # after removing 'zero value' from data set the data shape became as follows.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Do you think scaling is necessary in this case?"},{"metadata":{},"cell_type":"markdown","source":"Scaling or Standardizing the features around the centre and 0 with a standard deviation of 1 is important when we compare measurements that have different units. Variables that are measured at different scales do not contribute equally to the analysis and might end up creating a bias.\n\nFor example, A variable that ranges between 0 and 1000 will outweigh a variable that ranges between 0 and 1. Using these variables without standardization will give the variable with the larger range weight of 1000 in the analysis. Transforming the data to comparable scales can prevent this problem.\n\nIn this data set we can see the all the variable are in different scale i.e price are in 1000s unit and depth and table are in 100s unit, and carat is in 10s. So its necessary to scale or standardise the data to allow each variable to be compared on a common scale. With data measured in different \"units\" or on different scales (as here with different means and variances) this is an important data processing step if the results are to be meaningful or not dominated by the variables that have large variances.\n\n### But is scaling  necessary in this case? \nNo, it is not necessary, we'll get an equivalent solution whether we apply some kind of linear scaling or not. \nBut recommended for regression techniques as well because it would help gradient descent to converge fast and reach the global minima. When number of features becomes large, it helps is running model quickly else the starting point would be very far from minima, if the scaling is not done in preprocessing.\n\n<b>For now we will process the model without scaling and later we will check the output with scaled data of regression model output."},{"metadata":{},"cell_type":"markdown","source":"### 1.3 Encode the data (having string values) for Modelling. Data Split: Split the data into  train and test (70:30). Apply Linear regression. Performance Metrics: Check the performance of Predictions on Train and Test sets using Rsquare, RMSE."},{"metadata":{},"cell_type":"markdown","source":"### Encode the data (having string values)"},{"metadata":{},"cell_type":"markdown","source":"#### Geting unique counts of all Objects."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('cut\\n',lr_df.cut.value_counts())\nprint('\\n')\nprint('color\\n',lr_df.color.value_counts())\nprint('\\n')\nprint('clarity\\n',lr_df.clarity.value_counts())\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting objects to categorical codes."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df['cut']=np.where(lr_df['cut'] =='Ideal', '4', lr_df['cut'])\nlr_df['cut']=np.where(lr_df['cut'] =='Premium', '3', lr_df['cut'])\nlr_df['cut']=np.where(lr_df['cut'] =='Very Good', '2', lr_df['cut'])\nlr_df['cut']=np.where(lr_df['cut'] =='Good', '1', lr_df['cut'])\nlr_df['cut']=np.where(lr_df['cut'] =='Fair', '0', lr_df['cut'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df['color']=np.where(lr_df['color'] =='D', '6', lr_df['color'])\nlr_df['color']=np.where(lr_df['color'] =='E', '5', lr_df['color'])\nlr_df['color']=np.where(lr_df['color'] =='F', '4', lr_df['color'])\nlr_df['color']=np.where(lr_df['color'] =='G', '3', lr_df['color'])\nlr_df['color']=np.where(lr_df['color'] =='H', '2', lr_df['color'])\nlr_df['color']=np.where(lr_df['color'] =='I', '1', lr_df['color'])\nlr_df['color']=np.where(lr_df['color'] =='J', '0', lr_df['color'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df['clarity']=np.where(lr_df['clarity'] =='IF', '7', lr_df['clarity'])\nlr_df['clarity']=np.where(lr_df['clarity'] =='VVS1', '6', lr_df['clarity'])\nlr_df['clarity']=np.where(lr_df['clarity'] =='VVS2', '5', lr_df['clarity'])\nlr_df['clarity']=np.where(lr_df['clarity'] =='VS1', '4', lr_df['clarity'])\nlr_df['clarity']=np.where(lr_df['clarity'] =='VS2', '3', lr_df['clarity'])\nlr_df['clarity']=np.where(lr_df['clarity'] =='SI1', '2', lr_df['clarity'])\nlr_df['clarity']=np.where(lr_df['clarity'] =='SI2', '1', lr_df['clarity'])\nlr_df['clarity']=np.where(lr_df['clarity'] =='I1', '0', lr_df['clarity'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.dtypes # The'cut','color','clarity' column still showing as 'Object'.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the'cut','color','clarity' column from object / string type to float.\n\nlr_df['cut'] = lr_df['cut'].astype('float64')\nlr_df['color'] = lr_df['color'].astype('float64')\nlr_df['clarity'] = lr_df['clarity'].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train-Test Split:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy all the predictor variables into X dataframe\nX = lr_df.drop('price', axis=1)\n\n# Copy target into the y dataframe.This is the dependent variable\ny = lr_df[['price']]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us break the X and y dataframes into training set and test set. For this we will use\n#Sklearn package's data splitting function which is based on random function\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split X and y into training and test set in 70:30 ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 , random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invoke the LinearRegression function and find the bestfit model on training data\n\nregression_model = LinearRegression()\nregression_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us explore the coefficients for each of the independent attributes\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Observation-1: \nY=mx +c (m= m1,m2,m3...m9) here 9 diferent co-efficients will learn aling with the intercept which is \"c\" from the model.\n\nFrom the above coefficients for each of the independent attributes we can conclude\nThe one unit increase in carat increases price by 8901.941.\nThe one unit increase in cut increases price by 109.188.\nThe one unit increase in color increases price by 272.921.\nThe one unit increase in clarity increases price by 436.441.\nThe one unit increase in y increases price by 1464.827.\nThe one unit increase in depth increases price by 8.236,\n\nBut The one unit increase in table decreases price by -17.345,\nThe one unit increase in x decreases price by -1417.908,\nThe one unit increase in z decreases price by -711.225.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check the intercept for the model\n\nintercept = regression_model.intercept_[0]\n\nprint(\"The intercept for our model is {}\".format(intercept))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation-2:**\nThe intercept (often labelled the constant) is the expected mean value of Y when all X=0. If X never equals 0, then the intercept has no intrinsic meaning.\n\nThe intercept for our model is -3171.950447307667. In preset case when the other predictor variable are zero i.e like carat,cut, color, clarity all are zero then the C=-3172. ( Y = m1X1 + m2X2+ ….. + mnXn + C + e) that means price is -3172. which is meaningless. We can do Z score or scaling the data and make it nearly zero."},{"metadata":{"trusted":true},"cell_type":"code","source":" # R square on training data\nregression_model.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model score - R2 or coeff of determinant\n# R^2=1–RSS / TSS =  RegErr / TSS\n\n# R square on testing data\nregression_model.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation-3: \nR-square is the percentage of the response variable variation that is explained by a linear model. Or:\n\nR-square = Explained variation / Total variation\n\nR-squared is always between 0 and 100%: 0% indicates that the model explains none of the variability of the response data around its mean.100% indicates that the model explains all the variability of the response data around its mean.\nIn this regression model we can see the R-square value on Training and Test data respectively 0.9311935886926559 and 0.931543712584074."},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE on Training data\npredicted_train=regression_model.fit(X_train, y_train).predict(X_train)\nnp.sqrt(metrics.mean_squared_error(y_train,predicted_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RMSE on Testing data\npredicted_test=regression_model.fit(X_train, y_train).predict(X_test)\nnp.sqrt(metrics.mean_squared_error(y_test,predicted_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since this is regression, plot the predicted y value vs actual y values for the test data\n# A good model's prediction will be close to actual leading to high R and R2 values\ny_pred = regression_model.predict(X_test)\nplt.scatter(y_test['price'], y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation-4:\nwe can see that the is a linear plot, very strong corelation between the predicted y and actual y.\nBut there are lots of spread. That indicated some kind noise present on the data set i.e Unexplained variances on the output."},{"metadata":{},"cell_type":"markdown","source":"**Linear regression Performance Metrics:**\n\n*  intercept for the model:     -3171.950447307667\n*  R square on training data: 0.9311935886926559\n*  R square on testing data:   0.931543712584074\n* \n*  RMSE on Training data:      907.1312415459143\n*  RMSE on Testing data:        911.8447345328436\n\n\n As the training data & testing data score are almost inline,  we can conclude this model is a Right-Fit Model.\n"},{"metadata":{},"cell_type":"markdown","source":"### Applying zscore statsmodels"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import zscore\n\nX_train_scaled  = X_train.apply(zscore)\nX_test_scaled = X_test.apply(zscore)\ny_train_scaled = y_train.apply(zscore)\ny_test_scaled = y_test.apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invoke the LinearRegression function and find the bestfit model on training data\n\nregression_model = LinearRegression()\nregression_model.fit(X_train_scaled, y_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us explore the coefficients for each of the independent attributes\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intercept = regression_model.intercept_[0]\n\nprint(\"The intercept for our model is {}\".format(intercept))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model score - R2 or coeff of determinant\n# R^2=1–RSS / TSS\n\nregression_model.score(X_test_scaled, y_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation-5:\nNow we can observe by applying z score the intercept became -5.87961525130473e-16. Earlier it was -3171.950447307667.\nthe co-efficient has changed, the bias became nearly zero but the overall accuracy still same."},{"metadata":{},"cell_type":"markdown","source":"### Check Multi-collinearity using VIF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vif = [variance_inflation_factor(X.values, ix) for ix in range(X.shape[1])] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\nfor column in X.columns:\n    if i < 11:\n        print (column ,\"--->\",  vif[i])\n        i = i+1\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe there are very strong multi collinearity present in the data set. Ideally it should be within 1 to 5."},{"metadata":{},"cell_type":"markdown","source":"#### We are exploring the Linear Regression using statsmodels as we are interested in some more statistical metrics of the model."},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression using statsmodels."},{"metadata":{},"cell_type":"markdown","source":"### concatenate X and y into a single dataframe\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.concat([X_train, y_train], axis=1)\ndata_test=pd.concat([X_test,y_test],axis=1)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.formula.api as smf\nlm1 = smf.ols(formula= 'price ~ carat+cut+color+clarity+depth+table+x+y+z', data = data_train).fit()\nlm1.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm1.summary())  #Inferential statistics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation-6:\nAssuming null  hypothesis is true, i.e there is no relationship between this variable with price. from that universe we have drawn the sample and on this sample we have found this co-efficient for the variable shown above.\n\nNow we can ask what is the probability of finding this co-efficient in this drawn sample if in the real world the co-efficient is zero. \nAs we see here the overall P value is less than alpha, so rejecting H0 and accepting Ha that atleast 1 regression co-efficient is not '0'. Here all regression co-efficients are not '0'.\n\nFor an example:\nwe can see the p value is showing 0.449 for 'depth' variable, which is much higher than 0.05. That means this dimension is useless.\nSo we can say that the attribute which are having p value greater than 0.05 are poor predictor for price."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate MSE\nmse = np.mean((lm1.predict(data_train.drop('price',axis=1))-data_train['price'])**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate MSE\nmse1 = np.mean((lm1.predict(data_test.drop('price',axis=1))-data_test['price'])**2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Root Mean Squared Error - RMSE\nnp.sqrt(mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Root Mean Squared Error - RMSE\nnp.sqrt(mse1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction on Test data\ny_pred = lm1.predict(data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_test['price'], y_pred)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in np.array(lm1.params.reset_index()):\n    print('({}) * {} +'.format(round(j,2),i),end=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The final Linear Regression equation is  \n  \n<b> price = b0 + b1 *carat[T.True] + b2 * cut + b3 * color + b4 * clarity+ b5 * depth + b6 *  table + b7 * x + b8 * y + b9 *z True </b>\n  \n<b> price = (-3171.95) * Intercept + (8901.94) * carat + (109.19) * cut + (272.92) * color + (436.44) * clarity + (8.24) * depth + (-17.35) * table + (-1417.91)) * x + (1464.83) * y + (-711.23) * z _True</b>  \n  \n1. <b>When carat increases by 1 unit, diamond price increases by 8901.94 units, keeping all other predictors constant.\n2. <b>When cut increases by 1 unit, diamond price increases by 109.19 units, keeping all other predictors constant.\n3. <b>When color increases by 1 unit, diamond price increases by 272.92 units, keeping all other predictors constant.\n4. <b>When clarity increases by 1 unit, diamond price increases by 436.44 units, keeping all other predictors constant.\n5. <b>When y increases by 1 unit, diamond price increases by 1464.83 units, keeping all other predictors constant.\n\n<b> As per model these five attributes that are most important attributes 'Carat', 'Cut', 'color','clarity' and width i.e  <b>'y' for predicting the price.\n\n  \n  \nThere are also some negative co-efficient values, for instance, corresponding co-efficient (-1417.91) for 'x',(-711.23) for z and (-17.35) for table This implies, these are inversely proportional with diamond price."},{"metadata":{},"cell_type":"markdown","source":"### Observation-7:\n\n* On the given data set we can see the 'X' i.e Length of the cubic zirconia in mm. having negative co-efficient. And the p value is less than 0.05, so can conclude that as higher the length of the stone is a lower profitable stones.\n\n* Similarly for the 'z' variable\n  having negative co-efficient i.e -711.23. And the p value is less than 0.05, so we can conclude that  as higher the 'z' of the stone is a lower profitable stones.\n \n* Also we can see the 'y' width in mm having positive co-efficient. And the p value is less than 0.05, so we can conclude that  higher the width of the stone is a higher profitable stones.\n \n* Finally we can conclude that best 5 attributes that are most important are 'Carat', 'Cut', 'color','clarity' and width i.e 'y' for predicting the price."},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Inference: Basis on these predictions, what are the business insights and recommendations."},{"metadata":{},"cell_type":"markdown","source":"### Inference: \nwe can see that the from the linear plot, very strong corelation between the predicted y and actual y.\nBut there are lots of spread. That indicates some kind noise present on the data set i.e Unexplained variances on the output."},{"metadata":{},"cell_type":"markdown","source":"**Linear regression Performance Metrics:**\n\n*  intercept for the model:     -3171.950447307667\n*  R square on training data: 0.9311935886926559\n*  R square on testing data:   0.931543712584074\n* \n*  RMSE on Training data:      907.1312415459143\n*  RMSE on Testing data:        911.8447345328436\n\n\n**As the training data & testing data score are almost inline,  we can conclude this model is a Right-Fit Model.**"},{"metadata":{},"cell_type":"markdown","source":"### Impact of scaling: \nNow we can observe by applying z score the intercept became -5.87961525130473e-16. Earlier it was -3171.950447307667.\nthe co-efficient has changed, the bias became nearly zero but the overall accuracy still same."},{"metadata":{},"cell_type":"markdown","source":"### Multi collinearity: \nWe can observe there are very strong multi collinearity present in the data set."},{"metadata":{},"cell_type":"markdown","source":"### From statsmodels:\nwe can see <b>R-squared:0.931 and Adj. R-squared: 0.931 are same.\nThe overall P value is less than alpha."},{"metadata":{},"cell_type":"markdown","source":"Finally we can conclude that <b>Best 5 attributes that are most important are 'Carat', 'Cut', 'color','clarity' and width i.e 'y' for predicting the price."},{"metadata":{},"cell_type":"markdown","source":"1. <b>When 'carat' increases by 1 unit, diamond price increases by 8901.94 units, keeping all other predictors constant.\n2. <b>When 'cut' increases by 1 unit, diamond price increases by 109.19 units, keeping all other predictors constant.\n3. <b>When 'color' increases by 1 unit, diamond price increases by 272.92 units, keeping all other predictors constant.\n4. <b>When 'clarity' increases by 1 unit, diamond price increases by 436.44 units, keeping all other predictors constant.\n5. <b>When 'y' increases by 1 unit, diamond price increases by 1464.83 units, keeping all other predictors constant.\n    \n1. <b>we can see the p value is showing 0.449  for depth variable, which is much greater than 0.05. \n   <b>That means this attribute is useless.\n\n\n\n1. <b>There are also some negative co-efficient values, we can see the 'X' i.e Length of the cubic zirconia in mm. having              negative co-efficient -1417.9089.\n   <b> And the p value is less than 0.05, so can conclude that as higher the length of the stone is a lower profitable stones.\n    \n1. <b>Similarly for the 'z' variable\n   <b>having negative co-efficient i.e -711.23. And the p value is less than 0.05, so we can conclude that as higher the 'z'      <b>of the stone is a lower profitable stones.\n       "},{"metadata":{},"cell_type":"markdown","source":"### Recommendations: \n\n#### The Gem Stones company should consider the features'Carat', 'Cut', 'color','clarity' and width i.e 'y' as  most important for predicting the price."},{"metadata":{},"cell_type":"markdown","source":"To distinguish between higher profitable stones and lower profitable stones so as to have better profit share."},{"metadata":{},"cell_type":"markdown","source":"As we can see from the model Higher the width('y') of the stone is higher the price. \n#### So the stones having higher width('y') should consider in higher profitable stones.\n\nThe 'Premium Cut' on Diamonds are the most Expensive, followed by 'Very Good' Cut, these should consider in higher profitable stones.\n\nThe Diamonds clarity with 'VS1' &'VS2' are the most Expensive.So these two category also consider in higher profitable stones."},{"metadata":{},"cell_type":"markdown","source":"As we see for 'X' i.e Length of the stone, higher the length of the stone is lower the  price. \n#### So higher  the Length('x') of the stone are lower is the profitability."},{"metadata":{},"cell_type":"markdown","source":"higher the 'z' i.e Height of the stone is, lower the price.This is because if a Diamond's Height  is too large  Diamond will become 'Dark' in appearance because it will no longer return an Attractive amount of light. That is why\n#### Stones with higher  'z'  is also are lower  in profitability."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}