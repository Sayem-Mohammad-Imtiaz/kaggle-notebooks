{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.cluster import DBSCAN\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fetch_data(filename):\n    \"\"\"\n    This function is used to upload training file and convert into dataframe.\n    :param: filename, :type: .csv\n    :return: df\n    :return type: class 'pandas.core.frame.DataFrame'\n    \"\"\"\n    path = '../input/'+filename+'.csv'\n    df = pd.read_csv(path)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = \"ccdata/CC GENERAL\"\nfile2 = \"iris/Iris\"\nfile3 = \"headbrain/headbrain\"\ndf = fetch_data(file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()\ndf = df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if \"Species\" in df.columns:\n    df = df.drop(\"Species\",axis=1)\n    df = df.drop(\"Id\",axis=1)\nelif \"CUST_ID\" in df.columns:\n    df = df.drop(\"CUST_ID\",axis=1)\nelse:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df)\ndf=scaler.transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef fit_dbscan(eps,samples,max_amonaly_percentage):\n    dbscan = DBSCAN(eps=eps, min_samples=samples)\n    result = dbscan.fit(df)\n    print(result.labels_)\n    labels = result.labels_ \n    print(np.unique(labels))\n    labelsdf = pd.DataFrame(labels)\n    vals = labelsdf.value_counts()\n    print(vals)\n    print(\"iteration complete\")#First clusters are created with user defined eps,minsamples\n    for i in vals.index:\n        print(\"cluster\",int(i[0]),\" counts\",vals[i])\n        count = vals[i]\n        cluster = int(i[0])\n        if cluster == -1:\n            if count>((labels.size/100)*max_amonaly_percentage):\n                #If anomaly size is greater than max allowed, then increase eps to reduce anomaly\n                eps = eps+0.2\n                #If some clusters strictly follwing minsamples as 2, then remove them by increasing samples \n                if vals.min() <= 2:\n                    samples = samples+1\n            \n                labels, eps, samples, vals = fit_dbscan(eps,samples,max_amonaly_percentage)\n                break\n    return labels, eps, samples, vals\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#eps = 1.0\n#samples = 2\n#max_amonaly_percentage = 5 #How much percent of anomaly should be allowed in clusters, reduces values in cluster -1\n#labels, eps, samples, vals = fit_dbscan(eps,samples,max_amonaly_percentage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#labelsdf2 = pd.DataFrame(labels)\n#print(\"Final EPS: \",eps,\" Final Min-samples: \",samples)\n#labelsdf2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_dbscan2(eps,samples,anomaly_count2):\n    dbscan = DBSCAN(eps=eps, min_samples=samples)\n    result = dbscan.fit(df)\n    labels = result.labels_ \n    print(np.unique(labels))\n    labelsdf = pd.DataFrame(labels)\n    vals = labelsdf.value_counts()\n    print(vals)\n    print(\"iteration complete\")#First clusters are created with user defined eps,minsamples\n    anomaly_count = vals[-1]\n    print(\"anomaly_count \",anomaly_count)\n    print(\"anomaly_count2 \",anomaly_count2)\n    if anomaly_count2 == 0:\n        eps=eps+0.05\n        labels, eps, samples, vals = fit_dbscan2(eps,samples,anomaly_count)\n    elif (anomaly_count2-anomaly_count)>1: #Compare anomaly counts of n and n-1 iteration\n        anomaly_count2=anomaly_count\n        eps=eps+0.05\n        labels, eps, samples, vals = fit_dbscan2(eps,samples,anomaly_count2)\n    else:\n        return labels, eps, samples, vals\n    return labels, eps, samples, vals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eps = 1.0\nsamples = 2\nanomaly_count2=0\nlabels, eps, samples, vals = fit_dbscan2(eps,samples,anomaly_count2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labelsdf2 = pd.DataFrame(labels)\nprint(\"Final EPS: \",eps,\" Final Min-samples: \",samples)\nlabelsdf2.value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}