{"cells":[{"metadata":{},"cell_type":"markdown","source":"The nltp library simplfy data cleaning process by outputing the cleaned version of text passed with just one line of code.\n\nTo insatll simple pip install nltp on your terminal. You can get more inform in the documentation\nfrom the project github link: https://github.com/izzyx6/nltp","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nltp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom nltp import Preprocessor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fraud email dataset used here is gotten from kaggle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/fraud-email-dataset/fraud_email_.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={'Text':'Emails'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VISUALIZING OUR TARGET VALUES\nsns.countplot(df['Class'])\nplt.title(\"Plot of Target Variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the nltp library simplfies the text cleaning process, as it cleans the emails passed to it by default(removes bad pattern and stop words, tokenization and lemmatization). We can also get the token form of the text passed to it by using .token().","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pre = Preprocessor(df['Emails']).text_cleaner()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = pre\nplt.figure(figsize = (15,15))\nword_cloud  = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n               collocations=False).generate(\" \".join(words))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have the cleaned text from nltp, we can select our features and and split our data into training and testing set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pre\ny = df['Class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\nprint(f'Spliting Completed')\nprint(f'X Train: {len(X_train)} X Test: {len(X_test)} y Train: {len(y_train)} y Test: {len(y_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since different models will be used, it would be proper to modularize our codes by breaking them into functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a function that can be used to train and test our models\ndef fit_predict(model):   \n    #CREATING A PIPELINE TO PROCESSING THE REVIEWS INTO O's AND 1's WITH Tf idf VECTORIZER\n    clf = Pipeline([('tfidf',TfidfVectorizer()),\n                   ('clf',model)])\n\n    #training model\n    clf.fit(X_train, y_train)\n    print(f'Fitting Model Completed.')\n    \n    #USING THE TEST DATA TO EVALUATED THE MODEL CREATED\n    Score = clf.score(X_test,y_test)\n    print(f'Accuracy: {Score*100}') \n    \n    return clf\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also a class called \"Metrics\" is created to group the various perfromance metrics used for evaluation of the models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Metrics():\n    \n    def __init__(self, pred):\n        self.pred = pred\n        \n    def cm(self):\n        cm = confusion_matrix(y_test, self.pred)\n        labels = ['Not Spam','Spam']\n\n        f, ax = plt.subplots(figsize=(5,5))\n        sns.heatmap(cm,annot =True, linewidth=.6, linecolor=\"r\", fmt=\".0f\", ax = ax)\n\n        ax.set_xticklabels(labels)\n        ax.set_yticklabels(labels)\n        plt.show()\n\n    def report(self):\n        class_report = classification_report(y_test, self.pred)\n        print(class_report)\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instantiating the model, and testing the fitted Logistic Regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_model = fit_predict(LogisticRegression())\n\nLR_pred = LR_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting the Confusion metric and classification report from the Metrics calss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Metrics(LR_pred).cm()\n\nMetrics(LR_pred).report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instantiating the Linear SVC model, and testing the fitted model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SVC_model = fit_predict(LinearSVC())\n\nSVC_pred = SVC_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting the Confusion metric and classification report from the Metrics calss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Metrics(SVC_pred).cm()\n\nMetrics(SVC_pred).report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instantiating the  Naive Bayes classifier, and testing the fitted model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_model = fit_predict(BernoulliNB())\n\nNB_pred = NB_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting the Confusion metric and classification report from the Metrics calss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nMetrics(NB_pred).cm()\n\nMetrics(NB_pred).report()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Conclusion</h3>\n\nThe Linear SVC out perfroms the other classifiers for this example","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\n\nfilename = 'model.joblib'\njoblib.dump(LR_model,open(filename,'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('model.joblib','rb') as f:\n    model = joblib.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}