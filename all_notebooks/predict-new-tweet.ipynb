{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a535bfcc-36ad-85ef-67f6-fccf7d853f21"},"source":"## Classify Tweets. Compare classifiers and select best model at run time. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8a91ae8-10d2-d457-8139-b7293c9ff88f"},"outputs":[],"source":"# Import File and Packages\nimport sklearn\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n%matplotlib inline\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Read csv file into data frame\ntweet=pd.read_csv(\"../input/Tweets.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"b1ce6ebd-2f09-500d-2db5-0ee0528c93e4"},"source":"### Load and preprocess data using NLTK:\n##### Tokenize, Clean, Stem, Lemmatize, Remove stopwords"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"270dd1e0-cf8f-df03-30e5-02a964080aff"},"outputs":[],"source":"# Preprocess the data {'negative': 0 , 'positive': 1 , 'neutral': 2}\n\ndf=tweet.iloc[:,(10,1)]\ndf.columns = ['data', 'target']\ndf['target']=df['target'].str.strip().str.lower()\ndf['target']=df['target'].map({'negative': 0 , 'positive': 1 , 'neutral': 2})\n\n# Copy df to a temporary dataframe for pre-processing\n# Below assignment is causing problems\ndft=df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44f3ff1e-afee-375c-96a8-590bc3243dc3"},"outputs":[],"source":"%%time\n# Remove @tweets, numbers, hyperlinks that do not start with letters\ndft['data']=dft['data'].str.replace(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|([0-9])\",\" \")\n\n# tokenize into words\nimport nltk\ndft['data']=dft['data'].apply(nltk.word_tokenize)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af5f8f23-4f00-daaa-eb92-0f4dd710db2c"},"outputs":[],"source":"%%time\n# stem the tokens\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\ndft['data']=dft['data'].apply(lambda x: [stemmer.stem(y) for y in x])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5914b0bb-a998-8d67-8026-8455567a80c5"},"outputs":[],"source":"%%time\n# Lemmatizing\nlemmatizer = nltk.WordNetLemmatizer()\ndft['data']=dft['data'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0e32ab2-3e9e-afca-219c-3c793aa7a6c3"},"outputs":[],"source":"%%time\n# Remove stopwords\nstopwords = nltk.corpus.stopwords.words('english')\n\n# stem the stopwords\nstemmed_stops = [stemmer.stem(t) for t in stopwords]\n\n# remove stopwords from stemmed/lemmatized tokens\ndft['data']=dft['data'].apply(lambda x: [stemmer.stem(y) for y in x if y not in stemmed_stops])\n\n# remove words whose length is <3\ndft['data']=dft['data'].apply(lambda x: [e for e in x if len(e) >= 3])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9762683e-6edd-c829-41c3-d8de80782d61"},"outputs":[],"source":"%%time\n# Detokenize cleaned dataframe for vectorizing\ndft['data']=dft['data'].str.join(\" \")"},{"cell_type":"markdown","metadata":{"_cell_guid":"57f7e08e-e268-d780-14c1-6d30c9bee3c7"},"source":"### Printing data shape:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05e9ccb1-4b09-ddf2-0513-29657ae2a186"},"outputs":[],"source":"#Print attributes of tweet, X and y\nprint('Shape of original file : ', tweet.shape)\nprint('All columns of the original file : ', tweet.columns.tolist() , '\\n')\nprint('Columns dft dataframe : ',dft.columns.tolist(), '\\n') \nprint('Shape data and target : ', dft['data'].shape, dft['target'].shape, '\\n')\nprint('Mood Count target :\\n',tweet['airline_sentiment'].value_counts())"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e918bdf-b63e-3d1e-c562-28b4942fe3a7"},"source":"### Printing accuracy using DummyClassifier (baseline) w.r.t Train Test Split:\n#### Performance Measure: Accuracy_score and runtime  \n##### Testing the dataset using Dummy classifier and train test. Purpose is to find an optimum random state for train test which gives the best accuracy."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b030f96b-5680-a094-7d44-1a89df47fad0"},"outputs":[],"source":"%%time\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nX=dft['data']\ny=dft['target']\n\narr_Accu=[]\n\n#Using train_test_split\n#Selecting the best random state and comapring the accuracy using Dummy Classifier\nfor i in range(1,20):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, random_state=i)\n\n    vect = CountVectorizer(stop_words='english',analyzer=\"word\",min_df = 2, max_df = 0.8)\n    X_train_dtm = vect.fit_transform(X_train)\n    X_test_dtm = vect.transform(X_test)\n    feat_dtm = vect.get_feature_names()\n    #feat_dtm\n\n    clf = DummyClassifier()\n    clf.fit(X_train_dtm, y_train)\n    y_pred = clf.predict(X_test_dtm)\n\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    #print(accuracy)\n    arr_Accu.append(accuracy)\n\nfor j in range(1,20):\n    print(\"Random State : \", j, \"   Accuracy : \", arr_Accu[j-1])"},{"cell_type":"markdown","metadata":{"_cell_guid":"73f1d920-7941-b659-af96-792dcf0811fa"},"source":"### Printing accuracy using Dummy Classifier w.r.t K-fold:\n#### Performance Measure: Accuracy_score and runtime\n##### Testing the dataset using dummy classifier and K-fold. Purpose is to find an optimum K value for CV which gives the best accuracy."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4547d1dc-5d4d-4bb1-066e-38b3bf5a390c"},"outputs":[],"source":"%%time\n#Using K-fold validation\n#Selecting the best fold and comparing the accuracy using Naive Bayes\nfrom sklearn.cross_validation import cross_val_score\narr_Accu=[]\n\n#Selecting the best random state and comparing the accuracy using dummy classifier\nfor i in range(3,15):\n\n    vect = CountVectorizer(stop_words='english',analyzer=\"word\",min_df = 2, max_df = 0.8)\n    X_dtm = vect.fit_transform(X)\n\n    clf = DummyClassifier()\n    accuracy = cross_val_score(clf, X_dtm, y, cv=i, scoring='accuracy')\n    \n    arr_Accu.append(np.mean(accuracy))\n\n#print(arr_Accu)\nfor j in range(3,15):\n    print(\"K-Fold : \", j, \"   Accuracy : \", arr_Accu[j-3])"},{"cell_type":"markdown","metadata":{"_cell_guid":"76ad89c6-fc1a-c772-3086-cfe9d01b0cdd"},"source":"### Print top features with frequency:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"139c60db-7a81-efcf-d440-80f596ea0354"},"outputs":[],"source":"def print_top_words():    \n    # Print top words\n    vect = CountVectorizer(stop_words='english',analyzer=\"word\",min_df = 2, max_df = 0.8)\n    data_dtm = vect.fit_transform(dft['data'])\n    feat_dtm = vect.get_feature_names()\n\n    # Count words\n    freq_tbl=pd.DataFrame({'Word':feat_dtm,'Occurence':np.asarray(data_dtm.sum(axis=0)).ravel().tolist()})\n    freq_tbl['Word']=freq_tbl['Word'].str.strip()\n\n    # Print top words\n    topt = freq_tbl.sort(['Occurence'], ascending=[False]).head(10)\n    y = topt['Occurence']\n    plt.grid()\n    X = range(1, 11)\n    plt.bar(X,y,color='g')\n    plt.xlabel('Top words')\n    plt.ylabel('Occurence')\n    plt.title('Frequency of top 10 words')\n    plt.xticks(X,topt['Word'],rotation=90)\n    \ndef print_top_neg_words():    \n    # Print top negative words\n    vect = CountVectorizer(stop_words='english',analyzer=\"word\",min_df = 2, max_df = 0.8)\n    filt = dft[dft['target'] == 0]\n    data_dtm = vect.fit_transform(filt['data'])\n    feat_dtm = vect.get_feature_names()\n\n    # Count words\n    freq_tbl=pd.DataFrame({'Word':feat_dtm,'Occurence':np.asarray(data_dtm.sum(axis=0)).ravel().tolist()})\n    freq_tbl['Word']=freq_tbl['Word'].str.strip()\n\n    # Print top negative words\n    topt = freq_tbl.sort(['Occurence'], ascending=[False]).head(10)\n    y = topt['Occurence']\n    plt.grid()\n    X = range(1, 11)\n    plt.bar(X,y,color='g')\n    plt.xlabel('Top negative words')\n    plt.ylabel('Occurence')\n    plt.title('Frequency of top 10 negative words')\n    plt.xticks(X,topt['Word'],rotation=90)\n    \ndef print_top_pos_words():    \n    # Print top positive words\n    vect = CountVectorizer(stop_words='english',analyzer=\"word\",min_df = 2, max_df = 0.8)\n    filt = dft[dft['target'] == 1]\n    data_dtm = vect.fit_transform(filt['data'])\n    feat_dtm = vect.get_feature_names()\n\n    # Count words\n    freq_tbl=pd.DataFrame({'Word':feat_dtm,'Occurence':np.asarray(data_dtm.sum(axis=0)).ravel().tolist()})\n    freq_tbl['Word']=freq_tbl['Word'].str.strip()\n\n    # Print top positive words\n    topt = freq_tbl.sort(['Occurence'], ascending=[False]).head(10)\n    y = topt['Occurence']\n    plt.grid()\n    X = range(1, 11)\n    plt.bar(X,y,color='g')\n    plt.xlabel('Top positive words')\n    plt.ylabel('Occurence')\n    plt.title('Frequency of top 10 positive words')\n    plt.xticks(X,topt['Word'],rotation=90)\n\n\nplt.figure(1,figsize=(16, 16))\nplt.subplot(251)\nprint_top_words()  \nplt.subplot(253)\nprint_top_pos_words()\nplt.subplot(255)\nprint_top_neg_words()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f4f7e7fe-4630-e8fc-1fa2-084c91c9df83"},"source":"### Comparing different model accuracy using Train Test:\n#### 1. Multinomial Naive Bayes\n#### 2. Logistic Regression\n#### 3. KNN\n#### 4. Decision Tree\n#### 5. Random Forest\n#### 6. ADA Boost Classifier\n#### 7. LinearSVC\n#### 8. Gaussian Naive Bayes\n### Performance Measure: Accuracy_score and Run time\n##### Each model mentioned above will be tested for accuracy and Run time using Train Test and the best performing model will be automatically selected."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"433ac165-a447-d1b0-7c7a-8663cb58cc3b"},"outputs":[],"source":"%%time\nimport time\n#Train Test split data with random state=11\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, random_state=11)\n\n#Vectorize\nvect = CountVectorizer(stop_words='english',analyzer=\"word\",min_df = 2, max_df = 0.8)\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\nfeat_dtm = vect.get_feature_names()\n\n#Initialize classifier stats\nclf_stats=pd.DataFrame()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d865a99-8e32-e6ac-87da-f6d0940a2eb1"},"outputs":[],"source":"# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\n\nstart_time=time.time()\nclf.fit(X_train_dtm, y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm)\naccuracy = metrics.accuracy_score(y_test, y_pred)\n\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'Multinomial Naive Bayes', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = MultinomialNB()'}, ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05ae6bf2-425a-a1cc-c17b-328ff848d38d"},"outputs":[],"source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\n\nstart_time=time.time()\nclf.fit(X_train_dtm, y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm)\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'Logistic Regression', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = LogisticRegression()'}, ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddacbabf-c202-b509-3e11-7a9efd66356d"},"outputs":[],"source":"# K Nearest Neighbour\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=2)\n\nstart_time=time.time()\nclf.fit(X_train_dtm, y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm)\naccuracy = metrics.accuracy_score(y_test, y_pred)\n\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'KNN', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = KNeighborsClassifier(n_neighbors=2)'}, ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bf73d6a-9ec5-71e1-7b05-fdfe36c45994"},"outputs":[],"source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(criterion='entropy')\n\nstart_time=time.time()\nclf.fit(X_train_dtm, y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm)\naccuracy = metrics.accuracy_score(y_test, y_pred)\n\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'Decision Trees', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = DecisionTreeClassifier(criterion=''entropy'')'}, ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f715e41-32a1-ac99-1f71-4fc358b4c02c"},"outputs":[],"source":"# RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(criterion='entropy')\n\nstart_time=time.time()\nclf.fit(X_train_dtm, y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm)\naccuracy = metrics.accuracy_score(y_test, y_pred)\n\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'Random Forest', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = RandomForestClassifier(criterion=''entropy'')'}, ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81be450d-5c0f-b05b-51af-b9d45669bbab"},"outputs":[],"source":"# AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier()\n\nstart_time=time.time()\nclf.fit(X_train_dtm, y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm)\naccuracy = metrics.accuracy_score(y_test, y_pred)\n\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'ADA Boost', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = AdaBoostClassifier()'}, ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81ea10cd-728b-7fe0-26cb-0010ea4ee4ca"},"outputs":[],"source":"# SVM SVC\nfrom sklearn.svm import LinearSVC\nclf = LinearSVC()\n\nstart_time=time.time()\nclf.fit(X_train_dtm, y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm)\naccuracy = metrics.accuracy_score(y_test, y_pred)\n\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'SVM SVC', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = LinearSVC()'}, ignore_index=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9040eaac-87a9-aca0-c78a-594ca0b15e9a"},"outputs":[],"source":"# GaussianNB\n# Not good with words\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\n\nstart_time=time.time()\nclf.fit(X_train_dtm.toarray(), y_train)\nruntime = time.time()-start_time\ny_pred = clf.predict(X_test_dtm.toarray())\naccuracy = metrics.accuracy_score(y_test, y_pred)\n\nprint('Accuracy : ',accuracy)\n\n#Store stats for classifier\nclf_stats=clf_stats.append({'Classifier': 'Gaussian NB', 'Accuracy': accuracy, 'Runtime': runtime, 'Callable': 'clf = GaussianNB()'}, ignore_index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0278a0ff-e136-1adc-aa72-89c18d834a84"},"source":"### Plotting Classifier vs Accuracy and Runtime:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ec1cf5f-93bf-2b3e-c13d-8f8802b1214e"},"outputs":[],"source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nprint (clf_stats[['Classifier','Accuracy','Runtime']].sort(['Accuracy'], ascending=[False]))\n#Plot performance measures of classifiers\n\nx=range(1,(len(clf_stats.Classifier)+1))\nplt.figure(1,figsize=(14, 5))\n\nplt.subplot(131)\nplt.xlabel('Classifier')\nplt.ylabel('Classifier Accuracy')\nplt.plot(x, clf_stats['Accuracy'],color='g')\nplt.xticks(x,clf_stats.Classifier,rotation=90)\n\nplt.subplot(133)\nplt.xlabel('Classifier')\nplt.ylabel('Classifier Runtime')\nplt.plot(x, clf_stats['Runtime'],color='g')\nplt.xticks(x,clf_stats.Classifier,rotation=90)    "},{"cell_type":"markdown","metadata":{"_cell_guid":"31ee8089-c49c-afe0-7b3e-422c5d04945a"},"source":"### Classifying new tweets: Fit model, Clean Tweet, Predict Mood\n#### Clean incoming new tweet:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f296bf5e-7475-d342-7afc-b8b930c7a544"},"outputs":[],"source":"# Clean input tweet\n\ndef fmt_input_tweet(txt):\n    \n    # Remove @tweets, numbers, hyperlinks that do not start with letters\n    txt = re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|([0-9])\",\" \",txt)\n    #print(txt)\n    \n    # tokenize into words\n    tokens = [word for word in nltk.word_tokenize(txt)]\n    #print(tokens)\n\n    # only keep tokens that start with a letter (using regular expressions)\n    clean_tokens = [token for token in tokens if re.search(r'^[a-zA-Z]+', token)]\n    #print('clean_tokens:\\n',clean_tokens)\n\n    # stem the tokens\n    stemmer = SnowballStemmer('english')\n    stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n    #print('stemmed_tokens:\\n',stemmed_tokens)\n\n    #Lemmatizing\n    lemmatizer = nltk.WordNetLemmatizer()\n    lem_tokens = [lemmatizer.lemmatize(t) for t in stemmed_tokens]\n    #print('lemmatizer : \\n',lem_tokens)\n    \n    #Remove stopwords\n    stopwords = nltk.corpus.stopwords.words('english')\n\n    # stem the stopwords\n    stemmed_stops = [stemmer.stem(t) for t in stopwords]\n\n    # remove stopwords from stemmed/lemmatized tokens\n    lem_tokens_no_stop = [stemmer.stem(t) for t in lem_tokens if t not in stemmed_stops]\n\n    # remove words whose length is <3\n    clean_lem_tok = [e for e in lem_tokens_no_stop if len(e) >= 3]\n    #print('clean_lem_tok: ',clean_lem_tok)\n    \n    # Detokenize new tweet for vector processing\n    new_formatted_tweet=\" \".join(clean_lem_tok)\n    #print('new_formatted_tweet: ',new_formatted_tweet)\n    \n    return new_formatted_tweet\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"577dd0bf-3f88-931b-4141-31f9bdc6d3ca"},"source":"### Classify incoming new tweet:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23e54a1d-c1f1-2621-1eaa-2e36ff7a0351"},"outputs":[],"source":"# Logistic Regression performs better. So it will automatically used as an appropriate classifier\n\n# Vectorize, fit, transform. Select model randomly\nvect = CountVectorizer(stop_words='english', analyzer=\"word\", min_df = 2, max_df = 0.8)\nX_dtm = vect.fit_transform(X)\nfeat_dtm = vect.get_feature_names()\n\n# Select the best performing classifier\nCall_clf = str(clf_stats[['Callable','Accuracy']].sort(['Accuracy'], ascending=[False]).head(1).iloc[:,(0)])\ntemp = Call_clf.__repr__()\nCall_clf = temp[temp.index('c'):(temp.index(')'))+1]\nprint('Model :',temp[(temp.index('=') + 1) : temp.index('(')])\nexec(Call_clf)\nclf.fit(X_dtm.toarray(), y) \n\ndef classify_new_tweet(new_twt):  \n\n    fmt_twt = fmt_input_tweet(new_twt)\n    fmt_twt_dtm = vect.transform([fmt_twt])[0]\n    #print('Formatted Tweet :',fmt_twt)\n    pred = clf.predict(fmt_twt_dtm.toarray())\n\n    def mood(x):\n        return {\n            0: 'negative',\n            1: 'positive',\n            2: 'neutral'\n        }[x]\n\n    print('Mood of the incoming tweet is:',mood(pred[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6cfc0d31-12b4-338e-5c45-19ffeb82575e"},"outputs":[],"source":"twt='@united I am sick!! https://www.abc.com'\nclassify_new_tweet(twt)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c74129de-b184-c4f5-dd4d-f0cd3323d15d"},"outputs":[],"source":"# Predict performance of random prediction. Testing for positive classes.\ny_random_pred = np.ones(y.shape[0])\naccuracy = metrics.accuracy_score(y, y_random_pred)\nprint('Accuracy of random (positive) prediction : ',accuracy)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f10a9648-f9a8-475a-ead1-2aeb24498fc5"},"outputs":[],"source":"# Predict performance of majority class prediction. Testing for negative classes.\ny_random_pred = np.zeros(y.shape[0])\naccuracy = metrics.accuracy_score(y, y_random_pred)\nprint('Accuracy of majority class (negative) prediction : ',accuracy)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}