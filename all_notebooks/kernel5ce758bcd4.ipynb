{"cells":[{"metadata":{"_uuid":"fc54635109ea2d844deb13b68c9cff07f0720f1b"},"cell_type":"markdown","source":"### Importing Libraries"},{"metadata":{"trusted":false,"_uuid":"c15cba55eb4bb3077e79ea5985ac181ff0ccbed3"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport itertools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport visuals as vs\nfrom mpl_toolkits.mplot3d import axes3d\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA \nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn import tree\n\nfrom xgboost import XGBClassifier\n\n% matplotlib inline\n\n# Set Random Seed\nnp.random.seed(42)\nnp.random.RandomState(42)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"051e922972388d3c3d04333eda48a5f79e59aae5"},"cell_type":"markdown","source":"### Importing Data"},{"metadata":{"trusted":false,"_uuid":"4164a35bd741214f8d259e0d06f6cf86066dba1c"},"cell_type":"code","source":"# Read csv\ndata = pd.read_csv(\"data.csv\")\ndata.head()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d81223f4335450e55c646502a38b9bbe72a3b475"},"cell_type":"markdown","source":"### Analyzing Data"},{"metadata":{"trusted":false,"_uuid":"59de4c443b13dad631c56eeb6d84bd93ec96d488"},"cell_type":"code","source":"print \"Number of data points :\", len(data)","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0340d60d13b96ae6a29f6e1ab861a3f3e8980e8c"},"cell_type":"code","source":"# Describe data\ndata.describe()","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"feb5bc9667eff2f263a261c49d3b73fd7ea72a4c"},"cell_type":"code","source":"# print columns\ndata.columns","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"be25a5d39999f964d02f506c234e430ff96ff872"},"cell_type":"code","source":"data.info()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"4b3047a45cf233f95d228787fc071ef3766f531d"},"cell_type":"markdown","source":"### Preparing Data"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8efbc075a685d6cc87e1dd4c5d75077b408ff570"},"cell_type":"code","source":"# Save labels in y\ny = data[\"diagnosis\"]","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"fd6def294fc04d2fcbc2e716371f4eb5259929f3"},"cell_type":"markdown","source":"### Selecting Features\n\nWe don't need pacient \"id\", \"diagnosis\" is our labels and \"Unnamed: 32\" have only NaNs. Let exclude this tree columns."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a08537280682d22f73dfae1e976eb54907f0d52c"},"cell_type":"code","source":"# Drop columns\nX = data.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"17470b38402e10e14c8c8ab709f237438b42c8c4"},"cell_type":"markdown","source":"### Looking for correlation between features"},{"metadata":{"trusted":false,"_uuid":"3b92f407570d83c8203bc1610739559ca67bd0c0"},"cell_type":"code","source":"# Plot a Correlation chart\ncorr = X.corr() # .corr is used for find corelation\n#plt.figure(figsize=(20,15))\nsns.set(rc={'figure.figsize':(25,20)})\n# plot a heatmap\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n           xticklabels= X.columns, yticklabels= X.columns,\n           cmap= 'coolwarm') ","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"317384c1895d7f6fc2e1f462a3e277f861d4f7e4"},"cell_type":"markdown","source":"### **There is some stronge correlations: **\n\nFeatures like (something)_mean, (something)_se, (something)_worst, have a natural correlation because all these are generated using same data, for example: to generate radius_mean, radius_se and radius_worst, radius mesuraments is used. \n"},{"metadata":{"_uuid":"5383543ad2a333994e91285c498b61bb21114622"},"cell_type":"markdown","source":"### Positive Correlation:"},{"metadata":{"_uuid":"0e7e57cf894a22d21c1118ef02c61427c79db9e2"},"cell_type":"markdown","source":"Radius, Perimeter and Area have stronge positive correlation"},{"metadata":{"trusted":false,"_uuid":"0c2fc2390f6ff3a69f68da4eac628838c4a5a2a4"},"cell_type":"code","source":"# Plot correlation between 2 features and distribution\nsns.jointplot(X.loc[:,'radius_mean'], \n              X.loc[:,'area_mean'], \n              kind=\"scatter\")\n\nsns.jointplot(X.loc[:,'radius_mean'], \n              X.loc[:,'perimeter_mean'], \n              kind=\"regg\")\n\nsns.jointplot(X.loc[:,'area_mean'], \n              X.loc[:,'perimeter_mean'], \n              kind=\"scatter\")","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"81ecfee4b011ed20841df78be87cc3e6233f633a"},"cell_type":"markdown","source":"Radius have a strong positive correlation with Concave Points"},{"metadata":{"trusted":false,"_uuid":"bb843a630a0a799d10fd0a68298bc0b74c717a81"},"cell_type":"code","source":"# Plot correlation between 2 features and distribution\nsns.jointplot(X.loc[:,'radius_mean'], \n              X.loc[:,'concave points_mean'], \n              kind=\"regg\")","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"e20c00cb80898355a64226e6e16eca6510c295de"},"cell_type":"markdown","source":"Compacteness, Concavity and Concave Points have strong positive correlation"},{"metadata":{"trusted":false,"_uuid":"ade7478b2f0db7dbbfe4f6fcc870bf0b7df7d2ab"},"cell_type":"code","source":"# Plot correlation between 2 features and distribution\nsns.jointplot(X.loc[:,'compactness_mean'], \n              X.loc[:,'concavity_mean'], \n              kind=\"regg\")\n\nsns.jointplot(X.loc[:,'compactness_mean'], \n              X.loc[:,'concave points_mean'], \n              kind=\"regg\")\n\n\nsns.jointplot(X.loc[:,'concavity_mean'], \n              X.loc[:,'concave points_mean'], \n              kind=\"regg\")\n","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"add5105264e650d2554498087b57dad2c68be302"},"cell_type":"markdown","source":"### Negative Correlation"},{"metadata":{"_uuid":"900dd3c7f1fc224a3b0ecd800e260b9e25f9836a"},"cell_type":"markdown","source":"Fractal Dimention have some negative correlation with Radius, Perimeter and Area  "},{"metadata":{"trusted":false,"_uuid":"f480148550ec16782e0a24f3d0f3711191359eee"},"cell_type":"code","source":"# Plot correlation between 2 features and distribution\nsns.jointplot(X.loc[:,'fractal_dimension_mean'], \n              X.loc[:,'radius_mean'], \n              kind=\"regg\")\n\nsns.jointplot(X.loc[:,'fractal_dimension_mean'], \n              X.loc[:,'perimeter_mean'], \n              kind=\"regg\")\n\nsns.jointplot(X.loc[:,'fractal_dimension_mean'], \n              X.loc[:,'area_mean'], \n              kind=\"regg\")","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"eac0ea96c404db64b937a71836fb6920775f67a6"},"cell_type":"markdown","source":"### Verifing ig Dataset is Balanced"},{"metadata":{"trusted":false,"_uuid":"479bcab2f03b290a750f13aa4a8a8b6ab5f8bb5e"},"cell_type":"code","source":"# Plot a countplot\nsns.set(rc={'figure.figsize':(8,5)})\nsns.countplot(y) ","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"96356fa9e8c1972d36c731bdcbee54b99625a357"},"cell_type":"markdown","source":"Data ins't balenced, there is more case of benigns tumors that malignant. Later we'll use methods to balance data and analyze if results get better. "},{"metadata":{"trusted":false,"_uuid":"0c8f6d6d5bc16803c565a95d42b39df97e1be173"},"cell_type":"code","source":"# Print count\ncount = y.value_counts()\nprint 'Number of Benign : ',count[0] \nprint 'Number of Malignant : ',count[1] ","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"fb379a3b6ca87d102c10c725090162630641a06f"},"cell_type":"markdown","source":"### Feature Engineering\n\nCreating a Volume Mean Feature using radius_mean"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cc78f46ab05e79ed94cf45156dfd4c0d100abb24"},"cell_type":"code","source":"# Creating a empty list\nmean_volume = []\n# defining pi\npi = 3.1415\n\n# calculatin mean volume for each mean radius and saving result in mean_volume list\nfor i in range(len(X)):\n    #aving result in mean_volume list\n    mean_volume.append((math.pow(X[\"radius_mean\"][i], 3)*4*pi)/3)\n\n# Creating a new feature\nX[\"mean_volume\"]= mean_volume    ","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"29a8de4a07677c95ad5b6e0eb269c8c032705956"},"cell_type":"markdown","source":"Creating a simple new feature, measuraments_sum_mean just adding feature relatade with cell size"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"637686d8e71a7e55cc0fa4d9aff6e78b7ee317ba"},"cell_type":"code","source":"# Creating a new feature adding up some phisical measuraments\nX[\"mesuraments_sum_mean\"] = X[\"radius_mean\"] + X[\"perimeter_mean\"] + X[\"area_mean\"]","execution_count":17,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3223c206054ba85c7d12e74f2ba3fd910b098ce7"},"cell_type":"code","source":"X.head()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"83154789f68123859fef92d3861c95cb41f60162"},"cell_type":"markdown","source":"### Feature Scaling\n\nSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n\nAnother reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.[1]\n\n[Feature Scaling - Wikipedia](https://en.wikipedia.org/wiki/Feature_scaling)"},{"metadata":{"trusted":false,"_uuid":"fc46cd349afbe873b8b83695070a7c7685f865bf"},"cell_type":"code","source":"# Define a scaler function\ndef scaler(df):\n    \"\"\"The Function receive a Dataframe and return a Scaled Dataframe\"\"\"\n    scaler = preprocessing.MinMaxScaler()\n    scaled_df = scaler.fit_transform(df)\n    scaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n    \n    return scaled_df\n\n# testing scaler\nscaled_df = scaler(X)\n\nscaled_df.head()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"b178a41f830875a92ee0346ec58a52809de6c84b"},"cell_type":"markdown","source":"### Features Distribution"},{"metadata":{"trusted":false,"_uuid":"160a4b903db724b304e04a71367d3ec4e5d47278"},"cell_type":"code","source":"# Preparing data\ndata_plot = pd.concat([y,scaled_df],axis=1)\ndata_plot = pd.melt(data_plot,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n# Plot a violinplot\nsns.set(rc={'figure.figsize':(15,30)})\nsns.violinplot(x=\"value\", y=\"features\", hue=\"diagnosis\", data=data_plot,split=True, inner=\"quart\")","execution_count":20,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"39d5ddf01875a38ac30861bb843a4f2e25b25a57"},"cell_type":"code","source":"# Ploting a pairplot Grid\nsns.set(style=\"white\")\ndf = scaled_df.iloc[:,0:9]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","execution_count":22,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cffa18c88dd303cd5feef2a09b7336118a43df44"},"cell_type":"code","source":"# Plot a Swarmplot\nsns.set(style=\"whitegrid\", palette=\"muted\")\ndata_plot = scaled_df\ndata_plot = pd.concat([y,data_plot.iloc[:,0:]],axis=1)\ndata_plot = pd.melt(data_plot,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n#plt.figure(figsize=(10,10))\nsns.set(rc={'figure.figsize':(15,30)})\nsns.swarmplot(x=\"value\", y=\"features\", hue=\"diagnosis\", data=data_plot)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"e3619888ab831a85bf1f29395393c08ade62c2f8"},"cell_type":"markdown","source":"### Detect Outliers using [Tukey Method](http://datapigtechnologies.com/blog/index.php/highlighting-outliers-in-your-data-with-the-tukey-method/)"},{"metadata":{"trusted":false,"_uuid":"b03ce31aa937ce76f30698d32ba935f6d6b61240"},"cell_type":"code","source":"# Define a function to detect outliers\ndef remove_outliers(X, y, f=2, distance=1.5):\n    \n    \"\"\"The Function receive Features (X) and Label (y) a frequency (f) and Inter-Quartile distance (distance),  \n    and return features and labels without outliers (good_X, good_y)\"\"\"\n    \n    outliers  = []\n\n    # For each feature find the data points with extreme high or low values\n    for feature in X.keys():\n\n        # Calculate Q1 (25th percentile of the data) for the given feature\n        Q1 = np.percentile(X[feature], 25)\n\n        # Calculate Q3 (75th percentile of the data) for the given feature\n        Q3 = np.percentile(X[feature], 75)\n\n        # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n        step = (Q3 - Q1) * distance\n\n        outliers.append(X[~((X[feature] >= Q1 - step) & (X[feature] <= Q3 + step))].index.values)\n\n    # Select the indices for data points you wish to remove\n    flat_list = [item for sublist in outliers for item in sublist]\n\n    # importing Counter\n    from collections import Counter\n    \n    freq = Counter(flat_list)\n    # Create a list to store outliers to remove\n    outliers_to_remove = []\n    \n    for key, value in freq.iteritems():\n        if value > f:\n            outliers_to_remove.append(key)\n\n    # Remove the outliers, if any were specified\n    good_X = X.drop(X.index[outliers_to_remove]).reset_index(drop = True)\n    good_y    = y.drop(y.index[outliers_to_remove]).reset_index(drop = True)\n    # Sort list\n    outliers_to_remove.sort()\n    # Print outliers founded\n    for i in range(len(outliers_to_remove)):\n        print \"data point: \", outliers_to_remove[i], \"is considered outlier to more than \", f, \" feature\"\n\n    print \"All \", len(outliers_to_remove), \"were removed!\"\n    # return data without outliers\n    return good_X, good_y \n\n\ngood_X, good_y = remove_outliers(scaled_df, y, f=2, distance=1.5)","execution_count":24,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dfa70d2de53f35ca54b143348632ac48f41422f0"},"cell_type":"code","source":"good_X.head()","execution_count":25,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fa04e09c24ed09164db1504c1556e14d8f3608eb"},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(8,5)})\n\nsns.countplot(good_y) ","execution_count":26,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"714e64a6863e39bbc8e2506daba28b5f10a5420a"},"cell_type":"code","source":"count = y.value_counts()\ncount2 = good_y.value_counts()\n\nprint 'Number of Benign removed: ',count[0] - count2[0] \nprint 'Number of Malignant removed: ',count[1] - count2[1] ","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"98d6a4e033e1245cd1042b58543c8f36121ded54"},"cell_type":"markdown","source":"Many malignant were considered outilier, this make data even more unbalanced. Later we'll understand if remove outlier improve results in this Dataset."},{"metadata":{"_uuid":"e81f9487060e6d1f96a7b567f92dac5371302269"},"cell_type":"markdown","source":"### PCA"},{"metadata":{"trusted":false,"_uuid":"e80bbd0a35b887a4d2c3f89870fa3d3b5014c2cd"},"cell_type":"code","source":"# TODO: Apply PCA by fitting the good data with only two dimensions\npca = PCA(n_components=2).fit(good_X)\n\n# TODO: Transform the good data using the PCA fit above\nreduced_data = pca.transform(good_X)\n\n# TODO: Transform log_samples using the PCA fit above\npca_samples = pca.transform(good_X)\n\n# Generate PCA results plot\npca_results = vs.pca_results(good_X, pca)\n\nprint \"Cumulative explained variance:\"\nprint pca_results['Explained Variance'].cumsum()","execution_count":28,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"51f4524965f864a937520bbb38241903f30a4d27"},"cell_type":"code","source":"pca_df = pd.DataFrame(pca_samples, columns=[\"d1\", \"d2\"])\n\ndata_plot = pd.concat([good_y,pca_df.iloc[:,0:]],axis=1)\n\nsns.lmplot(x=\"d1\", y=\"d2\", hue=\"diagnosis\", data=data_plot,  markers=[\"x\", \"o\"], fit_reg=False)","execution_count":30,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0169f8a13d27575cff0824ff1b67cf9bca480503"},"cell_type":"code","source":"# TODO: Apply PCA by fitting the good data with the same number of dimensions as features\npca = PCA(n_components=3)\npca.fit(good_X)\n\n# TODO: Transform log_samples using the PCA fit above\npca_samples = pca.transform(good_X)","execution_count":31,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"06d02767141d31bd25d42fedb70bb77d12a4f780"},"cell_type":"code","source":"pca_df = pd.DataFrame(pca_samples, columns=[\"d1\", \"d2\", \"d3\"])\n\ndata_plot = pd.concat([good_y,pca_df.iloc[:,0:]],axis=1)\ndata_plot.head()","execution_count":32,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"921c04c08ad1c78dfbfd742900a0e40ca55a9361"},"cell_type":"code","source":"# plot\nfig = plt.figure(figsize=(20,12))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(data_plot['d1'], data_plot['d2'], data_plot['d3'], s=100)\n#ax.view_init(30, 185)\nplt.show()","execution_count":33,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c22d3aa9622f4b4de4b6f17b667766a893fbfb2c"},"cell_type":"code","source":"def pca(X, n_components=3):\n    \n    \"\"\"The function receive features (X), and a target number of components (n_components),\n    and return a PCA transformed with n_components dimentions\"\"\"\n\n    pca = PCA(n_components)\n    pca.fit(X)\n\n    # TODO: Transform log_samples using the PCA fit above\n    pca_samples = pca.transform(X)\n\n    return pd.DataFrame(pca_samples)\n    \n\npca_df = pca(X, 3)\npca_df.head()","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"2639bfa698892f56c8c03d10261312e54c1d14f1"},"cell_type":"markdown","source":"Runing PCA experiments I realized that is necessary many dimensions (or PCA components) to explain data variance, so I decided do not use PCA. "},{"metadata":{"_uuid":"2bb1c1b1a205a79105bb129e80037eb5b35600df"},"cell_type":"markdown","source":"## [Imbalanced Learning](http://contrib.scikit-learn.org/imbalanced-learn/stable/)\n\n*** Naive random over-sampling***  \nOne way to fight this issue is to generate new samples in the classes which are under-represented. The most naive strategy is to generate new samples by randomly sampling with replacement the current available samples. The RandomOverSampler offers such scheme:"},{"metadata":{"trusted":false,"_uuid":"fd1bcb8f24e07184cf5376f66df54a1393bf18fe"},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nX_resampled, y_resampled = RandomOverSampler().fit_sample(X, y)\nfrom collections import Counter\nprint(sorted(Counter(y_resampled).items()))\n\nsns.set(rc={'figure.figsize':(8,5)})\n\nsns.countplot(y_resampled) ","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"0d304b882a92bed3026c67ddb88d74e463d86374"},"cell_type":"markdown","source":"** From random over-sampling to SMOTE and ADASYN**  \n\nApart from the random sampling with replacement, there is two popular methods to over-sample minority classes: (i) Synthetic Minority Oversampling Technique (SMOTE) and (ii) Adaptive Synthetic (ADASYN) sampling method. These algorithm can be used in the same manner:"},{"metadata":{"trusted":false,"_uuid":"27ab1c2562299088ac9d009f511628dc2cbaaacc"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE, ADASYN\nX_resampled, y_resampled = SMOTE().fit_sample(X, y)\nprint(sorted(Counter(y_resampled).items()))\n\nsns.set(rc={'figure.figsize':(8,5)})\n\nsns.countplot(y_resampled) ","execution_count":36,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"69d5fef47b6e607a1240846c49cf291e04846061"},"cell_type":"code","source":"X_resampled, y_resampled = ADASYN().fit_sample(X, y)\nprint(sorted(Counter(y_resampled).items()))\n\nsns.set(rc={'figure.figsize':(8,5)})\n\nsns.countplot(y_resampled) ","execution_count":37,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"594b4cb379969791204c93ea824686054d36ff26"},"cell_type":"code","source":"# Define a function to rebalance data\ndef resample(X,y, method=\"RandomOverSampler\"):\n    \n    \"\"\"The function receive features and labels (X, y) and a method to balance data\n    available methods RandomOverSampler, ADASYN, SMOTE\n    \n    The funcion returns X_resampled, y_resampled\"\"\"\n    \n    if method == \"RandomOverSampler\":\n        X_resampled, y_resampled = RandomOverSampler().fit_sample(X, y)\n    if method == \"ADASYN\":\n        X_resampled, y_resampled = ADASYN().fit_sample(X, y)\n    else:\n        X_resampled, y_resampled = SMOTE().fit_sample(X, y)\n        \n    print(sorted(Counter(y_resampled).items()))\n    \n    X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n        \n    return X_resampled, y_resampled\n\n\n# choose between RandomOverSampler, ADASYN, SMOTE\nX_resampled, y_resampled = resample(X,y, \"SMOTE\")\n\nsns.set(rc={'figure.figsize':(8,5)})\n\nsns.countplot(y_resampled) \n","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"019a911bcd22c0d709d78d0af4a7c89667a2ea2d"},"cell_type":"markdown","source":"### Feature Selection using [Scikit Learn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)\n\nFeature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. SelectKBest removes all but the k highest scoring features"},{"metadata":{"trusted":false,"_uuid":"8191fc0f9ebb75ca9bd660c36142a31a612047cb"},"cell_type":"code","source":"def selector(X, y, k=12):\n    \n    \"\"\"The function receive features and labels (X, y) and a target number to select features (k)\n    and return a new dataset wiht k best features\"\"\"\n    \n    selector = SelectKBest(chi2, k)\n    \n    X_new = selector.fit_transform(X, y)\n    \n    return pd.DataFrame(X_new, columns=X.columns[selector.get_support()])\n\nX_new = selector(X, y, 5)\n\nX_new.head()","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"59036ef164b828f8273a6ed433d7a002321f870b"},"cell_type":"markdown","source":"## Test, Tune and Compare Classifiers with different parameters and data settings"},{"metadata":{"trusted":false,"_uuid":"efd23fc3fd54319b3cceabe91121f75e4dd6bef0"},"cell_type":"code","source":"data = pd.read_csv(\"data.csv\")\n\ny = data[\"diagnosis\"]\nX = data.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)\n\nmean_volume = []\npi = 3.1415\n\nfor i in range(len(X)):\n    \n    mean_volume.append((math.pow(X[\"radius_mean\"][i], 3)*4*pi)/3)\n\nX[\"mean_volume\"]= mean_volume  \n\n\n\nX[\"mesuraments_sum_mean\"] = X[\"radius_mean\"] + X[\"perimeter_mean\"] + X[\"area_mean\"]\n\nX.shape","execution_count":101,"outputs":[]},{"metadata":{"_uuid":"a95ffb2326d69657df3060525de244cd0a21f3dd"},"cell_type":"markdown","source":"### Importing Classifiers Algorithms and set parameters\n\n** [Grid Search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)**\n\nExhaustive search over specified parameter values for an estimator. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"36f130b2f0f0f5ffc741c86110d414c62d3e98ec"},"cell_type":"code","source":"# Random Forest Classifier \nRF_clf = RandomForestClassifier()\n# Parameters to tune\nRF_par = {\"max_depth\": [3, None], \"max_features\": [1, 3, 10], \"min_samples_split\": [2, 3, 10], \n          \"min_samples_leaf\": [1, 3, 10], \"bootstrap\": [True, False], \"criterion\": [\"gini\", \"entropy\"]}\n\n# Extra Trees Classifier\nXT_clf = ExtraTreesClassifier()\n# Parameters to tune\nXT_par = { 'n_estimators': [5, 10, 16], \"min_samples_split\": [2, 3, 10], \"min_samples_leaf\": [1, 3, 10]}\n\n# Decision Tree Classifier\nDT_clf =DecisionTreeClassifier()\n# Parameters to tune\nDT_par = { 'splitter': ['best', ], \"min_samples_split\": [2, 3, 10], \"min_samples_leaf\": [1, 3, 10]}\n\n# Support Vector Machine Classifier\nSV_clf = svm.SVC()\n# Parameters to tune\nSV_par = {'kernel': ['rbf'], 'C': [1]}\n\n# AdaBoost Classifier\nAD_clf = AdaBoostClassifier()\n# Parameters to tune\nAD_par = {'n_estimators':[10, 20, 50, 60], 'learning_rate':[0.1, 0.5, 1.0, 1.5], 'algorithm':['SAMME.R', 'SAMME']}\n\n# Gradient Boosting Classifier\nGB_clf = GradientBoostingClassifier()\n# Parameters to tune\nGB_par = {'loss':['deviance', 'exponential'], 'learning_rate':[0.01, 0.1, 0.5, 1.0], 'n_estimators':[50, 100, 150], \n          \"min_samples_split\": [2, 3], \"min_samples_leaf\": [1, 3], 'max_depth':[2, 3, 5]}\n\n# SGD Classifier\nSG_clf = SGDClassifier()\n# Parameters to tune\nSG_par = {'loss':['hinge', 'log', 'squared_hinge', 'perceptron'], 'penalty':['l2', 'l1'], \n          'alpha':[0.00001, 0.0001, 0.001], 'epsilon':[0.01, 0.1, 0.5]}\n\n# Logistic Regression\nLR_clf = LogisticRegression()\n# Parameters to tune\nLR_par= {'penalty':['l1','l2'], 'C': [0.5, 1, 5, 10], 'max_iter':[50, 100, 150, 200]}\n\n# XGB Classifier\nXB_clf = XGBClassifier()\n# Parameters to tune\nXB_par = {'max_depth':[2, 3, 5], 'learning_rate':[0.01, 0.1, 0.5, 1], 'n_estimators':[50, 100, 150, 200], 'gamma':[0, 0.001, 0.01, 0.1]}\n\n\nclassifiers = [RF_clf, XT_clf, DT_clf, SV_clf, AD_clf, GB_clf, SG_clf, LR_clf, XB_clf]\n\nclassifiers_names = ['Random Forest      ', 'Extra DecisionTrees', 'Decision Tree      ',\n                     'Support Vector     ', 'AdaBoost Classifier', 'Gradient Boosting  ',\n                     'SGD Classifier     ', 'Logistic Regression', 'XGB Classifier     ']\n\nparameters = [RF_par, XT_par, DT_par, SV_par, AD_par, GB_par, SG_par, LR_par, XB_par]","execution_count":149,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ad5cc62dac590259053d3fafbe65f926b21cae2c"},"cell_type":"code","source":"def tune_compare_clf(X, y, classifiers, parameters, classifiers_names):\n    \n    '''The function receive Data (X, y), a classifiers list, \n    a list of parameters to tune each chassifier (each one is a dictionary), \n    and a list with classifiers name. \n    \n    The function split data in Train and Test data, \n    train and tune all algorithms and print results using F1 score.\n    \n    The function also returns a Dataframe with predictions, each row is a classifier prediction,\n    and X_test and y_test.\n    '''\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n    \n\n    print \"\\n\" \"Train size : \", X_train.shape, \" and Train labels : \", y_train.shape, \"\\n\"\n\n    print \"Test size: \", X_test.shape, \" and Test labels : \", y_test.shape, \"\\n\", \"\\n\"\n    \n    results = []\n    \n    print \"  ---- F1 Score  ----  \", \"\\n\"\n\n    for clf, par, name in itertools.izip(classifiers, parameters, classifiers_names):\n        # Store results in results list\n        clf_tuned = GridSearchCV(clf, par).fit(X_train, y_train)\n        y_pred = clf_tuned.predict(X_test)\n        results.append(y_pred)   \n\n        print name, \": %.2f%%\" % (f1_score(y_test, y_pred, average='weighted') * 100.0)\n\n    result = pd.DataFrame.from_records(results)   \n    \n    return result, X_test,  y_test\n    ","execution_count":193,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"61653ebb7b4e6393ba9f0ca545ee960fe80f5dbc"},"cell_type":"code","source":"result, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","execution_count":148,"outputs":[]},{"metadata":{"_uuid":"351206d411d3bde61aea2d3a634daa1c04c0da17"},"cell_type":"markdown","source":"#### ** Result Dataset **\n\nStore all classifiers predictions, each column is a data point and rows is prediction of each classifiers, we can use describe() function to undestand what is more common prediction for each data point, this way we're colecting the \"votes\" for each data point."},{"metadata":{"trusted":false,"_uuid":"cc32f79c440c3ed82cde88672b4d2792f02159b2"},"cell_type":"code","source":"y_pred_votes = result.describe().iloc[[2]]\ny_pred_votes","execution_count":153,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f3ed460eab2e4f34445c88e7effc44964ae865e6"},"cell_type":"code","source":"print(\"Accuracy: %.2f%%\" % (f1_score(y_test, y_pred_votes.T, average='weighted') * 100.0))\n\nsns.set(rc={'figure.figsize':(5,5)})\ncm = confusion_matrix(y_test,y_pred_votes.T)\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":154,"outputs":[]},{"metadata":{"_uuid":"23e1642d879eb1ce0a7169171ba1bd0eb260a48f"},"cell_type":"markdown","source":"### Testing Algorithms with Different Data Manipulation Techniques\n\nBefore in this project we define and test differents aproachs to use our original dataset, and create some functions:\n\n* scaler(X)\n\n* selector(X, y, k)\n\n* remove_outliers(X, y, f, distance)\n\n* resample(X, y, method)\n\nNow we'll test using: \n\n* tune_compare_clf(X, y, classifiers, parameters, classifiers_names) - a function that tune each algorith to given data and print F1 Scores. \n\nWith Technique or set of techniques is more effective for this dataset to minimize error when classifies Breast Cancer. \n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4bd1c138fc8f3cb7bd212bf0be3a8372479ce316"},"cell_type":"code","source":"y = data[\"diagnosis\"]\nX = data.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)\n\nmean_volume = []\npi = 3.1415\n\nfor i in range(len(X)):\n    \n    mean_volume.append((math.pow(X[\"radius_mean\"][i], 3)*4*pi)/3)\n\nX[\"mean_volume\"]= mean_volume  \nX[\"mesuraments_sum_mean\"] = X[\"radius_mean\"] + X[\"perimeter_mean\"] + X[\"area_mean\"]","execution_count":155,"outputs":[]},{"metadata":{"_uuid":"9c8525c64272d28c0ce2630e0cbe240fb3160c0f"},"cell_type":"markdown","source":"#### Data Original plus feature engineering [mean_volume and mesuraments_sum_mean]\n"},{"metadata":{"trusted":false,"_uuid":"2ce05d109c7931cd49f16b9121293374f6f0673b"},"cell_type":"code","source":"result, X_test, y_test = tune_compare_clf(X, y, classifiers, parameters, classifiers_names)","execution_count":158,"outputs":[]},{"metadata":{"_uuid":"1a9ae3d17f0aefb96d9b33c89df926ad9e62c878"},"cell_type":"markdown","source":"#### Scale"},{"metadata":{"trusted":false,"_uuid":"fde183c966900ff5ec6062b30e6da82377efc5aa"},"cell_type":"code","source":"\n\nX_scaled = scaler(X)\n\nresult, X_test, y_test = tune_compare_clf(X_scaled, y, classifiers, parameters, classifiers_names)","execution_count":160,"outputs":[]},{"metadata":{"_uuid":"b49022d5900a7d0bce953f4bb83a3eb9f62b8557"},"cell_type":"markdown","source":"#### Outiliers"},{"metadata":{"trusted":false,"_uuid":"972c3882a2ab208c93b4830801366a0d70e1a8df"},"cell_type":"code","source":"\n\nX_good, y_good = remove_outliers(X, y, f=2, distance=2)\n\nresult, X_test, y_test = tune_compare_clf(X_good, y_good, classifiers, parameters, classifiers_names)","execution_count":164,"outputs":[]},{"metadata":{"_uuid":"d22f846833b7c210f9d50b02c811e26407c5880a"},"cell_type":"markdown","source":"#### Feature Selection (12 features)"},{"metadata":{"trusted":false,"_uuid":"27a97aa4fd7837be05c4a430732f47413e6b52c1"},"cell_type":"code","source":"\n\nX_selected = selector(X, y, k=12)\n\nresult, X_test, y_test = tune_compare_clf(X_selected, y, classifiers, parameters, classifiers_names)","execution_count":166,"outputs":[]},{"metadata":{"_uuid":"8bb723dc4e4d65d3a65004672ee008eb89f0f5d9"},"cell_type":"markdown","source":"#### Feature Selection (10 Features)"},{"metadata":{"trusted":false,"_uuid":"98746ed8fdf80ee88f9bfcce88403b257cff1307"},"cell_type":"code","source":"\n\nX_selected = selector(X, y, 10)\n\nresult, X_test, y_test = tune_compare_clf(X_selected, y, classifiers, parameters, classifiers_names)","execution_count":169,"outputs":[]},{"metadata":{"_uuid":"4aaa95d8f0eaa3ff6761484fb9265b1fbafc2262"},"cell_type":"markdown","source":"#### Resample Method 1"},{"metadata":{"trusted":false,"_uuid":"ccb14f87d34e9a4d3972b03cc6cbb852f640231b"},"cell_type":"code","source":"\n\nX_new, y_new = resample(X, y, method=\"RandomOverSampler\")\n\nresult, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","execution_count":170,"outputs":[]},{"metadata":{"_uuid":"5137dd859acbf6260a679d6ca8e086547788628d"},"cell_type":"markdown","source":"#### Resample Method 2"},{"metadata":{"trusted":false,"_uuid":"1728cd3f753d41ead25640b909c3a3a5545595e4"},"cell_type":"code","source":"\n\nX_new, y_new = resample(X, y, method=\"SMOTE\")\n\nresult, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","execution_count":171,"outputs":[]},{"metadata":{"_uuid":"99090459925ffee16fed927b855bc837c99cc1f8"},"cell_type":"markdown","source":"#### Resample Method 3"},{"metadata":{"trusted":false,"_uuid":"a4d42aa2f7dd6abb80b9e6acd64f5360c063927d"},"cell_type":"code","source":"\n\nX_new, y_new = resample(X, y, method=\"ADASYN\")\n\nresult, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","execution_count":172,"outputs":[]},{"metadata":{"_uuid":"342c95c48c3d11849b8e7d8f942286bf1e3f761d"},"cell_type":"markdown","source":"#### Scale and Outiliers Remove"},{"metadata":{"trusted":false,"_uuid":"d3b3a855ebd07aafde0e9803b498440e52f043e3"},"cell_type":"code","source":"\n\nX_scaled = scaler(X)\nX_good, y_good = remove_outliers(X_scaled, y, f=2, distance=2)\n\nresult, X_test, y_test = tune_compare_clf(X_good, y_good, classifiers, parameters, classifiers_names)","execution_count":174,"outputs":[]},{"metadata":{"_uuid":"87fece661a63d58632375733b7522a74e5334fc6"},"cell_type":"markdown","source":"#### Scale and Resample"},{"metadata":{"trusted":false,"_uuid":"4cf7b95e8dedca23c7d9ca4171ff822a2d66ceed"},"cell_type":"code","source":"\n\nX_scaled = scaler(X)\nX_new, y_new = resample(X_scaled, y, method=\"RandomOverSampler\")\n\nresult, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","execution_count":175,"outputs":[]},{"metadata":{"_uuid":"241b1b0aa5106221c0d68553093a48750f327650"},"cell_type":"markdown","source":"#### Scale, Outliers Remove and Resample"},{"metadata":{"trusted":false,"_uuid":"514b6001467dd023307598e0debbba93459ed018"},"cell_type":"code","source":"\n    \nX_scaled = scaler(X)\nX_good, y_good = remove_outliers(X_scaled, y, f=2, distance=2)\nX_new, y_new = resample(X_good, y_good, method=\"RandomOverSampler\")\n\nresult, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","execution_count":185,"outputs":[]},{"metadata":{"_uuid":"4c25cacb4bd16575a2ea0d74f0d8ec49a39b1d47"},"cell_type":"markdown","source":"#### Feature Selection, Scale, Outlier Remove and Resample"},{"metadata":{"trusted":false,"_uuid":"d555cd4ff04ae0815723b69d22b727340a57eeb7"},"cell_type":"code","source":"\n\n\nX_selected = selector(X, y, 10)\nX_scaled = scaler(X_selected)\nX_good, y_good = remove_outliers(X_scaled, y, f=2, distance=2)\nX_new, y_new = resample(X_good, y_good, method=\"RandomOverSampler\")\n\n\nresult, X_test, y_test = tune_compare_clf(X, y, classifiers, parameters, classifiers_names)","execution_count":182,"outputs":[]},{"metadata":{"_uuid":"e2c84984fb2cd8a8b776fb79e6d067d6ca66665e"},"cell_type":"markdown","source":"### Results with techniques alone"},{"metadata":{"_uuid":"c92c8f4af93a9748dbcfdd480759e60922043d59"},"cell_type":"markdown","source":"|Classifier          |Original  | Scaled   | Outiliers Removed | 12 Features | 10 Features | Resampled Randon | SMOTE      |      ADASYN |\n|--------------------|----------|----------|-------------------|-------------|-------------|------------------|------------|-------------|\n|Random Forest       | 94.23%   |96.47%    |94.23%             |94.68%       |95.58%       |96.50%            |96.50%      |96.52%       |\n|Extra DecisionTrees | 96.14%   |95.60%    |96.14%             |96.47%       |**98.24%**   |**99.30%**        |97.90%      |95.83%       |\n|Decision Tree       | 93.22%   |94.74%    |93.22%             |95.58%       |94.74%       |97.20%            |93.70%      |94.44%       |\n|Support Vector      | 50.05%   |96.45%    |50.05%             |47.80%       |47.80%       |37.32%            |37.32%      |44.34%       |\n|AdaBoost Classifier | 96.12%   |96.49%    |96.12%             |95.60%       |94.71%       |97.90%            |96.50%      |95.83%       |\n|Gradient Boosting   | 94.21%   |94.74%    |94.21%             |92.95%       |95.60%       |96.50%            |96.50%      |96.52%       |\n|SGD Classifier      | 90.16%   |**97.36%**|90.16%             |76.39%       |24.34%       |73.29%            |61.44%      |74.61%       |\n|Logistic Regression |**98.06%**|**97.36%**|**98.06%**         |95.58%       |97.35%       |98.60%            |**98.60%**  |**97.92%**   |\n|XGB Classifier      | 96.16%   |96.47%    |96.16%             |**96.49%**   |94.74%       |95.80%            |95.80%      |95.83%       |\n\n\n\n"},{"metadata":{"_uuid":"5b7714a7ea540e6dfa831fac4a9ce21771f15797"},"cell_type":"markdown","source":"### Results with set of techniques"},{"metadata":{"_uuid":"5282f504cac76ef707bccfcce67149b5ba28f88c"},"cell_type":"markdown","source":"\n\n\n|Classifier          | Scaled + Outiliers Removed | Scaled + Resampled | Scaled + Outiliers Removed+Resampled | Scaled + Feature + Out. Rem + Resampled |\n|--------------------|----------------------------|--------------------|--------------------------------------|-----------------------------------------|\n|Random Forest       |96.14%                      |**97.90%**          |97.83%                                |96.47%                                   |\n|Extra DecisionTrees |94.21%                      |96.50%              |97.10%                                |96.47%                                   |\n|Decision Tree       |91.24%                      |97.20%              |94.20%                                |94.74%                                   |\n|Support Vector      |**98.05%**                  |95.81%              |97.83%                                |47.80%                                   |\n|AdaBoost Classifier |96.14%                      |95.11%              |97.10%                                |**96.49%**                               |\n|Gradient Boosting   |95.20%                      |96.50%              |**99.28%**                            |95.60%                                   |\n|SGD Classifier      |**98.05%**                  |93.68%              |97.83%                                |85.07%                                   |\n|Logistic Regression |97.08%                      |95.80%              |97.83%                                |96.47%                                   |\n|XGB Classifier      |96.16%                      |97.20%              |98.55%                                |96.47%                                   |\n\n"},{"metadata":{"_uuid":"048793c6480f23851e79218ec8dd2ba4563152e6"},"cell_type":"markdown","source":"The highest and more consistents results across all classifiers was achieved using Scale, Removing Outiliers and Balancing data. The higher score was reached by Gradient Boosting Algorithm (F1 Score = 99.28) and lower was Decision Tree (F1 Score 94,20) which even been the lower is a very descent result. "},{"metadata":{"_uuid":"e299cfa86c4d927be93eee675e7c264c16314875"},"cell_type":"markdown","source":"### Plotting a [Confusion Matrix](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) with best results\n\n"},{"metadata":{"trusted":false,"_uuid":"258934ec8aa92fcd71a8c5ce9d501e6971bad712"},"cell_type":"code","source":"# Scale, Outliers Remove and Resample\n    \nX_scaled = scaler(X)\nX_good, y_good = remove_outliers(X_scaled, y, f=2, distance=2)\nX_new, y_new = resample(X_good, y_good, method=\"RandomOverSampler\")\n\nresult, X_test, y_test = tune_compare_clf(X_new, y_new, classifiers, parameters, classifiers_names)","execution_count":186,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4157a6009a842fff891cf371eff85100319e35ca"},"cell_type":"code","source":"y_pred_votes = result.describe().iloc[[2]]","execution_count":190,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b2386726abf409e3c67e8578e7dc61904e98c672"},"cell_type":"code","source":"print(\"Accuracy: %.2f%%\" % (f1_score(y_test, y_pred_votes.T, average='weighted') * 100.0))\n\nsns.set(rc={'figure.figsize':(5,5)})\ncm = confusion_matrix(y_test,y_pred_votes.T)\nsns.heatmap(cm,annot=True,fmt=\"d\")","execution_count":191,"outputs":[]},{"metadata":{"_uuid":"c538a2b0d3a460afb5d629529b71f9f8861f20c2"},"cell_type":"markdown","source":"# Results\n\n![F1 Score](conf_mat.png)\n\n- 71 data points was correctly predicted as Benign tumors  \n- 65 data points was correctly predicted as Malignant tumors  \n- 01 data point was wrongly predicted as Malignan tumor  \n- 01 data point was wrongly predictes as Benign tumor  \n\nUsing [F1 Score](https://en.wikipedia.org/wiki/F1_score) formula:\n\n![F1 Score](f12.jpg)\n\nIn statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall), where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n\n#### F1 Score = 0.9855\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ea50c23c9cb53bbb42c308122105054c5895c4ac"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}},"nbformat":4,"nbformat_minor":1}