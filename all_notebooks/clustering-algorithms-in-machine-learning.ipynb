{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv(\"../input/customer-segmentation/Mall_Customers.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(\"CustomerID\",axis=1,inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13,10))\nsns.heatmap(df.corr(),annot=True, cmap=\"viridis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.get_dummies(data=df, columns=[\"Genre\"],drop_first=True)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= df.values\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. KMeans Clustering","metadata":{}},{"cell_type":"markdown","source":"<font color=\"blue\">\n1.1. How KMeans Clustering Works:","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nurl=\"https://i.stack.imgur.com/FQhxk.jpg\"\nImage(url,width=800, height=800)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"blue\">\nIn the step 1 in the algorithm, each observation is randomly assigned to a cluster.\n\nIn the step 2a in the algorithm,the cluster centroid for each cluster is computed, which are shown as large colored disk as shown top-right of the figure.\n\nInitially these centroids are almost overlapping as we can see from the figure because initial cluster assignments are chosen randomly.\n\nIn the step 2a in the algorithm(bottom-left of the figure above), each observation is assigned to the nearest centroid.\n\nIn bottom-center of the figure above, step 2a once again is performed which lead to new cluster centroids.\n\nWe basically keep repeating these steps until there is no new cluster which means data points are being reassigned to a new cluster centroid.\n\nAt the bottom-right, we have the results obtained after about 10 iterations","metadata":{}},{"cell_type":"markdown","source":"<font color=\"blue\">\n1.2. Implementation of the Algorithm","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss=list()\nfor i in range(1,20):\n    kmeans=KMeans(n_clusters= i, init=\"k-means++\")\n    kmeans.fit(X)\n    loss.append(kmeans.inertia_)\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(12,10))\nplt.plot(range(1,20), loss)\nplt.title(\"Elbow Method\")\nplt.xlabel(\"Number of Cluster\")  \nplt.ylabel(\"loss\")\nplt.show()\n#As we can see, we can have best cluster value when number of cluster is equal to 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(kmeans.inertia_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans=KMeans(n_clusters=5, init=\"k-means++\")\nmy_clusters=kmeans.fit_predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_df=pd.DataFrame(my_clusters,columns=[\"KMeans Clusters\"])\ncluster_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df=pd.concat([df, cluster_df], axis=1)\nnew_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"blue\">\nWe can easily check the centroid values by averaging the features in each cluster as follows","metadata":{}},{"cell_type":"code","source":"new_df.groupby(\"KMeans Clusters\").mean()\n#It is apparnt that Annual Income and Spending Score plays important role in the number of clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans.cluster_centers_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.scatterplot(x=new_df[\"Annual Income (k$)\"],y= new_df[\"Spending Score (1-100)\"],hue=new_df[\"KMeans Clusters\"],palette=\"magma\")\n#form this plot we can say that, if the mall make ads, it has higher chance to sell its product to the customer in cluster 0 and 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.scatterplot(x=new_df[\"Genre_Male\"],y= new_df[\"Spending Score (1-100)\"],hue=new_df[\"KMeans Clusters\"],palette=\"viridis\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.scatterplot(x=new_df[\"Age\"],y= new_df[\"Spending Score (1-100)\"],hue=new_df[\"KMeans Clusters\"],palette=\"viridis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D \nfig = plt.figure(1, figsize=(15, 10))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\nplt.cla()\nax.set_xlabel('Spending Score (1-100)')\nax.set_ylabel('Age')\nax.set_zlabel('Annual Income (k$) ')\nax.scatter(X[:, 2], X[:, 0], X[:, 1], c= my_clusters.astype(np.float))\n#Here we can see better the combination of three feature with different clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Hierarchical Clustering","metadata":{}},{"cell_type":"markdown","source":"<font color=\"blue\">\nHierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters.\n\nStrategies for hierarchical clustering generally fall into two types:[1]\n\n1.Agglomerative: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n\n2.Divisive: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n\nIn general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering[2] are usually presented in a dendrogram.","metadata":{}},{"cell_type":"markdown","source":"<font color=\"blue\">\n2.2. Using Dendogram in order to Find the Optimal Number of Clusters","metadata":{}},{"cell_type":"code","source":"from scipy.cluster import hierarchy \nhier=hierarchy.dendrogram(hierarchy.linkage(X, method=\"ward\"))\nplt.title(\"Dendogram of Hierarchical Clustering\")\nplt.xlabel(\"Observation Points\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show() # it seems to have 3 or 5 clusters are better option","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"blue\">\n2.2. Using Agglomerative Clustering","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nac=AgglomerativeClustering(n_clusters=5, affinity=\"euclidean\",linkage=\"ward\")\nagglomerative_clusters= ac.fit_predict(X)\nagglomerative_clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3= pd.DataFrame(agglomerative_clusters, columns=[\"Agglomerative Clusters\"])\ndf3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df=pd.concat([new_df,df3],axis=1)\nnew_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Comparison of Both Algorithms","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\nfig.suptitle('Distribution of Cluster')\n\n# KMeans Clustering\nsns.scatterplot(ax=axes[0], x=new_df[\"Annual Income (k$)\"], y=new_df[\"Spending Score (1-100)\"],hue=new_df[\"KMeans Clusters\"],palette=\"viridis\")\naxes[0].set_title(\"According to KMeans Clusters\")\n#Agglomerative Clustering\nsns.scatterplot(ax=axes[1], x=new_df[\"Annual Income (k$)\"], y=new_df[\"Spending Score (1-100)\"],hue=new_df[\"Agglomerative Clusters\"],palette=\"viridis\")\naxes[0].set_title(\"According to Agglomerative Clusters\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"blue\">\nAbove we test the accuracy of both clusters with each other. It seems both of them creates approximately same clusters regardless of the value difference in the entire dataset","metadata":{}}]}