{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color=\"red\">\nContent:\n    \n1. [Attribute Information](#22)\n2. [Read the data](#1)\n3. [Exploratory Data Analysis](#2)\n    * [Heatmap](#23)\n    * [Correlation with target](#3)\n    * [Box Plot](#4)\n    * [Pair Plot](#5)\n4. [Outlier Detection](#6)\n    * [Local Outlier Factor](#7)\n    * [Drop Outliers](#8)\n5. [Train - Test Split](#9)   \n6. [Standardization](#10)\n7. [Modeling](#11)   \n    * [KNN](#12)\n      * [Best KNN Parameters](#13)\n      * [KNN Tuning](#14)\n8. [PCA](#15)    \n9. [NCA](#16)\n10. [Conclusion](#17)    "},{"metadata":{},"cell_type":"markdown","source":"<a id = \"22\"></a><br>\n# Attribute Information"},{"metadata":{},"cell_type":"markdown","source":"1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 / area - 1.0)\n\ng) concavity (severity of concave portions of the contour)\n\nh) concave points (number of concave portions of the contour)\n\ni) symmetry\n\nj) fractal dimension (\"coastline approximation\" - 1)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"1\"></a><br>\n# Read and check the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We dont need id and Unnamed:32 columns.\ndf.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\n\n# Change diagnosis to target.\ndf.rename(columns={\"diagnosis\":\"target\"},inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a quick look to target feature.\ndf.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df.target);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Binarize m and b\ndf.target.replace({\"M\":1,\"B\":0},inplace=True)\ndf.target.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no null data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"2\"></a><br>\n# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"23\"></a><br>\n## Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation\nplt.figure(figsize=(15,6))\nsns.heatmap(df.corr(),annot=True,linewidths=0.5,fmt=\".2f\",cmap=\"YlOrRd\")\nplt.title(\"Correlation Matrix\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are some correlated features, we can focus them in feature engineering section."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"3\"></a><br>\n## Correlation with target"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8), title=\"Correlation with target\",color=\"salmon\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Nearly all features correlated with target except fractral_dimension_mean, texture_se and symmetry_se"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation > 0.6\ndfcorr = df[[\"target\",\"radius_mean\",\"perimeter_mean\",\"area_mean\",\"concavity_mean\",\"concave points_mean\",\n             \"radius_worst\",\"perimeter_worst\",\"area_worst\",\"concavity_worst\",\"concave points_worst\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"4\"></a><br>\n## Box Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We should melt the data to visualize with box plot\ndf_melted = pd.melt(df,id_vars=\"target\",var_name=\"features\",value_name=\"value\")\nplt.figure(figsize=(10,6))\nsns.boxplot(x=\"features\",y=\"value\",hue=\"target\",data=df_melted)\nplt.xticks(rotation=90)\nplt.title(\"Box plot\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Box plot tell us nothing, because we should standardize the data first. We will do it later."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"5\"></a><br>\n## Pair Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(dfcorr,diag_kind=\"kde\",markers=\"+\",hue=\"target\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* According to histogram plots we have positive skewness problems.\n* We can use log(1-x) transformation for the positive skewness. (I will not do any transformation in this kernel))"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"6\"></a><br>\n# Outlier Detection"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"7\"></a><br>\n## Local Outlier Factor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data to X and y before LOF\ny=df[\"target\"]\nX=df.drop([\"target\"],axis=1)\ncolumns= df.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lof= LocalOutlierFactor()\ny_pred=lof.fit_predict(X)\ny_pred[0:10]\n#  1 = inlier\n# -1 = outlier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_score= lof.negative_outlier_factor_\noutlier_score= pd.DataFrame()\noutlier_score[\"score\"]=x_score\n\nlofthreshold= -2.5\nloffilter= outlier_score[\"score\"]< lofthreshold\noutlier_index= outlier_score[loffilter].index.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.scatter(X.iloc[outlier_index,0],X.iloc[outlier_index,4],color=\"darkblue\",s=50,label=\"outliers\")\nplt.scatter(X.iloc[:,0],X.iloc[:,4],color=\"k\",s=3,label=\"Data Points\")\n\nradius=(x_score.max()- x_score)/(x_score.max()-x_score.min())\noutlier_score[\"radius\"]=radius\nplt.scatter(X.iloc[:,0],X.iloc[:,4],s=1000*radius,edgecolors=\"r\",facecolors=\"none\",label=\"outlier scores\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We detected outliers, lets drop them."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"8\"></a><br>\n## Drop Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"X= X.drop(outlier_index)\ny= y.drop(outlier_index).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"9\"></a><br>\n# Train - Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"10\"></a><br>\n# Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dont fit the scaler while standardizate X_test !\nscaler = StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(columns[0])\nX_train_df = pd.DataFrame(X_train,columns=columns)\nX_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets take a look to box plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_df[\"target\"]=y_train\ndf_melted = pd.melt(X_train_df,id_vars=\"target\",var_name=\"features\",value_name=\"value\")\nplt.figure(figsize=(20,6))\nsns.boxplot(x=\"features\",y=\"value\",hue=\"target\",data=df_melted)\nplt.xticks(rotation=90)\nplt.title(\"Box plot\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"11\"></a><br>\n# Modeling"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"12\"></a><br>\n## KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn= KNeighborsClassifier(n_neighbors=2).fit(X_train,y_train)\ny_pred= knn.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nacc = accuracy_score(y_test,y_pred)\n\n\nprint(\"Knn Confusion Matrix:\\n\",cm)\nprint(\"Basic acc. score:\",acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"13\"></a><br>\n# Best KNN Parameters"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"14\"></a><br>\n## KNN Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def knn_best_params(X_train,X_test,y_train,y_test):\n    k_range=list(range(1,31))\n    weight_options=[\"uniform\",\"distance\"]\n    print()\n    param_grid=dict(n_neighbors=k_range,weights=weight_options)\n    \n    knn= KNeighborsClassifier()\n    grid=GridSearchCV(knn,param_grid,cv=10,scoring=\"accuracy\")\n    grid.fit(X_train,y_train)\n    print(\"Best acc. score: {}\\n Best parameters {}\".format(grid.best_score_,grid.best_params_))\n    print()\n    \n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(X_train,y_train)\n    y_pred_test= knn.predict(X_test)\n    y_pred_train=knn.predict(X_train)\n    cm_test=confusion_matrix(y_test,y_pred_test)\n    cm_train=confusion_matrix(y_train,y_pred_train)\n    \n    acc_test= accuracy_score(y_test, y_pred_test)\n    acc_train= accuracy_score(y_train, y_pred_train)\n    print(\"Test score {}\\n Train score {}\".format(acc_test,acc_train))\n    \n    print(\"CM Test\\n\",cm_test)\n    print(\"CM Train\\n\",cm_train)\n    return grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid= knn_best_params(X_train,X_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"15\"></a><br>\n# PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nx_scaled= scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca= PCA(n_components=2)\npca.fit(x_scaled)\nx_reduced_pca=pca.transform(x_scaled)\npcadata= pd.DataFrame(x_reduced_pca,columns=[\"p1\",\"p2\"])\npcadata[\"target\"]= y\nsns.scatterplot(x=\"p1\",y=\"p2\",hue=\"target\",data=pcadata)\nplt.title(\"p1 vs p2\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pca,X_test_pca,y_train_pca,y_test_pca=train_test_split(x_reduced_pca,y,test_size=0.3,random_state=42)\n\ngrid_pca = knn_best_params(X_train_pca,X_test_pca,y_train_pca,y_test_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_light = ListedColormap(['salmon',  'violet'])\ncmap_bold = ListedColormap(['darksalmon', 'purple'])\n\nh = .05 # step size in the mesh\nX = x_reduced_pca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(16,8))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"16\"></a><br>\n# NCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"nca=NeighborhoodComponentsAnalysis(n_components=2,random_state=42)\nnca.fit(x_scaled,y)\n\nx_reduced_nca = nca.transform(x_scaled)\nnca_data=pd.DataFrame(x_reduced_nca,columns=[\"p1\",\"p2\"])\nnca_data[\"target\"]=y\n\nplt.figure(figsize=(12,5))\nsns.scatterplot(x=\"p1\",y=\"p2\",hue=\"target\",data=nca_data)\nplt.title(\"p1 vs p2\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_nca,X_test_nca,y_train_nca,y_test_nca=train_test_split(x_reduced_nca,y,test_size=0.3,random_state=42)\n\ngrid_nca = knn_best_params(X_train_nca,X_test_nca,y_train_nca,y_test_nca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cmap_light = ListedColormap(['salmon',  'violet'])\ncmap_bold = ListedColormap(['darksalmon', 'purple'])\n\nh = .2 # step size in the mesh\nX = x_reduced_nca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(16,8))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_nca.best_estimator_.n_neighbors, grid_nca.best_estimator_.weights));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"17\"></a><br>\n# Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's find the wrong classifications we made.\nknn = KNeighborsClassifier(**grid_nca.best_params_)\nknn.fit(X_train_nca,y_train_nca)\ny_pred_nca = knn.predict(X_test_nca)\nacc_test_nca = accuracy_score(y_pred_nca,y_test_nca)\nprint(\"Score:   {}\".format(knn.score(X_test_nca,y_test_nca)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.DataFrame()\ntest_data[\"X_test_nca_p1\"] = X_test_nca[:,0]\ntest_data[\"X_test_nca_p2\"] = X_test_nca[:,1]\ntest_data[\"y_pred_nca\"] = y_pred_nca\ntest_data[\"y_test_nca\"] = y_test_nca\n\nplt.figure(figsize=(10,7))\ndiff = np.where(y_pred_nca!=y_test_nca)[0]\nplt.scatter(test_data.iloc[diff,0],test_data.iloc[diff,1],label = \"Wrong Classified\",alpha = 0.2,color = \"k\",s = 1000)\n\nsns.scatterplot(x=\"X_test_nca_p1\", y=\"X_test_nca_p2\", hue=\"y_test_nca\",data=test_data);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our final acc. score is : 0.9941520467836257 which is pretty good. (NCA)\n* Thank you for your time."},{"metadata":{},"cell_type":"markdown","source":"\n### If you liked this notebook please upvote :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}