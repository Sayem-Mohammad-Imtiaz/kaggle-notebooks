{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The first step to any kind of exploration and modelling requires us to load the data file into the environment.\n### The .read_csv() function helps us."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"full_data = pd.read_csv('/kaggle/input/faults.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's Check the dimensions of this data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(full_data.shape)\nprint(\"Number of rows: \"+str(full_data.shape[0]))\nprint(\"Number of columns: \"+str(full_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's take a look at the top 5 rows of the data to understand what this file contains.\n### The .head() function helps us."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis - EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To understand the data better that we are going to deal with we would like to have a look at the basic numerical stats of the data like the mean, maximum etc. columnwise.\n### The .describe() function helps us."},{"metadata":{},"cell_type":"markdown","source":"DataFrame.describe() method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include=\"all\" is passed.\n\nNow, let's understand the statistics that are generated by the describe() method:\n\n* count tells us the number of NoN-empty rows in a feature.\n* mean tells us the mean value of that feature.\n* std tells us the Standard Deviation Value of that feature.\n* min tells us the minimum value of that feature.\n* 25%, 50%, and 75% are the percentile/quartile of each features. This quartile information helps us to detect Outliers.\n* max tells us the maximum value of that feature."},{"metadata":{},"cell_type":"markdown","source":"### We can check the name of all columns in the dataset using the .columns property"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets try to understand what this data is all about.  "},{"metadata":{},"cell_type":"markdown","source":"### One of the first steps is to gather all possible information about the data and understand the problem statement that we are going to target with the data."},{"metadata":{},"cell_type":"markdown","source":"## Data Visualisation"},{"metadata":{},"cell_type":"markdown","source":"### Since this is a classification problem it would be important and interesting to the distribution of target variables for the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(1,2,figsize=(15,6))\n_ = sns.countplot(x='target', data=full_data, ax=ax[0])\n_ = full_data['target'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So there are 7 classes of faults in the steel. We can see that the distribution of the classes is greatly disbalanced. 'Other_Faults' class is in majority while 'Dirtiness' class is the minority here."},{"metadata":{},"cell_type":"markdown","source":"### Let's check the distribution of data using Histogram and Density visualisation method."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.hist(figsize=(15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.plot(kind=\"density\", layout=(6,5), \n             subplots=True,sharex=False, sharey=False, figsize=(15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Among the various questions that arise, one crucial question is to ask whether the data contains any missing values? Lets see how we can find the answer to that.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By looking at the numbers we can understand that the result is pointing to the number of missing values in each column of the data."},{"metadata":{},"cell_type":"markdown","source":"### Its important to check for missing values and get rid of them because most of the machine learning algorithms have not been designed to handle missing values and whenever row having missing values would go for training or prediction in the algorithm it would raise an error."},{"metadata":{},"cell_type":"markdown","source":"### Once we know that the data contains missing values, it is important that we fill the missing places. Now we will be looking at some of the majorly used techniques for missing value filling, usually referred to as missing value 'imputation'."},{"metadata":{},"cell_type":"markdown","source":"#### There are two main ways to fill any missing values:\n\n1. By Mean\n2. By Median\n\nAnd the decision of filling by mean or median is based on the distribution of the data columns individually. Here's when the histogram and density plots come into play. If the distribution of the column is skewed, we choose median to fill the missing values and if the distribution is normal we go for the mean.\n\nLet's look at the term skew in some more detail."},{"metadata":{},"cell_type":"markdown","source":"## Skewness\n\nA ***left-skewed distribution*** has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA ***right-skewed distribution*** has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n\n![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2014/02/pearson-mode-skewness.jpg)\n\n\n#### to learn more about skewness\nhttps://www.statisticshowto.datasciencecentral.com/probability-and-statistics/skewed-distribution/"},{"metadata":{},"cell_type":"markdown","source":"* X_Maximum   - skew\n* Steel_Plate_Thickness - skew\n* Empty_Index - No Skew"},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.X_Maximum.fillna(full_data.X_Maximum.median(),inplace=True)\nfull_data.Steel_Plate_Thickness.fillna(full_data.Steel_Plate_Thickness.median(),inplace=True)\nfull_data.Empty_Index.fillna(np.mean(full_data.Empty_Index),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_univariate_plot(dataset, rows, cols, plot_type):\n    column_names=dataset.columns.values\n    number_of_column=len(column_names)\n    fig, axarr=plt.subplots(rows,cols, figsize=(30,35))\n\n    counter=0\n    \n    for i in range(rows):\n        for j in range(cols):\n\n            if column_names[counter]=='target':\n                break\n            if 'violin' in plot_type:\n                sns.violinplot(x='target', y=column_names[counter],data=dataset, ax=axarr[i][j])\n            elif 'box'in plot_type :\n                #sns.boxplot(x='target', y=column_names[counter],data=dataset, ax=axarr[i][j])\n                sns.boxplot(x=None, y=column_names[counter],data=dataset, ax=axarr[i][j])\n\n            counter += 1\n            if counter==(number_of_column-1,):\n                break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_univariate_plot(dataset=full_data, rows=7, cols=4,plot_type=\"box\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we can see the target variable has text values as the name of classes. When a machine learning algorithm takes input it expects all values to be numerical and can not handle text values directly. It will simply throw an error if text value is fed to the model. Hence we need to replace text value with a number that can represent the class. Label Encoder tool helps us perform the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nX=full_data.drop('target',axis=1)\nY=le.fit_transform(full_data['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le.inverse_transform([0,1,2,3,4,5,6])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict(zip(le.inverse_transform([0,1,2,3,4,5,6]),[0,1,2,3,4,5,6]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Train Split and Cross Validation methods\n\n\n\n***Train Test Split*** : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n\n![](https://cdn-images-1.medium.com/max/1600/1*-8_kogvwmL1H6ooN1A1tsQ.png)\n\n\n***About Stratify*** : Stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.\n\nFor Reference : https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X, Y, stratify=Y, test_size = 0.3,random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_confusion_matrix(cm):\n    plt.figure(figsize=(12,8))\n    sns.heatmap(cm,annot=True,fmt=\"d\", center=0, cmap='autumn') \n    plt.title(\"Confusion Matrix\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nlogreg = LogisticRegression(random_state=42)\nlogreg.fit(X_train, y_train)\n\ny_predict_train_logreg = logreg.predict(X_train)\ny_predict_test_logreg = logreg.predict(X_test)\n\ntrain_accuracy_score_logreg = accuracy_score(y_train, y_predict_train_logreg)\ntest_accuracy_score_logreg = accuracy_score(y_test, y_predict_test_logreg)\n\nprint(train_accuracy_score_logreg)\nprint(test_accuracy_score_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_logreg = confusion_matrix(y_test,y_predict_test_logreg)\ndraw_confusion_matrix(cm_logreg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Introduction to Confusion Matrix\n\nThe confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n![](https://cdn-images-1.medium.com/max/1600/0*-GAP6jhtJvt7Bqiv.png)\n\n\n\n### ***In the famous cancer example***:\n\n\n###### Cases in which the doctor predicted YES (they have the disease), and they do have the disease will be termed as TRUE POSITIVES (TP). The doctor has correctly predicted that the patient has the disease.\n\n###### Cases in which the doctor predicted NO (they do not have the disease), and they don’t have the disease will be termed as TRUE NEGATIVES (TN). The doctor has correctly predicted that the patient does not have the disease.\n\n###### Cases in which the doctor predicted YES, and they do not have the disease will be termed as FALSE POSITIVES (FP). Also known as “Type I error”.\n\n###### Cases in which the doctor predicted NO, and they have the disease will be termed as FALSE NEGATIVES (FN). Also known as “Type II error”.\n\n![](https://cdn-images-1.medium.com/max/1600/0*9r99oJ2PTRi4gYF_.jpg)"},{"metadata":{},"cell_type":"markdown","source":"#### Since the score of accuracy is so low for training and testing it means that the data is not following any linear trend. We should try non linear machine learning algorithms."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nrf = RandomForestClassifier(random_state=42, n_estimators=50, max_depth=6, criterion = 'entropy', \n                            min_samples_leaf= 1,min_samples_split= 2)\nrf.fit(X_train, y_train)\n\ny_predict_train_rf = rf.predict(X_train)\ny_predict_test_rf = rf.predict(X_test)\n\ntrain_accuracy_score_rf = accuracy_score(y_train, y_predict_train_rf)\ntest_accuracy_score_rf = accuracy_score(y_test, y_predict_test_rf)\n\nprint(train_accuracy_score_rf)\nprint(test_accuracy_score_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_rf = confusion_matrix(y_test,y_predict_test_rf)\ndraw_confusion_matrix(cm_rf)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}