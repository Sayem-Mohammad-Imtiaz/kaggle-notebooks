{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello Kagglers!\n\nIn this notebook, the fake vs real news are classified using NLP techniques. The technique that is implemented in this notebook is Words Vector approach and Tensorflow Deep Learning model to classify the Fake News and Real News. NLTK python package is used to clean the data. Exploratory Data Analysis is also performed in this notebook.\n\nLets get started...\n\n![Fake News Image](https://upload.wikimedia.org/wikipedia/commons/f/f7/The_fin_de_si%C3%A8cle_newspaper_proprietor_%28cropped%29.jpg)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport numpy as np\nimport plotly.express as plot\nfrom wordcloud import WordCloud, STOPWORDS\nfrom matplotlib import pyplot as mplot\nfrom nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom tqdm.notebook import tqdm as tqdm\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.keras.layers as Layers\nimport tensorflow.keras.models as Models\nimport tensorflow.keras.initializers as Init\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The stopwords are the words that have no effect on the classification model is removed. For Example, the, is, i, am, etc has no effect on the model so we will remove the them from the data. Moreover, the TQDM package is initialized, which is used for progress bar during bulking operations."},{"metadata":{"trusted":true},"cell_type":"code","source":"#starting essential code\nnltk_STOPWORDS = set(stopwords.words(\"english\"))\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading the data from the CSV file with the help of Pandas."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/fake-news/fake_train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are about 20k instances and 5 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most important column is \"text\" in the database, so all the rows where the text is NULL are removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(subset=[\"text\"], axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column 'id' is useless. so, it is droped too."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"id\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing the 1 column and 49 rows, the new shape is:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are about 4k news author in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique Authors:\", len(df.author.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the figure below, the 'Pam Key' is the top authors with 243 news and the author which might be an admin of some blog or they may be admins of different blogs, they comes in second place with 193 News."},{"metadata":{"trusted":true},"cell_type":"code","source":"topAuthors = df.author.value_counts()[:10]\nplot.bar(x=topAuthors.keys(), y=topAuthors.values, title=\"Top Authors\", labels={\"x\":\"Authors\",\"y\":\"Number of News\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the top authors thats provided fake news only are computed. In figure below, Authors that spreads the fake news are showm. "},{"metadata":{"trusted":true},"cell_type":"code","source":"topFakeAuthors = df[df.label == 1].author.value_counts()[:10]\nplot.bar(x=topFakeAuthors.keys(), y=topFakeAuthors.values, title=\"Top Fake News Authors\", labels={\"x\":\"Authors\",\"y\":\"Number of Fake News\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the top authors thats provided fake news only are computed. In figure below, Authors that spreads the real news are showm. "},{"metadata":{"trusted":true},"cell_type":"code","source":"topRealAuthors = df[df.label == 0].author.value_counts()[:10]\nplot.bar(x=topRealAuthors.keys(), y=topRealAuthors.values, title=\"Top Real News Authors\", labels={\"x\":\"Authors\",\"y\":\"Number of Real News\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is equaly balanced i.e. there are 50% fake news and 50% real news."},{"metadata":{"trusted":true},"cell_type":"code","source":"labelCount = df.label.value_counts()\n\nplot.pie(values = labelCount.values, names=[\"Fake\",\"Real\"], title=\"Fake Vs Real\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The word cloud is the best technique to determine the frequency of the words in the text. The figure below, shows that the word \"New York\", \"York Times\" and \"Trump\" is the most frequent used words."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \" \".join(\"\" if pd.isnull(t) else t for t in df.title)\nwc = WordCloud(width=800, height=400,stopwords=STOPWORDS, background_color=\"white\").generate(text)\nfig = mplot.figure(figsize=(16, 16))\nmplot.imshow(wc, interpolation='bilinear')\nmplot.axis(\"off\")\nmplot.title(\"Word Cloud\")\nmplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Words cloud for only fake news are computed, the \"Trum\", \"Hillary\", and \"Video\" are the most common words."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \" \".join(\"\" if pd.isnull(t) else t for t in df[df.label == 1].title)\nwc = WordCloud(width=800, height=400,stopwords=STOPWORDS, background_color=\"white\").generate(text)\nfig = mplot.figure(figsize=(16, 16))\nmplot.imshow(wc, interpolation='bilinear')\nmplot.axis(\"off\")\nmplot.title(\"Fake News Word Cloud\")\nmplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Words cloud for only real news are computed, the \"Trum\", \"York Times\", and \"New York\" are the most common words."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \" \".join(\"\" if pd.isnull(t) else t for t in df[df.label == 0].title)\nwc = WordCloud(width=800, height=400,stopwords=STOPWORDS, background_color=\"white\").generate(text)\nfig = mplot.figure(figsize=(16, 16))\nmplot.imshow(wc, interpolation='bilinear')\nmplot.axis(\"off\")\nmplot.title(\"Real News Word Cloud\")\nmplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification"},{"metadata":{},"cell_type":"markdown","source":"**1. Pre Processing**"},{"metadata":{},"cell_type":"markdown","source":"It is assumed that the news text which length is less then 20 alphabets are Junk and considered not a news, so those types of news are removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df.text.map(len) > 20]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sentences are now converted to the tokens e.g. \"This is a cat and 2 Dogs.\" are now converted to [\"This\", \"is\", \"a\", \"cat\", \"and\", \"2\", \"Dogs.\"]."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"tokens\"] = df.text.progress_apply(word_tokenize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After tokenization, all the text are converted to the lowercase letters e,g. [\"This\", \"is\", \"a\", \"cat\", \"and\", \"2\", \"Dogs\"] witll become [\"this\", \"is\", \"a\", \"cat\", \"and\", \"2\", \"dogs.\"]"},{"metadata":{"trusted":true},"cell_type":"code","source":"def toLower(tokens)->list:\n    return [t.lower() for t in tokens]\n\ndf[\"tokens\"] = df.tokens.progress_apply(toLower)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After converting to lower, all the punctuations and numbers are removed from the tokens, because it is useless in the model e,g. [\"this\", \"is\", \"a\", \"cat\", \"and\", \"2\", \"dogs.\"] witll become [\"this\", \"is\", \"a\", \"cat\", \"and\", \"dogs\"]"},{"metadata":{"trusted":true},"cell_type":"code","source":"def removePunctuation(tokens)->list:\n    return [t for t in tokens if t.isalpha()]\n\ndf[\"tokens\"] = df.tokens.progress_apply(removePunctuation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this cell, the stopwords that are discussed above are removed and our tokens become more shorter and compact e.g. [\"this\", \"is\", \"a\", \"cat\", \"and\", \"dogs\"] becomes [\"cat\", \"dogs\"]."},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeStopwords(tokens) -> list:\n    return [t for t in tokens if t not in nltk_STOPWORDS]\n\ndf[\"tokens\"] = df.tokens.progress_apply(removeStopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the stopwords removing, some of the tokens in the dataset are shorter then 3 charchters, these tokens did not make any sense as there are only few words that are shorter then 3 charcters, so these are removed too. Hense the above tokens list is not effected i.e. [\"cat\",\"dogs\"]."},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeSingleLenWords(tokens)->list:\n    return [t for t in tokens if len(t) >= 2]\n        \n\ndf[\"tokens\"] = df.tokens.progress_apply(removeSingleLenWords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last step is lemitization, it basically normalize the words e.g. \"goats\" becomes \"goat\", \"working\" or \"works\" become \"work\" etc. The tokens [\"cat\",dogs\"] becomes [\"cat\",\"dog\"]"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lematize(tokens, lematizer)->list:\n    tokens = [lematizer.lemmatize(t, pos = \"v\") for t in tokens]\n    return [lematizer.lemmatize(t, pos = \"n\") for t in tokens]\nlemme = WordNetLemmatizer()\ndf[\"tokens\"] = df.tokens.progress_apply(lambda tokens: lematize(tokens,lemme))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Words Vector Creation**"},{"metadata":{},"cell_type":"markdown","source":"Here we dfine some constants, the MAX_LEN stores the length of largest text. However, the dimensions are basically used by Words Vector which will be described below."},{"metadata":{"trusted":true},"cell_type":"code","source":"DIMENSION = 100\nMAX_LEN = max([len(x) for x in df.tokens])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gensim is the python library used to create a Words Vector, we can also create a words vectors directly in the keras model, but the gensim provides the mechanisum to save the vectors and it provides more configurations.\n1. Sentences are the actual tokens that are already prepared.\n2. Size is basically a size of vectors, it is also called the dimensions. e.g. if we provide dimension of 3 the vector for word [\"dog\"] woud be [0.344,0.233,-3.33] (Values are assumed). So, what the vector means that the word \"dog\" now live in the 3d space represented by the vector.\n3. Window are the distance of the words that are effected by other words. e.g. [\"dog\",\"cat\",\"goat\"] let say the window is of size 1 then the dog is directly effected by the cat but not by the goat. However if the window size will be 2 then dog will be effected by goat too.\n4. Workers are the numbers of threads need to be run to train the vectors, 1 for no parallelisum.\n5. Min_count is used to describe that the if the frequency of particlular word is below the value, simply ignore them."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_word2vec = gensim.models.Word2Vec(sentences=df.tokens, size=DIMENSION, window=7, workers=2, min_count = 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VOCAB_SIZE is the size of the vocabolary that the words vector have."},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = len(model_word2vec.wv.vocab)\nVOCAB_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The three below cells displays the most similar to the words are provided."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_word2vec.wv.most_similar(\"bad\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_word2vec.wv.most_similar(\"man\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_word2vec.wv.most_similar(\"woman\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Procesing Words Vector to feed in Neural Network**"},{"metadata":{},"cell_type":"markdown","source":"After the words vector, the tokens are now converted to the sequences i.e. numbers e.g. [\"dogs\",\"cats\"] are converted to [1,2]. Note: 0 is already reserved for padding and not used in the sequence."},{"metadata":{"trusted":true},"cell_type":"code","source":"token = Tokenizer()\ntoken.fit_on_texts(df.tokens)\ndf[\"tokens\"] = token.texts_to_sequences(df.tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the most crucial part, preparing the matrix of words vector to feed in the NN. First the size of the matrix is defined by the constatns and dimensions, Then each vector for the words are extracted from the model and placed according to the index of the word sequence. e.g. [1,2] now becomes [[0.3,0.2,-3.33],[0.3,4.5,3.2]] and shape will be (2,3)."},{"metadata":{"trusted":true},"cell_type":"code","source":"words2vec_matrix = np.zeros((VOCAB_SIZE+1,DIMENSION))\n\nfor word, index in tqdm(token.word_index.items()):\n    words2vec_matrix[index] = model_word2vec.wv[word]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words2vec_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the matirix, the tokens are now padded with zeros to feed the NN because the NN always takes fixed sized numpy array so, it can prepare the tensors from it. e.g. The two list of tokens i,e, [2, 3, 4] and [1, 2] will become [2,3,4] and [1,2,0]. the 0 is padded to make the both list size equal."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pad_sequences(df.tokens, padding=\"post\", maxlen=MAX_LEN, dtype=int)\nlabels = df.label\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Deep Learning Model**"},{"metadata":{},"cell_type":"markdown","source":"Spliting the data into 30% test and 70% train data."},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_data, test_data, train_labels, test_labels) = train_test_split(data, labels, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sequential models are used with conv1d to extract the features from the data. The most important layer is embedding layers, that is used to combine the features of words vectors fromt the data. As the words vector is already trained, so we set the trainable false and provide the matrix to it that is already created."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Models.Sequential()\nmodel.add(Layers.Embedding(VOCAB_SIZE+1,DIMENSION, embeddings_initializer = Init.Constant(words2vec_matrix), input_length=MAX_LEN, trainable=False ))\nmodel.add(Layers.Conv1D(16,3, activation=\"relu\"))\nmodel.add(Layers.MaxPooling1D(5))\nmodel.add(Layers.Conv1D(8,3, activation=\"relu\"))\nmodel.add(Layers.Flatten())\nmodel.add(Layers.Dense(8,activation='relu'))\nmodel.add(Layers.Dropout(0.5))\nmodel.add(Layers.Dense(4,activation='relu'))\nmodel.add(Layers.Dense(1,activation='sigmoid'))\n\nmodel.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trained = model.fit(train_data,train_labels, batch_size=64,epochs=10,validation_split=0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_data\ndel train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mplot.plot(trained.history['accuracy'])\nmplot.plot(trained.history['val_accuracy'])\nmplot.title('Model accuracy')\nmplot.ylabel('Accuracy')\nmplot.xlabel('Epoch')\nmplot.legend(['Train', 'Test'], loc='upper left')\nmplot.show()\n\nmplot.plot(trained.history['loss'])\nmplot.plot(trained.history['val_loss'])\nmplot.title('Model loss')\nmplot.ylabel('Loss')\nmplot.xlabel('Epoch')\nmplot.legend(['Train', 'Test'], loc='upper left')\nmplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(model.predict(test_data).round(), test_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the accuracy of about **93%** is achived. The accuracy can be increased by using some different techniques and tricks. But for this kernel, I hope this is enough :)"},{"metadata":{},"cell_type":"markdown","source":"Thank you for viewing my kernel, If you have any sugestions or questions, Please start the discussion below."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}