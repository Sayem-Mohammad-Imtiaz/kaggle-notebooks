{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I'm Hadi here, new on kaggle and Data Science. I'm using data set having path ../input/all-trumps-twitter-insults-20152021/trump_insult_tweets_2014_to_2021.csv. for sure it is not collected by me. I'm just using it for learning purpose. i'm thankful to one who has collected this. regards: Hadi Bux","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n%matplotlib inline \n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/all-trumps-twitter-insults-20152021'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# read csv file and save it in df\ndf = pd.read_csv('../input/all-trumps-twitter-insults-20152021/trump_insult_tweets_2014_to_2021.csv')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show top five rows of dataset\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have 5 columns. **unnamed: 0** column which is just like index, **target** which has person name, **insult** which is particular word used for insult and **tweet** which represents actuall tweet","metadata":{}},{"cell_type":"markdown","source":"we see that column named Unnamed: 0 is unnecessary, we will delete it","metadata":{}},{"cell_type":"code","source":"# deleting unnecessary column\ndf.drop('Unnamed: 0', axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check data type of columns\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_data = df.isnull()\nmissing_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index 2533 and 7439 have target values null\nprint(df[df['target'].isnull()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_value_counts = df['target'].value_counts()\ntarget_value_counts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"there 866 different targets ","metadata":{}},{"cell_type":"markdown","source":"# get list all mentions in tweets","metadata":{}},{"cell_type":"markdown","source":"make function which concatinates strings if tweet contains more than 1 mentions","metadata":{}},{"cell_type":"code","source":"def concat(s):\n    temp = ''\n    i = 0\n    for value in s:\n        temp = temp +' '+ value\n    return temp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create mention list","metadata":{}},{"cell_type":"code","source":"m = re.compile(r'@\\w+')\n#mention_list = m.finditer(df['tweet'].to_json())\nmention_list = []\ntemp = '' # temporary variable for storing list of finds\nfor value in df['tweet']:\n    temp = m.findall(str(value))\n    if temp:\n        if len(temp) > 1: # if temp is greater than 1\n            mention_list.append(concat(temp)) # call concat function\n        else: # if list contains 1 item\n            mention_list.append(temp[0])\n    else:\n        mention_list.append('None')\n# print first 50 mentions\nfor i in range(50):\n    print(mention_list[i])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# letls make column of all mentions\nmention_array = np.array(mention_list)\ndf['mentions']= pd.Series(mention_array)\ndf['mentions'].value_counts().head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"most tweets don't have mentions 7927. there are 558 unique mentions and @nytimes is at top with value of 236 followed by CNN. note: some tweets contain more than 2 mentions. value_count() function count them as another unique value","metadata":{}},{"cell_type":"code","source":"some_mentions = ['@nytimes', '@CNN', '@FoxNews', '@foxandfriends', '@JebBush'] # top 5 mentions\ncounts = [236, 179, 172, 75, 43] # seats in assembly\nexplode=(0.2, 0.2, 0.2, 0.2, 0.2) # distance from each other\npie=plt.pie(counts,\n        explode=explode,\n        labels=some_mentions,\n        autopct='%.1f%%',\n        shadow=True,\n        startangle=45\n)\nplt.axis('equal')\nplt.title('Pie Graph of top 5 mentions')\nplt.legend(pie[0], some_mentions, loc=2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis of Tweets ","metadata":{}},{"cell_type":"code","source":"# import library\nfrom textblob import TextBlob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"polarity = list()\nfor value in df['tweet']:\n    value\n    if TextBlob(value).sentiment.polarity > 0:\n        polarity.append(\"Positive\")\n    elif TextBlob(value).sentiment.polarity < 0:\n        polarity.append(\"Negative\")\n    else:\n        polarity.append(\"Neutral\")    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'] = polarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['tweet', 'sentiment']].head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**it seems sarcasim does not have good relation with textblob libray. it can't detect sarcastic text**","metadata":{}},{"cell_type":"markdown","source":"# Visualization","metadata":{"trusted":true}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x = df['sentiment'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Commonly used keywords","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# required modules\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"tokens = []\nstop_words = stopwords.words(\"english\")\n# GET TEXT FROM DATAFRAME\nfor tweet in df['tweet']:\n# MAKE TOKENS OF TEXT AFTER CONVERTING IT TO LOWERCASE\n    #text = tweet\n    word_tokens = word_tokenize(tweet.lower())\n    # ITERATE OVER TOKEN\n    for word in word_tokens:\n        # IF WORD IS ALPHABETICAL\n        if word.isalpha(): # IGNORE ALL HASHTAGS \n            # IF WORD IS NOT IN STOPWORDS LIST\n            if word not in stop_words:\n                tokens.append( word)\nfwords = []\ncountwords = Counter(tokens)\n#release_list(tokens), it takes 5 more seconds \nfor word in countwords.most_common(50):\n    fwords.append(word[0] + ':' + \" used \" + str(word[1]) + ' times')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top 50 most used words\nfwords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}