{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pylab\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom inspect import signature\n#from sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"breast_df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\nbreast_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.id\n\n2.diagnosis\n\n3.radius_mean\n\n4.texture_mean\n\n5.perimeter_mean\n\n6.area_mean\n\n7.smoothness_mean\n\n8.compactness_mean\n\n9.concavity_mean\n\n10.concave_points_mean\n\n11.texture_worst\n\n12.perimeter_worst\n\n13.area_worst\n\n14.smoothness_worst\n\n15.compactness_worst\n\n16.concavity_worst\n\n17.concave_points_worst\n\n18.symmetry_worst\n\n19.fractal_dimension_worst","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Dictionary\ntarget_dict = { 'B': 0, 'M': 1}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating new DataFrame**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#breast_df.diagnosis.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = breast_df[['id']]\n\ndf['Radius Mean'] = breast_df[['radius_mean']]\n\ndf['Texture Mean'] = breast_df[['texture_mean']]\n\ndf['Perimeter Mean'] = breast_df[['perimeter_mean']]\n\ndf['Area Mean'] = breast_df[['area_mean']]\n\ndf['Smoothness Mean'] = breast_df[['smoothness_mean']]\n\ndf['Compactness Mean'] = breast_df[['compactness_mean']]\n\ndf['Concavity Mean'] = breast_df[['concavity_mean']]\n\ndf['Concave Points Mean'] = breast_df[['concave points_mean']]\n\ndf['Symmetry Mean'] = breast_df[['symmetry_mean']]\n\ndf['Fractal Dimension Mean'] = breast_df[['fractal_dimension_mean']]\n\n#-------------------------------------------------------------\n\ndf['Radius SE'] = breast_df[['radius_se']]\n\ndf['Texture SE'] = breast_df[['texture_se']]\n\ndf['Perimeter SE'] = breast_df[['perimeter_se']]\n\ndf['Area SE'] = breast_df[['area_se']]\n\ndf['Smoothness SE'] = breast_df[['smoothness_se']]\n\ndf['Compactness SE'] = breast_df[['compactness_se']]\n\ndf['Concavity SE'] = breast_df[['concavity_se']]\n\ndf['Concave Points SE'] = breast_df[['concave points_se']]\n\ndf['Symmetry SE'] = breast_df[['symmetry_se']]\n\ndf['Fractal Dimension SE'] = breast_df[['fractal_dimension_se']]\n\n#-------------------------------------------------------------\n\ndf['Radius Worst'] = breast_df[['radius_worst']]\n\ndf['Texture Worst'] = breast_df[['texture_worst']]\n\ndf['Perimeter Worst'] = breast_df[['perimeter_worst']]\n\ndf['Area Worst'] = breast_df[['area_worst']]\n\ndf['Smoothness Worst'] = breast_df[['smoothness_worst']]\n\ndf['Compactness Worst'] = breast_df[['compactness_worst']]\n\ndf['Concavity Worst'] = breast_df[['concavity_worst']]\n\ndf['Concave Points Worst'] = breast_df[['concave points_worst']]\n\ndf['Symmetry Worst'] = breast_df[['symmetry_worst']]\n\ndf['Fractal Dimension Worst'] = breast_df[['fractal_dimension_worst']]\n\ndf['Diagnosis'] = breast_df['diagnosis'].apply(lambda x:target_dict[x])\n\n#map({'M':1,'B':0})\n\n# target as Heart Disease\n#df['Breast Cancer'] = breast_df['diagnosis'].apply(lambda x:target_dict[x])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style = \"darkgrid\")\nsns.countplot(x = \"Diagnosis\", data = df, palette = \"bwr\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countNoDisease = len(df[df['Diagnosis'] == 0])\ncountHaveDisease = len(df[df['Diagnosis'] == 1])\nprint(\"Percentage of Patients having Benignant Breast Cancer: {:.2f}%\".format((countNoDisease / (len(df['Diagnosis'])) * 100)))\nprint(\"Percentage of Patients having Malignant Breast Cancer: {:.2f}%\".format((countHaveDisease / (len(df['Diagnosis'])) * 100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_mean=list(df.columns[1:11])\n# split dataframe into two based on diagnosis\ndfM=df[df['Diagnosis'] ==1]\ndfB=df[df['Diagnosis'] ==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.rcParams.update({'font.size': 8})\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\naxes = axes.ravel()\nfor idx,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(df[features_mean[idx]]) - min(df[features_mean[idx]]))/50\n    \n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(df[features_mean[idx]]), max(df[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5,stacked = True,  label=['M','B'],color=['r','g'])\n    ax.legend(loc='upper right')\n    ax.set_title(features_mean[idx])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the heatmap by using the correlation for the dataset. This helps us eliminate any features that may not help with prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize = (20,20))\n\n#plot heat map\nsns.heatmap(df[top_corr_features].corr(), annot = True, cmap = \"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation with output variable\ncor_diag = abs(corrmat[\"Diagnosis\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_diag[cor_diag > 0.40]\nrelevant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Fractal Dimension Mean','Texture SE','Smoothness SE','Compactness SE','Concavity SE','Symmetry SE','Fractal Dimension SE','Radius Worst','Perimeter Worst','Area Worst','Diagnosis','id'], 1)\nY = df['Diagnosis']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standardizing the input feature**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX = sc.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating Training Set and Test Set.**\n\nWith this section we are creating a training set and a test set by splitting the entire dataframe with a 70:30 proportion.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plt_conf_matrix(pred):\n  # Confusion Matrix\n  cm = confusion_matrix(Y_test, pred)\n  plt.figure(figsize = (5, 4))\n  sns.heatmap(cm, xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative'], annot = True, fmt = 'd')\n  plt.title('Confusion Matrix')\n  plt.ylabel('Actual Values')\n  plt.xlabel('Predicted Values')\n  plt.show()\n  \ndef plt_var_dev(scores):\n  #Variance & Dev. standard\n  print('\\nVariance: {}'.format(round(np.var(scores) * 100, 2)))\n  print('Dev. standard: {}\\n'.format(round(np.std(scores) * 100, 2)))\n  data = {'variance': np.var(scores), 'standard dev': np.std(scores)}\n  names = list(data.keys())\n  values = list(data.values())\n  fig,axs = plt.subplots(1, 1, figsize = (5, 3), sharey = True)\n  axs.bar(names, values)\n  plt.show()\n\ndef plt_auc(model):\n  #AUC\n  probs = model.predict_proba(X_test)\n  # keep probabilities for the positive outcome only\n  probs = probs[:, 1]\n  auc = roc_auc_score(Y_test, probs)\n  print('\\nAUC: %.2f\\n' % auc)\n  # calculate roc curve\n  fpr, tpr, thresholds = roc_curve(Y_test, probs)\n  plt.figure(figsize = (5, 4))\n  # plot no skill\n  plt.plot([0, 1], [0, 1], linestyle = '--')\n  # plot the roc curve for the model\n  plt.plot(fpr, tpr, marker = '.')\n  plt.xlabel('FP RATE')\n  plt.ylabel('TP RATE')\n  plt.show()\n  \ndef plt_prec_rec(pred):\n  #Precision-Recall Curve\n  average_precision = average_precision_score(Y_test, pred)\n  precision, recall, _ = precision_recall_curve(Y_test, pred)\n  print('\\nAP = {0:0.2f}\\n'.format(average_precision))\n  step_kwargs = ({'step': 'post'}\n                 if 'step' in signature(plt.fill_between).parameters\n                 else {})\n  plt.figure(figsize = (5, 4))\n  plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n  plt.fill_between(recall, precision, alpha = 0.2, color = 'b', **step_kwargs)\n  plt.xlabel('Recall')\n  plt.ylabel('Precision')\n  plt.ylim([0.0, 1.05])\n  plt.xlim([0.0, 1.0])\n  plt.title('Precision-Recall curve')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Supervised Classification Algorithm**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nmodels_name = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-Nearest Neighbor**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k_range = range(1,10)\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train,Y_train)\n    y_pred_knn = knn.predict(X_test)\n    score_knn = round(accuracy_score(y_pred_knn,Y_test) * 100, 2)\n    scores_list.append(score_knn)\n\nprint(\"The accuracy score achieved using KNN is: \" + str(max(scores_list)) + \" %\")\n\nscores.append(max(scores_list))\nmodels_name.append('KNN')\n\nplt_conf_matrix(y_pred_knn)\nplt_auc(knn)\nplt_prec_rec(y_pred_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stocastich Gradient Descent**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Random Forest**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 100, bootstrap = True)\nrf.fit(X_train, Y_train)\ny_pred_rf = rf.predict(X_test)\nscore_rf = round(accuracy_score(y_pred_rf, Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Random Forest is: \" + str(score_rf) + \" %\")\n\nscores.append(score_rf)\nmodels_name.append('RF')\n\nplt_conf_matrix(y_pred_rf)\nplt_auc(rf)\nplt_prec_rec(y_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,Y_train)\ny_pred_lr = lr.predict(X_test)\nscore_lr = round(accuracy_score(y_pred_lr,Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \" + str(score_lr) + \" %\")\nprint ('\\nClasification report:\\n', classification_report(Y_test, y_pred_lr))\n\nscores.append(score_lr)\nmodels_name.append('LGR')\n\nplt_conf_matrix(y_pred_lr)\nplt_auc(lr)\nplt_prec_rec(y_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gaussian Naive Bayes - Probabilistic Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train,Y_train)\ny_pred_nb = nb.predict(X_test)\nscore_nb = round(accuracy_score(y_pred_nb,Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Naive Bayes is: \" + str(score_nb) + \" %\")\n\nscores.append(score_nb)\nmodels_name.append('GNB')\n\nplt_conf_matrix(y_pred_nb)\nplt_auc(nb)\nplt_prec_rec(y_pred_nb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sv = SVC(kernel = 'sigmoid')\nsv.fit(X_train, Y_train)\ny_pred_svm = sv.predict(X_test)\nscore_svm = round(accuracy_score(y_pred_svm, Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Linear SVM is: \" + str(score_svm) + \" %\")\n\nscores.append(score_svm)\nmodels_name.append('SVM')\n\nplt_conf_matrix(y_pred_svm)\n#plt_auc(sv)\nplt_prec_rec(y_pred_svm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree - Probabilistic Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\ny_pred_dt = dt.predict(X_test)\nscore_dt = round(accuracy_score(y_pred_dt, Y_test) * 100, 2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \" + str(score_dt) + \" %\")\n\nscores.append(score_dt)\nmodels_name.append('DT')\n\nplt_conf_matrix(y_pred_dt)\nplt_auc(dt)\nplt_prec_rec(y_pred_dt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Neural Network**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = Sequential()\n# First Hidden Layer\nclassifier.add(Dense(4, activation = 'relu', kernel_initializer = 'random_normal', input_dim = 20))\n# Second  Hidden Layer\nclassifier.add(Dense(4, activation = 'relu', kernel_initializer = 'random_normal'))\n# Output Layer\nclassifier.add(Dense(1, activation = 'sigmoid', kernel_initializer = 'random_normal'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the neural network\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the data to the training dataset\nhistory = classifier.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 100, batch_size = 16, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(Y_test, y_pred)\nplt.figure(figsize = (5,4))\nsns.heatmap(cm, xticklabels = ['Positive','Negative'], yticklabels = ['Positive','Negative'], annot = True, fmt = 'd', cmap = \"BuGn\")\nplt.title('Confusion Matrix')\nplt.ylabel('Actual Values')\nplt.xlabel('Predicted Values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_pos = np.diag(cm)\nfalse_pos = np.sum(cm, axis = 0) - true_pos\nfalse_neg = np.sum(cm, axis = 1) - true_pos\nscore_nn = round(np.sum(true_pos)/(np.sum(true_pos) + np.sum(false_pos)) * 100, 2)\n\nprint(\"The accuracy score achieved using Neural Network is: \" + str(score_nn) + \" %\")\n\nplt_prec_rec(y_pred)\n\nscores.append(score_nn)\nmodels_name.append('NN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.set_style(\"whitegrid\")\nplt.figure(figsize = (8, 5))\nplt.yticks(np.arange(0, 100, 10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x = scores, y = models_name, palette = \"BuGn_r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Unsupervised Algorithm**\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**K-Means**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = ['Radius Mean','Texture Mean','Perimeter Mean','Area Mean','Smoothness Mean','Compactness Mean','Concavity Mean','Concave Points Mean','Symmetry Mean','Fractal Dimension Mean','Radius SE','Texture SE','Perimeter SE','Smothness SE','Compactness SE','Concavity SE','Concave Points SE','Symmetry SE','Fractal Dimension SE','Radius Worst','Texture Worst','Perimeter Worst','Area Worst','Smoothness Worst','Compactness Worst','Concavity Worst','Concave Points Worst','Symmetry Worst','Fractal Dimension Worst']\nfeature_dummied = ['Radius Mean','Texture Mean','Perimeter Mean','Area Mean','Compactness Mean','Concavity Mean','Concave Points Mean','Radius SE','Perimeter SE','Concave Points SE','Area SE','Radius Worst','Texture Worst','Perimeter Worst','Area Worst','Smoothness Worst','Compactness Worst','Concavity Worst','Concave Points Worst','Symmetry Worst']\ndata_dummies = pd.get_dummies(df, columns = feature_dummied)\ndata_dummies.head()\nX =  data_dummies.drop([\"Diagnosis\"], axis=1)\nY_df = pd.get_dummies(data_dummies['Diagnosis'], columns=['Diagnosis'])\nY_df = Y_df.drop([0], axis = 1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}