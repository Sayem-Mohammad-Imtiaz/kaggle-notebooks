{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction: Business Goal & Problem Definition\n\nThis project´s goal is creating a model to identify a vehicle based on its 18 features below. The dataset is \"Features extracted from the silhouette of vehicles\" and it´s available in Kaggle. The possible labels for the dependent variables are \"bus\", \"car\" and \"van\".\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\n* compactness\n* circularity\n* distance_circularity\n* radius_ratio\n* pr.axis_aspect_ratio\n* max.length_aspect_ratio\n* scatter_ratio\n* elongatedness\n* pr.axis_rectangularity\n* max.length_rectangularity\n* scaled_variance\n* scaled_variance.1\n* scaled_radius_of_gyration\n* scaled_radius_of_gyration.1\n* skewness_about\n* skewness_about.1\n* skewness_about.2\n* hollows_ratio"},{"metadata":{},"cell_type":"markdown","source":"# 2. Importing Basic Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Collection"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicles_ds = pd.read_csv(\"../input/vehicle/vehicle.csv\", sep=\",\")\n\nvehicles_ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Preliminary Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\nvehicles_ds.sample(n=10, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking dataset info by feature\n\nvehicles_ds.info(verbose=True, null_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of zeros in rows\n\n(vehicles_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(vehicles_ds==0).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the existence of duplicated rows\n\nvehicles_ds.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking data balancing (for classification)\n\ndata_balancing = pd.DataFrame()\ndata_balancing[\"Count\"] = vehicles_ds[\"class\"].value_counts()\ndata_balancing[\"Count%\"] = vehicles_ds[\"class\"].value_counts()/vehicles_ds.shape[0]*100\n\ndata_balancing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking basic statistical data by feature\n\nvehicles_ds.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data Cleaning\n\n    We´ll perform the following:\n\n    1. Use one-hot encoding to change the dependent variable to numerical\n\n    2. Replace all rows with NA by their means, so we are able to keep relevant info in all columns\n\n\n    * no outliers found"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1\n\n#for the ML model (label encoding)\nvehicles_ds[\"class_encoding\"] = vehicles_ds[\"class\"].apply(lambda x: [\"bus\", \"car\", \"van\"].index(x))+1\n\n#for the DL model (one-hot encoding)\nvehicles_ds = pd.concat([vehicles_ds, pd.get_dummies(vehicles_ds[\"class\"], prefix=\"class\")], axis=1)\nclass_encoding_dl = np.asarray(vehicles_ds[[\"class_bus\", \"class_car\", \"class_van\"]]) #creating for the DL model the response variable through the concatenation of the created dummy columns, forming an array\n\n#2\n\nvehicles_ds.fillna(vehicles_ds.mean(), inplace=True)\n\nvehicles_ds.to_excel(\"vehicles_ds_clean.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\nvehicles_ds[\"class\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nvehicles_ds[\"class\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Class Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n\n#Plotting Numerical Variables\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Compactness Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"compactness\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"compactness\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"compactness\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Circularity Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"circularity\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"circularity\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"circularity\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Distance_circularity Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"distance_circularity\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"distance_circularity\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"distance_circularity\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Radius_ratio Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"radius_ratio\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"radius_ratio\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"radius_ratio\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Pr.axis_aspect_ratio Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"pr.axis_aspect_ratio\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"pr.axis_aspect_ratio\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"pr.axis_aspect_ratio\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Max.length_aspect_ratio Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"max.length_aspect_ratio\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"max.length_aspect_ratio\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"max.length_aspect_ratio\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Scatter_ratio Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"scatter_ratio\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"scatter_ratio\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"scatter_ratio\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Elongatedness Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"elongatedness\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"elongatedness\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"elongatedness\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Pr.axis_rectangularity Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"pr.axis_rectangularity\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"pr.axis_rectangularity\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"pr.axis_rectangularity\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Max.length_rectangularity Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"max.length_rectangularity\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"max.length_rectangularity\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"max.length_rectangularity\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Scaled_variance Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"scaled_variance\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"scaled_variance\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"scaled_variance\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Scaled_variance.1 Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"scaled_variance.1\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"scaled_variance.1\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"scaled_variance.1\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Scaled_radius_of_gyration Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"scaled_radius_of_gyration\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"scaled_radius_of_gyration\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"scaled_radius_of_gyration\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Scaled_radius_of_gyration.1 Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"scaled_radius_of_gyration.1\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"scaled_radius_of_gyration.1\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"scaled_radius_of_gyration.1\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Skewness_about Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"skewness_about\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"skewness_about\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"skewness_about\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Skewness_about.1 Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"skewness_about.1\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"skewness_about.1\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"skewness_about.1\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Skewness_about Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"skewness_about\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"skewness_about\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"skewness_about\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Hollows_ratio Distribution\", fontsize=15)\nsns.distplot(vehicles_ds[\"hollows_ratio\"], ax=ax[0])\nsns.boxplot(vehicles_ds[\"hollows_ratio\"], ax=ax[1])\nsns.violinplot(vehicles_ds[\"hollows_ratio\"], ax=ax[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Alternatively using Profile Report to see variables statistics and correlations\n\n# from pandas_profiling import ProfileReport\n# profile = ProfileReport(vehicles_ds, title=\"Vehicles Silhouettes Classification with PCA & DL\")\n# profile.to_file(output_file=\"Vehicles Silhouettes Classification with PCA & DL.html\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Correlations Analysis & Features Selection\n\nWe´ll study the correlations but for the purpose of this exercise we´ll first use all features and then use PCA to see how it affects the model accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting original categorical columns\n\nvehicles_ds.drop([\"class\"], axis=1)\n\n#Plotting a Heatmap\n\nfig, ax = plt.subplots(1, figsize=(25,25))\nsns.heatmap(vehicles_ds.corr(), annot=True, fmt=\",.2f\")\nplt.title(\"Heatmap Correlation\", fontsize=20)\nplt.tick_params(labelsize=12)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n#Plotting a Pairplot\n\n# sns.pairplot(vehicles_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting a Feature Importance\n\nfrom xgboost import XGBClassifier\nfrom matplotlib import pyplot\n#Defining Xs and y\nX = vehicles_ds[[\"compactness\", \"circularity\", \"distance_circularity\", \"radius_ratio\", \"pr.axis_aspect_ratio\", \"max.length_aspect_ratio\", \"scatter_ratio\", \"elongatedness\", \"pr.axis_rectangularity\", \"max.length_rectangularity\", \"scaled_variance\", \"scaled_variance.1\", \"scaled_radius_of_gyration\", \"scaled_radius_of_gyration.1\", \"skewness_about\", \"skewness_about.1\", \"skewness_about.2\", \"hollows_ratio\"]]\ny = vehicles_ds[\"class_encoding\"]\n#Defining the model\nmodel = XGBClassifier().fit(X, y)\n#Getting importance\nimportance = model.feature_importances_\n#Summarizing feature importance\nfor i,v in enumerate(importance):\n    print(\"Feature:{0:}, Score:{1:,.4f}\".format(X.columns[i], v))\n#Plotting feature importance\npd.Series(model.feature_importances_[::-1], index=X.columns[::-1]).plot(kind=\"barh\", figsize=(25,25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Data Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining Xs and y\n\nX = vehicles_ds[[\"compactness\", \"circularity\", \"distance_circularity\", \"radius_ratio\", \"pr.axis_aspect_ratio\", \"max.length_aspect_ratio\", \"scatter_ratio\", \"elongatedness\", \"pr.axis_rectangularity\", \"max.length_rectangularity\", \"scaled_variance\", \"scaled_variance.1\", \"scaled_radius_of_gyration\", \"scaled_radius_of_gyration.1\", \"skewness_about\", \"skewness_about.1\", \"skewness_about.2\", \"hollows_ratio\"]]\ny = vehicles_ds[\"class_encoding\"]\ny_dl = class_encoding_dl #for the DL model\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Setting train/test split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)\nX_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_scaled, y_dl, random_state=0) #for the DL model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Machine Learning Algorithms Implementation & Assessment"},{"metadata":{},"cell_type":"markdown","source":"# 9.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Logistic Regression model and checking its Metrics\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n\n#Trying different polynomial degrees\ndegrees = [1, 2, 3, 4, 5]\nprint(\"Testing degrees:\")\nfor a in degrees:\n    poly = PolynomialFeatures(degree=a)\n    X_train_degree = poly.fit_transform(X_train)\n    X_test_degree = poly.fit_transform(X_test)\n    model_lr = linear_model.LogisticRegression(max_iter=1000000).fit(X_train_degree, y_train)\n    y_preds_train = model_lr.predict(X_train_degree)\n    y_preds_test = model_lr.predict(X_test_degree)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best polynomial degree\nchosen_degree = 4\npoly = PolynomialFeatures(degree=chosen_degree)\n\n#Working on X_train & X_test in the polynomial chosen degree\nX_train_degree = poly.fit_transform(X_train)\nX_test_degree = poly.fit_transform(X_test)\n\n#Fitting to the model\nmodel_lr = linear_model.LogisticRegression().fit(X_train_degree, y_train)\nprint(f\"Linear Regression Intercept: {model_lr.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_lr.coef_}, \\n\")\n\n#Getting the predictions & Metrics\ny_preds_train = model_lr.predict(X_train_degree)\ny_preds_test = model_lr.predict(X_test_degree)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen degree:\")\nprint(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\nX_degree = poly.fit_transform(X_scaled)\ny_preds_all = model_lr.predict(X_degree)\nvehicles_ds[\"class_predicted\"] = y_preds_all\nvehicles_ds.to_excel(\"model_lr.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9.2 Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Deep Learning model and checking its Metrics\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\n\n#Creating a model\nmodel_dl = Sequential()\n\n#Input and First Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\", input_dim=X_train.shape[1]))\n\n#Output Layer\nmodel_dl.add(Dense(units=3, activation=\"softmax\"))\n\n#Compiling the neural network\nmodel_dl.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n#Fitting to the model\nmodel_dl.fit(X_train_dl, y_train_dl, epochs=100, callbacks=[EarlyStopping(monitor=\"val_loss\", patience=\"10\")])\n\n#Getting the predictions & Metrics\ny_preds_train = model_dl.predict(X_train_dl)\ny_preds_train = (y_preds_train>0.5)\ny_preds_test = model_dl.predict(X_test_dl)\ny_preds_test = (y_preds_test>0.5)\naccuracy_train = accuracy_score(y_train_dl, y_preds_train)\naccuracy_test = accuracy_score(y_test_dl, y_preds_test)\nprecision_train = precision_score(y_train_dl, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test_dl, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train_dl, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test_dl, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train_dl, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test_dl, y_preds_test, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# from sklearn.metrics import confusion_matrix\n# confusion_matrix = confusion_matrix(y_test_dl, y_preds_test)\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='.0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_dl.predict(X_scaled)\ny_preds_all = (y_preds_all>0.5)\nvehicles_ds = pd.concat([vehicles_ds, pd.DataFrame(y_preds_all, columns=[\"class_bus\", \"class_car\", \"class_van\"])], axis=1)\nvehicles_ds.to_excel(\"model_dl.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Applying PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying PCA\n\nfrom sklearn.decomposition import PCA\n\n#Creating a model\npca = PCA(n_components=X_scaled.shape[1], random_state=0) #there are 18 features at the dataset\n\n#Fitting to the model\npca.fit(X_scaled)\n\n#Generating all components in an array\nX_pca = pca.transform(X_scaled)\n# X_pca_output = pd.DataFrame(X_pca)\n# X_pca_output.to_excel(\"X_pca_file.xlsx\",index=False)\n\n#Displaying the explained variance by number of components\nfor n in range(0, X_scaled.shape[1]):\n    print(f\"Variance explained by the first {n+1} principal components = {np.cumsum(pca.explained_variance_ratio_ *100)[n]:.1f}%\")\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Explained variance\")\n\n#Creating a model with the chosen number of components (#96% explainability = 6 components)\npca_selected = PCA(n_components=6, random_state=0)\npca_selected.fit(X_scaled)\nX_pca_selected = pca_selected.transform(X_scaled)\n# X_pca_selected_output = pd.DataFrame(X_pca_selected)\n# X_pca_selected_output.to_excel(\"X_pca_selected_file.xlsx\",index=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X_pca_selected, y, random_state=0)\nX_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_pca_selected, y_dl, random_state=0) #for the DL model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11. Machine Learning Algorithms Implementation & Assessment with PCA"},{"metadata":{},"cell_type":"markdown","source":"# 11.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Logistic Regression model and checking its Metrics\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n\n#Trying different polynomial degrees\ndegrees = [1, 2, 3, 4, 5]\nprint(\"Testing degrees:\")\nfor a in degrees:\n    poly = PolynomialFeatures(degree=a)\n    X_train_degree = poly.fit_transform(X_train)\n    X_test_degree = poly.fit_transform(X_test)\n    model_lr = linear_model.LogisticRegression(max_iter=100000).fit(X_train_degree, y_train)\n    y_preds_train = model_lr.predict(X_train_degree)\n    y_preds_test = model_lr.predict(X_test_degree)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best polynomial degree\nchosen_degree = 5\npoly = PolynomialFeatures(degree=chosen_degree)\n\n#Working on X_train & X_test in the polynomial chosen degree\nX_train_degree = poly.fit_transform(X_train)\nX_test_degree = poly.fit_transform(X_test)\n\n#Fitting to the model\nmodel_lr = linear_model.LogisticRegression().fit(X_train_degree, y_train)\nprint(f\"Linear Regression Intercept: {model_lr.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_lr.coef_}, \\n\")\n\n#Getting the predictions & Metrics\ny_preds_train = model_lr.predict(X_train_degree)\ny_preds_test = model_lr.predict(X_test_degree)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen degree:\")\nprint(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\nX_degree = poly.fit_transform(X_pca_selected)\ny_preds_all = model_lr.predict(X_degree)\nvehicles_ds[\"class_predicted\"] = y_preds_all\nvehicles_ds.to_excel(\"model_lr_pca.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11.2 Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a Deep Learning model and checking its Metrics\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\n\n#Creating a model\nmodel_dl = Sequential()\n\n#Input and First Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\", input_dim=X_train.shape[1]))\n\n#Output Layer\nmodel_dl.add(Dense(units=3, activation=\"softmax\"))\n\n#Compiling the neural network\nmodel_dl.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n#Fitting to the model\nmodel_dl.fit(X_train_dl, y_train_dl, epochs=100, callbacks=[EarlyStopping(monitor=\"val_loss\", patience=\"10\")])\n\n#Getting the predictions & Metrics\ny_preds_train = model_dl.predict(X_train_dl)\ny_preds_train = (y_preds_train>0.5)\ny_preds_test = model_dl.predict(X_test_dl)\ny_preds_test = (y_preds_test>0.5)\naccuracy_train = accuracy_score(y_train_dl, y_preds_train)\naccuracy_test = accuracy_score(y_test_dl, y_preds_test)\nprecision_train = precision_score(y_train_dl, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test_dl, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train_dl, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test_dl, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train_dl, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test_dl, y_preds_test, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# from sklearn.metrics import confusion_matrix\n# confusion_matrix = confusion_matrix(y_test_dl, y_preds_test)\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='.0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_dl.predict(X_pca_selected)\ny_preds_all = (y_preds_all>0.5)\nvehicles_ds = pd.concat([vehicles_ds, pd.DataFrame(y_preds_all, columns=[\"class_bus\", \"class_car\", \"class_van\"])], axis=1)\nvehicles_ds.to_excel(\"model_dl_pca.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 12. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nWe were able to implement highly accurate models, testing Logistic Regression and Neural Networks, with and without PCA. Neural Networks without PCA are the most accurate of all (98%), but it´s interesting to note that using PCA on it keeps a high accuracy as well (88%), even using only one third of the columns to train the model. The columns reduction with PCA (from 18 to 6) allows the model train faster but it loses explainability, so if the problem requires business clarity on the model I´d choose Logistic Regression without PCA (97% accuracy)."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}