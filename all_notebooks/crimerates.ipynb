{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport random\nimport tqdm\nimport seaborn as sns\nfrom keras.utils.vis_utils import plot_model\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import LSTM\nfrom keras.layers import TimeDistributed\nfrom fbprophet import Prophet ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Reading Data**","metadata":{}},{"cell_type":"code","source":"chicago_df_1 = pd.read_csv('/kaggle/input/crimes-in-chicago/Chicago_Crimes_2001_to_2004.csv', error_bad_lines=False)\nchicago_df_2 = pd.read_csv('/kaggle/input/crimes-in-chicago/Chicago_Crimes_2005_to_2007.csv', error_bad_lines=False)\nchicago_df_3 = pd.read_csv('/kaggle/input/crimes-in-chicago/Chicago_Crimes_2008_to_2011.csv', error_bad_lines=False)\nchicago_df_4 = pd.read_csv('/kaggle/input/crimes-in-chicago/Chicago_Crimes_2012_to_2017.csv', error_bad_lines=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Models","metadata":{}},{"cell_type":"code","source":"df = pd.concat([chicago_df_1, chicago_df_2, chicago_df_3, chicago_df_4], ignore_index=False, axis=0)\ndf.drop(['Unnamed: 0', 'Case Number', 'Case Number', 'IUCR', 'X Coordinate', 'Y Coordinate','Updated On','Year', 'FBI Code', 'Beat','Ward','Community Area', 'Location', 'District', 'Latitude' , 'Longitude'], inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.index = pd.DatetimeIndex(df.Date)\ndata = df.resample('M').size().reset_index()\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"Date\"] = data[\"Date\"].dt.strftime(\"%m/%d/%Y\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns = ['Date', 'Crime Count']\ndata.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ip = np.asarray(data['Crime Count'].values)\nip = np.asarray([[i] for i in ip])\nip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_splitting(data, n_test):\n    return data[:-n_test], data[-n_test:]\n\n# transform list into supervised learning format\ndef series_to_supervised(data, n_in=1, n_out=1):\n    df = pd.DataFrame(data)\n    cols = list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n    # concatenate together\n    dframe = pd.concat(cols, axis=1)\n    # drop rows with NaN values\n    dframe.dropna(inplace=True)\n    return dframe.values\n \n# root mean squared error or rmse\ndef measure_error(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted)), r2_score(actual, predicted)\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test, cfg):\n    predictions = list()\n    # split dataset\n    train, test = train_test_splitting(data, n_test)\n    # fit model\n    model = model_fit(train, cfg)\n    \n    # seed history with training dataset\n    history = [x for x in train]\n    # step over each time-step in the test set\n    for i in range(len(test)):\n        # fit model and make forecast for history\n        yhat = model_predict(model, history, cfg)\n        # store forecast in list of predictions\n        predictions.append(yhat)\n        # add actual observation to history for the next loop\n        history.append(test[i])\n    # estimate prediction error\n    rmse, r2_score = measure_error(test, predictions)\n    print(' RMSE: %.3f \\t R2 score: %.3f' % (rmse, r2_score))\n    return rmse, r2_score , model\n \n# repeat evaluation of a config\ndef repeat_evaluate(data, config, n_test, n_repeats=30):\n    # fit and evaluate the model n times\n    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n    return scores\n \n# summarize model performance\ndef summarize_scores(name, scores):\n    # print a summary\n    rmse_scores = [i[0] for i in scores]\n    r2_scores = [i[1] for i in scores]\n    rmse_mean, rmse_std = np.mean(rmse_scores), np.std(rmse_scores)\n    r2_mean, r2_std = np.mean(r2_scores), np.std(r2_scores)\n    print('%s' % name)\n    print('RMSE: %.3f (+/- %.3f)' % (rmse_mean, rmse_std))\n    print('R2: %.3f (+/- %.3f)' % (r2_mean, r2_std))\n    # box and whisker plot\n    plt.boxplot(rmse_scores)\n    plt.show()\n    plt.boxplot(r2_scores)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multi Layer Perceptron model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n# fit a model\ndef model_fit(train, config):\n    # unpack config\n    n_input, n_nodes, n_epochs, n_batch = config\n    # prepare data\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    # define model\n    mlp_model = Sequential()\n    mlp_model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n    mlp_model.add(Dense(3))\n    mlp_model.add(Dense(1))\n    mlp_model.compile(loss='mse', optimizer='adam')\n        \n    # fit\n    mlp_model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return mlp_model\n \n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_input, _, _, _ = config\n    # prepare data\n    x_input = np.array(history[-n_input:]).reshape(1, n_input)\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define config\nconfig = [24, 250, 100, 100]\nn_test = 12\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\nmlp_model = scores[0][2]\n# summarize scores\nsummarize_scores('mlp', scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(mlp_model, to_file='mlp_model.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convolutional Neural Network Model","metadata":{}},{"cell_type":"code","source":"def model_fit(train, config):\n    # unpack config\n    n_input, n_filters, n_kernel, n_epochs, n_batch = config\n    # prepare data\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_input, 1)))\n    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model\n\n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_input, _, _, _, _ = config\n    # prepare data\n    x_input = np.array(history[-n_input:]).reshape((1, n_input, 1))\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_test = 12\n# define config\nconfig = [24, 256, 3, 100, 100]\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\ncnn_model = scores[0][2]\n# summarize scores\nsummarize_scores('cnn', scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(cnn_model, to_file='cnn_model.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recurrent Neural Network Model with LSTM","metadata":{}},{"cell_type":"code","source":"# difference dataset\ndef difference(data, interval):\n    return [data[i] - data[i - interval] for i in range(interval, len(data))]\n \n# fit a model\ndef model_fit(train, config):\n    # unpack config\n    n_input, n_nodes, n_epochs, n_batch, n_diff = config\n    # prepare data\n    if n_diff > 0:\n        train = difference(train, n_diff)\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(n_nodes, activation='relu', input_shape=(n_input, 1)))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    \n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model\n \n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_input, _, _, _, n_diff = config\n    # prepare data\n    correction = 0.0\n    if n_diff > 0:\n        correction = history[-n_diff]\n        history = difference(history, n_diff)\n    x_input = np.array(history[-n_input:]).reshape((1, n_input, 1))\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return correction + yhat[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_test = 12\n# define config\nconfig = [24, 50, 100, 100, 12]\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\nrnn_model = scores[0][2]\n# summarize scores\nsummarize_scores('lstm', scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(rnn_model, to_file='rnn_model.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN with LSTM","metadata":{}},{"cell_type":"code","source":"# fit a model\ndef model_fit(train, config):\n    # unpack config\n    n_seq, n_steps, n_filters, n_kernel, n_nodes, n_epochs, n_batch = config\n    n_input = n_seq * n_steps\n    # prepare data\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], n_seq, n_steps, 1))\n    # define model\n    model = Sequential()\n    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(None,n_steps,1))))\n    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu')))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(n_nodes, activation='relu'))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    \n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model\n \n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_seq, n_steps, _, _, _, _, _ = config\n    n_input = n_seq * n_steps\n    # prepare data\n    x_input = np.array(history[-n_input:]).reshape((1, n_seq, n_steps, 1))\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_test = 12\n# define config\nconfig = [3, 12, 64, 3, 100, 200, 100]\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\ncnn_lstm_model = scores[0][2]\n# summarize scores\nsummarize_scores('cnn-lstm', scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(cnn_lstm_model, to_file='cnn_lstm_model.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prophet","metadata":{}},{"cell_type":"code","source":"df = pd.concat([chicago_df_1, chicago_df_2, chicago_df_3], ignore_index=False, axis=0)\ntest_df = chicago_df_4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape, test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the following columns: ID Case Number Date Block IUCR Primary Type Description Location Description Arrest Domestic Beat District Ward Community Area FBI Code X Coordinate Y Coordinate Year Updated On Latitude Longitude Location\ndf.drop(['Unnamed: 0', 'Case Number', 'Case Number', 'IUCR', 'X Coordinate', 'Y Coordinate','Updated On','Year', 'FBI Code', 'Beat','Ward','Community Area', 'Location', 'District', 'Latitude' , 'Longitude'], inplace=True, axis=1)\ntest_df.drop(['Unnamed: 0', 'Case Number', 'Case Number', 'IUCR', 'X Coordinate', 'Y Coordinate','Updated On','Year', 'FBI Code', 'Beat','Ward','Community Area', 'Location', 'District', 'Latitude' , 'Longitude'], inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Date = pd.to_datetime(df.Date, format='%m/%d/%Y %I:%M:%S %p')\ntest_df.Date = pd.to_datetime(test_df.Date, format='%m/%d/%Y %I:%M:%S %p')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.index = pd.DatetimeIndex(df.Date)\ntest_df.index = pd.DatetimeIndex(test_df.Date)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Primary Type'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.resample('M').size()\ntest_df.resample('M').size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(df.resample('M').size())\nplt.title('Crimes Count Per Month')\nplt.xlabel('Months')\nplt.ylabel('Number of Crimes')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"prophet = df.resample('M').size().reset_index()\nprophet.columns = ['Date', 'Crime Count']\nprophet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prophet_df = pd.DataFrame(prophet)\nprophet_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prophet_df_final = prophet_df.rename(columns={'Date':'ds', 'Crime Count':'y'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prop = Prophet()\nprop.fit(prophet_df_final)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"future = prop.make_future_dataframe(periods=1858)  #periods = no. of days for prediction\nforecast = prop.predict(future)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df = forecast[132:]\npreds_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df.Date = pd.to_datetime(preds_df.ds, format='%m/%d/%Y %I:%M:%S %p')\npreds_df.index = pd.DatetimeIndex(preds_df.Date)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = preds_df.yhat.resample('M').sum()/100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = test_df.resample('M').size()\nreqd = np.asarray(targets.values)\nans = np.asarray(preds.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('RMSE: %.3f' % sqrt(mean_squared_error(reqd, ans)))\nprint('R2 score: %.3f' % r2_score(reqd, ans))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = prop.plot(forecast, xlabel='Date', ylabel='Crime Rate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"figure = prop.plot_components(forecast)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}