{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers[\"ja\"]\n!pip install accelerate\n!pip install colorama\n\n# for TPU\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-23T05:15:40.478182Z","iopub.execute_input":"2021-08-23T05:15:40.478653Z","iopub.status.idle":"2021-08-23T05:17:38.654229Z","shell.execute_reply.started":"2021-08-23T05:15:40.478562Z","shell.execute_reply":"2021-08-23T05:17:38.653252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom accelerate import Accelerator\n\nfrom transformers import (AutoModel,\n                          AutoModelForSequenceClassification,\n                          AutoTokenizer,\n                          AutoConfig,\n                          get_cosine_schedule_with_warmup\n                         )\n\n# for TPU\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:18:58.80672Z","iopub.execute_input":"2021-08-23T05:18:58.807089Z","iopub.status.idle":"2021-08-23T05:19:05.706696Z","shell.execute_reply.started":"2021-08-23T05:18:58.80706Z","shell.execute_reply":"2021-08-23T05:19:05.705783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n#     'lr': 2e-5,\n    'lr': 0.00002,\n#     'wd':0.01,\n    'wd':1e-5,\n    'batch_size':16,\n    'valid_step':50,\n    'max_len':512,\n    'epochs':8,\n    'nfolds':5, # もう少し小さくてもよいかも\n    'seed':42,\n#     'device': torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    'device': xm.xla_device(),\n    \n#     https://huggingface.co/cl-tohoku\n    'model_name':'cl-tohoku/bert-base-japanese'\n#     'model_name':'cl-tohoku/bert-base-japanese-v2'\n#     'model_name':'cl-tohoku/bert-large-japanese'\n#     'model_name':''\n}\n\n# シード値の固定\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:17.461645Z","iopub.execute_input":"2021-08-23T05:24:17.462762Z","iopub.status.idle":"2021-08-23T05:24:17.474936Z","shell.execute_reply.started":"2021-08-23T05:24:17.462721Z","shell.execute_reply":"2021-08-23T05:24:17.473831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # make me\n# # https://www.ai-shift.co.jp/techblog/2138\n\n# def cut_head_and_tail(tokenizer,text):\n#     # まずは限界を設定せずにトークナイズする\n#     input_ids = tokenizer.encode(text)\n#     n_token = len(input_ids)\n\n#     # トークン数が最大数と同じ場合\n#     if n_token == config[\"max_len\"]:\n#         input_ids = input_ids\n#         attention_mask = [1 for _ in range(config[\"max_len\"])]\n#         token_type_ids = [1 for _ in range(config[\"max_len\"])]\n#     # トークン数が最大数より少ない場合\n#     elif n_token < config[\"max_len\"]:\n#         pad = [1 for _ in range(config[\"max_len\"]-n_token)]\n#         input_ids = input_ids + pad\n#         attention_mask = [1 if n_token > i else 0 for i in range(config[\"max_len\"])]\n#         token_type_ids = [1 if n_token > i else 0 for i in range(config[\"max_len\"])]\n#     # トークン数が最大数より多い場合\n#     else:\n#         harf_len = (config[\"max_len\"]-2)//2\n#         _input_ids = input_ids[1:-1]\n#         input_ids = [0]+ _input_ids[:harf_len] + _input_ids[-harf_len:] + [2]\n#         attention_mask = [1 for _ in range(config[\"max_len\"])]\n#         token_type_ids = [1 for _ in range(config[\"max_len\"])]\n\n#     d = {\n#         \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n#         \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n#         \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n#     }   \n#     return d\n\n# def make_max_len_context(text):\n#     tokenizer=AutoTokenizer.from_pretrained(config['model_name'])\n#     decode = cut_head_and_tail(tokenizer, text)\n#     context = tokenizer.decode(decode['input_ids'], skip_special_tokens=True)\n#     return context\n\n# # make_512_context(AutoTokenizer.from_pretrained(config['model_name']), train_all_df['context'][0])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:17.542279Z","iopub.execute_input":"2021-08-23T05:24:17.542833Z","iopub.status.idle":"2021-08-23T05:24:17.546968Z","shell.execute_reply.started":"2021-08-23T05:24:17.542799Z","shell.execute_reply":"2021-08-23T05:24:17.546064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# データの読み込み\n# train_all_df = pd.read_csv('../input/fakenews-nlp/train.csv')\n# test_df = pd.read_csv('../input/fakenews-nlp/test.csv')\ntrain_all_df = pd.read_csv('../input/truncation-512-tokens/512_train.csv')\ntest_df = pd.read_csv('../input/truncation-512-tokens/512_test.csv')\nsample_sub = pd.read_csv('../input/fakenews-nlp/sample_submission.csv')\n\ntrain_all_df['context_simple_len'] = train_all_df['context'].map(lambda x: len(x))\ntest_df['context_simple_len'] = test_df['context'].map(lambda x: len(x))\n\ndisplay(train_all_df.head())\ndisplay(test_df.head())\ndisplay(sample_sub.head())\nprint('='*20)\ndisplay(train_all_df.describe())\ndisplay(test_df.describe())","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:17.663585Z","iopub.execute_input":"2021-08-23T05:24:17.664059Z","iopub.status.idle":"2021-08-23T05:24:18.752682Z","shell.execute_reply.started":"2021-08-23T05:24:17.66403Z","shell.execute_reply":"2021-08-23T05:24:18.751929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_all_df['context'] = train_all_df['context'].map(lambda x: make_max_len_context(x))\n# test_df['context'] = test_df['context'].map(lambda x: make_max_len_context(x))\n\n# train_all_df['context_simple_len'] = train_all_df['context'].map(lambda x: len(x))\n# test_df['context_simple_len'] = test_df['context'].map(lambda x: len(x))\n\n# display(train_all_df.head())\n# display(test_df.head())\n# display(sample_sub.head())\n# print('='*20)\n# display(train_all_df.describe())\n# display(test_df.describe())","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:18.75379Z","iopub.execute_input":"2021-08-23T05:24:18.754182Z","iopub.status.idle":"2021-08-23T05:24:18.757791Z","shell.execute_reply.started":"2021-08-23T05:24:18.754146Z","shell.execute_reply":"2021-08-23T05:24:18.757076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_all_df.to_csv('512_train.csv', index=False)\n# test_df.to_csv('512_test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:18.759518Z","iopub.execute_input":"2021-08-23T05:24:18.759968Z","iopub.status.idle":"2021-08-23T05:24:18.768306Z","shell.execute_reply.started":"2021-08-23T05:24:18.75994Z","shell.execute_reply":"2021-08-23T05:24:18.767484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# foldの割当\ntrain_all_df['Fold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_all_df,y=train_all_df['isfake'])):\n    train_all_df.loc[valid_idx,'Fold'] = k","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:18.769828Z","iopub.execute_input":"2021-08-23T05:24:18.770306Z","iopub.status.idle":"2021-08-23T05:24:18.791572Z","shell.execute_reply.started":"2021-08-23T05:24:18.770276Z","shell.execute_reply":"2021-08-23T05:24:18.790682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# データセットの定義\nclass SeqDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.targets = df['isfake'].to_numpy()\n        self.context = df['context'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.context[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n \n        target = torch.tensor(self.targets[idx],dtype=torch.float) \n        return encode, target\n    \n    def __len__(self):\n        return len(self.context)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:18.794389Z","iopub.execute_input":"2021-08-23T05:24:18.794849Z","iopub.status.idle":"2021-08-23T05:24:18.802045Z","shell.execute_reply.started":"2021-08-23T05:24:18.794807Z","shell.execute_reply":"2021-08-23T05:24:18.800981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# モデルの定義\n\nclass AttentionHead(nn.Module):\n    ''' \n    BERT のヘッドにつけるアテンション機構。デフォルトのものを用いても良いが、オリジナルのものを作成して学習することも可能\n    '''\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n\nclass Model(nn.Module):\n    '''\n    モデル本体\n    '''\n    def __init__(self,path):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)  \n        self.config = AutoConfig.from_pretrained(path)\n\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n#         self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n#         x = self.dropout(x)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:18.803756Z","iopub.execute_input":"2021-08-23T05:24:18.804187Z","iopub.status.idle":"2021-08-23T05:24:18.818012Z","shell.execute_reply.started":"2021-08-23T05:24:18.804144Z","shell.execute_reply":"2021-08-23T05:24:18.816892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 推論用\nclass TestSeqDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['context'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n\ndef get_prediction(df,path,device=config['device']):        \n    model = Model(config['model_name'])\n    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n    model.load_state_dict(torch.load(path,map_location=device))\n    model.to(device)\n    model.eval()\n    \n    test_ds = TestSeqDataset(df,tokenizer)\n    test_dl = DataLoader(test_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True)\n    \n    predictions = list()\n    for i, (inputs) in tqdm(enumerate(test_dl)):\n        inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n        outputs = model(**inputs)\n        outputs = outputs.cpu().detach().numpy().ravel().tolist()\n        predictions.extend(outputs)\n        \n    torch.cuda.empty_cache()\n    return np.array(predictions)\n\n# 評価指標\ndef eval_fn(outputs,targets):\n    outputs =  torch.tensor(outputs, dtype=torch.float) \n    targets =  torch.tensor(targets, dtype=torch.float) \n    outputs = outputs.view(-1)\n    targets = targets.view(-1)\n    return torch.sqrt(nn.MSELoss()(outputs,targets)).cpu().detach().numpy().ravel().tolist()[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:18.819571Z","iopub.execute_input":"2021-08-23T05:24:18.819995Z","iopub.status.idle":"2021-08-23T05:24:18.836473Z","shell.execute_reply.started":"2021-08-23T05:24:18.819953Z","shell.execute_reply":"2021-08-23T05:24:18.835289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学習\ndef run(fold,verbose=True):\n\n    def loss_fn(outputs,targets):\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))  \n\n    def train_and_evaluate_loop(train_loader,valid_loader, model, loss_fn,optimizer,epoch,fold,best_loss,valid_step=10,lr_scheduler=None):\n        train_loss = 0\n\n\n        for i, (inputs1,targets1) in enumerate(tqdm(train_loader)):\n            model.train()\n            optimizer.zero_grad()\n            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n            outputs1 = model(**inputs1)\n            loss1 = loss_fn(outputs1,targets1)\n            loss1.backward()\n            optimizer.step()\n            \n            train_loss += loss1.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n            \n            # evaluating for every valid_step\n            # ここで保存するのの厳選してるのか\n            # 何してるのか把握すべきか...\n            # valid_stepってなに? => 今回は50 => ここいらなくね?\n            # 結局ここでは学習してないから、リークはしてなさそう. \n            # 上のほうで結局全てのtrainについて学習してるし、何してるんだこれ?\n\n            if (i % valid_step == 0) or ((i + 1) == len(train_loader)):\n\n                model.eval()\n                valid_loss = 0\n                with torch.no_grad():\n                    for j, (inputs2,targets2) in enumerate(valid_loader):\n                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n                        outputs2 = model(**inputs2)\n                        loss2 = loss_fn(outputs2,targets2)\n                        valid_loss += loss2.item()\n                     \n                    valid_loss /= len(valid_loader)\n                    if valid_loss <= best_loss:\n                        if verbose:\n                            print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n                            print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n                        best_loss = valid_loss\n                        torch.save(model.state_dict(),f'./model{fold}/model{fold}.bin')\n                        tokenizer.save_pretrained(f'./model{fold}')\n\n            \n\n\n        return best_loss\n    \n    \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n    \n    train_df, valid_df= train_all_df.query(f\"Fold != {fold}\"), train_all_df.query(f\"Fold == {fold}\")\n    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n    model = Model(config['model_name'])\n\n    train_ds = SeqDataset(train_df, tokenizer,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=True,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    valid_ds = SeqDataset(valid_df,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    print(f\"Fold: {fold}\")\n    os.makedirs(f'model{fold}',exist_ok=True)\n    best_loss = 9999\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,\n                                            optimizer,epoch,fold,best_loss,\n                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)\n\n\n    # oof用\n    pred = get_prediction(valid_df, f'./model{fold}/model{fold}.bin')\n    torch.save({'pred': pred}, f'./model{fold}/oof_{fold}.bin')\n    check_point = torch.load(f'./model{fold}/oof_{fold}.bin') \n    # valid_df['pred'] = check_point['pred'].cpu().detach().numpy().ravel().tolist()\n    valid_df['pred'] = check_point['pred']\n    return valid_df\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:19.045977Z","iopub.execute_input":"2021-08-23T05:24:19.04664Z","iopub.status.idle":"2021-08-23T05:24:19.06799Z","shell.execute_reply.started":"2021-08-23T05:24:19.04659Z","shell.execute_reply":"2021-08-23T05:24:19.067201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学習\noof_df = pd.DataFrame()   \nfor f in range(config['nfolds']):\n    _oof_df = run(f)\n    oof_df = pd.concat([oof_df, _oof_df])\n    _oof_score = eval_fn(_oof_df['pred'].clip(0,2).values, _oof_df['isfake'].values)\n    print(f\"========== oof_{f}: {_oof_score} ==========\")\n\noof_score = eval_fn(oof_df['pred'].clip(0,2).values, oof_df['isfake'].values)\nprint(f\"========== oof: {oof_score} ==========\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:24:19.588271Z","iopub.execute_input":"2021-08-23T05:24:19.588817Z","iopub.status.idle":"2021-08-23T05:34:59.049554Z","shell.execute_reply.started":"2021-08-23T05:24:19.588765Z","shell.execute_reply":"2021-08-23T05:34:59.047469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 予測\npred1 = get_prediction(test_df,'./model0/model0.bin')\npred2 = get_prediction(test_df,'./model1/model1.bin')\npred3 = get_prediction(test_df,'./model2/model2.bin')\npred4 = get_prediction(test_df,'./model3/model3.bin')\npred5 = get_prediction(test_df,'./model4/model4.bin')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:34:59.051029Z","iopub.status.idle":"2021-08-23T05:34:59.051455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# せっかく作ったしoofでアンサンブルしてもよさそう\nsample_sub['isfake'] = (pred1 + pred2 + pred3 + pred4 + pred5)/5\n# sample_sub['isfake'] = (pred1 + pred2 + pred3)/3\nsample_sub['isfake'] = sample_sub['isfake'].clip(0,2)\nsample_sub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:34:59.052462Z","iopub.status.idle":"2021-08-23T05:34:59.052862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-23T05:34:59.053757Z","iopub.status.idle":"2021-08-23T05:34:59.054171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}