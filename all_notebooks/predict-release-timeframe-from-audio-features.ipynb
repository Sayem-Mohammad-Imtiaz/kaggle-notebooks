{"cells":[{"metadata":{"_cell_guid":"8d3a0f2c-0b4b-49c5-8903-261cfb3378e0","_uuid":"8285d603bc538329a654e7d1bffbc5567ebd2b4e","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f0ea1b2e-cb70-4642-95bc-c5b67d94b960","_uuid":"01907c27b62960f5423e684a037b691adcec5b67"},"cell_type":"markdown","source":"## Introduction\n\nThe Million Song Dataset (MSD) is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This is a subset of the MSD and contains audio features of songs with the year of the song. The purpose here is to predict the decade a song was released in, based on its audio features."},{"metadata":{"_cell_guid":"7c68ab55-0001-42e8-b133-77c220691b04","collapsed":true,"_uuid":"9b66e0cea92077fa5a5f758f2d7d9f288bf7d4e0","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets, svm, metrics\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.utils import resample\nimport itertools\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.manifold import TSNE\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b51340b5-88b3-48f5-9969-af16f400f591","_uuid":"2d899d2e72bbcec351c61f6510591292a187bcf3"},"cell_type":"markdown","source":"### Load and inspect data\n\nThe original dataset has release year as the label for each song. Lets convert this to release decade, since we are trying to predict the decade a song was released in, and not the exact year."},{"metadata":{"_cell_guid":"a87ae18e-e84a-40f2-bb98-904c2e6ad760","_uuid":"93233b5c934ea35191708c96116417c8618512f0","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/year_prediction.csv')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d5fea4e-014f-4863-8b76-44002d0979e5","collapsed":true,"_uuid":"3a70b8a170544c7adef134dd21a3040227fec043","trusted":false},"cell_type":"code","source":"# Group release years into decades\ndf['label'] = df.label.apply(lambda year : year-(year%10))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"513322b5-6ed6-473e-b0ea-851416dce49f","_uuid":"a0137681854677fb0dd6d778e395e87032580c10"},"cell_type":"markdown","source":"The number of data samples are not uniform across release decades. There are too few samples of songs released before 1950."},{"metadata":{"_cell_guid":"cfae45b7-d071-40b6-ba0c-a762a4a3243e","_uuid":"5b47e8aeb73a2006606455abb5de0412259aaa63","trusted":false},"cell_type":"code","source":"sns.countplot(y=\"label\", data=df)\nplt.xlabel(\"Audio samples\")\nplt.ylabel(\"Release Decade\")\nplt.title(\"Samples in the dataset/release decade\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2f24571-2604-433e-bb3c-0be148b05b93","_uuid":"dbd01b3098c72d936644284575f8447ee9001b51"},"cell_type":"markdown","source":"Each sample has 90 features. Each feature takes a wide range of values."},{"metadata":{"_cell_guid":"6586a448-b398-4d5e-9a7b-5a5aeafe9329","_uuid":"5ff29681e9ee7affabd2bf1a603feb7a616de33d","trusted":false},"cell_type":"code","source":"print(\"(Samples, Features) {}\".format(df.iloc[:,1:].shape))\ndf.iloc[:,1:].describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1fde6106-b729-437b-a7c1-65e7c8ee8eae","_uuid":"3ae10309aef47ec55a77ad6f6285d7a574f97ed8"},"cell_type":"markdown","source":"### Scale Features\n\nAfter scaling these features using min-max scaling, each feature is reduced to a range of 0 to 1 "},{"metadata":{"_cell_guid":"016fe5ea-6c3e-41bb-8ae7-b60bbad69ebf","_uuid":"0d11f12ce7e4d66dcec8e233b7718d38c211e41b","trusted":false},"cell_type":"code","source":"df.iloc[:,1:] = (df.iloc[:,1:]-df.iloc[:,1:].min())/(df.iloc[:,1:].max() - df.iloc[:,1:].min())\ndf.iloc[:,1:].describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0566edf-b83a-4276-9a9b-65ae0d34d9b2","_uuid":"88e41dd786844d9320ffc52e652edd4ecedff141"},"cell_type":"markdown","source":"### Downsample\n\nWe have over 500k samples, but there are too few samples for some categories and too many for others. Lets pick equal number of random samples for each category (release decade). Also we have too few samples of songs older than 1950. We will exclude these for now and revisit this later."},{"metadata":{"_cell_guid":"6ff9d301-5100-4e13-9af2-1ad446201995","collapsed":true,"_uuid":"f201550704ca720d7e7a4373f802133d5b2beaa5","trusted":false},"cell_type":"code","source":"df_t = df[df.label>1940]\nmin_samples = df_t.label.value_counts().min()\ndecades = df_t.label.unique()\ndf_sampled = pd.DataFrame(columns=df_t.columns)\nfor decade in decades:\n    df_sampled = df_sampled.append(df_t[df_t.label==decade].sample(min_samples))\ndf_sampled.label = df_sampled.label.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"996ac30e-ade4-4248-ac9d-e629d1c36d3a","_uuid":"32093ed8513d236978c9e73f9bfb7e53dcad289c"},"cell_type":"markdown","source":"After downsampling our dataset has equal number of samples for each release decade."},{"metadata":{"_cell_guid":"1ace1039-06f6-44dc-8e92-57868b03dee9","_uuid":"9601c931618fa7e106ec775015a2a7c533b51a22","trusted":false},"cell_type":"code","source":"sns.countplot(x=\"label\", data=df_sampled)\nplt.ylabel(\"Audio samples\")\nplt.xlabel(\"Release Decade\")\nplt.title(\"Downsampled dataset\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3cfa15b5-c412-4ce9-8705-f16e3a4c34d8","_uuid":"92e060a044ee73bfa70bba2248aab80f0bd1921e"},"cell_type":"markdown","source":"### Analyze features\n\nOur dataset has 90 features. Heres a look at correlation between features and the output class.(Only first few features are included)"},{"metadata":{"_cell_guid":"d520e12d-4a81-4f31-8797-c27d555a889d","_uuid":"9f8d2645f250baab86fbd41b143f62522ff0cf64","trusted":false},"cell_type":"code","source":"# Correlation between the release decade and features\ncorr = df_sampled.iloc[:,:20].corr()\nfig, ax = plt.subplots(figsize=(10,10)) \nplt.title(\"Correlation\")\nsns.heatmap(corr, square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e121e3a-04f7-440d-8710-d91fd42cb778","collapsed":true,"_uuid":"b6d5ae19a43ea3ba36f56e0f7e719117425d68ee"},"cell_type":"markdown","source":"### How do features differ by release decade?\nThis heatmap visualizes how mean value of each feature differs based on the decade a song was released in. (Only first few features are included)"},{"metadata":{"_cell_guid":"687dc048-3c60-4ae3-ab57-560bb4ba5972","_uuid":"bddee597fd4a3f254a8ca7d9b13b31b72b58a95b","trusted":false},"cell_type":"code","source":"# How do features differ by release decade?\ncolumns = df_sampled.groupby(['label']).mean().columns\nlabels = [\"{:02d}'s\".format(l%100) for l in sorted(df_sampled.label.unique())]\nfig, ax = plt.subplots(figsize=(20,5)) \nsns.heatmap(df_sampled.groupby(['label']).mean().iloc[:,0:20], yticklabels=labels)\nplt.ylabel(\"Release Decade\")\nplt.xlabel(\"Features (Mean)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0730541-fb8b-4b39-b4ce-943a58c47a92","_uuid":"28846dcfb32f8a890132a1738443e9f9dbc8658f"},"cell_type":"markdown","source":"Visualize how each feature differs by output class (decade a song was released in). Only the first few shown here."},{"metadata":{"_cell_guid":"63187554-4556-4074-b793-c78b5b1f2792","_uuid":"c25381a70b4e8c6ef57222d225c741ba313d0bee","trusted":false},"cell_type":"code","source":"for component in df_sampled.columns[1:11]:\n    sns.FacetGrid(df_sampled, hue=\"label\", size=3) \\\n       .map(sns.kdeplot, component) \\\n       .add_legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"14170622-00a3-4bf5-b677-a8bc5032c08b","_uuid":"3c86d33d8a5c23785a479ed7b8b9cfaba2b1f587"},"cell_type":"markdown","source":"### Dimensionality Reduction for Visualization\n\nIt is hard to visualize this high dimensional data (90 features). Lets explore couple of techniques for translating high-dimensional data into lower dimensional data. Purpose of dimensionality reduction here is visualization alone.\n\nUse PCA to reduce to 20 principal components."},{"metadata":{"_cell_guid":"f876469d-81e8-465e-8349-bf40a3b45d80","_uuid":"7d863096950e323d73c541b4716e735a3be945e3","trusted":false},"cell_type":"code","source":"X = df_sampled.iloc[:,1:].values\ny = df_sampled.iloc[:,0].values\nprint(\"X \", X.shape, \", y \", y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d89550e4-f7db-46c5-980a-74003f9631f8","collapsed":true,"_uuid":"4819aceae87b8a68a3ed1b70958c7fb8a9e60938","trusted":false},"cell_type":"code","source":"pca = PCA(n_components=20).fit(X)\nX_pca = pca.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9411b1a0-b260-46e4-834b-6d3bd28e6c9e","_uuid":"7662ceccb3158def841a97f6433dbb7243992ba1","trusted":false},"cell_type":"code","source":"principal_components = []\nsamples, features = X_pca.shape\nfor m in range(1, features+1):\n    principal_components.append(\"Principal Component {}\".format(m))\ncols = principal_components+[\"Release Decade\"]    \ndf_pca = pd.DataFrame(np.append(X_pca, y.reshape(samples,1), axis=1), columns=cols)\ndf_pca[\"Release Decade\"] = df_pca[\"Release Decade\"].astype(int)\nprint(\"df_pca.shape = \",df_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d997fe94-fbc6-4c70-8689-5111341e3212","_uuid":"55768c023b430781cdd0d2cfbfb99c8a743ceef4"},"cell_type":"markdown","source":"Visualize principal components (only first two shown here)"},{"metadata":{"_cell_guid":"0244a097-b7b7-40d7-bd08-9987113d4942","_uuid":"fa492d906288a82d0b064d3330dcfb6cc3acf122","trusted":false},"cell_type":"code","source":"sns.pairplot(df_pca, hue=\"Release Decade\",x_vars=\"Principal Component 1\",y_vars=\"Principal Component 2\", size=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f7bfe69-05ba-4f1a-bfc6-2fbc20059134","_uuid":"7081766ce92336455303edc39d934ee59e1d5591"},"cell_type":"markdown","source":"Reducing this further to 2 components using t-SNE."},{"metadata":{"_cell_guid":"b4162df5-2a74-4a75-9777-6b9e8588db55","_uuid":"9bf7b68719461578e12a6db15650ed65e7ed0471","trusted":false},"cell_type":"code","source":"tsne_samples = df_pca.shape[0]\ntsne = TSNE(n_components=2, verbose=2, perplexity=50, n_iter=1000)\ntsne_results = tsne.fit_transform(df_pca.iloc[:tsne_samples,:-1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed8e1906-1b50-4c0c-b98e-6b2108e7d077","collapsed":true,"_uuid":"3694bc4b4451b044d8e0e91805e173bb24f11cca","trusted":false},"cell_type":"code","source":"df_tsne = pd.DataFrame(np.append(tsne_results, \n                                 df_pca.iloc[:tsne_samples,-1].values.reshape(tsne_results.shape[0],1), \n                                 axis=1), \n                       columns=[\"t-SNE Component 1\",\"t-SNE Component 2\",\"Release Decade\"])\ndf_tsne[\"Release Decade\"] = df_tsne[\"Release Decade\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a0d61ba-7073-4ac2-8244-9e141911e9b5","_uuid":"0570ed328f3c4cd2aacd191afb0a0ed77bffcf4e"},"cell_type":"markdown","source":"Visualize t-SNE components."},{"metadata":{"_cell_guid":"2305d0a7-210f-4670-9d5e-73b1733e2807","_uuid":"eb5db39be65c289aaefa58c80766b98f038a3c9a","trusted":false},"cell_type":"code","source":"sns.pairplot(df_tsne, hue=\"Release Decade\",x_vars=\"t-SNE Component 1\",y_vars=\"t-SNE Component 2\", size=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4d0eff4-5c33-4d79-83bc-50e50873b49c","_uuid":"89bf5db5ad6e0532c65d23d990dd1f2a195ffed4"},"cell_type":"markdown","source":"t-SNE components of songs by release decade. There appears to be separation between output classes that are far apart, but not so much for songs released in adjacent decades. "},{"metadata":{"_cell_guid":"4619f969-68c9-4d61-9bfb-20e71a8bddc6","_uuid":"e115c33862ec6fac6c3474b8fa9b19af82e908c0","trusted":false},"cell_type":"code","source":"for component in df_tsne.columns[:-1]:\n    sns.FacetGrid(df_tsne, hue=\"Release Decade\", size=6) \\\n       .map(sns.kdeplot, component) \\\n       .add_legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c7f11a46-33d8-4361-a395-804b54a43bb2","_uuid":"cb35dfc849ff3b5ba486e81b07118a2f4eee68c6","trusted":false},"cell_type":"code","source":"sns.pairplot(data=df_tsne.sample(1000), hue=\"Release Decade\", vars=df_tsne.columns[:-1], size=4)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"col = [\"Greens\", \"Oranges\", \"Oranges\",\"Purples\", \"Purples\", \"Blues\", \"Blues\"]\nfor idx, year in enumerate([1950,1960,1970,1980,1990,2000,2010]):\n    df_tsne_year = df_tsne[df_tsne['Release Decade']==year]\n    sns.kdeplot(df_tsne_year['t-SNE Component 1'].values, df_tsne_year['t-SNE Component 2'].values,cmap=col[idx], shade=True, shade_lowest=False, n_levels=20)\n    plt.xlabel(\"t-SNE Component 1\")\n    plt.ylabel(\"t-SNE Component 2\")\n    plt.title(\"Songs Released in the {}s\".format(year))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"40ff4fe5-365d-4841-a9f8-a13054e6d0b7","_uuid":"e91f867c370e5323dd3abd71969f7d83f8450a26"},"cell_type":"markdown","source":"### Classification \n\nBased on the analysis so far, there doesn't appear to be clear separation between output classes. Lets attempt classification using SVC. We will use complete set of features here and not the principal components visualized earlier.\n\nSplit the dataset into training and test set. Use grid search to find the best parameters for SVC."},{"metadata":{"_cell_guid":"bdb23805-dc1f-484e-90cb-45b36be2afd8","collapsed":true,"_uuid":"16a761a3eb2f7a55fd67b52d1c8569bab8c0db7a","trusted":false},"cell_type":"code","source":"df_sampled = shuffle(df_sampled)\ndf_train, df_test = train_test_split(df_sampled, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f475651-8fca-481a-a6be-98ebac42e5e6","_uuid":"fb3aede345c505c04c2d29f9a3f8567ed9e5a0dd","trusted":false},"cell_type":"code","source":"X_train = df_train.iloc[:,1:].values \ny_train = df_train.iloc[:,0].values\nprint(\"X_train \", X_train.shape, \", y_train \", y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6eaf378-c063-4971-bc7b-c7b5348c2b21","_uuid":"37f07a625dc350d6c1745ec3be23e803d6bae771","trusted":false},"cell_type":"code","source":"#grid_search = GridSearchCV(svm.SVC(),\n#                           {'kernel':['linear', 'rbf','poly'], \n#                            'C': [1, 5, 10,15,20,25], \n#                            'gamma' : [1, 5, 10,15,20]\n#                           },\n#                           cv=None)\n#grid_search.fit(X_train, y_train)\n#clf = grid_search.best_estimator_\n#print(clf)\nclf = svm.SVC(kernel='rbf',C=10,gamma=5);\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91257a4f-7304-4eb3-b5b5-fee50b2e3f8a","_kg_hide-output":true,"_uuid":"628082dc84a93c13392d786b5758dab57ce7e81b"},"cell_type":"markdown","source":"Predict the release decade for songs in the test set using the trained SVC model. Print classification metrics."},{"metadata":{"_cell_guid":"76b2c4d1-a4e3-4072-8768-1be833687836","_uuid":"2e90b1441ce9520b2971667aa6a3a9404e7d2476","trusted":false},"cell_type":"code","source":"tst = df_test\nX_test = tst.iloc[:,1:].values \ny_test = tst.iloc[:,0].values\nexpected = y_test\npredicted = clf.predict(X_test)\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (clf, metrics.classification_report(expected, predicted)))\ncnf_matrix = metrics.confusion_matrix(expected, predicted)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"876dd03f-558b-40ef-8e39-22679bef4660","_uuid":"10e8f103baed716558747c862a4af50fdfc9f0cf"},"cell_type":"markdown","source":"Plot the confusion matrix."},{"metadata":{"_cell_guid":"1f1f290d-4341-4c23-83ff-c1e894f793bd","_uuid":"d909015c543c5c42466616b4255c4ecb6ad725b6","trusted":false},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nlabels = sorted(df_test.label.unique())\nplot_confusion_matrix(cnf_matrix, classes=[\"{:02d}'s\".format(label%100) for label in labels],\n                      title='Confusion matrix')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[\"{:02d}'s\".format(label%100) for label in labels], normalize=True,\n                      title='Normalized')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee83c41e-ed32-4124-9c76-0fe2d06cc598","collapsed":true,"_uuid":"ce74f74d0ec2220fa198dbcc802dcf218a3ca221","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","version":"3.6.1","file_extension":".py","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}