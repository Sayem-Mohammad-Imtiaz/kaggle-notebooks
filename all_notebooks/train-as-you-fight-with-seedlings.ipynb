{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Train as you fight - with seedlings\n\nOr why should we care abaout data collection? \n\nAs kagglers we have to take data as it was provided for us and it's not seldom that a dataset has some kind of unconscious leakage that was introduced the way it was collected. This can mislead a model during learning and in the end the \"knowledge\" of the model is not what you expect it to be. With this kernel I invite you to dive deeper into this kind of problem. Let's explore a research image dataset with 12 different common species in Danish agriculture of cultivated and wild plants. \n\n<img src=\"https://images.unsplash.com/photo-1457530301326-d55420c9898a?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1950&q=80\" width=\"600px\">\n"},{"metadata":{},"cell_type":"markdown","source":"We like to develop a model that is able to classify the correct species given an image with seedlings. Solving this task we would be able to address an important problem of agriculture industries: Plants in the early-growth phase are heavily competing for nutrients and water and often this phase is crucial for a plant to prevail against all other seedlings. Consequently if we would be able to detect wild plants in early phases we could remove them manually which is an advantage for all crop seedlings.  \n\nSo let's start and see why this dataset is not sufficient to solve this kind of problem! ;-)"},{"metadata":{},"cell_type":"markdown","source":"## Table of contents\n\n* [Prepare to start](#load)\n* [Peek at the image files](#peek)\n    * [How many images per species do we have?](#speciescount)\n    * [How do the species classes look like?](#examples)\n* [An unconscious target leakage](#leakage)\n    * [Can we find some order in the names of the image files?](#filenames)\n    * [Does the images size reveal the growth state and the species?](#reveal)\n    * [Uncovering the target leakage](#targetleakage)\n* [What happens during training if we ignore the leakage?](#ignore)\n    * [Transfer learning with pytorch](#transfer)\n    * [Image preprocessing](#imagepreprocessing)\n    * [Dataset and dataloader](#dataset)\n    * [Validation strategy](#validation)\n    * [Model definition](#definition)\n    * [Training loop](#trainloop)\n    * [Run training or load model](#runorload)\n* [Explaining predictions with LIME](#lime)\n    * [What is LIME doing?](#howlimeworks)\n    * [Which components are important features in our images?](#imagecomponents)\n* [Summary](#summary)"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training=True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare to start <a class=\"anchor\" id=\"load\"></a>\n\nWe need to load some libraries:"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom PIL import Image\nfrom imageio import imread\n\nimport seaborn as sns\nsns.set_style(\"dark\")\nsns.set()\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data.dataset import Subset\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom IPython.display import HTML\n\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\nimport cv2\n\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\n\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what kind of data this kernel needs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the pretrained pytorch models we will use one of them for transfer learning to predict the species classes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input/seedlingsmodel\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the model we obtain after running the training loop in this kernel. I have stored it in a separate dataset to run this kernel faster everytime I'm editing this notebook. By this way I can just load the trained model and make predictions with it. "},{"metadata":{},"cell_type":"markdown","source":"## Peek at the image files <a class=\"anchor\" id=\"peek\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = \"../input/v2-plant-seedlings-dataset/nonsegmentedv2/\"\nOUTPUT_PATH = \"seedlings.pth\"\nMODEL_PATH = \"../input/seedlingsmodel/seedlings.pth\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The output path describes where to save a trained model after running the training loop in this kernel. \n* The model path instead describes where to load an already trained model of this kernel. \n* The basepath directs to the images data of the seedlings."},{"metadata":{},"cell_type":"markdown","source":"### How many images per species do we have? <a class=\"anchor\" id=\"speciescount\"></a>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"subfolders = listdir(base_path)\nnames = []\ncounts = []\n\nfor folder in subfolders:\n    images = listdir(base_path +folder)\n    names.append(folder)\n    counts.append(len(images))\n\ncounts = np.array(counts)\nnames = np.array(names)\n\nidx = np.argsort(counts)[::-1]\n    \nplt.figure(figsize=(20,5))    \nsns.barplot(x=names[idx], y=counts[idx], palette=\"Greens_r\")\nplt.xticks(rotation=90);\nplt.title(\"How many images per species are given in the data?\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* We have a lot of images that correspond to wild plants like: loose silky-bent, common chickweed, scentless mayweed, small-flowered cransbill, fat hen, charlock, cleavers, black-grass and shepherd's purse.\n* In contrast seedlings that belong to common agriculture plants like sugar beet, maize and common wheat are less present.\n* We have to deal with species **class-imbalance**.\n\nNow, let's take a look at some examples:"},{"metadata":{},"cell_type":"markdown","source":"### How do the species classes look like? <a class=\"anchor\" id=\"examples\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(4,3,figsize=(20,25))\n\nfor m in range(4):\n    for n in range(3):\n        folder = subfolders[m+n*4]\n        files = listdir(base_path + folder + \"/\")\n        image = imread(base_path + folder + \"/\" + files[0])\n        ax[m,n].imshow(image)\n        ax[m,n].grid(False)\n        ax[m,n].set_title(folder + \"/\" + str(m+n*4+1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* The images of the seedlings were taken on **different scales**. The image areas differ in pixel ranges (heigth and width). Why is this the case? Some images look similar in the ranges. Perhaps images of the same growth stage were taken with the same size? Or was there a manual cropping of images afterwards?\n* **By looking at the stones we can see that images with larger heigth and width are zoomed out (small stones) whereas small images are zoomed in (larger stones)**.\n* Furthermore we can see **additional material** in some of the images: A measureing tape and a container wall.\n* Last but not least - **Some images seem to be defocussed and blurred**. \n* We still have to check for more findings by iterating through the images. For this purpose I like to setup an image iterator later. But for a peek this is already sufficient."},{"metadata":{},"cell_type":"markdown","source":"## An unconscious target leakage <a class=\"anchor\" id=\"leakage\"></a>\n\n### Can we find some order in the names of the image files? <a class=\"anchor\" id=\"filenames\"></a>\n\nThe image names are counters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(base_path + \"Maize\")[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perhaps the way images were enumerated is not random and some kind of order is hidden in the file names. It would be of great advantage if the numeration has something to do with the growth state as well:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"image_indices = np.array([0, 50, 100, 150, 200, 250])+1\n\nfig, ax = plt.subplots(1,6,figsize=(20,5))\n\nfor n in range(6):\n    idx = image_indices[n]\n    image = imread(base_path + folder + \"/\" + str(idx) + \".png\")\n    ax[n].imshow(image)\n    ax[n].grid(False)\n    ax[n].set_title(folder + \"/\" + str(idx))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No, the latter is not the case. **Images of one species are not zooming out with higher number in the image name**. "},{"metadata":{},"cell_type":"markdown","source":"### Does the images size reveal the growth state and the species? <a class=\"anchor\" id=\"reveal\"></a>"},{"metadata":{},"cell_type":"markdown","source":"To answer this question let's load all images one after another and store the species together with image heigth and width into a pandas dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_images = 0\nfor folder in subfolders:\n    total_images += len(listdir(base_path + folder))\n\nplantstate = pd.DataFrame(index=np.arange(0, total_images), columns=[\"width\", \"height\", \"species\"])\n\nk = 0\nall_images = []\nfor m in range(len(subfolders)):\n    folder = subfolders[m]\n    \n    images = listdir(base_path + folder)\n    all_images.extend(images)\n    n_images = len(images)\n    \n    for n in range(0, n_images):\n        image = imread(base_path + folder + \"/\" + images[n])\n        plantstate.loc[k, \"width\"] = image.shape[0]\n        plantstate.loc[k, \"height\"] = image.shape[1]\n        plantstate.loc[k, \"species\"] = folder\n        plantstate.loc[k, \"image_name\"] = images[n]\n        k+=1\n\nplantstate.width = plantstate.width.astype(np.int)\nplantstate.height = plantstate.height.astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plantstate.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plantstate.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,1,figsize=(20,22))\nax[0].scatter(plantstate.width.values, plantstate.height.values, s=2)\nax[0].set_xlabel(\"Image width\")\nax[0].set_ylabel(\"Image height\");\nax[0].set_title(\"Is image width always equal to image height?\")\nfor single in plantstate.species.unique():\n    sns.kdeplot(plantstate[plantstate.species==single].width, ax=ax[1], label=single);\nax[1].legend();\nax[1].set_title(\"KDE-Plot of image width given species\")\nax[1].set_xlabel(\"Image width\");\nax[1].set_ylabel(\"Density\")\nsns.distplot(plantstate.width, ax=ax[2]);\nax[2].set_xlabel(\"Image width\")\nax[2].set_ylabel(\"Density\")\nax[2].set_title(\"Overall image width distribution\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Insights\n\n* Most of the images are **quadratic**. There are some outliers we should take a look at to understand why they are different. Perhaps we want to exclude them from our analysis. \n* Looking at the kde-plots of the species, we can observe:\n    * Almost all species have extreme outliers!\n    * There are at least two major groups of image sizes. \n    * All distributions are right skewed: Smaller images are more common! "},{"metadata":{},"cell_type":"markdown","source":"### Uncovering the target leakage <a class=\"anchor\" id=\"targetleakage\"></a>\n\nOn the kde-plots of image widths per species we have observed that there are at least two image shape groups that may correlate with the growth state of a plant. Let's take a closer look at the logarithmic growth state:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1,figsize=(20,15))\nfor single in plantstate.species.unique():\n    sns.kdeplot(np.log(plantstate[plantstate.species==single].width.values), label=single, ax=ax[0]);\nax[0].set_title(\"Species-wise image width distributions\")\nax[0].set_xlabel(\"Natural log-transformed image width\")\nax[0].set_ylabel(\"Density\");\nsns.distplot(np.log(plantstate.width.values), ax=ax[1], bins=100, kde=False);\nax[1].set_xlabel(\"Natural log-transform image width\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title(\"Overall image width distribution\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Insights\n\n* Given the natural log of the image widths we can see much more than 2 groups (3-5).\n* To which group an image belongs highly depends on the species of the plant! \n* Consequently the probability of observing a specific species is given by the image shape!!! \n* You might say that this leakge is not given if we resize the images, but it's still there as the growth state is also reflected by the size of the stones in the images! \n\nThis gives rise to the following assumption about the data collection: **Photos were taken at specific time points to cover different growth states of all plants**. To be fully covered by the image **some plant species need larger images than others** given a specific time point. \n\n\n#### Why is this a leakage?\n\nIf this is true then **we have observed a so called data leakage**: It was not intended that we can infer the growth state and predict the species given the shape of the images! In a competition we can use this kind of information to improve our model performance and to yield best evaluation scores. **In real situations and use cases we have to be careful with such kind of leakages**. We would still want our model to be able to predict the species if someone else starts to take photos that may not follow the patterns we have found in this data.    "},{"metadata":{},"cell_type":"markdown","source":"#### Clustering the growth states <a class=\"anchor\" id=\"statecluster\"></a>\n\nTo visualize the different species probabilities per growth state let's cluster the growth states with a simple k-means approach and create a heatmap of species frequencies given the state:"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n\nX = np.log(plantstate.width.values).reshape(-1,1)\nX = scaler.fit_transform(X)\n\nkm = KMeans(n_clusters=5)\nplantstate[\"growth_state\"] = km.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_states = plantstate.groupby(\"growth_state\").width.mean().values\nstate_order = np.argsort(mean_states)\nmean_states","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(len(state_order), 4, figsize=(20,5*len(state_order)))\n\nfor n in range(len(state_order)):\n    your_state=state_order[n]\n\n    example = np.random.choice(plantstate[plantstate.growth_state==your_state].index.values, size=4)\n    species = plantstate.loc[example].species.values\n    for m in range(4):\n        image_id = all_images[example[m]]\n        image = imread(base_path + species[m] + \"/\" + image_id)\n        ax[n,m].imshow(image)\n        ax[n,m].set_title(species[m] + \" \" + image_id + \"\\n\" + \"growth state: \" + str(your_state))\n        ax[n,m].grid(False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yeha! \n\nNow you can clearly see that **the image size is related to the growth state of the plants!** But does this also tell us how likely it is to observe a specific species?"},{"metadata":{},"cell_type":"markdown","source":"#### Exploring the target leakage"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_leakage = plantstate.groupby([\"growth_state\", \"species\"]).size().unstack().fillna(0) \ntarget_leakage = target_leakage / plantstate.species.value_counts() * 100\ntarget_leakage = target_leakage.apply(np.round).astype(np.int)\n\nplt.figure(figsize=(20,5))\nsns.heatmap(target_leakage, cmap=\"YlGnBu\", annot=True)\nplt.title(\"The growth state is related to the species!\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insights\n\n* Now you can see it directly. Take a look at scentless mayweed: 43 % plants of this class can be found in growth state 1. \n* This is a very bad leakage! Scentless mayweed is a common species in this dataset. Consequently our CNN could decide to predict this species when the stones of the images look very big (as this feature characterizes the related growth state). \n* Due to the fact that we have a class imbalance in the dataset this leakage becomes more dramatic. It should not be easy to detect it by only looking at the error analysis of a model. "},{"metadata":{},"cell_type":"markdown","source":"## What happens during training if we ignore the target leakage? <a class=\"anchor\" id=\"ignore\"></a>\n\nUnderstanding and exploring the data is a very important part of the data science workflow. Nonetheless I see it quite often that people just apply models and take predictions as they are (after doing some hyperparameter tuning). In our seedlings case this would be bad idea due to the target leakage. Let's discover the illness of a model that is just applied to this data. ;-)\n\n### Transfer learning with pytorch <a class=\"anchor\" id=\"transfer\"></a>\n\nWe are going to use transfer learning with pytorch to classify the plant species as our dataset is pretty small to train from scratch. We will fine-tune a network that was already trained on the imagenet dataset. By retraining the last fully connected layer of the network we adapt to the new task of classifying the seedling species. Usually this is sufficient as the fully connected layers hold most of the parameters. To setup the pipeline we need to label encode the species targets: "},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = LabelEncoder()\nlabels = encoder.fit_transform(plantstate.species.values)\nplantstate[\"target\"] = labels\n\nNUM_CLASSES = plantstate.target.nunique()\norg_plantstate = plantstate.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we need to write a dataset class that uses our transform method to perform image preprocessing:"},{"metadata":{},"cell_type":"markdown","source":"### Image preprocessing <a class=\"anchor\" id=\"imagepreprocessing\"></a>"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def transform(key=\"train\"):\n    data_transforms = {\n        'train': transforms.Compose([\n            transforms.Resize(size=256),\n            transforms.CenterCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n        'val': transforms.Compose([\n            transforms.Resize(size=256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n    }\n    return data_transforms[key]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that we are using image augmentations like random flips to increase the variability of the data. The images are resized and cropped to the target size of our pretrained model. Often the desired shape is (224,224,3) but it depends on the model. The resizing still leads to images with target leakage. Stones is small images become even larger and stones in large images become smaller. We will still be able to deduce the species given the size of the stones in an image."},{"metadata":{},"cell_type":"markdown","source":"### Dataset and Dataloader <a class=\"anchor\" id=\"dataset\"></a>\n\nWe need to write a small dataset class to load images and targets given the path of an image:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\nclass SeedlingsDataset(Dataset):\n    \n    def __init__(self, root_dir, df, transform=None):\n        self.root_dir = root_dir\n        self.states = df\n        self.transform=transform\n      \n    def __len__(self):\n        return len(self.states)\n        \n    def __getitem__(self, idx):\n        image_path = self.root_dir + self.states.species.values[idx] + \"/\" \n        image_path += self.states.image_name.values[idx]\n        image = Image.open(image_path)\n        image = image.convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n         \n        target = self.states.target.values[idx]\n        \n        \n        return {\"image\": image, \"label\": target}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation strategy <a class=\"anchor\" id=\"validation\"></a>\n\nLet's just split the data into train and dev set given a traditional fraction of 0.7 for training and 0.3 for evaluating the model performance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_idx, test_idx = train_test_split(plantstate.index.values,\n                                       test_size=0.3,\n                                       random_state=2019,\n                                       stratify=plantstate.target.values)\n\ntrain_df = plantstate.loc[train_idx].copy()\nval_df = plantstate.loc[test_idx].copy()\n\ntrain_dataset = SeedlingsDataset(base_path, train_df, transform=transform(key=\"train\"))\neval_dataset = SeedlingsDataset(base_path, val_df, transform=transform(key=\"val\"))\nimage_datasets = {\"train\": train_dataset, \"val\": eval_dataset}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\nprint(len(train_dataset), len(eval_dataset))\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32,\n                        shuffle=True, drop_last=True)\neval_dataloader = DataLoader(eval_dataset, batch_size=32, shuffle=False, drop_last=True)\ndataloaders = {\"train\": train_dataloader, \"val\": eval_dataloader}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model definition <a class=\"anchor\" id=\"definition\"></a>\n\nLet's use resnet18 to get started:"},{"metadata":{"trusted":true},"cell_type":"code","source":"listdir(\"../input/pretrained-pytorch-models/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only retrain the fully connected layer that direct to our output for 12 species classes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = torchvision.models.resnet18(pretrained=False)\nif run_training:\n    model.load_state_dict(torch.load(\"../input/pretrained-pytorch-models/resnet18-5c106cde.pth\"))\nnum_features = model.fc.in_features\nprint(num_features)\n\nmodel.fc = nn.Linear(num_features, NUM_CLASSES)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To reduce the effect of class imbalance let's use a weighted cross entropy loss: "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nweights = compute_class_weight(y=train_df.target.values, class_weight=\"balanced\", classes=train_df.target.unique())\nclass_weights = torch.FloatTensor(weights).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = optim.Adam(model.fc.parameters(), 1e-3)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training Loop <a class=\"anchor\" id=\"trainloop\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop(model, criterion, optimizer, scheduler, num_epochs = 10):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                scheduler.step()\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            tk0 = tqdm(dataloaders[phase], total=int(len(dataloaders[phase])))\n\n            counter = 0\n            for bi, d in enumerate(tk0):\n                inputs = d[\"image\"]\n                labels = d[\"label\"]\n                inputs = inputs.to(device, dtype=torch.float)\n                labels = labels.to(device, dtype=torch.long)\n                \n                outputs = model(inputs)\n                _, preds = torch.max(outputs.data, 1)\n                \n                if phase == \"train\":\n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(phase == 'train'):\n                        \n                        loss = criterion(outputs, labels)\n                        loss.backward()\n                        optimizer.step()\n                \n                \n                running_loss += loss.item() * inputs.size(0)\n                corrects = (preds == labels).sum().item()\n                running_corrects += corrects\n                \n                \n                counter += 1\n                tk0.set_postfix({'loss': running_loss / (counter * dataloaders[phase].batch_size),\n                                 'accuracy': running_corrects / (counter*dataloaders[phase].batch_size)})\n                \n                \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n        print()\n    \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))              \n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run training or load trained model <a class=\"anchor\" id=\"runorload\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"if run_training:\n    model = train_loop(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n    torch.save(model.state_dict(), OUTPUT_PATH)\nelse:\n    model.load_state_dict(torch.load(MODEL_PATH))\n    model.eval()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you are running the training loop you can see that the model achieves ~80% accuracy score on the validation data."},{"metadata":{},"cell_type":"markdown","source":"## Explaining predictions with LIME <a class=\"anchor\" id=\"lime\"></a>\n\nLIME stands for [Local Interpretable Model-Agnostic Explanations](https://github.com/marcotcr/lime) and is a tool developed by Marco Tulio Ribeiro, Sameer Singh and Carlos Guestrin that tries to explain why a machine learning model made its predictions as they are. It's model-agnostic which means that you can apply it to any machine learning algorithm of your choice. By using this tool we can decide more easily if a model behaves pathologically or not. \n\n### What is LIME doing? <a class=\"anchor\" id=\"howlimeworks\"></a>\n\nLIME analyzes **how predictions changes when the inputs are perturbed**. In case of images this means that an **image is first devided into contiguous components (like paper shavings). Now a perturbation is performed by turing some of the components \"off\" (for example by making them gray). The perturbed image with only some components \"on\" is passed trough the original model that computes a prediction.** In our case this could be the prediction whether the image contains a scentless mayweed. This is done **multiple times** with different components off & on. Doing so we obtain a new dataset of perturbed images & related predictions. In the next step a simpler model (like linear models) whose knowledge is easier to explain is trained on the new dataset in a weighted fashion. The weights are chosen such that perturbed images that look more similar to the original one have higher weights and contribute more to the final prediction. As the **components are the input features of the model we can obtain the importance of them by looking at the highest positive parameters the linear model learnt**. Click [here](https://www.kdnuggets.com/2016/08/introduction-local-interpretable-model-agnostic-explanations-lime.html) if you like to read more about it or watch the related video of the:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"HTML('<iframe width=\"800\" height=\"600\" src=\"https://www.youtube.com/embed/hUnRCxnydCc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Which components are important features in our images? <a class=\"anchor\" id=\"imagecomponents\"></a>\n\nIn the github repo of the research group of lime you can also find a tutorial on how to use LIME with pytorch. To use it we first need to split the image transformations into two groups:"},{"metadata":{"trusted":true},"cell_type":"code","source":"hold_out_data = org_plantstate.loc[org_plantstate.index.isin(test_idx)].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first one describes what we like to do to prepare the images (for example using data augmentation) and the second tells us what to do with the images to feed them into pytorch (for example turning images to pytorch tensors and normalizing with respect to the chosen pretrained model)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pil_transform():\n    return transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(224)])\n\ndef get_preprocess_transform():\n    normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    return transforms.Compose([\n        transforms.ToTensor(),\n        normalize\n    ])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pil_transf = get_pil_transform()\npreprocess_transform = get_preprocess_transform()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need a method that can be called by LIME that computes predictions given (probably) a perturbed image:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_predict(images):\n    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    batch = batch.to(device)\n    \n    logits = model(batch)\n    probs = F.softmax(logits, dim=1)\n    return probs.detach().cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the explanation which image components are most important to obtain the predictions of our model: "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,4,figsize=(20,20))\n\n\nm = 0\nfor i in range(3):\n    for j in range(4):\n        \n        spec=org_plantstate.species.unique()[m]\n        image_id = hold_out_data[hold_out_data.species==spec].image_name.values[0]\n        image = Image.open(base_path + spec + \"/\" + image_id)\n        image = image.convert('RGB')\n        \n        test_pred = batch_predict([pil_transf(image)])\n        explainer = lime_image.LimeImageExplainer()\n\n        explanation = explainer.explain_instance(np.array(pil_transf(image)), \n                                             batch_predict, # classification function\n                                             top_labels=5, \n                                             hide_color=0, \n                                             num_samples=1000)\n        temp, mask = explanation.get_image_and_mask(explanation.top_labels[0],\n                                                    positive_only=False,\n                                                    num_features=5,\n                                                    hide_rest=False)\n        img_boundry1 = mark_boundaries(temp/255.0, mask)\n        predicted_target = np.argmax(test_pred)\n        predicted_species = plantstate.loc[plantstate.target==predicted_target].species.unique()[0]\n        ax[i,j].imshow(img_boundry1)\n        ax[i,j].axis(\"off\")\n        ax[i,j].set_title(\"true \" + spec + \"\\n predicted: \" + predicted_species)\n        \n        m+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Huuu!\n\n### Insights\n\nDo you see it?\n\n* The **green components are those our model the used image features our model used to make its predictions**. Do you see that the green ones are **located at the stones or the measurement tape most of the time?**\n* This means that our model has not extracted its knowledge from the information given by the plants!\n* Now we can conclude that **we can't trust this model**. The way data was collected has led to an illness which was also not obvious without taking care about explainability in machine learning! "},{"metadata":{},"cell_type":"markdown","source":"## Conclusion <a class=\"anchor\" id=\"summary\"></a>\n\n* Just applying a model is dangerous if you don't care about what the model has learnt. \n* A difficult part in the data science workflow is the design of the dataset and all processes of data collection. You always have to ask yourself if the dataset is what you would expect after deploying your model to the real world..."},{"metadata":{},"cell_type":"markdown","source":"### Train as you fight in the greenhouse or on the fields!\n\nWhat went wrong during data collection?\n\n* The experimental setup of the dataset is too artifical. If you would like to use a machine learning pipeline that was developed with this dataset it's very likely to fail in a real world setting. Imagine a greenhouse setup: \n    * How do you ensure that you take images of growing plants in the same distance to the camera as in this experiment? \n    * Can you expect that the camera a company uses to feed your model with images is the same as yours in the lab?\n    * We have already seen that the stones in the background led to illness during the learning process. What if we could only use the plants by using image segmentation techniques? This approach is still likely to fail as you can't expect that the background is always of the same color etc.. It could be that moss grows on the ground with similar color as your plants. How would you tackle this kind of challenge?\n* What to do instead? \n    * Take images with many different setups that you would expect in the real world situation. \n    * Keep in mind to introduce variation. Use **different cameras, illumination, backgrounds, distances to plants, overlapping plants etc..** \n    * Let experts like **botanists and agriculture experts** label your target species of your images. \n    * With real world diversity in your data the model has to capture meaningful information that is given solely by the plants and nothing else! \n    * After writing this kernel I think it's important to collect the data with **Train as you fight** in your mind. :-)\n    \n<img src=\"https://cdn.pixabay.com/photo/2018/03/21/16/05/greenhouse-3247181_1280.jpg\" width=\"600px\">\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}