{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"INTRO\n\nThe Original Project Goal was to investigate a dataset of Jeopardy! questions and answers:\n\n* build and use Python lambdas functions\n* understand the basics of NumPy\n* create and manipulate pandas DataFrames\n* work with aggregates and multiple DataFrames using pandas\n\n\nBut while investigating the dataset I'll set up an ambitious goal for myself to try to make my program actually answer questions, because figuring out how many times a particular word appeared in question didn't seem like that big of a deal.\n\nI would like to notice, that I'm a complete novice in ML and I do not aim to build the ultimate Question Answering machine or even come close to what the whole IBM team achieved back in 2011, when computer Watson has beaten Jeopardy champions.\n\nQUESTION ANSWERING\n\nWhen I figured out what I want to do — answer questions, I found out that there is such discipline within NLP.\n\nI learned a lot from the book \"Speech and Language Processing, An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition\" by Daniel Jurafsky from Stanford University and James H. Martin from University of Colorado at Boulder. Below you'll find several passages from this book, that I considered would be helpful for understanding of this project.\n________________________\n\nAs far as the 1960s, there were two major paradigms of question answering — information-retrieval-based and knowledge-based.\n\nIR-based factoid question answering has two stages: retrieval, which returns relevant documents from the collection, and reading, in which a neural reading comprehension system extracts answer spans.\n\nIn the second paradigm, knowledge-based question answering, a system instead builds a semantic representation of the query. These meaning representations are then used to query databases of facts.\n\nWatson by IBM combines information from IR-based and knowledge-based sources.\n\nOUR TASK\n\nWe would try to use IR-based question answering and rely on Google with information retrieval, while our task will be to get the right answer from these retrieved passages.\n\nWithout further ado, let's get to work!\n________________________\n\nWhat are we going to do:\n\n1. Python Fundamentals (Functions, Lists, Loops, Strings, List comprehension)\n2. Data Manipulation with Pandas\n3. Data Acquisition (Web scraping with BeautifulSoup)\n4. Natural Language Processing (Text preprocessing, Regex)","metadata":{}},{"cell_type":"markdown","source":"Let's begin by importing pandas library and opening our dataset as dataframe.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/200000-jeopardy-questions/JEOPARDY_CSV.csv')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at several first entries of our dataset and get to know the length of our database.","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns we will make use of are Answer, Question and Category (if question is too short to make sense of, we would add Category to search query).\n\nWe'll need to know exact names of columns to work with them.","metadata":{}},{"cell_type":"code","source":"print(df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could see not much needed space before word in some columns, it wouldn't affect our task, but for the sake of it we'll change column names and check them out again.","metadata":{}},{"cell_type":"code","source":"df.columns = df.columns.str.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do we have any empty, null, NaN cells?","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 out of 216930 doesn't seem that bad, we could just get rid of them.","metadata":{}},{"cell_type":"code","source":"df = df.dropna()\ndf = df.reset_index(drop=True)\n\nprint(df.isnull().sum())\nprint(len(df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We would be asking google the question, and then how could we get the answer?\n\nWe could plug in some of this question into google and see that for some questions google gets second part of IR-based QA machine's work done - basically in first result we get the answer, for many of the questions we get linked to some Jeopardy site with answers and some queries get no relevant results at all.\n\nLet's first try to use brute force and assume that we could get the right answer from counting words in results' snippets, because the right answer would appear frequently in results. We would obviously have to exclude words from the question, but also the most frequent words are articles and prepositions such as 'the, a, to, in...', so we would have to get rid of them as well.\n\nIn order to match the most frequent word with our correct answer we would have to also strip down all the same words from the answers. So we need almost the same preprocessing for them, but not the same, because upon data analysis we found out some unique data traits that need to be dealt with separately (like we have dual answers, that both could be true, the second answer in parentheses orshort questions, that we could make no sense of without adding Category to it).\n\nWe would make use of nltk libraby and Wordnet Lemmatizer.","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we will crwte","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import wordnet\nfrom collections import Counter\nimport re \n\nlemmatizer = WordNetLemmatizer()\n\ndef get_part_of_speech(word):\n    #we get synonyms\n    probable_part_of_speech = wordnet.synsets(word)\n  \n    pos_counts = Counter()\n    \n    #then use synonyms to determine the most likely part of speech\n\n    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n  \n    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n    return most_likely_part_of_speech\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_question(data, n):\n    #we need variable to write on our preprocessed data\n    # to get rid of all the articles and prepositions we'll use stopwords from nltk\n    stop_words = set(stopwords.words('english'))\n    question = data['Question'][n]\n    # if length of a question is too short we would use category as well\n    if len(question) < 20:\n        question = data['Category'][n] + ' ' + question  \n    words = re.sub(\"[^ a-zA-Z-]|[&'-]{2,}\", \"\", question) #\" \\\\1\"\n    words = word_tokenize(words)\n    # we would make all the words lowercase and strip all the symbol that are not letters or numbers\n    words = [word.lower() for word in words]\n    words = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in words]\n    \n    final_tokens = [] \n    for each in words:\n        # if length of question is less than 5 words, \n        # we might not want to get rid of any words, that's why we append all of them\n        if len(words) < 5:\n            final_tokens.append(each)\n        else:\n            # else we would get rid of all the stop_words\n            if each not in stop_words:\n                final_tokens.append(each)\n    return final_tokens","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try and see what our function does to a question.","metadata":{}},{"cell_type":"code","source":"process_q_3699 = preprocess_question(df, 3699)\nprint(df['Question'][3699])\nprint(process_q_3699)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_q_5550 = preprocess_question(df, 5550)\nprint(df['Question'][5550])\nprint(process_q_5550)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we would create a bit simlified function for preprocessing of the answers.","metadata":{}},{"cell_type":"code","source":"def preprocess_answers(data):\n    answers = []\n    for answer in data:\n        #for some answers there is an alternative answer in parentheses, for now we would get rid of the alternative using regex\n        ans = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", answer) \n        ans = word_tokenize(ans)\n        ans = [word.lower() for word in ans if word.isalnum()]\n        ans = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in ans]\n        stop_words = set(stopwords.words('english'))\n        final_result_tokens = [] \n        for each in ans:\n            if each not in stop_words:\n                  final_result_tokens.append(each)\n        answers.append(final_result_tokens)\n    return answers\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we will prepocess them all answers, as it takes not so much time and compare our \"before and after\".","metadata":{}},{"cell_type":"code","source":"df['Processed answer'] = preprocess_answers(df['Answer'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Answer'][12]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Processed answer'][12]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = 0\ny = 0\nz = 0\nu = 0\n\nfor n in range(len(df['Processed answer'])):\n    if len(df['Processed answer'][n]) == 0:\n        x += 1\n    if len(df['Processed answer'][n]) == 1:\n        y += 1\n    if len(df['Processed answer'][n]) == 2:\n        z += 1\n    if len(df['Processed answer'][n]) >= 3:\n        u += 1\n    \nprint('There are ' + str(x) + ' answers with 0 words. \\nThe reason for that is not optimal preprocessing, during which we \\'lost\\' ' + \n       str(round((x/len(df['Processed answer'])*100), 1)) + \n       '% of all answers. We would get rid of this empty lists for now.\\n')\nprint('There are ' + str(y) + ' answers with 1 word, which is ' + str(round((y/len(df['Processed answer'])*100), 1)) + \n       '% of all answers. Them we would use.\\n')\nprint('There are also ' + str(z) + ' answers with 2 words, ' + str(u) + ' answers with 3 or more words. For simplification of a task we would not use them.\\n')\nprint('The total number of answers for now is ' + str(len(df['Processed answer'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_word_answer_data = pd.DataFrame()  \none_word_answer_data = df[df['Processed answer'].map(lambda d: len(d)) == 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_word_answer_data = one_word_answer_data.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = 0\nfor answer in one_word_answer_data['Processed answer']:\n    if len(answer) == 0:\n        h += 1\nprint(h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(one_word_answer_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get to know actual efficiency of our method. We found out (using allmighty Internet) that if we need 95% confidence level and 5% margin of error  with our population size of 118832 we would need sample size of 383 for our test. We will get those 383 questions randomly.","metadata":{}},{"cell_type":"code","source":"import random\n\nindices_list = []\n\nwhile len(indices_list) < 383:\n    index = random.randint(0,118832)\n    if index not in indices_list:\n        indices_list.append(index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(indices_list)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order not to use computing power wastefully we would not process all the questions, just the ones we randomly choose. After preprocessing them, we would form queries to add to our web-scraper. ","metadata":{}},{"cell_type":"code","source":"one_word_answer_data.loc[71073]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"queries = []\nsample_questions_processed = []\nfor index in indices_list:\n    x = preprocess_question(one_word_answer_data, index)\n    query = ''\n    for item in x:\n        query += item \n        query += '+'\n    queries.append(query)\n    sample_questions_processed.append(x)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(queries)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"queries[33]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, using BeautifulSoup we would get the results, preprocess them and right away compute the most frequent words. As they are given to us in the form of tuples, we would need to extract just the words for the ease of working with them later.\n\nNOTE: It takes time to go through with all the almost 400 queries. (~ 4 sec for a query)\n\nAlso we would use NER from spacy to get the most frequent proper nouns from the results. So then we could compare this two almost brute force methods to get our answer. For that purpose we would need spacy library.","metadata":{}},{"cell_type":"code","source":"pip install bs4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport spacy\nimport string\n\nsample_results_frequency = []\n\nsample_results_ner_fre = []\n\nfor index in range(383):\n    url = 'https://google.com/search?q=' + queries[index]\n    page = requests.get(url).text\n    soup = BeautifulSoup(page, \"html.parser\").select(\".s3v9rd.AP7Wnd\")\n    results_tokenized = []\n    results = []\n    for item in soup:\n        results.append(item.getText(strip=True))\n        results_df = pd.DataFrame(results, columns=['A'])\n        \n        for result in results:\n            words_from_results = word_tokenize(result)\n            words_from_results = [word.lower() for word in words_from_results if word.isalnum()]\n            words_from_results =  [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in words_from_results]\n            stop_words = set(stopwords.words('english'))\n            final_result_tokens = [] \n            for each in words_from_results:\n                if each not in stop_words:\n                    if each not in sample_questions_processed[index]:\n                        final_result_tokens.append(each)\n            results_tokenized.append(final_result_tokens)\n\n    bag = []\n    from collections import Counter\n    \n    no = ['jeopardy', 'answer']\n\n    for i in range(len(results_tokenized)):\n        current = results_tokenized[i]\n        for current_word in current:\n            for word in sample_questions_processed[index]:\n                if nltk.edit_distance(word, current_word) <= 1 and current_word in current and word not in no:\n                    current.remove(current_word)\n        bag += results_tokenized[i]\n\n\n    c = Counter(bag)\n    maybe = c.most_common(3)\n\n    wow = []\n    for tulip in maybe:\n        wow.append(tulip[0])\n    sample_results_frequency.append(wow)\n    \n    \n    \n\n    nouns_processed = []\n    nouns = []\n    \n    nlp = spacy.load(\"en\")\n    for result in results:\n        sample_results_ner_nouns = []\n        doc = nlp(result)\n        \n        ner_fre = []\n\n        for chunk in doc:\n            if chunk.pos_ == 'PROPN':\n                new_nouns = re.sub('\\.(?!(\\S[^. ])|\\d)', '', chunk.text)\n                nouns.append(new_nouns)\n\n\n        for noun in nouns:\n            ner = word_tokenize(noun)\n            ner = [word.lower() for word in ner if word.isalpha()]\n            ner =  [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in ner]\n            for word in ner:\n                if word not in sample_questions_processed[index] and word not in stop_words and word not in no:\n                    nouns_processed.append(word)\n        sample_results_ner_nouns.append(nouns_processed)\n        counter = Counter(nouns_processed)\n        maybe_ner = counter.most_common(3)\n\n        \n        for tulip in maybe_ner:\n            ner_fre.append(tulip[0])\n    sample_results_ner_fre.append(ner_fre)\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We would get the answers we need for our sample.","metadata":{}},{"cell_type":"code","source":"sample_answers = []\n\nfor index in indices_list:\n    sample_answers.append(one_word_answer_data['Processed answer'][index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check out what have gotten here.","metadata":{}},{"cell_type":"code","source":"for index in range(10):\n    #print(sample_questions_processed[index])\n    #print(queries[index])\n    print(sample_answers[index])\n    print(sample_results_frequency[index])\n    print(sample_results_ner_fre[index])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to get a score for our program. We would get a score for each question - if the word from the answer is among the 3 most frequent in our results (excepy stop words) we would add 1 point to our score.","metadata":{}},{"cell_type":"code","source":"score_fre = 0\nscore_ner = 0\nscore_ner_not_fre = 0\n\n\nfor x in range(10):\n    for word in sample_answers[x]:\n        if word in sample_results_frequency[x]:\n            score_fre += 1\n        else:\n            if word in sample_results_ner_fre[x]:\n                score_ner_not_fre += 1\n        if word in sample_results_ner_fre[x]:\n            score_ner += 1\n        \n       \n    \nprint('score fre: ' + str(round((score_fre/10*100), 2)), '%')  \nprint('score ner not fre: ' + str(round((score_ner_not_fre/10*100), 2)), '%')  \nprint('score ner: ' + str(round((score_ner/10*100), 2)), '%') \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"WORK IN PROGRESS...","metadata":{}}]}