{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is about \n\n\nIt discussed in some papers that \"hubness phenomena\" (i.e. presense of nodes with high degree in graphs ) creates problems for many machine learning algorithms. In particular for KNN classifier algorithm.\n\nThe Python package \"scikit-hubness\" ( https://pypi.org/project/scikit-hubness/ , https://arxiv.org/abs/1912.00706 \"scikit-hubness: Hubness Reduction and Approximate Neighbor Search\" ) proposes (in particular) certain \"hubness reduction\" algorithms which sometimes improve the scores for KNN classifiers. (See some in example in the paper.)\n\nIn this notebook we test that approach on the datasets with genes expressions from  OpenML database.\n(Datasets info is below).\n\nCurrent version uses only some default params of scikit-hubness tools. Might be tuning params would improve.\n\n-----------------------------------------------------------\n\n\nOpenML - is large dataset collection for machine learning. \nIt contains at least 45 datasets related to genes expressions. \nHere is example how to work with them.\n\nLoading from openml collections 45 datesets related to genes expressions - microarray technology, cancer cells\n\n\n\nhttps://www.ncbi.nlm.nih.gov/gds?term=GSE2109 \nSeems origins of datasets is here\n\nCan be found at openml dataset storage, for example:\nhttps://www.openml.org/d/1163\nGEMLeR provides a collection of gene expression datasets that can be used for benchmarking gene expression oriented machine learning algorithms. They can be used for estimation of different quality metrics (e.g. accuracy, precision, area under ROC curve, etc.) for classification, feature selection or clustering algorithms\"\nCan be found by feature number 10937\n\n\nhttps://pdfs.semanticscholar.org/4088/4ec5bcd5fbe1626cd3c95179e2ac601a0097.pdf\nHindawi Publishing Corporation\nJournal of Biomedicine and Biotechnology\nVolume 2010, Article ID 616358, 9 pages\ndoi:10.1155/2010/616358\nResearch Article\nStability of Ranked Gene Lists in Large Microarray\nAnalysis Studies\nGregor Stiglic1 and Peter Kokol1, 2\n\n\n-----------\n\nTo work with openml:\n\n**1 )**\n\nBasically we should know the \"did\" = dataset id, and then use two lines:\n\ndata = openml.datasets.get_dataset(did)\n\nX, y, categorical_indicator, attribute_names = data.get_data(\n      dataset_format=\"array\", target=data.default_target_attribute )\n\n**0 )**\n\nTo be aware of \"did\" (dataset id) we can load the information on all datasets first :\n\nopenml_df = openml.datasets.list_datasets(output_format=\"dataframe\") # get pandas dataframe\n\nand column \"did\" of that pandas  dataframe gives \"did\" , other columns contain other info \n\nSee code below\n\n\nFurther docs: \n\nsee docs: https://docs.openml.org/Python-guide/\n\nSee also some code here:\n\nhttps://datascience.stackexchange.com/a/84241/32240\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport time\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# scikit-hubness install (should be done before(!) install openml !)"},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\n!pip install scikit-hubness\n# If does not work on your comp (e.g. on google.colab - does not), then try:\n# !pip install git+https://github.com/j-bac/scikit-hubness.git\nprint(time.time()-t0,'seconds passed')\n\nfrom skhubness.neighbors import KNeighborsClassifier\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Install openml , get list of datasets, select Genes related datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pip install openml","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import openml","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t0 = time.time()\nopenml_df = openml.datasets.list_datasets(output_format=\"dataframe\")\nprint(time.time()-t0,'seconds passed ')\n\nif 0: ## The same can be done with BIGGER lines of code\n    openml_list = openml.datasets.list_datasets()  # returns a dict\n    # Show a nice table with some key data properties\n    openml_df = pd.DataFrame.from_dict(openml_list, orient=\"index\")\n    openml_df = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]\n\n    print(f\"First 10 of {len(datalist)} datasets...\")\n    openml_df.head(n=10)\n\n\nopenml_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Among all datasets there is subset of 45 datasets with  mircoarray Gene Expressions (mainly cancer) \n# can be selected by number of features == 10937\ndf_gemler = openml_df[ ( openml_df.NumberOfFeatures == 10937)    ].sort_values([\"NumberOfInstances\"], ascending=False)#.head(n=20)\nprint(df_gemler.shape)\ndf_gemler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load several datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nlist_datasets = []\nt00=time.time()\n\nfor i in range(45):\n  nm = df_gemler['name'].iloc[i]\n  print(nm, i )\n  did =  int( df_gemler['did'].iloc[i] )\n  t0 = time.time()\n  data = openml.datasets.get_dataset(did)\n  X, y, categorical_indicator, attribute_names = data.get_data(\n      dataset_format=\"array\", target=data.default_target_attribute\n  )\n  dict_dataset_data = {'X':X, 'y': y, 'name':nm , 'did':did}  \n  list_datasets.append(dict_dataset_data)\n  print(X.shape, y.shape, 'X size byets:', X.size*X.itemsize,  time.time()-t0,'secs passed' )\n\nprint(time.time()-t00,'seconds passed total' )\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare KNN to KNN+hubness reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat = pd.DataFrame()\nverbose = 0 \nt_previous_info_print = 0\ntimedelta4output_in_seconds = 3600\n\n\n    \nt00=time.time()\n\n#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n\n\nfor i,dict_dataset_data in enumerate(list_datasets):\n    X = dict_dataset_data['X']\n    y = dict_dataset_data['y']\n    nm = dict_dataset_data['name']\n    \n    cutoff4sample_size = 100_000 # To quickly test idea - put small cutoff\n    if cutoff4sample_size >= X.shape[0]: cutoff4sample_size = X.shape[0]\n\n    print(nm)\n    \n    t0=time.time()\n    df_stat.loc[i,'Dataset'] = nm\n    \n    t0=time.time()\n    from sklearn.model_selection import cross_val_score\n    from skhubness.neighbors import KNeighborsClassifier\n\n    # vanilla kNN\n    knn_standard = KNeighborsClassifier(n_neighbors=5,\n                                        metric='cosine')\n    acc_standard = cross_val_score(knn_standard, X[:cutoff4sample_size,:], y[:cutoff4sample_size], cv=5)\n    df_stat.loc[i,'Score'] = acc_standard.mean()\n\n    \n    # kNN with hubness reduction (mutual proximity)\n    knn_mp = KNeighborsClassifier(n_neighbors=5,\n                                  metric='cosine',\n                                  hubness='mutual_proximity')\n    acc_mp = cross_val_score(knn_mp, X[:cutoff4sample_size,:], y[:cutoff4sample_size], cv=5)\n    df_stat.loc[i,'Score Hub Reduced'] = acc_mp.mean()\n    \n    df_stat.loc[i,'Score Improve'] = acc_mp.mean() - acc_standard.mean()\n    \n    # Service things: \n    df_stat.loc[i,'%1 in target'] = np.round(y.sum()/len(y) * 100 , 1) \n    df_stat.loc[i,'Time (seconds)'] = np.round( -(t0-time.time() ) ,2)\n    df_stat.loc[i,'Sample size'] = cutoff4sample_size\n    \n    from skhubness import Hubness\n    hub = Hubness(k=5, metric='cosine')\n    hub.fit(X[:cutoff4sample_size,:])\n    k_skew = hub.score()\n    df_stat.loc[i,'Skewness'] = k_skew\n\n    #print(f'Skewness = {k_skew:.3f}')\n    #print(f'Robin hood index: {hub.robinhood_index:.3f}')\n    #print(f'Antihub occurrence: {hub.antihub_occurrence:.3f}')\n    #print(f'Hub occurrence: {hub.hub_occurrence:.3f}')\n\n    if verbose > 10:\n        print(f'Accuracy (vanilla kNN): {acc_standard.mean():.3f}')\n        print(f'Accuracy (kNN with hubness reduction): {acc_mp.mean():.3f}')\n        print( -(t0-time.time() ) , 'seconds passed')\n    if (time.time() - t_previous_info_print  ) > timedelta4output_in_seconds:\n        print(f'Processed {(i+1):d} datasets. Passed {time.time()-t00:.3f} seconds')\n        t_previous_info_print = time.time()\n        \n        \n        \nprint( -(t00-time.time() ) , 'total seconds passed')    \ndf_stat        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat['Score Improve'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(df_stat['Score Improve'],bins = 20, label = 'Score Improve')\nplt.legend()\nplt.grid()\nplt.show()\nplt.hist(df_stat['Score Improve'].iloc[:20],bins = 20, label = 'Score Improve Top 20 datasets')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.to_csv('df_stat.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize = (20,6))\nplt.plot(df_stat['Score Improve'], label = 'Score Improve')\nplt.legend()\nplt.grid()\nplt.show()\n\nplt.figure(figsize = (20,6))\nplt.plot(df_stat['Score'], label = 'Score')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stat.iloc[:50,:]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Plots "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport umap \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nt00=time.time()\n\nfor dict_dataset_data in list_datasets:\n    X = dict_dataset_data['X']\n    y = dict_dataset_data['y']\n    nm = dict_dataset_data['name']\n    \n    print(nm)\n    \n    t0=time.time()\n    fig = plt.figure(figsize = (20,4) )\n    fig.suptitle(nm +'\\n'+str(X.shape) )\n    c = 0;  n_x_subplots = 4\n\n    c += 1; fig.add_subplot(1,n_x_subplots,c)  \n    X2 = PCA().fit_transform(X)\n    plt.scatter(X2[:,0],X2[:,1]   , c = y )\n    plt.title('PCA')\n    plt.grid()\n    \n    c += 1; fig.add_subplot(1,n_x_subplots,c)  \n    X2 = PCA().fit_transform(scaler.fit_transform(X) ) \n    plt.scatter(X2[:,0],X2[:,1]   , c = y )\n    plt.title('StandardScaler+PCA')\n    plt.grid()\n\n    c += 1; fig.add_subplot(1,n_x_subplots,c)  \n    X2 = umap.UMAP().fit_transform(scaler.fit_transform(X) ) \n    plt.scatter(X2[:,0],X2[:,1]   , c = y )\n    plt.title('umap')\n    plt.grid()\n    \n    c += 1; fig.add_subplot(1,n_x_subplots,c)  \n    X2 = umap.UMAP().fit_transform(scaler.fit_transform(X) ) \n    plt.scatter(X2[:,0],X2[:,1]   , c = y )\n    plt.title('StandardScaler+umap')\n    plt.grid()\n    \n    print(time.time()-t0,'seconds passed')\n\nprint(time.time()-t00,'seconds passed total')\n\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}