{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/health-care-data-set-on-heart-attack-possibility/heart.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 1:Inspecting dataframe","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#listing categorical variables as per the data definition and converting them to category\ncatvar=['sex','cp','fbs','restecg','exang','ca','thal', 'slope']\ndf[catvar]=df[catvar].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### From above we see that there are no missing values and all the columns have correct datatype","metadata":{}},{"cell_type":"markdown","source":"### Step2:Data Preparation","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"2.1: Outlier Treatment","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Columns suspected to have outliers are restbps,chol,thalach,oldpeak","metadata":{}},{"cell_type":"code","source":"#create boxplots to inspect the presence of outliers\nnum_cols=['trestbps','chol','thalach','oldpeak']\n#  plot Numerical Data\na = 2  # number of rows\nb = 2  # number of columns\nc = 1  # initialize plot counter\n\nfig = plt.figure(figsize=(20,30))\nfor i in num_cols:\n    plt.subplot(a, b, c)\n    plt.title('{} (box)'.format(i, a, b, c))\n    plt.xlabel(i)\n    plt.boxplot(x = df[i])\n    c = c + 1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#function to remove outliers using IQR\ndef subset_by_iqr(df, column, whisker_width=1.5):\n   # Calculate Q1, Q2 and IQR\n    q1 = df[column].quantile(0.25)                 \n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    filter = (df[column] >= q1 - whisker_width*iqr) & (df[column] <= q3 + whisker_width*iqr)\n    return df.loc[filter]                                                     \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in num_cols:\n    df = subset_by_iqr(df, i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 :Dummy Variable Creation for categorical variables","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us perform one-hot encoding for these variables with multiple levels","metadata":{}},{"cell_type":"code","source":"#creating dummy variable and dropping the first one\ndf1=pd.get_dummies(df[['sex','cp','fbs','restecg','exang','ca','thal','slope']], drop_first=True)\n#adding results to original df\ndf=pd.concat([df,df1],axis=1)\ndf.head()\ndf.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping the repeated variables\ndf=df.drop(['sex','cp','fbs','restecg','exang','ca','thal','slope'],1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape\ndf.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3: Test-Train Split","metadata":{}},{"cell_type":"code","source":"#dropping target variable from X\nX=df.drop('target', 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#putting target variable to y\ny=df['target']\ny.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting data into train and test set\nX_train,X_test, y_train,y_test=train_test_split(X,y, train_size=0.7,test_size=0.3, random_state=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.4 :Feature Scaling","metadata":{}},{"cell_type":"code","source":"scaler=StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_col=['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\nX_train[numeric_col]=scaler.fit_transform(X_train[numeric_col])\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Checking the rate of heart attack\nrisk=(sum(df['target'])/len(df['target'].index))*100\nrisk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is almost 56% risk of suffering heart attack in the given dataset and this looks like a fairly balanced dataset","metadata":{}},{"cell_type":"markdown","source":"##### 2.5: Looking at correlations","metadata":{}},{"cell_type":"code","source":"#lets see the correlation matrix for the entire dataset\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Dropping highly correlated dummy variables","metadata":{}},{"cell_type":"code","source":"X_train=X_train.drop(['exang_1','thal_2','slope_2'],1)\nX_test=X_test.drop(['exang_1','thal_2','slope_2'],1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking the correlation matrix again","metadata":{}},{"cell_type":"markdown","source":"After dropping the highly correlated dummy variables let us again check the correlation matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(X_train.corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step3 :Model Building","metadata":{}},{"cell_type":"code","source":"#Logistic Regression Model\nlogm=sm.GLM(y_train,(sm.add_constant(X_train)), family=sm.families.Binomial())\nlogm.fit().summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step4: Feature Selection Using RFE","metadata":{}},{"cell_type":"code","source":"logreg=LogisticRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfe=RFE(logreg,13)\nrfe=rfe.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfe.support_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=X_train.columns[rfe.support_]\ncol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Assessing the model with statsmodels","metadata":{}},{"cell_type":"code","source":"X_train_sm=sm.add_constant(X_train[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logm1=sm.GLM(y_train, X_train_sm,famiily=sm.families.Binomial())\nres=logm1.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Checking VIF","metadata":{}},{"cell_type":"code","source":"vif=pd.DataFrame()\nvif['Features']=X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the variables have pretty low VIF and thus do not indicate multicollinearity. However the variable ca_4 has a significantly high p-value and hence can be dropped","metadata":{}},{"cell_type":"code","source":"col=col.drop('ca_4',1)\ncol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the variable thal_1 has a significantly high p-value and hence can be dropped","metadata":{}},{"cell_type":"code","source":"col=col.drop('thal_1',1)\ncol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the variable ca_3 has a significantly high p-value and hence can be dropped","metadata":{}},{"cell_type":"code","source":"col=col.drop('ca_3',1)\ncol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the variable restecg_1 has a significantly high p-value and hence can be dropped","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"col=col.drop('restecg_1',1)\ncol","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the p-values for all the variables are significantly low.Let us again check the VIF","metadata":{}},{"cell_type":"code","source":"vif=pd.DataFrame()\nvif['Features']=X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the p-values of all the variables are less than 0.05 and also the VIF are pretty low we can consider this model as final model and free from any multicollinearity","metadata":{}},{"cell_type":"markdown","source":"##### Predicting on training set","metadata":{}},{"cell_type":"code","source":"y_train_pred=res.predict(X_train_sm).values.reshape(-1)\ny_train_pred[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Creating a dataframe with actual risk probabilities and the predicted probabilities","metadata":{}},{"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'Risk':y_train.values, 'Risk_Prob':y_train_pred})\n\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Creating new column 'predicted' with 1 if Risk_Prob > 0.5 else 0","metadata":{}},{"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final.Risk_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step5:Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"##### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"confusion=metrics.confusion_matrix(y_train_pred_final.Risk,y_train_pred_final.predicted)\nprint(confusion)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Overall Accuracy of the model","metadata":{}},{"cell_type":"code","source":"print(metrics.accuracy_score(y_train_pred_final.Risk,y_train_pred_final.predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the problem at hand is to predict the probability of heart attack, we would like to maximise the sensitivity. We would not like to classify a person having high risk as one having low risk","metadata":{}},{"cell_type":"code","source":"TP=confusion[1,1]\nTN=confusion[0,0]\nFP=confusion[0,1]\nFN= confusion[1,0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Sensitivity","metadata":{}},{"cell_type":"code","source":"TP/float(TP+FN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Specificity","metadata":{}},{"cell_type":"code","source":"TN/float(TN+FP)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Clearly sensitivity is higher than specificity with our current model and threshold for prediction defined at 0.5","metadata":{}},{"cell_type":"markdown","source":"### Step6:Plotting the ROC Curve","metadata":{}},{"cell_type":"markdown","source":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","metadata":{}},{"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Risk, y_train_pred_final.Risk_Prob, drop_intermediate = False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_roc(y_train_pred_final.Risk, y_train_pred_final.Risk_Prob)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"94% of the area is under the ROC curve .","metadata":{}},{"cell_type":"markdown","source":"### Step7:Checking for Optimal Cut-off Point","metadata":{}},{"cell_type":"markdown","source":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","metadata":{}},{"cell_type":"code","source":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Risk_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Risk, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### From the above we see that cutoff of 0.5 which we had chosen earlier is the optimum cutof point","metadata":{}},{"cell_type":"markdown","source":"### Step8:Precision and Recall","metadata":{}},{"cell_type":"markdown","source":"##### Precision","metadata":{}},{"cell_type":"code","source":"precision_score(y_train_pred_final.Risk, y_train_pred_final.predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Recall","metadata":{}},{"cell_type":"code","source":"recall_score(y_train_pred_final.Risk, y_train_pred_final.predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step9:Making Predictions on the Test Set","metadata":{}},{"cell_type":"code","source":"X_test[numeric_col]=scaler.transform(X_test[numeric_col])\nX_test = X_test[col]\nX_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Making predictions on the test set","metadata":{}},{"cell_type":"code","source":"y_test_pred = res.predict(X_test_sm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Risk_Prob','target' : 'Risk'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's see the head of y_pred_final\ny_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final.Risk_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_pred_final.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Accuracy","metadata":{}},{"cell_type":"code","source":"metrics.accuracy_score(y_pred_final.Risk, y_pred_final.final_predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Confusion Matrix on Test Set","metadata":{}},{"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final.Risk, y_pred_final.final_predicted )\nconfusion2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Sensitivity","metadata":{}},{"cell_type":"code","source":"TP/float(TP+FN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Specificity","metadata":{}},{"cell_type":"code","source":"TN/float(TN+FP)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion","metadata":{}},{"cell_type":"markdown","source":"In the training set sensitivity is 89% and specificity is 85% <br>\nIn the test set sensistivity is 84% and specificity is 81%","metadata":{}},{"cell_type":"markdown","source":"The features that affect the probability of a heart attack are <br>\noldpeak,sex_1,cp_1,cp_2,cp_3,ca_1,ca_2,thal_3,slope_1","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}