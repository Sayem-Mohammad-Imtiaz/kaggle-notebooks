{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport math\nfrom tensorflow.keras import optimizers,regularizers\nfrom tensorflow.keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint,ReduceLROnPlateau\nimport  matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import Model\n\nfrom tensorflow.keras.models import load_model\nfrom  tensorflow.keras.layers import*\nif __name__ == '__main__':\n    # GPU settings\n    gpus= tf.config.experimental.list_physical_devices('GPU')\n\n    if gpus:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n\ndef round_filters(filters, multiplier):\n    depth_divisor = 8\n    min_depth = None\n    min_depth = min_depth or depth_divisor\n    filters = filters * multiplier\n    new_filters = max(min_depth, int(filters + depth_divisor / 2) // depth_divisor * depth_divisor)\n    if new_filters < 0.9 * filters:\n        new_filters += depth_divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, multiplier):\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\nclass SEBlock(tf.keras.layers.Layer):\n    def __init__(self, input_channels, ratio=0.25):\n        super(SEBlock, self).__init__()\n        self.num_reduced_filters = max(1, int(input_channels * ratio))\n        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n        self.reduce_conv = tf.keras.layers.Conv2D(filters=self.num_reduced_filters,\n                                                  kernel_size=(1, 1),\n                                                  strides=1,\n                                                  padding=\"same\")\n        self.expand_conv = tf.keras.layers.Conv2D(filters=input_channels,\n                                                  kernel_size=(1, 1),\n                                                  strides=1,\n                                                  padding=\"same\")\n\n    def call(self, inputs, **kwargs):\n        branch = self.pool(inputs)\n        branch = tf.expand_dims(input=branch, axis=1)\n        branch = tf.expand_dims(input=branch, axis=1)\n        branch = self.reduce_conv(branch)\n        branch = tf.nn.swish(branch)\n        branch = self.expand_conv(branch)\n        branch = tf.nn.sigmoid(branch)\n        output = inputs * branch\n        return output\n\n\nclass MBConv(tf.keras.layers.Layer):\n    def __init__(self, in_channels, out_channels, expansion_factor, stride, k, drop_connect_rate):\n        super(MBConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.stride = stride\n        self.drop_connect_rate = drop_connect_rate\n        self.conv1 = tf.keras.layers.Conv2D(filters=in_channels * expansion_factor,\n                                            kernel_size=(1, 1),\n                                            strides=1,\n                                            padding=\"same\",\n                                            use_bias=False)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.dwconv = tf.keras.layers.DepthwiseConv2D(kernel_size=(k, k),\n                                                      strides=stride,\n                                                      padding=\"same\",\n                                                      use_bias=False)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.se = SEBlock(input_channels=in_channels * expansion_factor)\n        self.conv2 = tf.keras.layers.Conv2D(filters=out_channels,\n                                            kernel_size=(1, 1),\n                                            strides=1,\n                                            padding=\"same\",\n                                            use_bias=False)\n        self.bn3 = tf.keras.layers.BatchNormalization()\n        self.dropout = tf.keras.layers.Dropout(rate=drop_connect_rate)\n\n    def call(self, inputs, training=None, **kwargs):\n        x = self.conv1(inputs)\n        x = self.bn1(x, training=training)\n        x = tf.nn.swish(x)\n        x = self.dwconv(x)\n        x = self.bn2(x, training=training)\n        x = self.se(x)\n        x = tf.nn.swish(x)\n        x = self.conv2(x)\n        x = self.bn3(x, training=training)\n        if self.stride == 1 and self.in_channels == self.out_channels:\n            if self.drop_connect_rate:\n                x = self.dropout(x, training=training)\n            x = tf.keras.layers.add([x, inputs])\n        return x\n\n\ndef build_mbconv_block(in_channels, out_channels, layers, stride, expansion_factor, k, drop_connect_rate):\n    block = tf.keras.Sequential()\n    for i in range(layers):\n        if i == 0:\n            block.add(MBConv(in_channels=in_channels,\n                             out_channels=out_channels,\n                             expansion_factor=expansion_factor,\n                             stride=stride,\n                             k=k,\n                             drop_connect_rate=drop_connect_rate))\n        else:\n            block.add(MBConv(in_channels=out_channels,\n                             out_channels=out_channels,\n                             expansion_factor=expansion_factor,\n                             stride=1,\n                             k=k,\n                             drop_connect_rate=drop_connect_rate))\n    return block\n\n\nclass EfficientNet(tf.keras.Model):\n    def __init__(self, width_coefficient, depth_coefficient, dropout_rate, drop_connect_rate=0.2):\n        super(EfficientNet, self).__init__()\n\n        self.conv1 = tf.keras.layers.Conv2D(filters=round_filters(32, width_coefficient),\n                                            kernel_size=(3, 3),\n                                            strides=2,\n                                            padding=\"same\",\n                                            use_bias=False)\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.block1 = build_mbconv_block(in_channels=round_filters(32, width_coefficient),\n                                         out_channels=round_filters(16, width_coefficient),\n                                         layers=round_repeats(1, depth_coefficient),\n                                         stride=1,\n                                         expansion_factor=1, k=3, drop_connect_rate=drop_connect_rate)\n        self.block2 = build_mbconv_block(in_channels=round_filters(16, width_coefficient),\n                                         out_channels=round_filters(24, width_coefficient),\n                                         layers=round_repeats(2, depth_coefficient),\n                                         stride=2,\n                                         expansion_factor=6, k=3, drop_connect_rate=drop_connect_rate)\n        self.block3 = build_mbconv_block(in_channels=round_filters(24, width_coefficient),\n                                         out_channels=round_filters(40, width_coefficient),\n                                         layers=round_repeats(2, depth_coefficient),\n                                         stride=2,\n                                         expansion_factor=6, k=5, drop_connect_rate=drop_connect_rate)\n        self.block4 = build_mbconv_block(in_channels=round_filters(40, width_coefficient),\n                                         out_channels=round_filters(80, width_coefficient),\n                                         layers=round_repeats(3, depth_coefficient),\n                                         stride=2,\n                                         expansion_factor=6, k=3, drop_connect_rate=drop_connect_rate)\n        self.block5 = build_mbconv_block(in_channels=round_filters(80, width_coefficient),\n                                         out_channels=round_filters(112, width_coefficient),\n                                         layers=round_repeats(3, depth_coefficient),\n                                         stride=1,\n                                         expansion_factor=6, k=5, drop_connect_rate=drop_connect_rate)\n        self.block6 = build_mbconv_block(in_channels=round_filters(112, width_coefficient),\n                                         out_channels=round_filters(192, width_coefficient),\n                                         layers=round_repeats(4, depth_coefficient),\n                                         stride=2,\n                                         expansion_factor=6, k=5, drop_connect_rate=drop_connect_rate)\n        self.block7 = build_mbconv_block(in_channels=round_filters(192, width_coefficient),\n                                         out_channels=round_filters(320, width_coefficient),\n                                         layers=round_repeats(1, depth_coefficient),\n                                         stride=1,\n                                         expansion_factor=6, k=3, drop_connect_rate=drop_connect_rate)\n\n        self.conv2 = tf.keras.layers.Conv2D(filters=round_filters(1280, width_coefficient),\n                                            kernel_size=(1, 1),\n                                            strides=1,\n                                            padding=\"same\",\n                                            use_bias=False)\n        self.conv3=tf.keras.layers.Conv2D(128, 1, strides=1, padding='same')\n        self.conv4=tf.keras.layers.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')\n        self.conv15 = tf.keras.layers.Conv2D(16, kernel_size=3, strides=1, padding='same', activation='relu')\n        self.conv5 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')\n        self.conv16 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')\n        self.conv6 = tf.keras.layers.Conv2D(64, kernel_size=1, strides=1, padding='same', activation='relu')\n        self.conv07  = tf.keras.layers.Conv2D(64, kernel_size=2, strides=1, padding='same', activation='relu')\n        self.conv7 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu')\n        self.conv8=tf.keras.layers.Conv2D(128, kernel_size=1, strides=1, padding='same', activation='relu')\n        self.conv18 = tf.keras.layers.Conv2D(128, kernel_size=1, strides=1, padding='same', activation='relu')\n        self.conv9 = tf.keras.layers.Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu')\n        self.conv19 = tf.keras.layers.Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu')\n        self.conv29 = tf.keras.layers.Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu')\n\n        self.conv10 = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu')\n        self.conv100 = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu')\n        self.conv11 = tf.keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='same', activation='relu')\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.pool = tf.keras.layers.GlobalAveragePooling2D()\n        self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n        self.fc = tf.keras.layers.Dense(units=100,\n                                        activation=tf.keras.activations.softmax)\n\n    def __call__(self, inputs, training=None, mask=None):\n        # model_1 = load_model('E:/python/tensorflow2.2/Basic_CNNs_TensorFlow2/logs/weights_500-0.0030.h5', compile=None)\n        #\n        # c5 = Model(inputs=model_1.input, outputs=model_1.get_layer('conv2d_2').output)\n        # c6 = c5(inputs)\n        x = self.conv1(inputs)\n        # x(112,112,32)\n        x = self.bn1(x, training=training)\n        x = tf.nn.swish(x)\n        # net = tf.nn.conv2d(inputs, kernel, strides=[1, 1, 1, 1], padding=\"SAME\")\n        net = self.conv4(inputs)\n        print('1111',net.shape)\n        net = self.conv15(net)\n        net = MaxPooling2D()(net)\n        x = Concatenate()([x, net])\n        # x(112,112,32)\n        x_1 = self.block1(x)\n        # x(112,112,32)\n        x_2 = self.block2(x_1)\n        # x(56,56,24)\n        net =  self.conv5(net)\n        net =  self.conv16(net)\n        net = MaxPooling2D()(net)\n        # print(net.shape)\n        # print(x_3.shape)\n        x_2= Concatenate()([x_2, net])\n        x_3 = self.block3(x_2)\n        # 28,28\n        net = self.conv6(net)\n        net = self.conv07(net)\n        net =  self.conv9(net)\n        net = MaxPooling2D()(net)\n        print(net.shape)\n        print(x_3.shape)\n        x_3 = Concatenate()([x_3, net])\n        x_4 = self.block4(x_3)\n        # 28,28\n\n        x_5 = self.block5(x_4)\n        # 28,28,112\n        net = self.conv8(net)\n        net = self.conv19(net)\n        net = self.conv10 (net)\n        net = MaxPooling2D()(net)\n        x_6 = Concatenate()([x_5, net])\n        x_6 = self.block6(x_6)\n        # 14,14,192\n        net = self.conv18(net)\n        net = self.conv29(net)\n        net =self.conv100(net)\n        net = MaxPooling2D()(net)\n\n\n        x_7 = self.block7(x_6)\n        x_7 = self.conv2(x_7)\n        print(x_7.shape)\n        x = self.bn2(x_7, training=training)\n        x = tf.nn.swish(x)\n        x = self.pool(x)\n        x = self.dropout(x, training=training)\n        print(x.shape)\n        out2=tf.keras.layers.GlobalAveragePooling2D()(x_6)\n        out3 = tf.keras.layers.GlobalAveragePooling2D()(x_4)\n        out= Concatenate()([x,out2,out3])\n        return out\n\n\ndef get_efficient_net(width_coefficient, depth_coefficient, resolution, dropout_rate):\n    x= EfficientNet(width_coefficient=width_coefficient,\n                       depth_coefficient=depth_coefficient,\n                       dropout_rate=dropout_rate)\n\n    return x\nmodel= get_efficient_net(1.0, 1.0, 224, 0.2)\nmodel = tf.keras.Sequential([model,\n                             tf.keras.layers.Flatten(),\n                             tf.keras.layers.Dense(512,activation='relu'),\n                             tf.keras.layers.Dropout(0.6),\n                            tf.keras.layers.Dense(200, activation='softmax')])\n\nmodel.build(input_shape=(None, 224, 224, 3))\nmodel.summary()\n\ntrain_dir='../input/bird200/train'\nvalidation_dir='../input/bird200/valid'\ntrain_datagen=ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n)\n\nvalidation_datagen=ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n)\n# test_datagent=ImageDataGenerator(rescale=1./255)\ntrain_generator=train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224,224),\n    batch_size=64,\n    class_mode='categorical',\n\n)\nvalidation_generator=validation_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(224,224),\n    batch_size=64,\n    class_mode='categorical'\n\n)\nmodel.compile(optimizer =optimizers.Adam(lr=0.0001), loss = 'categorical_crossentropy', metrics= ['accuracy'])\n#using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n# earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n# tensorboard=tf.keras.callbacks.TensorBoard(log_dir='logreffi2_dir',histogram_freq=1,embeddings_freq=1)\n#save the best model with lower validation loss\n# checkpointer = ModelCheckpoint(filepath=\"weight.hdf5\", verbose=1, save_best_only=True)\n\n# history = model.fit(train_generator, steps_per_epoch= train_generator.n // 32,\n#                     epochs =200, validation_data= validation_generator,\n#                     validation_steps= validation_generator.n // 32, callbacks=[earlystopping,tensorboard])\nearlystopping           = tf.keras.callbacks.EarlyStopping(\n    monitor = 'val_accuracy', \n    mode = 'max' , \n    patience = 5,\n    verbose = 1)\n\ntensorboard             = tf.keras.callbacks.TensorBoard(\n    log_dir='logseff_1_dir',\n    histogram_freq=1,\n    embeddings_freq=1)\n\nreduceonplateau         = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    mode='auto', \n    epsilon=0.0001, \n    cooldown=0, \n    min_lr=0,\n    factor=0.5,\n    patience=3)\nfilepath = './best_weights.hdf5'\ncheckpoint    = tf.keras.callbacks.ModelCheckpoint(filepath, \n                                monitor = 'val_accuracy', \n                                mode='max', \n                                save_best_only=True, \n                                verbose = 1)\ncallback=[earlystopping,tensorboard,checkpoint,reduceonplateau]\nhistory = model.fit(train_generator,\n                    epochs=3,callbacks=callback,steps_per_epoch=train_generator.n // 64,\n                    validation_data=validation_generator,validation_steps =validation_generator.n //64)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}