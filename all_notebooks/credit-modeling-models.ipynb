{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"application = pd.read_csv('../input/credit-card-approval-prediction/application_record.csv')\ncredit = pd.read_csv('../input/credit-card-approval-prediction/credit_record.csv')\n\n# only model in the intersection cases between 2 dataset\nids = set(application['ID']).intersection(set(credit['ID']))\napplication = application[application['ID'].isin(ids)]\ncredit = credit[credit['ID'].isin(ids)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **WOE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"OCCUPATION TYPE"},{"metadata":{},"cell_type":"markdown","source":"Because jobs in OCCUPATION_TYPE are generic, they will be less prone to overfit. Besides,dropping any value may cause a noticable loss of information."},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_occupation_type(application):\n    \n    x = application.copy()\n    \n    probability = x['OCCUPATION_TYPE'].value_counts().to_numpy()/x['OCCUPATION_TYPE'].value_counts().sum()\n    job_list = x['OCCUPATION_TYPE'].value_counts().index.to_numpy()\n    indexes = range(len(x['OCCUPATION_TYPE'].value_counts()))\n    null_size = len(x[x['OCCUPATION_TYPE'].isnull()]['OCCUPATION_TYPE'])\n    \n    random_index = np.random.choice(a=indexes, size=null_size, p=probability)\n    \n    x.loc[:,'IMPUTED_OCCUPATION_TYPE'] = 0\n    x.loc[x['OCCUPATION_TYPE'].isnull(),'IMPUTED_OCCUPATION_TYPE'] = 1\n    x.loc[x['OCCUPATION_TYPE'].isnull(),'OCCUPATION_TYPE'] = job_list[random_index]\n\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DAYS_EMPLOYED"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_unemployed_column(application):\n    x = application.copy()\n    \n    x.loc[x['DAYS_EMPLOYED']<=0,'UNEMPLOYED'] = 0 \n    x.loc[x['DAYS_EMPLOYED']>0,'UNEMPLOYED'] = 1 \n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform Skewed Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"before = application['CNT_FAM_MEMBERS'].skew()\nafter = np.log(application['CNT_FAM_MEMBERS']).skew()\nprint('Skewness coefficient')\nprint('CNT_FAM_MEMBERS ------')\nprint(f'Before: {before}')\nprint(f'After:  {after}')\n\nbefore = application['CNT_CHILDREN'].skew()\nafter = np.power(application['CNT_CHILDREN'],1/7).skew()\nprint('CNT_CHILDREN ------')\nprint(f'Before: {before}')\nprint(f'After:  {after}')\n\n# Only transform the ones < 0 (customers currently being employed)\nbefore = application.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED'].skew()\nafter = (-1*np.sqrt(-1*application.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED'])).skew()\nprint('DAYS_EMPLOYED ------')\nprint(f'Before: {before}')\nprint(f'After:  {after}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_skewed_data(application):\n    x = application.copy()\n    \n    x.loc[:,'CNT_FAM_MEMBERS'] = np.log(x['CNT_FAM_MEMBERS'])\n    x.loc[:,'CNT_CHILDREN'] = np.power(x['CNT_CHILDREN'],1/7)\n    x.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED']  = -1*np.sqrt(-1*x.loc[application['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED'])\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\ncategorical_columns = ['CODE_GENDER','FLAG_OWN_CAR', 'FLAG_OWN_REALTY','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE']\nencoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nencoder.fit(application[categorical_columns].dropna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(application):\n    x = application.copy().reset_index()\n    x = x.join(pd.DataFrame(encoder.transform(x[categorical_columns])))\n    x = x.drop(categorical_columns,axis=1)\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Labels**"},{"metadata":{},"cell_type":"markdown","source":"You may need to modify the function below for your way of classifying good/bad customers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this method reduced our dataset to 1/3 (36457 -> 15168)\ndef get_credit_status(credit):\n    group=credit.groupby('ID')\n    pivot_tb = credit.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\n    pivot_tb['open_month'] = group['MONTHS_BALANCE'].min()\n    pivot_tb['end_month'] = group['MONTHS_BALANCE'].max() \n    pivot_tb['ID'] = pivot_tb.index\n    pivot_tb = pivot_tb[['ID', 'open_month', 'end_month']]\n    pivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month'] \n    pivot_tb.reset_index(drop = True, inplace = True)\n    credit0 = credit.copy()\n    credit0 = pd.merge(credit0, pivot_tb, on = 'ID', how = 'left') \n    credit0=credit0[credit0['window']>15]\n    credit0['status']=np.where((credit0['STATUS']=='2')| (credit0['STATUS']=='3')|(credit0['STATUS']=='4')|(credit0['STATUS']=='5'),1,0)\n    \n    return credit0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Processing Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def oversampling(data, factor=40):\n    positive = data[data['status']==1]\n    \n    for i in range(factor):\n        data = data.append(positive)\n    \n    data = shuffle(data)\n    \n    return data\n\ndef downsampling(data, remove_amount=0):\n    negative = data[data['status']==0]\n    assert remove_amount>=0 and remove_amount < len(negative)\n    new_negative_len = len(negative) - remove_amount\n    \n    negative = shuffle(negative)\n    negative = negative.iloc[:new_negative_len,:]\n    \n    data = data[data['status']==1].append(negative)\n    data = shuffle(data)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_datasets(x_train, x_test, y_train, y_test, oversampling_factor=30, down_sampling_amount=0, transform_skewed = True):\n    \"\"\"\n    This function is a wrapper function for all of the preproccessing steps\n    \n    - x_train: unprocessed train application dataset\n    _ x_test: unprocessed test application dataset\n    - y_train: unprocessed train credit dataset\n    - y_test : unprocessed test credit dataset\n    - oversampling_factor:  oversample the positive cases by this factor (because the current label method only has 3% as positive)\n    - down_sampling_amount: remove this amount of negative cases to balance positive/negative cases\n    _ transform_skewed: if True, transform any skewed continuous data in the application datasset\n    \"\"\"\n    \n    # encode x\n    x_train = impute_occupation_type(x_train)\n    x_train = create_unemployed_column(x_train)\n    if transform_skewed:\n        x_train = transform_skewed_data(x_train)\n    x_train = encode(x_train)\n\n    x_test = impute_occupation_type(x_test)\n    x_test = create_unemployed_column(x_test)\n    if transform_skewed:\n        x_test = transform_skewed_data(x_test)\n    x_test = encode(x_test)\n    \n    # encode y\n    y_train = get_credit_status(y_train)[['ID','status']]\n    y_test = get_credit_status(y_test)[['ID','status']]\n\n    y_train = y_train.groupby('ID').any().reset_index()\n    y_test = y_test.groupby('ID').any().reset_index()\n    \n    # Merge x and y together to make sure the ids matches\n\n    merged_train = x_train.merge(y_train, on='ID')\n    merged_test = x_test.merge(y_test, on='ID')\n    \n    # oversampling\n    merged_train = oversampling(merged_train, factor=oversampling_factor)\n    merged_train = downsampling(merged_train, remove_amount=down_sampling_amount)\n    \n    x_train = merged_train.drop(['ID', 'status'],axis=1)\n    x_test = merged_test.drop(['ID', 'status'],axis=1)\n    y_train = merged_train['status']\n    y_test = merged_test['status']\n    \n    return x_train, x_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\ntrain_size = len(application)*80//100\ntest_size = len(application) - train_size\nfold_size = test_size\nprint(f'Train size: {train_size}, Test size: {test_size}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n\ndef cross_validation(model, application, credit, epochs=10, oversampling_factor=30, down_sampling_amount=0, transform_skewed = True, threshold=0.5):\n    \"\"\"\n    This function performs cross validation and acts as a wrapper function for preproccessing, fitting, and evaluating steps\n    \n    - model: machine learning model\n    - application: application dataset\n    - credit: credit dataset\n    - epochs: this only apply to the Neural Network, number of epochs to train\n    - oversampling_factor:  oversample the positive cases by this factor (because the current label method only has 3% as positive)\n    - down_sampling_amount: remove this amount of negative cases to balance positive/negative cases\n    _ transform_skewed: if True, transform any skewed continuous data in the application datasset\n    - threshold: this only apply to the Neural Network, threshold for the decision boundary\n    \"\"\"\n    application = shuffle(application)\n    total_acc = 0\n    total_f1 = 0\n    total_precision = 0\n    total_recall = 0\n    \n    for i in range(5):\n        x_train = application[:fold_size*i+1].append(application[fold_size*(i+1)-1:]).copy()\n        x_test = application[fold_size*i:fold_size*(i+1)].copy()\n\n        y_train = credit[credit['ID'].isin(x_train['ID'])].copy()\n        y_test = credit[credit['ID'].isin(x_test['ID'])].copy()\n    \n    \n        x_train, x_test, y_train, y_test = process_datasets(x_train, x_test, y_train, y_test, oversampling_factor=oversampling_factor, \n                                                            down_sampling_amount=down_sampling_amount, transform_skewed=transform_skewed)\n        \n        if str(type(model)) == \"<class 'tensorflow.python.keras.engine.sequential.Sequential'>\":\n            model.fit(x_train, y_train, epochs=epochs)\n        else:\n            model = model.fit(x_train, y_train)\n        \n        predictions = model.predict(x_test)\n        if str(type(model)) == \"<class 'tensorflow.python.keras.engine.sequential.Sequential'>\":\n            predictions = predictions > threshold\n        \n        total_acc = total_acc + accuracy_score(y_test, predictions)\n        total_f1 = total_f1 + f1_score(y_test, predictions)\n        total_precision = total_precision + precision_score(y_test, predictions)\n        total_recall = total_recall + recall_score(y_test,predictions)\n    \n    return total_acc/5, total_f1/5, total_precision/5, total_recall/5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Implement AUC_PR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Research SMOTE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train-Test Split**"},{"metadata":{},"cell_type":"markdown","source":"Regular Train-Test Split (This is used before Cross Validation to make it easier to tune the model)"},{"metadata":{"trusted":true},"cell_type":"code","source":"application = shuffle(application)\n\nx_train = application[:train_size].copy()\nx_test = application[train_size:].copy()\n\ny_train = credit[credit['ID'].isin(x_train['ID'])].copy()\ny_test = credit[credit['ID'].isin(x_test['ID'])].copy()\n\n\nx_train, x_test, y_train, y_test = process_datasets(x_train, x_test, y_train, y_test, oversampling_factor=30, down_sampling_amount=0, transform_skewed = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts() # this ratio is after oversampling/downsampling","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Models Selection**"},{"metadata":{},"cell_type":"markdown","source":"> **Logistic Regression**\n\nHyperparameter\n* max_iter = 400\n* threshold = 0.5\n* oversampling_factor = 30\n* down_sampling_amount = 0\n* transform_skewed = True"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(max_iter=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc, f1, precision, recall = cross_validation(lr_model, application, credit, oversampling_factor=30, down_sampling_amount=0, transform_skewed = True)\n\nprint(f'Acc: {acc}')\nprint(f'f1: {f1}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n# n_neighbors: number of neighbors nearby, weight: treat all equally, if 'distance' => treat the closest with greater influence\nmodelK = KNeighborsClassifier(n_neighbors = 2000, weights = 'uniform')\n\nacc, f1, precision, recall = cross_validation(modelK, application, credit)\n\nprint(f'Acc: {acc}')\nprint(f'f1: {f1}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\n\n\n# classify(modelK, x, y)\n\n# modelK.fit(x_train, y_train)\n#y_predK = modelK.predict(x_test)\n#cmK = confusion_matrix(y_test, y_predK)\n#cmK\n\n#sns.heatmap(cmK, annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \n#splitter: choose the best split, max_depth: more accuracy, but less generalized, features: number of feature to look for in the best split\nmodelTree = DecisionTreeClassifier(splitter='random', max_depth=300, max_features=10)\n\nacc, f1, precision, recall = cross_validation(modelTree, application, credit)\n\nprint(f'Acc: {acc}')\nprint(f'f1: {f1}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\n\n#classify(modelTree, x, y)\n\n#modelTree.fit(x_train, y_train)\n#from sklearn.metrics import confusion_matrix\n#y_predTree = modelTree.predict(x_test)\n#cmTree = confusion_matrix(y_test, y_predTree)\n#cmTree\n\n#sns.heatmap(cmTree, annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n#n_estimators: number of tress, max_depth: more accuracy, but less generalized, bootstrap: whether there are samples to use\n# if bootstrap = False, whole dataset will use to build each tree => Underfit\nmodelForest = RandomForestClassifier(n_estimators=100, max_depth=12, bootstrap = True)\n\nacc, f1, precision, recall = cross_validation(modelForest, application, credit)\n\nprint(f'Acc: {acc}')\nprint(f'f1: {f1}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\n\n\n#classify(modelForest, x, y)\n\n#modelForest.fit(x_train, y_train)\n#from sklearn.metrics import confusion_matrix\n#y_predForest = modelForest.predict(x_test)\n#cmForest = confusion_matrix(y_test, y_predForest)\n#cmForest\n\n#sns.heatmap(cmForest, annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Neuronetwork**\n\nHyperparameter\n* epochs = 10\n* threshold = 0.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nnn_model = keras.Sequential([layers.Dense(64, activation='relu'),\n                             layers.Dense(128, activation='relu'),\n                             layers.Dense(128, activation='relu'),\n                             layers.Dense(1, activation='sigmoid')])\n\nnn_model.compile(keras.optimizers.Adam(), keras.losses.BinaryCrossentropy(), metrics=[keras.metrics.BinaryAccuracy()])\n\nthreshold = 0.5\nepochs = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc, f1, precision, recall = cross_validation(nn_model, application, credit, epochs=epochs, threshold=threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Acc: {acc}')\nprint(f'f1: {f1}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}