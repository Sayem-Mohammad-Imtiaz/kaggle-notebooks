{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"![Paintings-museum](https://i.imgur.com/0ZQMAJ8.png)\n\nThe following Kernel looks into developing a paintings classifier based on [best-artworks-of-all-time](https://www.kaggle.com/ikarus777/best-artworks-of-all-time), a Kaggle public dataset with paintings and work of arts (drawings and sketches are included) from 50 of the best artists of all time. \n\nThe obvious aim of the classifier is to output the name of the artist that painted the input painting. However, I also believe that an important part of the Kernel can be found in the model interpretation part as we dig deeper into how the model predicted a certain output. The techniques used to evauate and visualize the results of the CNN are Heatmaps and Guided Backpropagation.\n\nIn order to train the model I have used fastai, a free open source library for deep learning that sits on top of Pytorch, as it provides really good out-of-the-box techniques for image classification. After various experiments, the model reached 90.2% accuracy. I am overall happy with the result but I believe there is room for improvement. Get in touch if you have any advice to do so!\n\nThis Kernel is divided into four parts:\n\n* Import Libraries and read data\n* Exploratory Data Analysis\n* Model And Learning\n* Model Evaluation and Visualization\n"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries and read data"},{"metadata":{},"cell_type":"markdown","source":"In this section we just read the data and import a few libraries that we are going to need."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from pathlib import Path\nfrom fastai.vision import *\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom fastai import basic_train\nfrom fastai.callbacks import *\nimport shap\nfrom fastai.callbacks.hooks import *\nfrom PIL import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path(\"../input/best-artworks-of-all-time\")\ndf = pd.read_csv(path/'artists.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the main nationalities and paintings' genres of the artists present in the datatset"},{"metadata":{"trusted":true},"cell_type":"code","source":"by_nationality = df[['nationality', 'paintings']].groupby(['nationality'], as_index = False).sum()\nnationality_top = by_nationality.sort_values('paintings', ascending = False)[:10]\n\ncount_names = df[['nationality', 'name']].groupby(['nationality']).count()\ncount_names = count_names.rename({'name' : 'number of artists'}, axis=1)\n\nnationality_top = nationality_top.join(count_names, on = 'nationality')\nprint(nationality_top)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"by_genre = df[['genre', 'paintings']].groupby(['genre'], as_index = False).sum()\ngenre_top = by_genre.sort_values('paintings', ascending = False)[:10]\n\ncount_names = df[['genre', 'name']].groupby(['genre']).count()\ncount_names = count_names.rename({'name' : 'number of artists'}, axis=1)\n\ngenre_top = genre_top.join(count_names, on = 'genre')\nprint(genre_top)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a list of 20 artists with the most number of paintings in the dataset. These names will be used as the model's precitions' classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"by_artist = df[['name', 'paintings']].groupby(['name'], as_index = False).sum()\nname_top = by_artist.sort_values('paintings', ascending = False)[:20]\nname_top","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also define the show_random_paintings() function. The function takes n_artist and n_paintings (default 4 and 4) as parameters and outputs a n_artist x n_paintings grid of random paintings and random artists (from the 20 selected artists)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set variables\n\nimages_dir = Path(path/'images/images')\nartists = name_top['name'].str.replace(' ', '_').values\nartists = np.delete(artists, 4)\n\n#let's give a look at some of these beautiful paintings !\n\ndef show_random_paintings(n_artists = 4, n_paintings = 4):\n  \n    fig, axes = plt.subplots(n_artists, n_paintings, figsize=(20,10))\n\n    for r in range(n_artists):\n        random_artist = random.choice(artists)\n        random_images = random.sample(os.listdir(os.path.join(images_dir, random_artist)), n_paintings)\n\n        c=0\n        for random_image in random_images:\n\n          random_image_file = os.path.join(images_dir, random_artist, random_image)\n          image = plt.imread(random_image_file)\n\n          axes[r, c].imshow(image)\n          axes[r, c].set_title(\"Artist: \" + random_artist.replace('_', ' '))\n          axes[r, c].axis('off')\n\n          c+=1\n\n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_random_paintings()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training And Learning"},{"metadata":{},"cell_type":"markdown","source":"Due to memory reason and convenience, the modelling and trainging part of the kernel has been done entirely on Google Colaboratory (Why google Colaboratory? Well, because it's free). You can look at the notebook [here](https://github.com/Attol8/paintings-classifier/blob/master/Paintings_Classifier_kaggle.ipynb). \n\nSome highlights from the modelling, learning and Trainging process:\n\n* The deep  Neural Network model trained on [best-artworks-of-all-time](https://www.kaggle.com/ikarus777/best-artworks-of-all-time) dataset uses `ResNet50` as the architecture\n* The model has been pretrained on Imagenet\n* fastai default [`get_transform`](https://www.kaggle.com/ikarus777/best-artworks-of-all-time) performs Data Augmentation on the datataset: flip, rotate, zoom, warp, lighting transforms are applied to the paintings \n* The final model is the result of many experiments with different architectures, learning rates, batch sizes and image sizes. \n* First, we train only the last group of layers, then (after using `learn.unfreeze()`) we use different learning rates for different groups of layers in the network (`max_lr=slice(1e-5,1e-4)`). This technique is called disicriminative learning \n* The biggest improvement in terms of accuracy seems to be yielded by setting image sizes to 256x256"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"artists","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"path = images_dir\npath\n!ls {path}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bs,size=32, 128\n#bs,size = 24,160\nbs,size = 24,256\narch = models.resnet50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path(\"../input/modelpaintings/valid.csv\")\nvalid_df = pd.read_csv(path)\nvalid_df = valid_df['0'].str.replace('data/images', '')\n\ni = 0\nfor vn in valid_df: \n    vn = os.path.basename(vn)\n    valid_df[i] = vn\n    i+=1\n    \nvalid_df.to_csv(\"../working/valid.csv\", index= False, index_label=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"valid_names = loadtxt_str(\"../working/valid.csv\")\nvalid_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"include = artists\npath=images_dir\n\nsrc = (ImageList.from_folder(path)\n.filter_by_folder(include=include)\n.filter_by_func(lambda fname: Path(fname).suffix == '.jpg')\n.split_by_fname_file(path = Path('../working/'), fname= 'valid.csv')\n.label_from_folder())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= (src.transform(get_transforms(max_zoom=2.), size=size)\n           .databunch(bs=bs).normalize(imagenet_stats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.classes)==len(artists)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dir = Path(\"/kaggle/working\")\nlearn = cnn_learner(data, arch, metrics=accuracy, model_dir=model_dir)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model_path = Path(\"../input\")\n!ls {model_path}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"model_path = Path(\"../input/modelpaintings/\")\ndest_path = model_dir\n!cp {model_path}/'best-2.pth' {dest_path}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!ls {dest_path}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model_path= Path(dest_path/'best-2')\nlearn.load(model_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\n\ndef validate_withtext():\n    val_loss, acc = learn.validate()\n    t = PrettyTable(['val_loss', 'accuracy'])\n    t.add_row([val_loss, round(float(acc), 6)])\n    return print(t) \n\nvalidate_withtext()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds,y,losses = learn.get_preds(with_loss=True)\ninterp = ClassificationInterpretation(learn, preds, y, losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_confusion_matrix(figsize=[20, 20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN Visualization - Heatmap"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"learn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx= np.random.randint(len(data.valid_ds))\nx,y = data.valid_ds[idx]\nx.show()\ndata.valid_ds.y[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = learn.model.eval();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"m[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xb, _ = data.one_item(x)\nxb_d, _ = data.one_item(x, denorm=True)\nxb_im = vision.Image(xb_d[0])\nxb = xb.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xb_im.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hooked_backward(cat=y):\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(xb)\n            preds[0,int(cat)].backward()\n    return hook_a,hook_g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hook_a,hook_g = hooked_backward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acts  = hook_a.stored[0].cpu()\nacts.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_acts = acts.mean(0)\navg_acts.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_heatmap(hm):\n    _,ax = plt.subplots()\n    xb_im.show(ax)\n    ax.imshow(hm, alpha=0.6, extent=(0,256,256,0),\n              interpolation='bilinear', cmap='magma');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_heatmap(avg_acts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More Visualization - Guided Backpropagation "},{"metadata":{},"cell_type":"markdown","source":"Guided backpropagation is a technique to vetter visualize and evaluate CNNs. Backpropagation is used to visualize which parts of the input picture most activate the output prediction. In order to do so, after a forward pass, when backpropagating the output, we set negative gradients to 0 (see clamp_gradients_hook function). This way we only keep positive gradients that corresponds to the parts of the network that highly activated the output prediction we are visualizing.\n\n![Imgur](https://i.imgur.com/hBCJR8W.png)\nKersner, M. (2018). CNN Visualization. [online] Available at: http://seoulai.com/presentations/CNN_Visualizations.pdf     "},{"metadata":{"trusted":true},"cell_type":"code","source":"![Paintings-museum](https://imgur.com/YQPExLw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(os.listdir(\"../usr/lib\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(os.listdir(\"../usr/lib/pytorch_cnn_visualization\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"print(os.listdir(\"../usr/lib/pytorch_cnn_visualization/src\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# python 3\n\nfrom src.misc_functions import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_get_example_params(example_index):\n    \n    \"\"\"\n        Gets used variables for almost all visualizations, like the image, model etc.\n\n    Args:\n        example_index (int): Image id to use from examples\n\n    returns:\n        original_image (numpy arr): Original image read from the file\n        prep_img (numpy_arr): Processed image\n        target_class (int): Target class for the image\n        file_name_to_export (string): File name to export the visualizations\n        pretrained_model(Pytorch model): Model to use for the operations\n    \"\"\"\n    prep_img, target_class = data.valid_ds[example_index]\n    file_name_to_export = str(target_class)\n    original_image = prep_img\n    original_image,_ = data.one_item(original_image, denorm=True)\n    original_image = vision.Image(original_image[0])\n    prep_img, _ = data.one_item(prep_img)\n    prep_img = prep_img.cuda()\n    # Define model\n    pretrained_model = m\n    return (original_image,\n            prep_img,\n            target_class,\n            file_name_to_export,\n            pretrained_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_example = np.random.randint(len(data.valid_ds))\n(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n    new_get_example_params(target_example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minmax_norm(x):\n    return (x - np.min(x))/(np.max(x) - np.min(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clamp_gradients_hook(module, grad_in, grad_out):\n    for grad in grad_in:\n        torch.clamp_(grad, min=0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hooked_ReLU(pretrained_model,prep_img,target_class):\n    relu_modules = [module[1] for module in m.named_modules() if str(module[1]) == \"ReLU(inplace)\"]\n    with callbacks.Hooks(relu_modules, clamp_gradients_hook, is_forward=False) as _:\n        preds = pretrained_model(prep_img)\n        preds[0,int(target_class)].backward()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def guided_backprop(pretrained_model,prep_img,target_class):\n    m = pretrained_model.eval();\n    prep_img.requires_grad_();\n    if not prep_img.grad is None:\n        prep_img.grad.zero_(); \n    hooked_ReLU(m,prep_img,target_class);\n    return prep_img.grad[0].cpu().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_backprop_image(pretrained_model,prep_img,target_class, show=True):\n    '''\n    Main function to get xb_grad for guided backprop\n    '''\n    m = pretrained_model.eval();\n    prep_img_grad = guided_backprop(pretrained_model,prep_img,target_class) # (3,256, 256). Gradient of the output w.r.t. input image       \n    #minmax norm the grad\n    prep_img_grad = minmax_norm(prep_img_grad)\n    \n    # multiply xb_grad and hmap_scaleup and switch axis\n    #xb_grad = np.einsum('ijk, jk->jki',xb_grad, hmap_scaleup) #(256,256,3)\n    prep_img_grad = np.transpose(prep_img_grad, (1,2,0))\n    if show == True:\n        return plt.imshow(prep_img_grad)\n    else:\n        return prep_img_grad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_backprop_image(pretrained_model,prep_img,target_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_GBP_and_input():\n\n    columns = 2\n    rows = 5\n    figsize = [20, 26]\n    \n    fig, ax = plt.subplots(nrows=rows, ncols=columns, figsize=figsize)\n    \n    for i in range(rows):\n        \n        #plot inputs\n        idx= np.random.randint(len(data.valid_ds))\n        (original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n        new_get_example_params(idx)\n        \n        x,y = data.valid_ds[idx]\n        x, _ = data.one_item(x, denorm=True)\n        x = x.cpu().numpy()\n        x = np.transpose(x[0], (1, 2, 0))\n        ax[i, 0].imshow(x)\n        ax[i, 0].set_title(F\"INPUT, {y}\")\n        \n        #plot gradients\n        ax[i, 1].imshow(get_backprop_image(pretrained_model,prep_img,target_class, show=False))\n        ax[i, 1].set_title(\"GBP\")\n\n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_GBP_and_input()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}