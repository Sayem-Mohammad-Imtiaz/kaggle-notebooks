{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **USA Cars Dataset: EDA and Model Implementation**\n\nIn this kernel, we will first explore the data and try to understand the relationships among variables. Then build a couple of machine learning models to predict the price of cars using the features provided in the dataset.\n\nLet's begin..."},{"metadata":{"id":"OcqVK3MN3w4M","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"id":"JxIojtCE31tv","outputId":"b9b88db2-61e0-463c-d6be-5f50b6ae1c3c","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/usa-cers-dataset/USA_cars_datasets.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"S7qoJ5Rl4_-e","outputId":"0b7d7646-2137-41d2-cbdd-23a84c36362e","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset includes 13 features of 2499 cars. Some of the features are important when determining the price of a car while some are redundant. For example, “vin”, “lot” and “Unnamed: 0” columns have no effect on the price. These three columns represent kind of an ID for a car. Since the dataset is taken from a website in US, I think the country column only includes “usa”. Let’s check:"},{"metadata":{"id":"15rwQCM9AGCK","outputId":"9d899328-ed05-49eb-e486-aaf5fa804d6d","trusted":true},"cell_type":"code","source":"df.country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"K1A_JkLC5bYx"},"cell_type":"markdown","source":"Not all but overwhelming majority is “usa” so there is no point in using “country” column as a feature in our model. So the columns that we will not use in the model are \"Unnamed, vin, lot, country\" so lets drop them."},{"metadata":{"id":"7pl_5y915BIE","outputId":"5ef7050d-e6b3-4314-e343-f551bdda77cc","trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 0','vin', 'lot','country'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"chCB4IHm5zhY","outputId":"2aec7026-e9c4-4291-c89e-4b728712bb8a","trusted":true},"cell_type":"code","source":"#missing values\ndf.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"1fxE7BIM55Vb","outputId":"e0644101-5bd0-492b-b4ac-3668ade7b333","trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"nxCOpfeN7QpA"},"cell_type":"markdown","source":"There is no missing value in the dataset and the datatypes seem appropriate.\n\nLet's start exploring the data.\n\nI always start with checking the target variable. Target variable is what we struggle to predict. At the end of the way, we will evaluate the model based on how close our predictions are to the target. So knowing the target variable well is a good practice. Let’s check the distribution of the target variable. We first need to import data visualization libraries that we will use during EDA process."},{"metadata":{"id":"jIAdaYKW7Omj","outputId":"43672e07-d05d-4ef7-8248-d455426fde4c","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Target Variable**"},{"metadata":{"id":"4kx633qu74__","outputId":"272bc5fb-1c49-41c9-e79d-d203cf4b6038","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nsns.distplot(df['price']).set_title('Distribution of Car Prices')","execution_count":null,"outputs":[]},{"metadata":{"id":"pqN2wMES8Gvf"},"cell_type":"markdown","source":"The price variable is not normally distributed. It is right-skewed meaning the tail of the distribution is longer on the right side than on the left side. Another way to prove skewness is comparing mean and median. If mean is higher than the median, the distribution is right-skewed. Mean is the average value and median is the value in the middle. So if mean is higher than the median, we have more samples on the upper side of the median."},{"metadata":{"id":"EBL-A5qg8CR4","outputId":"cbf6117b-d83a-442f-bbb4-00b4d31f4d86","trusted":true},"cell_type":"code","source":"print(\"Mean value of prices {}\".format(df.price.mean()))\nprint(\"Median value of prices {}\".format(df.price.median()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean is higher than median, as expected. So, there are more of expensive cars than of cheaper cars."},{"metadata":{},"cell_type":"markdown","source":"# **Brand and Model**"},{"metadata":{"id":"OlYZSl-GMFMF","outputId":"d4fe1a5b-61df-42b3-c7a0-9ddee59c7026","trusted":true},"cell_type":"code","source":"df.brand.value_counts().size","execution_count":null,"outputs":[]},{"metadata":{"id":"75tkkGicMPF9","outputId":"38b0ff30-e506-4a4a-a6bd-66f71bfc28b5","trusted":true},"cell_type":"code","source":"df.brand.value_counts()[:6].values.sum() / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 28 different brands in the dataset but almost 94% of the cars belong to six brands. Remaining 6% are distributed among 22 different brands. We can mark these 22 brands as other. We first create a list that includes these 22 brands and then use pandas replace function on “brand” column."},{"metadata":{"id":"cnaOXkxxOOEZ","outputId":"42bbf397-8804-43ea-913f-08a6d9179c77","trusted":true},"cell_type":"code","source":"other = df.brand.value_counts().index[6:]\nlen(other)","execution_count":null,"outputs":[]},{"metadata":{"id":"qXrRI24BOdKv","trusted":true},"cell_type":"code","source":"df.brand = df.brand.replace(other, 'other')","execution_count":null,"outputs":[]},{"metadata":{"id":"0gtPTkMDQwcp","outputId":"51bad66b-7164-401f-e95a-5f9e30b5e570","trusted":true},"cell_type":"code","source":"df.brand.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is time to check if how price changes according to different brands. Boxplot is an informative tool for this task. We can also use pandas groupby function to check average prices of each brand"},{"metadata":{"id":"0eq_Mwo_Rmdj","outputId":"e3355f25-43e4-4a7b-ab62-ce8b9229c31c","trusted":true},"cell_type":"code","source":"df[['price','brand']].groupby('brand').mean().sort_values(by='price', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nsns.set(style='darkgrid')\nsns.boxplot(x='brand', y='price', data=df).set_title(\"Price Distribution of Different Brands\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The line in the boxes shows the average price. Ford cars have the highest average price but please keep in mind that almost half of the cars in the dataset are ford. It might be useful when building and evaluating our model. There are also some extreme values (i.e. outliers) represented with block dots. We will handle outliers later on. The height of the boxplots represent how spread out the values are. The dataset includes more dodge cars than chevrolet cars but price of chevrolet cars are more spread out than dodge cars. Variety of models might be causing this spread out. Let’s check how many models each brand has:"},{"metadata":{"id":"n4_U40QBS4DZ","outputId":"4a1ae47a-d17f-4818-d68e-a5a40e23ca69","trusted":true},"cell_type":"code","source":"df[['brand','model']].groupby('brand').nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although there are more dodge cars than chevrolet cars, number of models of chevrolet is more than double of dodge which I think explains the difference in price range. Let’s again use boxplot to see price distribution depending on models taking \"Dodge\" brand as an example:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nsns.set(style='darkgrid')\nsns.boxplot(x='model', y='price', data=df[df.brand == 'dodge']).\\\n            set_title('Prices of Different Dodge Models')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Price range significantly changes depending on model.\n\nLet's check how many models we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.model.value_counts().size","execution_count":null,"outputs":[]},{"metadata":{"id":"Am2xEfZjsO0e","outputId":"a4f5210a-c408-4aee-c951-849b816a4f6f","trusted":true},"cell_type":"code","source":"df.model.value_counts()[:50].sum() / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 127 models in the entire dataset but 50 models cover almost 94% of cars so we can mark the remaining models as other."},{"metadata":{"id":"PLTPo6C6td63","outputId":"a5df9cea-77be-4154-9e8a-eaf8ec628374","trusted":true},"cell_type":"code","source":"other_models = df.model.value_counts().index[50:]\nlen(other_models)","execution_count":null,"outputs":[]},{"metadata":{"id":"PQZNI6APtmgJ","trusted":true},"cell_type":"code","source":"df.model = df.model.replace(other_models, 'other_models')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Title Status**"},{"metadata":{},"cell_type":"markdown","source":"Title status indicates whether a car is clean or salvage which is definitely a factor in determining the price. Let’s confirm:"},{"metadata":{"id":"2QJ2M5_7VdY0","outputId":"96f18304-c2d4-4e00-c7a4-3f4198ea0ca3","trusted":true},"cell_type":"code","source":"df[['price','title_status']].groupby('title_status').agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do not have many salvage cars in the dataset but it is a must-have feature with the remarkable price difference."},{"metadata":{},"cell_type":"markdown","source":"# **Color**"},{"metadata":{"id":"Gxtj01yxWewX","outputId":"919486ba-69fb-4c20-cb83-d624ef822578","trusted":true},"cell_type":"code","source":"df.color.value_counts().size","execution_count":null,"outputs":[]},{"metadata":{"id":"VzxH0Y7pWg5g","outputId":"25dda695-da31-4e66-afdc-72a814bf499a","trusted":true},"cell_type":"code","source":"df.color.value_counts()[:6].sum() / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 49 different colors but 90% of cars belong to 6 different colors. We can mark the remaining colors as other like we did in “brand” column."},{"metadata":{"id":"noucRUMuXlKI","outputId":"25bfcf4b-f10a-4a14-e0fc-59b93039b4bd","trusted":true},"cell_type":"code","source":"other_colors = df.color.value_counts().index[6:]\nlen(other_colors)","execution_count":null,"outputs":[]},{"metadata":{"id":"VLitwrFLZv_4","trusted":true},"cell_type":"code","source":"df.color = df.color.replace(other_colors, 'other_colors')","execution_count":null,"outputs":[]},{"metadata":{"id":"hd0dzxDKZ2J7","outputId":"0b939988-bf4d-45a6-999e-fc47855c2f74","trusted":true},"cell_type":"code","source":"df.color.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the effect of color on price using boxplots."},{"metadata":{"id":"Y0kGDvdlaHl5","outputId":"ecb7db8a-2c27-460a-9d38-180773abba21","trusted":true},"cell_type":"code","source":"sns.set(style='darkgrid')\nplt.figure(figsize=(12,8))\nsns.boxplot(x='color', y='price', data=df).set_title(\"Color vs Price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The image above tells us two important things:\n\n* Color is not a decisive factor for price. The mean and the amount of spread of price are not changing much for different colors. Thus, it is better not to include “color” as a feature in our model.\n\n* There are outliers and we somehow need to handle them. Otherwise, the model will try to capture them which prevents the model from generalizing well to the dataset.\n\nWe have some cars that have extremely high prices. These outliers are not grouped in a particular color. From the figure above, it seems like all of the outliers are above 45000. Let’s check how many outliers we have and decide if we can afford to drop them."},{"metadata":{"id":"DmHX4rQmfKLz","outputId":"d23b695d-90af-43df-ec28-b0e564aacbc4","trusted":true},"cell_type":"code","source":"len(df[df.price > 45000]) ","execution_count":null,"outputs":[]},{"metadata":{"id":"7tFqatx-gO-s","outputId":"679f000d-2a2e-4108-ae50-3d6e81c2bc76","trusted":true},"cell_type":"code","source":"len(df[df.price > 45000]) / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 102 cars above 45000 which makes up 4% of entire dataset. I think we can drop these samples. In some cases, 4% is too much to drop so it is up to you to decide how to mark and handle outliers. There is not a strict definition of outliers."},{"metadata":{"id":"wie6tJTuhUPD","outputId":"b66aa37a-595f-4dc3-eb7c-af98bc043efa","trusted":true},"cell_type":"code","source":"df = df[df.price < 45000]\nlen(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"S2PMYjtXhh1w","outputId":"71f01a46-1d74-435f-a700-7fd9fcad1265","trusted":true},"cell_type":"code","source":"sns.set(style='darkgrid')\nplt.figure(figsize=(8,6))\nsns.boxplot(y='price', data=df).set_title(\"Price Distribution Without Outliers\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Year and Mileage**"},{"metadata":{},"cell_type":"markdown","source":"Year and mileage definitely changes the price of car so we will use these features in the model. I prefer to convert the “year” column to “age” by substracting it from the current year."},{"metadata":{"id":"rdeon8Qyh3te","trusted":true},"cell_type":"code","source":"age = 2020 - df.iloc[:,3].values","execution_count":null,"outputs":[]},{"metadata":{"id":"0q23VtL4qCYn","trusted":true},"cell_type":"code","source":"df['age'] = age","execution_count":null,"outputs":[]},{"metadata":{"id":"nMpzvIYlizPD","trusted":true},"cell_type":"code","source":"df = df.drop(['year'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZY5-Keuxi7W7","outputId":"f12d94ab-2571-4aa8-87bb-215b7984933d","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Condition**"},{"metadata":{},"cell_type":"markdown","source":"Condition indicates time to the end of auction which I think may increase the demand and thus the price. However, we cannot use it in its current format. We should represent it with a proper data type which is integer showing number of minutes.\n\nThere are many way to do this task. Feel free to try different methods. I will do it as follows:\n\n1. Split condition column on spaces and expand to a new dataframe with three columns (e.g. 10 — days — left)\n2. Convert the second column to appropriate minutes values (e.g. replace days with 1440, hours with 60)\n3. Then multiply first column with the second column to get remaining time to auction end in minutes.\n\n**Note:** There are some rows in condition column with “Listing Expired”. For those rows, I will replace “Expired” with 0 and “Listing” with 1 so that the remaining time becomes zero."},{"metadata":{"id":"-V9eKryZkWVk","outputId":"56ef9320-ad13-47ad-b7a0-00fcecec2d11","trusted":true},"cell_type":"code","source":"df_condition = df.condition.str.split(' ', expand=True)\ndf_condition.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SrGymMqSpQ3R","outputId":"d368140c-4580-4882-f0b3-b1acd5427387","trusted":true},"cell_type":"code","source":"a = {'days':1440, 'hours':60, 'minutes':1, 'Expired':0}\n\ndf_condition[1] = df_condition[1].replace(a)\ndf_condition[0] = df_condition[0].replace('Listing',1)\n\n#convert to numeric and multiply\ndf_condition[0] = pd.to_numeric(df_condition[0]) \ndf_condition['time'] = df_condition[0] * df_condition[1]\n\n#create a new column in the original dataframe\ndf['time_left'] = df_condition['time']","execution_count":null,"outputs":[]},{"metadata":{"id":"3U3m-ZQTnPYF","outputId":"3dd3bc08-91c7-4609-cbf0-b01d8496c781","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Correlation Matrix**"},{"metadata":{},"cell_type":"markdown","source":"Another useful tool in EDA process is correlation matrix which can be used to find the correlations among continuous features. The **corr()** method can be applied on dataframe and the results can also be visualized using a **heatmap**."},{"metadata":{"id":"IsCxjaEGuwsp","outputId":"1174ae85-3142-4413-fef6-16bf32910853","trusted":true},"cell_type":"code","source":"corr = df[['price','mileage','age', 'time_left']].corr()\n\nplt.figure(figsize=(10,6))\nsns.heatmap(corr, cmap='Blues').set_title('Correlation Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like price is not correlated with time_left but there is significant negative correlation between price and age or mileage. This is expected because older cars are cheaper."},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"**Encoding**\n\nWe have categorical features such as brand and color. These features need to represented in a format that a model can process. We cannot just input strings in a model. We first need to convert the categories into numbers (**Label Encoding**). If a categorical variable is not ordinal (i.e. there is not a hierarchical order in them), it is not enough to just do label encoding. We need to represent the categories with binary columns. This can be done using \"**dummy**\" or \"**one hot**\" encoding. For example, we may “Dodge” label as 3 and “GMC” as 1. If we just do label encoding on not ordinal categorical variables, a model would think category 3 is somewhat more important than category 1 which is not true. One Hot Encoding represents each category as a column which only take two values, 1 and 0. For a car with “Dodge” brand, only the value in “Dodge” column becomes 1 and other columns are 0. In this way, we make sure there is not a hierarchy among categories.\n\n**Normalization**\n\nWe also need to normalize the numerical data so that the values are in the same range. Otherwise, the model might give more importance to the higher values. For example, the values in mileage column are much higher than the values in age column. We can normalize these values in the range of [0,1] so that the maximum values for each column becomes 1 and the minimum becomes zero."},{"metadata":{},"cell_type":"markdown","source":"Since the categorical variables in our dataset are not ordinal, label encoding is not enough. Therefore, we can directly apply pandas get_dummies function to do represent categorical variables with columns assigned to each category."},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_variables = pd.get_dummies(df[['brand','model','title_status']], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_variables.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_variables.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to normalize numerical variables."},{"metadata":{"id":"MOawiHId4a31","outputId":"00ca9bbf-715e-430d-92e9-95a0f9815c1d","trusted":true},"cell_type":"code","source":"num_features = df[['mileage', 'age', 'time_left', 'price']]\n\nsc = MinMaxScaler()\nnum_features = sc.fit_transform(num_features)\n\nnum_features[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need combine encoded categorical variables and normalized numerical variables to comprise the data that we will use in the model.\n\nThan we separate target variable (price) from independent variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = dummy_variables.values\ncat_features.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"0HQVauu-5IOY","outputId":"cbcab6eb-6e31-4a3a-c534-6e799028ab16","trusted":true},"cell_type":"code","source":"data = np.concatenate((cat_features, num_features), axis=1)\n\nX = data[:, :data.shape[1]-1]\ny = data[:, data.shape[1]-1]\n\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Traning and Test Sets**\n\nIn predictive analytics, we build machine learning models to make predictions on new, previously unseen samples. The whole purpose is to be able to predict the unknown. But the models cannot just make predictions out of the blue. We show some samples to the model and train it. Then we expect the model to make predictions on samples from the same distribution. In order to train the model and then test it, we need to divide the dataset into two subsets. One is training and the other one is test.\n\nThe separation of training and test set can easily be done using train_test_split function of scikit-learn."},{"metadata":{"id":"5Sf5Dtma6AXW","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use 80% of data in training and remaining 20% to test the model."},{"metadata":{},"cell_type":"markdown","source":"# **Building and Evaluating The Model**"},{"metadata":{},"cell_type":"markdown","source":"I will implement two different models:\n\n* Ridge Regression: A variation of linear regression with L2 regularization. Regularization adds penalty for higher terms in the model and thus controls the model complexity. If a regularization terms is added, the model tries to minimize both loss and complexity of model. Limiting the complexity prevents the model from overfitting.\n\n* Gradient Boosting Regressor: An ensemble method that combines many decision trees using boosting. This is a more advance model than ridge regression."},{"metadata":{"id":"uj108-_RyISJ"},"cell_type":"markdown","source":"# **Ridge regression**"},{"metadata":{"id":"UGk46zmPyG7Q","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"id":"zXxA-V-TyYoq","outputId":"13922514-46e0-431b-895d-4eca10018936","trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha=0.5)\n\n#Train the model\nridge.fit(X_train, y_train)\n\n#Evaluate the model\nprint('R-squared score (training): {:.3f}'\n     .format(ridge.score(X_train, y_train)))\n\nprint('R-squared score (test): {:.3f}'\n     .format(ridge.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R-squared is a regression score function. It measures how much of the variation in the target variables is explained by the independent variables. The closer the R-squared value is to 1, the better the performance of model. R-squared scores of training and test sets are very close so the model is not overfitting. It seems like regularization works well. However, the performance of the model in terms of prediction power is not good enough because R-squared scores are not close to 1."},{"metadata":{},"cell_type":"markdown","source":"# **Gradient Boosting Regressor**"},{"metadata":{"id":"m9F54m0O1SsI","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"id":"NxfWOnJC1Uoh","outputId":"8bdac6f2-dc8d-49cd-babe-b51186071d12","trusted":true},"cell_type":"code","source":"params = {'n_estimators': 600, 'max_depth': 5,\n          'learning_rate': 0.02, 'loss': 'ls'}\ngbr = GradientBoostingRegressor(**params)\n\n#Train the model\ngbr.fit(X_train, y_train)\n\n#Evaluate the model\nprint('R-squared score (training): {:.3f}'\n     .format(gbr.score(X_train, y_train)))\n\nprint('R-squared score (test): {:.3f}'\n     .format(gbr.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the results show, there is significant increase in R-squared score.\n\nThere is a slight difference between the values of training and test sets but I think this is acceptable with the data we have. We can create a model that has an R-squared value very close to 1 on training set but the performance on test set will be very low.\n\nThe main reason is the limited data. For complex models, we normally need lots of data to build a decent and robust model. The categorical variables in the dataset have many categories but not enough data for each category. Therefore, the first thing we should do to improve the model is to look for more data.\n\nAnother way to improve the performance is hyperparameter tuning. Hyperparameters define the properties of a model that we can adjust. For example, I tried different values for the following hyperparameters of gradient boosting regressor:\n\n* n_estimators: The number of boosting stages to perform.\n* max_depth: Maximum depth of the individual regression trees.\n* learning_rate: The contribution of each tree is shrinked by the learning rate.\n\nThere is no strict rule to determine optimum values for these parameters. We need to find the right balance. Feel free to try different values for these parameters and see how the performance changes. Please do not limit yourself to what we did in exploratory data anaysis section. You can always dig deeper to explore the dataset to find out the correlations and underlying structure of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"auction-car-prices.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}