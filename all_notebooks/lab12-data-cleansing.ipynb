{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"Material came from [here](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/?utm_medium=email&utm_source=mailchimp&utm_campaign=5DDC-data-cleaning). Days 1 to 5 were all used. Loading modules, reading data, setting seed to make things reproducible."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom mlxtend.preprocessing import minmax_scaling\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime\nimport chardet\nimport fuzzywuzzy\nfrom fuzzywuzzy import process\nnfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")\nsf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")\nearthquakes = pd.read_csv(\"../input/earthquake-database/database.csv\")\nlandslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")\nvolcanos = pd.read_csv(\"../input/volcanic-eruptions/database.csv\")\nnp.random.seed(0) ","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"05c95b0b-5e5f-4829-944a-66e29f7ed379","_uuid":"e80b21b58f8a3350b6dc4a381731062718c0632f"},"cell_type":"markdown","source":"You can check a sample of the data to check for missing data."},{"metadata":{"_cell_guid":"ad092906-bef8-485f-8538-2f5f5de523ad","_uuid":"dfbae956b8cc2a40f8baad3dbfa738e3d8fd30c7","trusted":true},"cell_type":"code","source":"nfl_data.sample(5)","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"da93b432-4478-457b-b315-8019c3ca28b4","_uuid":"b42cf78ec214fe566a20af3748646ab913c5c0a6"},"cell_type":"markdown","source":"Now we will try to quantify how much missing data there is. `isnull()` returns `True` for missing data. `sum()` sums the boolean as 0 and 1 for each column. We only check the results for the first ten columns."},{"metadata":{"_cell_guid":"3b25f56f-b3e4-4424-8dbe-320befeb6a83","_uuid":"b74be19543757e91c5c0d0bd0fd3b1ec133b5501","trusted":true},"cell_type":"code","source":"missing_values_count = nfl_data.isnull().sum()\nmissing_values_count[0:10]","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"f6dfea08-72db-4261-989b-7e85d92aefb6","_uuid":"60cdff0449bb50394ddd10b393dd85c34126c40b"},"cell_type":"markdown","source":"Let us check what proportion of our data is missing. `np.product` multiplies the elements of a list. `sum()` again sums the sum for each column. Giving the number as a percentage."},{"metadata":{"_cell_guid":"1e7cd2c8-6836-441e-9de0-15a93c7d5273","_uuid":"144d4f9e43581b15e46088d62ed7af6ba5e29d6d","trusted":true},"cell_type":"code","source":"total_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()\n(total_missing/total_cells) * 100","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"776ffa22-8c9c-4795-8b44-73bb0b8a7b31","_uuid":"540fcebd664799cdddb7f907b7f18fecb141c16e"},"cell_type":"markdown","source":"Let's try removing the missing data."},{"metadata":{"_cell_guid":"df006ebd-b2b1-4f29-bb1c-64be91791c01","_uuid":"fd078160c46847fa65ff0eaa95a5b93f1ab0af8a","trusted":true},"cell_type":"code","source":"nfl_data.dropna()","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"ddfd9503-a0bf-4e80-9031-275dcfa8efa3","_uuid":"3b032ecac6bc728063a6e3045239159e20a57302"},"cell_type":"markdown","source":"Because every row has some missing data, everything was removed. Instead let us remove columns that have missing data. "},{"metadata":{"_cell_guid":"eda43fde-c8b8-4baa-946f-129c47a044b8","_uuid":"0ae24c4366fc7839caadd80120ac9012eb3efd37","trusted":true},"cell_type":"code","source":"columns_with_na_dropped = nfl_data.dropna(axis=1)\nprint(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])\ncolumns_with_na_dropped.head()","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"48893b73-4ced-42ab-8cfb-656a203e5808","_uuid":"63bfbf43fe2c3154522ebf19fc07c92763716f31"},"cell_type":"markdown","source":"We will take a subset of the data to see how to perform imputation for missing data. One option is to replace by 0."},{"metadata":{"_cell_guid":"4ec685c7-88d4-44d2-9318-06b89204c8b3","_uuid":"b25422bdaad4d482b49dfb71f0056257cf10c3a7","trusted":true},"cell_type":"code","source":"subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\nsubset_nfl_data","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"048408d945de311ea72b6c7176a1f7a2dc48f740"},"cell_type":"code","source":"subset_nfl_data.fillna(0)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"b8077d4d-e4e5-4270-8b86-d39da788c9dc","_uuid":"fbc284b78d96c2a2c18c8730fbe1792a32451530"},"cell_type":"markdown","source":"You can use adjacent values for the missing values. `method` spcifices how to do this. `bfill` propagates the non-null value backwards to fill in the missing value. This is done column wise. Of course if the last element of the column is missing, we cannot back propagate. So this will be left as a missing value but by running the `fillna(0)` again it changes it to 0."},{"metadata":{"_cell_guid":"26636bbb-97f3-488e-b917-768f60039da0","_uuid":"fb0f1bb1627c4a2ab21e8e9221fa6d7cbcf3b750","trusted":true},"cell_type":"code","source":"subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(0)","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"581d8889-5bf8-41be-b136-9e306a5bd426","_uuid":"6eb7ed141e0951fcee93cb7011963e03903a564f","collapsed":true},"cell_type":"markdown","source":"Now we will look at scaling the data. This helps when you want to compare two different random variables on features other than scale."},{"metadata":{"_cell_guid":"027aa93f-12c7-4e0a-a3ff-0f4031884bbc","_uuid":"33f9adfe4c0f9989936a2ae5fb6b40016ffef3c7","trusted":true},"cell_type":"code","source":"original_data = np.random.exponential(size = 1000)\nscaled_data = minmax_scaling(original_data, columns = [0])\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")\nplt.show()","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"60167859-d1e1-4e13-a12d-786ed2c71d29","_uuid":"2b776c734371fc43019f089b1cc81e30d6e8badf"},"cell_type":"markdown","source":"Now we will try to transform a non-normal distribution to a Normal distribution by a Box-Cox transformation. Box-Cox transformation basically tries to power each observation by some number to make it more normal."},{"metadata":{"_cell_guid":"fc38f5ab-5c13-4d9b-93c4-24094fdd280f","_uuid":"7fdb2a59ffc3dbae88a8a71397d21733289868e1","scrolled":true,"trusted":true},"cell_type":"code","source":"normalized_data = stats.boxcox(original_data)\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_data[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")\nplt.show()","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"078e820e-94c5-4fcd-82b8-3fbfc5157e5b","_uuid":"a47610749d62fd8fe6bc194442d083cf4168add0","collapsed":true},"cell_type":"markdown","source":"Now we will look at how to work with date data. The data indeed looks like dates but the data type is `object`.  `dtype('O')` is also just signifying that it is an object. This tells us that the data is not being recognized as dates."},{"metadata":{"_cell_guid":"57539bb5-4cd4-4dcc-8784-fd60a21d7d29","_uuid":"d173fb06a1d18d0ee4ca93ef1d8f99bdb02a38ce","trusted":true},"cell_type":"code","source":"print(landslides['date'].head())\nlandslides['date'].dtype","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"4c0a19ba-16d9-46b8-968d-cb6ccd84c458","_uuid":"8329ed51f38cc3dbeae585dcdfc6ccfff7d6c15d"},"cell_type":"markdown","source":"We can convert to a datetime object by using the `to_datetime()` function. You would need to specify where the month, day, and year data appears. Adding `infer_datetime_format=True` will help you parse data that has multiple differing formats but it will not always work and it will make your program slower."},{"metadata":{"_cell_guid":"36329359-20ce-4340-a515-f2710d671f61","_uuid":"5a4937a08a445d6e47eca5481877ba095e4e0fe2","trusted":true},"cell_type":"code","source":"landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")\nlandslides['date_parsed'].head()","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"58061115-718f-4747-a897-9d681c05b1c2","_uuid":"8a0063bbd70ad7eb47b1b66eb1f6800e195029c1"},"cell_type":"markdown","source":"Now with the parsed data, you can retrieve the day information for example."},{"metadata":{"_cell_guid":"8748ab41-c0bd-4489-b63c-c504b1896f7d","_uuid":"d1bc68b3903b8deb6da4c6e72486e6c579ff444d","trusted":true},"cell_type":"code","source":"day_of_month_landslides = landslides['date_parsed'].dt.day\nday_of_month_landslides.head()","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"d3906cca-e334-42e9-9da8-e8eb9c27c75a","_uuid":"93a5ff53d5dae432d4a7187637a9544715a8c3a7"},"cell_type":"markdown","source":"Let's see on what day of the month landslides happen often."},{"metadata":{"_cell_guid":"4c77e9a9-910b-4479-b996-7b00b66cef51","_uuid":"3e14ec401d27227838145259f2d5fc1752b119bc","trusted":true},"cell_type":"code","source":"day_of_month_landslides = day_of_month_landslides.dropna()\nsns.distplot(day_of_month_landslides, kde=False, bins=31)\nplt.show()","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"ecac2795-1f60-4bf3-a8cb-327ca4a7b51b","_uuid":"21ce4664a98b085648034f0f1e62c30b38026d35"},"cell_type":"markdown","source":"Now we will talk about encodings. Basically if you are using UTF-8 you will be fine. We will first look at a string data (`str`). "},{"metadata":{"_cell_guid":"e8796927-41c4-4439-979b-267b38909806","_uuid":"47f3238c7079e9a34f1a4babb3f8ba7e6b61d633","trusted":true},"cell_type":"code","source":"before = \"This is the euro symbol: €\"\ntype(before)","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"91532de8-14f2-4df6-aefc-150394707977","_uuid":"21a93f4c7e6fbbf540f2aa0a254521df84656929"},"cell_type":"markdown","source":"You can also look at the string as a sequence of numbers. `errors=\"replace\"` just tries to find replacement characters for ones that cannot be encoded with the encoding used."},{"metadata":{"_cell_guid":"db5dfacd-7f68-43b9-9fe2-d602abb9a250","_uuid":"f7157156952c5bea24a75e10302a94e0b5cc07c1","trusted":true},"cell_type":"code","source":"after = before.encode(\"utf-8\", errors = \"replace\")\ntype(after)","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"20b72600a1492afc3d23686ed5b8b70db04302ff"},"cell_type":"markdown","source":"When you print it, it will try to show it as a string with ASCII encoding. But because we encoded it in utf-8, its not going to succeed perfectly."},{"metadata":{"_cell_guid":"9bdaa0fb-aede-4d27-95a4-a7423a5c1970","_uuid":"1d5a4c682ab9e0c33965a013b28593c22da3c9fb","trusted":true},"cell_type":"code","source":"after","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"f8734a595f83c0adfb70dd64c6a5ed2e968dbf50"},"cell_type":"markdown","source":"If you decode the byte data using utf-8, it will work fine. If you decode with ASCII, you will get an error."},{"metadata":{"_cell_guid":"e75b9b83-0568-42d3-97eb-a26863992c2e","_uuid":"9fa7768966eda01fc8841a730253c2e6e94fdd44","scrolled":true,"trusted":true},"cell_type":"code","source":"print(after.decode(\"utf-8\"))\n#print(after.decode(\"ascii\"))","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"12a32bc11cd63d19fd3c86b2797e2dafa99de7c3"},"cell_type":"markdown","source":"ASCII cannot encode €, so if we encode it with ASCII, it will replace it with some other character. And when we decode it back with ASCII, the original € will have been swapped with the replacement."},{"metadata":{"_cell_guid":"5716fb7d-647c-400b-9412-6edccd116fa4","_uuid":"c06641dec0de91eacb61e2ab3cf46f90533dfb99","trusted":true},"cell_type":"code","source":"before = \"This is the euro symbol: €\"\nafter = before.encode(\"ascii\", errors = \"replace\")\nprint(after.decode(\"ascii\"))","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"956d6424a23e202e71f55278397a071f7da5a4b7"},"cell_type":"markdown","source":"In below, just reading `ks-projects-201612.csv` will give you an error as by default the decoding used is utf-8, while this particular file was decoded with some other encoding. To find out what this encoding is, one can use `chardet.detect`. `rb` just means it opens the file to read a binary. This is read as `rawdata`, and we look at the first 10000 bytes of it. It thinks that with 73% confidence, the encoding is Windows-1252."},{"metadata":{"_cell_guid":"2db82b96-5cff-4e6f-8bc0-a2061ff0288a","_uuid":"d0277933933843a89b5433a144934a98a75b8d39","trusted":true},"cell_type":"code","source":"#kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")\nwith open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(10000))\nprint(result)","execution_count":44,"outputs":[]},{"metadata":{"_cell_guid":"cd92f813-f49c-4c68-8bb5-225d1c0f6c51","_uuid":"ce0229f3a253de2a53dc6bb8e19f214914bd7bff","trusted":true},"cell_type":"code","source":"kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\nkickstarter_2016.head()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"b5eec43f4fda9b8b28fa7b5e69c57271571ebde5"},"cell_type":"markdown","source":"One may save the data using the `to_csv()` function. You can download it from the Output tab (the one you see before hitting the \"Fork Notebook\"). "},{"metadata":{"_cell_guid":"ecb8d3d4-928d-4384-b564-f5da8e6e53c2","_uuid":"1b7e6b3c5e05331a4e5229bde626d3aa6ca3d134","trusted":true,"collapsed":true},"cell_type":"code","source":"kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"f8de72343d268c6bd96acce4430cda421db8bd92"},"cell_type":"markdown","source":"Now let us take a look at the Pakistan suicide attacks data. Again we try to find its encoding."},{"metadata":{"trusted":true,"_uuid":"080c70db08e8144e9a0860397cac5a098bf9c478"},"cell_type":"code","source":"with open(\"../input/pakistansuicideattacks/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nprint(result)","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"a2b919b59a63fae7f2b028f38c241dca812bd0b8"},"cell_type":"markdown","source":"Using the found encoding we read in the data. Let us see what kind of data is in the City column. There are some that look really similar, for example ATTOCK and Attock. "},{"metadata":{"trusted":true,"_uuid":"147a97d785ee13e9654997a101673254e79785b2"},"cell_type":"code","source":"suicide_attacks = pd.read_csv(\"../input/pakistansuicideattacks/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", \n                              encoding='Windows-1252')\ncities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"4060ba886ff3750de316ce741b637a68d5649562"},"cell_type":"markdown","source":"We wish to unify notations for referals to the same city. For a start we can make all characters lower case and remove the space in front of and after the string. Now we don't have the split in notation between ATTOCK and Attock, they both became attock. Other splits has also been resolved. But there are still some notations that seem to be referring to the same city. For example d. i khan and d.i khan. "},{"metadata":{"trusted":true,"_uuid":"fdd0b8f05afa137b0f644cb087a8da67346c2f24"},"cell_type":"code","source":"suicide_attacks['City'] = suicide_attacks['City'].str.lower()\nsuicide_attacks['City'] = suicide_attacks['City'].str.strip()\ncities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"fdd495662d9fe5964dff4a2e3df5bd82b7d58ee9"},"cell_type":"markdown","source":"To remove these kind of diversions, we may use the `fuzzywuzzy` module. It tries to quantify how similar strings are with eachother. For example if there are more replacements or removals required, the two strings would be considered more dissimilar. A 100 is assigned for similar characters and a 0 is assigned for dissimilar characters. We find the 10 strings in `cities` most similar to d.i khan."},{"metadata":{"trusted":true,"_uuid":"f014e8e97dcf3ed5303fec1df6a786e51729cd64"},"cell_type":"code","source":"matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\nmatches","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"64641330bfd59693dbbc16987e07c73eedfd00b1"},"cell_type":"markdown","source":"Since d.g khan turns out to be a different city, we will replace cities with similarity score larger than 90 with d.i khan. We make a function for this. `df` is the dataframe for which the `column` for which we are doing the replacement with `string_to_match` reside. `close_matches` are found by looking through all elements of `matches` and taking out the strings that had a similarity score larger than 90. Remember, the first column `matches[0]` stores the strings and the second column `matches[1]` stores the similarity score. Then we look through each element in the column in question and replace them with `string_to_match` if they are a close match. "},{"metadata":{"trusted":true,"_uuid":"3a54477be4b914122681261e0b56c54bc913e836"},"cell_type":"code","source":"def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n    strings = df[column].unique()\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n    rows_with_matches = df[column].isin(close_matches)\n    df.loc[rows_with_matches, column] = string_to_match\n    print(\"All done!\")\nreplace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"86202b0605a1ad706f99b6235fc1edd02b1154b5"},"cell_type":"markdown","source":"Now if we check the City column we do not have separate notations for d.i khan."},{"metadata":{"trusted":true,"_uuid":"713c128ec90e27653915517a4136274e7787905f"},"cell_type":"code","source":"cities = suicide_attacks['City'].unique()\ncities.sort()\ncities","execution_count":54,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}