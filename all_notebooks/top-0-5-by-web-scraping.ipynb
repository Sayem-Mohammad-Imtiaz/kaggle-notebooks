{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Not your usual solution, but an excellent solution nontheless\n\nFor the very start, I need to say that I am not going to use regression or any tree-based method here. I am going to __reverse engineer__ the kaggle titanic data set.\n\nWhy?\n\nHave you noticed that many people managed to get a perfect score for this problem? 100% correct. I wouldn't expect any machine learning algorithm to perform this good, and you should think this way too. One way to obtain a perfect score is to check manually the survival status of each person in the test set. A perfectly viable solution, but not Pythonic at all!!!\n\nA better solution, in my opinion, is to use web scraping to gather that info from the internet. Solving the problem this way will not garrentee a perfect score, but will substantially reduce the problem to the extent that can be solved by hand. For example, check a few dozen names on the internet instead of hundreds of names.\n\nThis solution has two parts. Part 1 is the web crawler which is straight forward and hosted [here](https://github.com/HVoltBb/misc/blob/main/src/kaggle/titanic/crawler.py) on GitHub. Part 2 is the data processing part, and it is shown below.\n\nThere are numerous online resources that you can crawl, but the one I used is the [Encyclopedia TITANICA](https://www.encyclopedia-titanica.org/titanic-survivors/). You may also use the [Wikipedia page](https://en.wikipedia.org/wiki/Passengers_of_the_Titanic), which also has a well formatted table.\n\nWARNING: I know some of you will hate this solution. You don't have to like it. But by doing this exercise, you learn far more about this problem than simply running some packaged models.\n\nMuch of the effort is spent on transforming non-ascii characters into ascii characters. You will see more below.\n\nWARNING: Also note that I wrote this notebook on my own laptop. Some of the statements require Python 3.8+, and Python on kaggle is 3.7. The 3.7 compatible alternative is given here by typing some extra characters.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\n\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:09.511476Z","iopub.execute_input":"2021-09-06T00:31:09.512049Z","iopub.status.idle":"2021-09-06T00:31:09.532374Z","shell.execute_reply.started":"2021-09-06T00:31:09.511996Z","shell.execute_reply":"2021-09-06T00:31:09.531259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{train.shape}_train, {test.shape}_test')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:10.552883Z","iopub.execute_input":"2021-09-06T00:31:10.553277Z","iopub.status.idle":"2021-09-06T00:31:10.559223Z","shell.execute_reply.started":"2021-09-06T00:31:10.553247Z","shell.execute_reply":"2021-09-06T00:31:10.557996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tables from Encyclopedia TITANTICA\n\nBoth `surv.csv` and `vict.csv` are generated by running this [script](https://github.com/HVoltBb/misc/blob/main/src/kaggle/titanic/crawler.py).\n\nFor your convenience, they can also be downloaded [here](https://github.com/HVoltBb/misc/blob/main/src/kaggle/titanic/surv.csv) and [here](https://github.com/HVoltBb/misc/blob/main/src/kaggle/titanic/vict.csv)","metadata":{}},{"cell_type":"code","source":"%run -t /kaggle/input/titanicx/src/kaggle/titanic/crawler.py","metadata":{"execution":{"iopub.status.busy":"2021-09-06T16:56:47.237888Z","iopub.execute_input":"2021-09-06T16:56:47.238257Z","iopub.status.idle":"2021-09-06T16:56:53.975847Z","shell.execute_reply.started":"2021-09-06T16:56:47.238227Z","shell.execute_reply":"2021-09-06T16:56:53.974957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"suv = pd.read_csv('surv.csv')\nvic = pd.read_csv('vict.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T16:57:57.854202Z","iopub.execute_input":"2021-09-06T16:57:57.854571Z","iopub.status.idle":"2021-09-06T16:57:57.869438Z","shell.execute_reply.started":"2021-09-06T16:57:57.854541Z","shell.execute_reply":"2021-09-06T16:57:57.868513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{suv.shape}_surv, {vic.shape}_vic')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T16:58:01.923972Z","iopub.execute_input":"2021-09-06T16:58:01.924506Z","iopub.status.idle":"2021-09-06T16:58:01.929264Z","shell.execute_reply.started":"2021-09-06T16:58:01.924471Z","shell.execute_reply":"2021-09-06T16:58:01.928318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"suv['survived'] = 1\nvic['survived'] = 0\nground_truth = pd.concat([suv, vic])\nground_truth['fsname'] = [re.search('^(.*?)( |$)', item).group(1) for item in ground_truth['given name']]\nground_truth.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:14.185305Z","iopub.execute_input":"2021-09-06T00:31:14.185684Z","iopub.status.idle":"2021-09-06T00:31:14.234063Z","shell.execute_reply.started":"2021-09-06T00:31:14.185648Z","shell.execute_reply":"2021-09-06T00:31:14.232936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Non-ascii names\n\n155 out of all the TITANIC passengers (including ship crew) have a non-ascii last name.\n\n70 out of all the passengers have a non-ascii first name.","metadata":{}},{"cell_type":"code","source":"tmp_f = [item.encode('ascii', 'ignore').decode('ascii') for item in ground_truth['family name']]\nnon_ascii = [True if x != y else False for x, y in zip(tmp_f, ground_truth['family name'])]\nground_truth['uni_f'] = non_ascii\nprint('Non-ascii family names')\npd.value_counts(non_ascii)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:17.070248Z","iopub.execute_input":"2021-09-06T00:31:17.070611Z","iopub.status.idle":"2021-09-06T00:31:17.088866Z","shell.execute_reply.started":"2021-09-06T00:31:17.07057Z","shell.execute_reply":"2021-09-06T00:31:17.087772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_fs = [item.encode('ascii', 'ignore').decode('ascii') for item in ground_truth['fsname']]\nnon_ascii_ = [True if x != y else False for x, y in zip(tmp_fs, ground_truth['fsname'])]\nground_truth['uni_g'] = non_ascii_\nprint('Non-ascii first names')\npd.value_counts(non_ascii_)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:18.182844Z","iopub.execute_input":"2021-09-06T00:31:18.183209Z","iopub.status.idle":"2021-09-06T00:31:18.197768Z","shell.execute_reply.started":"2021-09-06T00:31:18.183179Z","shell.execute_reply":"2021-09-06T00:31:18.196694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use unidecode to transform non-ascii names","metadata":{}},{"cell_type":"code","source":"#!pip install unidecode\nfrom unidecode import unidecode\nground_truth['family name'] = [unidecode(item) for item in ground_truth['family name']]\nground_truth['fsname'] = [unidecode(item) for item in ground_truth['fsname']]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:20.333539Z","iopub.execute_input":"2021-09-06T00:31:20.333935Z","iopub.status.idle":"2021-09-06T00:31:20.360366Z","shell.execute_reply.started":"2021-09-06T00:31:20.333901Z","shell.execute_reply":"2021-09-06T00:31:20.35922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Or get the ascii names from the url\n\nI noticed that the `unidecode` transformed non-ascii names do not match those names in the kaggle dataset AT ALL!!!\n\nApperantly, the conversion was done some other way.\n\nNote that urls can not have non-ascii characters, and the urls for those passengers can be parsed to extract their family and last names. You can see in the following that this works. ","metadata":{}},{"cell_type":"code","source":"ground_truth.set_index(np.arange(0, ground_truth.shape[0]), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:22.226995Z","iopub.execute_input":"2021-09-06T00:31:22.227413Z","iopub.status.idle":"2021-09-06T00:31:22.232669Z","shell.execute_reply.started":"2021-09-06T00:31:22.227384Z","shell.execute_reply":"2021-09-06T00:31:22.231878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, item in ground_truth.iterrows():\n    dash = re.search('-', item['alt name'])\n    if item.uni_f | item.uni_g | bool(dash):\n        ground_truth.at[i, 'family name'] = item['alt name'].split('-')[-1].upper()\n        ground_truth.at[i, 'fsname'] = item['alt name'].split('-')[0].capitalize()        \n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:24.670061Z","iopub.execute_input":"2021-09-06T00:31:24.670391Z","iopub.status.idle":"2021-09-06T00:31:25.023056Z","shell.execute_reply.started":"2021-09-06T00:31:24.670365Z","shell.execute_reply":"2021-09-06T00:31:25.022183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['fname'] = [re.search('^(.*?), ', item).group(1) for item in train.Name]\ntrain['prefix'] = [re.search('^.*?, (.*?)\\. ', item).group(1) for item in train.Name]\ntrain['gname'] = [re.search('^.*?, .*?\\. (.*)', item).group(1) for item in train.Name]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:31:29.69124Z","iopub.execute_input":"2021-09-06T00:31:29.691939Z","iopub.status.idle":"2021-09-06T00:31:29.706874Z","shell.execute_reply.started":"2021-09-06T00:31:29.691904Z","shell.execute_reply":"2021-09-06T00:31:29.705801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning up the names\n\nEven though the description of this problem says you don't need to do much data cleaning, it is not the case.","metadata":{}},{"cell_type":"code","source":"# cleaning\ntmp = [re.search('^.*?, .*?\\. ([^ ]*?)( |$)', item).group(1) for item in train.Name]\ntmp2 = [re.search('\\((.*?)( |\\)|$)', item).group(1) if item.startswith('(') else item for item in tmp]\n\n# more cleaning\n## 3.8+\n#tmp3 = [z.group(1) if y == 'Mrs' and (z:=re.search('^.*?\\((.*?)( |\\))', x)) is not None else w for x, y, w in zip(train.gname, train.prefix, tmp2)]\n## 3.7\ntmp3 = [re.search('^.*?\\((.*?)( |\\))', x).group(1) if y == 'Mrs' and re.search('^.*?\\((.*?)( |\\))', x) is not None else w for x, y, w in zip(train.gname, train.prefix, tmp2)]\ntrain['fsname'] = tmp3\n\n# dashes\ntrain['fname'] = [item.split('-')[-1] if bool(re.search('-', item)) else item for item in train['fname']]\n# spaces\ntrain['fname'] = [item.split(' ')[-1] if bool(re.search(' ', item)) else item for item in train['fname']]\n# quotes\ntrain['fname'] = [item.replace(\"'\", '') if bool(re.search(\"'\", item)) else item for item in train['fname']]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:33:07.702351Z","iopub.execute_input":"2021-09-06T00:33:07.702745Z","iopub.status.idle":"2021-09-06T00:33:07.723872Z","shell.execute_reply.started":"2021-09-06T00:33:07.702697Z","shell.execute_reply":"2021-09-06T00:33:07.722568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['fname'] = [re.search('^(.*?), ', item).group(1) for item in test.Name]\ntest['prefix'] = [re.search('^.*?, (.*?)\\. ', item).group(1) for item in test.Name]\ntest['gname'] = [re.search('^.*?, .*?\\. (.*)', item).group(1) for item in test.Name]\n# cleaning\ntmp = [re.search('^.*?, .*?\\. ([^ ]*?)( |$)', item).group(1) for item in test.Name]\ntmp2 = [re.search('\\((.*?)( |\\)|$)', item).group(1) if item.startswith('(') else item for item in tmp]\n\n# more cleaning\n## 3.8+\n#tmp3 = [z.group(1) if y == 'Mrs' and (z:=re.search('^.*?\\((.*?)( |\\))', x)) is not None else w for x, y, w in zip(test.gname, test.prefix, tmp2)]\n# 3.7\ntmp3 = [re.search('^.*?\\((.*?)( |\\))', x).group(1) if y == 'Mrs' and re.search('^.*?\\((.*?)( |\\))', x) is not None else w for x, y, w in zip(test.gname, test.prefix, tmp2)]\n\ntest['fsname'] = tmp3\n\n# dashes\ntest['fname'] = [item.split('-')[-1] if bool(re.search('-', item)) else item for item in test['fname']]\n# spaces\ntest['fname'] = [item.split(' ')[-1] if bool(re.search(' ', item)) else item for item in test['fname']]\n# quotes\ntest['fname'] = [item.replace(\"'\", '') if bool(re.search(\"'\", item)) else item for item in test['fname']]","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:33:56.597011Z","iopub.execute_input":"2021-09-06T00:33:56.597408Z","iopub.status.idle":"2021-09-06T00:33:56.621423Z","shell.execute_reply.started":"2021-09-06T00:33:56.597374Z","shell.execute_reply":"2021-09-06T00:33:56.620174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking names\n\nOut of the 1309 records provided by kaggle, we only failed to identify 57 of them. I say this is pretty good.\n\nI have checked those 57 records. The problem is misspelled names in the kaggle dataset. I see no point in manually checking these records, even though it is achievable in under 1 hour, assuming that you can identify 1 record in 1 min.\n\nAnother problem I see is that often times the Age field in the kaggle dataset is not correct. It is not a rounding issue. Sometimes the age is off by a few years. A few things are possible here:\n\n1. kaggle staff intentionally modified those values to defy a solution like this one\n2. kaggle staff scraped a less reliable source than the one used here\n\nI will be honest here. Before attempting this solution, I have tried a ML approach which only scored ~78%, and in that approach I found that Age is a very important predictor of survivalship. Given the provided Age is not the actual age of the passenger, now I feel that some of the significance of the Age field may have been engineered into this dataset by kaggle staff.","metadata":{}},{"cell_type":"code","source":"dataset = pd.concat([train, test])\nprint(dataset.shape)\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:34:06.754473Z","iopub.execute_input":"2021-09-06T00:34:06.754879Z","iopub.status.idle":"2021-09-06T00:34:06.784539Z","shell.execute_reply.started":"2021-09-06T00:34:06.754845Z","shell.execute_reply":"2021-09-06T00:34:06.78331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fails_count = 0\nsrved = ground_truth\n\nfor i, item in dataset.iterrows():\n    if (not np.isnan(item.Survived)) and int(item.Survived) == 0:\n        continue\n    mask_lastname = [item.fname.upper()==itemx for itemx in srved['family name']]\n    how_many = sum(mask_lastname)\n    if how_many == 1:\n        True\n    elif how_many > 1:\n        mask_prefix = [item.prefix == itemx for itemx in srved['prefix']]\n        mask_ = np.array(mask_lastname) & np.array(mask_prefix)\n        how_many = sum(mask_)\n        if how_many == 1:\n            True\n        elif how_many > 1:\n            mask_fstname = [item.fsname == itemx for itemx in srved['fsname']]\n            mask__ = np.array(mask_fstname) & np.array(mask_)\n            how_many = sum(mask__)\n            if how_many == 1:\n                True    \n            else:\n                fails_count += 1\n                print(f'failed at given name {item.fsname}, indix {i}, matched {how_many}')\n    else:\n        fails_count += 1\n        print(f'failed at family name {item.fname}, indix {i}, matched {how_many}')\n\nprint(f'{fails_count} failed')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:34:12.352447Z","iopub.execute_input":"2021-09-06T00:34:12.352876Z","iopub.status.idle":"2021-09-06T00:34:47.835814Z","shell.execute_reply.started":"2021-09-06T00:34:12.352841Z","shell.execute_reply":"2021-09-06T00:34:47.834722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What have I learned from this?\n\nThe majority of the missed records are due to typos in the kaggle dataset. I am not sure if those typos are intentional planted there or not. The training set should not be taken as facts, as I have encountered plenty of inconsistent age values. It is possible to get 100% correct on this, but I don't think it is worth the effort, so I am not trying to improve my score further.\n\n155 family names and 70 surnames have non-ascii characters in them, and converting these chars accounts for most of my effort in this problem. There are many ways to convert accented chars to latin chars. For this particular dataset, scraping the url (ascii by the standard) link works better than using the `uniencode` package.","metadata":{}},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:35:32.574943Z","iopub.execute_input":"2021-09-06T00:35:32.575419Z","iopub.status.idle":"2021-09-06T00:35:32.597442Z","shell.execute_reply.started":"2021-09-06T00:35:32.57537Z","shell.execute_reply":"2021-09-06T00:35:32.596037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:35:33.167204Z","iopub.execute_input":"2021-09-06T00:35:33.167584Z","iopub.status.idle":"2021-09-06T00:35:33.187347Z","shell.execute_reply.started":"2021-09-06T00:35:33.167554Z","shell.execute_reply":"2021-09-06T00:35:33.186558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['survived'] = None\n\nsrved = ground_truth[ground_truth.survived == 1]\nfails_count = 0\n\nfor i, item in test.iterrows():\n    mask_lastname = [item.fname.upper()==itemx for itemx in srved['family name']]\n    how_many = sum(mask_lastname)\n    if how_many == 1:\n        test.survived.at[i] = 1\n    #    print('\\u2713')\n    elif how_many > 1:\n        mask_prefix = [item.prefix == itemx for itemx in srved['prefix']]\n        mask_ = np.array(mask_lastname) & np.array(mask_prefix)\n        how_many = sum(mask_)\n        if how_many == 1:\n            test.survived.at[i] = 1\n    #        print('\\u2713')\n        elif how_many > 1:\n            mask_fstname = [item.fsname == itemx for itemx in srved['fsname']]\n            mask__ = np.array(mask_fstname) & np.array(mask_)\n            how_many = sum(mask__)\n            if how_many == 1:\n                test.survived.at[i] = 1\n    #            print('\\u2713')\n            else:\n                fails_count += 1\n                print(f'failed at given name {item.fsname}, indix {i}, matched {how_many}')\n    else:\n        fails_count += 1\n        print(f'failed at family name {item.fname}, indix {i}, matched {how_many}')\n\nprint(f'{fails_count} failed')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:35:34.413428Z","iopub.execute_input":"2021-09-06T00:35:34.413795Z","iopub.status.idle":"2021-09-06T00:35:38.91232Z","shell.execute_reply.started":"2021-09-06T00:35:34.413765Z","shell.execute_reply":"2021-09-06T00:35:38.911334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"srved = ground_truth[ground_truth.survived == 0]\nfails_count = 0\n\nfor i, item in test.iterrows():\n    mask_lastname = [item.fname.upper()==itemx for itemx in srved['family name']]\n    how_many = sum(mask_lastname)\n    if how_many == 1:\n        test.survived.at[i] = 0\n    #    print('\\u2713')\n    elif how_many > 1:\n        mask_prefix = [item.prefix == itemx for itemx in srved['prefix']]\n        mask_ = np.array(mask_lastname) & np.array(mask_prefix)\n        how_many = sum(mask_)\n        if how_many == 1:\n            test.survived.at[i] = 0\n    #        print('\\u2713')\n        elif how_many > 1:\n            mask_fstname = [item.fsname == itemx for itemx in srved['fsname']]\n            mask__ = np.array(mask_fstname) & np.array(mask_)\n            how_many = sum(mask__)\n            if how_many == 1:\n                test.survived.at[i] = 0\n    #            print('\\u2713')\n            else:\n                fails_count += 1\n                print(f'failed at given name {item.fsname}, indix {i}, matched {how_many}')\n    else:\n        fails_count += 1\n        print(f'failed at family name {item.fname}, indix {i}, matched {how_many}')\n\nprint(f'{fails_count} failed')","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:35:41.786966Z","iopub.execute_input":"2021-09-06T00:35:41.787333Z","iopub.status.idle":"2021-09-06T00:35:52.79381Z","shell.execute_reply.started":"2021-09-06T00:35:41.787301Z","shell.execute_reply":"2021-09-06T00:35:52.792781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.survived.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:35:54.954422Z","iopub.execute_input":"2021-09-06T00:35:54.954819Z","iopub.status.idle":"2021-09-06T00:35:54.962375Z","shell.execute_reply.started":"2021-09-06T00:35:54.954784Z","shell.execute_reply":"2021-09-06T00:35:54.961209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filling in missing values\n\nWe failed to identify 33 passengers in the test set. We are not going to manually check those 33 names, although it is possible. We are going to fill in the most probable survival status for these 33 passengers, which is '0' based on the training set. \n\nAt the very end, we used some statistical skills to fill in missing values with their most probable outcome. Now, I am feeling a bit better now. All those years studying statistics are not lost after all!\n\nThis submission scored 0.88995%. Not bad at all.","metadata":{}},{"cell_type":"code","source":"pd.value_counts(train.Survived)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:35:58.115734Z","iopub.execute_input":"2021-09-06T00:35:58.116174Z","iopub.status.idle":"2021-09-06T00:35:58.124218Z","shell.execute_reply.started":"2021-09-06T00:35:58.11614Z","shell.execute_reply":"2021-09-06T00:35:58.123201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.survived.fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:35:58.98575Z","iopub.execute_input":"2021-09-06T00:35:58.98609Z","iopub.status.idle":"2021-09-06T00:35:58.991722Z","shell.execute_reply.started":"2021-09-06T00:35:58.986062Z","shell.execute_reply":"2021-09-06T00:35:58.990521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame([test.PassengerId, test.survived]).T\nresult.astype({'PassengerId': 'int32', 'survived': 'int32'})\nresult.to_csv('submit.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T00:36:05.532845Z","iopub.execute_input":"2021-09-06T00:36:05.533197Z","iopub.status.idle":"2021-09-06T00:36:05.567187Z","shell.execute_reply.started":"2021-09-06T00:36:05.533167Z","shell.execute_reply":"2021-09-06T00:36:05.566153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End\n\nI hope you like this solution. All the scripts, including this notebook, and outputs with the exception of the final predictions can be found on [GitHub](https://github.com/HVoltBb/misc/blob/main/src/kaggle/titanic).\n\nLet me know if you learned something new!!!","metadata":{}}]}