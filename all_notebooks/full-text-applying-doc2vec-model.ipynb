{"cells":[{"metadata":{"_uuid":"706836ef-b3bf-45c0-a2a3-a72158989779","_cell_guid":"ad4a731b-6d55-4639-acda-6830fbd257bd","trusted":true},"cell_type":"markdown","source":"# Task: What do we know about non-pharmaceutical interventions?","execution_count":null},{"metadata":{"_uuid":"c0b657bc-0de1-4cad-b3f8-c21b43e5e28a","_cell_guid":"86f7163c-f43e-4b18-b3b5-1dc458c334ec","trusted":true},"cell_type":"markdown","source":"## Install/Load Packages","execution_count":null},{"metadata":{"_uuid":"d00bb57c-727a-4c89-8e84-faf449e11ed0","_cell_guid":"8c454862-eaae-4c63-bf76-c153560ff9e2","trusted":true},"cell_type":"markdown","source":"The first block of code is (almost) directly from kaggle","execution_count":null},{"metadata":{"_uuid":"cc57751a-b90e-4d29-84e1-a43c4f8d86f4","_cell_guid":"208e5fbd-049f-4554-a5f5-c3b307d2f879","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# There are too many paths and printing them takes\n# up too much space so I don't do this normally\nif 1==0: \n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            pass\n            #print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ee73fe4-7c71-4db7-9571-d169064382c5","_cell_guid":"330059fc-e820-4d3a-a3ee-61cd01fbc5fc","trusted":true},"cell_type":"markdown","source":"Next we installl scispacy, a repo of commands to deal with scientific documents. *Note that internet access needs to be switched on for this to work!*","execution_count":null},{"metadata":{"_uuid":"6563b882-fac3-4b7d-93c4-d19ee851ef0a","_cell_guid":"43013c9e-2c5b-4540-a1ec-3ec8b66428b3","trusted":true},"cell_type":"code","source":"!pip install swifter\n#!pip install scispacy\n#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d34ba8fc-afd0-458c-a49d-89c9c869c99e","_cell_guid":"4b1d8b09-7c89-4504-9aec-eec95b771c42","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Progress bar\nimport tqdm\n\n# Word2Vec\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec,TaggedDocument\n\nfrom nltk.tokenize import word_tokenize,sent_tokenize \nfrom scipy.spatial.distance import cdist,cosine\nimport gc\nimport swifter\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bd39b55-8052-4c60-a42f-005c123bf5fd","_cell_guid":"865bb4d7-28e1-4689-aafc-c8701cde1c74","trusted":true},"cell_type":"code","source":"#nlp = spacy.load(\"en_core_web_sm\")\n#tokenizer = nlp.Defaults.create_tokenizer(nlp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af0c4e85-891b-4ec0-9477-1df1e0d445c2","_cell_guid":"cac4d04f-f642-4073-a779-a212d38df2d7","trusted":true},"cell_type":"markdown","source":"## Introduction","execution_count":null},{"metadata":{"_uuid":"1e8852fe-19af-4fc6-9ef0-9a3728ad5fc2","_cell_guid":"aa7fd622-2481-4f2e-b787-1deed7354a25","trusted":true},"cell_type":"markdown","source":"Now that all our libraries are loaded we need data. We explore the full text in the files using the output generated from the following notebook:\nhttps://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv","execution_count":null},{"metadata":{"_uuid":"c7abd0aa-de70-42d8-a342-a001763537fc","_cell_guid":"1eaaf1c4-1c20-4f0d-b732-c6cba8952f91","trusted":true},"cell_type":"code","source":"all_data = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\n\"\"\"\nbiorxiv_clean = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/biorxiv_clean.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\nclean_comm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_comm_use.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\nclean_noncomm_use = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_noncomm_use.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\nclean_pmc = pd.read_csv(\"../input/cord-19-eda-parse-json-and-generate-clean-csv/clean_pmc.csv\").drop(columns = ['bibliography','raw_bibliography','raw_authors'])\n\nall_data = pd.concat([biorxiv_clean, clean_comm_use, clean_noncomm_use, clean_pmc]).reset_index(drop=True).drop_duplicates()\ndel biorxiv_clean,clean_comm_use,clean_noncomm_use,clean_pmc\ngc.collect()\n\"\"\"\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f5fc229-f915-45b7-9739-0a29898b4c19","_cell_guid":"ed715ede-4f72-48e9-aede-201e600b8063","trusted":true},"cell_type":"code","source":"print(\"Number of Rows in Table: %i\" % len(all_data))\nprint(\"Number of Titles: %i \" % all_data['title'].count())\nprint(\"Number of Abstracts: %i \" % all_data['abstract'].count())\nprint(\"Number of Texts: %i \" % all_data['text'].count())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b49f8ceb-52db-4716-9de1-fd33706b22d1","_cell_guid":"123990be-b748-4ff9-ac5b-f675e6b3c3ac","trusted":true},"cell_type":"markdown","source":"## Train Doc2Vec Model\nIn this notebook, I applied [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)\n\nThe algorithm is introduced [here](https://arxiv.org/pdf/1405.4053v2.pdf)","execution_count":null},{"metadata":{"_uuid":"1e04e304-1bf8-4c7c-a5f5-b95c29c48d60","_cell_guid":"fe567d46-273d-4bfd-a369-19bfebb0db08","trusted":true},"cell_type":"code","source":"def cos_sim(text,df,model):\n    # compute similarity\n    doc_sim = (\n        1-cdist(\n            df.values,\n            [model.wv[text]],\n            'cosine'\n        )\n    )\n    # convert result to a date frame\n    document_sim_df = (\n        pd.DataFrame(doc_sim, columns=[\"cos_sim\"])\n        .assign(document_id=list(df.index))\n    )\n    # sort from most similar to least\n    document_sim_df = document_sim_df.sort_values(\"cos_sim\", ascending=False)\n    \n    # perform left-join to get information about the documents\n    doc_sim_meta_df = document_sim_df.merge(all_data,\n                      how='left',\n                     left_on='document_id',\n                     right_on='paper_id')\n    return doc_sim_meta_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6354da4d-932f-4524-82fe-15bb0ead7df3","_cell_guid":"db281577-6b3a-46c3-9949-0b6e677a960e","trusted":true},"cell_type":"code","source":"def majority_voting(text,df,model):\n    # choose top 100\n    doc_sim_meta_dfs = [cos_sim(word,df,model).iloc[:100] for word in text.split()]\n    return pd.merge(*doc_sim_meta_dfs,how = 'inner',on = 'document_id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abb749e2-2f0d-4e0b-9a6d-e4162af393cc","_cell_guid":"e8ebdd26-b8d4-4436-b99f-0be0afdf3b64","trusted":true},"cell_type":"code","source":"# replace empty text with empty strings\nptn = r'\\[[0-9]{1,2}\\]'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5de25d13-8f22-42a2-867c-5b86d0686029","_cell_guid":"9394649f-4486-49fa-ad84-97262ad38673","trusted":true},"cell_type":"code","source":"def tokenize_and_tag(x):\n    return TaggedDocument(word_tokenize(x['text'].replace('\\n\\n', ' ').replace(ptn,'').strip()),[x['paper_id']])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea551f6e-d552-47ad-af02-4277f764ba3f","_cell_guid":"b79c5e67-cf9d-4779-be50-37be2787f96f","trusted":true},"cell_type":"code","source":"#text_documents = list(map( lambda x: tokenizer(x),all_data.text.str.replace('\\n\\n', ' ').replace(ptn,'').str.strip()))\ntext_documents = all_data.swifter.apply(tokenize_and_tag,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e86dc0e4-4b57-450e-a8de-2aabcab0c80c","_cell_guid":"b9682972-b7ed-4fcf-bb5c-cf352be82a1b","trusted":true},"cell_type":"code","source":"#text_documents = [TaggedDocument(doc, [all_data.loc[i,'paper_id']]) for i, doc in enumerate(text_documents)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"540aed0c-bcbc-46ff-b3c8-bbfea9da8db3","_cell_guid":"e0c05dec-fc3e-4ecc-ada9-632f1176ce0e","trusted":true},"cell_type":"code","source":"text_model = Doc2Vec(text_documents,vector_size = 200,window=10, min_count=3, workers=4)\ntext_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ba71d39-50f8-45b3-b695-9e59308d64bd","_cell_guid":"7fe72436-7331-4a2d-9dec-08254a059221","trusted":true},"cell_type":"code","source":"document_dict_text = {}\nfor idx, text_df in tqdm.tqdm(all_data[[\"paper_id\", \"text\"]].iterrows()):\n    document_dict_text[text_df['paper_id']] = text_model.docvecs[text_df['paper_id']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9002e67e-87d2-40f2-ac1d-cbcb7b53e10a","_cell_guid":"e136486d-02c9-41ba-9d71-c8bddf4e0a83","trusted":true},"cell_type":"code","source":"text_document_embeddings_df = pd.DataFrame.from_dict(document_dict_text, orient=\"index\")\ntext_document_embeddings_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bd28b3e-f74b-4d76-8b18-d2d0458ce2a6","_cell_guid":"73d42a6e-2738-4098-b895-c6996cb0b5f4","trusted":true},"cell_type":"code","source":"npi_papers = majority_voting('non-pharmaceutical interventions',text_document_embeddings_df,text_model).loc[:100,['text_x','title_x']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20e9feed-b206-48b5-bfde-5de140e1ab5d","_cell_guid":"b7b624ae-a547-4852-bed3-7fa607a2e1fe","trusted":true},"cell_type":"markdown","source":"## idenitfy most relevant paragraph\nOnce we identify the document, let's identify the most relevant paragraph to the question.\nThe paragraph is recognized by the line breaks.","execution_count":null},{"metadata":{"_uuid":"e97bf5b5-e6f4-4411-b03d-43e960ac533e","_cell_guid":"9a9f63b5-1eda-417c-9aa4-23391c4b5202","trusted":true},"cell_type":"code","source":"# identify paragraphs for each document\n# each line breaks will give us paragraph \nnpi_papers['text_x'] = npi_papers['text_x'].replace('\\n\\n', ' ').str.split('\\n')\nnpi_tagged_docs = []\nparagraph_table = []\nfor i,row in npi_papers.iterrows():\n    title = row['title_x']\n    npi_tagged_docs += [TaggedDocument(word_tokenize(text),[f\"{title} - {j}\"]) for j,text in enumerate(row['text_x']) if len(text) > 1]\n    paragraph_table += [[title,j,text] for j,text in enumerate(row['text_x']) if len(text) > 1]\nparagraph_df = pd.DataFrame(data = paragraph_table,columns = ['title','paragraph_id','text'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db559ef4-a010-4e3c-bd62-bf3d64353428","_cell_guid":"09bb8bec-85e0-4c22-9110-3003596fd239","trusted":true},"cell_type":"code","source":"paragraph_model = Doc2Vec(npi_tagged_docs,vector_size = 150,window=4, min_count=3, workers=4)\nparagraph_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84607335-d916-4a09-8fac-ecf1b8411825","_cell_guid":"087f982a-d29f-4ad3-beb3-af7b19e3571f","trusted":true},"cell_type":"code","source":"def get_paragraph_model_dic(x,text):\n    title = x[0]\n    paragraph_id = x[1]\n    doc_vec = paragraph_model.docvecs[f\"{title} - {paragraph_id}\"]\n    cos_sim = 1 - cosine(\n            doc_vec,\n            paragraph_model.wv[text]\n        )\n    return cos_sim","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c455271-60ab-4fd5-9c46-d7a1fdc33c87","_cell_guid":"b26cf401-b96a-4391-9a32-e5af6d5b9f32","trusted":true},"cell_type":"code","source":"paragraph_df['non_ph_cos_sim'] = paragraph_df.swifter.apply(get_paragraph_model_dic,axis=1,text = 'non-pharmaceutical')\nparagraph_df['interv_cos_sim'] = paragraph_df.swifter.apply(get_paragraph_model_dic,axis=1,text = 'intervention')\ntop_paragraphs = pd.merge(paragraph_df.sort_values(by = 'non_ph_cos_sim',ascending = False).iloc[:200],\n        paragraph_df.sort_values(by = 'interv_cos_sim',ascending = False).iloc[:200],\n         on = ['title','paragraph_id'],how = 'inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_paragraphs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_paragraphs.to_csv('top_paragraphs.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict subtasks \nparagraph_model.infer_vector(word_tokenize(\"Methods to control the spread in communities, barriers to compliance and how these vary among different populations.\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}