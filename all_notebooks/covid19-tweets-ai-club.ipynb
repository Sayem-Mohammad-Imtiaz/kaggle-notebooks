{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=+2 color=\"indigo\"><center><b>COVID19 Tweets - Sentiment & Geographical Analysis</b></center></font>"},{"metadata":{},"cell_type":"markdown","source":"![](https://pbs.twimg.com/profile_images/1308010958862905345/-SGZioPb_400x400.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Hello readers, I am going to perform analysis on peoples tweets about COVID throughout the world.I hope i will make you realize about sentiments and emotions spread out by people in terms of tweets in twitter.This kernel will be a mixture of geographical positions and sentiment analysis of tweets.Along with EDA,sentiment analysis, deep learning model is performed to predict people's real tone of voice.After modelling,i will showcase some of post prediction visulaizations which would give more insights and results.\n\n**Kernel Main Agenda:**\n\n*  Data\n*  EDA\n*  Geographical tweets\n*  Modeling & Predictions - Sentiment Analysis\n*  Post Prediction Visualization\n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content</h3>\n\n* [1. Library](#1)\n* [2. Data](#2)\n* [3. Location](#3)\n* [4. Tweet Series](#4)\n* [5. Tweet Source](#5)\n* [6. Hashtag](#6)\n* [7. Mentions](#7)\n* [8. Average Length](#8)  \n* [9. Most Used Words](#9)\n* [10. N-Gram](#10)\n* [11. Modeling](#11)\n* [12. Prediction](#12)\n* [13. Angry Bird vs Happy Bird](#13)\n* [14. Sentiment length](#14)\n* [15. World Emotions](#15)\n* [16. Top Countries - Emotional Bird](#16)\n* [17. Expand sentiment classes](#17)\n* [18. Vietnamese sentiment analysis](#18)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Library</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Import necessary libraries - Numpy,Ploltly,Sklearn,NLTK,Tensorflow"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport itertools\n\n#plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom nltk.util import ngrams\n\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport re\nfrom collections import Counter\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport requests\nimport json","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font size=\"+2\" color=\"indigo\"><b>2. Data</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"The main data  for this analysis and sentiment prediction is based on [Gabriel dataset](https://www.kaggle.com/gpreda/covid19-tweets)\n\n[Kazanova dataset](https://www.kaggle.com/kazanova/sentiment140) with 1.6 milllion tweets will be used for modeling.This dataset has target feature - sentiment as 0 & 4 (0 - Negative , 4 - Positive) which will be relabeled as (0 & 1)\n\nWe wil be scaling positive and negative emotions in our covid tweet data through predictions.\n\n**Postive emotions** - Hope,Pride,Interest,Joy,Satisfaction,Happy etc <br>\n**Negative emotions** - Fear,Anger,Discust,Sadness,Rude etc"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"covid=pd.read_csv('/kaggle/input/covid19-tweets/covid19_tweets.csv')\n\nsentiment=pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv',encoding = 'latin',header=None,names=['target','id',\n                                                                                                                             'time','query','usr','text'])\n# Useful for code matching with countries - Plotly Chlorepeth MAP\ncountry_code=pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv') \n\n# Below files are used for data cleaning - Retrieving countries with help of city codes/state codes...(optional)\nstate = json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/states.json\").text)\ncountry=json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/countries.json\").text)\ncity=json.loads(requests.get(\"https://raw.githubusercontent.com/praneshsaminathan/country-state-city/master/cities.json\").text)\nus_state_code=pd.read_csv('https://worldpopulationreview.com/static/states/abbr-name.csv',names=['state_code','state'])\n\n\n# All above mentioned data are functioned and retrieved in below file to get valid country name- without_country_name\n#without_country_name=pd.read_csv('../input/country-tweet/without_country_name.csv',low_memory=False)\nwithout_country_name = pd.DataFrame([state,country,city])\n\ncovid.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Total tweets in this data: {}'.format(covid.shape[0]))\nprint('Total Unique Users in this data: {}'.format(covid['user_name'].nunique()) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font size=\"+2\" color=\"indigo\"><b>3. Location</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Most tweeted countries on COVID matters. \n\n(Given that the location fields is  a mixture and noisy.I have done some data cleaning and fitted data into 'without_country_name.csv')"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"covid['country_name']=covid['user_location'].str.split(',').str[-1]\ncovid['only_date']=pd.to_datetime(covid['date']).dt.date\n\n#Keeping countries with valid country name\n\nwith_country_name=covid[covid['country_name'].isin(list(country_code['COUNTRY']))]\nwith_country_name['filtered_name']=covid['country_name']\n\n#Without valid country name is programmed below\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It took a few minutes"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# country=pd.DataFrame(country['countries'])\n# states =pd.DataFrame(state['states'])\n# city=pd.DataFrame(city['cities'])\n\n# all_world=pd.merge(city,states,left_on=\"state_id\",right_on=\"id\",how=\"left\")\n# all_world=pd.merge(all_world,country,left_on=\"country_id\",right_on=\"id\",how=\"left\")\n# all_world=pd.merge(all_world,us_state_code,left_on=\"name_y\",right_on=\"state\",how=\"left\")\n\n# temp_df=all_world[['name_x','name']].rename(columns={'name_x':'place'})\n# temp_df=temp_df.append(all_world[['name_y','name']].rename(columns={'name_y':'place'}))\n# temp_df=temp_df.append(all_world[['sortname','name']].rename(columns={'sortname':'place'}))\n# temp_df=temp_df.append(all_world[['state_code','name']].rename(columns={'state_code':'place'}))\n# temp_df=temp_df.drop_duplicates()\n# temp_df.shape\n\n\n# dict_count=dict({'USA':'United States','UK':'United Kingdom','भारत':'India','British Columbia':'Canada','Deutschland':'Germany','Jammu And Kashmir':'India',\n#                  'ON':'Canada','DC':'United States','UAE':'United Arab Emirates','hyderabad':'India','New York City':'United States','Everywhere':'United States',\n#                  'Republic of the Philippines':'Philippines','Africa':'South Africa','WORLDWIDE':'United States','Washington DC':'United States','mumbai':'India',\n#                  'INDIA':'India','Worldwide':'United States','North America':'United States','Washington DC & Virginia':'United States','PRC':'China',\n#                  'San Francisco Bay Area':'United States','America':'United States','BC':'United Kingdom','BENGALURU':'India','#AFRICA #MENA':'South Africa',\n#                  'online':'India','Québec':'Canada','Earth':'United States','Canberra':'Australia','Europe':'Canada','World':'India','Northern Ireland':'United Kingdom',\n#                  'Sun Valley Idaho':'United States','Hong Kong':'China','Sydney':'Australia','NYC':'United States','New South Wales':'Australia','D.C.':'United States','The Netherlands':'Netherlands','Global':'United States','Planet Earth':'India','Bangalore':'India','U.S.':'United States',\n#                  'CANADA':'Canada','Nig':'Nigeria','Western Australia':'Australia','The seventh house':'United States','Ngovhela Mahunguni':'Russia','Kashmir':'India','Etats-Unis':'United States','Mumbai | Kolkata':'India','VadaChennai':'India','SoCal':'United States','Sverige':'Sweden','Victoria BC':'Canada','Kingdom of Saudi Arabia':'Saudi Arabia','worldwide':'United States','Koramangala':'India','East Legon':'Ghana','india':'India','Silicon Valley':'United States','BHARAT':'India','Melbourne but I tour worldwide':'Australia','Remote':'United States','New Delhi.':'India','Cape Town':'South Africa','Nigeria.':'Nigeria','Netherlands':'Netherlands','Kamloops':'Canada','EU Citizen':'Canada','SF Bay Area':'United States','South Florida':'United States','Nova Scotia':'Canada','AB':'Canada','City of London':'United Kingdom','NOIDA':'India','NEW DELHI':'India','Lancashire and Europe':'United Kingdom','Washington D.C.':'United States','Middle East':'Saudi Arabia','Quezon City':'Philippines','@CapricornFMNews':'Russia','South Australia':'Australia','India.':'India','International':'United States','Kashmir & Ladakh':'India','WorldWide':'India','Ca':'united states','MontrÃ©al':'Canada','Asia':'India','CHINA':'China','World Wide':'India','Northern California':'United States','uk':'United Kingdom','Kuala Lumpur':'Singapore','Global Citizen':'United States','Johannesburg South Africa':'South Africa','J&K':'India','Australia ðŸ‡¦ðŸ‡º':'Australia','Abuja':'Nigeria','Makati City':'Philippines','Detroit-Northville-St. Heights':'United States','South Africa- Gauteng':'South Africa','Southern California':'United States','EspaÃ±a':'Spain',\n#                  'California USA ðŸ‡ºðŸ‡¸':'United States','United States of America':'United States','West of Minsk':'Belarus',\n#                  'Ontario Canada':'Canada','Greater Vancouver':'Canada','Chicago/Washington D.C.':'United States','California USA 🇺🇸':'United States','U.S.A.':'United States','Macau S.A.R.':'China','MontrÃ©al':'Spain','EspaÃ±a':'Spain','Montserrat':'United Kingdom','California USA ðŸ‡ºðŸ‡¸':'United States','TÃ¼rkiye':'Turkey','united states':'United States','Australia ðŸ‡¦ðŸ‡º':'Australia','Islamabad':'Pakistan',\n#                 'Netherlands The':'Netherlands','Australia 🇦🇺':'Australia' ,'Montréal':'Canada','España':'Spain','Türkiye':'Türkey','East of England':'United Kingdom','NY USA':'United States','Waikato New Zealand':'New Zealand','Mexico City':'Mexico','West Yorkshire':'United Kingdom','NIGERIA':'Nigeria','London UK':'United Kingdom','Ngunnawal Country Aka Canberra':'Australia','Blackburn with Darwen':'United Kingdom','JHB':'South Africa','New England':'United Kingdom','UK.':'United Kingdom','Odisha(India)':'India','london':'United Kingdom','B.C.':'United Kingdom','Mysore and BERLIN':'India','Appalachia':'United States','Philly':'United States','criminal australia':'Australia','EU':'Spain','New York Metropolitan Area':'United States','U.K.':'United States','Islamic Republic of Iran':'Iran','Yorkshire and The Humber':'United Kingdom',\n#                  'Northwest Indiana':'Indiana','Kenya.':'Kenya','Nairobi Kenya':'Kenya','Abu Dhabi':'Saudi Arabia'})\n\n# #dict_count.keys()\n# dict_country= pd.DataFrame(dict_count.items(), columns=['mislabel', 'correct_label'])\n# # dict_country['mislabel']=dict_count.keys()\n# # dict_country['correct_label']=dict_count.values()\n\n\n# def manual_fix(cnt):\n#     if cnt in list(dict_country['mislabel']):\n#         return dict_country[dict_country['mislabel']==cnt]['correct_label'].to_string().split(\"  \")[-1]  \n#     else:\n#         return cnt\n\n\n# def get_country(x):\n#     if type(x) is str:\n#         x = x.replace('\\D+', '')\n#         print(x)\n#         if(len(temp_df[temp_df['place']==x.strip()]['name'])>0):\n#             return temp_df[temp_df['place']==x.strip()]['name'][:1].to_string().split('  ')[-1]\n#         else:\n#             return manual_fix(x.strip())\n#     else:\n#         return x\n\n# without_country_name['filtered_name']=covid['country_name'].apply(lambda x:get_country(x))\n# without_country_name.to_csv('without_country_name.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tweet_df=with_country_name.append(without_country_name)\ntweet_state_count=tweet_df['filtered_name'].value_counts().to_frame().reset_index().rename(columns={'index':'country','filtered_name':'count'})\nall_tweet_location=pd.merge(tweet_state_count,country_code[['COUNTRY','CODE']],left_on=\"country\",right_on=\"COUNTRY\",how=\"left\")\nall_tweet_location=all_tweet_location[all_tweet_location['COUNTRY'].notnull()]\nall_tweet_location[['COUNTRY','count']].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure(go.Bar(\n    x=all_tweet_location['COUNTRY'][:10],y=all_tweet_location['count'][:10],\n    marker={'color': all_tweet_location['count'][:10], \n    'colorscale': 'blues'},  \n    text=all_tweet_location['count'][:10],\n    textposition = \"outside\",\n))\nfig.update_layout(title_text='Top Countries with most tweets',xaxis_title=\"Countries\",\n                  yaxis_title=\"Number of Tweets \",template=\"plotly_dark\",height=700,title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure(data=go.Choropleth(\n    locations = all_tweet_location['CODE'],\n    z = all_tweet_location['count'],\n    text = all_tweet_location['COUNTRY'],\n    colorscale = 'rainbow', \n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = '# of Tweets',\n))\n\nfig.update_layout(\n    title_text='Tweets over the world - ({} - {}) '.format(covid['only_date'].sort_values()[0].strftime(\"%d/%m/%Y\"),\n                                                       covid['only_date'].sort_values().iloc[-1].strftime(\"%d/%m/%Y\")),title_x=0.5,\n    geo=dict(\n        showframe=True,\n        showcoastlines=False,\n        \n        projection_type='equirectangular',\n    )\n)\n\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**US,India,UK,Canada & Australia** have produced more tweets."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font size=\"+2\" color=\"indigo\"><b>4. Tweet Series</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"How many tweets were posted everyday?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"covid['tweet_date']=pd.to_datetime(covid['date']).dt.date\ntweet_date=covid['tweet_date'].value_counts().to_frame().reset_index().rename(columns={'index':'date','tweet_date':'count'})\ntweet_date['date']=pd.to_datetime(tweet_date['date'])\ntweet_date=tweet_date.sort_values('date',ascending=False)\ntweet_date.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig=go.Figure(go.Scatter(x=tweet_date['date'],\n                                y=tweet_date['count'],\n                               mode='markers+lines',\n                               name=\"Submissions\",\n                               marker_color='dodgerblue'))\n\nfig.update_layout(\n    title_text='Tweets per Day : ({} - {}) '.format(covid['only_date'].sort_values()[0].strftime(\"%d/%m/%Y\"),\n                                                       covid['only_date'].sort_values().iloc[-1].strftime(\"%d/%m/%Y\")),template=\"plotly_dark\",\n    title_x=0.5)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More tweets were made on july 25 (weekend)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font size=\"+2\" color=\"indigo\"><b>5. Tweet Sources</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"What device/source did people use to tweet?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"source_df=covid['source'].value_counts().to_frame().reset_index().rename(columns={'index':'source','source':'count'})[:15]\nsource_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure(go.Bar(\n    x=source_df['source'],y=source_df['count'],\n    marker={'color': source_df['count'], \n    'colorscale': 'blues'},  \n    text=source_df['count'],\n    textposition = \"outside\",\n))\n\nfig.update_layout(title_text='Top Sources ',xaxis_title=\"Sources\",yaxis_title=\"Count \",\n                  template=\"plotly_dark\",title_x=0.5)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is quite expected as there are more **Web** users,we are observing high count on it.Next comes the **Android & Iphone** users which stands next to web apps"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n\n<font size=\"+2\" color=\"indigo\"><b>6. Hashtag</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"What hashtags has been viral/most used in covid tweets?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)\ncovid['hash']=covid['text'].apply(lambda x:find_hash(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hastags=list(covid[(covid['hash'].notnull())&(covid['hash']!=\"\")]['hash'])\nhastags = [each_string.lower() for each_string in hastags]\nhash_df=dict(Counter(hastags))\ntop_hash_df=pd.DataFrame(list(hash_df.items()),columns = ['word','count']).sort_values('count',ascending=False)[:20]\ntop_hash_df.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure(go.Bar(\n    x=top_hash_df['word'],y=top_hash_df['count'],\n    marker={'color': top_hash_df['count'], \n    'colorscale': 'blues'},  \n    text=top_hash_df['count'],\n    textposition = \"outside\",\n))\nfig.update_layout(title_text='Top Trended Hastags',xaxis_title=\"Hashtags \",\n                  yaxis_title=\"Number of Tags \",template=\"plotly_dark\",height=700,title_x=0.5)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n\n<font size=\"+2\" color=\"indigo\"><b>7. Mentions</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Let us find most mentioned user or organization in covid tweets"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def find_at(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\ncovid['mention']=covid['text'].apply(lambda x:find_at(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mentions=list(covid[(covid['mention'].notnull())&(covid['mention']!=\"\")]['mention'])\nmentions = [each_string.lower().split() for each_string in mentions]\nmentions=list(itertools.chain.from_iterable(mentions))\nmention_df=dict(Counter(mentions))\ntop_mention_df=pd.DataFrame(list(mention_df.items()),columns = ['word','count']).sort_values('count',ascending=False)[:20]\ntop_mention_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure(go.Bar(\n    x=top_mention_df['word'],y=top_mention_df['count'],\n    marker={'color': top_mention_df['count'], \n    'colorscale': 'blues'},  \n    text=top_mention_df['count'],\n    textposition = \"outside\",\n))\n\nfig.update_layout(title_text='Top Trended Hastags ',xaxis_title=\"Hashtags\",\n                  yaxis_title=\"Number of Tags \",template=\"plotly_dark\",height=700,title_x=0.5)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observed that most tweeted countries are **US,India and UK**.As a result of them,here we could see **Trump,Biden,Boris,Modi (Top Leaders)** are tagged.Apart from them,**WHO** has paid attention to whole word.youtube is in the game too."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a> \n<font size=\"+2\" color=\"indigo\"><b>8. Average Length</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"What is the average length for a covid tweet?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\n\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    #string=\"\".join(line)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line\ndef remove_thi_amp_ha_words(string):\n    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',string)\n    return line","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"covid['refine_text']=covid['text'].str.lower()\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_tag(str(x)))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_mention(str(x)))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_hash(str(x)))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_newline(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_url(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_number(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_punct(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:text_strip(x))\n\ncovid['text_length']=covid['refine_text'].str.split().map(lambda x: len(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure(data=go.Violin(y=covid['text_length'], box_visible=True, line_color='black',\n                               meanline_visible=True, fillcolor='royalblue ', opacity=0.6,\n                               x0='Tweet Text Length '))\n\nfig.update_layout(yaxis_zeroline=False,title=\"Distribution of Text length \",template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average length of tweets lies around **14** and median lies around **15**.And interquartiles lies between **11 & 18**.There is not much significant difference between mean and median which displays that people are more biased towrds this particular length."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n\n<font size=\"+2\" color=\"indigo\"><b>9. Most Used Words</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Finding the most used words from entire population of tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax2) = plt.subplots(1,1,figsize=[17, 10])\nwordcloud2 = WordCloud(background_color='black',colormap=\"Blues\", \n                        width=600,height=400).generate(\" \".join(covid['refine_text']))\n\nax2.imshow(wordcloud2,interpolation='bilinear')\nax2.axis('off')\nax2.set_title('Most Used Words in Comments ',fontsize=35)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**new cases,pandemic,due to** are most used ones"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n\n<font size=\"+2\" color=\"indigo\"><b>10. N-Gram</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Listing below the top N-gram sequential words used in covid tweets"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(covid['refine_text'],(1,1),20)\nbigram_df=ngram_df(covid['refine_text'],(2,2),20)\ntrigram_df=ngram_df(covid['refine_text'],(3,3),20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top N Grams',xaxis_title=\" \",yaxis_title=\" \", showlegend=False,title_x=0.5,\n                  height=1200,template=\"plotly_dark\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **new,people,pandemic** are most used single word.\n* **cases death,tested postive and active cases** are most used bigrams.\n* **cases new deaths,help slow spread,slow spread indentity** are most used trigrams\n\nAll observed words are biased towards covid detection and admission of patients.People tend to worry lot about other peoples around."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n\n<font size=\"+2\" color=\"indigo\"><b>11. Modeling</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"I will be using 40% of the data to train.I have optimized the text by removing noisy and unwanted characters present in tweet."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sentiment=sentiment.sample(int(sentiment.shape[0]*0.4))\nsentiment=sentiment[['text','target']]\nsentiment['emotion']=np.where(sentiment['target']==0,'negative',np.where(sentiment['target']==2,'neutral',np.where(sentiment['target']==4,'postitive',\"none\")))\nsentiment['target']=np.where(sentiment['target']==4,1,sentiment['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Optimizing text\nsentiment['refine_text']=sentiment['text'].str.lower()\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_tag(str(x)))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_mention(str(x)))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_hash(str(x)))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_newline(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_url(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_number(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_punct(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:text_strip(x))\nsentiment['text_length']=sentiment['refine_text'].str.split().map(lambda x: len(x))\n\n# Removing stopwords\nstop_words = set(stopwords.words('english'))\nsentiment['refine_text'] =  sentiment['refine_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Setting tha randomly\ntrain_data=sentiment[['refine_text','target']]\n\ntrain_sent=np.array(train_data['refine_text'])\ntrain_label=np.array(train_data['target'])\n\nx_train, x_test, y_train, y_test = train_test_split(train_sent, train_label, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning Parametes (Change below parameters/add or remove layers to get more accurate result)"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Hyper Parameters\nvocab_size = 1000\nembedding_dim =16 \nmax_length = 50\ntrunc_type='post'\noov_tok = \"<OOV>\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(x_train)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(x_train)\ntraining_padded = pad_sequences(training_sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(x_test)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\n\ntraining_label=y_train\ntesting_label=y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 1 # Edit here\nhistory=model.fit(training_padded, training_label, epochs=num_epochs, validation_data=(testing_padded, testing_label))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have almost hedged our model accuracy with validation accuracy.The model achieved around **75%** accuracy which is not too good or too bad to measure.Still we can tune hyper parameters and produce more good results."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n\n<font size=\"+2\" color=\"indigo\"><b>12. Prediction</b></font><br>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tweet_df['refine_text']=tweet_df['text'].str.lower()\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_tag(str(x)))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_mention(str(x)))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_hash(str(x)))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_newline(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_url(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_number(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_punct(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:text_strip(x))\ntweet_df['text_length']=tweet_df['refine_text'].str.split().map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sen = np.array(tweet_df.refine_text)\nseq = tokenizer.texts_to_sequences(sen)\npadd = pad_sequences(seq, maxlen=max_length, truncating=trunc_type)\nresult=model.predict(padd)\nvalidated_result=np.where(result>0.5,1,0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pred_df=pd.DataFrame({'text':tweet_df['refine_text'],'pred_sentiment':list(validated_result),'country':tweet_df['filtered_name'],'text_length':tweet_df['text_length']})\npred_df['pred_sentiment']=np.where(pred_df['pred_sentiment']>0.5,1,0)\npred_df[['text','pred_sentiment']].head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we will observe the post prediction visualizations.It may differ slightly for every change in models."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n\n<font size=\"+2\" color=\"indigo\"><b>13. Angry Bird vs Happy Bird</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"We will find most frequent used words on Negative tweet & Positive tweet."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom io import BytesIO\n\nresponse = requests.get('https://banner2.cleanpng.com/20180723/vvy/kisspng-computer-icons-clip-art-twitter-logo-vector-5b5693f7952128.7797517715324006316109.jpg')\nbird = np.array(Image.open(BytesIO(response.content)))\n\n# d = '../input/twitter/'\n# bird = np.array(Image.open(d + 'twitter_mask.png'))\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==0]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==1]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive Sentiment',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n<font size=\"+2\" color=\"indigo\"><b>14. Sentiment length</b></font><br>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Average length of Positive Sentiment tweets : {} \".format(round(pred_df[pred_df['pred_sentiment']==1]['text_length'].mean(),2)))\nprint(\"Average length of Negative Sentiment tweets : {} \".format(round(pred_df[pred_df['pred_sentiment']==0]['text_length'].mean(),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=pred_df[pred_df['pred_sentiment']==1]['text_length'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='green', opacity=0.6,name=\"Positive\",\n                               x0='Positive')\n             )\n\nfig.add_trace(go.Violin(y=pred_df[pred_df['pred_sentiment']==0]['text_length'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='red', opacity=0.6,name=\"Negative\",\n                               x0='Negative')\n             )\n\nfig.update_traces(box_visible=False, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - Tweet Length \",\n                  title_x=0.5)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average words for positive is around 13 whereas for negative is around 15.People tend to type long when they are in negative mode."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a>\n<font size=\"+2\" color=\"indigo\"><b>15. World Emotions</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Let us look at heatmap of positive and negative sentiment of each countries in the world."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_pos_country=pred_df[pred_df['pred_sentiment']==1]['country'].value_counts().reset_index().rename(columns={'index':'country','country':'count'})\n\nall_pos_country_df=pd.merge(all_pos_country,country_code[['COUNTRY','CODE']],left_on=\"country\",right_on=\"COUNTRY\",how=\"left\")\nall_pos_country_df=all_pos_country_df[all_pos_country_df['COUNTRY'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=go.Choropleth(\n    locations = all_pos_country_df['CODE'],\n    z = all_pos_country_df['count'],\n    text = all_pos_country_df['COUNTRY'],\n    colorscale = 'greens', \n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = '# of Tweets ',\n))\n\nfig.update_layout(\n    title_text='Tweets over the world - ({} - {})'.format(covid['only_date'].sort_values()[0].strftime(\"%d/%m/%Y\"),\n                                                       covid['only_date'].sort_values().iloc[-1].strftime(\"%d/%m/%Y\")),title_x=0.5,\n    \n    geo=dict(\n        showframe=True,\n        showcoastlines=False,\n        projection_type='equirectangular',\n    )\n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Positive Sentiment counts"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_pos_country_df[['country','count']][:5]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_neg_country=pred_df[pred_df['pred_sentiment']==0]['country'].value_counts().reset_index().rename(columns={'index':'country','country':'count'})\n\nall_neg_country_df=pd.merge(all_neg_country,country_code[['COUNTRY','CODE']],left_on=\"country\",right_on=\"COUNTRY\",how=\"left\")\nall_neg_country_df=all_neg_country_df[all_neg_country_df['COUNTRY'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=go.Choropleth(\n    locations = all_neg_country_df['CODE'],\n    z = all_neg_country_df['count'],\n    text = all_neg_country_df['COUNTRY'],\n    colorscale = 'reds',  \n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = '# of Tweets',\n))\n\nfig.update_layout(\n    title_text='Tweets over the world - ({} - {}) '.format(covid['only_date'].sort_values()[0].strftime(\"%d/%m/%Y\"),\n                                covid['only_date'].sort_values().iloc[-1].strftime(\"%d/%m/%Y\")),title_x=0.5,\n    geo=dict(\n        showframe=True,\n        showcoastlines=False,\n        projection_type='equirectangular',\n    )\n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Negative Sentiment counts"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"all_neg_country_df[['country','count']][:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparitive to positive sentiments,negatives are low in numbers.This is good insight which displays people are not hatred always instead handling this pandemic with good positive gesture"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\"></a>\n<font size=\"+2\" color=\"indigo\"><b>16. Top Countries - Emotional Bird</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"Now we will see the most tweeted countries positive and negative emotional words. US,India & UK are the most involved countries in twitter tweeting on CORONA.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==0)&(pred_df['country']=='United States')]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative US Bird',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==1)&(pred_df['country']=='United States')]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive US Bird',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==0)&(pred_df['country']=='India')]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative India Bird ',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==1)&(pred_df['country']=='India')]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive India Bird ',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==0)&(pred_df['country']=='United Kingdom')]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative UK Bird ',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==1)&(pred_df['country']=='United Kingdom')]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive UK Bird ',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\"></a>\n<font size=\"+2\" color=\"indigo\"><b>17. Homework</b></font><br>"},{"metadata":{},"cell_type":"markdown","source":"**In covid-19-nlp-text-classification we have 6 classes, but our model only predict two classes. So all you will re-build model for 6 classes and predict this dataset again.**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a>\n<font size=\"+2\" color=\"indigo\"><b>18. Vietnamese Sentiment Analyst</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"vn_sentiment=pd.read_csv('../input/vietnamese-sentiment/data - data.csv',usecols=[\"comment\",\"label\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vn_sentiment.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category = vn_sentiment['label'].unique()\ncategory_to_id = {cate: idx for idx, cate in enumerate(category)}\nid_to_category = {idx: cate for idx, cate in enumerate(category)}\nprint(category_to_id)\nprint(id_to_category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_label = vn_sentiment['label']\ndata_label = pd.DataFrame(data_label, columns=['label']).groupby('label').size()\ndata_label.plot.pie(figsize=(15, 15), autopct=\"%.2f%%\", fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In here we use transformer model to tokenize word"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In here, we have two options: bert and gpt pretrained model to tokenize word."},{"metadata":{},"cell_type":"markdown","source":"Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"vn_sentiment.isnull().values.any()\n\nvn_sentiment.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = []\nsentences = list(vn_sentiment['comment'])\nfor sen in sentences:\n    reviews.append(sen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews[10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vn_sentiment.label.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vn_sentiment.label.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = vn_sentiment['label']\n\n# NEU : 0 , POS : 1 , NEG : 2\ny = np.array(list(map(lambda x: 1 if x==\"POS\" else (2 if x == \"NEG\" else 0 ), y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(reviews[10],y[10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import libaries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### I used bert pretrained model to tokenize, you need use phobert to have a good performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"BertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n                            trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_reviews(text_reviews):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_reviews = [tokenize_reviews(review) for review in reviews]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenized_reviews[10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random \n\nreviews_with_len = [[review, y[i], len(review)]\n                 for i, review in enumerate(tokenized_reviews)]\n# Word Vector - Label - length of sequence.\nprint(reviews_with_len[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle random dataset\nrandom.shuffle(reviews_with_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_with_len[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once the data is shuffled, we will sort the data by the length of the reviews. To do so, we will use the sort() function of the list and will tell it that we want to sort the list with respect to the third item in the sublist i.e. the length of the review."},{"metadata":{"trusted":true},"cell_type":"code","source":"#reviews_with_len.sort(key=lambda x: x[2])\nsorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]\nsorted_reviews_labels\nprocessed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32 # You can edit here i.e 128\nbatched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(batched_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\nTOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\nTEST_BATCHES = TOTAL_BATCHES // 10\nbatched_dataset.shuffle(TOTAL_BATCHES)\ntest_data = batched_dataset.take(TEST_BATCHES)\ntrain_data = batched_dataset.skip(TEST_BATCHES)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build model, you need to re-build end-to end or using pretrained model as Bert, GPT-2"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TEXT_MODEL(tf.keras.Model):\n    \n    def __init__(self,\n                 vocabulary_size,\n                 embedding_dimensions=128,\n                 cnn_filters=50,\n                 dnn_units=512,\n                 model_output_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name=\"text_model\"):\n        super(TEXT_MODEL, self).__init__(name=name)\n        \n        self.embedding = layers.Embedding(vocabulary_size,\n                                          embedding_dimensions)\n        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=2,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=3,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=4,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        \n        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if model_output_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=model_output_classes,\n                                           activation=\"softmax\")\n    \n    def call(self, inputs, training):\n        l = self.embedding(inputs)\n        l_1 = self.cnn_layer1(l) \n        l_1 = self.pool(l_1) \n        l_2 = self.cnn_layer2(l) \n        l_2 = self.pool(l_2)\n        l_3 = self.cnn_layer3(l)\n        l_3 = self.pool(l_3) \n        \n        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n        concatenated = self.dense_1(concatenated)\n        concatenated = self.dropout(concatenated, training)\n        model_output = self.last_dense(concatenated)\n        \n        return model_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_LENGTH = len(tokenizer.vocab)\nEMB_DIM = 200\nCNN_FILTERS = 100\nDNN_UNITS = 256\nOUTPUT_CLASSES = 3\n\nDROPOUT_RATE = 0.2\n\nNB_EPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n                        embedding_dimensions=EMB_DIM,\n                        cnn_filters=CNN_FILTERS,\n                        dnn_units=DNN_UNITS,\n                        model_output_classes=OUTPUT_CLASSES,\n                        dropout_rate=DROPOUT_RATE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# es = EarlyStopping(monitor='val_f1_m', mode='max', verbose=1, patience=5)\n# reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', factor=0.2, patience=8, min_lr=1e7)\n# checkpoint = ModelCheckpoint('best_full.h5', monitor='val_f1_m', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if OUTPUT_CLASSES == 2:\n    text_model.compile(loss=\"binary_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"accuracy\"])\nelse:\n    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"sparse_categorical_accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = text_model.fit(train_data,validation_data=test_data, epochs=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualization \nYou can put a loss visualization in here"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 5))\n\nplt.subplot(121)\n\n# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model loss')\n\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Pylot iou-score \nplt.subplot(122)\n\n# Get training and test loss histories\ntraining_loss = history.history['sparse_categorical_accuracy']\ntest_loss = history.history['val_sparse_categorical_accuracy']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Accuracy')\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Accuracy', 'Val Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_to_size(vec, size):\n  zeros = [0] * (size - len(vec))\n  vec.extend(zeros)\n  return vec\n\ndef sample_predict(sample_pred_text, pad):\n  encoded_sample_pred_text = tokenize_reviews(sample_pred_text)\n\n  if pad:\n    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n  predictions = text_model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n\n  return (predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_pred_text = ('Bớt đùa đi, dạo này tao không còn vui tính như trước đâu.')\n\npredictions = sample_predict(sample_pred_text, pad=True)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}