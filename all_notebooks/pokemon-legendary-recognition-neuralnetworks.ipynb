{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nprint('\\n ')\nprint('Getting traing dataset...')\ndata = pd.read_csv('../input/pokemon/Pokemon.csv')\nprint('Traing data set obtained. \\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def type_numbering(string) : \n    number = 0\n    if string == 'Normal' :\n        number = 1\n    elif string == 'Fire' :\n        number = 2\n    elif string == 'Fighting' :\n        number = 3\n    elif string == 'Water' :\n        number = 4\n    elif string == 'Flying' :\n        number = 5\n    elif string == 'Grass' :\n        number = 6\n    elif string == 'Poison' :\n        number = 7\n    elif string == 'Electric' :\n        number = 8\n    elif string == 'Ground' :\n        number = 9\n    elif string == 'Psychic' :\n        number = 10\n    elif string == 'Rock' :\n        number = 11\n    elif string == 'Ice' :\n        number = 12\n    elif string == 'Bug' :\n        number = 13\n    elif string == 'Dragon' :\n        number = 14\n    elif string == 'Ghost' :\n        number = 15\n    elif string == 'Dark' :\n        number = 16\n    elif string == 'Steel' :\n        number = 17\n    elif string == 'Fairy' :\n        number = 18\n    else :\n        number = 0\n    \n    return number;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Description of the Neural Network\n\nAfter the standard data preprocessing, we will need to standardize them via the StandardScaler function of sklearn.preprocessing module. \n\nAfter that, we define a Neural Network with three hidden layers and an output layers, that hare\n1. 4 perceptrons with 11 inputs with REctified Linear Units (ReLU) activation;\n2. 4 perceptrons with REctified Linear Units (ReLU) activation;\n3. Again, 4 perceptrons with REctified Linear Units (ReLU) activation.\n4. An output layer with sigmoid activation \nAs this is a binary classification problem, we use *binary_crossentropy* to calculate the loss function between the actual output and the predicted output\n\nWe use the **Adaptive moment estimation (ADAM) optimizer**.\n\nThe output of the function will be \n1. Accuray and Loss value of the NN\n2. The plot of Accuray and Loss history for each epoch of the NN\n3. The confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def NN_classifier(data, test_size=0.3, batch_size = 10, epochs=10):\n    import numpy as np # linear algebra\n    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report,confusion_matrix\n    \n    print('Splitting data...')\n    df = data\n    df['Type 1'] = data['Type 1'].apply(type_numbering)\n    df['Type 2'] = data['Type 2'].apply(type_numbering)\n    X = df.drop('Legendary',axis=1).drop('Name', axis=1)\n    y = df['Legendary']\n    lenght = len(df.drop('Legendary',axis=1).drop('Name', axis=1).columns)\n    \n    # We need to standardize the data; \n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X = sc.fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n    print('Splitting done. \\n')\n    \n    \n    \n    print('Initializing classifier...')\n    from keras import Sequential\n    from keras.layers import Dense\n    \n    clf = Sequential()\n    \n    #First Hidden Layer\n    clf.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=lenght))\n    \n    #Second  Hidden Layer\n    clf.add(Dense(4, activation='relu', kernel_initializer='random_normal'))   \n    \n    #Third  Hidden Layer\n    clf.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n\n    #Output Layer\n    clf.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n    \n    #Compiling the neural network\n    clf.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n        # As this is a binary classification problem, we use binary_crossentropy \n        # to calculate the loss function between the actual output and the predicted output.  \n        # As optimizer we use Adaptive moment estimation (ADAM)\n    print('\\n')\n    print(clf.summary())\n    print('\\n')\n    history = clf.fit(X_train,y_train, batch_size=batch_size, epochs=epochs)\n    \n    print('Initialization done. \\n')\n    \n    \n    print('Evaluating the classifier...')\n    eval_model = clf.evaluate(X_train, y_train)\n    \n    epoch_nums = range(1,epochs+1)\n    training_loss = history.history[\"loss\"]\n    train_acc = history.history[\"accuracy\"]\n    \n    plt.figure(figsize=(13,5))\n\n    plt.subplot(1,2,1)\n    plt.plot(epoch_nums, training_loss)\n    plt.xlabel('epoch', fontsize=18)\n    plt.ylabel('loss', fontsize=18)\n    \n\n    plt.subplot(1,2,2)\n    plt.plot(epoch_nums, train_acc)\n    plt.xlabel('epoch', fontsize=18)\n    plt.ylabel('Accuracy', fontsize=18)\n    \n    \n    print('Accuray: ', eval_model[1])\n    print('Loss: ', eval_model[0])\n    print('\\n ')\n    \n    y_pred = clf.predict(X_test)\n    y_pred = (y_pred>0.5) #retain only output greater than 0.5\n    \n    cm = confusion_matrix(y_test, y_pred)\n    print(cm)\n    df_cm = pd.DataFrame(cm, index = ['Non-Legendary', 'Legendary'], columns = ['Non-Legendary', 'Legendary'])\n    plt.figure(figsize = (7,7))\n    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n    plt.xlabel(\"Predicted Class\", fontsize=18)\n    plt.ylabel(\"True Class\", fontsize=18)\n    \n    \n    print('\\n ')\n    print('Done.')\n    \n    return clf\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_classifier(data, batch_size=10, epochs=100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}