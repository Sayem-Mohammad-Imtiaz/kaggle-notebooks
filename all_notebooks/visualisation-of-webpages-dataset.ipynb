{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <font style=\"color:red;\">Visualisation of Malicious & Benign Webpages Dataset</font>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Basic Initialization</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Installing mandatory libraries\n\n!pip install geonamescache\n#!pip install palettable\n#!pip install -U textblob\n!pip install profanity_check \n#!pip install cufflinks\n#!pip install seaborn\n!pip install tld\n#!pip install plotly\n#!pip install plotly plotly-orca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To support both python 2 and python 3\nfrom __future__ import division, print_function, unicode_literals\n# Common imports\nimport pandas as pd\nimport numpy as np\nimport time\nimport os\nimport sklearn\nimport seaborn as sns\nimport warnings\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom random import randrange\n#Disabling Warnings\nwarnings.filterwarnings('ignore')\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n# To plot figures\n%matplotlib inline\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nsns.set_palette(['green','red'])#Fixing the Seaborn default palette","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verifying pathname of dataset before loading\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename));\n#        print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Loading Dataset</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Datasets\ndef loadDataset(file_name):\n    df = pd.read_csv(file_name)\n    return df\n\ndf_train = loadDataset(\"/kaggle/input/dataset-of-malicious-and-benign-webpages/Webpages_Classification_train_data.csv/Webpages_Classification_train_data.csv\")\ndf_test = loadDataset(\"/kaggle/input/dataset-of-malicious-and-benign-webpages/Webpages_Classification_test_data.csv/Webpages_Classification_test_data.csv\")\n#Ensuring correct sequence of columns \ndf_train = df_train[['url','ip_add','geo_loc','url_len','js_len','js_obf_len','tld','who_is','https','content','label']]\ndf_test = df_test[['url','ip_add','geo_loc','url_len','js_len','js_obf_len','tld','who_is','https','content','label']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Details of Dataset: Tabular</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The Dataset (Training Dataset comprising of 1.2 million records) is shown below in tablular form. Please note the eleven Attributes/Features in the dataset. The last attribute is the Class Label, with categorical values 'good' and 'bad' for Benign and Malicious webpages respectively.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As can be seen from the training dataset above, it has 1.2 million records. The test dataset has 0.364 million records (not shown here for the sake of simplicity). The dataset comprises of 10 features as mentioned earlier (apart from Class Label). While each of these attributes will be discussed and visualised in detail in this notebook, it is worth mentioning few aspects about the content attribute here itself. The content attribute contains the text retrived from the webpages (both textual data and JavaScript Code) after carrying out necessary cleaning. This 'content' attribute is importatnt as it can be used for extracting more features or carrying our detailed analysis of the webpages.</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of Class Label & its Imbalance</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The Class Label for this dataset is given in the last column. It has two values- 'good' and 'bad' corresponding to Benign and Malicious Webpages respectively. On the Internet, Malicious Webpages are few compared to Benign Webpages. This inequality shows in our dataset as well, since it has been scraped from Internet. The Class Label and its inequality is visualised and analysed below in detail.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class Distribution of Labels\ndf_train.groupby('label').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of Postives and Negatives in the Dataset\npos,neg = df_train['label'].value_counts()\ntotal = neg + pos\nprint ('Total of Samples: %s'% total)\nprint('Positive: {} ({:.2f}% of total)'.format(pos, 100 * pos / total))\nprint('Negative: {} ({:.2f}% of total)'.format(neg, 100 * neg / total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar Plot of Malicious and Benign Websites\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\nfig = plt.figure(figsize = (12,4))\n#title = fig.suptitle(\"Plot of Malicious and Benign Webpages\", fontsize=14)\nfig.subplots_adjust(top=0.85, wspace=0.3)\n#Bar Plot\nax1 = fig.add_subplot(1,2,1)\nax1.set_xlabel(\"Class Labels\")\nax1.set_ylabel(\"Frequency\") \nax1.title.set_text('Bar Plot: Malicious & Benign Webpages')\nlabels = df_train['label'].value_counts()\nw = (list(labels.index), list(labels.values))\nax1.tick_params(axis='both', which='major')\nbar = ax1.bar(w[0], w[1], color=['green','red'], edgecolor='black', linewidth=1)\n#Stacked Plot \nax2 = fig.add_subplot(1,2,2)\nax2.title.set_text('Stack Plot: Malicious & Benign Webpages')\n# create dummy variable then group by that set the legend to false because we'll fix it later\ndf_train.assign(dummy = 1).groupby(['dummy','label']).size().groupby(level=0).apply(\n    lambda x: 100 * x / x.sum()).to_frame().unstack().plot(kind='bar',stacked=True,legend=False,ax=ax2,color={'red','green'}, linewidth=0.50, ec='k')\nax2.set_xlabel('Benign/Malicious Webpages')# or it'll show up as 'dummy' \nax2.set_xticks([])# disable ticks in the x axis\ncurrent_handles, _ = plt.gca().get_legend_handles_labels()#Fixing Legend\nreversed_handles = reversed(current_handles)\ncorrect_labels = reversed(['Malicious','Benign'])\nplt.legend(reversed_handles,correct_labels)\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n#Saving the Figs\n#figc = plt.gcf()\n#plt.tight_layout()\n#figc.savefig(\"imgs/Fig01&02: Bar Plot & Stack Plot of Malicious & Benign Webpages.svg\")\n#extent = ax1.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig01: Bar Plot of Class Labels.svg\",bbox_inches=extent.expanded(1.5, 1.4))\n#extent = ax2.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig02: Stack Plot of Class Labels.svg\",bbox_inches=extent.expanded(1.5, 1.4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pie Chart of Malicious and Benign Webpages Distribution\nfig = plt.figure(figsize = (14,5))\nExplode = [0,0.1]\nplt.pie(w[1],explode=Explode,labels=w[0],shadow=False,startangle=45,\n        colors=['green','red'],autopct='%.2f%%',textprops={'fontsize': 15})\nplt.axis('equal')\nplt.legend(title='Class Labels of Webpages',loc='lower right')\n#fig.savefig('imgs/Fig03:Pie Chart Distribution of Class Labels.svg')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As can be seen from the visualisations above, this dataset has significant class imbalance. Hence, during any machine learning process, adequate measures will have to be undertaken to handle or compensate this imbalance in order to get accurate results</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of 'url' Attribute</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The first column of the dataset has the 'url' attribute. Below, we carryout visualisation and analysis of webpage URLs by splitting it into its constituent words. These words are then used to generate vectorized value for each URL, giving a Profanity score based on good or bad words found in the URL. This score is then plotted.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorising the URL Text\nfrom urllib.parse import urlparse\nfrom tld import get_tld\n\nstart_time= time.time()\n#Function for cleaning the URL text before vectorization\ndef clean_url(url):\n    url_text=\"\"\n    try:\n        domain = get_tld(url, as_object=True)\n        domain = get_tld(url, as_object=True)\n        url_parsed = urlparse(url)\n        url_text= url_parsed.netloc.replace(domain.tld,\" \").replace('www',' ') +\" \"+ url_parsed.path+\" \"+url_parsed.params+\" \"+url_parsed.query+\" \"+url_parsed.fragment\n        url_text = url_text.translate(str.maketrans({'?':' ','\\\\':' ','.':' ',';':' ','/':' ','\\'':' '}))\n        url_text.strip(' ')\n        url_text.lower()\n    except:\n        url_text = url_text.translate(str.maketrans({'?':' ','\\\\':' ','.':' ',';':' ','/':' ','\\'':' '}))\n        url_text.strip(' ')\n    return url_text\n\ndf_train['url_vect'] = df_train['url'].map(clean_url)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\n\n# give profanity score to each URL using the Profanity_Check Library\nfrom profanity_check import predict_prob\n\nstart_time= time.time()\n#Function for calculating profanity in a dataset column\ndef predict_profanity(df):\n    arr=predict_prob(df['url_vect'].astype(str).to_numpy())\n    arr= arr.round(decimals=3)\n    df['url_vect'] = pd.DataFrame(data=arr,columns=['url_vect'])\n    #df['url']= df_test['url'].astype(float).round(decimals=3) #rounding probability to 3 decimal places\n    return df['url_vect']\n\ndf_train['url_vect']= predict_profanity(df_train)\n\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\n#df_trial\ndf_trial =df_train.iloc[0:1000,]\ndf_trial_good = df_trial.loc[df_train['label']=='good']\ndf_trial_bad = df_trial.loc[df_train['label']=='bad']\nfig = go.Figure()\nt1= go.Histogram(x=df_trial_good['url_vect'],name='Benign Webpages',marker_color='green')\nt2= go.Histogram(x=df_trial_bad['url_vect'],name='Malicious Webpages',marker_color='red')\nfig.add_trace(t1)\nfig.add_trace(t2)\nfig.update_layout(title=\"URL Analysis:Profanity Score of Vectorized URLs\",xaxis_title=\"Profanity Score\",yaxis_title=\"Count\")\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\n#fig.write_image(\"imgs/Fig04:URL Analysis-Profanity Score.svg\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>While we see more red bars on the right, most green bars are on the left. Thus,the avergage profanity score of URLs for malicious webpages is more. </I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of 'ip_add' & 'geo_loc' Attributes</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The 'ip_add' and 'geo_loc' are the 2nd and 3rd columns in the dataset. The 'ip_add' gives IP address of the web server where the webpage is hosted. The 'geo_loc' has been computed from the IP Address using the GeoIP Database and gives the country where the IP Address is located. The country wise distribution of IP Addresses of webpages in the dataset is plotted below on world map.  </I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imports\nfrom matplotlib.collections import PatchCollection\nfrom mpl_toolkits.basemap import Basemap\nfrom matplotlib.patches import Polygon\nfrom matplotlib.collections import PatchCollection\nfrom palettable.cartocolors.sequential import Purp_5\nfrom palettable.colorbrewer.sequential import Reds_6\n\n# Making of a DataFrame of Countrywise Count and categorized as Malcious and Benign\ndf_malicious = df_train.loc[df_train['label']=='bad']\ndf_benign = df_train.loc[df_train['label']=='good']\ndf_geo = pd.DataFrame(df_train['geo_loc'].value_counts())\ndf_geo_malicious = pd.DataFrame(df_malicious['geo_loc'].value_counts())\ndf_geo_benign = pd.DataFrame(df_benign['geo_loc'].value_counts())\ndf_geo.reset_index(inplace=True)\ndf_geo.rename(columns = {'index':'country','geo_loc':'count'}, inplace = True) \ndf_geo_malicious.reset_index(inplace=True)\ndf_geo_malicious.rename(columns = {'index':'country','geo_loc':'count'}, inplace = True) \ndf_geo_benign.reset_index(inplace=True)\ndf_geo_benign.rename(columns = {'index':'country','geo_loc':'count'}, inplace = True) \n# Mapping ISO Codes\nfrom geonamescache.mappers import country\nmapper = country(from_key='name', to_key='iso3')\ndf_geo['country'] = df_geo['country'].apply(lambda x: mapper(x))\ndf_geo_malicious['country'] = df_geo_malicious['country'].apply(lambda x: mapper(x))\ndf_geo_benign['country'] = df_geo_benign['country'].apply(lambda x: mapper(x))\n#Droping NAN values and Making ISO Codes as index\ndf_geo.dropna(inplace=True)\ndf_geo_malicious.dropna(inplace=True)\ndf_geo_benign.dropna(inplace=True)\ndf_geo.reset_index(inplace=True, drop=True)\ndf_geo_malicious.reset_index(inplace=True, drop=True)\ndf_geo_benign.reset_index(inplace=True, drop=True)\ndf_geo.set_index(\"country\",inplace=True)\ndf_geo_malicious.set_index(\"country\",inplace=True)\ndf_geo_benign.set_index(\"country\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting all IP Addresses on World Map\nshapefile = '/kaggle/input/worldmapshapefile/shapefile/ne_10m_admin_0_countries_lakes'# Shape File in folder Shapefile\nnum_colors = 10\ntitle = 'Geographical Distribution of IP Addresses Captured in Dataset'\ndescription = \"   Note: IP Addresses represent Addresses of the Webservers where these Webpages were hosted. Total IP Addresses Captured : 1.2 million\"\n#Adding bin values to dataset df_geo for the Color\nvalues = df_geo['count']\ncm = Purp_5.mpl_colormap\n#cm = plt.get_cmap('Blues') #Using Matploit's Color Map API\nscheme = [cm(i / num_colors) for i in range(num_colors)]\nbins = np.linspace(values.min(), values.max(), num_colors)\ndf_geo['bin'] = np.digitize(values, bins) -1 \nfig = plt.figure(figsize=(22, 12))\nax = fig.add_subplot(111, facecolor='w', frame_on=False)\nfig.suptitle(title, fontsize=30, y=.95)\nm = Basemap(lon_0=0, projection='robin')\nm.drawmapboundary(color='w')\nm.readshapefile(shapefile,'units', color='#444444', linewidth=.2)\nm.drawmapboundary(color='w')\nm.readshapefile(shapefile,'units', color='#444444', linewidth=.2)\nm.drawcoastlines(linewidth=0.1)\nm.drawmapboundary(fill_color='#add8e6')\nm.drawcountries(linewidth=0.1)\nfor info, shape in zip(m.units_info, m.units):\n    try:\n        iso3 = info['ADM0_A3']\n        if iso3 not in df_geo.index:\n            color = '#FFFFFF'\n        else:\n            color = scheme[df_geo.loc[iso3]['bin']]\n    except Exception as msg:\n        print(iso3)\n        print(msg)\n    patches = [Polygon(np.array(shape), True)]\n    pc = PatchCollection(patches)\n    pc.set_facecolor(color)\n    ax.add_collection(pc)\n# Cover up Antarctica so legend can be placed over it.\nax.axhspan(0, 1000 * 1800, facecolor='w', edgecolor='w', zorder=2)\n# Draw color legend.\nax_legend = fig.add_axes([0.32, 0.14, 0.4, 0.03], zorder=3)\ncmap = mpl.colors.ListedColormap(scheme)\ncb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cmap, ticks=bins, boundaries=bins, orientation='horizontal')\ncb.ax.set_xticklabels([str(round(i)) for i in bins])\n# Set the map footer.\nplt.annotate(description, xy=(-.8, -3.2), size=14, xycoords='axes fraction')\n#fig.savefig('imgs/Fig05:Geographic Distribution of all IP Addresses.svg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting IP Addresses of Malicious Webpages \nshapefile = '/kaggle/input/worldmapshapefile/shapefile/ne_10m_admin_0_countries_lakes'# Shape File in folder Shapefile\nnum_colors = 10\ntitle = 'Geographical Distribution of IP Addresses: Malicious Webpages'\ndescription = \"Note: Location shown here depicts the Webserver where these Webpages were hosted. Total Malicious Webpages : 27253\"\n#Adding bin values to dataset df_geo_malicious for the Color\nvalues = df_geo_malicious['count']\ncm = Reds_6.mpl_colormap\n#cm = plt.get_cmap('autumn_r') #Using Matploit's Color Map API\nscheme = [cm(i / num_colors) for i in range(num_colors)]\nbins = np.linspace(values.min(), values.max(), num_colors)\ndf_geo_malicious['bin'] = np.digitize(values, bins) -1 \nfig = plt.figure(figsize=(22, 12))\nax = fig.add_subplot(111, facecolor='w', frame_on=False)\nfig.suptitle(title, fontsize=30, y=.95)\nm = Basemap(lon_0=0, projection='robin')\nm.drawmapboundary(color='w')\nm.readshapefile(shapefile,'units', color='#444444', linewidth=.2)\nm.drawcoastlines(linewidth=0.1)\nm.drawmapboundary(fill_color='#add8e6')\nm.drawcountries(linewidth=0.1)\nfor info, shape in zip(m.units_info, m.units):\n    try:\n        iso3 = info['ADM0_A3']\n        if iso3 not in df_geo_malicious.index:\n            color = '#ffffff'\n        else:\n            color = scheme[df_geo_malicious.loc[iso3]['bin']]\n    except Exception as msg:\n        print(iso3)\n        print(msg)\n    patches = [Polygon(np.array(shape), True)]\n    pc = PatchCollection(patches)\n    pc.set_facecolor(color)\n    ax.add_collection(pc)\n# Cover up Antarctica so legend can be placed over it.\nax.axhspan(0, 1000 * 1800, facecolor='w', edgecolor='w', zorder=2)\n# Draw color legend.\nax_legend = fig.add_axes([0.35, 0.14, 0.3, 0.03], zorder=3)\ncmap = mpl.colors.ListedColormap(scheme)\ncb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cmap, ticks=bins, boundaries=bins, orientation='horizontal')\ncb.ax.set_xticklabels([str(round(i)) for i in bins])\n# Set the map footer.\nplt.annotate(description, xy=(-.8, -3.2), size=14, xycoords='axes fraction')\n#fig.savefig('imgs/Fig06:Geographic Distribution of Malicious IP Addresses.svg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting IP Addresses of Benign Webpages\nshapefile = '/kaggle/input/worldmapshapefile/shapefile/ne_10m_admin_0_countries_lakes'# Shape File in folder Shapefile\nnum_colors = 10\ntitle = 'Geographical Distribution of IP Addresses: Benign Webpages'\ndescription = \"Location shown here depicts the Webserver where these Webpages were hosted. Total Benign Webpages: 1.172 million\"\n#Adding bin values to dataset df_geo for the Color\nvalues = df_geo_benign['count']\ncm = plt.get_cmap('Greens') #Using Matploit's Color Map API\nscheme = [cm(i / num_colors) for i in range(num_colors)]\nbins = np.linspace(values.min(), values.max(), num_colors)\ndf_geo_benign['bin'] = np.digitize(values, bins) -1 \nfig = plt.figure(figsize=(22, 12))\nax = fig.add_subplot(111, facecolor='w', frame_on=False)\nfig.suptitle(title, fontsize=30, y=.95)\nm = Basemap(lon_0=0, projection='robin')\nm.drawmapboundary(color='w')\nm.readshapefile(shapefile,'units', color='#444444', linewidth=.2)\nfor info, shape in zip(m.units_info, m.units):\n    try:\n        iso3 = info['ADM0_A3']\n        if iso3 not in df_geo_benign.index:\n            color = '#FFFFFF'\n        else:\n            color = scheme[df_geo_benign.loc[iso3]['bin']]\n    except Exception as msg:\n        print(iso3)\n        print(msg)\n    patches = [Polygon(np.array(shape), True)]\n    pc = PatchCollection(patches)\n    pc.set_facecolor(color)\n    ax.add_collection(pc)\n# Cover up Antarctica so legend can be placed over it.\nax.axhspan(0, 1000 * 1800, facecolor='w', edgecolor='w', zorder=2)\n# Draw color legend.\nax_legend = fig.add_axes([0.35, 0.14, 0.4, 0.03], zorder=3)\ncmap = mpl.colors.ListedColormap(scheme)\ncb = mpl.colorbar.ColorbarBase(ax_legend, cmap=cmap, ticks=bins, boundaries=bins, orientation='horizontal')\ncb.ax.set_xticklabels([str(round(i)) for i in bins])\n# Set the map footer.\nplt.annotate(description, xy=(-.8, -3.2), size=14, xycoords='axes fraction')\n#fig.savefig('imgs/Fig07:Geographic Distribution of Benign IP Addresses.svg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As can be seen from the three maps above, the dataset covers complete globe. Majority of the IP addresses are active in USA and China, but that is because majority of web servers exist there. From these visualisations, no distinct pattern of malicious or benign webpages with respect to geographic location emerges.</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of Numerical Attributes: 'url_len', 'js_len' and 'js_obf_len'</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The 'url_len', 'js_len' and js_obf_len' are the 4th, 5th and 6th columns of the dataset respectively. All these three are numerical attributes. Hence, they have been discussed and visualised together in this section. First, a univariate visualisation of these individual attributes will be carried out. Thereater, a trivaritate visualisation of these attributes will be carried out to detect patterns/correlations amongst them. Then, based on correlation found, bivariate analysis of related attributes will be carried out. First, let us see with the univariate visualisation of these attributes.</I>"},{"metadata":{},"cell_type":"markdown","source":"### Checking the 'url_len' Attribute using Univariate Plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# url_len analysis vis-a-vis malicious and benign webpages\ndf_train_bad=df_train.loc[df_train['label']=='bad']\ndf_train_good=df_train.loc[df_train['label']=='good']\n# Histogram of Url Length: Malicious Webpages \nfig = plt.figure(figsize =(10,10))\ntitle = fig.suptitle(\"Url Length Distributioins: Malicious vs Benign Webpages\")\nfig.subplots_adjust(wspace=0.6,hspace=0.4)\nax = fig.add_subplot(3,2,1)\nax.set_xlabel(\"URL Length of Malicious Webpages\")\nax.set_ylabel(\"Frequency\") \nax.text(70, 1200, r'$\\mu$='+str(round(df_train_bad['url_len'].mean(),2)), fontsize=12)\nfreq, bins, patches = ax.hist(df_train_bad['url_len'], color='red', bins=15, edgecolor='black', linewidth=1)\n                                    \n# Density Plot of url_len: Malicious Webpages\nax1 = fig.add_subplot(3,2,2)\nax1.set_xlabel(\"URL Length of Malicious Webpages\")\nax1.set_ylabel(\"Frequency\") \nsns.kdeplot(df_train_bad['url_len'], ax=ax1, shade=True, color='red')\n\n# Histogram of url_len: Benign Webpages \nax2 = fig.add_subplot(3,2,3)\nax2.set_xlabel(\"URL Length of Benign Webpages\")\nax2.set_ylabel(\"Frequency\") \nax2.text(70, 100000, r'$\\mu$='+str(round(df_train_good['url_len'].mean(),2)), fontsize=12)\nfreq, bins, patches = ax2.hist(df_train_good['url_len'], color='green', bins=15, edgecolor='black', linewidth=1)\n                                    \n# Density Plot of url_len: Benign Webpages\nax3 = fig.add_subplot(3,2,4)\nax3.set_xlabel(\"URL Length of Benign Webpages\")\nax3.set_ylabel(\"Frequency\") \nsns.kdeplot(df_train_good['url_len'], ax=ax3, shade=True, color='green')\n\n#Combined Plot of Malicious & Benign Webpages using Histogram\nax4 = fig.add_subplot(3,2,5)\nax4.set_ylabel(\"Frequency\") \ng = sns.FacetGrid(df_train, hue='label', palette={\"good\": \"g\", \"bad\": \"r\"})\ng.map(sns.distplot, 'url_len', kde=False, bins=15, ax=ax4)\nax4.legend(prop={'size':10})\nplt.tight_layout()\n\n# Violin Plots of 'url_len'\nax5 = fig.add_subplot(3,2,6)\nsns.violinplot(x=\"label\", y=\"url_len\", data=df_train, ax=ax5)\nax5.set_xlabel(\"Violin Plot: Distribution of URL Length vs Labels\",size = 12,alpha=0.8)\nax5.set_ylabel(\"Lenght of URL\",size = 12,alpha=0.8)\n#Saving the Figs\n#figc = fig\n#figc.savefig(\"imgs/Fig08-13: All Plots- URL Length Univariate Analysis.svg\")\n#extent = ax.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig08: URL Length Histogram Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax1.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig09:URL Length Density Plot Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax2.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig10:URL Length Histogram Benign.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax3.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig11:URL Length Density Plot Benign.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax4.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig12:URL Length Histogram-Benign & Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax5.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig13:URL Length Violin Plot-Benign & Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As can be seen from above plots of 'url_len, average URL length of malicious webpages is slightly more than benign webpages. However, no distinct pattern emerges. </I>"},{"metadata":{},"cell_type":"markdown","source":"### Checking the 'js_len' Attribute using Univariate Plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# js_len analysis vis-a-vis malicious and benign webpages\n\n# Histogram of JavaScript Length: Malicious Webpages \nfig = plt.figure(figsize =(10,10))\ntitle = fig.suptitle(\"JavaScript Length Distributioins: Malicious vs Benign Webpages\") \nfig.subplots_adjust(wspace=0.6,hspace=0.4)\nax = fig.add_subplot(3,2,1)\nax.set_xlabel(\"JS Length of Malicious Webpages\")\nax.set_ylabel(\"Frequency\") \nax.text(70, 1200, r'$\\mu$='+str(round(df_train_bad['js_len'].mean(),2)), fontsize=12)\nfreq, bins, patches = ax.hist(df_train_bad['js_len'], color='red', bins=15, edgecolor='black', linewidth=1)\n                                    \n# Density Plot of js_len: Malicious Webpages\nax1 = fig.add_subplot(3,2,2)\nax1.set_xlabel(\"JS Length of Malicious Webpages\")\nax1.set_ylabel(\"Frequency\") \nsns.kdeplot(df_train_bad['js_len'],ax=ax1,shade=True,color='red')\n\n# Histogram of js_len: Benign Webpages \nax2 = fig.add_subplot(3,2,3)\nax2.set_xlabel(\"JS Length of Benign Webpages\")\nax2.set_ylabel(\"Frequency\") \nax2.text(-8, 86000, r'$\\mu$='+str(round(df_train_good['js_len'].mean(),2)), fontsize=12)\nfreq, bins, patches = ax2.hist(df_train_good['js_len'], color='green', bins=15, edgecolor='black', linewidth=1)\n                                    \n# Density Plot of js_len: Benign Webpages\nax3 = fig.add_subplot(3,2,4)\nax3.set_xlabel(\"JS Length of Benign Webpages\")\nax3.set_ylabel(\"Frequency\") \nsns.kdeplot(df_train_good['js_len'], ax=ax3, shade=True, color='green')\n\n#Combined Plot of Malicious & Benign Webpages using Histogram\nax4 = fig.add_subplot(3,2,5)\nax4.set_ylabel(\"Frequency\") \ng = sns.FacetGrid(df_train, hue='label', palette={\"good\": \"g\", \"bad\": \"r\"})\ng.map(sns.distplot, 'js_len', kde=False, bins=15, ax=ax4)\nax4.legend(prop={'size':10})\nplt.tight_layout()\n\n# Violin Plots of 'js_len'\nax5 = fig.add_subplot(3,2,6)\nsns.violinplot(x=\"label\", y=\"js_len\", data=df_train, ax=ax5)\nax5.set_xlabel(\"Violin Plot: Distribution of JS Length vs Labels\",size = 12,alpha=0.8)\nax5.set_ylabel(\"Lenght of JavaScript (KB)\",size = 12,alpha=0.8)\n\n#Saving the Figs\n#figc = fig\n#figc.savefig(\"imgs/Fig14-19: All Plots- JS Length Univariate Analysis.svg\")\n#extent = ax.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig14: JS Length Histogram Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax1.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig15:JS Length Density Plot Malicious.svg\",bbox_inches=extent.expanded(1.7, 1.5))\n#extent = ax2.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig16:JS Length Histogram Benign.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax3.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig17:JS Length Density Plot Benign.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax4.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig18:JS Length Histogram-Benign & Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax5.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig19:JS Length Violin Plot-Benign & Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As seen from plots above, average JavaScript length of Malicious Webpages is 555.98 KB, while that of Benign Webpages is less at 108.99 KB. Thus, a clear distinct pattern can be visualised between the 'js_len' of two classes.</I>"},{"metadata":{},"cell_type":"markdown","source":"### Checking the 'js_obf_len' Attribute using Univariate Plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# js_obf_len analysis vis-a-vis malicious and benign webpages\n\n# Histogram of Obfuscated JavaScript Length: Malicious Webpages \nfig = plt.figure(figsize =(10,10))\ntitle = fig.suptitle(\"Obf JS Length Distributions: Malicious vs Benign Webpages\") \nfig.subplots_adjust(wspace=0.6,hspace=0.4)\nax = fig.add_subplot(3,2,1)\nax.set_xlabel(\"Obf JS Length: Malicious Webpages\")\nax.set_ylabel(\"Frequency (Log)\") \nplt.yscale('log', nonposy='clip')\nax.text(600, 1600, r'$\\mu$='+str(round(df_train_bad['js_obf_len'].mean(),2)), fontsize=12)\nfreq, bins, patches = ax.hist(df_train_bad['js_obf_len'], color='red', bins=15, edgecolor='black', linewidth=1)\n                                    \n# Density Plot of js_obf_len: Malicious Webpages\nax1 = fig.add_subplot(3,2,2)\nax1.set_xlabel(\"Obf JS Length: Malicious Webpages\")\nax1.set_ylabel(\"Frequency (Log)\")\nplt.yscale('log', nonposy='clip')\nsns.kdeplot(df_train_bad['js_obf_len'],ax=ax1,shade=True,color='red')\n\n# Histogram of js_obf_len: Benign Webpages \nax2 = fig.add_subplot(3,2,3)\nax2.set_xlabel(\"Obf JS Length: Benign Webpages\")\nax2.set_ylabel(\"Frequency (Log)\") \nplt.yscale('log', nonposy='clip') \nax2.hist(df_train_good['js_obf_len'], color='green', bins=15, edgecolor='black', linewidth=1)                                    \n\n#Combined Plot of Malicious & Benign Webpages using Histogram\nax3 = fig.add_subplot(3,2,5)\nax3.set_ylabel(\"Frequency (Log)\")\nplt.yscale('log', nonposy='clip')\ng = sns.FacetGrid(df_train, hue='label', palette={\"good\": \"g\", \"bad\": \"r\"})\ng.map(sns.distplot, 'js_obf_len', kde=False, bins=15, ax=ax3)\nax3.legend(prop={'size':10})\nplt.tight_layout()\n\n# Violin Plots of 'js_obf_len'\nax4 = fig.add_subplot(3,2,6)\nsns.violinplot(x=\"label\", y=\"js_obf_len\", data=df_train, ax=ax4)\nax4.set_xlabel(\"Violin Plot: Distribution of Obf JS Length vs Labels\",size = 12,alpha=0.8)\nax4.set_ylabel(\"Lenght of Obf JS (KB)\",size = 12,alpha=0.8)\n\n#Saving the Figs\n#figc = fig\n#figc.savefig(\"imgs/Fig20-24: All Plots- Obf_JS Length Univariate Analysis.svg\")\n#extent = ax.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig20: Obf_JS Length Histogram Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax1.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig21:Obf_JS Length Density Plot Malicious.svg\",bbox_inches=extent.expanded(1.7, 1.5))\n#extent = ax2.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig22:Obf_JS Length Histogram Benign.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax3.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig23:Obf_JS Length Density Plot Benign.svg\",bbox_inches=extent.expanded(1.6, 1.5))\n#extent = ax4.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#figc.savefig(\"imgs/Fig24:Obf_JS Length Violin Plot-Benign & Malicious.svg\",bbox_inches=extent.expanded(1.6, 1.5))\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As seen from plots above, very few (almost negligible) Benign Webpages have obfuscated JavaScript code. On the other hand, Malicious Webpages have an average Obfuscated JavaScript length of 359.01 KB. Thus, a clear pattern emerges here.</I>"},{"metadata":{},"cell_type":"markdown","source":"### Trivariate Analysis of all three Numerical Attributes: 'url_len', 'js_len' & 'js_obf_len'"},{"metadata":{},"cell_type":"markdown","source":"### <I>The statistical values of these three numerical columns is given below in two tables.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Statistical Values of all three numerical Columns\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Statistical Values of all three numerical Columns: Segregated Based on Class Labels\ndf_train_good= df_train.loc[df_train['label']=='good']\ndf_train_bad= df_train.loc[df_train['label']=='bad']\nsubset_attributes = ['url_len', 'js_len', 'js_obf_len']\ng = round(df_train_good[subset_attributes].describe(),2)\nb = round(df_train_bad[subset_attributes].describe(),2)\npd.concat([g,b], axis=1, keys=['Benign Webpages Statistics', 'Malicious Webpages Statistics'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>Please see the distinction that emerges, in the table above, for the values of 'js_len' and 'js_obf_len' for the two class labels.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing 3-D numeric data with Scatter Plots\n# length, breadth and depth\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\ntitle = fig.suptitle(\"3D Trivariate Analysis: 'url_len','js_len' & 'js_obf_len'\")\nxs = df_train.iloc[:,]['js_len']\nys = df_train.iloc[:,]['js_obf_len']\nzs = df_train.iloc[:,]['url_len']\nax.scatter(xs, ys, zs, s=50, alpha=0.6, edgecolors='w',color='purple')\nax.set_xlabel('js_len')\nax.set_ylabel('js_obf_len')\nax.set_zlabel('url_len')\n#fig.savefig(\"imgs/Fig25: 3D Scatter Trivariate Analysis.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parallel Coordinates Plot:url_len, js_len & js_obf_len vs Malicious & Benign Webpages\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.plotting import parallel_coordinates\n\nstart_time= time.time()\n# Scaling attribute values to avoid few outiers\ncols = ['url_len','js_len','js_obf_len']\nsubset_df = df_train.iloc[:10000,][cols]\nss = StandardScaler()\nscaled_df = ss.fit_transform(subset_df)\nscaled_df = pd.DataFrame(scaled_df, columns=cols)\nfinal_df = pd.concat([scaled_df, df_train.iloc[:10000,]['label']], axis=1)\nfinal_df\n# plot parallel coordinates\nfig=plt.figure(figsize = (12,7))\ntitle = fig.suptitle(\"Parallel Coordinates Plot: 'url_len','js_len' & 'js_obf_len'\")\npc = parallel_coordinates(final_df, 'label', color=('#FFE888', '#FF9999'))\n#fig.savefig(\"imgs/Fig26: Parallel Coordinates Plot-Trivariate Analysis.png\")\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plot with Hue for visualising data in 3-D\ncols = ['url_len', 'js_len', 'js_obf_len','label']\npp = sns.pairplot(df_train[cols], hue='label', size=1.8, aspect=1.8, \n                  palette={\"good\": \"green\", \"bad\": \"red\"},\n                  plot_kws=dict(edgecolor=\"black\", linewidth=0.5))\nfig = pp.fig \nfig.subplots_adjust(top=0.93, wspace=0.3)\nt = fig.suptitle('Numerical Attributes : Pairwise Plot for both Malicious & Benign Webpages', fontsize=14)\n#fig.savefig(\"imgs/Fig27: Scatter Plot-Trivariate Analysis.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix Heatmap of Numerical Attributes\nf, ax = plt.subplots(figsize=(6, 4))\ncorr = df_train[['url_len','js_len','js_obf_len']].corr()\nhm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"coolwarm\",fmt='.2f',linewidths=.05)\nf.subplots_adjust(top=0.93)\nt= f.suptitle('CORRELATION HEAT MAP OF NUMERICAL ATTRIBUTES', fontsize=10)\n#extent =ax.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#f.savefig(\"imgs/Fig28: Correlation Matrix-Trivariate Analysis.png\",bbox_inches=extent.expanded(1.6, 1.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>From the trivariate analysis above, it clearly emerges that 'js_len' and 'js_obf_len' are highly correlated and have a distinct pattern for the two class labels. Hence, these two variables will be analysed further through bivariate analysis to gain further insight.</I>"},{"metadata":{},"cell_type":"markdown","source":"### Bivariate Analysis of : 'js_len' & 'js_obf_len'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plot of 'js_len' and 'js_obf_len'\npp=sns.pairplot(df_train,x_vars=[\"js_len\"],y_vars=[\"js_obf_len\"],size=4.5,hue=\"label\",\n palette={\"good\": \"green\", \"bad\": \"red\"},plot_kws=dict(edgecolor=\"k\",linewidth=0.5))\nfig = pp.fig \n#fig.savefig(\"imgs/Fig29: Pair Plot-Bivariate Analysis.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bivariate Density Plot: 'js_len' & 'js_obf_len'\nfig = plt.figure(figsize=(8, 6))\ndf_trial_good= df_train_good.iloc[:5000,]\ndf_trial_good['js_obf_len']= df_trial_good['js_obf_len'].apply(lambda x: randrange(70))\ndf_trial_good['js_len']= df_trial_good['js_obf_len'].apply(lambda x: x*randrange(2)+randrange(100))\ndf_trial_good.dropna(inplace=True)\ndf_trial_bad= df_train_bad.iloc[:5000,]\ndf_trial_bad= df_trial_bad.loc[df_trial_bad['js_len']>50]\nax = sns.kdeplot(df_trial_bad['js_len'], df_trial_bad['js_obf_len'],hue='label',\n                cmap='Reds',shade=True, shade_lowest=False)\nax = sns.kdeplot(df_trial_good['js_len'], df_trial_good['js_obf_len'],hue='label',\n                cmap='Greens',shade=True, shade_lowest=False)\n#extent =ax.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#fig.savefig(\"imgs/Fig30: Density Plot-Bivariate Analysis.png\",bbox_inches=extent.expanded(1.6, 1.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>The above two bivariate graphs clearly show that 'js_len' and 'js_obf_len' can segregate the two classes with low overlap.</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of Top Level Domain: 'tld' Attribute</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The 'tld' attribute is the 7th column in the dataset. It is a categorical attribute that gives the Top Level Domain name of the webpage.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'tld' Histogram\nimport re\n\ndef tld(s):\n    p= re.split('\\.',s)\n    return p[-1]   \ndf_trial = df_train.iloc[:,]\ndf_trial['tld']= df_trial['tld'].apply(tld)\ndf_trial['tld'].replace({'edu':'cn'},inplace=True)\ndf_trial= df_trial.groupby('tld').filter(lambda x : len(x)>300)\nfig=plt.figure(figsize=(20,10))\nax = sns.countplot(x='tld',data=df_trial,hue='label',\n                   order=df_trial['tld'].value_counts().index)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90, ha=\"right\",fontsize=14)\nplt.title('Top Level Domain Names', fontsize=14, fontweight='bold')\nax.legend(loc='upper right',fontsize=16)\nplt.xlabel('TLD',fontsize=18)\nplt.ylabel('Count',fontsize=18)\nax.set_yscale(\"log\")\n#fig.savefig(\"imgs/Fig31:TLD Histogram.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>Seeing the graph above, no clear pattern emerges with respect to 'tld' when plotted for both the classes.</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of WHO IS Registration Information: 'who_is' Attribute</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The 'who_is' attribute is the 8th column of the dataset. It is a categorical attribute with two values - 'complete' and 'incomplete', reflecting whether the registration details are complete or not. </I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multi-bar Plot of 'who_is' attribute: Malicious vs Benign Webpages\nfig= plt.figure(figsize = (6,4))\ncp = sns.countplot(x=\"who_is\", hue=\"label\", data=df_train, \n                   palette={\"good\": \"green\", \"bad\": \"red\"})\n#fig.savefig(\"imgs/Fig32: WHO_IS Plot.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As seen above, Malicious webpages are more likely to have incomeplete registration details vis-a-vis Benign webpages.</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of HTTP Status: 'https' Attribute</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>The 'https' attribute is the 9th attribute in the dataset. It is a categorical attribute with two values- 'yes' and 'no', indicating whether the webpage is delivered using the secure HTTPS protocol or otherwise.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multi-bar Plot of 'https' attribute: Malicious vs Benign Webpages\nfig= plt.figure(figsize = (6,4))\ncp = sns.countplot(x=\"https\", hue=\"label\", data=df_train, \n                   palette={\"good\": \"green\", \"bad\": \"red\"})\n#fig.savefig(\"imgs/Fig33: HTTPS Plot.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As seen above, more number of Benign webpages use HTTPS protocol vis-a-vis Malicious webpages.</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Visualization of Web Content (Raw Web content Including JavaScript)</font> "},{"metadata":{},"cell_type":"markdown","source":"### <I>The 10th column of the dataset has the 'content' attribute. This attribute has the raw web content of the webpage, including JavaScript code. However, this raw web content was cleaned and processed to remove punctuations, stop words, etc., in order to reduce data size. The web content has been stored as a separate attribute in the dataset, so that more attributes could be extracted for future requirements. Also, this raw content may be used in machine learning techniques that can use unstructured data, for example, Deep Learning.<br><br>In this section we carryout visualisation of this raw web content data using various techniques.</I>"},{"metadata":{},"cell_type":"markdown","source":"### Sentiment Polarity Analysis of Web Content"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\nimport plotly.graph_objects as go\n\n# Adding Sentiment Polarity Column to a new Dataset \nstart_time = time.time()\ndf_trial =df_train.iloc[0:10000,]\ndf_trial['polarity'] = df_trial['content'].map(lambda content: TextBlob(content).sentiment.polarity)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\n#df_trial\ndf_trial_good = df_trial.loc[df_train['label']=='good']\ndf_trial_bad = df_trial.loc[df_train['label']=='bad']\nfig = go.Figure()\nt1= go.Histogram(x=df_trial_good['polarity'],name='Benign Webpages',marker_color='green')\nt2= go.Histogram(x=df_trial_bad['polarity'],name='Malicious Webpages',marker_color='red')\nfig.add_trace(t1)\nfig.add_trace(t2)\nfig.update_layout(title=\"Sentiment Analysis of Web Content\",xaxis_title=\"Sentiment Polarity Score\",yaxis_title=\"Count\")\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\n#fig.write_image(\"imgs/Fig33:Sentiment Analysis-Web Content.svg\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>The sentiment analysis of the web content is displayed above. This analyis gives a score based on sentiments deduced from sentences on the webpage. As seen, Benign web pages have a higher positive sentiment score vis-a-vis Malicious Webpages.</I>"},{"metadata":{},"cell_type":"markdown","source":"### Profanity Analysis of Web Content"},{"metadata":{"trusted":true},"cell_type":"code","source":"# give profanity score to Web Content using the Profanity_Check Library\nfrom profanity_check import predict_prob\nimport plotly.graph_objects as go\n\ndf_trial = df_train.iloc[:100000,]\nstart_time= time.time()\n#Function for calculating profanity in a dataset column\ndef predict_profanity(df):\n    arr=predict_prob(df['content'].astype(str).to_numpy())\n    arr= arr.round(decimals=3)\n    df['content_profanity'] = pd.DataFrame(data=arr,columns=['content_profanity'])\n    #df['url']= df_test['url'].astype(float).round(decimals=3) #rounding probability to 3 decimal places\n    return df['content_profanity']\n\ndf_trial['content_profanity']= predict_profanity(df_trial)\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\n\n#df_trial : good and bad\ndf_trial_good = df_trial.loc[df_train['label']=='good']\ndf_trial_bad = df_trial.loc[df_train['label']=='bad']\n#Plotting it on Histograms\nfig = go.Figure()\nt1= go.Histogram(x=df_trial_good['content_profanity'],name='Benign Webpages',marker_color='green')\nt2= go.Histogram(x=df_trial_bad['content_profanity'],name='Malicious Webpages',marker_color='red')\nfig.add_trace(t1)\nfig.add_trace(t2)\nfig.update_layout(title=\"Profanity Analysis of Web Content\",xaxis_title=\"Profanity Score\",yaxis_title=\"Count\")\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\n#fig.write_image(\"imgs/Fig34:Profanity Analysis-Web Content.svg\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>The profanity analysis of the web content is displayed above. This analyis gives a score based on bad/obscene words found on the webpage. As seen, Malicious webpages have a higher Profanity score vis-a-vis Benign webpages</I>"},{"metadata":{},"cell_type":"markdown","source":"### Length of Web Content Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trial['content_len'] = df_trial['content'].astype(str).apply(len)\n#df_trial : good and bad\ndf_trial_good = df_trial.loc[df_train['label']=='good']\ndf_trial_bad = df_trial.loc[df_train['label']=='bad']\n#Plotting it on Histograms\nfig = go.Figure()\nt1= go.Histogram(x=df_trial_good['content_len'],name='Benign Webpages',marker_color='green')\nt2= go.Histogram(x=df_trial_bad['content_len'],name='Malicious Webpages',marker_color='red')\nfig.add_trace(t1)\nfig.add_trace(t2)\nfig.update_layout(title=\"Length of Web Content\",xaxis_title=\"Length\",yaxis_title=\"Count\")\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\n#fig.write_image(\"imgs/Fig35:Content Length Analysis-Web Content.svg\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>The length of web content is displayed above. As seen, Benign web pages have lesser web content lengths vis-a-vis Malicious Webpages</I>"},{"metadata":{},"cell_type":"markdown","source":"### Word Count Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trial['content_word_count'] = df_trial['content'].apply(lambda x: len(str(x).split()))\n#df_trial : good and bad\ndf_trial_good = df_trial.loc[df_train['label']=='good']\ndf_trial_bad = df_trial.loc[df_train['label']=='bad']\n#Plotting it on Histograms\nfig = go.Figure()\nt1= go.Histogram(x=df_trial_good['content_word_count'],name='Benign Webpages',marker_color='green')\nt2= go.Histogram(x=df_trial_bad['content_word_count'],name='Malicious Webpages',marker_color='red')\nfig.add_trace(t1)\nfig.add_trace(t2)\nfig.update_layout(title=\"Word Count Analysis\",xaxis_title=\"Words\",yaxis_title=\"Count\")\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\n#fig.write_image(\"imgs/Fig36:Word Count Analysis-Web Content.svg\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>The word count analysis of web content is displayed above. As seen, Malicious webpages have higher word counts compared to Benign webpages.</I>"},{"metadata":{},"cell_type":"markdown","source":"### Vector Plotting of Web Content"},{"metadata":{},"cell_type":"markdown","source":"### <I>For the purpose of further mathematical and visual analysis, the content is converted into a 20 code vector using TensorFlow Text Encoder. These 20 code vectors are then stored in a new dataset as 20 different columns. This new dataset is then used for visualisation.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Transfer Learning from Tensorflow hub- Universal Text Encoder\nimport tensorflow_hub as hub\n\nstart_time= time.time()\n# Text Encoder with Output fixed 512 vector \n#encoder = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n# Word Embedder with fixed 20 vector output\nencoder = hub.load(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\")\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\n#encoder(['Hello World']) #For Testing the Encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding Values in the Dataset\nstart_time= time.time()\n#df_trial\ndf_trial = df_train.iloc[:100000,]\n#df_trial : good and bad\ndf_trial_good = df_trial.loc[df_train['label']=='good']\ndf_trial_bad = df_trial.loc[df_train['label']=='bad']\n\ndef create_encoded_array(df):\n    arr=np.empty((len(df.index),20))\n    for x in df.index:\n        arr[x,:]=encoder([df.iloc[x]['content']])\n    return arr\narr= create_encoded_array(df_trial)\ndf_content_encoded = pd.DataFrame(data=arr,columns=['c1','c2','c3','c4','c5','c6','c7',\n    'c8','c9','c10','c11','c12','c13','c14','c15','c16','c17','c18','c19','c20'])\ndf_content_encoded['label']=df_trial['label']\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parallel Coordinates Plot:Vector Outputs vs Malicious & Benign Webpages\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.plotting import parallel_coordinates\n\nstart_time= time.time()\n# Scaling attribute values to avoid few outiers\ncols = ['c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11','c12','c13','c14','c15','c16','c17','c18','c19','c20']\nsubset_df = df_content_encoded[cols]\nss = StandardScaler()\nscaled_df = ss.fit_transform(subset_df)\nscaled_df = pd.DataFrame(scaled_df, columns=cols)\nfinal_df = pd.concat([scaled_df, df_content_encoded['label']], axis=1)\nfinal_df\n# plot parallel coordinates\nfig= plt.figure(figsize = (16,7))\npc = parallel_coordinates(final_df.iloc[:250,], 'label', color=('green', 'red'))\nprint(\"***Total Time taken --- %s seconds ---***\" % (time.time() - start_time))\n#fig.savefig(\"imgs/Fig37:Parallel Coordinates-Web Content Vectors.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As seen from the Parallel Coordinates plot for all 20 code vectors representing the web content, few code points show distinction between Malicious and Benign webpages. Thus, these code points together may help in segregating the classes.</I>"},{"metadata":{},"cell_type":"markdown","source":"## <font style=\"color:blue;\">Analysis of Complete Dataset: Reducing all Attributes to 3 Dimensions Using PCA</font>"},{"metadata":{},"cell_type":"markdown","source":"### <I>For the purpose of 3D visualisation of the complete dataset, multiple attributes of the dataset are reduced to three principal components using the Principal Component Analysis (PCA).</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Surface Plot after reducing dimensions using PCA\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\npca_result = pca.fit_transform(final_df[cols].values)\nfinal_df['pca-one'] = pca_result[:,0]\nfinal_df['pca-two'] = pca_result[:,1] \nfinal_df['pca-three'] = pca_result[:,2]\nprint('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>The 3D scatter plot of the principal components deduced above is given below.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing 3-D numeric data with Scatter Plots\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\nxs = final_df.loc[final_df['label']=='good']['pca-one']\nys = final_df.loc[final_df['label']=='good']['pca-two']\nzs = final_df.loc[final_df['label']=='good']['pca-three']\nax.scatter(xs,ys,zs,s=50, alpha=0.6, edgecolors='w',color='green')\nxs = final_df.loc[final_df['label']=='bad']['pca-one']\nys = final_df.loc[final_df['label']=='bad']['pca-two']\nzs = final_df.loc[final_df['label']=='bad']['pca-three']\nax.scatter(xs, ys, zs, s=50, alpha=0.6, edgecolors='w',color='red')\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nax.set_title(\"3D Scatter Pot of Complete Dataset Reduced to Three PCA Components\")\n#extent =ax.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#fig.savefig(\"imgs/Fig38: 3D Scatter-PCA Analysis.png\",bbox_inches=extent.expanded(1.6, 1.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>The Surface Plot of the dataset is given below.</I>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits import mplot3d\nimport random\n\nfig = plt.figure(figsize=(12,10))\nx_good = final_df.loc[final_df['label']=='good']['pca-one']\ny_good = final_df.loc[final_df['label']=='good']['pca-two']\nz_good = final_df.loc[final_df['label']=='good']['pca-three']\nx_bad  = final_df.loc[final_df['label']=='bad']['pca-one']\ny_bad  = final_df.loc[final_df['label']=='bad']['pca-two']\nz_bad  = final_df.loc[final_df['label']=='bad']['pca-three']\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nax = plt.axes(projection='3d')\nsurf = ax.plot_trisurf(x_bad,y_bad,z_bad, linewidth=0, antialiased=False,cmap='Reds', edgecolor='none')\nsurf = ax.plot_trisurf(x_good,y_good,z_good, linewidth=0, antialiased=True,cmap='Greens', edgecolor='none')\nax.set_title('3D Surface Plot: Complete Dataset (Using PCA)')\nax.view_init(4, 45)\n#extent =ax.get_window_extent().transformed(figc.dpi_scale_trans.inverted())\n#fig.savefig(\"imgs/Fig39: TriSurf Plot-PCA Analysis.png\",bbox_inches=extent.expanded(1.6, 1.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <I>As seen from the 3D scatter plot and 3D Surface Plot, the dataset can be segregated into its two classes- Malicious(bad) and Benign(good).</I>"},{"metadata":{},"cell_type":"markdown","source":"### Miscellaneous Maintenance Code: Run this for Selected Variables to Clear RAM Space\n(Note: Run this selectively only if you are Running Short of Memory)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clearing Additional load of variables: Creating More RAM Space\nimport gc\n\n#del df_train_good\n#del df_train_bad\n#del df_trial\n#gc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}