{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install evalml","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evalml\nimport numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/credit-card-customers/BankChurners.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The first thing we'll do is drop CLIENTNUM from the data since a unique client identifier will have no correlation with attrition rates. Now there's clearly some diversity in the types of features, and at first glace it looks like we don't have to worry about any null or missing values. But that seems unlikely with a dataset of this size.**","metadata":{}},{"cell_type":"code","source":"data = data.drop(['CLIENTNUM'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in data.columns:\n    if data[feature].dtype not in ['int64', 'float64']:\n        print(f'{feature}: {data[feature].unique()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Education_Level, Marital_Status, and Income_Category have Unknown as a value. This is something we'll have to remember before we get to the model training, since Unknown isn't an acceptable value for any of the features.**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(nrows=3, ncols=1, figsize=(16, 28))\nsns.set(font_scale=1.6)\ncols_ = [\"Education_Level\", \"Marital_Status\", \"Income_Category\"]\n\nfor ind, col in enumerate(cols_):\n    sns.countplot(x=col, data=data, ax=ax[ind])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking to see how prevalent Unknown is proportionally to the the other values. Based on the count plots above, it doesn't look like Unknown is the most common value, but it's frequency is high enough that we probably don't want to drop rows containing it altogether.**","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We're also going to take a look at the correlation matrix to see if there are any features that are too closely tied to others. It looks like Avg_Open_To_Buy is perfectly correlated with Credit_Limit, so we're going to drop the latter.**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 16))\ndf_corr = data.corr(method=\"pearson\")\nmask = np.zeros_like(np.array(df_corr))\nmask[np.triu_indices_from(mask)] = True\nax = sns.heatmap(df_corr, mask=mask, annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Attrition_Flag'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The target feature is imbalanced so we will consider F1-score as our metric.**","metadata":{}},{"cell_type":"markdown","source":"**Encoding**","metadata":{}},{"cell_type":"code","source":"X = data.copy()\nX = X.drop(['Credit_Limit'], axis=1) # dropping Credit Limit since it is highly correlated with Avg_Open_To_Buy\ny = X.pop('Attrition_Flag')\n\nX['Income_Category'] = X['Income_Category'].replace({'Less than $40K':0,\n                                                     '$40K - $60K':1,\n                                                     '$60K - $80K':2,\n                                                     '$80K - $120K':3,\n                                                     '$120K +':4})\nX['Card_Category'] = X['Card_Category'].replace({'Blue':0,\n                                                 'Silver':1,\n                                                 'Gold':2,\n                                                 'Platinum':3})\nX['Education_Level'] = X['Education_Level'].replace({'Uneducated':0,\n                                                     'High School':1,\n                                                     'College':2,\n                                                     'Graduate':3,\n                                                     'Post-Graduate':4,\n                                                     'Doctorate':5})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encoding the Target feature**","metadata":{}},{"cell_type":"code","source":"y = y.replace({'Existing Customer':0,\n               'Attrited Customer':1})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Replacing the Unknown values that we saw earlier with the most frequent value encountered in that feature using SimpleImputer.**","metadata":{}},{"cell_type":"code","source":"from evalml.pipelines.components.transformers.imputers.simple_imputer import SimpleImputer\n\ndef preprocessing(X, y):\n    imputer = SimpleImputer(impute_strategy=\"most_frequent\", missing_values=\"Unknown\")\n    X = imputer.fit_transform(X, y)\n    return X\n\nX = preprocessing(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from evalml.utils import infer_feature_types","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = infer_feature_types(X, feature_types={'Income_Category': 'categorical',\n                                          'Education_Level': 'categorical'})\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting the dataset into 80% train and 20% test.**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y, problem_type='binary',test_size=.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initializing AutoMLSearch from EvalML**","metadata":{}},{"cell_type":"code","source":"from evalml import AutoMLSearch\n\nautoml = AutoMLSearch(X_train=X_train, y_train=y_train, problem_type=\"binary\", objective=\"F1\", \n                      allowed_model_families=['random_forest' , 'xgboost', 'lightgbm'],\n                      additional_objectives=['accuracy binary'], max_batches=5)\nautoml.search()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipelines Review**\n\nSo a lot just happened, let's review the pipelines that were created and tested. We can see that the best performing pipeline was with the LightGBM estimator. We want to learn a little more about it, which can be done with the describe_pipeline function. Notice that the pipeline included a preprocessing step of imputation. In this case, it ended up being unnecessary because of our earlier SimpleImputer and our lack of null values for our numerical features. However AutoMLSearch comes with the built-in capacity to automatically iterate over the hyperparameters for this preprocessing step as well.","metadata":{}},{"cell_type":"code","source":"automl.rankings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Obtaining the complete pipeline of the best model**","metadata":{}},{"cell_type":"code","source":"best_pipeline_ = automl.best_pipeline\nautoml.describe_pipeline(automl.rankings.iloc[1][\"id\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We got the best classifier with LightGBM Classifier.**","metadata":{}},{"cell_type":"code","source":"best_pipeline_.fit(X_train, y_train)\npredictions = best_pipeline_.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from evalml.model_understanding.graphs import (\n    graph_binary_objective_vs_threshold, \n    graph_permutation_importance, \n    graph_confusion_matrix\n)\n\ngraph_binary_objective_vs_threshold(best_pipeline_, X_test, y_test, \"F1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graph_permutation_importance(best_pipeline_, X_test, y_test, \"F1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Total Trans Ct is giving us the highest permutation importance score followed by Total Trans Amt.**","metadata":{}},{"cell_type":"code","source":"graph_confusion_matrix(y_test, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We are getting (1685+273) = 1958 correct observations and (52+16) = 68 incorrect observations.**","metadata":{}},{"cell_type":"code","source":"from evalml.objectives.standard_metrics import AccuracyBinary, AUC, F1, PrecisionWeighted, Recall\n\nacc = AccuracyBinary()\nauc = AUC()\nf1 = F1()\npre_w = PrecisionWeighted()\nrec = Recall()\n\nprint(f\"Accuracy (Binary): {acc.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"Area Under Curve: {auc.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"F1: {f1.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"Precision (Weighted): {pre_w.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"Recall: {rec.score(y_true=y_test, y_predicted=predictions)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We are getting an F1-score of 0.88 on the test set which is pretty good.**","metadata":{}}]}