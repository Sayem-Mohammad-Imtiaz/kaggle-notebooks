{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/Amazon_Unlocked_Mobile.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df.head()\n# Review votes in the table indicates number of people found the review helpful","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Brand Name'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True) # drop any rows with missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assuming rating with 3 are neutral reviews\n# so drop rows with rating = 3 (by chosing all the rows with rating!=3)\n\ndf = df[df['Rating']!=3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assuming rating with greater than 3 are rated as postive\n# so we assign 1 to Positively rated and 0 to those are not\n# if Rating > 3, then 'Positively Rated' = 1, else 'Positively Rated' = 0\n\ndf['Positively Rated'] = np.where(df['Rating']>3, 1, 0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Positively Rated'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],\n                                                    df['Positively Rated'],\n                                                    random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train[0], X_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll need to convert text into a numeric so that scikit-learn can use\n# The bag-of-words approach ignores structure and only counts how often each word occurs\n# CountVectorizer use the bag-of-words by converting text into a matrix of token counts.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we instantiate the CountVectorizer and fit it to our training data.\n\n# Fitting the CountVectorizer consists of the \n#     tokenization of the trained data and \n#     building of the vocabulary\n\n# Fitting the CountVectorizer \n#     tokenizes each document by finding \n#         all sequences of characters of \n#             at least two letters or \n#             numbers separated by word boundaries. \n# Converts everything to \n#     lowercase and \n#     builds a vocabulary using these tokens.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer().fit(X_train)\nvect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect.get_feature_names()[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect.get_feature_names()[::4000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use transform method to transform X_train to a document term matrix\n# giving us the bag-of-word representation of X_train\n\n# This representation is stored in a SciPy sparse matrix where \n#     each row corresponds to a document and \n#     each column a word from our training vocabulary.\n\n# The entries in this matrix are the number of times each word appears in each document.\n\n# Because the number of words in the vocabulary is so much larger \n# than the number of words that might appear in a single review, \n# most entries of this matrix are zero.\n\n# and the shape will be \n#     number of document/rows(here in dataframe)/reviews(in this case) *\n#     number of words in the vocabulary/tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here's a trivial example ... Let's suppose we have 3 documents:\n\n#     Doc1: Hello, World, the sun is shining\n#     Doc2: Hello world, the weather is nice\n#     Doc3: Hello world, the wind is cold\n\n\n# Then, our vocabulary would look like this (using 1-grams without stop word removal):\n\n#     Vocabulary: [hello, world, the, wind, weather, sun, is, shining, nice, cold]\n\n\n# The corresponding, binary feature vectors are:\n\n#     Doc1: [1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n#     Doc2: [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n#     Doc3: [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n\n\n# Which we use to construct the dense matrix / document term matrix:\n\n#     [[1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n#      [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n#      [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized = vect.transform(X_train)\nX_train_vectorized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(vect.transform(X_test))\nprint('roc accuracy score ', roc_auc_score(y_test, pred)\n\n# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(vect.transform(['not an issue, phone is working', \n                                    'an issue, phone is not working'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tf–idf, or Term frequency-inverse document frequency\n# allows us to weight terms based on how important they are to a document.\n# high weight is given to terms that appear often in a particular document, \n# but don't appear often in the corpus. \n\n# Features with low tf–idf are either commonly used across all documents \n# or rarely used and only occur in long documents.\n\n# Features with high tf–idf are frequently used within specific documents, \n# but rarely used across all documents.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similar to how we used CountVectorizer, \n# we'll instantiate the tf–idf vectorizer and fit it to our training data.\n\n# mindf, which allows us to specify a minimum number of documents \n# in which a token needs to appear to become part of the vocabulary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npred = model.predict(vect.transform(X_test))\n\nroc_auc_score(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(vect.transform(['not an issue, phone is working', \n                                    'an issue, phone is not working'])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# n-gram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# One way we can add some context is by adding sequences of word features known as n-grams. \n\n# For example, bigrams, which count pairs of adjacent words, \n# could give us features such as is working versus not working. \n# And trigrams, which give us triplets of adjacent words, \n# could give us features such as not an issue.\n\n# To create these n-gram features, \n# we'll pass in a tuple to the parameter ngram_range, \n# where the values correspond to the minimum length and maximum lengths of sequences.\n\n# For example, if I pass in the tuple, 1, 2, \n# CountVectorizer will create features using the individual words, \n# as well as the bigrams.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npred = model.predict(vect.transform(X_test))\n\nroc_auc_score(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(vect.transform(['not an issue, phone is working', \n                                    'an issue, phone is not working'])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}