{"cells":[{"metadata":{"id":"LeamvpPVXuS_"},"cell_type":"markdown","source":"# 1.7 Random Forest Regression","execution_count":null},{"metadata":{"id":"kNstyi26X_Qm"},"cell_type":"markdown","source":"For a better understanding of Random Forest if u don't have any idea whatsoever, visit the links:\n[Random Forest Intuition](https://www.youtube.com/watch?v=LIPtRVDmj1M)","execution_count":null},{"metadata":{"id":"dEJp1dtdZItz"},"cell_type":"markdown","source":"we can say that random forset is a whole large group of decision trees built from the subset of the dataset and their target is predicted by taking the majority of decision tree target in case of clasiificaion and average of decision tree target incase of regression.","execution_count":null},{"metadata":{"id":"_8EW07o3b5Ne"},"cell_type":"markdown","source":"For better understanding of current notebook for beginners go through the links:\n\n [1.1 Data Preprocessing](http://www.kaggle.com/saikrishna20/data-preprocessing-tools)\n\n\n[1.2 Simple linear Regression](https://www.kaggle.com/saikrishna20/1-2-simple-linear-regression) \n\n\n[1.3 Multiple linear Regression with Backward Elimination](http://www.kaggle.com/saikrishna20/1-3-multiple-linear-regression-backward-eliminat)\n\n[1.4 Polynomial Linear Regression](https://www.kaggle.com/saikrishna20/1-4-polynomial-linear-regression)\n\n[1.5 Support Vector Regression (SVR)](https://www.kaggle.com/saikrishna20/1-5-support-vector-regression-svr/edit/run/37240657)\n\n[1.6 Decision Tree Regressor](https://www.kaggle.com/saikrishna20/1-6-decision-tree-regression)\nDefinetely go through the decision tree link\n\nIt basically tells u about the preprocessing & Linear Regression which will help u in understanding this notebook better","execution_count":null},{"metadata":{"id":"O2wvZ7SKXzVC"},"cell_type":"markdown","source":"## Importing the libraries","execution_count":null},{"metadata":{"id":"PVmESEFZX4Ig","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"id":"zgbK_F8-X7em"},"cell_type":"markdown","source":"## Importing the dataset","execution_count":null},{"metadata":{"id":"adBE4tjQX_Bh","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/position-salaries/Position_Salaries.csv')\nX = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"id":"v4S2fyIBYDcu"},"cell_type":"markdown","source":"## Training the Random Forest Regression model on the whole dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"n_estimators (int) default=100\nThe number of trees(Decision Trees) in the forest.","execution_count":null},{"metadata":{"id":"o8dOCoJ1YKMc","outputId":"fe6443bb-8ad3-4740-8b8c-117056165fc8","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"id":"8IbsXbK3YM4M"},"cell_type":"markdown","source":"## Predicting a new result","execution_count":null},{"metadata":{"id":"pTXrS8FEYQlJ","outputId":"9b431424-6bba-423b-fe2c-978612d49919","trusted":true},"cell_type":"code","source":"regressor.predict([[6.5]])","execution_count":null,"outputs":[]},{"metadata":{"id":"kLqF9yMbYTon"},"cell_type":"markdown","source":"## Visualising the Random Forest Regression results (higher resolution)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we will predict each value i.e. 1.01 , 1.02, 1.03,1.04 and so.... on untill 10.00 and plot these outcomes for a wider understanding.","execution_count":null},{"metadata":{"id":"BMlTBifVYWNr","outputId":"b7492ae8-ec96-4e5a-d08f-d54f31e29ad2","trusted":true},"cell_type":"code","source":"X_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X, y, color = 'red')\nplt.plot(X_grid, regressor.predict(X_grid), color = 'blue')\nplt.title('Truth or Bluff (Random Forest Regression)')\nplt.xlabel('Position level')\nplt.ylabel('Salary')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the Position level is increasing there are more steps in the model.\n\n1.5 to 2.5 salary is same\n\n2.5 to 3.0 \n\n3.0 to 3.5\n\n3.5 to 4.0\n\n4.0 to 4.5\n\n4.5 to 5.0 \n\n5.0 to 5.5 \n\nu can clearly see the change in 7 to 7.5 and 7.5 to 8.0 there is a hike in salary.\n\nIn the decision tree the range is constant and is higher.\n\nIn the random forest the range is constant and is lower for a certain values whose salary is same, that's what the model is pedicting by using a 10 decision trees. By combining multiple trees we get forest.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Like this notebook then upvote it.\n\n\n# Need to improve it then comment below.\n\n\n# Enjoy Machine Learning","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}