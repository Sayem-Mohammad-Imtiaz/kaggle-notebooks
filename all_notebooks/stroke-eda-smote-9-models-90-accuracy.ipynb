{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Stroke Prediction"},{"metadata":{},"cell_type":"markdown","source":"Table of Contents:\n1. [Importing Libraries](#1) <a href= \"1\"></a>\n2. [Importing Dataset](#2) <a href= \"2\"></a>\n3. [Data Visualization](#3) <a href= \"3\"></a> <br> \n    3.1. [Heat Map Correlation](#3.1) <a href= \"3.1\"></a> <br>\n    3.2. [Count Plot](#3.2) <a href= \"3.2\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Gender](#3.2.1) <a href= \"3.2.1\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Hypertension](#3.2.2) <a href= \"3.2.2\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [Marriage Status](#3.2.3) <a href= \"3.2.3\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; d. [Work Type](#3.2.4) <a href= \"3.2.4\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; e. [Residence Type](#3.2.5) <a href= \"3.2.5\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; f. [Smoking Status](#3.2.6) <a href= \"3.2,6\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; g. [Stroke](#3.2.7) <a href= \"3.2.7\"></a> <br>\n    3.3 [Distribution Plot](#3.3) <a href= \"3.3\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Avg. Glucose Level](#3.3.1) <a href= \"3.3.1\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [BMI](#3.3.2) <a href= \"3.3.2\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; c. [No Stroke vs Stroke by BMI](#3.3.3) <a href= \"3.3.3\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; d. [No Stroke vs Stroke by Avg. Glucose Level](#3.3.4) <a href= \"3.3.4\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; e. [No Stroke vs Stroke by Age](#3.3.5) <a href= \"3.3.5\"></a> <br>\n    3.4 [Scatter Plot](#3.4) <a href= \"3.4\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Age vs BMI](#3.4.1) <a href= \"3.4.1\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Age vs Avg. Glucose Level](#3.4.2) <a href= \"3.4.2\"></a> <br>\n    3.5 [Cat Plot](#3.5) <a href= \"3.5\"></a> <br>\n    3.6 [Pair Plot](#3.6) <a href= \"3.6\"></a> <br>\n4. [Data Preprocessing](#4) <a href= \"4\"></a> <br>\n5. [Encoding](#5) <a href= \"5\"></a> <br>\n    5.1 [Categorical Encoding](#5.1) <a href= \"5.1\"></a> <br>\n    5.2 [Label Encoding](#5.2) <a href= \"5.2\"></a> <br>\n6. [Splitting the dataset into the Training set and Test set](#6) <a href= \"6\"></a> <br> \n7. [Feature Scaling](#7) <a href= \"7\"></a> <br>\n8. [Handling Imbalance data using SMOTE](#8) <a href= \"8\"></a> <br>\n9. [Model Selection](#9) <a href= \"9\"></a> <br>\n10. [Tuning the Models](#10) <a href= \"10\"></a> <br>\n11. [Models after Tuning Hyperparameters](#11) <a href= \"11\"></a> <br>\n    11.1 [RandomForest](#11.1) <a href= \"11.1\"></a> <br>\n    11.2 [XGBoost](#11.2) <a href= \"11.2\"></a> <br>\n12. [Keras ANN](#12) <a href= \"12\"></a> <br>\n    12.1 [Building the ANN](#12.1) <a href= \"12.1\"></a> <br>\n    12.2 [Evaluating the ANN (Cross Validation)](#12.2) <a href= \"12.2\"></a> <br>\n    12.3 [Tuning the ANN](#12.3) <a href= \"12.3\"></a> <br>\n    12.4 [ANN Model after Tuning](#12.4) <a href= \"12.4\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; a. [Loss Graph](#12.4.1) <a href= \"12.4.1\"></a> <br>\n    &nbsp;&nbsp;&nbsp;&nbsp; b. [Accuracy Graph](#12.4.2) <a href= \"12.4.2\"></a> <br>\n13. [Conclusion](#13) <a href= \"13\"></a> <br>"},{"metadata":{},"cell_type":"markdown","source":"### Let's Start!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing Libraries** <a id=\"1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Importing Dataset** <a id=\"2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are null values present in 'bmi'.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.bmi.replace(to_replace=np.nan, value=dataset.bmi.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We replaced null values of 'bmi' with mean in that column.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After checking, as you can see there are no null values present in our column.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Visualization** <a id=\"3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## **Heat Map Correlation** <a id=\"3.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Count Plot** <a id=\"3.2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### **Gender** <a id=\"3.2.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.gender.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"gender\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Above, you can see the Females present in our dataset is higher than males.*"},{"metadata":{},"cell_type":"markdown","source":"### **Hypertension** <a id=\"3.2.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.hypertension.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"hypertension\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From above, it shows that less people are suffering from hypertension.*"},{"metadata":{},"cell_type":"markdown","source":"### **Marriage Status** <a id=\"3.2.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.ever_married.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"ever_married\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The ratio can seen from above is around 2:1 for being ever married.*"},{"metadata":{},"cell_type":"markdown","source":"### **Work Type** <a id=\"3.2.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.work_type.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"work_type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*A lot of people works in Private sector.*"},{"metadata":{},"cell_type":"markdown","source":"### **Residence Type** <a id=\"3.2.5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.Residence_type.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"Residence_type\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The residence type is same for people present in our dataset.*"},{"metadata":{},"cell_type":"markdown","source":"### **Smoking Status** <a id=\"3.2.6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.smoking_status.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"smoking_status\")\nax.set_xticklabels(ax.get_xticklabels(), fontsize=10)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*A lot of people never smoked in their life. But, we also don't know the exact status of Unknowns in our dataset.*"},{"metadata":{},"cell_type":"markdown","source":"### **Stroke** <a id=\"3.2.7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.stroke.value_counts())\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(data=dataset, x=\"stroke\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From above dependent variable, we have really less peoples who suffered stroke. But, this also means that our dataset is imbalance. We likely have to use sampling techniques to make the data balance.*"},{"metadata":{},"cell_type":"markdown","source":"*But, first let's plot more to see how our data does in this state.*"},{"metadata":{},"cell_type":"markdown","source":"## **Distribution Plot** <a id=\"3.3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### **Avg. Glucose Level** <a id=\"3.3.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\nsns.distplot(dataset.avg_glucose_level, color=\"green\", label=\"avg_glucose_level\", kde= True)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*1. The normal glucose levels in adults should be around 80-140. Therefore, the density is higher around that range. So, we can see that the we have lot of people who have normal glucose level, so they are not suffering from diabetes.*\n\n*2. The range 140-200 can considered as pre-diabetes. But, looking at graph we can see that less people are in pre-diabetes zone.*\n\n*3. Anything above 200 can be seen that the person is suffering from diabetes. The density is more as compare to pre-diabetes by looking at the graph.*"},{"metadata":{},"cell_type":"markdown","source":"### **BMI** <a id=\"3.3.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\nsns.distplot(dataset.bmi, color=\"orange\", label=\"bmi\", kde= True)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*1. BMI below 19 can be seen as under weight. By looking at our graph, not lot of people are underweight.*\n\n*2. BMI between 19-25 can be seen as normal weight. We have relatively good amount of people who have normal weight.*\n\n*3. BMI higher than 25 can be seen as the person is likely overweight or obese. Our graph shows the density is higher around those BMI.*"},{"metadata":{},"cell_type":"markdown","source":"### **No Stroke vs Stroke by BMI** <a id=\"3.3.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.distplot(dataset[dataset['stroke'] == 0][\"bmi\"], color='green') # No Stroke - green\nsns.distplot(dataset[dataset['stroke'] == 1][\"bmi\"], color='red') # Stroke - Red\n\nplt.title('No Stroke vs Stroke by BMI', fontsize=15)\nplt.xlim([10,100])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From the graph, it shows that the density of overweight people who suffered a stroke is more.*"},{"metadata":{},"cell_type":"markdown","source":"### **No Stroke vs Stroke by Avg. Glucose Level** <a id=\"3.3.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.distplot(dataset[dataset['stroke'] == 0][\"avg_glucose_level\"], color='green') # No Stroke - green\nsns.distplot(dataset[dataset['stroke'] == 1][\"avg_glucose_level\"], color='red') # Stroke - Red\n\nplt.title('No Stroke vs Stroke by Avg. Glucose Level', fontsize=15)\nplt.xlim([30,330])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From graph, it shows that the density of people having glucose level less than 100 suffered stroke more.*"},{"metadata":{},"cell_type":"markdown","source":"### **No Stroke vs Stroke by Age** <a id=\"3.3.5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.distplot(dataset[dataset['stroke'] == 0][\"age\"], color='green') # No Stroke - green\nsns.distplot(dataset[dataset['stroke'] == 1][\"age\"], color='red') # Stroke - Red\n\nplt.title('No Stroke vs Stroke by Age', fontsize=15)\nplt.xlim([18,100])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From graph, it can be seen that the density of people having age above 50 suffered stroke more.*"},{"metadata":{},"cell_type":"markdown","source":"## **Scatter Plot** <a id=\"3.4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### **Age vs BMI** <a id=\"3.4.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data=dataset, x=\"age\", y=\"bmi\", hue='gender')\ngraph.axhline(y= 25, linewidth=4, color='r', linestyle= '--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From above plot, we can see that there are lot of people having BMI above 25 are overweight and obese.*"},{"metadata":{},"cell_type":"markdown","source":"### **Age vs Avg. Glucose Level** <a id=\"3.4.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\ngraph = sns.scatterplot(data=dataset, x=\"age\", y=\"avg_glucose_level\", hue='gender')\ngraph.axhline(y= 150, linewidth=4, color='r', linestyle= '--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*From above plot, we can see that people having glucose level above 150 are relatively less as compare one below. So, we can say that people above 150 might be suffering from diabetes.*"},{"metadata":{},"cell_type":"markdown","source":"## **Violin Plot** <a id=\"3.5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13,13))\nsns.set_theme(style=\"darkgrid\")\nplt.subplot(2,3,1)\nsns.violinplot(x = 'gender', y = 'stroke', data = dataset)\nplt.subplot(2,3,2)\nsns.violinplot(x = 'hypertension', y = 'stroke', data = dataset)\nplt.subplot(2,3,3)\nsns.violinplot(x = 'heart_disease', y = 'stroke', data = dataset)\nplt.subplot(2,3,4)\nsns.violinplot(x = 'ever_married', y = 'stroke', data = dataset)\nplt.subplot(2,3,5)\nsns.violinplot(x = 'work_type', y = 'stroke', data = dataset)\nplt.xticks(fontsize=9, rotation=45)\nplt.subplot(2,3,6)\nsns.violinplot(x = 'Residence_type', y = 'stroke', data = dataset)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Pair Plot** <a id=\"3.6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\nsns.pairplot(dataset)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preprocessing** <a id=\"4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding <a id=\"5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## **Categorical Encoding** <a id=\"5.1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"We are using **OneHotEncoder()** to encode the categorical columns: '**gender**', '**work_type**' and '**smoking_status**."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0,5,9])], remainder= 'passthrough')\nx = np.array(ct.fit_transform(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label Encoding <a id=\"5.2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"We are using **LabelEncoder()** to encode binary columns: '**ever_married**' and '**residence_type**'"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx[:, 15] = le.fit_transform(x[:, 15])\nx[:, 16] = le.fit_transform(x[:, 16])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of X: ', x.shape)\nprint('Shape of Y: ', y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the dataset into the Training set and Test set <a id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number transactions x_train dataset: \", x_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions x_test dataset: \", x_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling <a id=\"7\"></a>"},{"metadata":{},"cell_type":"markdown","source":"*StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler results in a distribution with a standard deviation equal to 1.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Imbalance data using SMOTE <a id=\"8\"></a>"},{"metadata":{},"cell_type":"markdown","source":"*SMOTE - **Synthetic Minority Oversampling Technique** is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nx_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(x_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection <a id=\"9\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(['Logistic Regreesion', LogisticRegression(random_state=0)])\nmodels.append(['SVM', SVC(random_state=0)])\nmodels.append(['KNeighbors', KNeighborsClassifier()])\nmodels.append(['GaussianNB', GaussianNB()])\nmodels.append(['BernoulliNB', BernoulliNB()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=0)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=0)])\nmodels.append(['XGBoost', XGBClassifier(eval_metric= 'error')])\n\nlst_1= []\n\nfor m in range(len(models)):\n    lst_2= []\n    model = models[m][1]\n    model.fit(x_train_res, y_train_res)\n    y_pred = model.predict(x_test)\n    cm = confusion_matrix(y_test, y_pred)  #Confusion Matrix\n    accuracies = cross_val_score(estimator = model, X = x_train_res, y = y_train_res, cv = 10)   #K-Fold Validation\n    roc = roc_auc_score(y_test, y_pred)  #ROC AUC Score\n    precision = precision_score(y_test, y_pred)  #Precision Score\n    recall = recall_score(y_test, y_pred)  #Recall Score\n    f1 = f1_score(y_test, y_pred)  #F1 Score\n    print(models[m][0],':')\n    print(cm)\n    print('Accuracy Score: ',accuracy_score(y_test, y_pred))\n    print('')\n    print(\"K-Fold Validation Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n    print('')\n    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n    print('')\n    print('ROC AUC Score: {:.2f}'.format(roc))\n    print('')\n    print('Precision: {:.2f}'.format(precision))\n    print('')\n    print('Recall: {:.2f}'.format(recall))\n    print('')\n    print('F1: {:.2f}'.format(f1))\n    print('-----------------------------------')\n    print('')\n    lst_2.append(models[m][0])\n    lst_2.append((accuracy_score(y_test, y_pred))*100) \n    lst_2.append(accuracies.mean()*100)\n    lst_2.append(accuracies.std()*100)\n    lst_2.append(roc)\n    lst_2.append(precision)\n    lst_2.append(recall)\n    lst_2.append(f1)\n    lst_1.append(lst_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(lst_1, columns= ['Model', 'Accuracy', 'K-Fold Mean Accuracy', 'Std. Deviation', 'ROC AUC', 'Precision', 'Recall', 'F1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values(by= ['Accuracy', 'K-Fold Mean Accuracy'], inplace= True, ascending= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning the Models <a id=\"10\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*The **GridSearchCV** is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_models = [(LogisticRegression(),[{'C':[0.25,0.5,0.75,1],'random_state':[0]}]), \n               (KNeighborsClassifier(),[{'n_neighbors':[5,7,8,10], 'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}]), \n               (SVC(),[{'C':[0.25,0.5,0.75,1],'kernel':['linear', 'rbf'],'random_state':[0]}]), \n               (GaussianNB(),[{'var_smoothing': [1e-09]}]), \n               (BernoulliNB(), [{'alpha': [0.25, 0.5, 1]}]), \n               (DecisionTreeClassifier(),[{'criterion':['gini','entropy'],'random_state':[0]}]), \n               (RandomForestClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}]), \n              (XGBClassifier(), [{'learning_rate': [0.01, 0.05, 0.1], 'eval_metric': ['error']}])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in grid_models:\n    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 10)\n    grid.fit(x_train_res, y_train_res)\n    best_accuracy = grid.best_score_\n    best_param = grid.best_params_\n    print('{}:\\nBest Accuracy : {:.2f}%'.format(i,best_accuracy*100))\n    print('Best Parameters : ',best_param)\n    print('')\n    print('----------------')\n    print('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Looking at output after **GridSearch**, we can determine that the **RandomForest** and **XGBoost** seems best fit for the model.*"},{"metadata":{},"cell_type":"markdown","source":"# Models after Tuning Hyperparameters <a id=\"11\"></a>"},{"metadata":{},"cell_type":"markdown","source":"*We only see **RandomForest** and **XGBoost** performance as they have high accuracy.*"},{"metadata":{},"cell_type":"markdown","source":"## RandomForest <a id=\"11.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting RandomForest Model\nclassifier = RandomForestClassifier(criterion= 'gini', n_estimators= 100, random_state= 0)\nclassifier.fit(x_train_res, y_train_res)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost <a id=\"11.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting XGBClassifier Model\nclassifier = XGBClassifier(eval_metric= 'error', learning_rate= 0.1)\nclassifier.fit(x_train_res, y_train_res)\ny_pred = classifier.predict(x_test)\ny_prob = classifier.predict_proba(x_test)[:,1]\ncm = confusion_matrix(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))\n\n# Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()\n\n# Roc Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras ANN <a id=\"12\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.regularizers import l2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building the ANN <a id=\"12.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Builing the function\ndef ann_classifier():\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    tf.keras.layers.Dropout(0.6)\n    ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\n    ann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n    return ann","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Passing values to KerasClassifier \nann = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating the ANN (Cross Validation) <a id=\"12.2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"*Wrapping k-fold cross validation into keras model*"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# We are using 5 fold cross validation here\naccuracies = cross_val_score(estimator = ann, X = x_train_res, y = y_train_res, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the mean and standard deviation of the accuracies obtained\nmean = accuracies.mean()\nstd_deviation = accuracies.std()\nprint(\"Accuracy: {:.2f} %\".format(mean*100))\nprint(\"Standard Deviation: {:.2f} %\".format(std_deviation*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tuning the ANN <a id=\"12.3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"*We use the Grid Search method for this task*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Builing the function\ndef ann_classifier(optimizer = 'adam'):\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    ann.add(tf.keras.layers.Dense(units= 8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n    tf.keras.layers.Dropout(0.6)\n    ann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\n    ann.compile(optimizer= optimizer, loss= 'binary_crossentropy', metrics= ['accuracy'])\n    return ann","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Passing values to KerasClassifier \nann = KerasClassifier(build_fn = ann_classifier, batch_size = 32, epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# Using Grid Search CV to getting the best parameters\nparameters = {'batch_size': [25, 32],\n             'epochs': [50, 100, 150],\n             'optimizer': ['adam', 'rmsprop']}\n\ngrid_search = GridSearchCV(estimator = ann, param_grid = parameters, scoring = 'accuracy', cv = 5, n_jobs = -1)\n\ngrid_search.fit(x_train_res, y_train_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ANN Model after Tuning <a id=\"12.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann = tf.keras.models.Sequential()\nann.add(tf.keras.layers.Dense(units= 32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\nann.add(tf.keras.layers.Dense(units= 32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\ntf.keras.layers.Dropout(0.6)\nann.add(tf.keras.layers.Dense(units= 1, activation='sigmoid'))\nann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"ann_history = ann.fit(x_train_res, y_train_res, batch_size= 25, epochs= 150, validation_split= 0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Graph <a id=\"12.4.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = ann_history.history['loss']\nloss_val = ann_history.history['val_loss']\nepochs = range(1,151)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Accuracy Graph <a id=\"12.4.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = ann_history.history['accuracy']\nloss_val = ann_history.history['val_accuracy']\nepochs = range(1,151)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:\\n',cm)\n\n# Calculate the Accuracy\naccuracy = accuracy_score(y_pred,y_test)\nprint('Accuracy: ',accuracy)\n\n#Visualizing Confusion Matrix\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion <a id=\"13\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Therefore, after the multiple visualizations of our and going through all the performance of the models. I tune the hyperparameters with the help of GridSearch to get models. After that, I came to conclusion that ***RandomForestClassifier*** is best model for this dataset."},{"metadata":{},"cell_type":"markdown","source":"### Thank You!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}