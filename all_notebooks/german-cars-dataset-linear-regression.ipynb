{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Can we predict the price of a car based on its properties?","metadata":{"editable":false}},{"cell_type":"markdown","source":"Can we predict the price of a car based on the dataset we have through a linear regression?\nA somewhat classical example of a linear regression, I will be working on a dataset where some properties of more than 45k cars to predict their price.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false}},{"cell_type":"markdown","source":"So, we should start as usual with importing the relevant libraries.","metadata":{"editable":false}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:38:02.033924Z","iopub.execute_input":"2021-07-31T20:38:02.03435Z","iopub.status.idle":"2021-07-31T20:38:03.115184Z","shell.execute_reply.started":"2021-07-31T20:38:02.034265Z","shell.execute_reply":"2021-07-31T20:38:03.113943Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we are opening our dataset, again as usual.","metadata":{"editable":false}},{"cell_type":"code","source":"df = pd.read_csv('../input/cars-germany/autoscout24-germany-dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:39:58.781949Z","iopub.execute_input":"2021-07-31T20:39:58.782339Z","iopub.status.idle":"2021-07-31T20:39:58.888772Z","shell.execute_reply.started":"2021-07-31T20:39:58.782308Z","shell.execute_reply":"2021-07-31T20:39:58.88759Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_rows = 1000","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:40:19.045777Z","iopub.execute_input":"2021-07-31T20:40:19.046351Z","iopub.status.idle":"2021-07-31T20:40:19.051269Z","shell.execute_reply.started":"2021-07-31T20:40:19.046316Z","shell.execute_reply":"2021-07-31T20:40:19.050136Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:40:30.834758Z","iopub.execute_input":"2021-07-31T20:40:30.835165Z","iopub.status.idle":"2021-07-31T20:40:30.881009Z","shell.execute_reply.started":"2021-07-31T20:40:30.835133Z","shell.execute_reply":"2021-07-31T20:40:30.879891Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{"editable":false}},{"cell_type":"markdown","source":"### Outliers","metadata":{"editable":false}},{"cell_type":"markdown","source":"Let's detect, if any, outliers and get rid of them.","metadata":{"editable":false}},{"cell_type":"code","source":"df.corr()['price'].sort_values()\n#As seen the most correlated property is 'hp' vis-Ã -vis the price.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:44:07.214244Z","iopub.execute_input":"2021-07-31T20:44:07.214642Z","iopub.status.idle":"2021-07-31T20:44:07.233543Z","shell.execute_reply.started":"2021-07-31T20:44:07.214612Z","shell.execute_reply":"2021-07-31T20:44:07.232002Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the data, to visually see the outliers, again if any.","metadata":{"editable":false}},{"cell_type":"code","source":"sns.scatterplot(x='hp',y='price', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:44:43.343946Z","iopub.execute_input":"2021-07-31T20:44:43.344342Z","iopub.status.idle":"2021-07-31T20:44:43.788446Z","shell.execute_reply.started":"2021-07-31T20:44:43.344306Z","shell.execute_reply":"2021-07-31T20:44:43.786868Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='year',y='price', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:45:00.220814Z","iopub.execute_input":"2021-07-31T20:45:00.22119Z","iopub.status.idle":"2021-07-31T20:45:00.590315Z","shell.execute_reply.started":"2021-07-31T20:45:00.22116Z","shell.execute_reply":"2021-07-31T20:45:00.589297Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we do have a few outliers, as visually seen from the graphs. We will remove them from our dataset.","metadata":{"editable":false}},{"cell_type":"code","source":"df[(df['price']>600000)]\n#These three cars are outliers due to their prices.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:46:14.126487Z","iopub.execute_input":"2021-07-31T20:46:14.126854Z","iopub.status.idle":"2021-07-31T20:46:14.145158Z","shell.execute_reply.started":"2021-07-31T20:46:14.126825Z","shell.execute_reply":"2021-07-31T20:46:14.144051Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_ind = df[(df['price']>600000)].index\ndf = df.drop(drop_ind, axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:46:34.828371Z","iopub.execute_input":"2021-07-31T20:46:34.828776Z","iopub.status.idle":"2021-07-31T20:46:34.841094Z","shell.execute_reply.started":"2021-07-31T20:46:34.828745Z","shell.execute_reply":"2021-07-31T20:46:34.83978Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the current version of our dataset.","metadata":{"editable":false}},{"cell_type":"code","source":"sns.scatterplot(x='hp',y='price', data = df)\n#The data is dispersed especially after the 600 hp, but the current version is better to work on.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:48:46.341009Z","iopub.execute_input":"2021-07-31T20:48:46.34174Z","iopub.status.idle":"2021-07-31T20:48:46.731107Z","shell.execute_reply.started":"2021-07-31T20:48:46.341684Z","shell.execute_reply":"2021-07-31T20:48:46.73003Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()\n#We have some null values in our dataset. We need to either get rid of them, or fill them with some rational values.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:50:33.000095Z","iopub.execute_input":"2021-07-31T20:50:33.000464Z","iopub.status.idle":"2021-07-31T20:50:33.04137Z","shell.execute_reply.started":"2021-07-31T20:50:33.000433Z","shell.execute_reply":"2021-07-31T20:50:33.040248Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()\n#This is a better representation of our null data.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:51:15.311736Z","iopub.execute_input":"2021-07-31T20:51:15.3121Z","iopub.status.idle":"2021-07-31T20:51:15.345345Z","shell.execute_reply.started":"2021-07-31T20:51:15.31207Z","shell.execute_reply":"2021-07-31T20:51:15.34448Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if these null values are meaningful.","metadata":{"editable":false}},{"cell_type":"code","source":"100 * df.isnull().sum() / len(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:51:58.613611Z","iopub.execute_input":"2021-07-31T20:51:58.613978Z","iopub.status.idle":"2021-07-31T20:51:58.647833Z","shell.execute_reply.started":"2021-07-31T20:51:58.61394Z","shell.execute_reply":"2021-07-31T20:51:58.646454Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1)As seen from the table, there are 3 features containing null values. We are more interested in 'make', 'mileage', 'hp' and 'year'.\n\n2)In order to have as many data as possible, I will try to fill these values rather than simply removing them. \n","metadata":{"editable":false}},{"cell_type":"code","source":"df['model'] = df['model'].fillna('None')\ndf['gear'] = df['gear'].fillna('None')\n100 * df.isnull().sum() / len(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:56:01.911434Z","iopub.execute_input":"2021-07-31T20:56:01.912129Z","iopub.status.idle":"2021-07-31T20:56:01.959129Z","shell.execute_reply.started":"2021-07-31T20:56:01.912089Z","shell.execute_reply":"2021-07-31T20:56:01.958101Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order not to remove the rows where 'hp' data is missing, I will adopt a very unorthodox approach and fill these data with the average values of 'hp', which we will later on see that it is '132'.","metadata":{"editable":false}},{"cell_type":"code","source":"df['hp'].mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:59:11.459446Z","iopub.execute_input":"2021-07-31T20:59:11.459823Z","iopub.status.idle":"2021-07-31T20:59:11.467824Z","shell.execute_reply.started":"2021-07-31T20:59:11.459788Z","shell.execute_reply":"2021-07-31T20:59:11.466678Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['hp'] = df['hp'].fillna(132)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:59:17.875745Z","iopub.execute_input":"2021-07-31T20:59:17.876438Z","iopub.status.idle":"2021-07-31T20:59:17.88262Z","shell.execute_reply.started":"2021-07-31T20:59:17.876394Z","shell.execute_reply":"2021-07-31T20:59:17.881802Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"100 * df.isnull().sum() / len(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T20:59:20.699536Z","iopub.execute_input":"2021-07-31T20:59:20.700115Z","iopub.status.idle":"2021-07-31T20:59:20.735243Z","shell.execute_reply.started":"2021-07-31T20:59:20.700067Z","shell.execute_reply":"2021-07-31T20:59:20.734158Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the Dummy Variables","metadata":{"editable":false}},{"cell_type":"code","source":"my_object_df = df.select_dtypes(include = 'object')\nmy_numeric_df = df.select_dtypes(exclude = 'object')\nmy_object_df","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:00:09.650655Z","iopub.execute_input":"2021-07-31T21:00:09.651052Z","iopub.status.idle":"2021-07-31T21:00:09.678922Z","shell.execute_reply.started":"2021-07-31T21:00:09.651018Z","shell.execute_reply":"2021-07-31T21:00:09.677884Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_objects_dummies = pd.get_dummies(my_object_df, drop_first = True)\ndf_objects_dummies\n#So we have created dummy variables, instead of having them as strings.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:01:10.693394Z","iopub.execute_input":"2021-07-31T21:01:10.693753Z","iopub.status.idle":"2021-07-31T21:01:10.937135Z","shell.execute_reply.started":"2021-07-31T21:01:10.693724Z","shell.execute_reply":"2021-07-31T21:01:10.936044Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = pd.concat([my_numeric_df,df_objects_dummies],axis=1)\nfinal_df\n#We are concatenating the dummy variables with the numeric columns.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:01:45.082222Z","iopub.execute_input":"2021-07-31T21:01:45.082564Z","iopub.status.idle":"2021-07-31T21:01:45.203458Z","shell.execute_reply.started":"2021-07-31T21:01:45.082535Z","shell.execute_reply":"2021-07-31T21:01:45.202333Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df.info()\n#Let's see the final version of our dataframe.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:02:09.643145Z","iopub.execute_input":"2021-07-31T21:02:09.643626Z","iopub.status.idle":"2021-07-31T21:02:09.702766Z","shell.execute_reply.started":"2021-07-31T21:02:09.643585Z","shell.execute_reply":"2021-07-31T21:02:09.701693Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating our Features (X) and Target (y)","metadata":{"editable":false}},{"cell_type":"code","source":"X = final_df.drop('price', axis = 1)\ny = final_df['price']","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:03:06.610097Z","iopub.execute_input":"2021-07-31T21:03:06.610466Z","iopub.status.idle":"2021-07-31T21:03:06.671302Z","shell.execute_reply.started":"2021-07-31T21:03:06.610434Z","shell.execute_reply":"2021-07-31T21:03:06.670518Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating our Training and Test Sets","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#At this stage, we are certainly importing train_test_split module from sklearn.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:04:11.523365Z","iopub.execute_input":"2021-07-31T21:04:11.523753Z","iopub.status.idle":"2021-07-31T21:04:11.73035Z","shell.execute_reply.started":"2021-07-31T21:04:11.523721Z","shell.execute_reply":"2021-07-31T21:04:11.729525Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:04:22.308307Z","iopub.execute_input":"2021-07-31T21:04:22.308851Z","iopub.status.idle":"2021-07-31T21:04:22.597136Z","shell.execute_reply.started":"2021-07-31T21:04:22.308804Z","shell.execute_reply":"2021-07-31T21:04:22.596287Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling our X Features","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n#For this we are going to need StandScaler module from sklearn library.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:05:18.361315Z","iopub.execute_input":"2021-07-31T21:05:18.361923Z","iopub.status.idle":"2021-07-31T21:05:18.365868Z","shell.execute_reply.started":"2021-07-31T21:05:18.361857Z","shell.execute_reply":"2021-07-31T21:05:18.364836Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:05:25.315115Z","iopub.execute_input":"2021-07-31T21:05:25.315674Z","iopub.status.idle":"2021-07-31T21:05:25.319482Z","shell.execute_reply.started":"2021-07-31T21:05:25.315625Z","shell.execute_reply":"2021-07-31T21:05:25.318687Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_X_train = scaler.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:05:31.373301Z","iopub.execute_input":"2021-07-31T21:05:31.37395Z","iopub.status.idle":"2021-07-31T21:05:32.363111Z","shell.execute_reply.started":"2021-07-31T21:05:31.373913Z","shell.execute_reply":"2021-07-31T21:05:32.361805Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_X_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:05:37.110788Z","iopub.execute_input":"2021-07-31T21:05:37.111207Z","iopub.status.idle":"2021-07-31T21:05:37.189238Z","shell.execute_reply.started":"2021-07-31T21:05:37.11117Z","shell.execute_reply":"2021-07-31T21:05:37.188153Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating our Linear Regression","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n#We are importing LinearRegression module in order to create and run our linear regression.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:07:19.572514Z","iopub.execute_input":"2021-07-31T21:07:19.572919Z","iopub.status.idle":"2021-07-31T21:07:19.809986Z","shell.execute_reply.started":"2021-07-31T21:07:19.572873Z","shell.execute_reply":"2021-07-31T21:07:19.809016Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg = LinearRegression()\nreg.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:07:33.167949Z","iopub.execute_input":"2021-07-31T21:07:33.16831Z","iopub.status.idle":"2021-07-31T21:07:35.829015Z","shell.execute_reply.started":"2021-07-31T21:07:33.168282Z","shell.execute_reply":"2021-07-31T21:07:35.827776Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg.score(X_train,y_train)\n#Our regression score is around 0.924. Does not seem so bad I guess.","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:08:47.838338Z","iopub.execute_input":"2021-07-31T21:08:47.838686Z","iopub.status.idle":"2021-07-31T21:08:47.966197Z","shell.execute_reply.started":"2021-07-31T21:08:47.838654Z","shell.execute_reply":"2021-07-31T21:08:47.964855Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg.fit(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:09:02.23809Z","iopub.execute_input":"2021-07-31T21:09:02.238436Z","iopub.status.idle":"2021-07-31T21:09:02.90889Z","shell.execute_reply.started":"2021-07-31T21:09:02.238407Z","shell.execute_reply":"2021-07-31T21:09:02.908116Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T21:09:09.49045Z","iopub.execute_input":"2021-07-31T21:09:09.4911Z","iopub.status.idle":"2021-07-31T21:09:09.54424Z","shell.execute_reply.started":"2021-07-31T21:09:09.491062Z","shell.execute_reply":"2021-07-31T21:09:09.542988Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our test score is around 0.9277. This implies some overfitting, since I would expect the test score to be a little lower than the training regression score. But in overall, I think the linear regression model gave a satisfactory result.","metadata":{"editable":false}}]}