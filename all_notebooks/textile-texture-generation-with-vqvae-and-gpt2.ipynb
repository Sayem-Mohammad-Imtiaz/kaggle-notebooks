{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://gist.github.com/belkhir-nacim/5230ccfcab05f30c35abb03444f6a216 dataset_util\n!pip install git+https://github.com/belkhir-nacim/generative_model_toolbox\n!pip install pytorch-lightning\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from collections import OrderedDict\nimport os,h5py\nimport numpy as np\nimport pandas as pd\nfrom argparse import Namespace\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch import nn,optim\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport pytorch_lightning as pl\nfrom dataset_util.kaggle_textile_texuture_dataset import TextureDataset\nfrom functools import partial\nimport math\nfrom generative_models_toolbox.algos.vqvae import VQVAEModel\nfrom transformers import GPT2LMHeadModel, GPT2Config\nfrom generative_models_toolbox.utils.device import Cudafy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cudafy = Cudafy(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class TrainerVQVae(pl.LightningModule):\n\n    def __init__(self, hparams):\n        super(TrainerVQVae, self).__init__()\n        #init model\n        self.model = self.init_model(hparams=hparams)\n        self.hparams = hparams\n        self.last_imgs = None\n        \n    def init_model(self,hparams):\n        num_hiddens = 128  if hparams.num_hiddens is None else hparams.num_hiddens\n        num_residual_hiddens = 32 if hparams.num_residual_hiddens is None else hparams.num_residual_hiddens\n        num_residual_layers = 2 if hparams.num_residual_layers is None else hparams.num_residual_layers\n        embedding_dim = 64 if hparams.embedding_dim is None else hparams.embedding_dim\n        num_embeddings = 128 if hparams.num_embeddings is None else hparams.num_embeddings\n        commitment_cost = 0.25 if hparams.commitment_cost is None else hparams.commitment_cost\n        decay = 0.99 if hparams.decay is None else hparams.decay\n        return VQVAEModel(num_hiddens=num_hiddens,num_residual_hiddens=num_residual_hiddens,num_residual_layers=num_residual_layers, embedding_dim=embedding_dim,num_embeddings=num_embeddings,commitment_cost=commitment_cost,decay=decay)\n        \n    def prepare_data(self):\n        \n        self.data_mean = 0.3541\n        self.data_variance = 0.1352\n        transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize([self.data_mean], [self.data_variance]) ])\n        assert((self.hparams.root is not None) and (os.path.exists(self.hparams.root)))\n        self.hparams.patch_size = 64 if self.hparams.patch_size is None else  self.hparams.patch_size\n        \n        self.hparams.sub_sample_train = 1000 if self.hparams.sub_sample_train is None else self.hparams.sub_sample_train\n        self.hparams.sub_sample_valid = 500 if self.hparams.sub_sample_valid is None else self.hparams.sub_sample_valid\n        self.hparams.sub_sample_test = 2000 if self.hparams.sub_sample_test is None else self.hparams.sub_sample_test\n        self.keep_angles = False if self.hparams.keep_angles is None else self.hparams.keep_angles\n        self.batch_size = 256 if self.hparams.batch_size is None else self.hparams.batch_size\n        self.dataset_train = TextureDataset(self.hparams.root,train=True,patch_size=self.hparams.patch_size, keep_angles=self.keep_angles, keep_defects=False, transformation=transformation, sub_sample=self.hparams.sub_sample_train)\n        self.dataset_valid = TextureDataset(self.hparams.root,train=False, patch_size=self.hparams.patch_size, keep_angles=self.keep_angles, keep_defects=False, transformation=transformation,sub_sample=self.hparams.sub_sample_valid)\n        self.dataset_test = TextureDataset(self.hparams.root,train=False, patch_size=self.hparams.patch_size, keep_angles=self.keep_angles, keep_defects=True, transformation=transformation, sub_sample=self.hparams.sub_sample_test)\n        \n    def configure_optimizers(self):\n        self.hparams.lr = 5e-3  if self.hparams.lr is  None else self.hparams.lr\n        if self.hparams.optim is not None:\n            if self.hparams.optim =='adam':\n                optim = torch.optim.Adam(self.parameters(), lr=self.hparams.lr,amsgrad=False)\n        else:\n            optim = torch.optim.Adam(self.parameters(), lr=lr)\n        return optim\n    \n    def _step_run(self,z):\n        z = self(z)\n        z = self.model(z)\n        return self(z)\n    \n    def forward(self, x):\n        return x\n\n    def encode(self, x):\n        return self.model.encode(x)\n\n    def reconstruct_from_code(self, code):\n        return self.model.reconstruct_from_code(code)\n    \n    \n    def train_dataloader(self):\n        return DataLoader(dataset=self.dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    \n    def training_step(self, batch, batch_nb):\n        X, _, _ = batch\n        commit_loss, XR, perplexity = self.model(X)\n        recons_loss = F.mse_loss(X, XR)/self.data_variance\n        loss = recons_loss + commit_loss\n        return OrderedDict({\"loss\": loss, \"log\": {\"loss\": loss,\"commit_loss\": commit_loss,\"recons_loss\": recons_loss,}})\n        \n    \n    def training_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        tqdm_dict = {'train_loss': avg_loss}\n        return {'train_loss': avg_loss, 'log': tqdm_dict, 'progress_bar': tqdm_dict}\n    \n            \n    def val_dataloader(self):\n        return DataLoader(dataset=self.dataset_valid, batch_size=self.batch_size, shuffle=False, num_workers=0, pin_memory=False)\n\n    def validation_step(self, batch, batch_nb):\n        data, _, _ = batch\n        vq_loss, data_recon, perplexity = self._step_run(data)\n        loss = F.mse_loss(data_recon, data) / self.data_variance\n        return {'val_loss': loss}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        tqdm_dict = {'val_loss': avg_loss}\n        return {'val_loss ': avg_loss, 'log': tqdm_dict, 'progress_bar': tqdm_dict}\n\n        \n    def test_dataloader(self):\n        return DataLoader(dataset=self.dataset_test, batch_size=self.batch_size, shuffle=False, num_workers=0, pin_memory=False)\n    \n    def test_step(self, batch, batch_nb):\n        data, _, _ = batch\n        if self.last_imgs is None: self.last_imgs = data\n        vq_loss, data_recon, perplexity = self._step_run(data)\n        loss = F.mse_loss(data_recon, data) / self.data_variance\n        return {'test_loss': loss}\n    \n    def test_epoch_end(self, output):\n        if self.last_imgs is not None:\n            # log sampled images\n            _, sample_imgs, _ = self.model(self.last_imgs)\n            grid = make_grid(sample_imgs,16)*self.data_variance + self.data_mean\n            self.logger.experiment.add_image(f'GeneratedImages', grid, self.current_epoch)\n            grid = make_grid(self.last_imgs,16)*self.data_variance + self.data_mean\n            self.logger.experiment.add_image(f'InputImages', grid, self.current_epoch)\n        avg_loss = torch.stack([x['test_loss'] for x in output]).mean()\n        tqdm_dict = {'test_loss': avg_loss}\n        return {'test_loss': avg_loss, 'log': tqdm_dict, 'progress_bar': tqdm_dict}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_train_vqvae(params= None):\n    if params is None:\n        params = {'root':'/kaggle/input/textiledefectdetection','patch_size':64, 'keep_angles':True, 'sub_sample_train':2000, 'sub_sample_valid': 512, 'sub_sample_test':1024, 'batch_size':256, 'lr': 1e-3,  'num_hiddens':64, 'num_residual_hiddens': 32, 'num_residual_layers' : 2, 'embedding_dim' :64, 'num_embeddings' : 128, 'commitment_cost' : 0.25,'decay' : 0.99, 'optim':'adam'}\n    hparams = Namespace(**params)\n    vqvae_model = TrainerVQVae(hparams)\n    vqvae_trainer = pl.Trainer(gpus=hparams.gpus, max_epochs=hparams.max_epochs, check_val_every_n_epoch=hparams.check_val_every_n_epoch, show_progress_bar=False )\n    vqvae_trainer.fit(vqvae_model)\n    return vqvae_model, vqvae_trainer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_train_vqvae(params = {'root':'/kaggle/input/textiledefectdetection','patch_size':64, 'keep_angles':True, 'sub_sample_train':2000, 'sub_sample_valid': 512, 'sub_sample_test':1024, 'batch_size':256, 'lr': 1e-3,  'num_hiddens':128 ,\n                          'num_residual_hiddens': 32, 'num_residual_layers' : 2, 'embedding_dim' :64, 'num_embeddings' : 256, 'commitment_cost' : 0.25,'decay' : 0.99, 'optim':'adam','check_val_every_n_epoch':10,'max_epochs':300,\n                          'gpus':1 if torch.cuda.is_available() else 0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_result_images(path,checkpoint):\n    vqvae_model = TrainerVQVae.load_from_checkpoint(checkpoint)\n    vqvae_trainer = pl.Trainer(gpus=1, max_epochs=300, check_val_every_n_epoch=3, show_progress_bar=False )\n    vqvae_trainer.test(vqvae_model)\n    from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n    event_acc = EventAccumulator(path)\n    event_acc.Reload()\n    # Show all tags in the log file\n    print(event_acc.Tags())\n    input_images = event_acc.Images('InputImages')[0]\n    generated_images = event_acc.Images('GeneratedImages')[0]\n    import tensorflow as tf\n    i = tf.image.decode_image(input_images.encoded_image_string)\n    d = tf.image.decode_image(generated_images.encoded_image_string)\n    import matplotlib.pyplot as plt\n    fig,axes = plt.subplots(1,2, figsize=(20,20))\n    axes[0].imshow(i.numpy())\n    axes[1].imshow(d.numpy())\nimport torch, gc\ngc.collect()\ntorch.cuda.empty_cache()\n# display_result_images('/kaggle/working/lightning_logs/version_1/',checkpoint='/kaggle/working/lightning_logs/version_0/checkpoints/epoch=284.ckpt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformer GPT2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(pl.LightningModule):\n    def __init__(self, hparams, load_dataset=True):\n        super().__init__()\n        if load_dataset:\n            self.dataset = self.load_dataset(hparams)\n            hparams.vocab_size = self.dataset.vocab_size\n            hparams.height, hparams.width = self.dataset.shape[1:]\n            hparams.max_length = self.dataset.length\n            hparams.start_token = self.dataset.start_token\n        self.model = self.build_model(hparams)\n        self.hparams = hparams\n        \n    def load_dataset(self, hparams):\n        print(\"Loading the dataset of codes into memory...\")\n        device = \"cpu\"\n        vqvae = TrainerVQVae.load_from_checkpoint(hparams.vqvae_model_path)\n        vqvae = vqvae.to(device)\n        vqvae.prepare_data()\n        codes = []\n        nb = 0\n        for X, _, _ in vqvae.train_dataloader():\n            X = X.to(device)\n            zinds = vqvae.encode(X)\n            if nb==0:\n                print('zinds', zinds.shape)\n            codes.append(zinds.data.cpu())\n            nb += len(zinds)\n            if hasattr(hparams,'nb_examples') and hparams.nb_examples is not None and  nb >= hparams.nb_examples:\n                break\n        codes = torch.cat(codes)\n        print('Code shape', codes.shape, type(codes),'min',codes.min(),'max', codes.max())\n        print(np.unique(codes))\n        if hparams.nb_examples and len(codes) >= hparams.nb_examples:\n            codes = codes[:hparams.nb_examples]\n        vocab_size = vqvae.model.num_embeddings + 1\n        start_token = vqvae.model.num_embeddings\n        codes_ = codes.view(len(codes), -1)\n        codes_ = torch.cat([(torch.ones(len(codes_), 1).long() * start_token), codes_.long(),], dim=1)\n        print('codes', codes_.shape)\n        length = codes_.shape[1]\n        dataset = TensorDataset(codes_)\n        dataset.vocab_size = vocab_size\n        dataset.shape = codes.shape\n\n        dataset.length = length\n        dataset.start_token = start_token\n        print(\"Done loading dataset\")\n        return dataset\n\n    def forward(self, x):\n        return self.model(x)\n\n    def build_model(self, hparams):\n        config = GPT2Config(vocab_size=hparams.vocab_size, n_positions=hparams.max_length, n_ctx=hparams.max_length, n_embd=256, n_layer=4, n_head=4, resid_pdrop=0.2,embd_pdrop=0.2,attn_pdrop=0.2)\n        return GPT2LMHeadModel(config)\n\n    def generate(self, nb_examples=1, **kwargs):\n        \n        input_ids = torch.zeros(nb_examples, 1).long().to(self.device)\n        input_ids[:] = self.hparams.vocab_size - 1\n        result = self.model.generate(input_ids, max_length=self.hparams.max_length, **kwargs)\n        print(result.shape,'output of model')\n        result = result[:, 1:]\n        result = result.contiguous()\n        print(result.shape, 'result afeter reshaping')\n        result = result.view(nb_examples, self.hparams.height, self.hparams.width)\n        return result\n\n    def training_step(self, batch, batch_idx):\n        (X,) = batch\n        loss, *rest = self.model(X, labels=X)\n        output = OrderedDict({\"loss\": loss, \"log\": {\"loss\": loss,},})\n        return output\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(self.dataset, batch_size=self.hparams.batch_size, shuffle=True, num_workers=self.hparams.num_workers)\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(),lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=self.hparams.scheduler_gamma)\n        return [optimizer], [scheduler]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_transformer_generator(hparams=None):\n    if hparams is None: \n        hparams = {'folder': 'out/generator','epochs': 200,'vqvae_model_path':None, 'lr': 5e-3, 'weight_decay': 0, 'scheduler_gamma': 1, 'batch_size': 64, 'num_workers': 1, 'gpus': 1,'nb_examples':2000}\n    hparams = Namespace(**hparams)\n    model = Generator(hparams)\n    logger = pl.loggers.TensorBoardLogger(save_dir=hparams.folder, name=\"logs\")\n    trainer = pl.Trainer(default_root=hparams.folder, max_epochs=hparams.epochs,show_progress_bar=False,gpus=hparams.gpus,logger=logger, resume_from_checkpoint=hparams.resume_from_checkpoint)\n    trainer.fit(model)\n    return model, trainer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hparams = {'resume_from_checkpoint':None,\n           'folder': 'out/generator', 'epochs': 200,  'lr': 1e-3, 'weight_decay': 0, 'scheduler_gamma':  1, 'batch_size': 64,\n           'num_workers': 1, 'nb_examples':3000, 'vqvae_model_path':'/kaggle/working/lightning_logs/version_0/checkpoints/epoch=299.ckpt', 'gpus': 1 if torch.cuda.is_available() else 0}\ngenerator, trainergenerator = train_transformer_generator(hparams=hparams)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Texture Images from the GPT2\n\nThe model is generated from the GPT2 model and VQVAE:\n* First the GPT2 model is used to randomly generate code indices\n*  It is followed by the VQVAE used for generating the final image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_generate_from_GPT2(path_generator='/kaggle/working/out/generator/logs/version_3/checkpoints/epoch=18.ckpt',\n                              path_vqvae='/kaggle/working/lightning_logs/version_0/checkpoints/epoch=9.ckpt', nb_examples = 2):\n    vqvae_model = TrainerVQVae.load_from_checkpoint(path_vqvae)\n    vqvae_model = cudafy(vqvae_model)\n    vqvae_model.eval()\n    generator = Generator.load_from_checkpoint(path_generator)\n    generator = cudafy(generator)\n    generator.eval()\n\n    input_ids = cudafy(torch.randint(0,generator.hparams.vocab_size-1, (nb_examples**2, 1)).long())\n    result = generator.model.generate(input_ids, max_length=generator.hparams.max_length, do_sample=True)\n    result = result[:, 1:]\n    result = result.contiguous()\n    result = result.view(nb_examples**2, generator.hparams.height, generator.hparams.width)\n    result[result>=generator.hparams.vocab_size-1] = generator.hparams.vocab_size-2\n    with torch.no_grad():\n        generated = vqvae_model.reconstruct_from_code(result)\n\n        fig, axes = plt.subplots(1,2, figsize=(25,25))\n        img = make_grid(generated, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n        axes[0].imshow(cudafy.get(img).detach().cpu().numpy())\n        axes[0].set_title('Generated from the sampled codes')\n        vqvae_model.model.eval()\n        _, XR, _ = vqvae_model.model(generated)\n        img = make_grid(XR, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n        axes[1].imshow(cudafy.get(img).detach().cpu().numpy())\n        axes[1].set_title('Reconstructed from the generated images')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generate_from_GPT2(path_generator='/kaggle/working/out/generator/logs/version_0/checkpoints/epoch=199.ckpt', path_vqvae='/kaggle/working/lightning_logs/version_0/checkpoints/epoch=299.ckpt', nb_examples = 16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare  with original Images\n\nCompare original with reconstructed image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_compare_original():\n    path_vqvae='/kaggle/working/lightning_logs/version_0/checkpoints/epoch=299.ckpt'\n    vqvae = TrainerVQVae.load_from_checkpoint(path_vqvae)\n    vqvae.eval()\n    vqvae.prepare_data()\n    codes = []\n    nb = 0\n    with torch.no_grad():\n        for X, _, _ in vqvae.val_dataloader():\n            fig, axes = plt.subplots(1,2, figsize=(25,25))\n            img = make_grid(X, 16).permute(1,2,0)*0.1352 + 0.3541\n            axes[0].imshow(cudafy.get(img).detach().cpu().numpy())\n            axes[0].set_title('Original Images (without defects)')\n\n            vqvae.model.eval()\n            _, XR, _ = vqvae.model(X)\n            img = make_grid(XR, 16).permute(1,2,0)*0.1352 + 0.3541\n            axes[1].imshow(cudafy.get(img).detach().cpu().numpy())\n            axes[1].set_title('Reconstructed Images (without defects)')        \n            break\n    with torch.no_grad():\n        for X, _, _ in vqvae.test_dataloader():\n            fig, axes = plt.subplots(1,2, figsize=(25,25))\n            img = make_grid(X, 16).permute(1,2,0)*0.1352 + 0.3541\n            axes[0].imshow(cudafy.get(img).detach().cpu().numpy())\n            axes[0].set_title('Original Images (with defects)')\n\n            vqvae.model.eval()\n            _, XR, _ = vqvae.model(X)\n            img = make_grid(XR, 16).permute(1,2,0)*0.1352 + 0.3541\n            axes[1].imshow(cudafy.get(img).detach().cpu().numpy())\n            axes[1].set_title('Reconstructed Images (with defects)')        \n            break\n\n    erasor = transforms.RandomErasing(p=1., value='random')\n\n    with torch.no_grad():\n        for X, _, _ in vqvae.test_dataloader():\n\n            for i in range(X.shape[0]):\n                X[i] = erasor(X[i])\n\n            fig, axes = plt.subplots(1,2, figsize=(25,25))\n            img = make_grid(X, 16).permute(1,2,0)*0.1352 + 0.3541\n            axes[0].imshow(cudafy.get(img).detach().cpu().numpy())\n            axes[0].set_title('Original Images (with defects)')\n\n            vqvae.model.eval()\n            _, XR, _ = vqvae.model(X)\n            img = make_grid(XR, 16).permute(1,2,0)*0.1352 + 0.3541\n            axes[1].imshow(cudafy.get(img).detach().cpu().numpy())\n            axes[1].set_title('Reconstructed Images (with defects)')        \n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_compare_original()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_generate_from_erased_images_GPT2(path_generator='/kaggle/working/out/generator/logs/version_3/checkpoints/epoch=18.ckpt',\n                              path_vqvae='/kaggle/working/lightning_logs/version_0/checkpoints/epoch=9.ckpt', nb_examples = 2,test_image=True):\n    vqvae = TrainerVQVae.load_from_checkpoint(path_vqvae)\n    vqvae = cudafy(vqvae)\n    vqvae.eval()\n    vqvae.prepare_data()\n    erasor = transforms.RandomErasing(p=1., value='random',inplace=False)\n    codes = []\n    codes = []\n    nb = 0\n    \n    x_example = []\n    x_example_erased = []\n    loader = vqvae.test_dataloader() if test_image else vqvae.val_dataloader()\n    for X, _, _ in loader:\n        X = cudafy(X)\n        x_example.append(torch.zeros_like(X) + X)\n        for iii in range(X.shape[0]):\n            X[iii] = erasor(X[iii])\n            \n        x_example_erased.append(X)\n        zinds = vqvae.encode(X)\n        if nb==0:\n            print('zinds', zinds.shape)\n        codes.append(zinds.data.cpu())\n        nb += len(zinds)\n        if len(codes) >= nb_examples**2:\n            break\n            \n    x_example = torch.cat(x_example)\n    x_example_erased = torch.cat(x_example_erased)\n    codes = torch.cat(codes)\n    print('Code shape', codes.shape, type(codes),'min',codes.min(),'max', codes.max())\n    print(np.unique(codes))\n    if len(codes) >= nb_examples**2:\n        codes = codes[:nb_examples**2]\n        x_example = x_example[:nb_examples**2]\n        x_example_erased = x_example_erased[:nb_examples**2]\n    vocab_size = vqvae.model.num_embeddings + 1\n    print(vocab_size,'vocab_size')\n    start_token = vqvae.model.num_embeddings\n    print(start_token,'start_token')\n    codes_ = codes.view(len(codes), -1)\n    print(codes_.shape)\n    codes_ = torch.cat([(torch.ones(len(codes_), 1).long() * start_token), codes_.long(),], dim=1)\n    \n    generator = Generator.load_from_checkpoint(path_generator)\n    generator = cudafy(generator)\n    generator.eval()\n    \n    input_ids = cudafy(codes_[:,:256//4])\n    result = generator.model.generate(input_ids, max_length=generator.hparams.max_length, do_sample=True)\n    result = result[:, 1:]\n    result = result.contiguous()\n    result = result.view(nb_examples**2, generator.hparams.height, generator.hparams.width)\n    result[result>=generator.hparams.vocab_size-1] = generator.hparams.vocab_size-2\n    with torch.no_grad():\n        generated = vqvae.reconstruct_from_code(result)\n\n        fig, axes = plt.subplots(1,4, figsize=(25,25))\n        img = make_grid(generated, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n        axes[0].imshow(cudafy.get(img).detach().cpu().numpy())\n        axes[0].set_title('Generated from the sampled codes')\n        vqvae.model.eval()\n        _, XR, _ = vqvae.model(generated)\n        img = make_grid(XR, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n        axes[1].imshow(cudafy.get(img).detach().cpu().numpy())\n        axes[1].set_title('Reconstructed from the generated images')\n        \n        img = make_grid(x_example, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n        axes[2].imshow(cudafy.get(img).detach().cpu().numpy())\n        axes[2].set_title('Original Image')\n        \n        img = make_grid(x_example_erased, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n        axes[3].imshow(cudafy.get(img).detach().cpu().numpy())\n        axes[3].set_title('Original Image with Cutout')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Start investigate for reconstruction/image inpainting\nCheck if image can be  well reconstructed if we provide only a subset of the code\n\n* Given Input test image  do random cutout\n* Encode the image using vqvae resulting in a code for each image\n* Use the code  and select a portion 25% of the code to generate the sample\n* Generate the image with the vqvae decoder resulting in a generated image by VQVAE\n* finally the generated image if passed again to the vqvae for a reconstruction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### On Defect images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generate_from_erased_images_GPT2(path_generator='/kaggle/working/out/generator/logs/version_0/checkpoints/epoch=199.ckpt', path_vqvae='/kaggle/working/lightning_logs/version_0/checkpoints/epoch=299.ckpt', nb_examples = 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### On Healthy images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random_generate_from_erased_images_GPT2(path_generator='/kaggle/working/out/generator/logs/version_0/checkpoints/epoch=199.ckpt', path_vqvae='/kaggle/working/lightning_logs/version_0/checkpoints/epoch=299.ckpt', nb_examples = 8,test_image=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}