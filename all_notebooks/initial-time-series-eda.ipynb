{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"47951884-9bcf-892b-6a43-b27306a36f7f"},"source":"# Kernel git revision history EDA\n\nSo here we are, **12 years** of git kernel history and changed files. This is the first notebook showing the basic properties of the dataset, including time series analysis and commit subject message analysis."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ebcd679-aa18-75ad-3026-ba379ffb00cf"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4524cd53-6a1c-63c5-eccc-1561f0366764"},"outputs":[],"source":"def clean_ts(df):\n    return df[(df['author_timestamp'] > 1104600000) & (df['author_timestamp'] < 1487807212)]\ndf = clean_ts(pd.read_csv('../input/linux_kernel_git_revlog.csv'))\ndf['author_dt'] = pd.to_datetime(df['author_timestamp'],unit='s')\ndf.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bdc4c035-0df0-5c64-6f3c-1bcc0f3b23bd"},"source":"We have now read the data and removed some outliers that were either too far in the past or future.\n\nFor first step, we will have a look at the file additions and deletions over time to get an overview of the activity of the linux kernel."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"599134e5-1089-ee74-93ac-38e1e2f49e83"},"outputs":[],"source":"time_df = df.groupby(['author_timestamp', 'author_dt'])[['n_additions', 'n_deletions']].agg(np.sum).reset_index().sort_values('author_timestamp', ascending=True)\ntime_df['diff'] = time_df['n_additions'] - time_df['n_deletions']\ntime_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a215782-75c2-40e1-c310-0ab505458254"},"outputs":[],"source":"t = pd.Series(time_df['diff'].values, index=time_df['author_dt'])\nt.plot(title='lines of code added', figsize=(12,8))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3614d247-b706-ad6c-8028-6b53cdb0d3af"},"source":"Also interesting could be to have a look at the number of commits and files changed over time."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14c697da-ff1c-4bca-b19e-00a21f4d4b7b"},"outputs":[],"source":"commits_over_time = df.groupby('author_dt')['commit_hash'].nunique().reset_index().sort_values('author_dt', ascending=True)\ncommits_series = pd.Series(commits_over_time['commit_hash'].values, index=commits_over_time['author_dt'])\ncommits_series.plot(title='number of commits on original time series', figsize=(12,8))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10211b84-21c5-9d4e-06c8-74071d36132c"},"outputs":[],"source":"commits_series.resample('M').mean().plot(title='number of commits on monthly resampled data', figsize=(12,8))"},{"cell_type":"markdown","metadata":{"_cell_guid":"2cc1fbc6-c6bb-e863-7d20-7389980a973c"},"source":"Now lets have a look at the number of files changed per commit over time, is there something interesting to see there?\n\nThere is definitely a few spikes of activies where a lot of files have been changed in one commit!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"132caeeb-1efb-7839-15e8-b7ddf160fc91"},"outputs":[],"source":"files_changed_per_commit = df.groupby(['author_dt', 'commit_hash'])['filename'].agg('count').reset_index().sort_values('author_dt', ascending=True)\nfiles_changed_per_commit = pd.Series(files_changed_per_commit['filename'].values, index=files_changed_per_commit['author_dt'])\nfiles_changed_per_commit.plot(title='number files changed per commit', figsize=(12,8))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3dca3146-fbf0-c4a1-8943-8cd04a2a7359"},"source":"# Changed files and their commit messages\n\nHere we will look at how the number of changed files per commit distributed and if we can learn something from the commit subjects about the changed files."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"557a14bc-24a2-e925-6970-6da1d34e2b26"},"outputs":[],"source":"# trim distribution, there are a few heavy outliers in the data as we saw above \nn_files_changed_per_commit = df.groupby('commit_hash')['filename'].agg('count')\nn_files_changed_per_commit = n_files_changed_per_commit[n_files_changed_per_commit < 20]\nsns.distplot(n_files_changed_per_commit, kde=False)\nplt.title('distribution of number of files changed per commit')\nplt.xlabel('number of changed files')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2680277-9462-2813-8f69-77ce8ca20b22"},"outputs":[],"source":"# trim distribution, there are a few heavy outliers in the data as we saw above \nadditions_per_commit = df.groupby('commit_hash')['n_additions'].agg(np.sum)\nadditions_per_commit = additions_per_commit[additions_per_commit < 100]\nsns.distplot(additions_per_commit)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dcda544f-a1cb-7324-02e7-0603038c0ada"},"source":"Now, let us transform the collection of commit subject messages into a vectorized representation.\n\nHashingVectorizer is doing that by tokenizing each commit message and deciding, by hashing, the integer index of the vector where the count of that token in that commit message will be stored. That, naturally, will produce more and more collisions as the vector dimensionality is decreased.\n\nFirst, we need to **deduplicate** commit subjects though. Normally we would do this by grouping by the commit hash and selecting any string compatible aggregation like *MAX* or *MIN* of subject to get ahold of the subject of each commit message.\nThis operation takes very long in pandas so we will assume that there is no subject that is exactly the same with any other subject for a differing commit."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"039269c7-6c06-ad4a-4693-c8a8e66282f2"},"outputs":[],"source":"from sklearn.feature_extraction.text import HashingVectorizer\n# we will consider each unique subject\nunique_subjects = np.sort(df['subject'].unique())\nprint(unique_subjects)\nprint(unique_subjects.shape)\n\n# now vectorize each subject\nhashed_subjects = HashingVectorizer(n_features=1024).fit_transform(unique_subjects)\nhashed_subjects"},{"cell_type":"markdown","metadata":{"_cell_guid":"0d54fc17-45ce-1657-1b62-5acceb365b77"},"source":"The hashed subjects are now a sparse matrix with 1024 columns, exactly the number of buckets that we allowed in the hashing vectorizer. We have a sparisty of approx **1 - 4399278 / (602739 * 1024) = 99,28%**, so although the matrix is very large is is not occupying a lot of memory for us.\n\nIt can be interesting to do inference on the **number lines added** by only looking at the commit message, let's prepare some data.\nWe do this by grouping the number of lines added in each commit into **five** bins to turn this into a multi-class classification problem."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa732f73-c6fd-7b32-5387-655df1b9f1b1"},"outputs":[],"source":"n_additions_per_subject = df.groupby('subject')['n_additions'].agg(np.sum).reset_index().sort_values('subject')\n\ndef bucketize(row):\n    if row.n_additions > 80:\n        return 'XXL'\n    elif row.n_additions <= 80 and row.n_additions > 60:\n        return 'XL'\n    elif row.n_additions <= 60 and row.n_additions > 40:\n        return 'L'\n    elif row.n_additions <= 40 and row.n_additions > 20:\n        return 'M'\n    elif row.n_additions < 20:\n        return 'S'\n\n#y = n_additions_per_subject.apply(bucketize, axis=1)\n\n#X = hashed_subjects\n#X.shape, y.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"92f59088-b54b-3cb6-ab7e-9bab93872267"},"source":"Now, that we have some data, let us look at the AUC ROCs of a few hashing vectorizer sizes. We will use multinomial logistic regression for this classification task."},{"cell_type":"markdown","metadata":{"_cell_guid":"097be5a8-57da-ebce-219d-efb8c7c918da"},"source":"# Stay tuned, will continue tomorrow."},{"cell_type":"markdown","metadata":{"_cell_guid":"a0c9019e-18ff-a4a7-a39d-a68db72493d2"},"source":"# Time zones and their activity share of the linux kernel project"},{"cell_type":"markdown","metadata":{"_cell_guid":"7edb8fa4-f3ef-17b0-6feb-d2c72d4d058d"},"source":"Most active timezones of authors by number of files changed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d4fc684-c9ab-80fd-d18c-d17122b3109a"},"outputs":[],"source":"files_changed_per_utc_offset = df.groupby('commit_utc_offset_hours')['filename'].agg('count').reset_index().sort_values('filename', ascending=False)\nsns.barplot(x = 'commit_utc_offset_hours', y = 'filename', data = files_changed_per_utc_offset)"},{"cell_type":"markdown","metadata":{"_cell_guid":"05351276-5971-7f9b-451d-0792d3d56d44"},"source":"Which timezones have the most active kernel authors?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c79ac092-3676-2fa4-74e7-9bcd236948ed"},"outputs":[],"source":"n_authors_by_offset = df.groupby('commit_utc_offset_hours')['author_id'].nunique().reset_index().sort_values('author_id', ascending=False)\nsns.barplot(x = 'commit_utc_offset_hours', y = 'author_id', data = n_authors_by_offset)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c4fce836-ff49-6065-1ab1-901b2d44d026"},"source":"The number of authors is strongly proportional to the number of files changed."},{"cell_type":"markdown","metadata":{"_cell_guid":"39800b7b-8a79-96d3-2e59-b6c5a125a58b"},"source":"Let's have a look at most common words used in commit subjects now. For that we will split each subject by space (0x20) and do a word count. To not exceed runtime of kernels, also subsample."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4bb801d-619c-fdaa-7e24-39e8b1f87727"},"outputs":[],"source":"from collections import Counter\nimport operator\n\nn_rows = 1e4\nsubject_words = []\nfor row_number, row in df.ix[0:n_rows].iterrows():\n    ws = row['subject'].split(\" \")\n    subject_words = subject_words + [w.lower() for w in ws]\n\nwords = []\ncounts = []\nfor word, count in sorted(Counter(subject_words).items(), key=operator.itemgetter(1), reverse=True):\n    words.append(word)\n    counts.append(count)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33267987-a31f-c23c-088e-8d72bef1bb6f"},"outputs":[],"source":"wcdf = pd.DataFrame({'word': words, 'count': counts})\nsns.barplot(y = 'word', x = 'count', data = wcdf[0:20])"},{"cell_type":"markdown","metadata":{"_cell_guid":"9fe06e9d-be8a-1251-ee38-8aea8cfb0318"},"source":"It's probably a good idea to remove stop words and redo the word counts, but first let's also havea look at a nice visualization of the words collection."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97345a22-3424-dc49-fc9f-a5c092728cd8"},"outputs":[],"source":"from wordcloud import WordCloud\n\nwordcloud = WordCloud().generate(\" \".join(subject_words))\n\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd5cf132-8696-050e-4b80-03d25ef1493b"},"source":"What would be interesting to know is whether there is a difference in length of commit subjects by UTC offset, let's have a look.\n\nWhat is deceiving in this plot, is the low confidence in the estimate for UTC offsets around +7, if you go back up, you can see that there are barely any commits from these timezones."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c83089c-ee93-3b10-d42c-584f243188be"},"outputs":[],"source":"df['subject_char_len'] = df['subject'].str.len()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ebbdb4c3-14e8-4981-ee92-961a89e621d1"},"outputs":[],"source":"df.groupby('commit_utc_offset_hours')['subject_char_len'].agg(np.mean).plot()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1546bc6d-4f76-d1a1-ee96-d4f4b32d1df9"},"outputs":[],"source":"df['commit_activity'] = df['n_additions'] + df['n_deletions']\ncmap = plt.get_cmap('viridis')\nsns.heatmap(df[['commit_utc_offset_hours', 'commit_activity', 'subject_char_len']].corr(), cmap=cmap)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1c0da13b-be6b-e880-b113-9a326f76eb2f"},"outputs":[],"source":"#sns.pairplot(df[['commit_utc_offset_hours', 'commit_activity', 'subject_char_len']])"},{"cell_type":"markdown","metadata":{"_cell_guid":"e3f24fca-24fb-07a1-443b-67aad7511d3c"},"source":"Distribution of length of subject words."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d60763e-708d-8387-f592-dd27f48f9d6f"},"outputs":[],"source":"sns.distplot(list(map(lambda w: len(w), words)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"de5a0f0f-b3e2-d64e-667a-6c065c49a337"},"source":"Do authors of the linux kernel use different length of words to describe their commits?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"285f414f-e1ec-31a2-4311-1cfe7f199386"},"outputs":[],"source":"n_rows = 1e4\nword_lengths = []\ntimezones = []\n\nfor row_number, row in df.ix[0:n_rows].iterrows():\n    ws = row['subject'].split(\" \")\n    word_lengths = word_lengths + list(map(lambda w: len(w), ws))\n    timezones = timezones + [row['commit_utc_offset_hours'] for x in range(len(ws))]\n\ntz_ws = pd.DataFrame({'tz': timezones, 'word_length': word_lengths})\ntz_ws.head(5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a949e732-dfdf-32dc-e7cb-abe32ef121ca"},"outputs":[],"source":"tz_ws.groupby('tz')['word_length'].agg(np.mean).plot()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a80461dc-c25a-ac21-b0bb-8c3d985e2bed"},"source":"Let's have a look at how many distinct words we have in the whole history of git with and without stop word removal."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"806764ff-de1b-c3a3-25ee-47256cb3074f"},"outputs":[],"source":"len(np.unique(subject_words))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c474811e-5322-84ca-3b7f-33f19f5a1b6e"},"outputs":[],"source":"from stop_words import get_stop_words\n\nstop_words = get_stop_words('english')\n\nfiltered_subject_words = [w for w in subject_words if w not in stop_words]\n\nlen(np.unique(filtered_subject_words))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"962f2b0d-af13-b9b6-ed03-19d16c7ab46d"},"outputs":[],"source":"words = []\ncounts = []\nfor word, count in sorted(Counter(subject_words).items(), key=operator.itemgetter(1), reverse=True):\n    if word in get_stop_words('english'):\n        continue\n    words.append(word)\n    counts.append(count)\n\nwcdf = pd.DataFrame({'word': words, 'count': counts})\nsns.barplot(y = 'word', x = 'count', data = wcdf[0:20])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5556417-e2be-de77-0eae-66c8dafb23bb"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"115f53f0-50ca-032d-823e-0c8315c1d109"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}