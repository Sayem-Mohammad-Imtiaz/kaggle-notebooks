{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\nfrom sklearn.model_selection import train_test_split\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    \n    files = []\n    \n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I don't feel like dealing with the NaNs in the unclean files\n# I want a 'pure' kNN\n\nclean_files = files[:3]\nclean_files.append(files[4])\nclean_files += files[6:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weak kNN attempt\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import normalize\nimport sklearn.metrics\nimport numpy\n\nfor f in clean_files:\n    \n        data = open(f, 'r')\n        df = pandas.read_csv(data)\n        print(df['tax'], df['price'])\n        df = pandas.get_dummies(df)\n        X = df.drop('price', axis=1)\n        y = df['price']\n        X = normalize(X)\n        \n        kNN = KNeighborsClassifier(n_neighbors=10)\n        kNN.fit(X, y)\n        predictions = []\n        k_nearest = kNN.kneighbors(X)\n        \n        preds = []\n        for indices in k_nearest[1]:\n            prices = []\n            for index in indices[1:]:\n                price = y[index]\n                prices.append((9 - len(prices)) * price)\n            preds.append(sum(prices)/45)  # prices' importance weighted by proximity\n        \n        print(f[-10:], sklearn.metrics.r2_score(y, preds))  # some are decent (.65ish), and others suck\n        print(sum(abs(y - numpy.array(preds)))/len(preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LinReg attempt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import normalize\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\ndef change_model(model, vc, n):\n    if vc[model] < .01 * n:\n        return ' Other'\n    else:\n        return model\n\navg_misses = []\nfor f in clean_files:\n    \n        data = open(f, 'r')\n        df = pandas.read_csv(data)\n        \n        #df['model'] = df['model'].apply(change_model, args=( df['model'].value_counts(), df.shape[0]))  # get rid of infrequent models (eventual feature reduction)\n\n        df = pandas.get_dummies(df)\n        try:\n            X = df.drop(['price'], axis=1)\n        except:\n            X = df.drop('price', axis=1)\n        y = df['price']\n        X = normalize(X)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n        \n        lin_reg = LinearRegression()\n        lin_reg.fit(X_train, y_train)\n        \n        preds = lin_reg.predict(X_test)\n        print(f[-10:], sklearn.metrics.r2_score(y_test, preds))  # some r2 values are whack (e.g. ford w/ -3.7 r2)\n        avg_misses.append(sum(abs(y_test - preds))/preds.shape[0])\nprint('costish:', sum(avg_misses)/len(avg_misses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# another kNN attempt, where I throw out some of the models before getting dummies (making a lot of one hot columns)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import normalize\nimport sklearn.metrics\nimport numpy\n\ndef change_model(model, vc, n):\n    if vc[model] < .01 * n:\n        return ' Other'\n    else:\n        return model\n\nfor f in clean_files:\n    \n        data = open(f, 'r')\n        df = pandas.read_csv(data)\n        \n        df['model'] = df['model'].apply(change_model, args=( df['model'].value_counts(), df.shape[0]))  # get rid of infrequent models (eventual feature reduction)\n        \n        df = pandas.get_dummies(df)\n        X = df.drop('price', axis=1)\n        y = df['price']\n        X = normalize(X)\n                \n        kNN = KNeighborsClassifier(n_neighbors=10)\n        kNN.fit(X, y)\n        predictions = []\n        k_nearest = kNN.kneighbors(X)\n        \n        preds = []\n        for indices in k_nearest[1]:\n            prices = []\n            for index in indices[1:]:\n                price = y[index]\n                prices.append((9 - len(prices)) * price)\n            preds.append(sum(prices)/45)  # prices' importance weighted by proximity\n        \n        print(f[-10:], sklearn.metrics.r2_score(y, preds))  # some are decent (.65ish), and others suck\n        print(sklearn.metrics.explained_variance_score(y, preds))\n        print(sum(abs(y - numpy.array(preds)))/len(preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}