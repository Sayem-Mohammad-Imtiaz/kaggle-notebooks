{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# set some display options:\nsns.set(style=\"whitegrid\")\npd.set_option(\"display.max_columns\", 36)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load datas\ntrain = pd.read_csv('/kaggle/input/banking-dataset-marketing-targets/train.csv')\ntest = pd.read_csv('/kaggle/input/banking-dataset-marketing-targets/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print train and test data shape\nprint('train set size is : ', train.shape)\nprint('test set size is : ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see the features of train set\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ID coluns have no value in classification so lets drop it.\n\ntrain.drop('ID', axis = 1, inplace = True)\ntest.drop('ID', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check infos of train set\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so **NaN** values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# subscribed feature is our target variabe wwe will convert it to numeric \n\ntrain['subscribed'].replace('no', 0 , inplace=True)\ntrain['subscribed'].replace('yes', 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical and Categorical features\nI will separate the data into Numerical and Categorical features."},{"metadata":{},"cell_type":"markdown","source":"### Numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# numerical features\nnumerical_features = [cols for cols in train.columns if train[cols].dtype != 'O']\nnumerical_features = train[numerical_features]\nnumerical_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation between independent features and dependent features\ncorr = train.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation in heatmap\nsns.heatmap(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features.hist(figsize = (15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical features\ncategorical_features = [cols for cols in train.columns if train[cols].dtype == 'O']\ncategorical_features = train[categorical_features]\ncategorical_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Lets see the unique values in Categorical_features:')\nprint('#########################################################\\n')\n\nfor feature in categorical_features:\n    print('The unique values in '+ feature + \" \" + 'feature are:' )\n    print(train[feature].unique())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Univariate analysis of categorical features\n\nThe figures or grapbs are self-explanatory so i will not be describing the graphs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the target features\ntrain.subscribed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train.subscribed)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above plot clearly shows the issues of **Class Imbalance.**\n\nThere are few ways to deal with such issues we will look at them later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of job feature\nsns.countplot(train.job)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of marital features\nsns.countplot(train.marital)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of marital feature\nsns.countplot(train.education)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of default feature\nsns.countplot(train.default)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of housing features\nsns.countplot(train.housing)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of loan features\nsns.countplot(train.loan)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of contact feature\nsns.countplot(train.contact)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of month feature\nsns.countplot(train.month)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Bivariate analysis of Categorical features with target features**.\n\nthe figures are self explanatory so i will not be describing them"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"marital\", hue=\"subscribed\", data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"education\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"job\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"housing\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"loan\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"default\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"contact\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"month\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"poutcome\", hue=\"subscribed\", data=train)\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Combining train set and test set\nWE wil combine the train and test set together and do necessary things like one **hot encoding, feature scaling** etc. The reason for combining train and test data is so that we dont need to repeat above task like one hot encoding etc for test data again."},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine train adn test data together\ndata = pd.concat([train,test], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking null values\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"we can see null values in subscribed feature. If you see carefully the number of null values is exactly equal to test data. This null values is because target variable is missing in test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets separate newly combined data into numerical and categorical features\n\nnumerical_features = [cols for cols in data.columns if data[cols].dtype != 'O']\ncategorical_features = [cols for cols in data.columns if data[cols].dtype == 'O']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[numerical_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[categorical_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get dummies of categorical variable. Its same as one hot encoding.\n# we do this one hot encoding of categorical variables because our ML algorithms only works with numeric\n\ncat_dummies = pd.get_dummies(data[categorical_features])\ncat_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat numerical features and one hot encoded features fromm above\n\nnewdata = pd.concat([data[numerical_features], cat_dummies], axis = 1)\nnewdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Feature scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create feature and target vectors\n\nfeatures = newdata.drop('subscribed', axis = 1)\ntarget = newdata.subscribed\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape, target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = features.columns # columns of features ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaling is done here\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nfeatures = scaler.fit_transform(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After scaling features above it is changed into ndarray. so we will change it back to Dataframe\nfeatures = pd.DataFrame(features, columns = [cols])\nprint(features.shape)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets add subscribed to our features \nfeatures['subscribed'] = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Now i will separate the train and test data which we we cobined above.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate traina dnd test data\n\ntrain2 = features.iloc[:31647]\ntest2 = features.iloc[31647:]\ntrain2.shape, test2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop target feature from test data\ntest2.drop('subscribed', axis = 1, inplace = True)\nprint(test2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Model building and Evaluation\nI will build classification models for our data and perform evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Define feature and target vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define feature and target vectors\nx = train2.iloc[:,:-1]\ny = train2.iloc[:,-1]\nx.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train and test set\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model creation\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict result for x_test\n\ny_pred = logreg.predict(x_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check model accuracy\n\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check training set accuracy\n\ny_pred_train = logreg.predict(x_train)\ny_pred_train\n\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(logreg.score(x_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(logreg.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting target class for unseen test data.\n# this data is test.csv given by kaggle\n\ny_pred_test2 = logreg.predict(test2)\ny_pred_test2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_y_pred_test2 = pd.DataFrame(y_pred_test2, columns = ['test2pred'])\ndf_y_pred_test2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_y_pred_test2['test2pred'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_y_pred_test2['test2pred'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix and classification report\n\ncm = confusion_matrix(y_test,y_pred)\nprint('confusion matrix: \\n ', cm)\nprint('\\n')\nprint('classification report: \\n ', classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Holyshit!!! Our accuracy is **0.90** but look at the recall value of minority calss 1, it just only **0.32**.\n\nAll the best accuracy score that we got above turns out to be **LIES**. You can also see the prediction in unseen test data **test2**(given by kaggle) gives much priority to majority class **0** \n\nWhy so? These all is due to **class imbalance** "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Over-sampling of minority class observation\n\nIt means we will randomly replicate minority class observation until it becomes actionablely proportional to majority class observation."},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\n\n# separate the maority and minority class observation\ndata_major = train2[y == 0.0]\ndata_minor = train2[y == 1.0]\n\n# over-sample the minority class observations\ndata_minor_oversample = resample(data_minor, replace = True, n_samples=27932, random_state = 0)\n\n# finally combine the majority class observation and oversampled minoiry class observation\ndata_oversampled = pd.concat([data_major, data_minor_oversample])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class label count after oversampled.we will see that minoity class now is proportionate to majority class\ndata_oversampled.iloc[:,-1].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# again lets splt our over samoled data into feature and traget variables\nX = data_oversampled.iloc[:,:-1]\nY = data_oversampled.iloc[:,-1]\n\n# lets split data into train and test set\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)\n\n# model building\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\n\n# Lets evaluate our model\ny_pred_train = logreg.predict(x_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\nprint(\"Accuracy score: \", accuracy_score(y_test,y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))\nprint('\\n')\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here our accuracy comes out be **0.83** which is less than previous accuracy score but this model have one good thing and thats recall value of minority class 1 which is **0.81** more improved than previous model whose recall value was **0.32**.\n\nThis model shows a slight case of **overfitting** but the train and test set accuracies are almost comparable. Even than if we want to reduce this overfitting we can **tune hyperparameter** which i will not be doing here"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### I have stopped my notebook here but we can still test our data on other classification algorithms. There are many more techniques with wich we can improve our model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}