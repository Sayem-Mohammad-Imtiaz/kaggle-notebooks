{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport torch.utils.data as Data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pylab as pl\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"samples       = ['ggH125_ZZ4lep','llll'] # datafiles for input\nepochs        = 10                       # number of training epochs\nbatch_size    = 32                       # number of samples per batch\ninput_size    = 2                        # The number of features\nnum_classes   = 2                        # The number of output classes\nhidden_size   = 10                       # The number of perceptrons in the hidden layer\nlearning_rate = 1e-3                     # The speed of convergence\nverbose       = True                     # flag for printing out stats at each epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_cuda = torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if use_cuda:\n    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n    print('__Number CUDA Devices:', torch.cuda.device_count())\n    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if use_cuda else \"cpu\")\nkwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\nprint(\"Device: \", device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DataFrames = {} # define empty dictionary to hold dataframes\nfor s in samples: # loop over samples\n    DataFrames[s] = pd.read_csv('/kaggle/input/4lepton/'+s+\".csv\") # read .csv file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cut on lepton charge\ndef cut_lep_charge(lep_charge_0,lep_charge_1,lep_charge_2,lep_charge_3):\n# only want to keep events where sum of lepton charges is 0\n    sum_lep_charge = lep_charge_0 + lep_charge_1 + lep_charge_2 + lep_charge_3\n    if sum_lep_charge==0: return True\n    else: return False\n\n# apply cut on lepton charge\nfor s in samples:\n    # cut on lepton charge using the function cut_lep_charge defined above\n    DataFrames[s] = DataFrames[s][ np.vectorize(cut_lep_charge)(DataFrames[s].lep_charge_0,\n                                                    \t    DataFrames[s].lep_charge_1,\n                                                    \t    DataFrames[s].lep_charge_2,\n                                                    \t    DataFrames[s].lep_charge_3) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ML_inputs = ['lep_pt_1','lep_pt_2'] # list of features for ML model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_MC = [] # define empty list that will contain all features for the MC\n\nfor s in samples: # loop over the different samples\n    if s!='data': # only MC should pass this\n        all_MC.append(DataFrames[s][ML_inputs]) # append the MC dataframe to the list containing all MC features\n        \nX = np.concatenate(all_MC) # concatenate the list of MC dataframes into a single 2D array of features, called X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_y = [] # define empty list that will contain labels whether an event in signal or background\n\nfor s in samples: # loop over the different samples\n    if s!='data': # only MC should pass this\n        if 'H125' in s: # only signal MC should pass this\n            all_y.append(np.ones(DataFrames[s].shape[0])) # signal events are labelled with 1\n        else: # only background MC should pass this\n            all_y.append(np.zeros(DataFrames[s].shape[0])) # background events are labelled 0\n            \ny = np.concatenate(all_y) # concatenate the list of lables into a single 1D array of labels, called y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# make train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                  test_size=0.33, \n                                                  random_state=492 ) # set the random seed for reproducibility","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler() # initialise StandardScaler\n\nscaler.fit(X_train) # Fit only to the training data\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX = scaler.transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train  = torch.tensor(X_train, dtype=torch.float)\ny_train  = torch.tensor(y_train, dtype=torch.long)\n\nX_train, y_train = Variable(X_train), Variable(y_train)\n\nx_valid, y_valid = X_train[:100], y_train[:100]\nx_train_nn, y_train_nn = X_train[100:], y_train[100:]\n\ntrain_data = Data.TensorDataset(x_train_nn, y_train_nn)\nvalid_data = Data.TensorDataset(x_valid, y_valid)\n\ntrain_loader = Data.DataLoader(dataset=train_data,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               **kwargs)\n\nvalid_loader = Data.DataLoader(dataset=valid_data,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier_MLP(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim):\n        super().__init__()\n        \n        self.h1  = nn.Linear(in_dim, hidden_dim)\n        self.h2  = nn.Linear(hidden_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, out_dim)\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        \n        x = F.relu(self.h1(x))\n        x = F.relu(self.h2(x))\n        x = self.out(x)\n        \n        return x, F.softmax(x, dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Classifier_MLP(in_dim=input_size, hidden_dim=hidden_size, out_dim=num_classes)\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)\n\nstart = time.time()\n\n_results = []\nfor epoch in range(epochs):  # loop over the dataset multiple times\n\n    # training loop for this epoch\n    model.train() # set the model into training mode\n    \n    train_loss = 0.\n    for batch, (x_train, y_train) in enumerate(train_loader):\n        \n        x_train, y_train = x_train.to(device), y_train.to(device)\n        \n        model.zero_grad()\n        out, prob = model(x_train)\n        \n        loss = F.cross_entropy(out, y_train)\n        \n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * x_train.size(0)\n    \n    train_loss/= len(train_loader.dataset)\n\n    if verbose:\n        print('Epoch: {}, Train Loss: {:4f}'.format(epoch, train_loss))\n\n    # validation loop for this epoch:\n    model.eval() # set the model into evaluation mode\n    with torch.no_grad():  # turn off the gradient calculations\n        \n        correct = 0.; valid_loss = 0.\n        for i, (x_valid, y_valid) in enumerate(valid_loader):\n            \n            x_valid, y_valid = x_valid.to(device), y_valid.to(device)\n            \n            out, prob = model(x_valid)\n            loss = F.cross_entropy(out, y_valid)\n            \n            valid_loss += loss.item() * x_valid.size(0)\n            \n            preds = prob.argmax(dim=1, keepdim=True)\n            correct += preds.eq(y_valid.view_as(preds)).sum().item()\n            \n        valid_loss /= len(valid_loader.dataset)\n        accuracy = correct / len(valid_loader.dataset)\n\n    if verbose:\n        print('Validation Loss: {:4f}, Validation Accuracy: {:4f}'.format(valid_loss, accuracy))\n\n    # create output row:\n    _results.append([epoch, train_loss, valid_loss, accuracy])\n\nresults = np.array(_results)\nprint('Finished Training')\nprint(\"Final validation error: \",100.*(1 - accuracy),\"%\")\n\nif use_cuda: torch.cuda.synchronize() \nend = time.time()\nprint(\"Run time [s]: \",end-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pl.subplot(111)\npl.plot(results[:,0],results[:,1], label=\"training\")\npl.plot(results[:,0],results[:,2], label=\"validation\")\npl.xlabel(\"Epoch\")\npl.ylabel(\"Loss\")\npl.legend()\npl.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}