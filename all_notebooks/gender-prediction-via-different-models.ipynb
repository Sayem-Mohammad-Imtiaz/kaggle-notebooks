{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd\nfrom pylab import rcParams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/students-performance-in-exams/StudentsPerformance.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to check presence of missing observations\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to print unique values in all columns\nunique_columns = [col+\" \"+df[col].unique() for col in df.select_dtypes(exclude=np.number)]\nunique_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a column Gender1 - where it assumes value 1 when gender = female\n\ngender1 = [1 if each == \"female\" else 0 for each in df.gender]\n\ndf['Gender1']=gender1\n\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation b/w variables\n\ncorr = df.corr()\nprint(corr)\n\nsns.heatmap(corr, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting densities for female vs male basis score in various subjects\nf=df[['math score','reading score','writing score','Gender1']]\ndef plot_densities(data):\n    '''\n    Plot features densities depending on the outcome values\n    '''\n    # change fig size to fit all subplots\n    rcParams['figure.figsize'] = 20, 7\n    fig, axs = plt.subplots(3, 1)\n    plt.subplots_adjust(left = 0.25, right = 0.9, bottom = 0.1, top = 0.95,\n                        wspace = 0.2, hspace = 0.9)\n\n    # plot densities\n    for column_name in names[:-1]: \n        ax = axs[names.index(column_name)]\n        data[data['Gender1'] == 0][column_name].plot(kind='density', ax=ax, subplots=True, \n                                    sharex=False, color=\"red\", legend=True,\n                                    label=column_name + ' for Male')\n        data[data['Gender1'] == 1][column_name].plot(kind='density', ax=ax, subplots=True, \n                                     sharex=False, color=\"green\", legend=True,\n                                     label=column_name + ' for Female')\n        ax.set_xlabel(column_name + ' values')\n        ax.set_title(column_name + ' density')\n        ax.grid('on')\n    plt.show()\n\nnames = list(f.columns)\n\n# plot correlation & densities\nplot_densities(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of test preparation\ndf['average score']=df[['math score','reading score','writing score']].mean(axis=1)\nprint(df.std())\n\n\n\nsns.catplot(x=\"average score\", y=\"test preparation course\", hue=\"gender\",\n            kind=\"violin\", inner=\"stick\", split=True,\n            palette=\"pastel\", data=df)\n\nsns.catplot(x=\"average score\", y=\"parental level of education\", hue=\"gender\",\n            kind=\"violin\", inner=\"stick\", split=True,\n            palette=\"pastel\", data=df)\nsns.catplot(x=\"average score\", y=\"lunch\", hue=\"gender\",\n            kind=\"violin\", inner=\"stick\", split=True,\n            palette=\"pastel\", data=df)\nsns.catplot(x=\"average score\", y=\"race/ethnicity\", hue=\"gender\",\n            kind=\"violin\", inner=\"stick\", split=True,\n            palette=\"pastel\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.pairplot(f,hue = 'Gender1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Dummy Values\ndummy = pd.get_dummies(df[['race/ethnicity','lunch','test preparation course']])\ndummy.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate dummy df with original df\ndf1 = pd.concat([df, dummy], axis = 1)\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average of values by gender\ndf_group = df1.groupby('gender').mean()\ndf_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns for which dummies are created and y variables\nx = df1.drop(['race/ethnicity', 'lunch', 'test preparation course', 'parental level of education','gender','average score','Gender1'], axis = 1)\n\n#standardize data\nx= (x-np.min(x)) / (np.max(x)-np.min(x))\n\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['Gender1']\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train / test data\nfrom sklearn.model_selection import train_test_split\nx_train , x_test , y_train, y_test = train_test_split(x,y,test_size = 0.2 , random_state = 21)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = DecisionTreeClassifier(criterion = 'gini', random_state= 0,max_depth=10 )\nclassifier.fit(x_train, y_train)\ny_pre = classifier.predict(x_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\ncm = confusion_matrix(y_test, y_pre)\nsum1 = np.sum(cm, axis=1, keepdims=True)\nperc1 = cm / sum1.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\n\nnrows, ncols = cm.shape\nfor i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = perc1[i, j]\n                if i == j:\n                    s = sum1[i]\n                    annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n\n\nsns.heatmap(cm, annot=annot, fmt='')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\ntext_representation = tree.export_text(classifier)\nprint(text_representation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"decistion_tree.log\", \"w\") as fout:\n    fout.write(text_representation)\nfig = plt.figure(figsize=(40,20))\n_ = tree.plot_tree(classifier, filled=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error = []\nfor i in range(2,20):\n \n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit(x_train,y_train)\n pred = knn.predict(x_test)\n error.append(np.mean(pred != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(2,20),error,color='blue', linestyle='dashed', marker='o',\n markerfacecolor='red', markersize=10)\nplt.title('Error vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error')\n\n#optimal neighbour count=7\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train,y_train)\ny_pre = knn.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pre)\nsum1 = np.sum(cm, axis=1, keepdims=True)\nperc1 = cm / sum1.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\n\nnrows, ncols = cm.shape\nfor i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = perc1[i, j]\n                if i == j:\n                    s = sum1[i]\n                    annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n\n\nsns.heatmap(cm, annot=annot, fmt='')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pre))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"error=[]\nfrom sklearn.ensemble import RandomForestClassifier\n\nfor i in range(1,100):\n rfc = RandomForestClassifier(n_estimators = i,random_state = 21,bootstrap = \"False\",criterion=\"gini\",min_samples_split = 10 , min_samples_leaf = 2)\n rfc.fit(x_train,y_train)\n pred = rfc.predict(x_test)\n error.append(np.mean(pred != y_test))\nplt.figure(figsize=(10,6))\nplt.plot(range(1,100),error,color='blue', linestyle='dashed', marker='o',\n markerfacecolor='red', markersize=10)\nplt.title('Error vs. No of Trees')\nplt.xlabel('No of Trees')\nplt.ylabel('Error')\n\nrfc = RandomForestClassifier(n_estimators = 50,random_state = 21,bootstrap = \"False\",criterion=\"gini\",min_samples_split = 10 , min_samples_leaf = 2)\nrfc.fit(x_train,y_train)\ny_pre = rfc.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_pre)\nsum1 = np.sum(cm, axis=1, keepdims=True)\nperc1 = cm / sum1.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\n\nnrows, ncols = cm.shape\nfor i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = perc1[i, j]\n                if i == j:\n                    s = sum1[i]\n                    annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n\n\nsns.heatmap(cm, annot=annot, fmt='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pre))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}