{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist\nnp.random.seed(2)\n\nmeans = [[2, 2], [4, 2]]\ncov = [[.3, .2], [.2, .3]]\nN = 10\nX0 = np.random.multivariate_normal(means[0], cov, N).T\nX1 = np.random.multivariate_normal(means[1], cov, N).T\n\nX = np.concatenate((X0, X1), axis = 1)\ny = np.concatenate((np.ones((1, N)), -1*np.ones((1, N))), axis = 1)\n# Xbar \nX = np.concatenate((np.ones((1, 2*N)), X), axis = 0)\n\nN = X.shape[1]\nd = X.shape[0]\nmix_id = np.random.permutation(N)\nprint(X,y)\nxi = X[:, 0].reshape(d, 1)\nyi = y[0, 0]\nprint(xi,yi, xi*yi)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nimport math\nfig = plt.figure(figsize=(12, 7))\nax = fig.gca(projection='3d')\nplt.title('', fontsize=16)\n\ndef h(w, x):    \n    return np.sign(np.dot(w.T, x))\n\ndef has_converged(X, y, w):    \n    return np.array_equal(h(w, X), y) \ndef perceptron(X, y, w_init):\n    w = [w_init]\n    N = X.shape[1]\n    d = X.shape[0]\n    mis_points = []\n    dem = 0\n    while dem < 1000:\n        # mix data \n        mix_id = np.random.permutation(N)\n        for i in range(N):\n            xi = X[:, mix_id[i]].reshape(d, 1)\n            yi = y[0, mix_id[i]]\n            if h(w[-1], xi)[0] != yi: # misclassified point\n                mis_points.append(mix_id[i])\n                w_new = w[-1] + yi*xi \n                w.append(w_new)      \n        if has_converged(X, y, w[-1]):\n            break\n        dem += 1\n    return (w, mis_points)\n\nd = X.shape[0]\nw_init = np.random.randn(d, 1)\n(w, m) = perceptron(X, y, w_init)\nprint(w[-1])\n\nax.scatter(X.T[0:10,1],X.T[0:10,2],y.T[0:10,0],marker=\"o\",color='r')\nax.scatter(X.T[11:20,1],X.T[11:20,2],y.T[11:20,0],marker=\"o\",color='g')\n\nww = w[-1].T\n# w0, w1, w2 = ww[0,0], ww[0,1], ww[0,2]\n# x11, x12 = -100, 100\n# plt.plot([x11, x12], [-(w1*x11 + w0)/w2, -(w1*x12 + w0)/w2], 'k')\nplt.plot(ww[0] * X.T[0], ww[0] * X.T[19], marker=\"o\", color=\"blue\")  \nax.set_xlabel('θ_zero')\nax.set_ylabel('θ_one')\nax.set_zlabel('θ_two')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 7))\nax = fig.gca(projection='3d')\nplt.title('Stochastic Gradient Descent', fontsize=16)\ntheta = w_init\n\nyy = y.T\nXX= X.T\nm = XX.shape[0]\neta = 2e-8\nerr = 1\nerror = 2\nwhile (abs(error - err) > 0.0005):\n    error = err;\n    gradients = (1/m) * XX.T.dot(XX.dot(theta) - yy)\n    theta = theta - eta * gradients\n    err = 1/m * math.sqrt((XX.dot(theta)-yy).T.dot(XX.dot(theta)-yy))\n\nprint(\"MSE:\", err) \nprint(theta)\n\nax.scatter(X.T[0:10,1],X.T[0:10,2],y.T[0:10,0],marker=\"o\",color='r')\nax.scatter(X.T[11:20,1],X.T[11:20,2],y.T[11:20,0],marker=\"o\",color='g')\n\nplt.plot(theta * X.T[0], theta * X.T[19], marker=\"o\", color=\"blue\")  \nax.set_xlabel('θ_zero')\nax.set_ylabel('θ_one')\nax.set_zlabel('θ_two')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array([[0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25, 2.50, \n              2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75, 5.00, 5.50]])\ny = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\nX = np.concatenate((np.ones((1, X.shape[1])), X), axis = 0)\ndef sigmoid(s):\n    return 1/(1 + np.exp(-s))\n\ndef logistic_sigmoid_regression(X, y, w_init, eta, tol = 1e-4, max_count = 10000):\n    w = [w_init]    \n    it = 0\n    N = X.shape[1]\n    d = X.shape[0]\n    count = 0\n    check_w_after = 20\n    while count < max_count:\n        # mix data \n        mix_id = np.random.permutation(N)\n        for i in mix_id:\n            xi = X[:, i].reshape(d, 1)\n            yi = y[i]\n            zi = sigmoid(np.dot(w[-1].T, xi))\n            w_new = w[-1] + eta*(yi - zi)*xi\n            count += 1\n            # stopping criteria\n            if count%check_w_after == 0:                \n                if np.linalg.norm(w_new - w[-check_w_after]) < tol:\n                    return w\n            w.append(w_new)\n    return w\neta = .05 \nd = X.shape[0]\nw_init = np.random.randn(d, 1)\n\nw = logistic_sigmoid_regression(X, y, w_init, eta)\nprint(w[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sigmoid(np.dot(w[-1].T, X)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}