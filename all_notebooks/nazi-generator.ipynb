{"cells":[{"metadata":{},"cell_type":"markdown","source":"**QUALITY DATA IS TOUGH TO BE FOUND.** \n\nYes it is tough to be found and no one wants to spend days trying to read research papers in this ordeal of trying to find that perfect dataset. The perfect publicly available dataset DOES NOT EXIST. \nSure you could be doing hours of feature engineering, but wait, we can generate our own dataset. YES. THAT IS THE PURPOSE."},{"metadata":{},"cell_type":"markdown","source":"(Upvote this or the Chinese Government will take away my access to the internet)"},{"metadata":{},"cell_type":"markdown","source":"> #### So what? Should I waste my money to use AWS or Azure's annotation services and get my already shady data more shady labels?\nAnswer: **NO.**"},{"metadata":{},"cell_type":"markdown","source":"So how do I plan to solve this problem?\n## Dumb Idea: Use Text Generator to Generate more data to feed into a classifier."},{"metadata":{},"cell_type":"markdown","source":"# Importing Stuff"},{"metadata":{},"cell_type":"markdown","source":"### So, what are the three main libraries required here?\nThe libraries required to using a hugging face transformer would be: the `Transformers`, `FastAI`, `Pandas` and `PyTorch` Libraries."},{"metadata":{"id":"FzWuMERNRP82","outputId":"6f068a90-19c0-4c6e-e711-f2b4090f8346","trusted":true,"collapsed":true},"cell_type":"code","source":"# Installing:\n!pip install -Uqq fastai\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"id":"J2SoFpT2RP9E","trusted":true},"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\nimport torch\nfrom fastai.text.all import *\n\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"id":"OlhL-nP7RP9F","outputId":"dbf93065-0339-4fe7-e8a3-da539b5c762d","trusted":true,"collapsed":true},"cell_type":"code","source":"# Importing the model and the tokenizer:\n\npretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)","execution_count":null,"outputs":[]},{"metadata":{"id":"tyBFGWn1RP9F"},"cell_type":"markdown","source":"Before we move on to the fine-tuning part, let's have a look at this `tokenizer` and this `model`. The tokenizers in `HuggingFace` usually do the tokenization and the numericalization in one step (we ignore the padding warning for now):"},{"metadata":{},"cell_type":"markdown","source":"# Playing Around with the Tokenizer üèÉüèΩ‚Äç‚ôÄÔ∏è\n"},{"metadata":{"id":"rKOsLh1IRP9G","outputId":"d868832b-f0e9-4fb5-c694-2d7fdd4ae0a2","trusted":true},"cell_type":"code","source":"# Encoding a sentence and checking it out:\n\nids = tokenizer.encode('This is an example of text, and')\nids","execution_count":null,"outputs":[]},{"metadata":{"id":"nyoYXKIgRP9I","outputId":"28f0c2e7-616c-403c-a024-d8e68a618f03","trusted":true},"cell_type":"code","source":"# Decoding the same bad boy: \n\ntokenizer.decode(ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing üë©‚Äçüíª"},{"metadata":{"id":"_SzEaoZdRP9L"},"cell_type":"markdown","source":"Let's have a look at what those `csv` files look like:"},{"metadata":{"id":"9VxnEdSKRP9L","outputId":"f14495bb-08c7-49d7-8eff-9d7339dc23d6","trusted":true},"cell_type":"code","source":"# Reading the training CSV:\ndf_train = pd.read_csv(\"../input/nazidataset/nazitweets.csv\", header=None, lineterminator='\\n')\n\n# Removing the NaN rows.\ndf_train = df_train.dropna()\n\n# Taking a look at it:\ndf_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"0H3SEfDnRP9L","trusted":true},"cell_type":"code","source":"# Converting all the texts into a numpy array:\nall_texts = np.array([df_train[1].values])","execution_count":null,"outputs":[]},{"metadata":{"id":"qXZIpI57RP9N","trusted":true},"cell_type":"code","source":"# (Stolen Code Alert)\nclass TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        toks = self.tokenizer.tokenize(x)\n        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))","execution_count":null,"outputs":[]},{"metadata":{"id":"CVww7S8DRP9N","outputId":"13be3ae7-dd56-49b8-9f59-3ab372555b76","trusted":true},"cell_type":"code","source":"# Defining the splits for the dataloader:\nsplits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]\n\n# Defining the Transformed Lists:\ntls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)","execution_count":null,"outputs":[]},{"metadata":{"id":"Gt__p0LkRP9O"},"cell_type":"markdown","source":"We specify `dl_type=LMDataLoader` for when we will convert this `TfmdLists` to `DataLoaders`: we will use an `LMDataLoader` since we have a language modeling problem, not the usual fastai `TfmdDL`."},{"metadata":{"id":"5tPCeinlRP9U","outputId":"3c4d866f-bf5c-40ad-d25a-0dd16189d648","trusted":true},"cell_type":"code","source":"show_at(tls.train, 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"n3Q6NlzZRP9U"},"cell_type":"markdown","source":"The fastai library expects the data to be assembled in a `DataLoaders` object (something that has a training and validation dataloader). We can get one by using the `dataloaders` method. We just have to specify a batch size and a sequence length. Since the GPT2 model was trained with sequences of size 1024, **we will not use this sequence length (it's a stateless model, so it will mess the perplexity if we use less, I will do that nonetheless)**:"},{"metadata":{"id":"OpVVe18URP9U","trusted":true},"cell_type":"code","source":"# bs refers to the Batch Size while sl refers to the sequence length:\nbs,sl = 4,240 \n\n# Defining the Dataloader:\ndls = tls.dataloaders(bs=bs, seq_len=sl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a final look at the data:"},{"metadata":{"id":"1VM9MvQwRP9V","outputId":"ead18129-2ab7-4dce-ec39-cb4e56ca7768","trusted":true},"cell_type":"code","source":"dls.show_batch(max_n=2)","execution_count":null,"outputs":[]},{"metadata":{"id":"AZZXEnxLRP9V"},"cell_type":"markdown","source":"Another way to gather the data is to preprocess the texts once and for all and only use the transform to decode the tensors to texts:"},{"metadata":{"id":"woG6HvVcRP9W","outputId":"e14fb2cb-983a-4784-9615-02f5b0bc9dda","trusted":true},"cell_type":"code","source":"# Defining the function:\ndef tokenize(text):\n    toks = tokenizer.tokenize(text)\n    return tensor(tokenizer.convert_tokens_to_ids(toks))\n\n# Actually Tokenizing everything:\ntokenized = [tokenize(t) for t in progress_bar(all_texts)]","execution_count":null,"outputs":[]},{"metadata":{"id":"WLodrg_1RP9X"},"cell_type":"markdown","source":"(Stolen Code Alert) Now we change the previous `Tokenizer` like this:"},{"metadata":{"id":"WZFZtoZBRP9X","trusted":true},"cell_type":"code","source":"class TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        return x if isinstance(x, Tensor) else tokenize(x)\n        \n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))","execution_count":null,"outputs":[]},{"metadata":{"id":"rj_kp4WeRP9X","trusted":true},"cell_type":"code","source":"# Getting the dataloader:\n\ntls = TfmdLists(tokenized, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\ndls = tls.dataloaders(bs=bs, seq_len=sl)","execution_count":null,"outputs":[]},{"metadata":{"id":"cCDoJkTERP9X"},"cell_type":"markdown","source":"And we can check it still works properly for showing purposes:"},{"metadata":{"id":"zUu4n2h6RP9X","outputId":"47ac476e-2db3-45ca-d29f-0897fff981f0","trusted":true},"cell_type":"code","source":"# Hey again, my old friend.\n\ndls.show_batch(max_n=2)","execution_count":null,"outputs":[]},{"metadata":{"id":"2CAg3Ni5RP9Y"},"cell_type":"markdown","source":"# Fine-tuning the model ü§πüèΩ‚Äç‚ôÄÔ∏è"},{"metadata":{"id":"kcqU6qU_RP9Y"},"cell_type":"markdown","source":"The HuggingFace model will return a tuple in outputs, with the actual predictions and some additional activations (should we want to use them in some regularization scheme). To work inside the fastai training loop, we will need to drop those using a `Callback`: we use those to alter the behavior of the training loop.\n\nHere we need to write the event `after_pred` and replace `self.learn.pred` (which contains the predictions that will be passed to the loss function) by just its first element. In callbacks, there is a shortcut that lets you access any of the underlying `Learner` attributes so we can write `self.pred[0]` instead of `self.learn.pred[0]`. That shortcut only works for read access, not write, so we have to write `self.learn.pred` on the right side (otherwise we would set a `pred` attribute in the `Callback`)."},{"metadata":{"id":"e19gVFHGRP9Y","trusted":true},"cell_type":"code","source":"# The DropOutput Callback:\n\nclass DropOutput(Callback):\n    def after_pred(self): self.learn.pred = self.pred[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"LRTEj3u8RP9Y"},"cell_type":"markdown","source":"Now, we are ready to create our `Learner`, which is a fastai object grouping data, model and loss function and handles model training or inference. Since we are in a language model setting, we pass perplexity as a metric, and we need to use the callback we just defined. Lastly, we use mixed precision to save every bit of memory we can (and if you have a modern GPU, it will also make training faster):\n\nI genuinely have no clue how bad the perplexity is going to get since I literally passed the Sequence Length like 25% of the one they used while training the GPT2."},{"metadata":{"id":"qGiQIDhNRP9Y","trusted":true},"cell_type":"code","source":"learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"id":"EhWyFJX0RP9Z"},"cell_type":"markdown","source":"Checking if the current configuration allow the model to actually work:"},{"metadata":{"id":"YMr8vuK6RP9Z","outputId":"79c20d69-9742-4c57-c24b-b0b657b0441b","trusted":true},"cell_type":"code","source":"learn.validate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes it is **working**, time to find the Learning Rate."},{"metadata":{},"cell_type":"markdown","source":"## Learning Rate"},{"metadata":{},"cell_type":"markdown","source":"The `lr_find()` will find you the best suited learning rate."},{"metadata":{"id":"AnNg5LnSRP9Z","outputId":"36c12db8-2385-4896-c044-894fe857726e","trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting the Model ü§πüèΩ‚Äç‚ôÄÔ∏è"},{"metadata":{"id":"fIz6fB-TRP9a","outputId":"b822247b-150c-432c-c533-eb1b07f1fc64","trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 1e-5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen above, pretty aweful loss and even awefully good perplexity."},{"metadata":{},"cell_type":"markdown","source":"# Generating some Stuff üòè"},{"metadata":{},"cell_type":"markdown","source":"Defining a function to generate a sentence:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(prompt, length, beams, temp):\n    prompt_ids = tokenizer.encode(prompt)\n    inp = tensor(prompt_ids)[None].cuda()\n    preds = learn.model.generate(inp, max_length=length, num_beams=beams, temperature=temp)\n    return tokenizer.decode(preds[0].cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally trying the model out ü§î"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(\"immigrants are bad for\", 10, 5, 1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(\"i hate it that these brown dudes have\", 13, 5, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like it is kinda working, not that well BUT it is kinda working."},{"metadata":{},"cell_type":"markdown","source":"# If you liked what I am doing here: UPVOTE!\n\nIf you don't the chinese government will take away my access to this laptop and I will be left alone in this basement, please save me."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}