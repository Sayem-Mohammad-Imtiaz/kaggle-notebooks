{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom sklearn.metrics import accuracy_score\nimport json\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder, scale\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import r2_score\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom pandas.plotting import scatter_matrix\n\nimport torch.utils.data\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv(\"../input/train.csv\", keep_default_na=False)\ntest_csv = pd.read_csv(\"../input/test.csv\", keep_default_na=False)\n\n# train = train[0:30]\nprint(train_csv.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convertKgtoL(x):\n    if x != \"\":\n        l = x.split(\" \") \n        if l[1] and l[1] == \"km/kg\":\n            return float(l[0]) / 0.42\n        return l[0]\n    return x\n\ndef convertCrtoLak(x):\n    if x != \"\":\n        l = x.split(\" \") \n        if l[1] and l[1] == \"Cr\":\n            return float(l[0]) * 100\n        return l[0]\n    return x\n\ndef convertToBrand(x):\n    if x != \"\":\n        l = x.split(\" \")\n        return l[0].upper()\n    return x\n\ndef convertYearDiff(x):\n    if x != \"\":\n        return 2019 - int(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(dataset):\n    #dataset['Power'] = dataset[\"Power\"].replace(\"bhp\", \"\", regex=True).replace(\"null\", \"\", regex=True)\n    #dataset['Power'] = pd.to_numeric(dataset[\"Power\"].str.strip())\n    #dataset['Engine'] = dataset[\"Engine\"].replace(\"CC\", \"\", regex=True)\n    #dataset['Engine'] = pd.to_numeric(dataset[\"Engine\"].str.strip())\n    #dataset['Mileage'] = dataset[\"Mileage\"].apply(convertKgtoL)\n    #dataset['Mileage'] = pd.to_numeric(dataset[\"Mileage\"])\n    #dataset['New_Price'] = dataset[\"New_Price\"].apply(convertKgtoL)\n    #dataset['New_Price'] = pd.to_numeric(dataset[\"New_Price\"])\n    \n    #dataset['Year_Old'] = dataset[\"Year\"].apply(convertYearDiff)\n    #dataset['Car_Brand'] = dataset[\"Name\"].apply(convertToBrand)\n    #dataset['Kilometers_Driven'] = dataset[\"Kilometers_Driven\"]/dataset[\"Kilometers_Driven\"].max()\n    \n    \n    \n    #dataset['Mileage'].fillna(dataset['Mileage'].mean(), inplace=True)\n    #dataset['Power'].fillna(dataset['Power'].mean(), inplace=True)\n    #dataset['Engine'].fillna(dataset['Engine'].mean(), inplace=True)\n    #dataset['New_Price'].fillna(0.0, inplace=True)\n    \n    dataset = dataset.replace('NaN', '')\n    for col in list(dataset.columns):\n        if col != 'Company ' and col != 'Date':\n            dataset[col] = pd.to_numeric(dataset[col])\n\n     \n    dataset = dataset.drop(['ID'],axis=1)\n    #dataset = dataset.drop(['Year'],axis=1)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = preprocess_data(train_csv)\ntest = preprocess_data(test_csv)\n\ndisplay(train.head())\n#train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['Price']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: %f\" % train['Price'].skew())\nprint(\"Kurtosis: %f\" % train['Price'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set = pd.concat([train, test], axis=0, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corcolm = ['SMA', 'WMA', 'MACD', 'MACD_Hist', 'FastK', 'RSI',\n       'FatD', 'FatK', 'ADX', 'PPO', 'MOM', 'BOP',\n       'ROC', 'ROCR', 'Aroon Down', 'Aroon Up', \n       'MFI', 'ULTOSC', 'DX', 'MINUS_DI', 'MINUS_DM',\n       'MIDPOINT', 'MIDPRICE', 'ATR', 'Chaikin A/D',\n       'ADOSC', 'LEAD SINE', 'SINE', 'TRENDMODE',\n       'DCPERIOD', 'HT_DCPHASE', 'QUADRATURE', 'Company ', 'Price'];\n#correlation matrix\ncorrmat = combined_set[corcolm].corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(corrmat, vmax=.8, square=True);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set = combined_set[corcolm]\n#train = train[corcolm]\n\ntotal = combined_set.isnull().sum().sort_values(ascending=False)\npercent = (combined_set.isnull().sum()/combined_set.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncombined_set = combined_set.interpolate(method ='linear', limit_direction ='both')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set.isnull().sum().max() #just checking that there's no missing data missing...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saleprice correlation matrix\nk = 7 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Price')['Price'].index\ncm = np.corrcoef(combined_set[:11997][cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(15, 15))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(combined_set[:11997]['Price'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_train_num = list(cols)\ncol_train_num.append(\"Company \")\n\ncol_train_num_bias = list(cols)\ncol_train_num_bias.append(\"Company \")\ncol_train_num_bias.remove('Price')\n\nprint(col_train_num_bias)\n\n#test = test[col_train_num_bias]\n#train = train[col_train_num]\n\ncombined_set = combined_set[col_train_num]\n\n#test = test.interpolate(method ='linear', limit_direction ='both') \n#scatter_matrix(train[col_train_num], figsize=(25, 25))\n#plt.show()\n#test.isnull().sum().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n#histogram and normal probability plot\nsns.distplot(combined_set[:11997]['Price'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(combined_set[:11997]['Price'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set[:11997]['Price'] = np.log(combined_set[:11997]['Price'])\nsns.distplot(combined_set[:11997]['Price'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(combined_set[:11997]['Price'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_numerical = train.select_dtypes(exclude=['object'])\n\nfor col in col_train_num:\n    print(col)\n    try:\n        plt.figure()\n        sns.distplot(combined_set[:11997][col], fit=stats.norm);\n    except TypeError:\n        print(\"No graph for this {} column\".format(col))\n    #fig = plt.figure()\n    #stats.probplot(train[col], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in col_train_num:\n    print(col)\n    try:\n        data = pd.concat([combined_set[:11997]['Price'], train[col]], axis=1)\n        data.plot.scatter(x=col, y='Price', ylim=(0,10));\n    except ValueError:\n        print(\"No graph for this {} column\".format(col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set['SMA'] = np.log(combined_set['SMA'])\nsns.distplot(combined_set['SMA'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(combined_set['SMA'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set['WMA'] = np.log(combined_set['WMA'])\nsns.distplot(combined_set['WMA'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(combined_set['WMA'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set['MINUS_DM'] = np.log(combined_set['MINUS_DM'])\nsns.distplot(combined_set['MINUS_DM'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(combined_set['MINUS_DM'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntrain['PLUS_DI'] = np.log(train['PLUS_DI'])\ntest['PLUS_DI'] = np.log(test['PLUS_DI'])\nsns.distplot(train['PLUS_DI'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train['PLUS_DI'], plot=plt)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntrain.loc[train['WILLR']>0, 'WILLR'] = np.log(train[train['WILLR']>0]['WILLR'])\nsns.distplot(train['WILLR'], fit=stats.norm);\nfig = plt.figure()\nres = stats.probplot(train['WILLR'], plot=plt)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined_set = pd.get_dummies(combined_set)\ndisplay(combined_set.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(combined_set.shape)\n\ntrain = combined_set[:11997]\ntest = combined_set[11997:]\ntest = test.drop(\"Price\",axis = 1)\n\ncol_train = list(train.columns)\ncol_train_bis = list(train.columns)\ncol_train_bis.remove(\"Price\")\n\nmat_train = np.matrix(train)\nmat_test  = np.matrix(test)\nmat_new = np.matrix(train.drop('Price',axis = 1))\n\nmat_y = np.array(train.Price).reshape((11997,1))\n\nprepro_y = MinMaxScaler()\nprepro_y.fit(mat_y)\n\nprepro = MinMaxScaler()\nprepro.fit(mat_train)\n\nprepro_test = MinMaxScaler()\nprepro_test.fit(mat_new)\n\n# trimed_test.to_csv(\"output_final_3.csv\")\ntrain_set = pd.DataFrame(prepro.transform(train),columns = col_train)\n\n# test = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n\ntest_set  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_bis)\n\ndisplay(train_set.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"COLUMNS = col_train\nFEATURES = col_train_bis\nLABEL = \"Price\"\n\n#FEATURES.remove('Price')\n\n# Training set and Prediction set with the features to predict\ntraining_set = train_set[col_train]\nprediction_set = training_set.Price\n\n# print(prediction_set)\n\nX_train, X_val, y_train, y_val = train_test_split(training_set[FEATURES] , prediction_set, test_size=0.4)\n\ntrain_set_tensor = torch.utils.data.TensorDataset(torch.FloatTensor(X_train.values), torch.FloatTensor(y_train.values))\nval_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_val.values), torch.FloatTensor(y_val.values))\n\nbatch_size = 8\ntrain_loader = torch.utils.data.DataLoader(train_set_tensor,batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntrain_numerical = train.select_dtypes(exclude=['object'])\ntrain_numerical.fillna(0,inplace = True)\ntrain_categoric = train.select_dtypes(include=['object'])\ntrain_categoric.fillna('NONE',inplace = True)\ntrain_new = train_numerical.merge(train_categoric, left_index = True, right_index = True)\n\ntest_numerical = test.select_dtypes(exclude=['object'])\ntest_numerical.fillna(0,inplace = True)\ntest_categoric = test.select_dtypes(include=['object'])\ntest_categoric.fillna('NONE',inplace = True)\ntest_new = test_numerical.merge(test_categoric, left_index = True, right_index = True) \n\ntrain.dtypes\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Removie the outliers\nfrom sklearn.ensemble import IsolationForest\n\nclf = IsolationForest(max_samples = 100, random_state = 42)\nclf.fit(train_numerical)\ny_noano = clf.predict(train_numerical)\ny_noano = pd.DataFrame(y_noano, columns = ['Top'])\ny_noano[y_noano['Top'] == 1].index.values\n\ntrain_numerical = train_numerical.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain_numerical.reset_index(drop = True, inplace = True)\n\ntrain_categoric = train_categoric.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain_categoric.reset_index(drop = True, inplace = True)\n\ntrain_new = train_new.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain_new.reset_index(drop = True, inplace = True)\ndisplay(train_new.head())\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#col_train = list(train_new.columns)\ncol_train_num = list(train_numerical.columns)\ncol_train_num_bis = list(train_numerical.columns)\n\ncol_train_cat = list(train_categoric.columns)\n\ncol_train_num_bis.remove('Price')\n\nmat_train = np.matrix(train_numerical)\nmat_test  = np.matrix(test_numerical)\nmat_new = np.matrix(train_numerical.drop('Price',axis = 1))\nmat_y = np.array(train_new.Price)\n\nprint(mat_y.shape)\n\nprepro_y = MinMaxScaler()\nprepro_y.fit(mat_y.reshape(5417,1))\n\nprepro = MinMaxScaler()\nprepro.fit(mat_train)\n\nprepro_test = MinMaxScaler()\nprepro_test.fit(mat_new)\n\ntrain_num_scale = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\ntest_num_scale  = pd.DataFrame(prepro_test.transform(mat_test),columns = col_train_num_bis)\n\ndef oneHotEncode(df,colNames):\n    for col in colNames:\n        if( df[col].dtype == np.dtype('object')):\n            dummies = pd.get_dummies(df[col],prefix=col)\n            df = pd.concat([df,dummies],axis=1)\n\n            #drop the encoded column\n            df.drop([col],axis = 1 , inplace=True)\n    return df\n\ntrain_new[col_train_num] = pd.DataFrame(prepro.transform(mat_train),columns = col_train_num)\ntest_new[col_train_num_bis]  = test_num_scale\n\nprediction_set = train_new.Price\nprint(prediction_set.shape)\ncombined = train_new.drop('Price',axis = 1).append(test_new)\ncombined.reset_index(inplace=True)\nprint('There were {} columns before encoding categorical features'.format(combined.shape[1]))\ncombined = oneHotEncode(combined, col_train_cat)\nprint('There are {} columns after encoding categorical features'.format(combined.shape[1]))\n\ntrain_new = combined[:5417]\ntest_new = combined[5417:]\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Train and Test \nX_train, X_val, y_train, y_val = train_test_split(train_new, prediction_set, test_size=0.33, random_state=42)\ntrain_set_tensor = torch.utils.data.TensorDataset(torch.FloatTensor(X_train.values), torch.FloatTensor(y_train.values))\nval_set = torch.utils.data.TensorDataset(torch.FloatTensor(X_val.values), torch.FloatTensor(y_val.values))\n\nbatch_size = 1\ntrain_loader = torch.utils.data.DataLoader(train_set_tensor,batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\n# batch_no = len(X_train) // batch_size  #batches\n# cols=X_train.shape[1] #Number of columns in input matrix\n\n# Sequence Length\n#sequence_length = 6  # of words in a sequence 892110\n# Batch Size\n# batch_size = 128\n# train_loader = batch_data(int_text, sequence_length, batch_size)\n# Number of Epochs\nnum_epochs = 3000\n# Learning Rate\nlearning_rate = 0.002\n# Model parameters\n# Input size\ninput_size = X_train.shape[1]\n# Output size\noutput_size = 1\n# Embedding Dimension\n#embedding_dim = 128\n# Hidden Dimension\nhidden_dim = 64\n# Number of RNN Layers\nn_layers = 2\n\n# Show stats for every n number of batches\nshow_every_n_batches = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass DNNClassifier(nn.Module):\n    \"\"\"\n    This is the simple DNN model we will be using to perform Sentiment Analysis.\n    \"\"\"\n\n    def __init__(self, hidden_dim, input_size, output_size, dropout=0.5):\n        \"\"\"\n        Initialize the model by settingg up the various layers.\n        \"\"\"\n        super(DNNClassifier, self).__init__()\n\n        self.sig = nn.Sigmoid()        \n        # self.word_dict = None\n        \n        self.fc1 = nn.Linear(input_size, hidden_dim * 4)\n        self.fc2 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n        #self.fc3 = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n        self.fc4 = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.out = nn.Linear(hidden_dim, output_size)\n        self.dropout = nn.Dropout(p=0.5)\n        self.init_weights()\n        \n    def init_weights(m):\n        initrange = 0.08\n        classname = m.__class__.__name__\n        if classname.find('Linear') != -1:\n            # get the number of the inputs\n            n = m.in_features\n            y = 1.0/np.sqrt(n)\n            m.weight.data.normal_(0.0, y)\n            m.bias.data.fill_(0)\n        \n    def forward(self, x):\n        \"\"\"\n        Perform a forward pass of our model on some input.\n        \"\"\"\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        #x = F.relu(self.fc3(x))\n        #x = self.dropout(x)\n        x = F.relu(self.fc4(x))\n        x = self.dropout(x)\n        out = self.out(x)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.autograd import Variable\n\ndef forward_back_prop(rnn, optimizer, criterion, inputs, labels, clip=9):\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n\n    hidden = {}\n    # hidden = tuple([each.data for each in hidden_dim])\n    \n    rnn.zero_grad()\n    optimizer.zero_grad()\n    #print(inputs)\n    try:\n        # get the output from the model\n        # output, hidden = rnn(inputs, hidden)\n        output = rnn.forward(inputs)\n        #output = rnn(inputs.unsqueeze(0))\n        output = output.squeeze()\n        #print(output)\n    except RuntimeError:\n        raise\n    #print(labels)\n    loss = criterion(output, labels)\n    loss.backward()\n    \n    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n    # nn.utils.clip_grad_norm_(rnn.parameters(),  clip)\n   \n    optimizer.step()\n\n    return loss.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n    batch_losses = []\n    val_batch_losses = []\n    valid_loss_min = np.Inf\n    \n    rnn.train()\n    \n    previousLoss = np.Inf\n    minLoss = np.Inf\n\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        \n        # initialize hidden state\n        # hidden = rnn.init_hidden(batch_size)\n        # print(\"epoch \",epoch_i)\n        rnn.train()\n        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n            # batch_last = batch_i\n            # n_batches = len(train_loader.dataset) // batch_size\n            loss = forward_back_prop(rnn, optimizer, criterion, inputs, labels, clip=5)\n            #print(loss)\n            # record loss\n            batch_losses.append(loss)\n            \n        rnn.eval()\n        for batch_i, (inputs, labels) in enumerate(val_loader, 1):\n            # batch_last = batch_i\n            # n_batches = len(val_loader.dataset) // batch_size\n            if(train_on_gpu):\n                inputs, labels = inputs.cuda(), labels.cuda()\n            # if(batch_i > n_batches):\n                # break\n            try:\n                output = rnn.forward(inputs)\n                output = output.squeeze()\n            except RuntimeError:\n                raise\n            # print(labels)\n            loss = criterion(output, labels)\n\n            val_batch_losses.append(loss.item())\n\n        # printing loss stats\n        if epoch_i%show_every_n_batches == 0:\n            average_loss = np.average(batch_losses)\n            val_average_loss = np.average(val_batch_losses)\n            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch_i, average_loss, val_average_loss))\n\n            ## TODO: save the model if validation loss has decreased\n            # save model if validation loss has decreased\n            if val_average_loss < valid_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                valid_loss_min,\n                val_average_loss))\n                with open('trained_rnn_new', 'wb') as pickle_file:\n                    # print(pickle_file)\n                    torch.save(rnn, pickle_file)\n                valid_loss_min = val_average_loss\n\n            batch_losses = []\n            val_batch_losses = []\n            \n    return rnn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\nif not train_on_gpu:\n    print('No GPU found. Please use a GPU to train your neural network.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create model and move to gpu if available\n# rnn = RNN(input_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.25)\n# rnn.apply(weight_init)\n#rnn = LSTMClassifier(embedding_dim, hidden_dim, input_size, n_layers, output_size)\nrnn = DNNClassifier(hidden_dim, input_size, output_size)\n\n#rnn = torch.load(\"trained_rnn_new\")\n\nif train_on_gpu:\n    rnn.cuda()\n\ndecay_rate = learning_rate / num_epochs\n\n# print(decay_rate)\n# defining loss and optimization functions for training\n#optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\noptimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate, momentum=0.9, weight_decay=decay_rate)\n\n# criterion = nn.CrossEntropyLoss()\ncriterion = nn.MSELoss()\n#rnn = torch.load(\"trained_rnn_new\")\n\n# training the model\n#trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n\n# saving the trained model\n# helper.save_model('./save/trained_rnn', trained_rnn)\nprint('Model Trained and Saved')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, inputs):\n\n    if(train_on_gpu):\n        inputs = inputs.cuda()\n    \n    try:\n        output = model.forward(inputs)\n        output = output.squeeze()\n        #print(output)\n    except RuntimeError:\n        raise\n    \n    # prediction = np.array(output).argmax(0)\n    # p = F.softmax(output, dim=1).data\n    # p = F.sigmoid(output)\n    # p = F.logsigmoid(output)\n    p = output.cpu().detach().numpy().flatten()\n    #print(p[0])\n    # prediction = np.argmax(p)\n    # print(prediction)\n    return p[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rnn = torch.load(\"trained_rnn_new\")\nmodel_rnn.eval()\ndisplay(X_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(test_set.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Val_outputs = []\nprint(train[:500].shape)\n\npred_training_set = train_set[col_train][:500]\npred_training_set = pred_training_set.drop('Price',axis = 1)\n\nfor row in pred_training_set.values:\n    valoutput = predict(model_rnn, torch.FloatTensor(row))\n    Val_outputs.append(valoutput)\n\nprint(Val_outputs[:10])\nprint(y_val.values[:10])\n\ns_out = pd.Series(prepro_y.inverse_transform(np.array(Val_outputs).reshape(500,1)).squeeze())\nt_out =  np.exp(s_out)\nprint(t_out.values[:20])\nprint(train_csv['Price'].values[:20])\nr2_score(train_csv['Price'].values[:500], t_out.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_outputs = []\nfor row in test_set.values:\n    testoutput = predict(model_rnn, torch.FloatTensor(row))\n    Test_outputs.append(testoutput)\n\nprint(Test_outputs[:30])\nprint(len(Test_outputs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntest_input = torch.randn(3, 5, requires_grad=True)\ntest_target = torch.randn(3, 5)\nX = Variable(torch.FloatTensor(X_train.values)) \nprint(X)\npred = predict(model_rnn, X)\nprint(pred[:30])\nprint(pred.shape)\n# pred= result\nprint(y_val.values[:30])\nr2_score(y_train.values, pred)\n\nloss = nn.L1Loss()\noutput_loss = loss(torch.FloatTensor(y_train.values),torch.FloatTensor(pred))\nprint(1 - output_loss)\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntest_X = Variable(torch.FloatTensor(test_set.values))\nprint(test_X)\ntest_pred = predict(model_rnn, test_X)\nprint(test_pred)\nprint(len(test_pred))\n# print(np.array(test_p).reshape(9614,1))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.array([2,4,8,10,12,18, 100, 200, 400])\nlog_a = np.log(a)\nexp_a = np.exp(log_a)\nprint(a)\nprint(log_a)\nprint(exp_a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_out = pd.Series(prepro_y.inverse_transform(np.array(Test_outputs).reshape(4161,1)).squeeze())\nt_out =  np.exp(s_out)\npredictions = pd.DataFrame(test_csv[\"ID\"].values, columns = [\"ID\"])\n# predictions = pd.DataFrame(np.array(test_pred).reshape(8037,1), columns = [\"FORECLOSURE\"])\n# predictions[\"FORECLOSURE\"] = predictions[\"FORECLOSURE\"]\n# predictions['SalePrice'] = predictions['SalePrice']\n# predictions['FORECLOSURE'] = predictions['FORECLOSURE'].apply(lambda x: 0 if x < 0.01 else 1)\n# predictions['FORECLOSURE'] = predictions['FORECLOSURE'].apply(lambda x: 1 if x > 0 else x)\n# predictions = predictions.round(2)\n# predictions[\"ID\"] = test_csv[\"ID\"]\npredictions[\"Price\"] = t_out\ndisplay(predictions.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.to_csv(\"submission_3.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename=\"submission_1.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe\ncreate_download_link(predictions)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"}},"nbformat":4,"nbformat_minor":1}