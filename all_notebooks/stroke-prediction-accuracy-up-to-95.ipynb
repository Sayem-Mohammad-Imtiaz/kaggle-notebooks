{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# handling missing values\ndf['bmi'].fillna(df['bmi'].mean(), inplace=True)\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= 'gender', hue='stroke', data=df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Female have more chances of getting a stroke","metadata":{}},{"cell_type":"code","source":"sns.displot(df['age'], bins=10, kde=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 40s-60s have more chances","metadata":{}},{"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\")\nsns.countplot(data=df, x=\"ever_married\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df, x=\"work_type\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df, x=\"work_type\", hue='gender')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Private sector is more vernuable, makes sence","metadata":{}},{"cell_type":"code","source":"sns.countplot(data=df, x=\"smoking_status\")\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### shocking, peple who never smoked have more chances","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(7,7))\nsns.displot(df.bmi, color=\"orange\", label=\"bmi\", kde=True)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\n\nsns.distplot(df[df.stroke == 0][\"age\"], color=\"green\")\nsns.distplot(df[df.stroke != 0][\"age\"], color=\"red\")\n\nplt.title(\"No strock Vs Stroke By BMI\", fontsize=15)\nplt.xlim([10,100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Preprocessing for ML","metadata":{}},{"cell_type":"code","source":"# Import LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Instantiate LabelEncoder\nle=LabelEncoder()\n\n# Iterate over all the values of each column and extract their dtypes\nfor col in df.columns:\n    # Compare if the dtype is object\n    if df[col].dtypes=='object':\n    # Use LabelEncoder to do the numeric transformation\n        df[col]=le.fit_transform(df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['id', 'stroke'], axis=1)\ny = df.stroke\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = df.stroke.value_counts(sort=True).index\nsizes = df.stroke.value_counts(sort=True)\n\ncolors=[\"lightblue\", \"red\"]\nexplode=(0.05,0)\nplt.figure(figsize=(7,7))\nplt.pie(sizes, \n        explode=explode, \n        labels=labels,\n        colors=colors,\n        autopct=\"%1.1f%%\",\n        shadow=True,\n        startangle=90)\n\nplt.title(\"Stroke Percent\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### As you can see, percentage of people with no stroke is low and this will affect our model","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Searching right hyperparameters","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\npara = {'n_neighbors':range(0,10)}\n\ngrid_cv = GridSearchCV(knn, para, cv=5)\n\ngrid_cv.fit(X_train_res, y_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nbest = {}\nfor i in range(1, 50):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_res, y_train_res)\n    y_pred = knn.predict(X_test)\n    acc = knn.score(X_test, y_test)\n    #print(i,':',acc)\n    best[i] = round(acc, 3)\n    \nplt.plot(best.keys(), best.values())\nplt.xticks([i for i in range(0, 100, 5)])\nplt.grid(True)\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting best model","metadata":{}},{"cell_type":"code","source":"best_model = grid_cv.best_estimator_\nbest_model.fit(X_train_res, y_train_res)\ny_pred = best_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\nprint(classification_report(y_test, y_pred))\nprint(accuracy_score(y_pred, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['stroke'] == 1].iloc[10].tolist()\n\nbest_model.predict([[0.0, 81.0, 1.0, 0.0, 1.0, 2.0, 0.0, 80.43, 29.7, 2.0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import PCA\nfrom sklearn.decomposition import PCA\n\n# Create PCA instance: model\nmodel = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model.fit_transform(X_train_res)\n\n# Assign 0th column of pca_features: xs\nxs = pca_features[:,0]\n\n# Assign 1st column of pca_features: ys\nys = pca_features[:,1]\n\n# Scatter plot xs vs ys\nplt.scatter(xs, ys)\nplt.axis('equal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dimension reduction (if necessary)","metadata":{}},{"cell_type":"code","source":"# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(X_train_res)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Rather than first, every other feature have similar importance. So I don't think its necessary","metadata":{}},{"cell_type":"markdown","source":"# DecisionTrees","metadata":{}},{"cell_type":"code","source":"dtc = DecisionTreeClassifier()\ndepth = np.arange(1,30)\nleaves = [1,2,4,5,10,20,30,40,80,100]\nparam_grid =[{'max_depth':depth,\n             'min_samples_leaf':leaves}]\ngrid_search = GridSearchCV(estimator = dtc,param_grid = param_grid,\n                           scoring='roc_auc',cv=10)\ngrid_search = grid_search.fit(X_train_res,y_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = grid_search.best_estimator_\ny_pred = dt.predict(X_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ny_pred_proba = dt.predict_proba(X_test)[:,1]\n\n# Compute test_roc_auc\ntest_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print test_roc_auc\nprint('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_pred, y_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.predict([[0.0, 81.0, 1.0, 0.0, 1.0, 2.0, 0.0, 80.43, 29.7, 2.0]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Accuracy is 95% and thats impressive","metadata":{}},{"cell_type":"markdown","source":"# Comparing Models and Choosing best one","metadata":{}},{"cell_type":"code","source":"# Set seed for reproducibility\nSEED=1\n\n# Instantiate lr\nlr = LogisticRegression(random_state=SEED)\n\n# Instantiate knn\nKNN = KNeighborsClassifier\nknn = KNN(n_neighbors=1)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=27, min_samples_leaf=5, random_state=SEED)\n\n# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt)]\n\n# Iterate over the pre-defined list of classifiers\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train_res, y_train_res)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## VotingClassifier","metadata":{}},{"cell_type":"code","source":"# Import VotingClassifier from sklearn.ensemble\nfrom sklearn.ensemble import VotingClassifier\n\n# Instantiate a VotingClassifier vc\nvc = VotingClassifier(estimators=classifiers)     \n\n# Fit vc to the training set\nvc.fit(X_train_res, y_train_res)   \n\n# Evaluate the test set predictions\ny_pred = vc.predict(X_test)\n\n# Calculate accuracy score\naccuracy = accuracy_score(y_pred, y_test)\nprint('Voting Classifier: {:.3f}'.format(accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}