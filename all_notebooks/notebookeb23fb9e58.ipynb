{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport pandas as pd\nimport random\nfrom collections import Counter\nfrom sklearn import preprocessing\nimport time\nimport os\nos.chdir(\"/kaggle/input/knearestneighbours/\")\n\n#for plotting\nplt.style.use('ggplot')\n\nclass CustomKNN:\n\t\n\tdef __init__(self):\n\t\tself.accurate_predictions = 0\n\t\tself.total_predictions = 0\n\t\tself.accuracy = 0.0\n\n\tdef predict(self, training_data, to_predict, k = 3):\n\t\tif len(training_data) >= k:\n\t\t\tprint(\"K cannot be smaller than the total voting groups(ie. number of training data points)\")\n\t\t\treturn\n\t\t\n\t\tdistributions = []\n\t\tfor group in training_data:\n\t\t\tfor features in training_data[group]:\n\t\t\t\t# Find euclidean distance using the numpy function\n\t\t\t\teuclidean_distance = np.linalg.norm(np.array(features)- np.array(to_predict))\n\t\t\t\tdistributions.append([euclidean_distance, group])\n\t\t# Find the k nearest neighbors\n\t\tresults = [i[1] for i in sorted(distributions)[:k]]\n\t\t# Figure out which is the most common class amongst the neighbors.\n\t\tresult = Counter(results).most_common(1)[0][0]\n\t\tconfidence = Counter(results).most_common(1)[0][1]/k\n\t\t\n\t\treturn result, to_predict\n\t\n\tdef test(self, test_set, training_set):\n\n\t\tarr = {}             \n\t\tarr = {group: [] for group in test_set}\n\t\tfor group in test_set:\n\t\t\tfor entry in test_set[group]:\n\t\t\t\tarr[group].append(self.predict(training_set, entry, 3))          \n\t\tfor group in test_set:\n\t\t\tfor data in test_set[group]:\n\t\t\t\tfor i in arr[group]:\n\t\t\t\t\tif data == i[1]:\n\t\t\t\t\t\tself.total_predictions += 1\n\t\t\t\t\t\tif group == i[0]:\n\t\t\t\t\t\t\tself.accurate_predictions+=1\n\t\t\n\t\tself.accuracy = 100*(self.accurate_predictions/self.total_predictions)\n\t\tprint(\"\\nSerial Version\\nAcurracy :\", str(self.accuracy) + \"%\")\n\ndef mod_data(df):\n\tdf.replace('?', -999999, inplace = True)\n\t\n\tdf.replace('yes', 4, inplace = True)\n\tdf.replace('no', 2, inplace = True)\n\n\tdf.replace('notpresent', 4, inplace = True)\n\tdf.replace('present', 2, inplace = True)\n\t\n\tdf.replace('abnormal', 4, inplace = True)\n\tdf.replace('normal', 2, inplace = True)\n\t\n\tdf.replace('poor', 4, inplace = True)\n\tdf.replace('good', 2, inplace = True)\n\t\n\tdf.replace('ckd', 4, inplace = True)\n\tdf.replace('notckd', 2, inplace = True)\n\ndef main():\n\tdf = pd.read_csv(\"water_potability.csv\")\n\tmod_data(df)\n\tdataset = df.astype(float).values.tolist()\n\t\n\t#Normalize the data\n\tx = df.values #returns a numpy array\n\tmin_max_scaler = preprocessing.MinMaxScaler()\n\tx_scaled = min_max_scaler.fit_transform(x)\n\tdf = pd.DataFrame(x_scaled) #Replace df with normalized values\n\t\n\t#Shuffle the dataset\n\trandom.shuffle(dataset)\n\n\t#20% of the available data will be used for testing\n\ttest_size = 0.1\n\n\t#The keys of the dict are the classes that the data is classfied into\n\ttraining_set = {2: [], 4:[]}\n\ttest_set = {2: [], 4:[]}\n\t\n\t#Split data into training and test for cross validation\n\ttraining_data = dataset[:-int(test_size * len(dataset))]\n\ttest_data = dataset[-int(test_size * len(dataset)):]\n\t\n\t#Insert data into the training set\n\tfor record in training_data:\n\t\ttraining_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n\n\t#Insert data into the test set\n\tfor record in test_data:\n\t\ttest_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n\ts = time.process_time()\n\tknn = CustomKNN()\n\tknn.test(test_set, training_set)\n\te =  time.process_time()\n\t\n\tprint(\"Exec Time: \", e-s)\n\t\nif __name__ == \"__main__\":\n\tmain()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-15T16:16:17.003754Z","iopub.execute_input":"2021-06-15T16:16:17.004196Z","iopub.status.idle":"2021-06-15T16:16:37.064204Z","shell.execute_reply.started":"2021-06-15T16:16:17.00414Z","shell.execute_reply":"2021-06-15T16:16:37.062772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport pandas as pd\nimport random\nfrom collections import Counter\nfrom sklearn import preprocessing\nfrom itertools import repeat\nimport multiprocessing as mp\nimport time\nimport os\nos.chdir(\"/kaggle/input/knearestneighbours/\")\n\n#for plotting\nplt.style.use('ggplot')\n\nclass CustomKNN:\n\t\n\tdef __init__(self):\n\t\tself.accurate_predictions = 0\n\t\tself.total_predictions = 0\n\t\tself.accuracy = 0.0\n\n\tdef predict(self, training_data, to_predict, k = 3):\n\t\tif len(training_data) >= k:\n\t\t\tprint(\"K cannot be smaller than the total voting groups(ie. number of training data points)\")\n\t\t\treturn\n\t\t\n\t\tdistributions = []\n\t\tfor group in training_data:\n\t\t\tfor features in training_data[group]:\n\t\t\t\t# Find euclidean distance using the numpy function\n\t\t\t\teuclidean_distance = np.linalg.norm(np.array(features)- np.array(to_predict))\n\t\t\t\tdistributions.append([euclidean_distance, group])\n\t\t# Find the k nearest neighbors\n\t\tresults = [i[1] for i in sorted(distributions)[:k]]\n\t\t# Figure out which is the most common class amongst the neighbors.\n\t\tresult = Counter(results).most_common(1)[0][0]\n\t\tconfidence = Counter(results).most_common(1)[0][1]/k\n\t\t\n\t\treturn result, to_predict\n\t\n\tdef test(self, test_set, training_set):\n\t\tpool = mp.Pool(processes= 8)\n\n\t\tarr = {}\n\t\ts =  time.process_time()\n\t\t\n\t\t# This is where we parallelize our code. While testing for the classes of incoming points,\n\t\t# we divide the incoming data points and feed them into the predict funtion in parallel.\n\t\t# I have used the starpmap function of multiprocessing library for this purpose. \n\t\t# The training data gets repeated to get an iterable of the training dataset for the map function, ie. the predict funtion, to be applied on.\n\t\tfor group in test_set:\n\t\t\tarr[group] =  pool.starmap(self.predict, zip(repeat(training_set), test_set[group], repeat(3)))\n\t\te =  time.process_time()\n\n\t\t#Calculating Accuracy - The accuracy code has to be modified due to the induced parallelism. \n\t\t# It is no longer possible to determinstically calculate the accurate predictions where multiple subprocesses are doing the same increment.\n\n\t\tfor group in test_set:\n\t\t\tfor data in test_set[group]:\n\t\t\t\tfor i in arr[group]:\n\t\t\t\t\tif data == i[1]: \t\n\t\t\t\t\t\tself.total_predictions += 1\n\t\t\t\t\t\tif group == i[0]:\n\t\t\t\t\t\t\tself.accurate_predictions+=1\n\t\t\n\t\tself.accuracy = 100*(self.accurate_predictions/self.total_predictions)\n\t\tprint(\"\\nParallel Version\\nAcurracy :\", str(self.accuracy) + \"%\")\n\ndef mod_data(df):\n\tdf.replace('?', -999999, inplace = True)\n\t\n\tdf.replace('yes', 4, inplace = True)\n\tdf.replace('no', 2, inplace = True)\n\n\tdf.replace('notpresent', 4, inplace = True)\n\tdf.replace('present', 2, inplace = True)\n\t\n\tdf.replace('abnormal', 4, inplace = True)\n\tdf.replace('normal', 2, inplace = True)\n\t\n\tdf.replace('poor', 4, inplace = True)\n\tdf.replace('good', 2, inplace = True)\n\t\n\tdf.replace('ckd', 4, inplace = True)\n\tdf.replace('notckd', 2, inplace = True)\n\ndef main():\n\tdf = pd.read_csv(\"water_potability.csv\")\n\tmod_data(df)\n\tdataset = df.astype(float).values.tolist()\n\t\n\t#Normalize the data\n\tx = df.values #returns a numpy array\n\tmin_max_scaler = preprocessing.MinMaxScaler()\n\tx_scaled = min_max_scaler.fit_transform(x)\n\tdf = pd.DataFrame(x_scaled) #Replace df with normalized values\n\t\n\t#Shuffle the dataset\n\trandom.shuffle(dataset)\n\n\t#20% of the available data will be used for testing\n\ttest_size = 0.1\n\n\t#The keys of the dict are the classes that the data is classfied into\n\ttraining_set = {2: [], 4:[]}\n\ttest_set = {2: [], 4:[]}\n\t\n\t#Split data into training and test for cross validation\n\ttraining_data = dataset[:-int(test_size * len(dataset))]\n\ttest_data = dataset[-int(test_size * len(dataset)):]\n\t\n\t#Insert data into the training set\n\tfor record in training_data:\n\t\ttraining_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n\n\t#Insert data into the test set\n\tfor record in test_data:\n\t\ttest_set[record[-1]].append(record[:-1]) # Append the list in the dict will all the elements of the record except the class\n\t\n\ts = time.process_time()\n\tknn = CustomKNN()\n\tknn.test(test_set, training_set)\n\te =  time.process_time()\n\t\n\tprint(\"Exec Time: \", e-s)\n\t\nif __name__ == \"__main__\":\n\tmain()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T16:16:37.066334Z","iopub.execute_input":"2021-06-15T16:16:37.066645Z","iopub.status.idle":"2021-06-15T16:16:44.943464Z","shell.execute_reply.started":"2021-06-15T16:16:37.066615Z","shell.execute_reply":"2021-06-15T16:16:44.942557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}