{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\n\nimport numpy as np\nimport pylab as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/coursesdata/\"\nstudentInfo = pd.read_csv(PATH + 'studentInfo.csv')\ncourses = pd.read_csv(PATH + 'courses.csv')\nassessments = pd.read_csv(PATH + 'assessments.csv')\nstudentAssessment = pd.read_csv(PATH + 'studentAssessment.csv')\nstudentReview = pd.read_csv(PATH + 'studentReview.csv')\n\nstudentInfo.head()\n# courses.head()\n# assessments.head()\n# studentAssessment.head()\n# studentReview.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging Tables\nresult = pd.merge(studentInfo, courses, left_on=('course','run'), right_on=('course','run'),how='left', sort=False);\nresult = pd.merge(result, assessments, left_on=('course','run'), right_on=('course','run'),how='left', sort=False);\nresult = pd.merge(result, studentAssessment, left_on=('student_id','assessment_id'), right_on=('student_id','assessment_id'),how='left', sort=False);\nresult = pd.merge(result, studentReview, left_on=('student_id','course'), right_on=('student_id','course'),how='left', sort=False);\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reorder Columns\nresult = result[['student_id','course', 'run',  'gender', 'region', 'highest_education_level', 'age_range', 'completed', \n                 'date_enrolled', 'date_unenrolled', 'course_length', 'assessment_id','assessment_type', 'date', 'weight',\n                 'date_submitted', 'score', 'student_review','upgraded']]\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping by (student, Course and run), so we can predict for each (user, couurse, run) the upgraded value\n\nresult.groupby(['student_id', 'course','run']).agg({\n    'gender': lambda x: x[0],\n    'region': lambda x: x[0],\n    'highest_education_level': lambda x: x[0],\n    'age_range': lambda x: x[0],\n    'completed': lambda x: x[0],\n    'date_enrolled': lambda x: x[0],\n    'date_unenrolled': lambda x: x[0],\n    'course_length': lambda x: x[0],\n    'assessment_id': 'count',\n    'assessment_type': lambda x: x[0],\n    'date': lambda x: x[0],\n    'weight': lambda x: x[0],\n    'date_submitted': lambda x: x[0],\n    'score': lambda x: x[0],\n    'student_review': lambda x: x[0],\n    'upgraded' :lambda x: x[0]\n})\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new User_Course_Run identifier\nresult['ID'] = result['student_id'].map(str) + '_' + result['course'] + '_' + result['run']\n\n# Making User_Course_Run the first in the dataframe, and removing [student_id, course, run]\nresult['student_id'] = result['ID']\nresult.rename(columns={'student_id': 'Student_course_Run_id'}, inplace=True)\nresult.drop(['course', 'run', 'ID'], axis=1, inplace=True)\n\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turning non numeric values into numbers using labelEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Lebel encoding Target column\nleup = LabelEncoder()\nleup.fit(result.upgraded)\nresult.upgraded=leup.transform(result.upgraded)\n\ncat_cols = ['gender','region','highest_education_level','age_range','completed','date_enrolled','assessment_type']\nfor col in cat_cols:\n    if col in result.columns:\n        le = LabelEncoder()\n        le.fit(list(result[col].astype(str).values))\n        result[col] = le.transform(list(result[col].astype(str).values))\n        \nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Target (Most studnets don't upgrade)\nimport seaborn as sns\n\nsns.countplot(x='upgraded', data=result);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running this command, we can see that some columns have missing values\nresult.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the distribution of each column\nresult.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Destribution of columns ['date_enrolled', 'course_length', 'date', 'weight', 'score']\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16,6))\nboxplot = result.boxplot(column=['date_enrolled', 'course_length', 'date', 'weight', 'score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missig values (We will use the mean to impute the missing values)\nresult.score = result.score.fillna(result.score.mean())\n\nresult['date_submitted'] = result['date_submitted'].fillna(result['date_submitted'].mean())\nresult['date_unenrolled'] = result['date_unenrolled'].fillna(result['date_unenrolled'].mean())\nresult['date'] = result['date'].fillna(result['date'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Baseline Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the baseline model, we will use just the numeric columns.\n# In order to not lose the review effect, We will create a \"student_review_len\"\n# column before removing the \"student_review\" column.\n\ndef add_review_features(df):\n    df['student_review'] = df['student_review'].apply(lambda x:str(x))\n    df['student_review_len'] = df['student_review'].apply(len)\n    df['student_review_n_capitals'] = df['student_review'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['student_review_n_words'] = df['student_review'].str.count('\\S+')\n    return df\n\nresult = add_review_features(result)\n# Removing unique identifiers + studnt review\ndata = result.drop(['assessment_id','student_review'],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data into 80% training and 20% test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nX = data.drop(['Student_course_Run_id', 'upgraded'],axis=1)\ny = data.upgraded\n\n# Standardize features by removing the mean and deviding by variance\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\n\nskf = StratifiedKFold(n_splits=5)\nskf.get_n_splits(X, y)\n\n# Accuracies and F-Scores across k folds\naccs, fsc = [], []\n\nprint(skf)\nStratifiedKFold(n_splits=5, random_state=10, shuffle=False)\nfor train_index, test_index in skf.split(X, y):\n    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create Model\n    clf =  RandomForestClassifier(n_estimators=10, random_state=10)\n    # Train Decision Tree Classifer\n    clf = clf.fit(X_train,y_train)\n    # Predict the response for test dataset\n    y_pred = clf.predict(X_test)\n    \n    # Evaluate performance\n    print(\"Fold Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n    print(\"Fold F1-Score:\",metrics.f1_score(y_test, y_pred), end='\\n\\n')\n    accs.append(metrics.accuracy_score(y_test, y_pred))\n    fsc.append(metrics.f1_score(y_test, y_pred))\n    \nprint(\"Overall Accuracy: {:0.2f} +/- {:0.2f}\".format(np.mean(accs), np.std(accs)))\nprint(\"Overall F1-Score: {:0.2f} +/- {:0.2f}\".format(np.mean(fsc), np.std(fsc)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCrouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we will plot WordCrouds for the two classes (upgrade) and (Not upgrade)\n# We can see that words like \"Great\" are indicators for the decision of the student\n\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n\nn_posts = 1000\ndata = result\nrev_Up = ' '.join(data[data['upgraded'] == 0]['student_review'].str.lower().values[:n_posts])\nrev_Nup = ' '.join(data[data['upgraded'] == 1]['student_review'].str.lower().values[:n_posts])\n\nwordcloud_S = WordCloud(max_words=20, scale = 2, stopwords=stop, contour_width=3, contour_color='steelblue').generate(rev_Up)\nwordcloud_I = WordCloud(max_words=20, scale = 2, stopwords=stop, contour_width=3, contour_color='steelblue').generate(rev_Nup)\n\nfig, ax = plt.subplots(1,2, figsize=(22, 6))\nax[0].imshow(wordcloud_S)\nax[0].set_title('Top words studnet review (Not upgrade)',fontsize = 20)\nax[0].axis(\"off\")\n\nax[1].imshow(wordcloud_I)\nax[1].set_title('Top words studnet review (upgrade)',fontsize = 20)\nax[1].axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Creating model using student Review (Bert Large)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Official Tokenizer: create input_ids, input_masks, and segment_ids\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\nimport os, re, pickle\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# Tensorflow imports\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nlp_preprocessing(text):\n    filter_char = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n    text = text.lower()\n    text = text.replace(filter_char,'')\n    text = text.replace('[^a-zA-Z0-9 ]', '')\n    return text\n\nresult[\"student_review\"] = result[\"student_review\"].apply(nlp_preprocessing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, _, _ = train_test_split(result, result, test_size=0.15, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper Functions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, n_num, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    input_num = Input(shape=(n_num,))\n    hidden = concatenate([clf_output, input_num])\n    #hidden = Dense(10, activation='relu')(hidden)\n    \n    out = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids, input_num], outputs=out)\n    model.compile(Adam(lr=2e-2), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Load BERT from Tensorflow Hub\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load tokenizer from BERT Layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling Data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# Drop Unique identifier\nresult = result.drop(['assessment_id'],axis=1)\nX_num  = result.drop(['Student_course_Run_id', 'upgraded', 'student_review'],axis=1)\nX_text = result[\"student_review\"]\ny      = result.upgraded\n\n# Standardize features by removing the mean and deviding by variance\nscaler = StandardScaler()\nX_num_scaled = scaler.fit_transform(X_num)\nX_num_scaled = pd.DataFrame(X_num_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Encode the text into tokens, masks, and segment flags\nX_text = bert_encode(X_text.values, tokenizer, max_len=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn import metrics\n\nskf = StratifiedKFold(n_splits=5)\nskf.get_n_splits(X_num, y)\n\n# Accuracies and F-Scores across k folds\naccs, precs, fsc = [], [], []\n\n# numerc Input Shape\nn_num = X_num.shape[-1]\nBsize = 128 #256\n\nStratifiedKFold(n_splits=5, random_state=10, shuffle=False)\nfor train_index, test_index in skf.split(X_num, y):\n    \n    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_num_train, X_num_test = X_num.iloc[train_index], X_num.iloc[test_index]\n    \n    X_text_ids_train, X_text_ids_test = X_text[0][train_index], X_text[0][test_index]\n    X_text_masks_train, X_text_masks_test = X_text[1][train_index], X_text[1][test_index]\n    X_text_seg_train, X_text_seg_test = X_text[2][train_index], X_text[2][test_index]\n    \n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create Model\n    model = build_model(bert_layer, n_num, max_len=50)\n    \n    # Train Decision Tree Classifer\n    train_history = model.fit(\n        [X_text_ids_train, X_text_masks_train, X_text_seg_train, X_num_train],\n        y_train,\n        validation_split=0.2,\n        epochs=3,\n        batch_size=Bsize\n    )\n\n    # model.save('model.h5')\n    \n    # Predict the response for test dataset\n    y_pred = model.predict([X_text_ids_test, X_text_masks_test, X_text_seg_test, X_num_test],\n                           batch_size=Bsize)\n    \n    # Evaluate performance\n    print(\"Fold Accuracy:\",metrics.accuracy_score(y_test, y_pred.round()))\n    print(\"Fold Precision:\",metrics.precision_score(y_test, y_pred.round()))\n    print(\"Fold F1-Score:\",metrics.f1_score(y_test, y_pred.round()), end='\\n\\n')\n    \n    accs.append(metrics.accuracy_score(y_test, y_pred.round()))\n    precs.append(metrics.precision_score(y_test, y_pred.round()))\n    fsc.append(metrics.f1_score(y_test, y_pred.round()))\n    \nprint(\"Overall Accuracy: {:0.2f} +/- {:0.2f}\".format(np.mean(accs), np.std(accs)))\nprint(\"Overall Precision: {:0.2f} +/- {:0.2f}\".format(np.mean(precs), np.std(precs)))\nprint(\"Overall F1-Score: {:0.2f} +/- {:0.2f}\".format(np.mean(fsc), np.std(fsc)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, n_num, max_len=50)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Conclusion:\n..."},{"metadata":{},"cell_type":"markdown","source":"### References:\n- https://www.tensorflow.org/hub/migration_tf2\n- https://www.tensorflow.org/hub/tf2_saved_model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}