{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Deep Neural Networks"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from keras import backend as K, metrics\nfrom keras.layers import (\n    Input,\n    Activation,\n    Dense,\n    Flatten\n)\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import SGD, Adam\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass Shallow1D:\n    def __call__(self):\n        inputs = Input(shape=(1,))\n        # 4 hidden nodes and 2000 iterations work too.\n        x = Dense(16, activation=\"tanh\", kernel_initializer='he_normal')(inputs)\n        y = Dense(1, activation=\"linear\")(x)\n        model = Model(inputs=inputs, outputs=y)\n        return model\n    \nclass Deep1D:\n    def __call__(self):\n        inputs = Input(shape=(1,))\n        x = Dense(4, activation=\"tanh\", kernel_initializer='he_normal')(inputs)\n        x = Dense(4, activation=\"tanh\", kernel_initializer='he_normal')(x)\n        x = Dense(4, activation=\"tanh\", kernel_initializer='he_normal')(x)\n        x = Dense(4, activation=\"tanh\", kernel_initializer='he_normal')(x)\n        y = Dense(1, activation=\"linear\")(x)\n        model = Model(inputs=inputs, outputs=y)\n        return model\n\ndef plot_result(X, Y, loss, val_loss, fun):\n    epochs = range(1, len(loss) + 1)\n    fig = plt.figure(figsize=(14,6))\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    ax1.plot(epochs, loss, \"r--\", label=\"Training\")\n    ax1.plot(epochs, val_loss, \"r\", label=\"Validation\")\n    ax1.set_title(\"Training and validation loss\")\n    ax1.legend()\n    Xref = np.linspace(0, 2 * np.pi, 256)\n    ax2.plot(Xref, fun(Xref), color=\"r\", label=\"sin(x)\")\n    ax2.scatter(X, Y, s=6, label=\"approximation\")\n    ax1.set_xlabel(\"Epochs\")\n    ax1.set_ylabel(\"Error\")\n    ax1.set_title(\"Training error\")\n    ax2.set_title(\"Decision boundary\")\n    ax2.set_xlabel(\"x\")\n    ax2.set_ylabel(\"y\")\n    ax2.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Approximating $\\sin(x)$ using Keras (shallow model)\n1 hidden layer with 16 neurons. Training might take 10–20 seconds so be patient."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def func1d(x):\n    return np.sin(x)\n\nn_training_examples = 1024\nX0 = np.random.rand(n_training_examples, 1) * 2 * np.pi\nT = np.reshape(func1d(X0), (n_training_examples, 1))\n\n# Train model.\nn_epochs = 200\nmodel = Shallow1D()()\nmodel.compile(optimizer=Adam(), loss=\"mse\")\nmodel.count_params()\nmodel.summary()\nhistory = model.fit(X0, T, validation_split=0.2, epochs=n_epochs, verbose=0)\n\n# Test model.\nn_test_examples = 100\nX = np.linspace(0, 2 * np.pi, n_test_examples)\nY = model.predict(X)\n\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nprint(\"Best loss: {:f} (trn), {:f} (val)\".format(np.min(loss), np.min(val_loss)))\nplot_result(X, Y, loss, val_loss, func1d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Approximating $\\sin(x)$ using Keras (deep model)\n4 hidden layers with 4 neurons each."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_training_examples = 1024\nX0 = np.random.rand(n_training_examples, 1) * 2 * np.pi\nT = np.reshape(func1d(X0), (n_training_examples, 1))\n\n# Train model.\nn_epochs = 200\nmodel = Deep1D()()\nmodel.compile(optimizer=Adam(), loss=\"mse\")\nmodel.count_params()\nmodel.summary()\nhistory = model.fit(X0, T, validation_split=0.2, epochs=n_epochs, verbose=0)\n\n# Test model.\nn_test_examples = 100\nX = np.linspace(0, 2 * np.pi, n_test_examples)\nY = model.predict(X)\n\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nprint(\"Best loss: {:f} (trn), {:f} (val)\".format(np.min(loss), np.min(val_loss)))\nplot_result(X, Y, loss, val_loss, func1d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare the number of parameters for the shallow and deep network.\n* How many learnable parameters would we have with 400 neurons in the shallow network versus 100 neurons in 4 layers for the deep network?\n* What if the input is an image? How many learnable parameters for a 28 x 28 black and white image? How many if it would be RGB?\n\n### Fashion Classifier\nMNIST is a classic dataset of handwritten digits. Zalando has made a similar dataset of different garments: [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist). Run the below code to see some examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K, metrics\nfrom keras.layers import Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPooling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nclass FashionClassifier:\n    def __call__(self):\n        inputs = Input(shape=(28, 28, 1))\n        x = Flatten()(inputs)\n        x = Dense(128, kernel_initializer='he_normal', use_bias=False)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        category = Dense(10, activation=\"softmax\")(x)\n        model = Model(inputs=inputs, outputs=category)\n        return model\n\ndef fashion_mnist():\n    data_train = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\n    data_test = pd.read_csv('../input/fashionmnist/fashion-mnist_test.csv')\n\n    X_train = np.array(data_train.iloc[:, 1:])\n    X_test = np.array(data_test.iloc[:, 1:])\n    y_train = to_categorical(np.array(data_train.iloc[:, 0]))\n    y_test = to_categorical(np.array(data_test.iloc[:, 0]))\n\n    img_rows, img_cols = 28, 28\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n\n    X_train = X_train.astype(\"float32\")\n    X_test = X_test.astype(\"float32\")\n    X_train /= 255\n    X_test /= 255\n    return (X_train, y_train), (X_test, y_test)\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist()\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nprint(train_images.shape, train_labels.shape)\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i, :, :, 0], cmap=\"binary\")\n    plt.xlabel(class_names[np.argmax(train_labels[i])])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above sets up a small two-layer MLP for multi-class classification (`FashionClassifier`). Lets try to learn to differentiate between the different types of garments (10 epochs of training will take a while)."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = FashionClassifier()()\nmodel.compile(optimizer=Adam(), \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.count_params()\nmodel.summary()\nhistory = model.fit(train_images, train_labels, validation_split=0.15, epochs=10)\n\nacc = history.history[\"acc\"]\nval_acc = history.history[\"val_acc\"]\nprint(\"Best accuracy: {:f} (trn), {:f} (val)\".format(np.max(acc), np.max(val_acc)))\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(epochs, acc, \"r--\", label=\"Training\")\nax1.plot(epochs, val_acc, \"r\", label=\"Validation\")\nax1.set_title(\"Training and validation accuracy\")\nax1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 444\nprediction = model.predict(test_images[n].reshape((1, 28, 28, 1)))\nlabel = test_labels[n]\nfor i, p in enumerate(prediction[0]):\n    print(\"{:=5.2f} % - {:s} {:s}\".format(p * 100, class_names[i], \"<--- correct\" if label[i] > 0 else \"\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Change `n` to try other predictions. Can you find an incorrect prediction?\n* What does training and validation mean in the chart above?\n* The curve for training accuracy seems to be on the rise but the curve for validation is flattening out. This is what overfitting looks like. If we keep training, do you think the training accuracy will reach 100%? \n\n### Convolutional classifier\nThe first layer of the above network provides ~100000 parameters to learn. Lets use a completely different architecture: a convolutional network. This will take a few minutes to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FashionCNN:\n    def __call__(self):\n        inputs = Input(shape=(28, 28, 1))\n        x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(inputs)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.2)(x)\n        x = Flatten()(x)\n        x = Dense(128, activation=\"relu\")(x)\n        category = Dense(10, activation=\"softmax\")(x)\n        model = Model(inputs=inputs, outputs=category)\n        return model\n\n# Fancier model that you might try instead of FashionCNN below.\nclass FashionMiniVGGNet:\n    \"\"\"Source: https://www.pyimagesearch.com/2019/02/11/fashion-mnist-with-keras-and-deep-learning/\n    Modified to have batch norm before activation (per the original paper but order is debated).\n    \"\"\"\n    def __call__(self):\n        inputs = Input(shape=(28, 28, 1))\n        # first CONV => RELU => CONV => RELU => POOL layer set\n        x = Conv2D(32, (3, 3), padding=\"same\", use_bias=False)(inputs)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        x = Conv2D(32, (3, 3), padding=\"same\", use_bias=False)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n \n        # second CONV => RELU => CONV => RELU => POOL layer set\n        x = Conv2D(64, (3, 3), padding=\"same\", use_bias=False)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        x = Conv2D(64, (3, 3), padding=\"same\", use_bias=False)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.25)(x)\n \n        # first (and only) set of FC => RELU layers\n        x = Flatten()(x)\n        x = Dense(512, use_bias=False)(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        x = Dropout(0.5)(x)\n \n        # softmax classifier\n        category = Dense(10, activation=\"softmax\")(x)\n        model = Model(inputs=inputs, outputs=category)\n        return model\n\nmodel = FashionCNN()()\nmodel.compile(optimizer=Adam(), \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.count_params()\nmodel.summary()\nhistory = model.fit(train_images, train_labels, validation_split=0.15, batch_size=64, epochs=10)\n\nacc = history.history[\"acc\"]\nval_acc = history.history[\"val_acc\"]\nprint(\"Best accuracy: {:f} (trn), {:f} (val)\".format(np.max(acc), np.max(val_acc)))\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(epochs, acc, \"r--\", label=\"Training\")\nax1.plot(epochs, val_acc, \"r\", label=\"Validation\")\nax1.set_title(\"Training and validation accuracy\")\nax1.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(test_images, test_labels, verbose=0)\nprint(\"Test loss: {:.4f}\".format(score[0]))\nprint(\"Test accuracy: {:.4f}\".format(score[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This network has about 6 times the parameters in total compared to the MLP so a better performance is to be expected. However, look at the number of parameters for the first convolutional layer (`conv_2d_1`): Only 320 parameters!\n\n### Parameter reuse\nOne reason for convolutional networks is just performance: it would be computationally very expensive to process larger RGB images in a dense (fully-connected) deep network. Another reason is this ⤵️\n![](https://imgur.com/JwuSnQd.png)\n\nThat is, a cat ear feature detector needs to work everywhere in the image, at many scales. It also needs to be precise — not all floofy ears at a certain location are cat ears. With a dense network, all locations would need to be specialized at identifiying cat ears but also cat noses, cat tails, etc.\n\nConvolutions are small \"patches\" that are run over the image that can learn low level features (in the early layers) and high level features (deeper in the network)\n\n![](https://cdn-images-1.medium.com/max/1000/1*2EX-2NE-tb2iGhyfOOV8Gw.png)\n\nVGG16 Network architecture:\n![](https://peltarion.com/static/vgg_pa03.jpg)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Classifying Swedish historical handwritten digits\n[ARDIS: a Swedish historical handwritten digit dataset](https://link.springer.com/article/10.1007/s00521-019-04163-3) (Kusetogullar et al. 2019).\n\n![](https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs00521-019-04163-3/MediaObjects/521_2019_4163_Fig1_HTML.jpg)\n\nHere we will use intermediate RGB images from a dataset associated with the ARDIS paper (dataset III). These images have been isolated and cropped but are of different sizes. Will this even work? We will almost be doing original research here :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import ceil\nimport os\n\nfrom keras import backend as K, metrics\nfrom keras.layers import (\n    Input,\n    Activation,\n    Dense,\n    Flatten,\n    Dropout,\n    GlobalAveragePooling2D,\n)\nfrom keras.layers.convolutional import AveragePooling2D, Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import SGD, Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.regularizers import l2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass ArdisCNN:\n    def __call__(self):\n        inputs = Input(shape=(48, 48, 3))\n        x = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(inputs)\n        x = MaxPooling2D(pool_size=(2, 2))(x)\n        x = Dropout(0.2)(x)\n        \n        x = Flatten()(x)\n        x = Dense(128, activation=\"relu\")(x)\n        category = Dense(10, activation=\"softmax\")(x)\n        model = Model(inputs=inputs, outputs=category)\n        return model\n\nseed = 42  # For repeatability.\nnp.random.seed(seed)\n\nbatch_size = 64\nimage_size = (48, 48)\ntrain_dir = \"../input/ardis-dataset-3/ardis-dataset-3/\"\n\n# Generators for images. Automatically assigns class '3'\n# to all images in subdirectory '3'.\n# Also uses 20% of the training set for validation during training.\ndatagen = ImageDataGenerator(\n    rescale=1.0 / 255,\n    # horizontal_flip=True,\n    validation_split=0.2\n)\ntrain_generator = datagen.flow_from_directory(\n    train_dir,\n    target_size=image_size,\n    color_mode=\"rgb\",\n    interpolation=\"lanczos\",\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    subset=\"training\",\n    seed=seed\n)\nvalidation_generator = datagen.flow_from_directory(\n    train_dir,\n    target_size=image_size,\n    color_mode=\"rgb\",\n    interpolation=\"lanczos\",  # Fancy but slow interpolation for resizing!\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    subset=\"validation\",\n    seed=seed\n)\n\n# Compile model.\nmodel = ArdisCNN()()\nmodel.compile(\n    optimizer=Adam(lr=1e-3),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\nmodel.count_params()\nmodel.summary()\n\n# Increase this!\nn_epochs = 4\n\n# Train model.\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=ceil(train_generator.n / batch_size),\n    epochs=n_epochs,\n    validation_data=validation_generator,\n    validation_steps=ceil(validation_generator.n / batch_size),\n)\n\n#\n# Plot performance\n#\nacc = history.history[\"acc\"]\nval_acc = history.history[\"val_acc\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nprint(\"Best accuracy: {:f} (trn), {:f} (val)\".format(np.min(acc), np.min(val_acc)))\nprint(\"Best loss: {:f} (trn), {:f} (val)\".format(np.min(loss), np.min(val_loss)))\n\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, acc, \"b--\", label=\"Training\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation\")\nplt.title(\"Training and validation Accuracy\")\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, \"r--\", label=\"Training\")\nplt.plot(epochs, val_loss, \"r\", label=\"Validation\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = list(validation_generator.class_indices.keys())\nx, y = next(validation_generator)\nprediction = model.predict(x)\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    # plt.imshow(x[i, :, :, 0], cmap=\"binary\")  # For grayscale images.\n    plt.imshow(x[i, :, :])  # For color images.\n    plt.xlabel(\"{:s} ({:.0f}%)\".format(\n        class_names[np.argmax(prediction[i])],\n        prediction[i, np.argmax(prediction[i])] * 100\n    ))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Can you find some errors? How sure is the classifier in these cases? I.e. what are the related probabilities (e.g. 95%).\n* Would *you* be able to correctly classify all these digits? What do you think the human level performance on this dataset would be?\n\nThanks all! :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}