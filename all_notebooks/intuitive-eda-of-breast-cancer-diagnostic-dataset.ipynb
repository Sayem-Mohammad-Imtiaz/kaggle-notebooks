{"cells":[{"metadata":{"collapsed":true,"_kg_hide-output":false,"_kg_hide-input":false,"trusted":true,"_uuid":"85c6cfa20d192b1c3884654fe5e5f7e192bfb3a3"},"cell_type":"code","source":"\"\"\"\nImporting all neccessary packages for data analysis and visualization.\n\"\"\"\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport sklearn\n\nfrom IPython.display import display\nfrom pandas import Series, DataFrame\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92787b9ed3822360dd7f7c8b6edd28caea710269"},"cell_type":"code","source":"\"\"\"\nLoading the Dataset and Listing the Features/Attributes given in the dataset.\n\"\"\"\ncomplete_data = pd.read_csv('../input/data.csv')\ncomplete_data = complete_data.drop('Unnamed: 32', axis=1) #drop Unnamed Column from dataset\n\ndisplay(complete_data.head()) #Show the first 5 rows of the dataset\n\nprint(\"Number of Data Points: {}\".format(complete_data.shape[0])) #print number of data points\nprint(\"Number of Features/Attributes: {}\".format(complete_data.shape[1])) # print number of features\nprint(\"Features/Attributes:\", complete_data.columns) # print the list of all features in the dataset ","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b63aa62c2c4328d8276c3c399886f5c37c2781c0"},"cell_type":"code","source":"\"\"\"\nPrint description of the dataset. Includes Count, Mean, Standard Deviation, Minimum Value, 25th Percentile, 50th Percentile or Median, \n75th Percentile and Maximum Value for each of the Features in the Dataset\n\"\"\"\ncomplete_data.describe()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc4de9fa40cb471831aad9d3584121fd7b8fbc8d"},"cell_type":"code","source":"\"\"\"\nSeparate ID Data and Labels from Dataset\n\"\"\"\nid_data = complete_data['id'] #ID Column from complete_data\nlabels = complete_data['diagnosis'] #Lables column from complete_data\n\nclass_distribution = labels.value_counts() #distribution between malignant and benign tumors \nprint (class_distribution)","execution_count":10,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"7866cb378c3789284cbde43d0d07ea0c2bb4707f"},"cell_type":"code","source":"\"\"\"\nForm data by dropping unneccesary ID column from complete_data and dropping diagnosis column from complete_data as it serves as labels. \nMap labels/targets from letters to number\n\"\"\"\ndata = complete_data.drop('id', axis=1) #drop the ID Column from complete_data\ndata = data.drop('diagnosis', axis=1) #drop the diagnosis column from complete_data\nlabels = labels.map({'B': 0, 'M': 1}) #Map the lables/targets vector, with 0 representing benign tumors and 1 represeting malignant tumors","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ec8af345122434dc53b30da8287269265699274"},"cell_type":"code","source":"\"\"\"\nPlot Correlation Heatmap for data to observe the nature and extent correlation between various features in the dataset \n\"\"\"\n\nplt.figure(figsize=(9,9)) \nsns.heatmap(data.iloc[:,:10].corr(),cbar=True,yticklabels=True,annot=True)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4fdfdcc85cb56a6dd289964a015c731d35d2ce3"},"cell_type":"code","source":"\"\"\"\nDisplay first 5 columns of data to ensure that data does not contain unnecessary features and labels\n\"\"\"\ndata.head()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fefe1cca322ca3dfebf36b37f65d0bfdbeb7c86","collapsed":true},"cell_type":"code","source":"\"\"\"\nScale the data and split it into training and testing sets\n\"\"\"\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nscaler = MinMaxScaler() #Create MinMaxScaler object from SKLearn\nscaled_data = scaler.fit_transform(data) #Fit our data to the object to scale the data.\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.15) #Split data with 15% of data in the test set ","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"102e9c477429e531f6d24a7ce8dc2e7da0b3a1c2"},"cell_type":"code","source":"\"\"\"\nCreate dataset for finding contribution of individual features towards whether or not a certain cancer tumor is malignant or benign. \n\"\"\"\ncomplete_data['diagnosis'] = complete_data['diagnosis'].map({'B': 0, 'M': 1}) #Map values in diagnosis column, 0 representing benign  and 1 represeting malignant \ndata_for_corr = complete_data[['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'diagnosis']]\n#Create data_for_corr with various features and diagnosis \ndata_for_corr.head() ","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"546e6622e8df78ede054ffb98eddfeaed9c2d97b"},"cell_type":"code","source":"\"\"\"\nCreate Pairplot from Seaborn to see relationship between individual features and diagnosis\n\"\"\"\nsns.pairplot(data_for_corr,palette='coolwarm',hue= 'diagnosis')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b946d4c8e35c190931193a098cd6a50d2e9fcbe"},"cell_type":"code","source":"\"\"\"\nBuild Random Forest Classifier which we will use to find feature importances for the various features in the dataset.\n\"\"\"\nfrom sklearn.ensemble import RandomForestClassifier #import the classfier from SKLearn\nrfc = RandomForestClassifier() #build the classfier\nrfc.fit(X_train, y_train) #fit the model with our data\nnames = data.columns\n# Print the results\nprint(\"Features sorted by their score:\")\n#print the features in descending order of feature importance\nprint(sorted(zip(map(lambda x: round(x, 4), rfc.feature_importances_), names), reverse=True)) ","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51bc356fe2df7e18ad13b0afe47b2c73bf9f4ba2"},"cell_type":"code","source":"\"\"\"\nCreate plot showing variable feature importances of all features\n\"\"\"\nimportance = rfc.feature_importances_ #isolate feature importances\nsorted_importances = np.argsort(importance) #sort the feature importances\npadding = np.arange(len(names)-1) + 0.5 #insert padding\nplt.barh(range(len(sorted_importances)), importance[sorted_importances], align='center') #plot the data\n\n#customize and show the plot\nplt.yticks(padding, names[sorted_importances])\nplt.xlabel(\"Relative Importance\")\nplt.title(\"Variable Importance\")\nplt.show()","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0c82a7042b4a73fe7a6c590170731fb317ad8d9"},"cell_type":"code","source":"\"\"\"\nAnother and more complicated way of plotting feature importances using box plots and feature importances of features obtained from various \ndifferent Random Forest models each with varying hyperparameters\n\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import make_scorer\n\nf1_scorer = make_scorer(f1_score, pos_label=0)\nparameters = [{'n_estimators': np.arange(10,150,10), 'max_features': np.arange(5,30,5), \n                 'min_samples_split': np.arange(2, 8, 2)}]\nrfc_grid = GridSearchCV(rfc, parameters, scoring = f1_scorer)\n\nrfc_grid = rfc_grid.fit(X_train, y_train)\nrfc_tuned = rfc_grid.best_estimator_\nrfc_tuned.fit(X_train, y_train)\n\nstd = np.std([tree.feature_importances_ for tree in rfc_tuned.estimators_], axis=0)\nindices = np.argsort(importance)[::-1]\nlabel_features_sorted=[]\nheader = list(data.columns.values)\nfor i in indices:\n    label_features_sorted.append(header[i])\n    \nplt.title(\"Feature importances\")\nplt.bar(range(data.shape[1]), importance[indices], color = \"r\", yerr = std[indices], align=\"center\")\nplt.xticks(range(data.shape[1]), label_features_sorted, rotation=90)\nplt.xlim([-1, data.shape[1]])\nplt.show","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36f65603220f6ef66f9ae7b54e4963c61fad87bc"},"cell_type":"code","source":"\"\"\"\nPerform PCA (Principal Component Analysis) in order reduce the dimensionality of the data to print out the dimensionally reduced data (2D Data)\nand use TSNE to visualize the same\n\"\"\"\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2) #build the model\nreduced_data = pca.fit_transform(data) #reduce the data, output is ndarray\nprint(\"Shape of Reduced Data: \", reduced_data.shape) #inspect shape of the `reduced_data`\nprint(\"Reduced Data:\")\nprint(reduced_data)#print out the reduced data\n\n#Plot the data reduced to 2 dimensions using TSNE with PCA\ntsne = TSNE(init='pca')\ntsne_plot = tsne.fit_transform(scaled_data)\nplt.scatter(tsne_plot[:, 0], tsne_plot[:,1], c=labels)\nplt.show","execution_count":27,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2734d77a79c7653f39e1ad329bc805f31f25c1c3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}