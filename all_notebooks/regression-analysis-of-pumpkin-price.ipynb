{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6ddc8afc-2da6-4450-a673-3b4c61176597","_uuid":"4d38f4da354a65468770afc6ab7a35458eaf5c0e"},"source":"**Performing Linear analysis on size vs price:**\n    \n    Overview:\n    Method: Least Squares method\n        For convience sake , we assign integer values for size , sourced from :\n         https://www.kaggle.com/residentmario/pumpkin-price-linear-regression/notebook\n        Dependent Variable : avgPrice\n        Independent variable : Size\n        Read all csv files and create 2 new columns : avgPrice and size . Along with size these  , size       column is copied to a new dataframe"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9ee5635e-075b-4ed6-b017-f4e37e1f5f4d","_uuid":"a654f0bc5891afa0804e5eac726326d5a76aa9cd"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndef processDataFrame(df):\n    size_map = {'jbo': 1,    'sml': 2,    'med': 3,\n                'med-lge': 4,    'lge': 5,    'xlge': 6,\n                'exjbo': 7}\n    df = df.assign(\n         size = df['Item Size'].map(size_map),\n         avgPrice = (df['Low Price'] + df['High Price'])/2,\n         sizeClass =(df['Item Size'].map(size_map) >= 3).astype(int)\n         )\n    df = df[['size','avgPrice','sizeClass']]  \n    return df\n    \n# Read all cssv files    \natlanta = pd.read_csv('../input/atlanta_9-24-2016_9-30-2017.csv')\natlanta = processDataFrame(atlanta)\nbaltimore = pd.read_csv('../input/baltimore_9-24-2016_9-30-2017.csv')\nbaltimore = processDataFrame(baltimore)\nboston = pd.read_csv('../input/boston_9-24-2016_9-30-2017.csv')\nboston = processDataFrame(boston)\nchicago = pd.read_csv('../input/chicago_9-24-2016_9-30-2017.csv')\nchicago = processDataFrame(chicago)\ncolombia = pd.read_csv('../input/columbia_9-24-2016_9-30-2017.csv')\ncolombia = processDataFrame(colombia)\ndallas = pd.read_csv('../input/dallas_9-24-2016_9-30-2017.csv')\ndallas = processDataFrame(dallas)\ndetroit = pd.read_csv('../input/detroit_9-24-2016_9-30-2017.csv')\ndetroit = processDataFrame(detroit)\nlosAngles = pd.read_csv('../input/los-angeles_9-24-2016_9-30-2017.csv')\nlosAngles = processDataFrame(losAngles)\nmiami = pd.read_csv('../input/miami_9-24-2016_9-30-2017.csv')\nmiami = processDataFrame(miami)\nnewYork = pd.read_csv('../input/new-york_9-24-2016_9-30-2017.csv')\nnewYork = processDataFrame(newYork)\nphilD = pd.read_csv('../input/philadelphia_9-24-2016_9-30-2017.csv')\nphilD = processDataFrame(philD)\nsanFran = pd.read_csv('../input/san-fransisco_9-24-2016_9-30-2017.csv')\nsanFran = processDataFrame(sanFran)\nstLouis = pd.read_csv('../input/st-louis_9-24-2016_9-30-2017.csv')\nstLouis = processDataFrame(stLouis)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0ab2442f-38fb-4be7-96d3-6c6149b6e30f","_uuid":"e9f81f14b54bb7237ba5434de00334208549dd58"},"source":" **Methods in this notebook: **\n \n     A common method to process and perfom dataframe oeration like new column creation , comverting data inside a column is available above.\n     A Common method  for segregating test and train data is available . Works as expected. Did not use this in the subsequent step for the want of practice. \n     \n     The trianing test split is 70 : 30 ratio . \n     The correlation bwtween dependent and independent variable is computed  for training data. Discarded all dataframes with no correlation. "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"60c37b40-5184-4610-8a69-e5f66921fe0b","_uuid":"1ecab028e150c068914a2779d300b07285e4adb3"},"source":"# Perform Train test split in all data\ndef trainTestSegregation(df) : \n    all_records= np.arange(df.shape[0])\n    trainingRecordCount = round(0.7 *df.shape[0])\n    testRecordCount = round(0.3 * df.shape[0])\n    np.random.seed(100)\n    trainingRecordsIds = np.random.choice(all_records,trainingRecordCount ,replace=False)\n    testingRecordsIds =all_records[~np.in1d(all_records,trainingRecordsIds)] \n    trainingRecords = df.iloc[testingRecordsIds,:]\n    testRecords = df.iloc[testingRecordsIds,:]\n    return trainingRecords, testRecords\n\natlantaTraining,atlantaTesting = trainTestSegregation(atlanta)\nbaltimoreTraining,baltimoreTesting = trainTestSegregation(baltimore)\nbostonTraining,bostonTesting = trainTestSegregation(boston)\nchicagoTraining,chicagoTesting = trainTestSegregation(chicago)\nnewYorkTraining,newYorkTesting = trainTestSegregation(newYork)\n\n# Identify Direct and Indirect Variable\n#Based on size estimate price \n# DV : Price IDV : size\n# Correlation\nprint(atlantaTraining['size'].corr(atlantaTraining['avgPrice'])) # 0.004489 or 0.01228\nprint(baltimoreTraining['size'].corr(baltimoreTraining['avgPrice']))#0.715499939368 -> strong +\nprint(bostonTraining['size'].corr(bostonTraining['avgPrice']))#0.472942356909 ->\nprint(chicagoTraining['size'].corr(chicagoTraining['avgPrice']))# 0.603218651914 -> strong +\nprint(newYorkTraining['size'].corr(newYorkTraining['avgPrice'])) # 0.785152670629 -> strong +\n # 0: Weak correlation (no relationship).\n## Concluding that no relationship can be built\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"3229190f-c5a3-4558-bc1d-33befd0efe0a","_uuid":"2d888b100fa5a981e1add6216bf667259045d688"},"source":"**Model Building**\n\n    1. Generted models for dataframes with good correlation \n    2. Verified R-Squared 2 value , P value along with generating the y = mX+c  equation for every model\n    3. Few Models did not have good R-Squared 2 value and P values were insignificant (> 0.050).\n    4. Significant P values for Intercept and size were considered.\n    5. Few cases had insignificant P values for size. Not sure whether to test those models with test data\n    6. The 2 dataframes which made the cut are NewYork and StLouis."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"c76d5b42-3ca1-4718-a9e9-5baba999bc9e","_uuid":"ad683d5df0b42a15ccca4dd84c1b8979b90e6ac9"},"source":"## Step 4: Build Regression Model\n# Least Squares method\n# Formula: DV ~ IDV\nbaltimoreModel = smf.ols(formula='avgPrice ~ size',data = atlantaTraining).fit()\nbaltimoreModel.summary()\n# R - Squared : 0.000 (# 0 - Bad model)\n# avgPrice =  147.5515 + 0.3750 * size\n# P  value : 0.963 -> size\n# only IDVs with p values less than 0.05 should be included\n# p value should be less than 0.05 for the IDV to be significant\n# size  is insignificant - can be ignored\n# Intercept is significant \n### Boston Model\nbostonModel = smf.ols(formula='avgPrice ~ size',data = bostonTraining).fit()\nbostonModel.summary()\n# price   = 100.49 + 20.4214 * size\n# R square: 0.224 - poor model.\n# P value is significant for both intercept and idv\n\nchicagoModel = smf.ols(formula='avgPrice ~ size',data = chicagoTraining).fit()\nchicagoModel.summary()\n# R squqare : 0.364\n# P - Intercept : 0.102 - Insignificant\n# P - IDV : 0.000 - significant\n#44.3997 * size - 45.1840\nnewYorkModel = smf.ols(formula='avgPrice ~ size',data = newYorkTraining).fit()\nnewYorkModel.summary()\n# R sqaure  : 0.616 \n# P Intercept : 0.299 - Insignificant\n# P size: 33.2642\n#avgWeight = 33.2642 * size -22.6877\n# Since p value is insignificant  for intercept we ignore ,  avgWeight = 33.2642 * size"},{"cell_type":"markdown","metadata":{"_cell_guid":"d5e6a476-634d-4ad0-8187-d192d3864fce","_uuid":"f1ac6f0a730471b41dc1b301519d83281b6e0395"},"source":" **Training agaist test data**\n \n     Method MAPE computes mean for difffrence between actual and predicted value  (ie) Error Component of the model.\n     Testing the model for new york \n         1. Created a copy from testing dataframe\n         2. Deleted avg Price column\n         3. Used Predict Method for the newYork model with the newYorkTestingCopy variable to predict the avgPrice value of pumpkin.\n         4 . To test the difference , we call the mape method which takes difference of actual avgPrice and predicted avgPrice. \n         5.  The mean value is computed and is approximatly 60 %  . Ie 60 % difference between predicted and actual value . Huge differece :( . \n         6. Repriducing the same steps by taking training data as a copy and verifying MAPE value for equivality.\n         7. A plot is drawn with \n                 7.1 Size and avg Price with training data\n                 7.2 Size and Predicted Avg Price of training as test data\n               This plot visually shows the difference in actual and predicted value."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"9e0c15d3-3638-4903-a668-2fd846473927","_uuid":"c3e11ac0ef1aa294805803edec13b7aea3a82800"},"source":"# Build new York model : All values decently matching\n# Copy data to new dataframe : newYorkTraining,newYorkTesting\ndef MAPE(actual,predicted):\n    abs_percent_diff = abs(actual-predicted)/actual\n    # 0 actual values might lead to infinite errors\n    # replacing infinite with nan\n    abs_percent_diff = abs_percent_diff.replace(np.inf,np.nan)\n    median_ape = np.median(abs_percent_diff)\n    mean_ape = np.mean(abs_percent_diff)\n    mape = pd.Series([mean_ape,median_ape],index = [\"Mean_APE\",\"Median_APE\"])\n    return mape\nnewYorkTestingCopy  =newYorkTesting.copy()\ndel newYorkTestingCopy['avgPrice']\npredictedAvgPrice = newYorkModel.predict(newYorkTestingCopy)\n#MAPE(newYorkTesting['avgPrice'],predictedAvgPrice) # Mean: 0.603389 Median : Nan 60 % error :(  - Bad Model\nnewYorkTrainingasTestingCopy  =newYorkTraining.copy()\ndel newYorkTrainingasTestingCopy['avgPrice']\nnewYorkTrainingasTestingCopy['predictedavgPrice'] = newYorkModel.predict(newYorkTrainingasTestingCopy)\nMAPE(newYorkTesting['avgPrice'],newYorkTrainingasTestingCopy['predictedavgPrice']) # Mean: 0.603389 Median : Nan 60 % error :(  - Bad Model\nplt.scatter('size','avgPrice',data=newYorkTraining)\nplt.scatter('size','predictedavgPrice',data=newYorkTrainingasTestingCopy,color='r')\n#newYorkModel\n"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"fe9eceb6-0605-4289-8bb6-459a85bb7f0f","_uuid":"23b3502c44b33413c6bc9bbcd7f7919bc3f561a1"},"source":"print(colombia['size'].corr(colombia['avgPrice'])) # Weak +  Corr:0.18116\nprint(dallas['size'].corr(dallas['avgPrice'])) # Weak - correlation : -0.2775\nprint(detroit['size'].corr(detroit['avgPrice'])) # weak + corr : 0.1705\nprint(losAngles['size'].corr(losAngles['avgPrice'])) # weal + corr : 0.2573\nprint(miami['size'].corr(miami['avgPrice'])) # strong + correlation : 1.0\nprint(philD['size'].corr(philD['avgPrice'])) # weak + corr : 0.18772\nprint(sanFran['size'].corr(sanFran['avgPrice'])) # weak - corr : -0.10569\nprint(stLouis['size'].corr(stLouis['avgPrice'])) # st louis negative corr : -0.56222"},{"cell_type":"markdown","metadata":{"_cell_guid":"17ac679b-d067-4fa8-9987-cdf859a0f4fc","_uuid":"d83b7e62175f1d7c2c34ff5e726c71b04b051fc0"},"source":"       The trianing test split is 70 : 30 ratio . \n       StLouis negative correlation  : -0.56222 . \n       Attempting to slit the St Louis data and generate Model.\n       Rvalue : 0.316 # Bad model fit\n       avgprice = 220.3333 -17.6667 * size\n       P value is significant for both values "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"0f3f996a-42c3-4f54-b3d1-f81af52d3abf","_uuid":"d400db395302c8d4d3196879d0b17d5bc4053e6c"},"source":"# Build Model for st Louis\n# Perform Train test split \n# Get all row index using arange\nstLouisCount = np.arange(stLouis.shape[0])\nstLouisTrCount = round(0.7 *stLouis.shape[0]) # 72\nstLouisTeCount = round(0.3 * stLouis.shape[0]) #31\nnp.random.seed(10)\nstLouisTrRows = np.random.choice(stLouisCount,stLouisTrCount,replace=False)\nstLouisTeRows = stLouisCount[~np.in1d(stLouisCount,stLouisTrRows)]\nstLouisTrValue = stLouis.iloc[stLouisTrRows,:]\nstLouisTeValue = stLouis.iloc[stLouisTeRows,:]\n\n# Build Model\nstLouisModel = smf.ols(formula = 'avgPrice ~ size', data =stLouis).fit()\nstLouisModel.summary()\n# Rvalue : 0.316 # Bad model fit\n#avgprice = 220.3333 -17.6667 * size\n# P value is significant for both values "},{"cell_type":"markdown","metadata":{"_cell_guid":"faa96c46-8131-48c0-aa7c-0be244931c8c","_uuid":"0570b329834aeaf3b7baaf91da58658a2ef2621c"},"source":"\n    Method MAPE computes mean for difffrence between actual and predicted value  (ie) Error Component of the model.\n     Testing the model for StLouis \n         1. Created a copy from testing dataframe\n         2. Deleted avg Price column\n         3. Used Predict Method for the StLouis model with the newYorkTestingCopy variable to predict the avgPrice value of pumpkin.\n         4 . To test the difference , we call the mape method which takes difference of actual avgPrice and predicted avgPrice. \n         5.  The mean value is computed and is approximatly 19 %  . Ie 19 % difference between predicted and actual value. Difference less than 50 % is a comparativly better model\n         6. Repriducing the same steps by taking training data as a copy and verifying MAPE value for equivality.\n         7. A plot is drawn with \n                 7.1 Size and avg Price with training data\n                 7.2 Size and Predicted Avg Price of training as test data\n               This plot visually shows the difference in actual and predicted value."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"5f378f6f-b642-465c-947d-f66c744e1d41","_uuid":"882555760f65fe31ec37f7034525985942e42348"},"source":"# Test the effieicny of the model\n# Test for testing data\nstLouisTeValueCopy = stLouisTeValue.copy()\ndel stLouisTeValueCopy['avgPrice']\npredictedstLouisavgPrice = stLouisModel.predict(stLouisTeValueCopy)\n#MAPE(stLouisTeValue['avgPrice'],predictedstLouisavgPrice)  # MEan : 0.19389 Median : Nan -  19 % error\n# Decent model with little variation\n\n## Create a copy of training data as test and verify against this model\nstLouisTrTestCopy = stLouisTrValue.copy()\ndel stLouisTrTestCopy['avgPrice']\nstLouisTrTestCopy['predictedavgPrice'] = stLouisModel.predict(stLouisTrTestCopy)\nMAPE(stLouisTrValue['avgPrice'],stLouisTrTestCopy['predictedavgPrice'])  # 18 % - 0.180719\n## Plot a graph with traning data avg Price and predicted avgPrice in training as test data\nplt.scatter('size','avgPrice',data=stLouisTrValue)\nplt.scatter('size','predictedavgPrice',data=stLouisTrTestCopy,color='g')\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"88942809-37f5-4054-91ed-e7554a9f6112","_uuid":"42afc22b8c430f05ceed167087595a00ebbe88af"},"source":"**Conclusion**\n        \n        On comparing and performing regression of data of all cities , we were able to generate prediction model for St.Louis with error around 18 %.\n        \n   **End of learning**"}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.3","name":"python"}}}