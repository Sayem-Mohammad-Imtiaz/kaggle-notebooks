{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport os,json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models.keyedvectors import KeyedVectors\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T07:09:45.357408Z","iopub.execute_input":"2021-06-14T07:09:45.357857Z","iopub.status.idle":"2021-06-14T07:09:47.607149Z","shell.execute_reply.started":"2021-06-14T07:09:45.357766Z","shell.execute_reply":"2021-06-14T07:09:47.605742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DocSim:\n    def __init__(self, w2v_model, stopwords=None):\n        self.w2v_model = w2v_model\n        self.stopwords = stopwords if stopwords is not None else []\n\n    def vectorize(self, doc: str) -> np.ndarray:\n        \"\"\"\n        Identify the vector values for each word in the given document\n        :param doc:\n        :return:\n        \"\"\"\n        doc = doc.lower()\n        words = [w for w in doc.split(\" \") if w not in self.stopwords]\n        word_vecs = []\n        for word in words:\n            try:\n                vec = self.w2v_model[word]\n                word_vecs.append(vec)\n            except KeyError:\n                # Ignore, if the word doesn't exist in the vocabulary\n                pass\n\n        # Assuming that document vector is the mean of all the word vectors\n        # PS: There are other & better ways to do it.\n        vector = np.mean(word_vecs, axis=0)\n        return vector\n\n    def _cosine_sim(self, vecA, vecB):\n        \"\"\"Find the cosine similarity distance between two vectors.\"\"\"\n        csim = np.dot(vecA, vecB) / (np.linalg.norm(vecA) * np.linalg.norm(vecB))\n        if np.isnan(np.sum(csim)):\n            return 0\n        return csim\n\n    def calculate_similarity(self, source_doc, target_docs=None, threshold=0):\n        \"\"\"Calculates & returns similarity scores between given source document & all\n        the target documents.\"\"\"\n        if not target_docs:\n            return []\n\n        if isinstance(target_docs, str):\n            target_docs = [target_docs]\n\n        source_vec = self.vectorize(source_doc)\n        results = []\n        for doc in target_docs:\n            target_vec = self.vectorize(doc)\n            sim_score = self._cosine_sim(source_vec, target_vec)\n            if sim_score > threshold:\n                results.append({\"score\": sim_score, \"doc\": doc})\n                #results.append(sim_score)\n            # Sort results by score in desc order\n            results.sort(key=lambda k: k[\"score\"], reverse=True)\n            #results.sort(reverse=True)\n\n        return results","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:13:18.914169Z","iopub.execute_input":"2021-06-14T07:13:18.914738Z","iopub.status.idle":"2021-06-14T07:13:18.931925Z","shell.execute_reply.started":"2021-06-14T07:13:18.914702Z","shell.execute_reply":"2021-06-14T07:13:18.92989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def textPreprocessing(text): \n    # 소문자로\n    text = text.lower()\n    \n    # 구두점 제거\n    text = text.translate(text.maketrans('', '', string.punctuation))\n    \n    # 영문자 이외 문자는 공백으로 변환\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    \n    # 불용어 제거\n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(text)\n    result = []\n    for w in word_tokens: \n        if w not in stop_words and len(w)>1: \n            result.append(w) \n            \n    # 표제어 추출 (어간추출보다 단어 복원 성능이 좋아서 선택)\n    n=WordNetLemmatizer()\n    lemmatized_words = [n.lemmatize(word) for word in result]\n    return (' '.join(lemmatized_words))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:13:23.539326Z","iopub.execute_input":"2021-06-14T07:13:23.539725Z","iopub.status.idle":"2021-06-14T07:13:23.547289Z","shell.execute_reply.started":"2021-06-14T07:13:23.53969Z","shell.execute_reply":"2021-06-14T07:13:23.546266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/word2vec-model/GoogleNews-vectors-negative300.bin' \nw2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\nds = DocSim(w2v_model,stopwords=stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:14:26.072291Z","iopub.execute_input":"2021-06-14T07:14:26.072711Z","iopub.status.idle":"2021-06-14T07:15:41.829454Z","shell.execute_reply.started":"2021-06-14T07:14:26.072676Z","shell.execute_reply":"2021-06-14T07:15:41.827833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    path = '/kaggle/input/netflix-shows/netflix_titles.csv'\n    data = pd.read_csv(path)\n    \n    data['processed_description'] = ''\n    # Description 텍스트 전처리\n    for i in range(len(data)):\n        data['title'][i] = data['title'][i].lower()\n        data['processed_description'][i] = textPreprocessing(data['description'][i])\n    \n    userInput = input(\"재미있게 봤던 영화나 드라마 제목을 입력해주세요!  \").lower()\n    \n    while userInput not in list(data['title']):\n        userInput = input(\"제목이 올바르지 않습니다. 다시 입력해 주세요. 종료하고 싶으시면 0을 눌러주세요 \").lower()\n        if userInput == '0':\n            break\n        \n    for i in range(len(data)):\n        if data['title'][i] == userInput: # 유저가 찾는 제목이랑 같으면\n            target = data['processed_description'][i]\n        \n    # Word2vec Model을 이용해 코사인 유사도 점수 계산\n    res = ds.calculate_similarity(target, list(data['processed_description']))[1:11]\n    \n    print(\"<\",userInput,\"과 가장 유사한 영화/TV쇼 리스트>\")\n    for i in range(len(res)):\n        res[i] = res[i]['doc']        \n        for j in range(len(data)):\n            if data['processed_description'][j] == res[i]:\n                print(i+1)\n                print(\"Title: \",data['title'][j])\n                print(\"Description: \",data['description'][j])\n                print()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T07:15:54.568049Z","iopub.execute_input":"2021-06-14T07:15:54.568788Z","iopub.status.idle":"2021-06-14T07:16:26.769133Z","shell.execute_reply.started":"2021-06-14T07:15:54.568746Z","shell.execute_reply":"2021-06-14T07:16:26.767991Z"},"trusted":true},"execution_count":null,"outputs":[]}]}