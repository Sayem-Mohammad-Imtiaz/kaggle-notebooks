{"cells":[{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Dataset\n* Exposure\n* IDpol\n* ClaimNb/Freq\n* 9 Explanatory variables\n* 678013 rows\n\n### Objective\n\n* Use ML to predict ClaimNb (Freq) from the 9 explanatory variables.\n\n### Steps\n\n* Data exploration - no missing data\n* Data manipulation - e.g. dummy variables/logdensity\n* Modelling: Random Forest/XGBoost (GBM) / GLM\n* Evaluation/comparing of models: Deviance/RMSE/Lift (Double lift) charts/Shap values/PDPs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of the three models RF, GLM  and GBM\n\n#1 First section is loading data and models, checking all inline\n\n#2 Defining function to create lift and double lift charts to compare models\n\n#3 PDP of models\n\nObjective: succesfully evaluate each model and understand how they are predicting, their weaknesses, strengths, and if they make logical sense. To quantify by various means: lift metric, deviancee explained metric, total sum predicted, how accurate they are as models.\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom pandas._libs.lib import is_integer\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 0 csv file in the current version of the dataset:\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport gzip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing dataframes\nrf_df = pd.read_pickle('/kaggle/input/alex-farquharson-rf-dataframe/Alex_Farquharson_rf_dataframe.gzip')\nglm_df = pd.read_pickle('/kaggle/input/french-motor-claims-glm-output/df_validation_GLM_preds.gzip')\ngbm_df = pd.read_pickle('/kaggle/input/gbm-french-claims/xgb_filtered_pred_valid_set_new.gzip')\ngbm_df_with_columns = pd.read_pickle('/kaggle/input/gbm-french-claims/xgb_pred_valid_set_new.gzip')\ndf_raw = pd.read_csv('/kaggle/input/french-motor-claims-datasets-fremtpl2freq/freMTPL2freq.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw['Freq'] = df_raw['ClaimNb']/df_raw['Exposure']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing models\nfrom joblib import dump,load\nimport pickle\ngbm_model = pickle.load(open('/kaggle/input/gbm-french-claims/xgbmodel.pkl', \"rb\"))\nrf_model = load('/kaggle/input/alex-farquharson-rf-dataframe/rf_model.gzip')\nglm_model = load('/kaggle/input/french-motor-claims-glm-output/GLMResults_obj.pkl')\n\nmodels = {'rf_model':rf_model,'glm_model':glm_model,'gbm_model':gbm_model}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check common columns match\nrf_df['ClaimNb'].value_counts() == gbm_df_with_columns['ClaimNb'].value_counts()\ngbm_df['ClaimNb'].value_counts() == glm_df['ClaimNb'].value_counts()\n#get datasets ready to merge\nrf_df['IDpol'] = glm_df['IDpol']\ngbm_df_with_columns['gbm predictions'] = gbm_df_with_columns['pred_ClaimNb']\n#merge datasets\nmaster_df = pd.merge(rf_df,glm_df,how='outer',on='IDpol')\nmaster_df = pd.merge(master_df,gbm_df_with_columns[['gbm predictions','IDpol']],how='outer',on='IDpol')\nassert all(master_df.notnull()) ==True\nprint('no nulls')\nassert len(master_df)==101702\nprint('length unchanged')\nassert master_df['ClaimNb_x'].sum() - master_df['ClaimNb_y'].sum() ==0\nprint('columns appear to be concatenated correctly')\nmaster_df.drop(['ClaimNb_y','Exposure_y','wgt','pred_freq','act_Nb',],axis=1,inplace=True)\nmaster_df.rename(columns = {'pred_Nb':'glm predictions'},inplace=True)\npredictions_df = master_df[['Random Forest Predictions','glm predictions','gbm predictions', 'ClaimNb_x','Exposure_x']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GBM overpredicts (1100) GLM very accurate (85) RF overpredicts (1250)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('rf',(predictions_df['Random Forest Predictions']*predictions_df['Exposure_x']).sum())\nprint('glm',(predictions_df['glm predictions']*predictions_df['Exposure_x']).sum())\nprint('gbm',(predictions_df['gbm predictions']*predictions_df['Exposure_x']).sum())\nprint('actual',(predictions_df['ClaimNb_x']*predictions_df['Exposure_x']).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this shows, when weighting the results by exposure rf is actually predicting the best","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lift_chart function\n#to plot lift chart, predicted colm actual colm and weights colm take just the predictions_df colm names only, q is no. of quartiles\ndef lift_chart(predicted_colm,actual_colm, weights_colm,q,y_min=0,y_max=0.18):\n    predictions_df.sort_values(by=predicted_colm,inplace=True)\n    def weighted_qcut(predicted_colm, weights_colm, q):\n        #Return weighted quantile cuts from a given series\n        if is_integer(q):\n            quantiles = np.linspace(0, 1, q + 1)\n        else:\n            quantiles = q\n        order = predictions_df[weights_colm].iloc[predictions_df[predicted_colm].argsort()].cumsum() #makes series of cumulative exposure (sorted by values)\n        bins = pd.cut(order / order.iloc[-1], quantiles, labels=False) #cuts into q quantiles along order (cumulative exposure) column\n        return bins.sort_index() #makes column in line with index of original dataframe\n    #3 cut by weight\n    predictions_df['weighted_cut'] = weighted_qcut(predicted_colm,weights_colm,10)\n    #4 function to make means and plot them\n    def means(predicted_colm,actual_colm):\n        predicted_mean = []\n        actual_mean = []\n        for x in np.arange(10):\n            pred = predictions_df[predictions_df['weighted_cut']==x][predicted_colm].mean()\n            predicted_mean.append(pred)\n            actual = predictions_df[predictions_df['weighted_cut']==x][actual_colm].mean()\n            actual_mean.append(actual)\n        means = pd.DataFrame(data = list(zip(predicted_mean,actual_mean)), columns = ['predicted','actual'])\n        sns.scatterplot(data=means,x='actual',y='actual')\n        sns.scatterplot(data=means, x='actual',y='predicted')\n        a = means.iloc[9]['actual'] / means.iloc[0]['actual']\n        b = means.iloc[9]['predicted'] / means.iloc[0]['predicted']\n        print(predicted_colm[:-12],'actual differentiation',a)\n        print(predicted_colm[:-12], 'model differentiation',b)\n        print(predicted_colm[:-12], 'factor',b/a )\n    means(predicted_colm,actual_colm)\n    plt.title(predicted_colm)\n    plt.ylim(y_min,y_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(1,3,figsize=(20,4))\nplt.subplot(1,3,1)\nlift_chart('Random Forest Predictions','ClaimNb_x','Exposure_x',10)\nplt.subplot(1,3,2)\nlift_chart('glm predictions','ClaimNb_x','Exposure_x',10)\nplt.subplot(1,3,3)\nlift_chart('gbm predictions','ClaimNb_x','Exposure_x',10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these incredibly manufactured graphs, one can see the GBM model appears the closest fit.\n\nLooking at the factor, (how different the model differentiation is from the real) one can see that gbm<(1/RF)<glm - i.e. GBM is best\n\nhowever the gbm lower and upper decile values BOTH being below the actual value mean these errors cancel eachother out slightly, resulting in a better differentitation value.\n\nGLM appeared to very accurately predict the overall no. of claims, but this lift chart shows the error in the highest and lowest decile are offsetting each other to inaccurately predict an accurate sum.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#double_lift_chart function\n#to plot double lift chart, predicted colm1, predicted colm2 actual colm and weights colm take just the predictions_df colm names only, q is no. of quartiles\ndef double_lift_chart(predicted_colm1,predicted_colm2, actual_colm, weights_colm,q,y_min=0,y_max=0.18):\n    predictions_df['ratio'] = predictions_df[predicted_colm1]/predictions_df[predicted_colm2]\n    predictions_df.sort_values('ratio',inplace = True)\n    def weighted_qcut(predicted_colm1, predicted_colm2, weights_colm, q):\n        #Return weighted quantile cuts from a given series\n        if is_integer(q):\n            quantiles = np.linspace(0, 1, q + 1)\n        else:\n            quantiles = q\n        order = predictions_df[weights_colm].iloc[predictions_df['ratio'].argsort()].cumsum() #makes series of cumulative exposure (sorted by values)\n        bins = pd.cut(order / order.iloc[-1], quantiles, labels=False) #cuts into q quantiles along order (cumulative exposure) column\n        return bins.sort_index() #makes column in line with index of original dataframe\n    #3 cut by weight\n    predictions_df['weighted_cut'] = weighted_qcut(predicted_colm1, predicted_colm2,weights_colm,10)\n    #4 function to make means and plot them\n    def means(predicted_colm1,predicted_colm2,actual_colm):\n        predicted_1_mean = []\n        predicted_2_mean = []\n        actual_mean = []\n        for x in np.arange(10):\n            pred = predictions_df[predictions_df['weighted_cut']==x][predicted_colm1].mean()\n            predicted_1_mean.append(pred)\n            pred = predictions_df[predictions_df['weighted_cut']==x][predicted_colm2].mean()\n            predicted_2_mean.append(pred)\n            actual = predictions_df[predictions_df['weighted_cut']==x][actual_colm].mean()\n            actual_mean.append(actual)\n        means = pd.DataFrame(data = list(zip(predicted_1_mean,predicted_2_mean,actual_mean)), columns = ['predicted 1','predicted 2','actual'])\n        sns.scatterplot(data=means,x='actual',y='actual',color='blue')\n        sns.scatterplot(data=means, x='actual',y='predicted 1',color = 'green')\n        sns.scatterplot(data=means, x='actual',y='predicted 2', color = 'red')\n        a = means.iloc[9]['actual'] / means.iloc[0]['actual']\n        b = means.iloc[9]['predicted 1'] / means.iloc[0]['predicted 1']\n        c = means.iloc[9]['predicted 2'] / means.iloc[0]['predicted 2']\n        print(predicted_colm1[:-12],'and',predicted_colm2[:-12], 'actual differentiation',a)\n        print(predicted_colm1[:-12], 'model differentiation',b)\n        print(predicted_colm2[:-12], 'model differentiation',c)\n    means(predicted_colm1,predicted_colm2,actual_colm)\n    plt.title(predicted_colm1 + ' (green) ' + predicted_colm2 + ' (red)')\n    plt.ylim(y_min,y_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(1,3,figsize=(20,4))\nplt.subplot(1,3,1)\ndouble_lift_chart('Random Forest Predictions','glm predictions','ClaimNb_x','Exposure_x',10)\nplt.subplot(1,3,2)\ndouble_lift_chart('Random Forest Predictions','gbm predictions','ClaimNb_x','Exposure_x',10)\nplt.subplot(1,3,3)\ndouble_lift_chart('glm predictions','gbm predictions','ClaimNb_x','Exposure_x',10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows that glm slightly better than rf, follows data better, but have very similar factor, so only marginally better.\n\nGBM better than RF (marginally) was more pronounced with past FREQ_pred data\nGBM only marginally better than GLM, again was more pronounced with past FREQ_pred data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#datasets for pdp\n#rf_model\nrf_X_train = pd.read_pickle('/kaggle/input/alex-farquharson-rf-dataframe/Alex_Farquharson_X_train_dataframe.gzip')\nrf_y_train = pd.read_pickle('/kaggle/input/alex-farquharson-rf-dataframe/Alex_Farquharson_y_train_dataframe.gzip')\nrf_features = ['VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'LogDensity', 'B', 'C', 'D','E', 'F', 'B10', 'B11', 'B12', 'B13', 'B14', 'B2', 'B3', 'B4', 'B5','B6', 'Regular',\n               'R21', 'R22', 'R23', 'R24', 'R25', 'R26', 'R31', 'R41','R42', 'R43', 'R52', 'R53', 'R54', 'R72', 'R73', 'R74', 'R82', 'R83','R91', 'R93', 'R94']\nrf_pdp_dataset = pd.concat((rf_X_train,rf_y_train),axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of models (SHAP and PDPs)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(gbm_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Weight - Number of times feature is used to split data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(gbm_model, importance_type=\"cover\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Cover - The number of times a feature is used to split the data weighted by number of training data points that go through those splits.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(gbm_model, importance_type=\"gain\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Gain - The average training loss reduction when using a feature for splitting.\n* order changes again - need robust understanding of how features affect output\n* nice to understand models relationship of each feature with the target variable - transparency","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"importances_rf=pd.Series(data=rf_model.feature_importances_,\n                      index=rf_X_train.columns).sort_values(ascending=False).iloc[:20]\nimportances_rf.plot(kind='bar')\nplt.title('Influence of Each Feature on the Model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* weight\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## PDP plots\n* plots average predicted outcome as the feature varies\n* illogical virtual results (e.g. drivage 18, Minimum BonusMalus score)\n* pdpbox only compatible with sklearn (xgboost if fitted with sklearn API)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using pdpbox module for PDP plots\n\nfrom pdpbox import pdp, info_plots\n#first plot target plot\n\nfeature_to_be_analysed = 'DrivAge'\nfeature_name = 'DrivAge'\ntarget_feature = 'ClaimNb'\n\nfig, axes, summary_df = info_plots.target_plot(df=rf_pdp_dataset,\n                                               feature=feature_to_be_analysed,\n                                               feature_name=feature_name,\n                                               target= target_feature,\n                                               num_grid_points = 20,\n                                               show_percentile=True,figsize= (20,8))\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* can't weight count by exposure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_features = ['VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'LogDensity', 'B', 'C', 'D','E', 'F', 'B10', 'B11', 'B12', 'B13', 'B14', 'B2', 'B3', 'B4', 'B5','B6', 'Regular','R21', 'R22', 'R23', 'R24', 'R25', 'R26', 'R31', 'R41','R42', 'R43', 'R52', 'R53', 'R54', 'R72', 'R73', 'R74', 'R82', 'R83','R91', 'R93', 'R94']\n\n#get model partial dependency values\npdp_features = pdp.pdp_isolate(model=rf_model, dataset=rf_pdp_dataset, model_features=list_of_features, feature='DrivAge', num_grid_points=20)\n\n#plot values\nfig, axes = pdp.pdp_plot(pdp_features, feature_name = 'DrivAge', plot_lines=True, frac_to_plot=50, plot_pts_dist=False,figsize=(20,8))\naxes['pdp_ax'].set_ylim(-0.025, 0.010)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Effect of feature on target (y)\n* Expected U plot, model doesn't show any variation in ClaimNb for ages 50+\n* just shows averages, no y dispersion, if another feature interacts very strongly with DrivAge this isnt shown","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Interactions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(rf_X_train[['VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'LogDensity']].corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* correlation between BonusMalus and DriverAge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#two way plots\nfig,axes,summary_df = info_plots.target_plot_interact(df=rf_pdp_dataset,\n                                               features=['DrivAge','BonusMalus'],\n                                               feature_names=['DrivAge','BonusMalus'],\n                                               target= target_feature,\n                                               show_percentile=True,figsize= (20,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Stronger effet of BonusMalus on ClaimNb at higher ages\n* As you can see a one way pdp plot doesnt give us everything we would like to see, takes the average effect of a variable over all features\n* If we have interactions it would be useful to see if our model makes logcial sense of them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pdp_features_interact = pdp.pdp_interact(model=rf_model, dataset=rf_pdp_dataset, model_features=list_of_features,\n                                         features=['DrivAge','BonusMalus'])\n\n#plot values\nfig, axes = pdp.pdp_interact_plot(pdp_features_interact, feature_names = ['DrivAge','BonusMalus'],figsize=(20,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 way plot of drivage and bonusmalus vs. predicted ClaimNb\n* as previous PDP plot suggests, no change in ClaimNb prediction with age > 50\n* model doesnt increase claimNb with BonusMalus more as age increases - model doesnt pick up subtleties - maybe due to too big leaf bucket size\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get datasets for pdp and model \n#model is rf_model\nfrom pdpbox import pdp, info_plots\nrf_X_train = pd.read_pickle('/kaggle/input/alex-farquharson-rf-dataframe/Alex_Farquharson_X_train_dataframe.gzip')\nrf_y_train = pd.read_pickle('/kaggle/input/alex-farquharson-rf-dataframe/Alex_Farquharson_y_train_dataframe.gzip')\nrf_features = ['VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'LogDensity', 'B', 'C', 'D','E', 'F', 'B10', 'B11', 'B12', 'B13', 'B14', 'B2', 'B3', 'B4', 'B5','B6', 'Regular',\n               'R21', 'R22', 'R23', 'R24', 'R25', 'R26', 'R31', 'R41','R42', 'R43', 'R52', 'R53', 'R54', 'R72', 'R73', 'R74', 'R82', 'R83','R91', 'R93', 'R94']\nrf_pdp_dataset = pd.concat((rf_X_train,rf_y_train),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rf_pdp_plot(features,feature_name,fraction_to_plot = 10, plot_dist = False,fig_size=(20,8)):\n    pdp_features = pdp.pdp_isolate(model=rf_model, dataset=rf_pdp_dataset, model_features=rf_features, feature=features)\n    fig, axes = pdp.pdp_plot(pdp_features, feature_name = feature_name, plot_lines=True, frac_to_plot=fraction_to_plot, plot_pts_dist=plot_dist,figsize=fig_size)\n    \ndef rf_target_plot(features, feature_name,fig_size = (20,8)):\n    fig, axes, summary_df = info_plots.target_plot(df=rf_pdp_dataset, feature=features,\n                                               feature_name=feature_name, target='ClaimNb', show_percentile=True,figsize=fig_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes, summary_df = info_plots.target_plot(df=rf_pdp_dataset, feature='BonusMalus',feature_name='BonusMalus', target='ClaimNb', num_grid_points=15, show_percentile=True)\n\npdp_features = pdp.pdp_isolate(model=rf_model, dataset=rf_pdp_dataset, model_features=rf_features, feature='BonusMalus',  num_grid_points=15)\nfig, axes = pdp.pdp_plot(pdp_features, feature_name = 'BonusMalus', plot_lines=True, frac_to_plot=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* spike could be starting BonusMalus no. - coorelation iwth age\n* model picks up on ClaimNb spike at BonusMalus 65, overfitting to BonusMalus?\n* importances plot, Bonus Malus by far  most influential feature\n* BonusMalus <100 account for 99 % of data\n* BonusMalus outliers affecting model - all other featues effects are negligible if BonusMalus >100.\n* compared with target plot, change of 0.04 compared to 0.1 in model, albeit traget plot data is bucketed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(rf_pdp_dataset['BonusMalus'],kde=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"axes['pdp_ax'].set_xlim(50,125)\naxes['pdp_ax'].set_ylim(0,0.05)\nfig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_target_plot(features = ['B10', 'B11', 'B12', 'B13', 'B14', 'B2', 'B3', 'B4', 'B5','B6'],feature_name = 'VehType')\nrf_pdp_plot(features = ['B10', 'B11', 'B12', 'B13', 'B14', 'B2', 'B3', 'B4', 'B5','B6'],feature_name = 'VehType')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* pdpbox works with categorical/binary/numerical data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_target_plot(features = 'LogDensity',feature_name = 'LogDensity')\nrf_pdp_plot(features = 'LogDensity',feature_name = 'LogDensity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scale is not proportional to target, overall change of 0.012, but model predicts change by 0.006.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_X_validation = pd.read_pickle('/kaggle/input/alex-farquharson-rf-dataframe/Alex_Farquharson_X_validation_dataframe.gzip')\ngbm_X_test = gbm_df_with_columns.drop(['Exposure','ClaimNb','IDpol','freq','pred_ClaimNb','gbm predictions'],axis=1)\ngbm_y_test = gbm_df_with_columns['ClaimNb']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SHAP values\n* takes an instance, measures contribution (+ve, -ve) of a certain factor on the prediction made\n* no intervention, no averaging, no illogical results, \n* in a shap dependence plot a lot easier to see interaction effects","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\n#for jupyter notebooks\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgboost model\nD_test = xgb.DMatrix(gbm_X_test,gbm_y_test)\nexplainer = shap.TreeExplainer(gbm_model)\nshap_values = explainer.shap_values(D_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rf model\nexplainer_rf = shap.TreeExplainer(rf_model)\nshap_values_rf  = explainer_rf.shap_values(rf_X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value,shap_values[10,:],gbm_X_test.iloc[10,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* takes base value (mean prediction, if no feature values known)\n* shap values\n* feature values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer_rf.expected_value,shap_values_rf[10,:],rf_X_validation.iloc[10,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('SHAP summary plot - XGBoost')\nshap.summary_plot(shap_values,gbm_X_test)\n\nplt.title('SHAP summary plot - Random Forest')\nshap.summary_plot(shap_values_rf,rf_X_validation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* feature contribution to prediction for each individual row - more robust than importance plots (vary if change tree e.g. root split)\n* Comparison of plots - logical sense?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('DrivAge',shap_values,gbm_X_test)\nshap.dependence_plot('DrivAge',shap_values_rf,rf_X_validation, interaction_index = 'BonusMalus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* shap dependence plots (XGBoost and RF)\n* not just average output, get vertical dispersion\n* can show feature interaction as hue\n* XGBoost shows larger effect of BonusMalus on ClaimNb at high ages - in line with dataset\n* Random Forest predicts lower age drivers are riskier - in line with dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot('BonusMalus',shap_values,gbm_X_test, interaction_index = 'VehBrand_B12')\nshap.dependence_plot('BonusMalus',shap_values_rf,rf_X_validation, interaction_index = 'B12')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pdp with glm model\ngbm_pdp = gbm_df_with_columns.drop(['IDpol','Exposure','freq','pred_ClaimNb','gbm predictions'],axis=1)\ngbm_features = list(gbm_pdp.columns)\ngbm_features.remove('ClaimNb')\n\ndef glm_pdp_plot(features,feature_name,fraction_to_plot = 10, plot_dist = False,fig_size=(20,8)):\n    pdp_features = pdp.pdp_isolate(model=glm_model, dataset=glm_pdp, model_features=glm_features, feature=features)\n    fig, axes = pdp.pdp_plot(pdp_features, feature_name = feature_name, plot_lines=True, frac_to_plot=fraction_to_plot, plot_pts_dist=plot_dist,figsize=fig_size)\n    \ndef gbm_target_plot(features, feature_name,fig_size = (20,8)):\n    fig, axes, summary_df = info_plots.target_plot(df=glm_pdp, feature=features,\n                                               feature_name=feature_name, target='Freq', show_percentile=True,figsize=fig_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glm_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#deviance\ndef poisson_deviance(y,p):\n    d = -2*np.where(y==0,-(y-p),(y*np.log(y/p))-(y-p))\n    deviance = sum(d)\n    return(deviance)\n\ndef proportion_deviance_explained(y,p,w):\n    deviance = poisson_deviance(y,p)\n    null_deviance = poisson_deviance(y, w * np.sum(y) / np.sum(w))\n    propn_dev_expl  = (null_deviance - deviance) / null_deviance\n    return (deviance, null_deviance, propn_dev_expl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev_list=[]\nfor x in ['Random Forest Predictions','glm predictions','gbm predictions']:\n    dev_list.append(proportion_deviance_explained(predictions_df['ClaimNb_x'],predictions_df[x],predictions_df['Exposure_x']))\n\nlst = ['Random Forest Predictions','glm predictions','gbm predictions']\npd.DataFrame(data=dev_list, index = lst,columns = ('deviance', 'null_deviance', 'propn_dev_expl'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"deviance is a measure of how good the model fits to the data, the closer deviance is to 0 and the closer the proportion deviance explained to 1, the closer the model fits the data.\nExpect small values as error in individual predictions of model is always going to be large. e.g. (1 - 0.2). GBM model still appears to be the best.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}