{"cells":[{"metadata":{},"cell_type":"markdown","source":"****Prediction on who survived in Titanic Disaster-1912****\n\nQuestions below:\n\nThe data has been split into two groups:\n\n* training set (train.csv)\n* test set (test.csv)\n\n\n\n* Training set:-  we have a total of 891 entries for training (12 columns)\n* Test set :- 417 entries for testing (11 columns)\n* used :  Pandas, a data manipulation library in python\n\nwith the given data, we have to predict the survived people in Titanic Disaster.\n\n**Variable Notes**\n\n* pclass: A proxy for socio-economic status (SES)\n       1st = Upper\n       2nd = Middle\n       3rd = Lower\n* survival - Survival (0 = No; 1 = Yes)\n* class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n* name - Name\n* sex - Sex\n* age - Age\n* sibsp - Number of Siblings/Spouses Aboard\n* parch - Number of Parents/Children Aboard\n* ticket - Ticket Number\n* fare - Passenger Fare\n* cabin - Cabin\n* embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)"},{"metadata":{},"cell_type":"markdown","source":"**Step 1: Importing Library**\n\nLibraries i used:\n* Pandas :-  for data manipulation and analysis\n* Numpy  :-  adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n* Matplotlib :-  It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+.\n* Seaborn   :-  statistical data visualization\n* DecisionTreeClassifier :- to create a model that predicts the value of a target variable based on several input variables."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#libraries\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 2 :- Loading the csv files**\n\nWe are loading the given two csv files."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training (891 Entries) & Testing (418 Entries) data\ntrain_data = pd.read_csv('../input/test-dataset-for-titanic-competition/titanic_train.csv')\ntest_data = pd.read_csv('../input/test-dataset-for-titanic-competition/titanic_test.csv')\nall_data = [train_data, test_data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to know the rows and column of a train_data\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training (891 Entries)\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To get top 5 enteries of train_data\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to see how many null value in Train_data set\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to know the rows and column of a test_data\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Testing (418 Entries)\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To know the number of null values in each column\ntest_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bar Chart Function**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bar chat function\ndef bar_chart(feature):\n    survived = train_data[train_data['Survived']==1][feature].value_counts()\n    dead= train_data[train_data['Survived']==0][feature].value_counts()\n    df=pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True,figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 3 : Going through each column**\n\nwe now will go through each column and ananlyse it, if we could use it for creating new ones that can make a significant improvement in our output ."},{"metadata":{},"cell_type":"markdown","source":"Feature 1: Pclass\nPclass contains three classes , class1 , class2 and class3 in which\nclass1 is more expensive than class2 follow by class3.\nHence, the important and more valuable people life are saved first .\nclass1 people survived more than class2 followed by class3."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 1: Pclass\nprint( train_data[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean() )\nbar_chart('Pclass')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature 2 : Sex\n\nUndoubtfully Female has survived more than male since female and children are saved first."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 2: Sex\nprint( train_data[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean() )\nbar_chart('Sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature 3: Family\n\nFamily = No. of siblings + No. of ParentsChildren + 1(himself)\n\nwe hav taken two column SibSp and Parch and added them to get Family size plus the person himself"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 3: Family\nfor data in all_data:\n    data['family_size']=data['SibSp']+data['Parch']+1\n\n#print(train_data[[\"family_size\",\"Survived\"]].groupby([\"family_size\"],as_index=False).mean())\nbar_chart('family_size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These output are not much helping us to analyse anything , lets take the output if a person is alone , he survived or not."},{"metadata":{},"cell_type":"markdown","source":"Feature 3.1  : is_alone?\nif family size is 1 that is himself then he is alone."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 3.1: is alone?\n\nfor data in all_data:\n    data['is_alone']=0\n    data.loc[data['family_size']==1,'is_alone']=1\n\n#print(train_data[['is_alone','Survived']].groupby(['is_alone'],as_index=False).mean())\nbar_chart('is_alone')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Feature 4 : Embarked part 1 \n\nAs Embarked port are divided into S, C and Q (full form mentioned in the beginning)\n\nWe can count how many number of people from S , C and Q went to Class1 of Pclass respectively,\nsame for class2 and class3"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 4: Embarked part 1\n\n\nPclass1 = train_data[train_data['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train_data[train_data['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train_data[train_data['Pclass']==3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['lst class','2nd class','3rd class']\n\n#print(train_data[[\"Embarked\",\"Survived\"]].groupby([\"Embarked\"],as_index=False).mean())\ndf.plot(kind='bar',stacked=True,figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we can see , maximum no. of people from S has gone to class3 and almost half of that in class2.\nWe could analyse from this that more no. of people died in class3 were from S, since class3 people contain maximum no. of dead people.\nand similar analysis for class2 and class3 as well who were from S."},{"metadata":{},"cell_type":"markdown","source":"#Feature 4 : Embarked part 2\nfill the null values with S , as there are maximum no. of people who were from S."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 4: Embarked part 2\nfor data in all_data:\n    data['Embarked']=data['Embarked'].fillna('S')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature 5 : Fare\nThe people who paid higher has maximum chances of getting Survived,\nbut these same results we can get from Pclass as well as Pclass also talks about Fare\nhigher paid - class1\naverage paid - class2\nlow paid - class3\n\nSo, this column may not help to analyse."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 5: Fare\nfor data in all_data:\n    data['Fare'] = data['Fare'].fillna(data['Fare'].median())\n    \ntrain_data['category_fare']=pd.qcut(train_data['Fare'],4)\n\nprint(train_data[[\"category_fare\",\"Survived\"]].groupby([\"category_fare\"],as_index=False).mean())\nbar_chart('category_fare')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Feature 6: Name part 1\nNow, here comes the most interesting part, where we takes tha Saluatation of the Name.\n\nMost Common Saluatation : Mrs. , Mr. , Miss , Master ,Other\nwe have name it as Title.\nSo, we divide the Name list according to its Saluataion and count the number of each Saluatation present .\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 6: Name part 1\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\. ', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor data in all_data:\n    data['title'] = data['Name'].apply(get_title)\n\ndata['title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Feature 6: Name part 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 6: Name part 2\n\n#replacing every title with the common title \nfor data in all_data:\n    data['title'] = data['title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],'Rare')\n    data['title'] = data['title'].replace('Mlle','Miss')\n    data['title'] = data['title'].replace('Ms','Miss')\n    data['title'] = data['title'].replace('Mme','Mrs')\n    \n#We compute the name title with Sex.\nprint(pd.crosstab(train_data['title'], train_data['Sex']))\nprint(\"----------------------\")\n\nprint(train_data[['title','Survived']].groupby(['title'], as_index = False).mean())\nbar_chart('title')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature 7: Age \nSince there are many null in this Column, So we fill null with any random number which comes between difference of average age and standard deviation and sums of average age and standard deviation.\nAnd Categories age in 5 parts."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature 7: Age\n#train_data['Age'].fillna(train_data.groupby(\"title\")[\"Age\"].transform(\"median\"), inplace=True)\nfor data in all_data:\n    age_avg  = data['Age'].mean()\n    age_std  = data['Age'].std()\n    age_null = data['Age'].isnull().sum()\n\n    random_list = np.random.randint(age_avg - age_std, age_avg + age_std , size = age_null)\n    data['Age'][np.isnan(data['Age'])] = random_list\n    data['Age'] = data['Age'].astype(int)\n\ntrain_data['category_age'] = pd.cut(train_data['Age'], 5)\nprint( train_data[[\"category_age\",\"Survived\"]].groupby([\"category_age\"], as_index = False).mean() )\nbar_chart('category_age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Mapping Data\n\nMachine Learning only takes numerical values and not strings , so, every entry must be converted to integer.\nSo, we map every string entry to integer.\n\nAnd also drop unwanted columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Map Data\nfor data in all_data:\n\n    #Mapping Sex\n    sex_map = { 'female':0 , 'male':1 }\n    data['Sex'] = data['Sex'].map(sex_map).astype(int)\n\n    #Mapping Title\n    title_map = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Rare':5}\n    data['title'] = data['title'].map(title_map)\n    data['title'] = data['title'].fillna(0)\n\n    #Mapping Embarked\n    embark_map = {'S':0, 'C':1, 'Q':2}\n    data['Embarked'] = data['Embarked'].map(embark_map).astype(int)\n\n    #Mapping Fare\n    data.loc[ data['Fare'] <= 7.91, 'Fare']                            = 0\n    data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\n    data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare']   = 2\n    data.loc[ data['Fare'] > 31, 'Fare']                               = 3\n    data['Fare'] = data['Fare'].astype(int)\n\n    #Mapping Age\n    data.loc[ data['Age'] <= 16, 'Age']                       = 0\n    data.loc[(data['Age'] > 16) & (data['Age'] <= 32), 'Age'] = 1\n    data.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\n    data.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\n    data.loc[ data['Age'] > 64, 'Age']                        = 4\n\n#Feature Selection\n#Create list of columns to drop\ndrop_elements = [\"Name\", \"Ticket\", \"Cabin\", \"SibSp\", \"Parch\"]\n\n#Drop columns from both data sets\ntrain_data = train_data.drop(drop_elements, axis = 1)\ntrain_data = train_data.drop(['PassengerId','category_fare', 'category_age'], axis = 1)\ntest_data = test_data.drop(drop_elements, axis = 1)\n\n#Print ready to use data\nprint(train_data.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Prediction\n\nwe need to train our model. To do that, we need to provide data in two parts — X and Y.\n\n* X : X_train : Contains all the features\n* Y : Y_train : Contains the actual output (Survived)\n\nwe need to tell our model that we are looking for this output. Just like we shop online and if the dress gets out of stock , we search for similar dress, \"hey, i want similar kind dress\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction\n#Train and Test data\nX_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Running our classifier\n\nWe have data separated, now we call our classifier, fit data (training) with help of .fit method of the scikit-learn library, and predict the output on testing data, with .predict method.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Running our classifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\naccuracy = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(\"Model Accuracy: \",accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Creating a CSV with results\n\nThe output submission.csv file contain only two columns — Passenger Id and Survived — as mentioned on the competition page."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a CSV with results\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_data[\"PassengerId\"],\n    \"Survived\": Y_pred\n})\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**References:**\n\n* 1. [Predict Who Survived the Titanic Disaster](https://towardsdatascience.com/your-first-kaggle-competition-submission-64da366e48cb)\n* 2. [User Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html)\n* 3. [Kaggle - Titanic Solution [1/3] - data analysis](https://www.youtube.com/watch?v=3eTSVGY_fIE&t=31s)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}