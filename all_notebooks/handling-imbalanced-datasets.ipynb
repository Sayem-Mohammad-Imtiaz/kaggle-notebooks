{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this, We are going to Handle the Imbalanced Datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1) Under Sampling \n\n2) Over Sampling\n\n3) Smote technique\n\n4) Ensemble technique","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\n\nwarnings. simplefilter(action = \"ignore\", category = Warning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard/creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#It is highly imbalanced","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# First Find the model accuracy by keeping the variable as it is","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(columns = ['Class'])\nY = df[['Class']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y, train_size = 0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier()\n\nclassifier.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = classifier.predict(X_test)\nprint(confusion_matrix(Y_test,Y_pred))\nprint(accuracy_score(Y_test,Y_pred))\nprint(classification_report(Y_test, Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because of the unbalanced dataset. we dont need to concentrate on the Accuracy. It biased towards one category.\n\nWe need to focus on Precision and Recall.\n\nIt looks good(bcz precision is 0.92). we will see what will happend after sampling techniques.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Under Sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import NearMiss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ns = NearMiss(0.7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_ns, Y_train_ns = ns.fit_sample(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_train['Class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_train_ns['Class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NearMiss of 0.7 explains that 512 * 0.7 = 361","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Performing randomforest to find the result","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I am going to sample the test data also","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier()\nclassifier.fit(X_train_ns,Y_train_ns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_ns, Y_test_ns = ns.fit_sample(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = classifier.predict(X_test_ns)\nprint(confusion_matrix(Y_test_ns, Y_pred))\nprint(accuracy_score(Y_test_ns,Y_pred))\nprint(classification_report(Y_test_ns,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"its also looks good(Precision and Recall Increases) if we under sampling the test data.\n\nBut we loose more informations and data's\n\nIt is good only if we have the less number of the datasets.\n\nlet see, what happend if we are not implementing the under sampling to the test data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I need to find the result without sampling the test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(Y_test, Y_pred))\nprint(accuracy_score(Y_test,Y_pred))\nprint(classification_report(Y_test,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks very bad\n\nMost of the higher data cases, under sampling doesnt work","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Over Sampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os = RandomOverSampler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_os, Y_train_os = os.fit_sample(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_train['Class'].value_counts())\nprint(Y_train_os['Class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### without giving any sampling strategy, it equalizes the variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train_os,Y_train_os)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By giving sampling strategy of 0.5\nos = RandomOverSampler(0.5)\nX_train_os, Y_train_os = os.fit_sample(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### After the sampling strategy of giving 0.5, ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train_os,Y_train_os)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we over sampling the test data also, ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_os,Y_test_os = os.fit_sample(X_test, Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test_os)\nprint(confusion_matrix(Y_test_os,y_pred))\nprint(accuracy_score(Y_test_os,y_pred))\nprint(classification_report(Y_test_os,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best sampling technique for the particular dataset is only based on the results. \n\nTrail and error method.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Appling SMOTE method","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.combine import SMOTETomek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTETomek() #first try without giving the sampling strategy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm, Y_train_sm = sm.fit_sample(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_train['Class'].value_counts())\nprint(Y_train_sm['Class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = RandomForestClassifier()\nclassifier.fit(X_train_sm,Y_train_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\n\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It gives much better for this dataset.\n\nBecause Precision and Recall is almost good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#trying with giving the sampling strategy\n\nsm = SMOTETomek(0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm,Y_train_sm = sm.fit_sample(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y_train['Class'].value_counts())\nprint(Y_train_sm['Class'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\nclassifier.fit(X_train_sm,Y_train_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\n\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensembled Techniques","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.ensemble import EasyEnsembleClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"easy = EasyEnsembleClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"easy.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = easy.predict(X_test)\n\nprint(confusion_matrix(Y_test,y_pred))\nprint(accuracy_score(Y_test,y_pred))\nprint(classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### By changing the parameters in the Regression models are also sometimes useful\n\n### (i.e) Cross Validation with Kfold & Tuning the Hyper parameters with the model In LOR and also in LR","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"for more...\n\n***Handling Categorical data's (Feature Engineering)***\n\nhttps://www.kaggle.com/ganeshbalaji1608/handling-categorical-data-s-feature-engineering\n\n***Handling Missing Categories (Feature Engineering)***\n\nhttps://www.kaggle.comhandling-missing-categories-feature-engineering\n\n***Handling Missing Numerical Features(Feature Engineering)***\n\nhttps://www.kaggle.comhandling-missing-numerical-features-f-e\n\n***Handling Outliers***\n\nhttps://www.kaggle.comhandling-outliers-feature-engieering\n\n***Transformations (Feature Scaling)***\n\nhttps://www.kaggle.comtransformation-techniques-feature-scaling","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}