{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction - Problem Definition\n\nThe focus of this project will be the Sonar Mines vs Rocks dataset. The problem is to predict metal or rock objects from sonar return data. Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The label associated with each record contains the letter R if the object is a rock and M if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.\n\nWe are going to cover the following steps:\n1. Load the Dataset (Import libraries and Load dataset)\n2. Analyze Data (Descriptive Statistics, Unimodal and Multimodel Data Visualizations)\n3. Validation Dataset\n4. Evaluate Algorithms: Baseline\n5. Evaluate Algorithms: Standardize Data\n6. Algorithm Tuning\n7. Ensemble Methods\n8. Finalize Model\n9. Summary\n10. Reference\n\n<u>Goal</u>: Predict metal or rock objects from sonar return data\n\n# 1. Load the Dataset\n\nLet's start off by loading the libraries required for this project.\n\n## 1.1 Import libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Load dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfilename = '/kaggle/input/mines-vs-rocks/sonar.all-data.csv'\ndataset = read_csv(filename, header=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are not specifying the names of the attributes. This is because other than the class attribute (the last column), the variables do not have meaningful names. We also indicate that there is no header information, this is to avoid file loading code taking the first record as the column names. Now that we have the dataset loaded we can take a look at it.\n\n# 2. Analyze Data\n\nLet's take a closer look at our loaded data.\n\n## 2.1 Descriptive Statistics\n\nWe will start off by confirming the dimensions of the dataset, e.g. the number of rows and columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape\nprint(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 208 instances to work with and can confirm the data has 61 attributes including the class attribute. \n\nLet's also look at the data types of each attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# types\nset_option('display.max_rows', 500)\nprint(dataset.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all of the attributes are numeric (float) and that the class value has been read in as an object.\n\nLet's now take a peek at the first 5 rows of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# head\nset_option('display.width', 100)\nprint(dataset.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This does not show all of the columns, but we can see <u>all of the data has the same scale</u>. We can also see that the class attribute (60) has string values.\n\nLet's summarize the distribution of each attribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# descriptions, change precision to 3 places\nset_option('precision', 3)\nprint(dataset.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, as we expect, the data has the same range, but interestingly <u>differing mean values</u>. There <u>may be some benefit from standardizing the data</u>.\n\nLet's take a quick look at the breakdown of class values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# class distribution\nprint(dataset.groupby(60).size())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the classes are reasonably balanced between M (mines) and R (rocks).\n\n## 2.2 Unimodal Data Visualizations\n\nLet's look at visualizations of individual attributes. It is often useful to look at our data using multiple different visualizations in order to spark ideas. Let's look at histograms of each attribute to get a sense of the data distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# histograms\ndataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are a lot of Gaussian-like distributions and perhaps some exponential-like distributions for other attributes.\n\nLet's take a look at the same perspective of the data using density plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"# density\ndataset.plot(kind='density', subplots=True, layout=(8,8), sharex=False, legend=False, fontsize=1)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<u>Inference</u>:\n* We can see that many of the attributes have a skewed distribution. \n* A power transform like a Box-Cox transform that can correct for the skew in distributions might be useful.\n\nIt is always good to look at box and whisker plots of numeric attributes to get an idea of the spread of values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# box and whisker plots\ndataset.plot(kind='box', layout=(8,8), sharex=False, sharey=False, fontsize=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that attributes do have quite different spreads. <u>Given the scales are the same, it may suggest some benefit in standardizing the data for modeling to get all of the means lined up</u>.\n\n## 2.3 Multimodal Data Visualizations\n\nLet's visualize the correlations between the attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')\nfig.colorbar(cax)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<u>Inference</u>: \n* It looks like there is also some structure in the order of the attributes. \n* The yellow around the diagonal suggests that attributes that are next to each other are generally more correlated with each other. \n* The purple patches also suggest some moderate negative correlation the further attributes are away from each other in the ordering. \n* This makes sense if the order of the attributes refers to the angle of sensors for the sonar chirp.\n\n# 3. Validation Dataset\n\nIt is a good idea to use a validation hold-out set. This is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to confirm the accuracy of our final model. It is a smoke test that we can use to see if we messed up and to give us confidence on our estimates of accuracy on unseen data. We will use 80% of the dataset for modeling and hold back 20% for validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split-out validation dataset\narray = dataset.values\nX = array[:,0:60].astype(float)\nY = array[:,60]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\ntest_size=validation_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Evaluate Algorithms: Baseline\n\nWe don't know what algorithms will do well on this dataset. Gut feel suggests distance based algorithms like k-Nearest Neighbors and Support Vector Machines may do well. Let's design our test harness. We will use 10-fold cross validation. The dataset is not too small and this is a good standard test harness configuration. We will evaluate algorithms using the accuracy metric. This is a gross metric that will give a quick idea of how correct a given model is. More useful on binary classification problems like this one."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'accuracy'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a baseline of performance on this problem and spot-check a number of different algorithms. We will select a suite of different algorithms capable of working on this classification problem. The six algorithms selected include:\n* Linear Algorithms: Logistic Regression (LR) and Linear Discriminant Analysis (LDA).\n* Nonlinear Algorithms: Classification and Regression Trees (CART), Support Vector Machines (SVM), Gaussian Naive Bayes (NB) and k-Nearest Neighbors (KNN)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The algorithms all use default tuning parameters. Let's compare the algorithms. We will display the mean and standard deviation of accuracy for each algorithm as we calculate it and collect the results for use later."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<u>Inference</u>:  \n* The results suggest that k-Nearest Neighbors, Support Vector Machines and Logistic Regression may be worth further study.\n\nThese are just mean accuracy values. It is always wise to look at the distribution of accuracy values calculated across cross validation folds. We can do that graphically using box and whisker plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<u>Inference</u>:\n* The results show a tight distribution for KNN which is encouraging, suggesting low variance.\n\nIt is possible that the <u>varied distribution of the attributes is having an effect on the accuracy of algorithms such as SVM</u>. \n\nIn the next section we will repeat this spot-check with a standardized copy of the training dataset.\n\n# 5. Evaluate Algorithms: Standardize Data\n\nWe suspect that the differing distributions of the raw data may be negatively impacting the skill of some of the algorithms. Let's evaluate the same algorithms with a standardized copy of the dataset. This is where the data is transformed such that each attribute has a mean value of zero and a standard deviation of one. We also need to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<u>Inference</u>: \n* We can see that KNN is still doing well, even better than before. \n* We can also see that the standardization of the data has lifted the skill of SVM to be the most accurate algorithm tested so far.\n\nAgain, we should plot the distribution of the accuracy scores using box and whisker plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<u>Inference</u>:\n* The results suggest digging deeper into the SVM and KNN algorithms. \n* It is very likely that configuration beyond the default may yield even more accurate models.\n\n# 6. Algorithm Tuning\n\nIn this section we investigate tuning the parameters for two algorithms that show promise from the spot-checking in the previous section: KNN and SVM.\n\n## 6.1 Tuning KNN\n\nWe can start off by tuning the number of neighbors for KNN. The default number of neighbors is 7. Below we try all odd values of k from 1 to 21, covering the default value of 7. Each k value is evaluated using 10-fold cross validation on the training standardized dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune scaled KNN\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nneighbors = [1,3,5,7,9,11,13,15,17,19,21]\nparam_grid = dict(n_neighbors=neighbors)\nmodel = KNeighborsClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have printed the configuration that resulted in the highest accuracy as well as the accuracy of all values tried.\n\nWe can see that the optimal configuration is K=1. This is interesting as the algorithm will make predictions using the most similar instance in the training dataset alone.\n\n## 6.2 Tuning SVM\n\n* We can tune two key parameters of the SVM algorithm, the value of C (how much to relax the margin) and the type of kernel. \n* The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. \n* Like with KNN, we will perform a grid search using 10-fold cross validation with a standardized copy of the training dataset. \n* We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune scaled SVM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have printed the best configuration, the accuracy as well as the accuracies for all configuration combinations.\n\n<u>Inference</u>\n* We can see the most accurate configuration was SVM with an RBF kernel and a C value of 1.5. \n* The accuracy 86.7470% is seemingly better than what KNN could achieve.\n\n# 7. Ensemble Methods\n\nAnother way that we can improve the performance of algorithms on this problem is by using ensemble methods. In this section we will evaluate four different ensemble machine learning algorithms, two boosting and two bagging methods:\n* Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).\n* Bagging Methods: Random Forests (RF) and Extra Trees (ET).\n\nWe will use the same test harness as before, 10-fold cross validation. <u>No data standardization is used in this case because all four ensemble algorithms are based on decision trees that are less sensitive to data distributions</u>."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensembles\nensembles = []\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\nensembles.append(('RF', RandomForestClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above accuracy scores are available to us.\n\n<u>Inference</u>:\n* We can see that both boosting techniques provide strong accuracy scores in the low 80s (%) with default configurations. \n\nWe can plot the distribution of accuracy scores across the cross validation folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results suggest ET may be worthy of further study, with a strong mean and a spread that skews up towards high 90s (%) in accuracy.\n\n# 8. Finalize Model\n\nThe SVM showed the most promise as a low complexity and stable model for this problem. In this section we will finalize the model by training it on the entire training dataset and make predictions for the hold-out validation dataset to confirm our findings. A part of the findings was that SVM performs better when the dataset is standardized so that all attributes have a mean value of zero and a standard deviation of one. We can calculate this from the entire training dataset and apply the same transform to the input attributes from the validation dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = SVC(C=1.5)\nmodel.fit(rescaledX, Y_train)\n# estimate accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we achieve an accuracy of nearly 86% on the held-out validation dataset. A score that matches closely to our expectations estimated above during the tuning of SVM.\n\n# 9. Summary\n\nWe have covered the following points:\n* Problem Definition (Sonar return data).\n* Loading the Dataset.\n* Analyze Data (same scale but different distributions of data).\n* Evaluate Algorithms (SVM and KNN looked good).\n* Evaluate Algorithms with Standardization (SVM and KNN looked good).\n* Algorithm Tuning (K=1 for KNN was good, SVM with an RBF kernel and C=1.5 was best).\n* Ensemble Methods (Bagging and Boosting, not quite as good as SVM).\n* Finalize Model (use all training data and confirm using validation dataset).\n\n# 10. References\n\n* Thank you to Jason Brownlee https://machinelearningmastery.com/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}