{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndata = pd.read_csv('../input/newagustus/new.csv')\ndata = data.drop(['Unnamed: 0'],axis=1)\ndata.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:43:52.95491Z","iopub.execute_input":"2021-08-11T10:43:52.955449Z","iopub.status.idle":"2021-08-11T10:43:53.740341Z","shell.execute_reply.started":"2021-08-11T10:43:52.95536Z","shell.execute_reply":"2021-08-11T10:43:53.739215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndata['label'] = labelencoder.fit_transform(data['label'])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:43:53.742266Z","iopub.execute_input":"2021-08-11T10:43:53.743304Z","iopub.status.idle":"2021-08-11T10:43:54.783653Z","shell.execute_reply.started":"2021-08-11T10:43:53.743251Z","shell.execute_reply":"2021-08-11T10:43:54.782503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(data.AF3);","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:43:54.785252Z","iopub.execute_input":"2021-08-11T10:43:54.785542Z","iopub.status.idle":"2021-08-11T10:43:55.076499Z","shell.execute_reply.started":"2021-08-11T10:43:54.785516Z","shell.execute_reply":"2021-08-11T10:43:55.075495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def window_data(df, window, feature_col_number, target_col_number):\n    X = []\n    y = []\n    for i in range(len(df) - window - 1):\n        features = df.iloc[i : (i + window), feature_col_number].values\n        target = df.iloc[(i + window), target_col_number]\n        X.append(features)\n        y.append(target)\n    return np.array(X), np.array(y).astype(np.float64).reshape(-1, 1)\n\nx,y = window_data(data,50,0,14)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:43:55.078229Z","iopub.execute_input":"2021-08-11T10:43:55.078559Z","iopub.status.idle":"2021-08-11T10:44:11.729041Z","shell.execute_reply.started":"2021-08-11T10:43:55.078527Z","shell.execute_reply":"2021-08-11T10:44:11.728013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize =(10,10))\nax = fig.add_subplot(1, 1, 1, projection='3d')\ndatawind = (pd.DataFrame(x)).sample(frac=0.1)\ndf=(datawind).unstack().reset_index()\ndf.columns=[\"X\",\"Y\",\"Z\"]\ndf['X']=pd.Categorical(df['X'])\ndf['X']=df['X'].cat.codes\nax.plot_trisurf(df['Y'], df['X'], df['Z'], cmap=plt.cm.plasma,linewidth=0.1,antialiased=True,shade=False);\n  ","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:47.455976Z","iopub.execute_input":"2021-08-11T10:44:47.458027Z","iopub.status.idle":"2021-08-11T10:53:10.948141Z","shell.execute_reply.started":"2021-08-11T10:44:47.457976Z","shell.execute_reply":"2021-08-11T10:53:10.945414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandar = StandardScaler().fit_transform(x) \nplt.plot(standar);\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:53:31.756071Z","iopub.execute_input":"2021-08-11T10:53:31.756512Z","iopub.status.idle":"2021-08-11T10:53:36.859348Z","shell.execute_reply.started":"2021-08-11T10:53:31.756478Z","shell.execute_reply":"2021-08-11T10:53:36.857756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=sv.drop(['label'],axis=1)\ny = sv.label\nxn =np.array(X)\nyn =np.array(y)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.087166Z","iopub.status.idle":"2021-08-11T10:44:12.087687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn import preprocessing\n\nnormal1 = Normalizer().fit(xn) \nstandar2 = StandardScaler().fit(xn) \nx1 = normal1.transform(xn)\nx2 = standar2.transform(xn)\n\nx1 = pd.DataFrame(x2)\ny_t = pd.DataFrame(yn)\n\n\ncobak = pd.concat([x1,y_t],axis=1)\ncobak = cobak.sample(frac=1)\ncobak.to_csv('trans.csv')\ncobak","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.08852Z","iopub.status.idle":"2021-08-11T10:44:12.089066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndef window_data(df, window, feature_col_number, target_col_number):\n    X = []\n    y = []\n    for i in range(len(df) - window - 1):\n        \n        features = df.iloc[i : (i + window), feature_col_number].values\n        target = df.iloc[(i + window), target_col_number]\n     \n        X.append(features)\n        y.append(target)\n    return np.array(X), np.array(y).astype(np.float64).reshape(-1, 1)\n    \nwindow_size = 50 \n(X, y) = window_data(sv, window_size, 0, 14)\nX.shape, y.shape\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n#X =pd.DataFrame(x_tr)\n#y =pd.DataFrame(y_tr)\nfrom sklearn.model_selection import train_test_split\nx_tr, x_val, y_tr, y_val = train_test_split(X,y,stratify=y,test_size = 0.2,random_state=50,shuffle=True)\nx_tr.shape, x_val.shape, y_tr.shape, y_val.shape\n\nX= pd.DataFrame(x_tr)\ny =pd.DataFrame(y_tr)\ndata_window = pd.concat([X,y],axis=1)\ndata_window.to_csv('windAF3.csv')\ndata_window.head()\n'''","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.089847Z","iopub.status.idle":"2021-08-11T10:44:12.090353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time \nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom numpy import linalg as LA\nimport scipy\nfrom scipy import io\nimport pdb","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.091143Z","iopub.status.idle":"2021-08-11T10:44:12.09162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class meanStd(object):\n    def __init__(self):\n        self.mean     = 0.0\n        self.mean_old = 0.0\n        self.std      = 0.001\n        self.count    = 0.0\n        self.minMean  = 100.0\n        self.minStd   = 100.0\n        self.M_old    = 0.0\n        self.M        = 0.0\n        self.S        = 0.0\n        self.S_old    = 0.0\n        \n    def calcMeanStd(self, data, cnt = 1):\n        self.data     = data\n        self.mean_old = copy.deepcopy(self.mean)\n        self.M_old    = self.count*self.mean_old\n        self.M        = self.M_old + data\n        self.S_old    = copy.deepcopy(self.S)\n        if self.count > 0:\n            self.S    = self.S_old + ((self.count*data - self.M_old)**2)/(self.count*(self.count + cnt))\n        \n        self.count   += cnt\n        self.mean     = self.mean_old + np.divide((data-self.mean_old),self.count)\n        self.std      = np.sqrt(self.S/self.count)\n        \n        if (self.std != self.std).any():\n            print('There is NaN in meanStd')\n            pdb.set_trace()\n    \n    def resetMinMeanStd(self):\n        self.minMean = copy.deepcopy(self.mean)\n        self.minStd  = copy.deepcopy(self.std)\n        \n    def calcMeanStdMin(self):\n        if self.mean < self.minMean:\n            self.minMean = copy.deepcopy(self.mean)\n        if self.std < self.minStd:\n            self.minStd = copy.deepcopy(self.std)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.092379Z","iopub.status.idle":"2021-08-11T10:44:12.09286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utils ADL","metadata":{}},{"cell_type":"code","source":"def probitFunc(meanIn,stdIn):\n    stdIn += 0.0001  # for safety\n    out = meanIn/(torch.ones(1) + (np.pi/8)*stdIn**2)**0.5\n    \n    return out","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.093631Z","iopub.status.idle":"2021-08-11T10:44:12.094158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generateWeightXavInit(nInput,nNode,nOut,nNewNode):\n    copyNet         = smallAdl(nInput,nNode,nOut)\n    newWeight       = copyNet.linear.weight.data[0:nNewNode]\n    newWeightNext   = copyNet.linear.weight.data[:,0:nNewNode]\n    newOutputWeight = copyNet.linearOutput.weight.data[:,0:nNewNode]\n    \n    return newWeight, newOutputWeight, newWeightNext","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.094922Z","iopub.status.idle":"2021-08-11T10:44:12.095408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deleteRowTensor(x,index):\n    x = x[torch.arange(x.size(0))!=index] \n    \n    return x","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.096177Z","iopub.status.idle":"2021-08-11T10:44:12.096666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deleteColTensor(x,index):\n    x = x.transpose(1,0)\n    x = x[torch.arange(x.size(0))!=index]\n    x = x.transpose(1,0)\n    \n    return x","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.097467Z","iopub.status.idle":"2021-08-11T10:44:12.097985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Network","metadata":{}},{"cell_type":"code","source":"class smallAdl(nn.Module):\n    def __init__(self, no_input, no_hidden, classes):\n        super(smallAdl, self).__init__()\n        # hidden layer\n        self.linear = nn.Linear(no_input, no_hidden,  bias=True)\n        self.activation = nn.Sigmoid()\n        nn.init.xavier_uniform_(self.linear.weight)\n        self.linear.bias.data.zero_()\n        \n        # softmax layer\n        self.linearOutput = nn.Linear(no_hidden, classes,  bias=True)\n        nn.init.xavier_uniform_(self.linearOutput.weight)\n        self.linearOutput.bias.data.zero_()\n        \n    def forward(self, x):\n        x  = self.linear(x)\n        h  = self.activation(x)\n        x  = self.linearOutput(h)\n        \n        h2 = (h.clone().detach())**2\n        x2 = self.linearOutput(h2)\n        \n        return x, h.clone().detach(), x2.clone().detach()\n    \ndef createSmallAdl(no_input,no_hidden,classes):\n    obj = smallAdl(no_input,no_hidden,classes)\n    return obj","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.098742Z","iopub.status.idle":"2021-08-11T10:44:12.099256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adlFeedforwardTest(netList,x,votingWeight,device):\n    # feedforward to all layers\n    with torch.no_grad():\n        classes = netList[0].linearOutput.weight.shape[0]\n        nData   = x.shape[0]\n        y       = torch.zeros(nData,classes)\n        yList   = []\n        hList   = []\n\n        minibatch_data = x.to(device)\n        minibatch_data = minibatch_data.type(torch.float)\n        tempVar = minibatch_data\n\n        for netLen in range(len(netList)):\n            currnet          = netList[netLen]\n            obj              = currnet.eval()\n            obj              = obj.to(device)\n            tempY, tempVar,_ = obj(tempVar)\n            hList            = hList + [tempVar.tolist()]\n            y                = y + tempY*votingWeight[netLen]\n            if votingWeight[netLen] == 0:\n                yList        = yList + [[]]\n            else:\n                yList        = yList + [F.softmax(tempY,dim=1).tolist()]\n\n    return y, yList, hList","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.100252Z","iopub.status.idle":"2021-08-11T10:44:12.100761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adlFeedforwardBiasVar(netList,netWinIdx,x,y,device):\n    # feedforward from the input to the winning layer\n    # y in one hot vector form, float, already put in device\n    with torch.no_grad():\n        minibatch_data  = x.to(device)\n        minibatch_data  = minibatch_data.type(torch.float)\n        minibatch_label = y\n        \n        tempVar = minibatch_data\n        for netLen in range(len(netList)):\n            currnet               = netList[netLen]\n            obj                   = currnet.eval()\n            obj                   = obj.to(device)\n            tempY, tempVar,tempY2 = obj(tempVar)\n            \n            if netLen == 0:\n                tempVar2          = (tempVar.clone().detach())**2\n            else:\n                tempY2,tempVar2,_ = obj(tempVar2)\n                \n            if netLen == netWinIdx:\n                break\n        \n        tempY    = F.softmax(tempY,dim=1)\n        tempY2   = F.softmax(tempY2,dim=1)\n        bias     = torch.norm((tempY - minibatch_label)**2)\n        variance = torch.norm(tempY2 - tempY**2)\n\n    return bias.tolist(), variance.tolist(), tempVar","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.101538Z","iopub.status.idle":"2021-08-11T10:44:12.102021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adlFeedforwardTrain(netWin,xWin,device):\n    # feed forward only on winning layer\n    minibatch_data = xWin.to(device)\n    minibatch_data = minibatch_data.type(torch.float)\n    minibatch_data.requires_grad_()\n    \n    netWin = netWin.train()\n    netWin = netWin.to(device)\n    y,_,_  = netWin(minibatch_data)\n    \n    return y","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.102764Z","iopub.status.idle":"2021-08-11T10:44:12.103259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and Test","metadata":{}},{"cell_type":"code","source":"def adlTrain(netList,netWinIdx,xWin,x,y,nClass,miuX,miuBias,miuVar,lr,criterion,device,epoch=1):\n    \n    print('Adjust ',netWinIdx+1,'-th hidden layer')\n    \n    # flags\n    growNode  = False\n    pruneNode = False\n    \n    # shuffle the data\n    nData = x.shape[0]\n    shuffled_indices = torch.randperm(nData)\n    \n    # label for bias var calculation\n    y_biasVar = F.one_hot(y, num_classes =nClass).float()\n    \n    for iData in range(0,nData):\n        # load data\n        \n        indices                 = shuffled_indices[iData:iData+1]\n               \n        minibatch_xWin          = xWin[indices]\n        minibatch_xWin          = minibatch_xWin.to(device)\n               \n        minibatch_label         = y[indices]\n        minibatch_label         = minibatch_label.to(device)\n        minibatch_label_biasVar = y_biasVar[indices]\n        minibatch_label_biasVar = minibatch_label_biasVar.to(device)\n        \n        minibatch_x             = x[indices]\n        minibatch_x             = minibatch_x.to(device)\n        \n        # calculate mean of input\n        miuX.calcMeanStd(minibatch_x)\n        \n        # get bias and variance\n        outProbit = probitFunc(miuX.mean,miuX.std)\n        bias, variance, nodeSignificance = adlFeedforwardBiasVar(netList,netWinIdx,\n                                                                 outProbit,minibatch_label_biasVar,device)\n        \n        # calculate mean of bias\n        miuBias[netWinIdx].calcMeanStd(bias)\n        if miuBias[netWinIdx].count < 1 or growNode:\n            miuBias[netWinIdx].resetMinMeanStd()\n        else:\n            miuBias[netWinIdx].calcMeanStdMin()\n        \n        # calculate mean of variance\n        miuVar[netWinIdx].calcMeanStd(variance)\n        if miuVar[netWinIdx].count < 20 or pruneNode:\n            miuVar[netWinIdx].resetMinMeanStd()\n        else:\n            miuVar[netWinIdx].calcMeanStdMin()\n        \n        # growing\n        growNode = growNodeIdentification(bias,miuBias[netWinIdx].minMean,miuBias[netWinIdx].minStd,\n                                          miuBias[netWinIdx].mean,miuBias[netWinIdx].std)\n        if growNode and miuBias[netWinIdx].count >= 1:\n            # grow a node\n            netList = nodeGrowing(netList,netWinIdx,1)\n        \n        # pruning\n        pruneNode = pruneNodeIdentification(variance,miuVar[netWinIdx].minMean,miuVar[netWinIdx].minStd,\n                                            miuVar[netWinIdx].mean,miuVar[netWinIdx].std)\n        if (pruneNode and not growNode and miuVar[netWinIdx].count >= 20 and \n           netList[netWinIdx].linear.weight.data.shape[0] > netList[netWinIdx].linearOutput.weight.data.shape[0]):\n            pruneIdx = findLeastSignificantNode(nodeSignificance)\n            \n            # prune a node\n            netList  = nodePruning(netList,netWinIdx,pruneIdx)\n            \n        # active learning\n        # if not growNode and not pruneNode and activeLearn:\n            # active learning can be executed if there is no growing and pruning and active learning is triggered\n        \n        # declare parameters to be trained\n        netOptim  = []\n        netOptim  = netOptim + list(netList[netWinIdx].parameters())\n        optimizer = torch.optim.SGD(netOptim, lr = lr, momentum = 0.95) #, weight_decay = 5e-4)\n        # optimizer = torch.optim.Adam(netOptim, lr = 0.05, weight_decay = 5e-4)\n        \n        # feedforward\n        scores    = adlFeedforwardTrain(netList[netWinIdx],minibatch_xWin,device)\n        \n        # calculate loss\n        minibatch_label = minibatch_label.long()\n        loss            = criterion(scores,minibatch_label)\n        \n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print('Bias: ',miuBias[netWinIdx].mean)\n    print('Variance: ',miuVar[netWinIdx].mean)\n    \n    return netList, miuX, miuBias, miuVar","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.10408Z","iopub.status.idle":"2021-08-11T10:44:12.104608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adlTest(netList,votingWeight,test_data,test_label,batch_size,criterion,device):\n    # load data\n    test_data  = test_data.to(device)\n    test_label = test_label.to(device)\n    test_label = test_label.long()\n    \n    # testing\n    start_test              = time.time()\n    scores,scoresList,_     = adlFeedforwardTest(netList,test_data,votingWeight,device)\n    rawPredicted, predicted = torch.max(F.softmax(scores.data,dim=1), 1)\n    #rawPredicted, predicted = torch.max(F.softmax(scores.data,dim=4), 4)\n    end_test                = time.time()\n\n    # performance calculation\n    loss          = criterion(scores,test_label)\n    residualError = torch.tensor([1.0]) - rawPredicted\n    correct       = (predicted == test_label).sum().item()\n    accuracy      = 100*correct/(predicted == test_label).shape[0]  # 1: correct, 0: wrong\n    testing_time  = end_test - start_test\n    F_matrix      = (predicted != test_label).int().tolist()  # 1: wrong, 0: correct\n    lossList      = []\n    F_matrixList  = []\n    for netLen in range(len(netList)):\n        if votingWeight[netLen] == 0:\n            F_matrixList = F_matrixList + [[]]\n            lossList     = lossList + [-1]  # -1 loss value indicate that the layer is already pruned\n        else:\n            _, predicted = torch.max(torch.FloatTensor(scoresList[netLen]).data, 1)\n            F_matrixList = F_matrixList + [(predicted != test_label).int().tolist()]  # 1: wrong, 0: correct\n            loacalLoss   = criterion(torch.FloatTensor(scoresList[netLen]),test_label)\n            lossList     = lossList + [loacalLoss.tolist()]\n        \n    print('Testing Accuracy: {}'.format(accuracy))\n    print('Testing Loss: {}'.format(loss))\n    print('Testing Time: {}'.format(testing_time))\n    \n    return scores, scoresList, loss, lossList, residualError, accuracy, testing_time, F_matrix, F_matrixList","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.105387Z","iopub.status.idle":"2021-08-11T10:44:12.105859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def updateVotingWeight(votingWeight,dynamicDecreasingFactor,decreasingFactor,FmatrixList):\n    for idx in range(0,len(votingWeight)):\n        currFmat = FmatrixList[idx]\n        for iData in range(0,len(currFmat)):\n            if currFmat[iData] == 1:  # detect wrong prediction\n                # penalty\n                dynamicDecreasingFactor[idx] = np.maximum(dynamicDecreasingFactor[idx] - \n                                                          decreasingFactor, decreasingFactor)\n                votingWeight[idx]            = np.maximum(votingWeight[idx]*dynamicDecreasingFactor[idx], \n                                                          decreasingFactor)\n            elif currFmat[iData] == 0:  # detect correct prediction\n                # reward\n                dynamicDecreasingFactor[idx] = np.minimum(dynamicDecreasingFactor[idx] + decreasingFactor, 1)\n                votingWeight[idx]            = np.minimum(votingWeight[idx]*(1 + dynamicDecreasingFactor[idx]), 1)\n    \n    votingWeight = (votingWeight/np.sum(votingWeight)).tolist()\n    \n    return votingWeight, dynamicDecreasingFactor","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.106611Z","iopub.status.idle":"2021-08-11T10:44:12.107137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Network Evaluation","metadata":{}},{"cell_type":"code","source":"def driftDetection(fMat,alphaDrift,alphaWarning,driftStatusOld):\n    driftStatus = 0  # 0: no drift, 1: warning, 2: drift\n    \n    nData = len(fMat)\n    F_max = np.max(fMat)\n    \n    if F_max != 0:  # all predictions are correct, no need to check drift\n        F_min = np.min(fMat)\n        miu_F = np.mean(fMat)\n        errorBoundF = np.sqrt((1/(2*nData))*np.log(1/alphaDrift))\n\n        cutPointCandidate = [int(nData/4),int(nData/2),int(nData*3/4)]\n        cutPoint = 0\n\n        # confirm the cut point\n        for iCut in cutPointCandidate:\n            miu_G = np.mean(fMat[0:iCut])\n            nG    = len(fMat[0:iCut])\n            errorBoundG = np.sqrt((1/(2*nG))*np.log(1/alphaDrift))\n            if (miu_F + errorBoundF) <= (miu_G + errorBoundG):\n                cutPoint = iCut\n                print('A cut point is detected cut: ', cutPoint)\n                break\n\n        # confirm drift\n        if cutPoint != 0:\n            errorBoundDrift = (F_max - F_min)*np.sqrt(((nData - cutPoint)/(2*cutPoint*nData))*\n                                                      np.log(1/alphaDrift))\n            if (miu_G - miu_F) >= errorBoundDrift:\n                print('H0 is rejected with size: ', errorBoundDrift)\n                print('Status: DRIFT')\n                driftStatus = 2\n            else:\n                errorBoundWarning = (F_max - F_min)*np.sqrt(((nData - cutPoint)/(2*cutPoint*nData))*\n                                                            np.log(1/alphaWarning))\n                if (miu_G - miu_F) >= errorBoundWarning and driftStatusOld != 1:\n                    print('H0 is rejected with size: ', errorBoundWarning)\n                    print('Status: WARNING')\n                    driftStatus = 1\n                else:\n                    print('H0 is NOT rejected')\n                    print('Status: STABLE')\n                    driftStatus = 0\n        else:\n            print('Status: STABLE')\n    else:\n        print('Status: STABLE')\n    \n    return driftStatus","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.107938Z","iopub.status.idle":"2021-08-11T10:44:12.108426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def winLayerIdentifier(votWeight):\n    idx = 0\n    # idx = np.argmax(np.asarray(votWeight)/(np.asarray(allLoss) + 0.001))\n    idx = np.argmax(np.asarray(votWeight))\n    \n    return idx","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.109161Z","iopub.status.idle":"2021-08-11T10:44:12.109637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def growNodeIdentification(bias,minMeanBias,minStdBias,meanBias,stdBias):\n    growNode = False\n    \n    dynamicKsigmaGrow = 1.3*np.exp(-bias) + 0.7\n    growCondition1    = minMeanBias + dynamicKsigmaGrow*minStdBias\n    growCondition2    = meanBias + stdBias\n    \n    if growCondition2 > growCondition1:\n        growNode = True\n    \n    return growNode","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.110425Z","iopub.status.idle":"2021-08-11T10:44:12.110945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pruneNodeIdentification(var,minMeanVar,minStdVar,meanVar,stdVar):\n    pruneNode = False\n    \n    dynamicKsigmaPrune = 1.3*np.exp(-var) + 0.7\n    pruneCondition1    = minMeanVar + 2*dynamicKsigmaPrune*minStdVar\n    pruneCondition2    = meanVar + stdVar\n    \n    if pruneCondition2 > pruneCondition1:\n        pruneNode = True\n    \n    return pruneNode","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.111684Z","iopub.status.idle":"2021-08-11T10:44:12.11217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def findLeastSignificantNode(nodeSig):\n    leastSigIdx = torch.argmin(torch.abs(nodeSig)).tolist()\n    \n    return leastSigIdx","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.112919Z","iopub.status.idle":"2021-08-11T10:44:12.113573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evolving","metadata":{}},{"cell_type":"code","source":"def layerGrowing(netList,votWeight,dyDecFactor,avgBias,avgVar):\n    nInput      = netList[-1].linearOutput.in_features\n    nOutput     = netList[-1].linearOutput.out_features\n    netList     = netList + [createSmallAdl(nInput,nOutput,nOutput)]\n    votWeight   = votWeight + [1.0]\n    dyDecFactor = dyDecFactor + [1.0]\n    votWeight   = (votWeight/np.sum(votWeight)).tolist()\n    avgBias     = avgBias + [meanStd()]\n    avgVar      = avgVar + [meanStd()]\n    print('*** ADD a new LAYER ***')\n    \n    return netList, votWeight, dyDecFactor, avgBias, avgVar","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.114348Z","iopub.status.idle":"2021-08-11T10:44:12.114835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def layerPruning(yList,votingWeight,pruneThreshold):\n    prunedLayerList = []\n    nLayer = np.count_nonzero(votingWeight)\n    for i in range(0,len(yList)):\n        if votingWeight[i] == 0:\n            continue\n        \n        for j in range(i+1,len(yList)):\n            if votingWeight[j] == 0:\n                continue\n            \n            A = torch.FloatTensor(yList[i]).transpose(0,1)\n            B = torch.FloatTensor(yList[j]).transpose(0,1)\n            nOutput = A.shape[0]\n            MICI = []\n            for k in range(0,nOutput):\n                varianceA = np.var(A[k].tolist())\n                varianceB = np.var(B[k].tolist())\n                corrAB = np.corrcoef(A[k].tolist(),B[k].tolist())[0][1]\n                \n                if (corrAB != corrAB).any():\n                    print('There is NaN in LAYER pruning')\n                    corrAB = 0.0\n                \n                mici = (varianceA + varianceB - np.sqrt((varianceA + varianceB)**2 - \n                                                        4*varianceA*varianceB*(1-corrAB**2)))/2\n                    \n                print('mici of ',i,'-th layer and ',j,'-th layer and ',k,'-th output is: ',mici)\n                MICI.append(mici)\n\n            if np.max(np.abs(MICI)) < pruneThreshold:\n                print('layer ',i+1, 'and layer ',j+1, 'are highly correlated with MICI ', np.max(np.abs(MICI)))\n                if votingWeight[i] < votingWeight[j]:\n                    prunedLayerList.append(i)\n                    votingWeight[i] = 0\n                    print('\\\\\\ hidden LAYER ',i+1, 'is PRUNED ///')\n                else:\n                    prunedLayerList.append(j)\n                    votingWeight[j] = 0\n                    print('\\\\\\ hidden LAYER ',j+1, 'is PRUNED ///')\n                \n                nLayer -= 1\n                if nLayer <= 1:\n                    break\n    \n    votingWeight = (votingWeight/np.sum(votingWeight)).tolist()\n    \n    return votingWeight","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.115606Z","iopub.status.idle":"2021-08-11T10:44:12.116128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeLastLayer(netList,votWeight,dyDecFactor,avgBias,avgVar):\n    while votWeight[-1] == 0:\n        del netList[-1]\n        del votWeight[-1]\n        del dyDecFactor[-1]\n        del avgBias[-1]\n        del avgVar[-1]\n        print('### A LAST hidden LAYER is REMOVED ###')\n    \n    return netList, votWeight, dyDecFactor, avgBias, avgVar","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.116877Z","iopub.status.idle":"2021-08-11T10:44:12.117386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nodeGrowing(netList,winIdx,nNewNode):\n    if winIdx <= (len(netList)-1):\n        netList      = copy.deepcopy(netList)\n        \n        nInputWin    = netList[winIdx].linear.weight.shape[1]\n        nNodeWin     = netList[winIdx].linear.weight.shape[0]\n        nOutput      = netList[winIdx].linearOutput.weight.shape[0]\n        nNewNodeCurr = nNodeWin + nNewNode\n\n        # grow node for current layer, output\n        newWeight, newOutputWeight,_         = generateWeightXavInit(nInputWin,nNewNodeCurr,nOutput,nNewNode)\n        netList[winIdx].linear.weight.data   = torch.cat((netList[winIdx].linear.weight.data,\n                                                          newWeight),0)  # grow input weights\n        netList[winIdx].linear.bias.data     = torch.cat((netList[winIdx].linear.bias.data,\n                                                          torch.zeros(nNewNode)),0)  # grow input bias\n        netList[winIdx].linear.out_features  = nNewNodeCurr\n        del netList[winIdx].linear.weight.grad\n        del netList[winIdx].linear.bias.grad\n        \n        # grow input weight of linearOutput\n        netList[winIdx].linearOutput.weight.data = torch.cat((netList[winIdx].linearOutput.weight.data,\n                                                                newOutputWeight),1)\n        netList[winIdx].linearOutput.in_features = nNewNodeCurr\n        del netList[winIdx].linearOutput.weight.grad\n        del netList[winIdx].linearOutput.bias.grad\n\n        if winIdx != (len(netList)-1):\n            nextIdx       = winIdx + 1\n            nInputNext    = netList[nextIdx].linear.weight.shape[1]\n            nNodeNext     = netList[nextIdx].linear.weight.shape[0]\n            nOutputNext   = netList[nextIdx].linearOutput.weight.shape[0]\n            nNewInputNext = nInputNext + nNewNode\n\n            # grow input weight of next layer\n            _,_,newWeightNext = generateWeightXavInit(nNewInputNext,nNodeNext,nOutputNext,nNewNode)\n            netList[nextIdx].linear.weight.data = torch.cat((netList[nextIdx].linear.weight.data,newWeightNext),1)\n            del netList[nextIdx].linear.weight.grad\n\n            # update input features\n            netList[nextIdx].linear.in_features = nNewInputNext\n            \n        print('+++ GROW a hidden NODE +++')\n    else:\n        raise IndexError\n    \n    return copy.deepcopy(netList)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.118157Z","iopub.status.idle":"2021-08-11T10:44:12.118653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nodePruning(netList,winIdx,pruneIdx):\n    if winIdx <= (len(netList)-1):\n        netList      = copy.deepcopy(netList)\n        \n        nNodeWin     = netList[winIdx].linear.weight.shape[0]\n        nPrunedNode  = 1\n        nNewNodeCurr = nNodeWin - nPrunedNode  # prune a node\n\n        # prune node for current layer, output\n        netList[winIdx].linear.weight.data  = deleteRowTensor(netList[winIdx].linear.weight.data,\n                                                           pruneIdx)  # prune input weights\n        netList[winIdx].linear.bias.data    = deleteRowTensor(netList[winIdx].linear.bias.data,\n                                                           pruneIdx)  # prune input bias\n        netList[winIdx].linear.out_features = nNewNodeCurr\n        del netList[winIdx].linear.weight.grad\n        del netList[winIdx].linear.bias.grad\n\n        # prune input weight of linearOutput\n        netList[winIdx].linearOutput.weight.data = deleteColTensor(netList[winIdx].linearOutput.weight.data,pruneIdx)\n        netList[winIdx].linearOutput.in_features = nNewNodeCurr\n        del netList[winIdx].linearOutput.weight.grad\n        del netList[winIdx].linearOutput.bias.grad\n\n        if winIdx != (len(netList)-1):\n            nextIdx       = winIdx + 1\n            nInputNext    = netList[nextIdx].linear.weight.shape[1]\n            nNewInputNext = nInputNext - nPrunedNode\n\n            # prune input weight of next layer\n            netList[nextIdx].linear.weight.data = deleteColTensor(netList[nextIdx].linear.weight.data,pruneIdx)\n            del netList[nextIdx].linear.weight.grad\n\n            # update input features\n            netList[nextIdx].linear.in_features = nNewInputNext\n        \n        print('--- Hidden NODE No: ',pruneIdx,' is PRUNED ---')\n        \n    else:\n        raise IndexError\n    \n    return copy.deepcopy(netList)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.119457Z","iopub.status.idle":"2021-08-11T10:44:12.119977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data loader","metadata":{}},{"cell_type":"code","source":"# load data\n#data1      = scipy.io.loadmat(r'../input/dataadl/sea.mat')  # change your folder\n#data       = data1.get('data')\n#data       = data1.get('data')\ndata1 = pd.read_csv( \"./trans.csv\")\nprint(data1)\ndata1.drop(['Unnamed: 0'], axis=1, inplace=True)\ndata=np.array(data1)\nprint(data.dtype,data.shape)\n\ndata       = torch.from_numpy(data)\ndata       = data.float()\n#preq_data  = data[:,0:-1]\npreq_data  = data[:,0:-1]\npreq_label = data[:,-1]\npreq_label = preq_label.long()\nnData      = preq_data.shape[0]\nbatchSize  = 500\nnBatch     = int(nData/batchSize)\nnInput     = preq_data.shape[1]\nnOutput    = torch.unique(preq_label).shape[0]\nprint('Number of input: ', nInput)\nprint('Number of output: ', nOutput)\nprint('Number of batch: ', nBatch)\n\n#pd.DataFrame(data[:,0:-1])\n#pd.DataFrame(data[:,-1])","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.120739Z","iopub.status.idle":"2021-08-11T10:44:12.121259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialization","metadata":{}},{"cell_type":"code","source":"# initial network\nnet                     = [createSmallAdl(nInput,nOutput,nOutput)]\nvotingWeight            = [1.0]\ndynamicDecreasingFactor = [1.0]\n\n# network significance\naverageBias = [meanStd()]\naverageVar  = [meanStd()]\n\n# parameters\ndecreasingFactor    = 0.001\npruneLayerThreshold = 0.05\nalphaWarning        = 0.0005\nalphaDrift          = 0.0001\nlr                  = 0.02  #0.01 learning rate","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.122109Z","iopub.status.idle":"2021-08-11T10:44:12.122667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialization\ncriterion    = nn.CrossEntropyLoss()\ndevice       = torch.device('cpu')\naverageInput = meanStd()\nbufferData   = torch.Tensor().float()\nbufferLabel  = torch.Tensor().long()\n\n# performance\nAccuracy     = meanStd()\ntestingTime  = meanStd()\ntrainingTime = meanStd()\n\n# flags\ndriftStatus = 0\ngrowCount   = 0\ngrowLayer   = 0\n\n# batch loop\nfor iBatch in range(0,nBatch):\n    print(iBatch,'- th batch of:', nBatch)\n    \n    # load data\n    batchIdx   = iBatch + 1\n    batchData  = preq_data[(batchIdx-1)*batchSize:batchIdx*batchSize]\n    batchLabel = preq_label[(batchIdx-1)*batchSize:batchIdx*batchSize]\n    nBatchData = batchData.shape[0]\n\n    # testing\n    scores, scoresList, loss, lossList, residualError, accuracy, testing_time, F_matrix, F_matrixList = adlTest(\n        net,votingWeight,batchData,batchLabel,batchSize,criterion,device)\n\n    # update voting weight\n    start_train = time.time()\n    votingWeight, dynamicDecreasingFactor = updateVotingWeight(votingWeight,dynamicDecreasingFactor,\n                                                                decreasingFactor,F_matrixList)\n    \n    if iBatch > 2:\n        # grow layer identification\n        driftStatus = driftDetection(F_matrix,alphaDrift,alphaWarning,driftStatus)\n        growCount  += 1\n        \n        # grow layer\n        if driftStatus == 2:\n            net, votingWeight, dynamicDecreasingFactor, averageBias, averageVar = layerGrowing(\n                net, votingWeight, dynamicDecreasingFactor, averageBias, averageVar)\n            growCount = 0\n            growLayer = 1\n        else:\n            growLayer = 0\n\n        # prune layer identification\n        if np.count_nonzero(votingWeight) > 1 and driftStatus == 0 and growCount > 1:\n            votingWeight = layerPruning(scoresList,votingWeight,pruneLayerThreshold)\n            net, votingWeight, dynamicDecreasingFactor, averageBias, averageVar = removeLastLayer(\n                    net,votingWeight,dynamicDecreasingFactor,averageBias,averageVar)\n                \n    # winning layer identification\n    if growLayer == 0:\n        netWinIdx = winLayerIdentifier(votingWeight)\n    elif growLayer == 1:\n        netWinIdx = len(net) - 1  # it is the new layer\n        \n    # data augmentation\n    # augment batchData and batchLabel\n    \n    # data preparation for training\n    if driftStatus == 0 or driftStatus == 2:  # STABLE or DRIFT\n        # check buffer\n        if bufferData.shape != 0:\n            # add buffer to the current data batch\n            batchData  = torch.cat((bufferData,batchData),0)\n            batchLabel = torch.cat((bufferLabel,batchLabel),0)\n        \n        # get input for the winning layer\n        if netWinIdx == 0:\n            xWin = batchData\n        else:\n            _,_,hrList = adlFeedforwardTest(net,batchData,votingWeight,device)\n            xWin       = torch.FloatTensor(hrList[netWinIdx-1])\n        \n        # clear buffer\n        bufferData  = torch.Tensor().float()\n        bufferLabel = torch.Tensor().long()\n        \n    elif driftStatus == 1:  # WARNING\n        # store data to buffer\n        bufferData  = batchData\n        bufferLabel = batchLabel\n    \n    # training\n    if driftStatus == 0 or driftStatus == 2:  # only train if it is stable or drift\n        net,averageInput,averageBias,averageVar = adlTrain(net,netWinIdx,xWin,batchData,batchLabel,nOutput,\n                                                           averageInput,averageBias,averageVar,lr,criterion,device)\n    \n    end_train = time.time()\n    training_time = end_train - start_train\n    \n    # calculate performance\n    if iBatch > 0:\n        Accuracy.calcMeanStd(accuracy)\n        testingTime.calcMeanStd(testing_time)\n        trainingTime.calcMeanStd(training_time)\n    \n    print('\\n')\n    \nprint('=== FINAL result ===')\nprint('Accuracy: ',Accuracy.mean,'(+/-)',Accuracy.std)\nprint('Training Time: ',trainingTime.mean,'(+/-)',trainingTime.std)\nprint('Testing Time: ',trainingTime.mean,'(+/-)',testingTime.std)\nprint('Network structure: ')\nprint(net)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-11T10:44:12.123524Z","iopub.status.idle":"2021-08-11T10:44:12.124112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(models[0].state_dict(), \"test0.pth\")","metadata":{"execution":{"iopub.status.busy":"2021-08-11T10:44:12.124905Z","iopub.status.idle":"2021-08-11T10:44:12.1254Z"},"trusted":true},"execution_count":null,"outputs":[]}]}