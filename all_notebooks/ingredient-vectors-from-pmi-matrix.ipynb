{"cells":[{"metadata":{"_cell_guid":"eee4b49d-b15a-446c-af88-9f308914f6f5","_uuid":"39bc9aa3b1dc1c834b1845ada4f253a1a8b6d7a6"},"cell_type":"markdown","source":"# Ingredient Vectors from Decomposing a Ingredient-Ingredient Pointwise Mutual Information Matrix\nLet's create some simple ingredients embeddings based on the principle of [word vectors](https://en.wikipedia.org/wiki/Word_embedding) by applying a [singular value decomposition](https://en.wikipedia.org/wiki/Singular-value_decomposition) to a [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) ingredient-ingredient matrix.  There are many other ways to create embeddings, but matrix decomposition is one of the most straightforward.  A well cited description of the technique used in this notebook can be found in Chris Moody's blog post [Stop Using word2vec](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/).    If you are interested in reading further about the history of word embeddings and a discussion of modern approaches check out the following blog post by Sebastian Ruder, [An overview of word embeddings and their connection to distributional semantic models](http://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/).\n\nThe end goal here is to find good substitutes to ingredients when those are not readily available to cook a given recipe. We will compare those embeddings to pre-trained word vectors.\n\nWe will be using a list of recipes that I scraped from the web. Here the recipes don't contain any particular indications and are just equivalent to sets of ingredients (ingredients' names are in french).\nIn this notebook tutorial we will implement as much as we can without using libraries that obfuscate the algorithm.  We're not going to write our own linear algebra or sparse matrix routines, but we will calculate unigram frequency, skipgram frequency, and the pointwise mutual information matrix \"by hand\".  Hopefully this will make the method easier to understand!"},{"metadata":{"_cell_guid":"ac9fffef-d727-4851-912b-ad80e958abcf","_uuid":"9cb0bce33b3e078eaf9bb593f265bf2ea13133b1","_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nfrom collections import Counter\nimport itertools\n\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse import linalg \nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport json\nimport sys\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ebb84c68-543c-4833-b9d0-5a76e3b5f674","_uuid":"b6ab8bbf571d5c9042d72b8be8fc286b1fc70005"},"cell_type":"markdown","source":"# Read Data and Preview"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"recipes = []\n\nwith open(os.path.join('/kaggle/input/ingredient-sets', 'recipes.json'), 'r') as f:\n    recipes = json.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recipes[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"471ed485-cb05-465b-9885-21d7449ed1b1","_uuid":"9cb0417c29d8d83c7be1e3798729f6754799d5ca"},"cell_type":"markdown","source":"# Minimal Preprocessing\nWe're going to create a ingredient-ingredient co-occurrence matrix from the ingredients in the recipes.  We will define two ingredients as \"co-occurring\" if they appear in the same recipe.  Using this definition, single ingredient recipes are not interestintg for us. "},{"metadata":{"_cell_guid":"ee944644-9b88-4ae3-b546-6ef3445461ef","_uuid":"d5d57ad7adab679ad1aebb821c3b1a6d0c6044d7","trusted":true},"cell_type":"code","source":"ingredients = [[ingredient['ingredientName'] for ingredient in recipe['ingredients']] for recipe in recipes]\n\n# remove single ingredients recipes\ningredients = [ing for ing in ingredients if len(ingredients) > 1]\n# show results\ningredients[0:5]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bbb476d1-ba2c-4207-8621-4727ecf6a1c3","_uuid":"614a82366eda763f440f41a797c03559b41c7e5d","collapsed":true},"cell_type":"markdown","source":"# Unigrams\nNow lets calculate a unigram vocabulary.  The following code assigns a unique ID to each token, stores that mapping in two dictionaries (`tok2indx` and `indx2tok`), and counts how often each token appears in the corpus."},{"metadata":{"_cell_guid":"c0c7a8b4-d82f-49ce-bf1c-62d02a9678c1","_uuid":"2d1cf9102cdf241bd367cb35d0ec7b741f1d4683","trusted":true},"cell_type":"code","source":"unigram_counts = Counter()\nfor i, ingredient in enumerate(ingredients):\n    for token in ingredient:\n        unigram_counts[token] += 1\n\ntok2indx = {tok: indx for indx,tok in enumerate(unigram_counts.keys())}\nindx2tok = {indx: tok for tok,indx in tok2indx.items()}\n\nprint('Done')\nprint('Vocabulary size: {}'.format(len(unigram_counts)))\nprint('Most common: {}'.format(unigram_counts.most_common(10)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a8bc838-6fa8-4f40-af0a-a11e962c33ed","_uuid":"d304e9ea8eb34b9b54a658347649d0c2cfac12d8"},"cell_type":"markdown","source":"# Skipgrams\nNow lets calculate a skipgram vocabulary.  We will loop through each ingredient in a recipe (the focus ingredient) and then form skipgrams by examing other ingredients within the same recipe.  As an example, the first recipe ,\n```\n['tortilla', 'abricot', 'beurre', 'sucre vanillé']\n```\nwould produce the following skipgrams, \n```\n('tortilla', 'abricot')\n('tortilla', 'beurre')\n('tortilla', 'sucre vanillé')\n('abricot', 'tortilla')\n('abricot', 'beurre')\n('abricot', 'sucre vanillé')\n('beurre', 'tortilla')\n('beurre', 'abricot')\n('beurre', 'sucre vanillé')\n('sucre vanillé', 'tortilla')\n('sucre vanillé', 'abricot')\n('sucre vanillé', 'beurre')\n```"},{"metadata":{"_cell_guid":"a9565c4f-4de5-480d-87c8-7e0281b13ac4","scrolled":true,"_uuid":"aaf990fdd624c3cdc0f850ba203239b9e64c30d9","trusted":true},"cell_type":"code","source":"# Note we store the token vocab indices in the skipgram counter\n\nskipgram_counts = Counter()\nfor ingredient in ingredients:\n    tokens = [tok2indx[tok] for tok in ingredient]\n    for i_ingredient, ingredient in enumerate(tokens):\n        for i_context in range(len(tokens)):\n            if i_ingredient == i_context:\n                continue\n            skipgram = (tokens[i_ingredient], tokens[i_context])\n            skipgram_counts[skipgram] += 1    \n        \nprint('Done')\nprint('Number of skipgrams: {}'.format(len(skipgram_counts)))\nmost_common = [\n    (indx2tok[sg[0][0]], indx2tok[sg[0][1]], sg[1]) \n    for sg in skipgram_counts.most_common(10)]\nprint('Most common: {}'.format(most_common))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8bcfafa6-98f8-40b4-972e-d7df8c5c94f6","_uuid":"31537b3568b29f04549466e251aec617baa026f4"},"cell_type":"markdown","source":"# Sparse Matrices\n\nWe will calculate several matrices that store ingredient-ingredient information.  These matrices will be $N \\times N$ where $N = 350$ is the size of our vocabulary.  We will need to use a sparse format so that it will fit into memory.  A nice implementation is available in [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).  To create these sparse matrices we create three iterables that store row indices, column indices, and data values. "},{"metadata":{"_cell_guid":"a4b24c6a-946a-40f8-937b-467a4f31770b","_uuid":"377457bac35cb7bdab01421e870049685d7cfbc5"},"cell_type":"markdown","source":"# Ingredient-Ingredient Count Matrix\nOur very first ingredient vectors will come from a ingredient-ingredient count matrix.  This matrix is symmetric so we can (equivalently) take the ingredient vectors to be the rows or columns.  However we will try and code as if the rows are ingredient vectors and the columns are context vectors. "},{"metadata":{"_cell_guid":"66089b1a-3fc8-4691-b083-cfa54eabc747","_uuid":"f3a107b7903f6ca9c6c8dc621b6ecb8e92cad51a","trusted":true},"cell_type":"code","source":"row_indxs = []\ncol_indxs = []\ndat_values = []\ni = 0\nfor (tok1, tok2), sg_count in skipgram_counts.items():\n    i += 1  \n    row_indxs.append(tok1)\n    col_indxs.append(tok2)\n    dat_values.append(sg_count)\n\niicnt_mat = sparse.csr_matrix((dat_values, (row_indxs, col_indxs)))\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"21f99f7e-4a19-4f85-afc9-e2c65ff70c4b","_uuid":"82317edef50fb1427ba2d8b6a84d2033bdb82a06"},"cell_type":"markdown","source":"# Ingredient Similarity with Sparse Count Matrices"},{"metadata":{"_cell_guid":"e4510fb3-03f9-4e74-b833-982e21011b60","_uuid":"60b7d344a2c923b6ac12a6ba10b84b42f4a199a0","trusted":true},"cell_type":"code","source":"def ii_sim(ingredient, mat, topn=10):\n    \"\"\"Calculate topn most similar ingredients to ingredient\"\"\"\n    indx = tok2indx[ingredient]\n    if isinstance(mat, sparse.csr_matrix):\n        v1 = mat.getrow(indx)\n    else:\n        v1 = mat[indx:indx+1, :]\n    sims = cosine_similarity(mat, v1).flatten()\n    sindxs = np.argsort(-sims)\n    sim_ingredient_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n    return sim_ingredient_scores","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6110a0c3-c020-4b7e-83d5-d4417f1a63ae","_uuid":"fa9079fc583ff3eeb941264025c7e2ac6508050e","trusted":true},"cell_type":"code","source":"ii_sim('beurre', iicnt_mat)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51431bda-a865-4345-b380-08f2b5b19ade","_uuid":"31fda68f475b96aca89edbb6af4d2b8842bdbe54","trusted":true},"cell_type":"code","source":"# Normalize each row using L2 norm\niicnt_norm_mat = normalize(iicnt_mat, norm='l2', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Demonstrate normalization\nrow = iicnt_mat.getrow(10).toarray().flatten()\nprint(np.sqrt((row*row).sum()))\n\nrow = iicnt_norm_mat.getrow(10).toarray().flatten()\nprint(np.sqrt((row*row).sum()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ad14fd9-0ff4-436c-897b-3af8cf41f446","_uuid":"70033cd3a5d70a951fa83656f11ad9ffc379e17c","trusted":true},"cell_type":"code","source":"ii_sim('poulet', iicnt_norm_mat)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56f1b0ac-ea0c-437e-b066-8918ba12251b","_uuid":"fd0988563bade6e57537a7eb980561be3db30461"},"cell_type":"markdown","source":"# Pointwise Mutual Information Matrices\nThe pointwise mutual information (PMI) for a (ingredient, context) pair in our corpus is defined as the probability of their co-occurrence divided by the probabilities of them appearing individually, \n$$\n{\\rm pmi}(w, c) = \\log \\frac{p(w, c)}{p(w) p(c)}\n$$\n\n$$\np(w, c) = \\frac{\nf_{i,j}\n}{\n\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n}, \\quad \np(w) = \\frac{\n\\sum_{j=1}^N f_{i,j}\n}{\n\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n}, \\quad\np(c) = \\frac{\n\\sum_{i=1}^N f_{i,j}\n}{\n\\sum_{i=1}^N \\sum_{j=1}^N f_{i,j}\n}\n$$\nwhere $f_{i,j}$ is the ingredient-ingredient count matrix we defined above.\nIn addition we can define the positive pointwise mutual information as, \n$$\n{\\rm ppmi}(w, c) = {\\rm max}\\left[{\\rm pmi(w,c)}, 0 \\right]\n$$\n\nNote that the definition of PMI above implies that ${\\rm pmi}(w, c) = {\\rm pmi}(c, w)$ and so this matrix will be symmetric.  However this is not true for the variant in which we smooth over the contexts."},{"metadata":{"_cell_guid":"7dbc6c77-7fd2-413b-b3ad-2006a5352725","_uuid":"c780eebf5597754c9a07b1a0e403986388edb39f","trusted":true},"cell_type":"code","source":"num_skipgrams = iicnt_mat.sum()\nassert(sum(skipgram_counts.values()) == num_skipgrams)\n\n# for creating sparce matrices\nrow_indxs = []\ncol_indxs = []\n\npmi_dat_values = []    # pointwise mutual information\nppmi_dat_values = []   # positive pointwise mutial information\nspmi_dat_values = []   # smoothed pointwise mutual information\nsppmi_dat_values = []  # smoothed positive pointwise mutual information\n\n# reusable quantities\n\n# sum_over_rows[i] = sum_over_ingredients[i] = iicnt_mat.getcol(i).sum()\nsum_over_ingredients = np.array(iicnt_mat.sum(axis=0)).flatten()\n# sum_over_cols[i] = sum_over_contexts[i] = iicnt_mat.getrow(i).sum()\nsum_over_contexts = np.array(iicnt_mat.sum(axis=1)).flatten()\n\n# smoothing\nalpha = 0.75\nsum_over_ingredients_alpha = sum_over_ingredients**alpha\nnca_denom = np.sum(sum_over_ingredients_alpha)\n\nfor (tok_ingredient, tok_context), sg_count in skipgram_counts.items():\n    # here we have the following correspondance with Levy, Goldberg, Dagan\n    #========================================================================\n    #   num_skipgrams = |D|\n    #   nwc = sg_count = #(w,c)\n    #   Pwc = nwc / num_skipgrams = #(w,c) / |D|\n    #   nw = sum_over_cols[tok_ingredient]    = sum_over_contexts[tok_ingredient] = #(w)\n    #   Pw = nw / num_skipgrams = #(w) / |D|\n    #   nc = sum_over_rows[tok_context] = sum_over_ingredients[tok_context] = #(c)\n    #   Pc = nc / num_skipgrams = #(c) / |D|\n    #\n    #   nca = sum_over_rows[tok_context]^alpha = sum_over_ingredients[tok_context]^alpha = #(c)^alpha\n    #   nca_denom = sum_{tok_content}( sum_over_ingredients[tok_content]^alpha )\n    \n    nwc = sg_count\n    Pwc = nwc / num_skipgrams\n    nw = sum_over_contexts[tok_ingredient]\n    Pw = nw / num_skipgrams\n    nc = sum_over_ingredients[tok_context]\n    Pc = nc / num_skipgrams\n    \n    nca = sum_over_ingredients_alpha[tok_context]\n    Pca = nca / nca_denom\n    \n    # note \n    # pmi = log {#(w,c) |D| / [#(w) #(c)]} \n    #     = log {nwc * num_skipgrams / [nw nc]}\n    #     = log {P(w,c) / [P(w) P(c)]} \n    #     = log {Pwc / [Pw Pc]}\n    pmi = np.log2(Pwc/(Pw*Pc))   \n    ppmi = max(pmi, 0)\n    spmi = np.log2(Pwc/(Pw*Pca))\n    sppmi = max(spmi, 0)\n    \n    row_indxs.append(tok_ingredient)\n    col_indxs.append(tok_context)\n    pmi_dat_values.append(pmi)\n    ppmi_dat_values.append(ppmi)\n    spmi_dat_values.append(spmi)\n    sppmi_dat_values.append(sppmi)\n        \npmi_mat = sparse.csr_matrix((pmi_dat_values, (row_indxs, col_indxs)))\nppmi_mat = sparse.csr_matrix((ppmi_dat_values, (row_indxs, col_indxs)))\nspmi_mat = sparse.csr_matrix((spmi_dat_values, (row_indxs, col_indxs)))\nsppmi_mat = sparse.csr_matrix((sppmi_dat_values, (row_indxs, col_indxs)))\n\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"739efb22-e1b0-4201-a0f3-0136fd1cf051","_uuid":"3712faead6d13739149977db8d590c7642f541d2"},"cell_type":"markdown","source":"# Ingredient Similarity with Sparse PMI Matrices"},{"metadata":{"_cell_guid":"2a5e99fd-2457-42b1-a206-7812601144ab","_uuid":"179af697701291cad547e984d149cdcbcac18dd3","trusted":true},"cell_type":"code","source":"ii_sim('carotte', pmi_mat)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3dae981a-25ab-45eb-a39e-d38284165c12","_uuid":"a6d565e83c2fccbd0dc1e17a36feb2253fcdd248","trusted":true},"cell_type":"code","source":"ii_sim('carotte', ppmi_mat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad93f771f8ceb60dfb3af926d902915372064379","trusted":true},"cell_type":"code","source":"ii_sim('carotte', spmi_mat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76cc04c466972013d3aa9ae10db11fad41575149","trusted":true},"cell_type":"code","source":"ii_sim('carotte', sppmi_mat)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dcfe9b79-e0cd-49e4-ac5a-b53482f202df","_uuid":"57d1fbb84a249feb9524012e03f4a8949bbdb1c9"},"cell_type":"markdown","source":"# Singular Value Decomposition\nWith the PMI and PPMI matrices in hand, we can apply a singular value decomposition to create dense ingredients vectors from the sparse ones we've been using. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define a function to plot the vectors in a 2D space\ndef tsne_plot(word_to_vec_map):  \n    labels, tokens = [], []\n\n    for word, vector in word_to_vec_map.items():\n        tokens.append(vector)\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x, y = [], []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(12, 12)) \n    for i in range(len(x)):\n        plt.scatter(x[i], y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1bec58d-deda-4c45-89e6-6a2108ba8f56","_uuid":"2999b834c8b28f42d23b692ffcd6e1de99f4a5a0","trusted":true},"cell_type":"code","source":"pmi_use = ppmi_mat\nembedding_size = 30\nuu, ss, vv = linalg.svds(pmi_use, embedding_size) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e2291b9-5b66-4785-b9e1-fd2854ac9929","_uuid":"5cf87c0ad01129424deecb3c80a403155776bfd1","trusted":true},"cell_type":"code","source":"print('Vocab size: {}'.format(len(unigram_counts)))\nprint('Embedding size: {}'.format(embedding_size))\nprint('uu.shape: {}'.format(uu.shape))\nprint('ss.shape: {}'.format(ss.shape))\nprint('vv.shape: {}'.format(vv.shape))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0dcc5d06-e31c-46e2-97a5-7e7522fed8e5","_uuid":"444d98626f4cb22c5fe415532ead17fd5d5731aa","trusted":true},"cell_type":"code","source":"unorm = uu / np.sqrt(np.sum(uu*uu, axis=1, keepdims=True))\nvnorm = vv / np.sqrt(np.sum(vv*vv, axis=0, keepdims=True))\n\ningredient_vecs = uu + vv.T\ningredient_vecs_norm = ingredient_vecs / np.sqrt(np.sum(ingredient_vecs*ingredient_vecs, axis=1, keepdims=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb591e15f276b3e7f5869802e69cb3ec3e5c6c13"},"cell_type":"code","source":"# Let's create a mapping from ingredient to vector\ningredient_to_vec_map = {}\nfor idx, vector in enumerate(ingredient_vecs_norm):\n    ingredient_to_vec_map[indx2tok[idx]] = vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_plot(ingredient_to_vec_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ingredient_sim(ingredient, sim_mat):\n    sim_ingredient_scores = ii_sim(ingredient, sim_mat)\n    for sim_ingredient, sim_score in sim_ingredient_scores:\n        print(sim_ingredient, sim_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85146587a2114cfb254c606a545b690e7fdf7574"},"cell_type":"code","source":"ingredient = 'carotte'\ningredient_sim(ingredient, ingredient_vecs)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a78e0cbb-f6b7-4b8b-af43-39e8fe2cf32d","_uuid":"56f4c9812a4e33f03c7eb529ca3a2081a3002dcb","trusted":true},"cell_type":"markdown","source":"# Ingredient substitution\n\nLet's use the matrices built above to find good substitudes to ingredients. The idea is to suggest other ingredients when those needed for a recipe are not readily available."},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_substitutes(ingredient, sim_mat, pmi_mat, threshold=None):\n    substitutes = []\n    \n    candidates = ii_sim(ingredient, sim_mat, 20)\n    indx1 = tok2indx[ingredient]\n    for candidate, score in candidates:\n        if candidate == ingredient:\n            continue\n        if threshold:\n            if score < threshold:\n                continue\n                \n        indx2 = tok2indx[candidate]\n        pmi = pmi_mat[indx1, indx2]\n        \n        if pmi == 0: # We want ingredient that doesn't appear together within recipes\n            substitutes.append((candidate, score))\n    return substitutes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_substitutes('tomate', iicnt_mat, ppmi_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_substitutes('tomate', ingredient_vecs_norm, ppmi_mat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" In order to be conservative, i.e. in order to suggest substitutes that have a high chance to be valid substitutes, let's use a high threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"substitutes = {}\nfor ingredient in unigram_counts.keys():\n    sub = find_substitutes(ingredient, iicnt_mat, ppmi_mat, 0.95)\n    substitutes[ingredient] = sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of ingredients with possible substitutes\ncount = sum(1 for val in substitutes.values() if len(val) > 0)\n\n# Ratio of ingredients covered\ncount / len(unigram_counts.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-trained word vectors\n\nLet's use already pre-trained word vectors to see if we can find better ingredients' substitutes. To do so, we will use the [Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/) (GloVe) from Stanford. The idea here is to replace ingredient vectors by their names' word vectors.\n\nBy doing so, we are taking the risk to don't find matching vectors for some of our ingredients. Indeed, some of the ingredients' names are composed of several words and even by using the pre-trained word vectors with the biggest vocabulary (2.2M) we are not sure to find correspondances.\n\nNote also that, as those pre-trained word vectors are for words in english and that our ingredients are in french, we will need to translate our ingredients in english first to be able to use them. Here we will use a translation file to do so but it is also to possible to use the [Google Cloud Translation API](https://cloud.google.com/translate/docs/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retrieve ingredients and create translation map\ningredients_fr_en = {}\ningredients_en_fr = {}\n\nwith open(os.path.join('/kaggle/input/ingredient-sets', 'ingredients_fr_en.csv'), 'r') as f:\n    for line in f:\n        ing_fr, ing_en = line.split(',')\n        ing_fr, ing_en = ing_fr.strip().lower(), ing_en.strip().lower()\n            \n        ingredients_fr_en[ing_fr] = ing_en\n        ingredients_en_fr[ing_en] = ing_fr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is all ingredients in french map to single translation in english ?\nlen(ingredients_fr_en), len(ingredients_en_fr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_to_vec_map(glove_file, words_to_keep): \n    word_to_vec_map = {}\n    with open(glove_file, 'r', encoding='utf-8') as f: \n        for row in f:\n            row = row.strip().split()\n            word = row[0]\n            if word in words_to_keep:\n                word_to_vec_map[word] = np.array(row[1:], dtype=np.float64)\n    return word_to_vec_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_best_substitute(word, word_to_vec_map, ingredients_fr_en, ingredients_en_fr, threshold=None):\n    word = word.lower()\n    \n    if not word in ingredients_fr_en.keys():\n        return []\n    word_en = ingredients_fr_en[word]\n    \n    if not word_en in word_to_vec_map:\n        return []\n    e_a = word_to_vec_map[word_en]\n    \n    max_cosine_sim = -sys.maxsize        # Initialize max_cosine_sim to a large negative number\n    best_word = None                     # Initialize best_word with None, it will help keep track of the word to output\n\n    for w in word_to_vec_map.keys():         \n        # Compute cosine similarity between the vector e_a and the w's vector representation\n        if w == word_en:\n            continue\n        cosine_sim = cosine_similarity([e_a], [word_to_vec_map[w]])[0][0]\n        if threshold:\n            if cosine_sim < threshold:\n                continue\n        \n        if cosine_sim > max_cosine_sim:\n            max_cosine_sim = cosine_sim\n            best_word = w\n     \n    if best_word:\n        return [(ingredients_en_fr[best_word], max_cosine_sim)]\n    else:\n        return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_to_vec_map = get_word_to_vec_map(os.path.join('/kaggle/input/glove840b300dtxt', 'glove.840B.300d.txt'), ingredients_en_fr.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is all ingredients have a corresponding vectors within GloVe ?\nlen(word_to_vec_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_plot(word_to_vec_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_best_substitute('beurre', word_to_vec_map, ingredients_fr_en, ingredients_en_fr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wv_substitutes = {}\nfor ingredient in unigram_counts.keys():\n    sub = find_best_substitute(ingredient, word_to_vec_map, ingredients_fr_en, ingredients_en_fr, 0.7)\n    wv_substitutes[ingredient] = sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the number of ingredients with possible substitutes\ncount = sum(1 for val in wv_substitutes.values() if len(val) > 0)\n\n# Ratio of ingredients covered\ncount / len(unigram_counts.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What's next\n\nThere is a lot of research done to find automatically ingredients' subsitutes and there are more advanced and more efficient approaches to tackle this issue than the ones presented in this notebook. If you are interested by ths subject you can check the Master thesis (and its references) of Swaan Dekkers: [Automatic ingredient replacement indigital recipes:  combining machinelearning with expert knowledge](http://flavourspace.com/wp-content/uploads/2018/06/Master_Thesis_Swaan-Dekkers_final.pdf).\n\nIt is also possible to look at other fields of research and import their approaches to tackle this issue. One approach that would be interesting to try and that is in the continuation of the word vectors approach is the use of [BERT](https://en.wikipedia.org/wiki/BERT_(language_model). You can check the original paper [here](https://arxiv.org/abs/1810.04805) and its implementation on [GitHub](https://github.com/google-research/bert). The idea behind such a technique is that static word vectors are not good enough and we need embeddings that are adapted to the context. Take for example the word 'bank' which can siginify, according to the context, either a financial institution or a raised portion of seabed along the edge of a river (see [Bank (disambiguation)](https://en.wikipedia.org/wiki/Bank_(disambiguation))). With word2vec or GloVe we will have only one embedding for the token 'bank' but with BERT it is possible to have embeddings according to the context.\n\nOne use of BERT that can be particularly interesting for our case is lexical simplification. Such approach is presented by Qiang et al. in their paper [A Simple BERT-Based Approach for Lexical Simplification](https://arxiv.org/abs/1907.06226). The idea, as presented on the [GitHub of the project](https://github.com/qiang2100/BERT-LS), is:\n> Suppose that there is a sentence \"the cat perched on the mat\" and the complex word \"perched\". We concatenate the original sequence S and S' as a sentence pair, and feed the sentence pair {S,S'} into the BERT to obtain the probability distribution of the vocabulary corresponding to the mask word. Finally, we select as simplification candidates the top words from the probability distribution, excluding the morphological derivations of the complex word. For this example, we can get the top three simplification candidate words \"sat, seated, hopped\".\n\n![BERT-LS illustration](https://raw.githubusercontent.com/qiang2100/BERT-LS/master/BERT_LS.png)\n\nWe could adapt this technique to perform substitution. Indeed, instead of feeding the BERT with sentences we could feed it with lists of ingredients (one list of ingredients for each recipe). Or course, it will ask us to train a BERT especially for this purpose. Then, by masking out of the ingredient we could retrive the best substitute to it according to the context i.e. for a particular recipe. By doing so, we will not have a static map for ingredients' substitutes but it would be possible to adapt the ingredient substitution to the context."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}