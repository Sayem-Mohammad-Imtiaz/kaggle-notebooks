{"cells":[{"metadata":{"_uuid":"30424ddc72f0117430fbc936fa60ae4ae6731166"},"cell_type":"markdown","source":"# - Introduction -\n\n## Hello, in this kernel you'll find:\n*  <a href=\"#1\"> EDA </a>\n*  <a href=\"#2\"> Label Encoding  (sklearn) </a>\n*  <a href=\"#3\"> Handmade Logistic Regression </a>  \n*  <a href=\"#4\"> SciKit-Learn Logistic Regression </a>\n*  <a href=\"#5\"> Comparison of the Results and Conclusion </a>"},{"metadata":{"_uuid":"8943976ab640f381d2bf4f90b77a05974bf73306"},"cell_type":"markdown","source":"<div id=\"1\"/>\n## EDA"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra \nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/mushrooms.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc2b73dbf3e3eebd3f5b89e2763cc0d624f11939"},"cell_type":"code","source":"df.head(10)   # as we can see whole data structured by letter instead of numbers.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d3c1cd08f0b4f4d2495849285bc2950ebfdb1e1"},"cell_type":"code","source":"df.info()   # we can see that they are all objects. Which is Letters in our case","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4472c3bb2a9039664c25ea9c3c1443fc34d0a824"},"cell_type":"code","source":"df.describe()   # as we see, there are some columns with 12 unique rows while some others has only 2 unique rows.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b617b6089c880d8223d52d996af3e4efe9fcc4e"},"cell_type":"code","source":"# let's do Label Encoding and do EDA again","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"306773e569cc25397527c053387e395ea9e6de6a"},"cell_type":"markdown","source":"<div id=\"2\"/>\n## Label Encoding"},{"metadata":{"trusted":true,"_uuid":"8f656d6a326fafe7fd4eca488127b38e551c6381"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4631c340549bcda748780ededee0e90771bcce7"},"cell_type":"code","source":"# applying label encoder to whole dataset...\ndf = df.apply(label_encoder.fit_transform)\n\n# checking the result\ndf.head(10)                # which seems great.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b89625e746aebf4d354ebd32b3bbb141ed00aed7"},"cell_type":"code","source":"df.info()  # they are all int64 now and there is no null value which is very good.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18c386d230b21266b783046d2800a18a9a2a30a2"},"cell_type":"markdown","source":"<div id=\"3\"/>\n## Handmade Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"3a846339041d7424dcd2e13750ed98f547d0c466"},"cell_type":"markdown","source":"### Pre Preocessing the data\n\n\n\ndividing the labels and the features.."},{"metadata":{"trusted":true,"_uuid":"36abc48fcea70300f237aa30786cb976079578c2"},"cell_type":"code","source":"y = df[\"class\"].values   # our labels.. okay to eat or poison.\n\ndf.drop([\"class\"],axis=1,inplace=True)  # dropping the lables from the data\n\nx_data = df  # our features..","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"150abde279e380bd3a7e9a29c7a298c188d66141"},"cell_type":"markdown","source":"normalization"},{"metadata":{"trusted":true,"_uuid":"0b0a47b32dca72341057a90d624159520a536625"},"cell_type":"code","source":"x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\n\nx.drop([\"veil-color\"],axis=1,inplace=True)\nx.drop([\"veil-type\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"793ce182a605c8ef125b4d2c1a59a37af851fa59"},"cell_type":"markdown","source":"splitting test and train by sklearn"},{"metadata":{"trusted":true,"_uuid":"d5612a811a1456424dde5c603185643a767843c6"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)   # 20% would be enough","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e89c9c87f967ad6d31f4edb3b8db715f1457dc9"},"cell_type":"markdown","source":"transpose of matrixes"},{"metadata":{"trusted":true,"_uuid":"4c454e988275778ab8af1d67029664d210d9b4be"},"cell_type":"code","source":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef679e8703315a7d7942b565ef3c1b7e6714d0d4"},"cell_type":"markdown","source":"\n## Creating our Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"3210a11967925d41f7b1e96f3298745108c13c87"},"cell_type":"code","source":"def init_weight_and_bias(dimension):\n    w = np.full((dimension,1),0.01)          # just creating a dimension sized vector filled with our weight value (0.01)\n    b = 0.0                                  # smallest float value is setted to the bias (0.0)\n   # print(\"w::\",w)\n   # print(\"b::\",b)\n    return w,b;\n\ndef sigmoid(z):\n    y_head = 1/(1 + np.exp(-z))               # implementing the sigmoid function\n   # print(\"y_head::\",y_head)\n    return y_head;\n\ndef forward_and_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b             # first phase of actual Computation of the Logistic Regression:: multiplying the weights with corresponding values then adding bias..\n    y_head = sigmoid(z)                     # second phase:: Applying the sigmoid on the result of first phase to get result between 0 and 1.\n    \n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)        # calculating loss and cost is key to optimize since they are the value of fail \n  #  print(\"loss::\",loss)\n    cost = (np.sum(loss)) / x_train.shape[1]\n  #  print(\"forw_bck::\",cost)\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/ x_train.shape[1]     # applying gradient descent\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    \n    gradients = {\"deriv_weight\":derivative_weight,\"deriv_bias\":derivative_bias}  # putting all in dictionary\n    \n    return cost, gradients;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d4805933cf0ff346a5ffb2c7ba204f15e4dcdcb"},"cell_type":"code","source":"def update(w,b,x_train,y_train,learning_rate,num_of_iter):\n    cost_list = []   # empty arrays to store costs\n    index = []\n    \n    for i in range(num_of_iter):\n        cost, gradients = forward_and_backward_propagation(w,b,x_train,y_train)  # do the training as much as iteration given\n      #  print(\"update:: \",cost)\n        cost_list.append(cost)      # insert the calculated cost on array\n        index.append(i)\n        \n        w = w - learning_rate*gradients[\"deriv_weight\"]     # set new weights and bias for next iteration\n        b = b - learning_rate*gradients[\"deriv_bias\"]\n        \n    parameters = {\"weight\":w,\"bias\":b}    # save all the weights and biases on a dictionary\n    plt.plot(index,cost_list)            # draw the plot to visualize (optional)\n    plt.show()\n    \n    return parameters, gradients, cost_list;\n\n\ndef predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test) + b)           # do the first and second phase of Computation and store in array z\n    y_prediction = np.zeros((1,x_test.shape[1]))  # create empty array to fill by results of z\n   # print(\"y_pred:::\", np.zeros((1,x_test.shape[1])))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i] = 0;\n        else:\n            y_prediction[0,i] = 1;\n    \n    return y_prediction;    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc25965048169cb3e7683921f6489e9fb8520f88"},"cell_type":"markdown","source":"Now, I can easily implement my Logistic Regression on the dataset and see the results.."},{"metadata":{"trusted":true,"_uuid":"f452810de7533b3c42eff4b1e49dfe4c5a483a69"},"cell_type":"code","source":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_of_iter):\n    \n    dimension = x_train.shape[0]\n    w,b = init_weight_and_bias(dimension)\n    \n    parameters, gradients, col_list = update(w,b,x_train,y_train,learning_rate,num_of_iter)\n    \n    y_pred_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_pred_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    print(\"train accuracy: {} %\".format(100-np.mean(np.abs(y_pred_train-y_train))*100))\n    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(y_pred_test-y_test))*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2588d25bee4de0e7bee7aed4de34acf78a2cadc"},"cell_type":"code","source":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=1,num_of_iter=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7753c382ce7d7a17d387baac71214cff9e5d36cc"},"cell_type":"markdown","source":"<div id=\"4\"/>\n## SciKit-Learn Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"bddaebd0a1ae7fa17e21daff0f45838651f2d322"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\n\nlr_model.fit(x_train.T,y_train.T)\n\ny_head = lr_model.predict(x_test.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fbb14764e27a8675feb0608771b807b5d72d44b"},"cell_type":"code","source":"print(\"test accuracy: \", lr_model.score(x_test.T,y_test.T))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d39b6ac082bde2686545fb736440c2e815709bb"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_head)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b2c000ba0b818b21d846063ef64f37c530e4397"},"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(figsize=(16,10))\nsns.heatmap(cm,annot=True,fmt='.0f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53580bd6cec1f3080b27fe0cc6f5732f35a0cdee"},"cell_type":"markdown","source":"<div id=\"5\"/>\n# Comparison and Conclusion"},{"metadata":{"_uuid":"0edd0c1822ffefe49f330c8ce9326797fcb385c4"},"cell_type":"markdown","source":"To sum up, Sklearn is much more simple, easy to code and more sophisticated based on results. (1% better)\n\nBut in handmade version, we can modify it to have better results freely. (hyperParameter Tuning)"},{"metadata":{"trusted":true,"_uuid":"c2a9bdbc9ac3ea2c88fa55912406d3d7005563ef"},"cell_type":"code","source":"# I am currently trying to learn and improve my Machine Learning skills.\n# Your Comments,Advices and Votes are important for me. Best Regards, Efe.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}