{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport os\nprint(os.listdir(\"../input/california-housing-prices\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = pd.read_csv('../input/california-housing-prices/housing.csv')\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Identify the business problem and metrics to measure performance\n\nWe have been given a dataset that contains the housing prices in  the California area using the Calfiornia census data. This dataset contains information such as population, median house value, housesize (in terms of total beds and rooms), latitute and longitude (to geolocalize the households). <br>\nHere, we are supposed to look create a model to <b>predict the districts meanding housing price </b>. In this scenario, this request has been given to us by the stakeholders. The next step, consists in identifying any current relevant solutions that have been implemented. We do this for twofold reasons: reference in performance as well as an insight on how to solve the problem. Following this, we find out that existing modeling has been dony following complex and costly rules, with a typical error rate of 15%. <br>\nThis is a typical example of <b>supervised learning task</b> as we are given labeled data, and, more in particular, this is a <b>regression problem</b> as our target is a continuous feature. Finally, there is no continuous flow of data coming in, so we a <b>batch learning</b> approach should work fine<br>.\nThe typical performance measure for regression problems is the <b> Root Mean Square Error (RMSE)</b>. It measures the standard deviation of the errors the system makes in its predictions. Formula to compute: <br>\n $RMSE = \\sqrt{\\frac{1}{m}\\Sigma_{i=1}^{m}{\\Big({h}({x^{(i)}) -y^{(i)}}\\Big)^2}}$ <br> In the case that there are many outlier districts in our set, we may consider using the <b>Mean Absolute Error </b>: <br>\n $MAE({X},{h})= {\\frac{1}{m}\\Sigma_{i=1}^{m}{\\Big|{h}({x^{(i)}) -y^{(i)}}\\Big|}}$ <br> Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and te vector of target value."},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory Data Analysis "},{"metadata":{},"cell_type":"markdown","source":"Once uploaded our dataset, the first thing is familariase ourselves with it. This means looking at the various datatypes, columns, check distributions of the features, cardinality of categorical features, correlations, outliers, and missing values."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"print('Number of entries in the dataset: {}.'.format(len(housing)))\nprint('There are {} features in the dataset.'.format(len(housing.columns)))\nprint('--------------------')\nprint('List of categorical features: \\n{}'.format([x for x in housing.select_dtypes(include='O').columns]))\nprint('List of continuous features: \\n{}'.format([x for x in housing.select_dtypes(exclude='O').columns]))\nprint('------------------')\nprint('Features with missing values include:')\n_ = housing.isnull().sum()\nfor x,y in zip(_.index,_):\n    if y>0:\n        print('{} with {} missing values.'.format(x,y))\nprint('------------------')\nprint('Cardinality of the categorical feature:')\n_ = housing.ocean_proximity.value_counts()\nfor x,y in zip(_.index,_):\n    print('{} has {} labels.'.format(x,y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this initial analysis, we can see that our dataset is made of <b>10 features</b> (9 numerical and 1 categorical) with <b>20,640 districts</b>. Of the numerical features, 'total_bedrooms' is <b>missing 207 values</b>. This will need to be imputed. All other features have the totality of the data and are numerical, with the exception of 'ocean_proximity'. We can see that this feature has a cardinality of 5 (1H OCEAN, INLAND, NEAR OCEAN, NEAR BAY, ISLAND). Now let's look at some basic summary statistics, for this we can quickly use the method <i>describe()</i> which will give us a quick overview of the count, men, max, and percentiles."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the things we can immediately note from looking at the mean and std of the above summary statistics is that 'population' is highly affected by outliers with most values falling outside the centre. Why? Because std is almost as big as the mean, meaning that our values are very far and distant from each other. Hence, if population is relevant, we might want to consider it using the median or maybe winsorize the outliers. After, we will plot the feature and I am expecting to see 'population' skewed towards the right. Also, if we look at the percentiles, we can see that 75% of district houses will have lower than 3147 'total_rooms'. Finally, if we look at the 50% percentile (which corrensponds at the median) we can see that the 'median_house_value' is 179700. We can also explore the distribution of our features with method <i>hist()</i>."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.hist(bins=50,figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of the things we can note from the above histogram:\n- 'median_income' is not expressed in usd, and presumably tit has been capped at 15.0001.\n- 'housing_median_age' and 'house_median_value' have also been capped at 52 and 500001.0 respectively. The latter could cause issues since it is our target feature. The machine learning model could learn that house_median_value never go beyond 500k. If the requirement includes the possibility that such value could go beyond; we have two options: find the uncapped label values for those district or remove the capping districts all togher from our dataset.\n- the majority of our features do not follow a bell curve with a strong tail towards the right. This could cause few issues when applying machine learning algos (e.g. a normal linear regression will perform poorly in this problem because it expects the features to follow a normal distribution).<br>\n\nBefore continuing with our exploratory data analysis, it is good practice to divide the dataset into <i>training</i> and <i>testing set</i>. Sklearn provides the train_test_split() method which would normally be an optimal way to split them when the dataset is large enough. For this dataset, a better idea would be to follow a stratified sampling approach. Essentially in this way we make sure that our train and test set are representative of overall population. In this case, we would want ideally our stratified sampling to be based on the 'median_income' (because we are assuming that the household income is a good predictor of the median housing prices). Since this feature is a continuous feature, what we have to do is to first discretize it."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"housing['income_cat'] = np.ceil(housing['median_income'] /1.5)\nprint('Cardinality of median_income before discretization {} and after {} .'.format(len(housing.median_income.value_counts()),len(housing.income_cat.value_counts())))\nprint('After discretization:\\n',housing.income_cat.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By discretizing the continuous feature, median_income, to 'income_cat' we have reduced the dimensionality of our feature to 11. Following the principle of dimensionality reduction, I have decided to merge all categories greater than 5 into one."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing['income_cat'] = np.where(housing['income_cat']>5,5.0,housing['income_cat'])\nhousing.income_cat.plot(kind='hist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of our median income groups seem to be centred around the 2-3 groups. Now we can do a stratified sampling based on the income category."},{"metadata":{"trusted":true},"cell_type":"code","source":"split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_index, test_index in split.split(housing,housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if this worked as expected. We have to first look at the income category in the full dataset and then compare it with the one generated from the strat_test_set and the test_set (generated from the train_test_split method)."},{"metadata":{"trusted":true},"cell_type":"code","source":"original = pd.Series(housing['income_cat'].value_counts() / len(housing), name='Original')\nstrat = pd.Series(strat_test_set['income_cat'].value_counts() / len(strat_test_set),name='Stratified')\ntrain_set, test_set = train_test_split(housing,test_size=0.2,random_state=42)\nrandom = pd.Series(test_set['income_cat'].value_counts() / len(test_set), name='Random')\ntest_sets_comparisons = pd.DataFrame([original,strat,random]).T.sort_index()\ntest_sets_comparisons['% Error Strat'] = 100 * (test_sets_comparisons['Stratified'] / test_sets_comparisons['Original']) - 100\ntest_sets_comparisons['% Error Random'] = 100 * (test_sets_comparisons['Random'] / test_sets_comparisons['Original']) - 100\ntest_sets_comparisons","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the testing set generated by StratifiedShuffleSplit provises the closest resamblance to the distribution of our original housing set. Hence, we can proceed by using the stratified sampled sets. We can now remove the income_cat and have the data to our original state."},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in (strat_train_set,strat_test_set):\n    _.drop(['income_cat'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nhousing.plot.scatter(x='longitude',y='latitude', alpha=0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,10))\nhousing.plot.scatter(x='longitude',y='latitude'\n                     ,alpha=0.3,s=housing['population']/100,label='population'\n                     ,c='median_house_value',cmap=plt.get_cmap('jet'),colorbar=True, legend=True, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, we can see that there are several areas with a low population area but who have high median_house_value. They appear to be between latitude 36,38 and longitude -124,-122 (centre left area of the chart), and  32,34 latitude -120,-118 (bottom centre). We can zoom in:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,5))\nhousing[housing['median_house_value']>400000].plot.scatter(x='longitude',y='latitude'\n                     ,alpha=0.3,s=housing['population']/100,label='population'\n                     ,c='median_house_value',cmap=plt.get_cmap('jet'),colorbar=True, legend=True, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It would be interesting to see which areas/counties these coordinates correspond to. To do that, we can use te geopy library. First we create coordinate_transformer which will transform our latitude and longitude coordinates and then lookup their correnspoding area using the library."},{"metadata":{"trusted":true},"cell_type":"code","source":"def coordinate_transformer(latitude,longitude):\n    \"\"\"\n    This method takes the latitude and longitude coordinates, adding the number of missing zeros needed for the geolocator \n    request. The outputs are then used to find the county name.\n    \"\"\"\n    number_rounder_lat,number_rounder_long = (9 - len(str(latitude))),(9 - len(str(longitude)))\n    latitude = str(latitude) + str(0)*(number_rounder_lat)\n    longitude = str(longitude) + str(0)*(number_rounder_long)\n    from geopy.geocoders import Nominatim\n    geolocator = Nominatim(user_agent=\"california_median_housing_price\")\n    try:\n        location = geolocator.reverse(latitude+\", \"+longitude)\n        return location.raw['address']['county']\n    except:\n        return 'Not Found'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we were to run a lookup request per every single record, that will take us almost 5 hours of run time (assuming that one request per second is being handled). We already now that our latitude and longitude values are repeated multiple times. The ideal scenario is to look up the unique combinations of latitude and longitude and then use those propagate to the remaining records."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} instances of lat/long in the dataset.'.format(housing.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"_ = housing.groupby(['latitude','longitude'])['housing_median_age'].count().reset_index().drop(['housing_median_age'],axis=1)\nprint('There are {} unique combinations of lat/long in the dataset.'.format(_.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, even 10k is way too much for ourset to handle. What if we rounded the the values it 1 decimal positions?"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"_['latitude'],_['longitude'] = np.round(_['latitude'],1),np.round(_['longitude'],1)\n_ = _.groupby(['latitude','longitude']).count().reset_index()\nprint('If rounded to 1 decimal point, we have {} unique combinations.'.format(_.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, seems a little better. Let's see what happens if we round to just the longitude."},{"metadata":{"trusted":true},"cell_type":"code","source":"_['longitude'] = np.round(_['longitude'])\n_ = _.groupby(['latitude','longitude']).count().reset_index()\nprint('If rounded to 0 decimal points the longitude, we have {} unique combinations.'.format(_.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"380 unique combinations is enough. Let find their respective locations."},{"metadata":{"trusted":true},"cell_type":"code","source":"#from timeit import default_timer as timer\ncounty_list = []\n#start = timer()\nfor lat, long in zip(_.latitude,_.longitude):\n    county_list.append(coordinate_transformer(lat,long))\n#end = timer()\n#county_list = pd.Series(county_list)\n#print(end - start)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"_['county'] = county_list\nhousing['latitude_join'] = np.round(housing['latitude'],1)\nhousing['longitude_join'] = np.round(np.round(housing['longitude'],1))\nhousing = pd.merge(housing,_,how='left',left_on=['latitude_join','longitude_join'], right_on=['latitude','longitude']).drop(['latitude_join',\n       'longitude_join', 'latitude_y', 'longitude_y'],axis=1)\nhousing.rename(columns={'longitude_x':'longitude','latitude_x':'latitude'},inplace=True)\nhousing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 400000\nplt.axhline(y=threshold,linewidth=4, color='red')\nhousing[(housing.median_house_value>350000)].groupby(['county'])['median_house_value'].mean().plot(kind='bar',legend=True,figsize=(10,7),cmap=plt.get_cmap('jet'))\nplt.legend(loc='best')\nprint('List of Counties that exceed the threshold:')\nhigh_valued_houses_counties = []\nfor x in housing[(housing.median_house_value>400000)].groupby(['county'])['median_house_value'].mean().index:\n    high_valued_houses_counties.append(x)\n    print(x)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('The percentage of districts in highly valued counties (£400k and above) is {:.2%}.'.format(housing[housing.county.isin(high_valued_houses_counties)].shape[0]/housing.shape[0]))\n(housing[housing.county.isin(high_valued_houses_counties)]['ocean_proximity'].value_counts()/len(housing))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I suspected, house prices is much related to the location of the property. We can now start to look into the any underlying correlations between our features."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"corr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation coeffiencient is useful for finding features that relate to each other. In this way, we can use the value of one of our features (x) to infer our target (y). In this case, we are look at how our target correlates to the remaining feature. We can see that there is a strong linear relationship with 'median_income' <b>(0.687160)</b>. Another tools that we can use to investigate the correlation between attributes is the scatter_matrix of pandas."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nbest_f = corr_matrix['median_house_value'].sort_values(ascending=False).head(4).index.to_list()\nscatter_matrix(housing[best_f],figsize=(12,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this scatter plot we can note previosly mentioned cappings on 'median_house_value' and 'median_income'. In our correlation analysis, we have identified 'median_income' as the strongest indicator for our target. Let's zoom in."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.plot(kind='scatter',x='median_income',y='median_house_value',alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can clearly see the straight capping line at the 500k mark, but also one at approximately at 450k and 350k. These can interf we the performance of our model, let's remove them."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# here we want to remove the ones that appear in the scatter plot - the capping values.\nhousing[housing.median_house_value==350000].shape\nhousing[housing.median_house_value==450000].shape\nhousing[housing.median_house_value==500000].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features that are skewed and we might want to transform them (e.g. computing their log) <br>\n- population\n- median_income\n- households\n- total_bedrooms\n- total_rooms"},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature Engineering"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for f in housing.columns[2:]:\n    print(f,housing[f].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before extracting any feature, we have to deal with the missing values and decide what to do with our skewed features. In our case, the 'total_bedrooms' features seems to have 158 unrecorded examples with skew towards the right. Therefore, a good strategy is to use sklearn and impute with the median for the missing feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot((housing.total_bedrooms.dropna()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\nimputer.fit(housing['total_bedrooms'].values.reshape(-1,1))\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The median value for total_bedrooms looks like 433. To get the equivalent on pandas, we would have done the below:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"housing.total_bedrooms.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now we are sure that the value is indeed the value. First, we have to save its value (and apply it to our stratified test set) then we can apply the transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"median_t = imputer.statistics_ \nhousing['total_bedrooms'] = imputer.transform(housing['total_bedrooms'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if the values have been imputed correctly."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'total_rooms' was not the only skewed feature, let's visualiza them along with a possible log transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"start =1 \nend = 3\ncols = ['population','median_income','households','total_bedrooms','total_rooms']\nax, fig = plt.subplots(nrows=5,ncols=2,figsize=(20,20))\nfor col in cols:\n    for i in range(start,end):\n        plt.subplot(5,2,i)\n        sns.distplot(housing[col], label = col)\n        plt.legend()\n        try:\n            plt.subplot(5,2,i+1)\n        except:\n            plt.subplot(5,2,i)\n        sns.distplot(np.log(housing[col]), label= [str(col)+'_log  base'])\n        plt.legend()\n        break\n    start=end\n    end=end+2\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that using the np.log methods increases the uniformity of our features, making them more normally distributed. Let's apply the transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['population','median_income','households','total_bedrooms','total_rooms']\nfor col in cols:\n    housing[col] = np.log(housing[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the dataset features, we can also extract some additional features from. These can include:\n- rooms_per_household. Knowing the total number of rooms in a district is not very informative for our prediction. However, having the number of rooms per household could be useful information.\n- bedrooms_per_room. Here we calculate the number of beds per room in each household.\n- population_per_household. Here we calculate the number of people in each household.\n- bedrooms_per_household. Here we calculate the number of bedrooms in each household."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['rooms_per_household'] = housing['total_rooms'] / housing['households']\nhousing['bedrooms_per_room'] = housing['total_bedrooms'] / housing['total_rooms']\nhousing['population_per_household'] = housing['population'] / housing['households']\nhousing['bedrooms_per_household'] = housing['total_bedrooms'] / housing['households']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now if any of this new features are correlated with our target value"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_corr = housing.corr()\nhousing_corr['median_house_value'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The new 'bedrooms_per_room' is much more correlated with our target than the total number of rooms or bedrooms. Apparently, houses wih a lower bedroom/room ratio tend to be more expensive. We also find that the number of 'population_per_household' is also more informative than the total population (houses with lower population/household ratio tend to be more expensive). 'bedrooms_per_household' has a lower correlation coefficient, it seems to indicate the lack of a linear relationship with our target."},{"metadata":{},"cell_type":"markdown","source":"Before we build our model we have one more things to do: deal with the categorical features (as scikit-learn only works with numerical ones). List of categorical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.select_dtypes(include=['O']).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'county' is a feature that we have extracted from the latitude and longitude coordinates. It is important to note, that we have used approximate lat/long coordinates so the real location might be slightly different. However, we have seen the trend that the most expensive houses tend to be closer to the ocean. This information is already encoded in 'ocean_proximity' with a cardinality of 5 possible labels. Hence, we can discount 'county' as using this additional feature will just increase our feature space (which we do not want) and encode ocean_proximity."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.drop(['county'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\nencoded_f = OneHotEncoder(handle_unknown='ignore').fit_transform(housing.ocean_proximity.values.reshape(-1,1)).toarray()\nn = housing['ocean_proximity'].unique()\ncols = ['{}_{}'.format('Ocean_prox_',n)for n in n]\nencoded_f = pd.DataFrame(encoded_f,index=housing.index,columns=cols)\nhousing = pd.concat([housing,encoded_f],axis=1)\nhousing.drop(['ocean_proximity'],axis=1,inplace=True)\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Build the pipeline"},{"metadata":{},"cell_type":"markdown","source":"We have imputed the missing values, transformed our skewed features and handled our outliers, extracted some additional features, and encoded the categorical attributes. Now let's put them all together inside a pipeline. First, I will start with a fresh clean copy of the original stratified training set and saperate our target labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.drop(['median_house_value'],axis=1).copy()\nhousing_labels = strat_train_set['median_house_value'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(housing.shape, housing_labels.shape)\nhousing.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to create a couple of custom transformers to pass to our pipeline. To do that we can use the method FunctionTransformer. I will create two transformer: add_extra_features (to add the features we have previously extracted back to the training set) and log_transformation (which will transform our skewed features and make them more normal)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\n\ndef add_extra_features(X, add_bedrooms_per_room=True):\n    # here I take the col index of each feature of interest\n    rooms_ix, bedrooms_ix, population_ix, household_ix, median_income_ix = [\n    list(housing.columns).index(col) for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\",'median_income')]\n    \n    # here I replicate the calculations I did before but this time I am using directly the col indexes\n    rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n    population_per_household = X[:, population_ix] / X[:, household_ix]\n    bedrooms_per_household = X[:,bedrooms_ix] / X[:,household_ix]\n    median_income_per_household = X[:,median_income_ix] / X[:,household_ix]\n    #I let the user decide if return bedrooms_per_room additional to the above calculate\n    if add_bedrooms_per_room:\n        bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n        return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_household,\n                     median_income_per_household,bedrooms_per_room]\n    else:\n        return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_household,median_income_per_household]\n\n\ndef log_transformation(X):\n    import numpy as np\n    # get index cols\n    population_ix,median_income_ix,household_ix,bedrooms_ix,rooms_ix =[\n        list(housing.columns).index(col) for col in ('population','median_income','households','total_bedrooms','total_rooms')\n    ]\n    # log tranformation\n    population_log = np.log(X[:,population_ix].astype('float64'))\n    median_income_log = np.log(X[:,median_income_ix].astype('float64'))\n    household_log = np.log(X[:,household_ix].astype('float64'))\n    bedrooms_log = np.log(X[:,bedrooms_ix].astype('float64'))\n    rooms_log = np.log(X[:,rooms_ix].astype('float64'))\n    # return results\n    return np.c_[X,population_log,median_income_log,household_log,bedrooms_log,rooms_log]\n\nattr_adder = FunctionTransformer(add_extra_features, validate=False,\n                                 kw_args={\"add_bedrooms_per_room\": True})\nlog_transformed = FunctionTransformer(log_transformation,validate=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_extra_attribs = attr_adder.fit_transform(housing.values)\nhousing_log_transformed = log_transformed.fit_transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FunctionTransfomer will return an array. We can also visualise them as dataframes."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+['rooms_per_household', 'population_per_household','bedrooms_per_household',\n                     'median_income_per_household','bedrooms_per_room'],\n    index=housing.index)\nhousing_extra_attribs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_log_transformed = pd.DataFrame(\n    housing_log_transformed\n    ,columns=list(housing.columns) + ['population_log','median_income_log','household_log','bedrooms_log','rooms_log']\n    ,index=housing.index\n)\nhousing_log_transformed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can create our transformation pipeline :)."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnumerical_pipeline= Pipeline([\n    ('imputer',SimpleImputer(strategy='median',missing_values=np.nan))\n    ,('log_transform',FunctionTransformer(log_transformation,validate=False))\n    ,('add_features',FunctionTransformer(add_extra_features,validate=False))\n    ,('std_scaler',StandardScaler())\n])\n\nhousing_numerical_transformed = numerical_pipeline.fit_transform(housing.drop(['ocean_proximity'],axis=1))\nhousing_numerical_transformed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We apply the ColumnTransfomer because it allows us to apply different transformations to different features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nnumerical_f = list(housing.drop(['ocean_proximity'],axis=1))\ncategorical_f = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n    ('numericals',numerical_pipeline,numerical_f)\n    ,('categorical',OneHotEncoder(),categorical_f)\n])\nhousing_completed = full_pipeline.fit_transform(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_completed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also visualise our array as the original dataframe. To do that we simply pass the indexes and columns from our original housing set + the columns of the transformed features (following the order output of the custom transform functions) and dropping the categorical feature as this has already been encoded with onehotencoder."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing_completed_df = pd.DataFrame(housing_completed, columns=list(housing.drop(['ocean_proximity'],axis=1).columns) +['population_log','median_income_log','household_log','bedrooms_log','rooms_log','rooms_per_household', 'population_per_household','bedrooms_per_household','median_income_per_household','bedrooms_per_room']+['Ocean_prox__<1H OCEAN',\n 'Ocean_prox__NEAR OCEAN','Ocean_prox__INLAND','Ocean_prox__NEAR BAY','Ocean_prox__ISLAND'],index=housing.index)\nhousing_completed_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Model Preparation"},{"metadata":{},"cell_type":"markdown","source":"Now the fun part, here we can finally start using ML models to find the ones which best fit our data."},{"metadata":{},"cell_type":"markdown","source":"#### 5.1 Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"Since we have standardized our features, we can initially attempt using a linear regression model."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_completed_df,housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check how this model performs. First I take a subset from the original train set and its corresponding target labels from the housing_labels set. I put the former through our pipeline transformation and compare the predictions of the linear model with the actual values."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint('Predictions', np.round(lin_reg.predict(some_data_prepared),1))\nprint('Labels:',list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are they close enough? We have to use some metrics before we can answer this question."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_completed_df)\nlin_mse = mean_squared_error(housing_labels,housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RMSE measures the average squared difference between the estimated values and the actual value (the errors). A lower value indicates a good estimator. In this case a prediction error of 65856 with target value median of 179500 is not satisfying. We can also use another metric, r2_score which provides a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('Target Summary Statistics:\\nMean: {:.2f}\\nMedian: {:.2f}\\nStandard Deviation: {:.2f}'.format(housing_labels.mean(),housing_labels.median(),housing_labels.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr_score = r2_score(housing_labels,housing_predictions)\nr_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our predictions will be correct 67% of the time. This an example of model underfitting the training data. To solve this issue we can choose a more powerful model, engineer better features and feed them to the linear model, or reduce any constraints on the model. The last option can be ruled out as it has not introduced any regularization. Therefore we are left with the remaining two. Before spending time in extracting additional features, let's choose to train our training set with a more powerful model."},{"metadata":{},"cell_type":"markdown","source":"#### 5.2 DecisionTree Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_completed_df,housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"housing_predictions = tree_reg.predict(housing_completed_df)\ntree_mse = mean_squared_error(housing_labels,housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An error of 0 seems to indicate that our model has overfit the data. In this case we can use the cross-validation from scikit-learn to select a fold of the set and train it against the other folds one at the time. In this case I am choosing 10 folds, meaning that 1 fold will be picked and evaluated against the other 9 folds, this will be done 10 times over."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg,housing_completed_df,housing_labels,scoring='neg_mean_squared_error',cv=10)\nrmse_score = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('Scores:\\n{}\\nMean: {}\\nStandard Deviation:{} '.format(rmse_score,rmse_score.mean(),rmse_score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The decision tree model has a score approximately of 71471±2268(meaning 95% of values range between 66935 and 76007). Clearly, this model doesn't seem to score that well. Actually, it is underperforming when compared to our linear regression score (65856.0815). Just to be that this is the case, let's cross validate on the linear regression model too."},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_score = cross_val_score(lin_reg,housing_completed_df,housing_labels,scoring='neg_mean_squared_error',cv=10)\nlin_rmse_score = np.sqrt(-lin_score)\nprint('Scores:\\n{}\\nMean: {}\\nStandard Deviation:{} '.format(lin_rmse_score,lin_rmse_score.mean(),lin_rmse_score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3 RandomForest Regressor"},{"metadata":{},"cell_type":"markdown","source":"Another model we can apply is the RandomForestRegressor. This model works like Decision Tree but creates the trees on random subsets of the features, then averaging out their prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor(n_estimators=10,random_state=0)\nforest_reg.fit(housing_completed_df,housing_labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_predictions = forest_reg.predict(housing_completed_df)\nforest_mse = mean_squared_error(housing_labels,housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg,housing_completed_df,housing_labels,scoring='neg_mean_squared_error',cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint('Scores:\\n{}\\nMean: {}\\nStandard Deviation:{} '.format(forest_rmse_scores,forest_rmse_scores.mean(),forest_rmse_scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model looks more promising than the previous two. We can save this model for future use and jump in the hyperparamter tuning phase (which is essentialy where we tune the parameters of our model)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\njoblib.dump(forest_reg,'forest_reg.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Model Tuning"},{"metadata":{},"cell_type":"markdown","source":"The most efficient way to find the optimal parameters for our models is to make use of a GridSearchCV which will automatically create all the possible combinations of input parameters configurations."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30,40,50], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\nforest_reg = RandomForestRegressor(random_state=0)\ngrid_search = GridSearchCV(forest_reg,param_grid,cv=5,scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(housing_completed_df,housing_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the best parameters and estimator:"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also check the scores of each hyperparameter combination:"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_results = grid_search.cv_results_\ngrid_results.keys()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for mean_score,params in zip(grid_results['mean_test_score'],grid_results['params']):\n    print(np.sqrt(-mean_score),params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this example, we obtain the the best solution by setting max_features to 8 and the n_estimators to 50. The RMSE score for this combination is 51604, which is a slight better score than using the default parameters (53129). The fact that our gridsearch has chosen 50 for n_estimators (which was the max value we have provided), may indicate that we should re-run the search using higher parameters. In this case, when the hyperparameter search space is large, it's often preferable to use a RandomsizedSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\nparam_distributions = {\n    'n_estimators': randint(low=1, high=200)\n    ,'max_features': randint(low=1, high=8)\n    }\n\nforest_reg = RandomForestRegressor(random_state=0)\nrandom_search = RandomizedSearchCV(forest_reg,param_distributions=param_distributions,n_iter=20,cv=5,scoring='neg_mean_squared_error', return_train_score=True)\nrandom_search.fit(housing_completed_df,housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for mean_scores,params in zip(random_search.cv_results_['mean_test_score'],random_search.cv_results_['params']):\n    print(np.sqrt(-mean_scores),params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hey, our randomised search with 7 features and 151 estimators has given us a slight better error score (51212 vs 51604 of the previous one). Once picked our model with the best hyperparameters, we can check the relative importance of each feature for making accurate predictions"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = random_search.best_estimator_\nmodel.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They don't mean much, unless we pair them with our corresponding features:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"feature_names = housing_completed_df.columns\nsorted(zip(model.feature_importances_,feature_names),reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like only one categorical feature from the onehotencoder actually contributes to the model performance. And in general some our log transformed feature perform better than their not-transformed correspondants. So, we can train another regressor to see the differences (keep the same hyperparameters)."},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_completed_less_features = housing_completed_df.drop(['Ocean_prox__<1H OCEAN','Ocean_prox__INLAND', 'Ocean_prox__NEAR BAY', 'Ocean_prox__ISLAND','population','total_rooms','household_log','household_log'],axis=1)\n\nforest_reg_2 = RandomForestRegressor(n_estimators=151,random_state=0,max_features=7)\nforest_reg_2.fit(housing_completed_less_features,housing_labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_predictions = forest_reg_2.predict(housing_completed_less_features)\nforest_mse = mean_squared_error(housing_labels,housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not bad, especially when compared to the the first default parameters random forest rmse scores (22279). However, it's better to cross-validate once more."},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg_2,housing_completed_less_features,housing_labels,scoring='neg_mean_squared_error',cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\nprint('Scores:\\n{}\\nMean: {}\\nStandard Deviation:{} '.format(forest_rmse_scores,forest_rmse_scores.mean(),forest_rmse_scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We went from a mean error score of 53129 to 50613. Not bad. In theory we could have another random gridsearch to find even more optimal parameters for our model. But hey, r2_score is 80% and this is my second notebook. I call it a win for today. In the future we could also try to utilise a SVM or a GradientBoostingRegressor and tune their parameters to search for the best scores. For now, this model will do just fine."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"forest_r2_scores = cross_val_score(forest_reg_2,housing_completed_less_features,housing_labels,scoring='r2',cv=10)\nprint('Scores:\\n{}\\nMean: {}\\nStandard Deviation:{} '.format(forest_r2_scores,forest_r2_scores.mean(),forest_r2_scores.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Evaluation"},{"metadata":{},"cell_type":"markdown","source":"Okay, in the final part we take our unused strat testing set, quickly transform it via the pipeline, make our predictions with the latest random forest regressor model and calculate rmse and r2 scores."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"final_model = forest_reg_2\nfinal_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = strat_test_set.drop(['median_house_value'],axis=1)\ny_test = strat_test_set['median_house_value'].copy()\n\nX_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_preprocessed = full_pipeline.transform(X_test)\n\n# we have to remove the features that we are not using anymore - the original pipeline does not reflect the latest changes\nX_test_preprocessed = pd.DataFrame(X_test_preprocessed,columns=housing_completed_df.columns,index=strat_test_set.index).drop(['Ocean_prox__<1H OCEAN','Ocean_prox__INLAND', 'Ocean_prox__NEAR BAY', 'Ocean_prox__ISLAND','population','total_rooms','household_log','household_log'],axis=1)\n\nfinal_predictions = final_model.predict(X_test_preprocessed)\n\nfinal_mse = mean_squared_error(y_test,final_predictions)\nfinal_rmse = np.sqrt(final_mse)\nfinal_r2_score = r2_score(y_test,final_predictions)\nprint('RMSE Score: {}\\nR2 Score: {}'.format(final_rmse,final_r2_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also compute a 95% confidence interval z-scores for the RMSE test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nmean = squared_errors.mean()\nm = len(squared_errors)\n\nzscore = stats.norm.ppf((1 + confidence) / 2)\nzmargin = zscore * squared_errors.std(ddof=1) / np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's it folks. We have created a model with a r2 score of 81% and RMSE of 48842 (which is 25% lower than the original 65856 given by the linear regressor). I will come back to this notebook and updated it from time to time with new things I'll learn along the way. <br>\n\nIdeas for future improvement:\n- add references and expand description of the notebook\n- use SVM, GradientBoostingRegressor\n- create a single pipeline to do transformation and prediction all at once\n- deal with the capping intervals (e.g. 500k,450k, etc.) shown by the scatter plot during the EDA section."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}