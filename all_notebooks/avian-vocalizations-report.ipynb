{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"\"\"\" `imageio_ffmpeg` contains a pre-built `ffmpeg` binary, needed for mp3 decoding by `librosa`. \n    It is installed as a custom package on Kaggle. If no `ffmpeg` binary is found in \n    `/usr/local/bin` then create a softlink to the `imageio_ffmpeg` binary. \n\"\"\"\nimport os\nif not os.path.exists(\"/usr/local/bin/ffmpeg\"): \n    #! pip install imageio_ffmpeg \n    import imageio_ffmpeg\n    os.link(imageio_ffmpeg.get_ffmpeg_exe(), \"/usr/local/bin/ffmpeg\")\n    \n\"\"\" Common imports \"\"\"\nimport pandas as pd\nimport numpy as np\nimport librosa \nfrom librosa.display import specshow\nfrom IPython.display import Audio\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport keras\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom glob import glob\nimport re\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.naive_bayes import GaussianNB\nfrom itertools import islice\n\ndef parse_shape(shape_str):\n    \"\"\"Shape was saved in feature_shapes as a string. Woops.\n       Convenience funtion to parse out the values. \"\"\"\n    a,b = re.search('\\((\\d+), (\\d+)\\)',shape_str).groups()\n    return int(a), int(b)\n\ndef log_clipped(a):\n    \"\"\"Convenience function to clip the input to positive values then return the log.\"\"\" \n    return np.log(np.clip(a,.0000001,a.max()))\n\n\"\"\" Datasets \"\"\"\nsg_dir = \"../input/avian-vocalizations-spectrograms-and-mfccs/melspectrograms/features/\"\nsglog_dir = \"../input/avian-vocalizations-melspectrograms-log-norm/melspectrograms_logscaled_normalized/features/\"\nmfcc_dir = \"../input/avian-vocalizations-spectrograms-and-mfccs/mfccs/features/\"\n\nshapes_df = pd.read_csv(\"../input/avian-vocalizations-spectrograms-and-mfccs/feature_shapes.csv\",\n                        index_col=0 )\n# Parse the length from the melspectrogram_shape field, which was inadvertantly saved as a string\ntrain_df = pd.read_csv(\"../input/avian-vocalizations-partitioned-data/train_file_ids.csv\",\n                       index_col=0)\ntest_df = pd.read_csv(\"../input/avian-vocalizations-partitioned-data/test_file_ids.csv\",\n                      index_col=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Engineer Nanodegree\n## Capstone Project\nSam Hiatt\nAug 15, 2019\n\n## I. Project Definition\n\n### Overview\n\n#### Background\n\nMany social animals communicate using vocalizations that can give clues to their species as well as to their intent. The ability to automatically classify audio recordings of animal vocalizations opens up countless opportunities for sound-aware computer applications and could help accelerate studies of these animals. For example, a classifier trained to recognize the call of a specific species of bird could be used to trigger a camera recording, or automatically tag a live audio stream containing avian calls with the species of the bird that made it, producing a time-series record of the presence of this species.\n\n[Xeno-Canto.org](https://www.xeno-canto.org/) is an online community and crowd-sourced Creative Commons database containing audio recordings of avian vocalizations from around the world, indexed by species. It presents a good opportunity for experimentation with machine learning for classification of audio signals. The [Xeno-Canto Avian Vocalizations CA/NV, USA](https://www.kaggle.com/samhiatt/xenocanto-avian-vocalizations-canv-usa) dataset was procured for the purpose of jumpstarting exploration into this space. It contains a small subset of the available data, including 30 varying-length audio samples for each of 91 different bird species common in California and Nevada, USA.\n\n[Spectrograms](https://en.wikipedia.org/wiki/Spectrogram) (also called sonograms) map audio signals into 2-dimensional frquency-time space, and have long been used for studying animal vocalizations. [Bird Song Research: The Past 100 Years](https://courses.washington.edu/ccab/Baker%20-%20100%20yrs%20of%20birdsong%20research%20-%20BB%202001.pdf) describes how a device called the Sona-Graphâ„¢, developed by Kay Electric in 1948, began to be used by ornithologists in the early 1950's and accelerated avian bioacoustical research. The project [DeepSqueak](https://github.com/DrCoffey/DeepSqueak) at the University of Washington in Seattle uses spectrograms of ultrasonic vocalizations of mice and takes a deep learning approach to classifying their recordings. Their publication in Nature, [DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations](https://www.nature.com/articles/s41386-018-0303-6), shows how their classifier us used to study correlations between types of vocalizations and specific behaviors.\n\n[Mel-frequency Cepstral Coefficients (MFCCs)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) also map audio signals into 2-dimensional space and are commonly used in voice recognition tasks. Inspired by the DeepSqueak's use of spectrograms as inputs to convolutional neural networks, this project uses spectrograms and MFCCs to train a classifier for avian vocalizations.\n\n### Problem Statement\n\nThis project defines and trains a digital audio classifier to predict bird species given an audio sample of avian vocalizations. After being trained on a preprocessed dataset of labeled samples, the classifier will be able to read an mp3 file and return a single label representing the common English name of the most prevalent predicted species in the recording.\n\nThis effort focuses on just a small subset of the available xeno-canto.org data. While many additional samples are available and could be used to improve the accuracy for any particular species of interest, or more species could be added, such refinement is left for future work.\n\n### Metrics\n\nPerformance is evaluated during model selection and training by calculating the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) on a 3-fold cross validation data split of the training data. Originally a 5-fold cross validation was planned, but after some experimentation it was determined that using 3 folds was sufficient as results were stable between splits.\n\nEvaluation of accuracy gives equal weight to each class, considering them all equally important to identify, and is simply defined as the portion of samples correctly classified. So, for example, a model that correctly predicts the label (one out of 91 species) half of the time, would have a score of `0.50`. \n\nFinal model performance is evaluated by first training the best model chosen during the model selection phase on the entire training dataset (without cross validation splits), and final test accuracy is evaluated by predicting labels on the designated test dataset and comparing to their true values.\n\n## II. Analysis\n\n### Data Exploration\n\nLet's take a look at the dataset. We'll load the index and take a look at how the classes are distributed and verify that each species is equally represented. "},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"data_dir = '../input/xenocanto-avian-vocalizations-canv-usa/xeno-canto-ca-nv/'\ndf = pd.read_csv(\"../input/xenocanto-avian-vocalizations-canv-usa/xeno-canto_ca-nv_index.csv\", \n                 index_col='file_id')\nlabel_encoder = LabelEncoder().fit(df['english_cname'] )\nn_classes = len(label_encoder.classes_)\nprint(\"The dataset contains %i distinct species labels.\"%n_classes)\nprint(\"%i mp3s found in %s\"%(len(glob(data_dir+\"*.mp3\")), data_dir))\n\ny_encoded_entire_dataset = np.array(label_encoder.transform(df['english_cname']))\nplt.figure(figsize=(15,2)); plt.title(\"Distribution of Samples per Species\")\nplt.hist(y_encoded_entire_dataset, bins=91 ); plt.xlim(-1,91)\nplt.ylabel(\"Number of Samples\")\nplt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset has a balanced distribution in terms of the number of samples per species, with 30 samples for each of 91 species. \n\nThe fact that the number of samples per class is balanced is important as each species should be represented by recordings with a variety of different environmental conditions. If, say, a single sample was chopped up and used as multiple examples for training the model would likely end up overfitting to environental factors specific to that recording, for example by picking up on the the sound of a waterfall in the background instead of listening to the birds. Having a balanced number of samples per class should help regularize environmental factors like these. \n\nLet's take at how the data is distributed in terms of the total duration of audio samples per class."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"duration_by_class=[df.loc[df.english_cname==y,'duration_seconds'].sum()/60 \n                   for y in label_encoder.classes_]\nplt.figure(figsize=(15,2))\nplt.title(\"Total Duration of Audio Samples per Species\")\nplt.bar(range(n_classes), duration_by_class, .75)\nplt.ylabel(\"minutes of audio\")\nplt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical')\nplt.xlim(-1,91)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sample lengths range from 185 seconds to 1877 seconds of audio, with a mean of 80 seconds. The data is _not_ evenly distributed in terms of the total duration of audio per class. This is due to the process used to compile the list of audio samples for each species. In particular, the 30 _shortest_ samples for each species recorded in California and Nevada were downloadedfrom xeno-canto.org, with the intention of reducing the load on the servers. This results in a dataset containing shorter samples for species that are more commonly recorded.  It will be interesting to see how this affects model performance. "},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Visualization\n\nLet's listen to and take a look at a few samples.  "},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"ryans_samples=df[df.recordist.str.contains('O\\'Donnel')]\nfor i,sample in islice(ryans_samples.iterrows(),0,3):\n# sample = df.loc[119160]\n    print(\"%s: %s, contributed by: %s %s\"%(\n        sample.file_name,sample.full_name,sample.recordist,sample.recordist_url))\n    data, samplerate = librosa.load(\n        \"../input/xenocanto-avian-vocalizations-canv-usa/xeno-canto-ca-nv/\"+sample.file_name)\n    Audio(data, rate = samplerate)\n    hop_length = 512\n    sg = librosa.feature.melspectrogram(data, sr=samplerate, hop_length=hop_length, n_fft=2048)\n    mfcc = librosa.feature.mfcc(data, sr=samplerate, hop_length=hop_length, n_fft=2048)\n    fig = plt.figure(figsize=(15,5)); #plt.subplots_adjust(hspace=4)\n    gs = GridSpec(12,2, fig, hspace=5) # get a grid of 12 rows x 2 cols\n    fig.suptitle(\"%s: %s\"%(sample.file_name, sample.full_name))\n    hist_ax1 = fig.add_subplot(gs[0:4,0])\n    hist_ax2 = fig.add_subplot(gs[4:8,0])\n    mfcc_hist_ax = fig.add_subplot(gs[8:,0])\n    sg_ax = fig.add_subplot(gs[:8,1])\n    mfcc_ax = fig.add_subplot(gs[8:,1])\n    hist_ax1.hist(sg.flatten(), bins=100)\n    hist_ax1.set_title(\"Histogram of Melspectrogram Pixels\")\n    hist_ax2.hist(np.log(sg.flatten()), bins=100)\n    hist_ax2.set_title(\"Histogram of log(Melspectrogram)\")\n    mfcc_hist_ax.hist(mfcc.flatten(), bins=100)\n    mfcc_hist_ax.set_title(\"Histogram of MFCC Pixels\")\n    specshow(np.log(sg), y_axis='mel', x_axis='time', ax=sg_ax, hop_length=hop_length)\n    sg_ax.set_title(\"log(Melspectrogram)\")\n    plt.colorbar(sg_ax.get_children()[0], ax=sg_ax)\n    specshow(mfcc, x_axis='time', ax=mfcc_ax, hop_length=hop_length)\n    mfcc_ax.set_title(\"MFCC\")\n    plt.colorbar(mfcc_ax.get_children()[0], ax=mfcc_ax)\n    display(Audio(data, rate=samplerate))\n    fig.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After visualizing the spectrograms it is clear that their values have an exponential distribution. Scaling them with `np.log()` allows us to visualize the textures of the vocalizations. \n\nThe MFCC values are mostly normally distributed, but it appears the bottom row has values that fall well below all the rest.  \n\nThe mean pixel and standard deviation of the spectrogram, log(melspectrogram), and mfcc arays are calculed here. "},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"sg_scaler = StandardScaler()\nsg_log_scaler = StandardScaler()\nct=0\nfor file_id in df.index:\n    shape = parse_shape(shapes_df[shapes_df.file_id==file_id]['melspectrogram_shapes'].values[0])\n    sg = np.memmap('%s/XC%s_melspectrogram.dat'%(sg_dir,file_id), \n                   dtype='float32', mode='readonly', shape=shape).flatten()\n    sg_scaler.partial_fit(sg.reshape(-1,1))\n    sg_log_scaler = sg_log_scaler.partial_fit(log_clipped(sg).reshape(-1,1))\n    ct+=shape[0]*shape[1]\nprint(\"Total number of melspectrogram pixels: %i, mean: %.5f, std. dev: %.5f\"%(ct, sg_scaler.mean_, np.sqrt(sg_scaler.var_)))\nprint(\"log(melspectrogram_agg) mean: %.5f, std. dev: %.5f\"%(sg_log_scaler.mean_, np.sqrt(sg_log_scaler.var_)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```Total number of melspectrogram pixels: 412313856, mean: 0.29080, std. dev: 15.13515\nlog(melspectrogram_agg) mean: -7.34926, std. dev: 3.56474```"},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"mfcc_agg=np.array([])\nmfcc_scaler = StandardScaler()\nct=0\nfor file_id in df.index:\n    shape = parse_shape(shapes_df[shapes_df.file_id==file_id]['mfcc_shapes'].values[0])\n    mfcc = np.memmap('%s/XC%s_mfcc.dat'%(mfcc_dir,file_id), \n                     dtype='float32', mode='readonly', shape=shape).flatten()\n    mfcc_scaler.partial_fit(mfcc.reshape(-1,1))\n    ct+=shape[0]*shape[1]\nprint(\"Total number of MFCC pixels: %i, mean: %.5f, std. dev: %.5f\"%(ct,mfcc_scaler.mean_,np.sqrt(mfcc_scaler.var_)))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Total number of MFCC pixels: 64424040, mean: -19.00330, std. dev: 86.45496`"},{"metadata":{},"cell_type":"markdown","source":"### Algorithms and Techniques\n\nThe log scaled spectrograms produce visualizations with distinctive shapes and textures. The inherent interdependence of pixels that are near each other in the spectrogram make it an appropriate task for a convolutional neural network as this essentially turns this problem into a classic image classification problem. A similar model to that which was used in the [dog species classifier](https://www.kaggle.com/samhiatt/udacity-dog-project-model-selection-and-tuning) project. This model seems to perform well classifying images of dogs and using it should test the hypothesis that a CNN will perform better than the benchmark Naive Bayes model, and similarly it will be trained using gradient descent. \n\nThe MFCC features contain information about the vocal characteristics of the frame. Adding them to the feature space can perhaps help improve predictions. Since they are correlated in time with the spectrogram, a convenience technique is applied to concatenate the two input arrays to produce a single 2-dimensional array for input to the CNN. \n\nData augmentation will be employed by using a data generator that crops samples from equal-length windows of input data with a random offset. \n\nIn order to evaluate the performance of models during experimentation, experimental models are trained and evalued on a 3-fold stratified and shuffled split to help exaluate stability and prevent model over-fitting. Final performance is evaluated by re-training the model on the entire training dataset and then evaluating against the test dataset. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Benchmark\n\nA purely random predictor would be correct 1.1% of the time (1/91 classes). A [Gaussian Naive Bayes classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) applied to the spectrogrampixels should perform better than random guessing and is used as a benchmark predictor. It is expected that this predictor will become sensitive to certain frequency bands that are common in a particular species' vocalizations and that this will give it some predictive power. The naieve assumption of feature independence is explected to limit this models's performance, but it should still provide a good baseline. "},{"metadata":{},"cell_type":"markdown","source":"\n## III. Methodology\n\n### Data Preprocessing\n\nThe Kaggle kernel [Avian Vocalizations: Data Preprocessing](https://www.kaggle.com/samhiatt/avian-vocalizations-data-preprocessing) documents the data preprocessing methodology used to decode audio input files, generate spectral features, calculate statistics, and then scale and normalize data. The same steps are taken for decoding and visualizing the samples in the Exploratory Visualization section above. \n\nMp3s are first decoded and Spectrograms and MFCCs are computed using librosa. The resulting arrays are stored as memory-mapped data files and saved in the Kaggle dataset [Avian Vocalizations: Spectrograms and MFCCs](https://www.kaggle.com/samhiatt/avian-vocalizations-spectrograms-and-mfccs) and used as input for further processing steps. \n\nLog scaling the spectrograms accounts for their exponential disctibution, and normalization zero-centers the data, making it ready for intput into a neural network. The dataset [Avian Vocalizations: Melspectrograms Log Norm](https://www.kaggle.com/samhiatt/avian-vocalizations-melspectrograms-log-norm) contains the scaled and normalized spectrograms preprocessed by calculating the `log`, subtracting the mean log scaled pixel (`mean(log(melspectrograms)) = -7.34926`, and dividing by the standard deviation (`std(log(melspectrogram)) = 3.56474`) This preprocessing is documented in the kernel [Fork of Avian Vocalizations: Data Transformation](https://www.kaggle.com/samhiatt/fork-of-avian-vocalizations-data-transformation?scriptVersionId=18761461) Use of this preprocessed data is optional as data scaling and normalization can alternatively be done in the data generation step.\n\nA data generator modeled after [Shervine Amidi's example](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) is used to read the mem-mapped spectrograms and MFCCs and produce optionally shuffled batches of equal-length normalized samples with one-hot encoded labels. A seed is used for shuffling to allow reproducibility. One-hot encoding the labels enables categorical classification as it removes the ordinality of the encoded labels. Normalization can be done in this step if the data on disk is not already normalized. This is the case with the MFCC data due to storage space errors encountered during Kaggle kernel execution. It's fast enough to do during data generation, so for convenience MFCC normalization is done in this phase. \n\nThe data generator is also responsible for combining the spectrogram and MFCC inputs into a single 2-dimensional array by either concatenating the MFCCs to the top of the spectrograms, or by overwriting the lower frequency bands of the spectrograms with the MFCC data. Both of these approaches for combining the arrays were evaluated for performance.\n\nIn order to select a random window of specified length from the input sample, the data generator randomly selects (again using a seed, for reproducibility) a window offset for each sample. If the input file is shorter than the crop window then the output array is padded with the dataset mean pixel value, or 0 in the case of a normalized dataset. This choice for padding the samples has implications that are discussed in the results section. \n\nThe dataset was first partitioned with [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) reserving 1/3 of the dataset for testing, and again supplying a seed for reproducibility. This output of this split is saved in the dataset [Avian Vocalizations: Partitioned Data](https://www.kaggle.com/samhiatt/avian-vocalizations-partitioned-data) and used for training / testing in subsequent steps. \n\nLet's load the partitioned data take a look at some outputs from the generator."},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = list(train_df.index)\ny_train = list(train_df.label)\nprint(\"Training data len:\",len(X_train))\nX_test = list(test_df.index)\ny_test = list(test_df.label)\nprint(\"Test data len:    \",len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\nclass AudioFeatureGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size, n_frames=128, n_channels=1,\n                 n_classes=10, shuffle=False, seed=37):\n        'Initialization'\n        self.n_frames = n_frames\n        self.dim = (128, self.n_frames)\n        self.batch_size = batch_size\n        self.labels = {list_IDs[i]:l for i,l in enumerate(labels)}\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.seed = seed\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n\n    def on_epoch_end(self):\n        'Update indexes, to be called after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.seed(self.seed)\n            self.seed = self.seed+1 # increment the seed so we get a different batch.\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        #X = np.empty((self.batch_size, 128+20, self.dim[1], self.n_channels))\n        y = np.empty((self.batch_size, self.n_classes), dtype=int) # one-hot encoded labels\n\n        for i, ID in enumerate(list_IDs_temp):\n            sg_lognorm = np.memmap(sglog_dir+'XC%s_melspectrogram_logscaled_normalized.dat'%ID, \n                    shape=parse_shape(shapes_df[shapes_df.file_id==ID]['melspectrogram_shapes'].values[0]),  \n                    dtype='float32', mode='readonly')\n#             sg = np.memmap(sg_dir+'XC%s_melspectrogram.dat'%ID, \n#                     shape=parse_shape(shapes_df[shapes_df.file_id==file_id]['melspectrogram_shapes'].values[0]),  \n#                     dtype='float32', mode='readonly')\n            mfcc = np.memmap(mfcc_dir+'XC%s_mfcc.dat'%ID, \n                    shape=parse_shape(shapes_df[shapes_df.file_id==ID]['mfcc_shapes'].values[0]),  \n                    dtype='float32', mode='readonly')\n            # Normalize MFCCs\n            mfcc = mfcc_scaler.transform(mfcc)\n            \n            # Filter out quiet frames, thanks to:\n            # https://www.kaggle.com/fleanend/extract-features-with-librosa-predict-with-nb\n            # Take mean amplitude M from frame with highest energy\n#             m = sg[:,np.argmax(sg.mean(axis=0))].mean()\n#             # Filter out all frames with energy less than 5% of M\n#             mask = sg.mean(axis=0)>=m/20\n#             sg = sg[:,mask]\n#             sg_lognorm = sg_lognorm[:,mask]\n#             mfcc = mfcc[:,mask]\n            \n            # Pick a random window from the sound file\n            d_len = mfcc.shape[1] - self.dim[1]\n            if d_len<0: # Clip is shorter than window, so pad with mean value.\n                n = int(np.random.uniform(0, -d_len))\n                pad_range = (n, -d_len-n) # pad with n values on the left, clip_length - n values on the right \n#                 sg_cropped = np.pad(sg, ((0,0), pad_range), 'constant', constant_values=sg.mean())\n                sg_lognorm_cropped = np.pad(sg_lognorm, ((0,0), pad_range), 'constant', constant_values=0)\n                mfcc_cropped = np.pad(mfcc, ((0,0), pad_range), 'constant', constant_values=0)\n            else: # Clip is longer than window, so slice it up\n                n = int(np.random.uniform(0, d_len))\n#                 sg_cropped = sg[:, n:(n+self.dim[1])]\n                sg_lognorm_cropped = sg_lognorm[:, n:(n+self.dim[1])]\n                mfcc_cropped = mfcc[:, n:(n+self.dim[1])]\n                \n            # Stack the MFCCs and spectrograms to create a single array\n            #X[i,] = np.concatenate([sg_lognorm_cropped.reshape(1,128,self.dim[1],1), \n            #                        mfcc_cropped.reshape(1,20,self.dim[1],1)], axis=1)\n            X[i,] = sg_lognorm_cropped.reshape(1,128,self.dim[1],1)\n            # Overwrite the bottom of X with MFCCs (we don't need the low frequency bands anyway) \n            X[i,:20] = mfcc_cropped.reshape(1,20,self.dim[1],1)\n            y[i,] = to_categorical(self.labels[ID], num_classes=self.n_classes)\n\n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"generator = AudioFeatureGenerator(X_train, y_train, \n            batch_size=3, shuffle=False, seed=37, n_frames=128, n_classes=n_classes)\n# the 32nd batch happens to include a short sample that has been paded. Let's show it.\nXs, ys = generator[31] \nfor X, y in list(zip(Xs, ys)): #(X,y for each of the 3 samples in the batch)\n    plt.figure(figsize=(10,4))\n    spec_ax = specshow(X.squeeze(), x_axis='time', y_axis='mel')\n    plt.title(label_encoder.classes_[np.argmax(y)])\n    plt.colorbar()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the generator is producing equal-length clips of scaled, zero-centered 2-dimensional data. Notice how the first sample has a reocrding that is shorter than the clip window length and so it has been padded with zeros. \n\nLet's take a look to see how data augmention is functioning. Does it produce different clips from the same sample? Let's call the generator again and compare to the clips above."},{"metadata":{"trusted":false},"cell_type":"code","source":"Xs, ys = generator[31] # Calling the generator again grabs a different random window.\nfor X, y in list(zip(Xs, ys)): \n    plt.figure(figsize=(10,4))\n    spec_ax = specshow(X.squeeze(), x_axis='time', y_axis='mel')\n    plt.title(label_encoder.classes_[np.argmax(y)])\n    plt.colorbar()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice how the samples are shifted along the time axis. So each time the generator is called a new clip is created. This technique should help the model better generalize to new data by making it sensitive to the patterns at whatever time step they occur in the sample. \n\n### Implementation\n\nThe model is implemented using a similar architecture to that used for the [dog species classifier](https://www.kaggle.com/samhiatt/udacity-dog-project-model-selection-and-tuning) project. This model contains three stacks of 2-d Convolution and MaxPooling layers followed by a Dropout layer with a rate of `0.2`. Convolutional layers use a ReLU activation function. The Dropout layer masks 20% of the input neurons in each layer and effectively causes the model to develop redundant neural pathways which will help the model generalize betetr to unseen data. THe output of these stacks is fed into a global average pooling layer, followed by a fully-connected layer with a softmax activation function. The position of the maximum value of this output corresponds to the predicted label.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\ndim=(128,128)\nmodel.add(Conv2D(64,3,input_shape=(dim[0], dim[1], 1),padding='valid',activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=3))\nmodel.add(Dropout(rate=.2))\nmodel.add(Conv2D(64,3,padding='valid',activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=3))\nmodel.add(Dropout(rate=.2))\nmodel.add(Conv2D(64,3,padding='valid',activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=3))\nmodel.add(Dropout(rate=.2))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(n_classes, activation=\"softmax\"))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A [Stratified Shuffle Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) is created from the training data, and then each training split is used to train the network using the data generator implemented above. The [keras Sequence.fit_generator](https://keras.io/models/sequential/#fit_generator) method is used to train and evaluate the model after each training epoch using instances of the generator implemented above. Evaluation is done on a single batch containing all of the validation samples. The validation data generator does not shuffle the data, however it does still augment the data by producing different cropped windows for each epoch. \n\nLet's try out the pipeline and train the model with cross-validation for just 3 epochs of 3 different splits."},{"metadata":{"trusted":false},"cell_type":"code","source":"n_splits = 1\nn_epochs = 1\nsss = StratifiedShuffleSplit(n_splits=n_splits, test_size=1/4, random_state=37)\nscores = []\nparams = {'n_frames': 128,\n          'n_classes': n_classes,\n          'n_channels': 1}\nfor cv_train_index, cv_val_index in sss.split(X_train, y_train):\n    training_generator = AudioFeatureGenerator(\n        [X_train[i] for i in cv_train_index], \n        [y_train[i] for i in cv_train_index], \n        batch_size=64, shuffle=True, seed=37, **params)\n    validation_generator = AudioFeatureGenerator(\n        [X_train[i] for i in cv_val_index], \n        [y_train[i] for i in cv_val_index], \n        batch_size=len(cv_val_index), **params)\n    \n    partial_filename = \"cnn.split%02i\"%len(scores)\n    checkpointer = ModelCheckpoint(filepath='weights.best.%s.hdf5'%partial_filename, verbose=1, save_best_only=True)\n    \n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    learning = model.fit_generator(\n                training_generator, \n                validation_data=validation_generator,\n                epochs=n_epochs, \n                steps_per_epoch=np.ceil(len(cv_train_index)/training_generator.batch_size),\n                validation_steps=1,\n                callbacks=[checkpointer], \n#                 use_multiprocessing=True, workers=4,\n                verbose=0, )\n#     pd.DataFrame(learning.history).to_csv('training_history_split%i.csv'%len(scores), index_label='epoch')\n#     vis_learning_curve(learning)\n#     plt.savefig(\"learning_curve.%s.png\"%partial_filename)\n#     plt.show()\n    acc_at_min_loss = learning.history['val_acc'][np.argmin(learning.history['val_loss'])]\n    scores.append(acc_at_min_loss)\n    print(\"Split %i: min loss: %.5f, accuracy at min loss: %.5f\"%(\n        len(scores), np.min(learning.history['val_loss']), acc_at_min_loss ))\nprint(\"Cross Validation Accuracy: mean(val_acc[argmin(val_loss)]): %.4f\"%(np.mean(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model is able to retrieve batches from the training generator and evaluate accuracy against the validataion data. The same structure above is used to train and evaluate different versions of the model in separate kaggle kernels. The results of these experiments are reported in the next section.\n\n\n### Refinement\n\nSeveral different model configurations were trained and evaluated. It was observed in general that increasing the number of neurons per layer improved accuracy, as was expected.\n\nFrame filtering was implemented following the methodolgy presented in Edoardo Ferrante's notebook [Extract features with Librosa, predict with NB](https://www.kaggle.com/fleanend/extract-features-with-librosa-predict-with-nb), however after some initial experimentation it didn't seem to improve results. It was apparent that it resulted in many more short samples requiring padding and also removed information related to tempo, distorting many of the distinguishing characteristics of the vocalizations in the spectrograms. So this approach was abandoned.\n\nModels with different kernel and max pooling sizes, including 1-row tall convolutional kernels and MaxPooling layers were tried. The hypothesis was that the pitch of each vocalization is important and that convolution applied only to the time dimension might preserve these frequencies. This approach was tried and evaluated in the notebook [Version 16: CNN Classifier](https://www.kaggle.com/samhiatt/avian-vocalizations-cnn-classifier/output?scriptVersionId=18872310). This model includes 64 filters for each convolutional lateyer, and uses 1x4 convolutional kernels and max pooling sizes of 1x4, 1x3, and 1x2, respectively for each layer. It is evaluated on 3 splits for 100 epochs and achieves a score of: `0.0762`. Not much better than the benchmark. \n\nIn [Version 18](https://www.kaggle.com/samhiatt/avian-vocalizations-cnn-classifier/output?scriptVersionId=18878731) a similar model was evaluated, except with 3x3 convolutional kernels and 2x2 max max pooling. It was trained on 3 splits to 100 epochs each and achieved a score of: `0.1238`. \n\nThe model in [Version 17](https://www.kaggle.com/samhiatt/avian-vocalizations-cnn-classifier/output?scriptVersionId=18872556) similarly has 60 filters per layer and uses 3x3 convolutions, but 3x3 max pooling. It achieves a score of: `0.18680`.\n\nAll of the versions above use a generator that concatenates the MFCCs to the top of the spectrograms. Another aproach was evaluated using an alternative method to combine the input arrays, simply overwriting the lowest 20 frequencies of the spectrograms with the MFCCs. The hypothesis was that the lower frequencies are unimportant for avian vocalization identification, and the results seemed to support this, as the model in [Version 20](https://www.kaggle.com/samhiatt/avian-vocalizations-cnn-classifier?scriptVersionId=18897485) has an identical structure to that in version 17, except that it uses this alternative method of combining inputs. It achieved the top score of: `0.1927`. This model architecture and data generation method was chosen for final evaluation.\n\nShown below are the learning curves from the output of this training session. The minimum loss is achieved after about 80-100 epochs, and this point is indicated in the plots with a red vertical line. \n\n![](https://www.kaggleusercontent.com/kf/18897485/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..qjUgwDBHC6j_jcBL2Ml3Pw.JrBRZDZ9DFs_ICdzSx5A3ErbgmQKnTESxp4Om5V7jcx0U7c7zck1SLyUnY9y5cy_966QawB19jS7yA1KUX4hUXB4_l76uF4gV1f8jdjvQdVZV4cptcwN3AqB5V1JcLul4EcsAmPET4PftAi3xC5KMWB46-RgveJ1RmTpHZtpp-U.pfIsGXgKNZzCuOlQKHZdJQ/learning_curve.cnn.16.32.64.split02.png)\n![](https://www.kaggleusercontent.com/kf/18897485/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..J9LeyHynwn2a1XOqeXGlFA.fPq-m2YMwbcoUB62rLVfxUx-PqJVO7w05H-LNc1ryHVePcCFeWQfoMVuVgJbzKxjy8iZegf7OcR70gpgJxp4qDpyRiejaI_KlJ8UEb7-Tc1vHO7DnfAsIMih_gWJ6rciTKNuQfkN4ujpR21se6hPR5BiCCJQHr5Au2e1gwEqbu0.N04K-JiEZQNMATxTIx_HqQ/learning_curve.cnn.16.32.64.split01.png)\n![](https://www.kaggleusercontent.com/kf/18897485/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..IhEwMSfm02v3FCJRmAza7A.Jo_gjrFFg7uyYbkb2fxTMM-vFMsXaMl3SNabQYzF9UnyorfapApwqA9Y7KzRvHT5Av9R7PWONq7w_fnqffTLpNoi_TekWh5mgMbgl8UVghMPFgEq0S8X10pqsGfTBu8yWfRkcPDIFbsFcSwiBEqted1RozU7MKwUy-6MzOMbTFc.wDWfWhGH0dum-qoIrfOQ7Q/learning_curve.cnn.16.32.64.split00.png)\n\n"},{"metadata":{},"cell_type":"markdown","source":"## IV. Results\n\n### Model Evaluation and Validation\n\nThe model from [CNN Classifier: Version 20](https://www.kaggle.com/samhiatt/avian-vocalizations-cnn-classifier?scriptVersionId=18897485) achieved the best score during scross-validation, and it is trained in the kernel [CNN Classifier - Train & Test](https://www.kaggle.com/samhiatt/avian-vocalizations-cnn-classifier-train-test?scriptVersionId=18943170) on the entire training datataset (without cross-validation). The resulting weights are saved in the dataset [CNN Classifier weights](https://www.kaggle.com/samhiatt/avian-vocalizations-cnn-classifier-weights). Let's load them and test the model."},{"metadata":{"trusted":false},"cell_type":"code","source":"model.load_weights(\"../input/avian-vocalizations-cnn-classifier-weights/weights.best.hdf5\")\nparams = {'n_frames': 128,\n          'n_classes': n_classes,\n          'n_channels': 1}\ntest_generator = AudioFeatureGenerator(X_test, y_test, batch_size=len(X_test), \n                                       seed=37, **params)\nX_batch, y_batch = test_generator[0]\npredictions = model.predict(X_batch) \ny_predicted = [np.argmax(p) for p in predictions]\ny_true = [np.argmax(y) for y in y_batch]\ntest_score = accuracy_score(y_true, y_predicted)\nprint(\"Test accuracy score: %.5f\"%test_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final test accuracy actually exceeds the accuracy evaluated during training. This is somewhat surprising; however, considering that the model was trained on the entire training dataset, as opposed to only 1/3 of the training data the the cross-validation models saw. Having more training examples is apparently improving the model's predictive power.\n\nTo evaluate the sensitivity of the model, let's test do some more rounds of testing. Successive batches of test data will be cropped with different windows, so let's see how this stability."},{"metadata":{"trusted":false},"cell_type":"code","source":"test_scores = []\nfor i in range(3):\n    X_batch, y_batch = test_generator[0]\n    predictions = model.predict(X_batch) \n    y_predicted = [np.argmax(p) for p in predictions]\n    y_true = [np.argmax(y) for y in y_batch]\n    test_score = accuracy_score(y_true, y_predicted)\n    print(\"Epoch %i test accuracy score: %.5f\"%(i+1,test_score))\n    test_scores.append(test_score)\nprint(\"Mean test score: %.5f, standard deviation: %.5f\"%(\n        np.mean(test_scores), np.std(test_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model appears to be stable, consistently scoring around `0.24`. \n\nLet's download a new mp3 and try it out. Let's try a sample of an [Elegant Tern](https://www.xeno-canto.org/449570), contributed by [Richard E. Webster](https://www.xeno-canto.org/contributor/KZYUWIRZVH) which the model has not seen before.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from requests import get\nresp = get(\"https://www.xeno-canto.org/449570/download\")\nif resp.ok:\n    with open('test_sample.mp3','wb') as f:\n        \n        f.write(resp.content)\nelse:\n    raise(\"Error downloading sample. Do you have internet access?\")\ndata, sr = librosa.load('test_sample.mp3')\ndisplay(Audio(data, rate=sr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Transform the data\nsg = librosa.feature.melspectrogram(data, sr=sr, hop_length=512, n_fft=2048)\nsg_lognorm = sg_log_scaler.transform(np.log(sg))\nmfcc = librosa.feature.mfcc(data, sr=sr, hop_length=512, n_fft=2048)\nmfcc_norm = mfcc_scaler.transform(mfcc)\n# Grab the first 128 frames\nsg_lognorm_cropped = sg_lognorm[:, :128]\nmfcc_norm_cropped = mfcc_norm[:, :128]\nX = sg_lognorm_cropped.copy()\nX[:20] = mfcc_norm_cropped\n# Visualize it\nspecshow(X, x_axis='time', y_axis='mel')\nplt.colorbar()\n# Make a prediction\npredictions = model.predict([X.reshape(1,*X.shape,1)])\npredicted_label = label_encoder.classes_[np.argmax(predictions)]\nprint(\"The vocalization is predicted to be from a \"+predicted_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Elegant Tern had the highest accuracy in testing so it should get this one right. Unfortunately, although the model did make a prediction, it incorrectly predicted the sample to be a Phainopepla. There's still room for improvement."},{"metadata":{},"cell_type":"markdown","source":"### Justification\n\nUsing the data generator defined above, the benchmark model is trained and tested in the cells below. "},{"metadata":{"trusted":false},"cell_type":"code","source":"training_generator = AudioFeatureGenerator(X_train, y_train, batch_size=len(X_train), \n                                           shuffle=True, seed=37, n_frames=128, \n                                           n_classes=n_classes)\nscores=[]\nnb = GaussianNB()\nXs, ys = training_generator[0] #  batch_size=len(X_test), so just the first batch\nXs = Xs.reshape(Xs.shape[0],Xs.shape[1]*Xs.shape[2])\nys = np.argmax(ys,axis=1)\nnb.partial_fit(Xs, ys, classes=range(n_classes))\npredictions = nb.predict(Xs) \ntraining_accuracy = accuracy_score(ys, predictions)\nprint(\"Training accuracy of benchmark model: %.5f\"%training_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_generator = AudioFeatureGenerator(X_test, y_test, batch_size=len(X_test),\n                                       seed=37, n_frames=128, n_classes=n_classes)\nXs, ys = test_generator[0] # batch_size=len(X_test), so just the first batch\nXs = Xs.reshape(Xs.shape[0],Xs.shape[1]*Xs.shape[2])\nys = np.argmax(ys,axis=1)\npredictions = nb.predict(Xs) \ntest_accuracy = accuracy_score(ys, predictions)\nprint(\"Test accuracy of benchmark model: %.5f\"%test_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While the benchmark model achieves a training score of `0.18022`, when evaluated aginst the test dataset its accuracy only reaches `0.05275`. The test accuracy of the CNN-based model was `0.23663`, out-performing the benchmark model by a factor of `4.5 X`. \n\nLet's see a breakdown of how the predictor fares for each species by plotting a confusion matrix. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Draw a confusion matrix\nconf_matrix = confusion_matrix(y_true, y_predicted, labels=range(n_classes))\nplt.figure(figsize=(20,20))\nplt.imshow(conf_matrix)\nplt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical')\nplt.xlabel(\"true label\")\nplt.yticks(range(n_classes), label_encoder.classes_)\nplt.xlabel(\"predicted label\")\nplt.colorbar(shrink=.25);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the confusion matrix shows that the accurate predictions along the diagonal are starting to line up. \n\nRecall that an artifact of the data collection process used to create the original dataset was that species with the most samples available actually end up having shorter samples in the dataset. It is possible that the classifier is somehow picking up on this clue. Knowing that padded clips likely came from one of these classes with shorter samples is a big clue to the classifier, one it won't have when being tested in the wild. This would be a form of data leakage. \n\nTo see if there is a correlation between the total duration of audio per class and class accuracy, we can take a look at a scatter plot. "},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"# Is there a correlation between total duration of audio and class accuracy?\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\npct_correct_by_class = np.zeros(n_classes)\ncounts = np.sum(conf_matrix,axis=0)\nnp.divide(np.array([conf_matrix[i,i] for i in range(n_classes)]), counts, \n          out=pct_correct_by_class, where=counts!=0)*100\nplt.figure(figsize=(15,2))\nplt.bar(range(n_classes), pct_correct_by_class/100, .75); plt.xlim(-1,91)\nplt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical')\nplt.title(\"Species Accuracy Score\")\nplt.show();\n\nduration_by_class=[df.loc[df.english_cname==y,'duration_seconds'].sum()/60 for y in label_encoder.classes_]\nplt.figure(figsize=(15,2))\nplt.title(\"Total Duration of Audio Samples per Species\")\nplt.bar(range(n_classes), duration_by_class, .75); plt.xlim(-1,91)\nplt.ylabel(\"minutes of audio\")\nplt.xticks([],[])\n# plt.xticks(range(n_classes), label_encoder.classes_, rotation='vertical')\nplt.show();\n\nlin_model = LinearRegression()\nlin_model.fit(np.array(duration_by_class).reshape(-1,1), pct_correct_by_class.reshape(-1,1))\npred = lin_model.predict(np.array(duration_by_class).reshape(-1,1))\n# mse = mean_squared_error(pct_correct_by_class,pred)\nr2 = r2_score(pct_correct_by_class, pred)\n\nplt.title(\"Accuracy / Duration of Audio : R2=%.4f\"%(r2))\nplt.scatter(np.array(duration_by_class),pct_correct_by_class)\nplt.plot(duration_by_class, pred, 'orange')\nplt.xlabel('Total Minutes of Audio in Class')\nplt.ylabel('Accuracy');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is evident that there is a negative correlation between the total duration of audio samples and the species class accuracy. The species with shorter samples in the collection end up with greater test accuracy. The `Elegant Tern` species appears to be an outlier. It's scoring 100%. It is also a species with one of the lowest counts for total duration of audio. This is further evidence of data leakage that should be fixed."},{"metadata":{},"cell_type":"markdown","source":"\n## V. Conclusion\n\n### Free-Form Visualization\nSee the Justification section for free-form visualization.\n\n### Reflection\n\nA convolutional neural network was trained to predict bird species heard in input audio using audio samples collected from xeno-canto.org. The samples were transformed into spectrograms and MFCCs and fed into a data generator that creates batches of equal length samples clipped from random windows of input data. Several classifiers and network configurations were evaluated using 3-fold cross validation. The best performing classifier, as measured by calculating overall prediction accuracy, was selected and trained on the entire training dataset. Final classifier performance was evaluated against the test dataset. \n\nThe naive assumption of feature independence inherent in the benchmark Naive Bayes classifier prevents it from learning the distinguishing patterns present in the spectrograms. The translational invariance of the CNN allows it learn from these patterns even when they appear in different regions of the input data. This is in line with expected results, and in the end a CNN-based classifier was found to achieve a roughly 3X increase in accuracy over the benchmark Naieve Bayes model. \n\nInitial results are encouraging, but they also uncovered some issues with the dataset collection process and the methodology used by the data generator to standardize input sample lengths. \n"},{"metadata":{},"cell_type":"markdown","source":"### Improvement\n\nSeveral improvements could be made to increase the accuracy of this classifier. The model architecture could be refined, experimenting with different convolution kernel and pooling sizes or by increasing the network depth and increasing memory requirements. Rigorous hyperparameter tuning could further improve accuracy. \n\nTransfer learning could be employed. An initial attempt was made in the kernel [Avian Vocalizations: Transfer Learning](https://www.kaggle.com/samhiatt/avian-vocalizations-transfer-learning?scriptVersionId=18845751), but found little success, likely because the spectrograms don't resemble any of the classes in the pre-trained network.\n\nHowever, It is anticipated that the best gains would result from addressing the data leakage issue presented by zero-padding missing values in short samples, identified in the reflections above. Modifying the generator to simply loop short clips to fill the window would be simple technique that could address this. Additionally, balancing the training dataset during the data collection process by setting an appropriate lower limit for each audio clip duration would circumvent the need for padding.  \n\nAdditionally, improving the data generator methods with appropriate frame filtering to ensure that each cropped sample contains an identifiable vocalization would likely improve predictor performance. Any frame filtering should ensure that any rhythmic features of vocalizations are not distorted, perhaps by setting a buffer around any retained frame. Exploration into more robust audio filtering and window selection methods would likely be rewarded with additional accuray gains.\n\nThe project DeepSqueak addresses this by using a recurrent convolutional neural network (FasterR-CNN) with object region proposals that identify the locations of the vocalizations in the spectrograms. The xeno-canto.org training data does not have object regions labeled and so it can not be readily used in the same way. However, an effort could be made to create such a training dataset with expert input helping to define the regions containing vocalizations. \n\nMore experimentation could be done with the MFCCs. They could be used independently as inputs and analyzed for their predictive power, and they could be combined with the spectrograms in a lower layer of the network. The decision to stack them onto the spectrograms was made out of convenience as it was compatible with the model from the dog breed classifier project.\n"},{"metadata":{},"cell_type":"markdown","source":"## References\n\n*  Baker, Myron C. (2001). [Bird Song Research: The Past 100 Years](https://courses.washington.edu/ccab/Baker%20-%20100%20yrs%20of%20birdsong%20research%20-%20BB%202001.pdf)\n* Kevin R. Coffey, Russell G. Marx & John F. Neumaier (2019) [DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations](https://doi.org/10.1038/s41598-017-05982-x) Neuropsychopharmacology vol 44, pp 859â€“868.\n* [Xeno Canto](https://www.xeno-canto.org), founded by Bob PlanquÃ© and Willem-Pier Vellinga.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}