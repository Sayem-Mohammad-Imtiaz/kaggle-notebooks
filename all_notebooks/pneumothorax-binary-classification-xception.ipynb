{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:orange;\">\n    <h1><center>What is Pneumothorax?</center></h1>\n</div>\n\n* A pneumothorax can be defined as air in the pleural cavity. This occurs when there is a breach of the lung surface or chest wall which allows air to enter the pleural cavity and consequently cause the lung to collapse.\n\n* Pneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifyingâ€”it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event.\n\n* Pneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more confident diagnosis for non-radiologists.\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:orange;\">\n    <h1><center>Importing Libraries</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\n\nfrom keras.preprocessing.image import load_img\nfrom keras.utils import to_categorical\nfrom keras.models import Model\nfrom keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D,Lambda, Dropout, InputLayer, Input\nfrom tensorflow import keras\nfrom keras.applications import Xception\nfrom keras.applications.xception import preprocess_input\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T16:32:09.587254Z","iopub.execute_input":"2021-07-27T16:32:09.587974Z","iopub.status.idle":"2021-07-27T16:32:10.97949Z","shell.execute_reply.started":"2021-07-27T16:32:09.587856Z","shell.execute_reply":"2021-07-27T16:32:10.978159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:orange;\">\n    <h1><center>Importing The Dataset</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"train_img_path = '../input/pneumothorax-binary-classification-task/small_train_data_set/small_train_data_set'\nlabels = pd.read_csv(r'../input/pneumothorax-binary-classification-task/train_data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:32:14.925868Z","iopub.execute_input":"2021-07-27T16:32:14.926524Z","iopub.status.idle":"2021-07-27T16:32:14.970425Z","shell.execute_reply.started":"2021-07-27T16:32:14.926472Z","shell.execute_reply":"2021-07-27T16:32:14.969295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pneumothorax small dataset contains 2027 images medical images of lungs done by radiologist during chest x-ray of the patients.","metadata":{}},{"cell_type":"code","source":"labels.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:44:37.588836Z","iopub.execute_input":"2021-07-27T16:44:37.589416Z","iopub.status.idle":"2021-07-27T16:44:37.599119Z","shell.execute_reply.started":"2021-07-27T16:44:37.589381Z","shell.execute_reply":"2021-07-27T16:44:37.59836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop unnecessary columns\nlabels.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:32:21.326226Z","iopub.execute_input":"2021-07-27T16:32:21.326825Z","iopub.status.idle":"2021-07-27T16:32:21.334626Z","shell.execute_reply.started":"2021-07-27T16:32:21.326791Z","shell.execute_reply":"2021-07-27T16:32:21.333464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of pictures in the training dataset: {labels.shape[0]}\\n')\nprint(f'Number of different labels: {len(labels.target.unique())}\\n')\nprint(f'Labels: {labels.target.unique()}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-27T16:32:24.165446Z","iopub.execute_input":"2021-07-27T16:32:24.165987Z","iopub.status.idle":"2021-07-27T16:32:24.175865Z","shell.execute_reply.started":"2021-07-27T16:32:24.165939Z","shell.execute_reply":"2021-07-27T16:32:24.174599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:orange;\">\n    <h1><center>Data Visualization</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,40))\ni=1\nfor idx,s in labels.head(6).iterrows():\n    img_path = os.path.join(train_img_path,s['file_name'])\n    img=cv2.imread(img_path)\n    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    fig=plt.subplot(6,2,i)\n    fig.imshow(img)\n    fig.set_title(s['target'])\n    i+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:32:28.896749Z","iopub.execute_input":"2021-07-27T16:32:28.897302Z","iopub.status.idle":"2021-07-27T16:32:31.372663Z","shell.execute_reply.started":"2021-07-27T16:32:28.897267Z","shell.execute_reply":"2021-07-27T16:32:31.371828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting different classes\nclasses = sorted(labels['target'].unique())\nn_classes = len(classes)\nprint(f'number of class: {n_classes}')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:32:41.126918Z","iopub.execute_input":"2021-07-27T16:32:41.127311Z","iopub.status.idle":"2021-07-27T16:32:41.13405Z","shell.execute_reply.started":"2021-07-27T16:32:41.127278Z","shell.execute_reply":"2021-07-27T16:32:41.132531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes_to_num = dict(zip(classes,range(n_classes)))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:32:44.617581Z","iopub.execute_input":"2021-07-27T16:32:44.617948Z","iopub.status.idle":"2021-07-27T16:32:44.62308Z","shell.execute_reply.started":"2021-07-27T16:32:44.617914Z","shell.execute_reply":"2021-07-27T16:32:44.62175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:orange;\">\n    <h1><center>Converting Images to Array</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"#Function to load and convert images to array\n\ndef images_to_array(data_dir,df,image_size):\n    image_names = df['file_name']\n    image_labels = df['target']\n    data_size = len(image_names)\n    \n    X = np.zeros([data_size,image_size[0],image_size[1],image_size[2]],dtype = np.uint8)\n    y = np.zeros([data_size,1],dtype = np.uint8)\n    \n    for i in range(data_size):\n        img_name = image_names[i]\n        img_dir = os.path.join(data_dir,img_name)\n        img_pixels = load_img(img_dir,target_size=image_size)\n        X[i] = img_pixels\n        y[i] = classes_to_num[image_labels[i]]\n        \n    y = to_categorical(y)\n    ind = np.random.permutation(data_size)\n    X = X[ind]\n    y = y[ind]\n    print('Ouptut Data Size: ', X.shape)\n    print('Ouptut Label Size: ', y.shape)\n    return X, y  \n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:32:48.216677Z","iopub.execute_input":"2021-07-27T16:32:48.217128Z","iopub.status.idle":"2021-07-27T16:32:54.713023Z","shell.execute_reply.started":"2021-07-27T16:32:48.217094Z","shell.execute_reply":"2021-07-27T16:32:54.712036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Selecting image size according to pretrained models\nimg_size = (299,299,3)\nX, y = images_to_array(train_img_path,labels,img_size)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:32:57.705274Z","iopub.execute_input":"2021-07-27T16:32:57.705632Z","iopub.status.idle":"2021-07-27T16:34:01.630538Z","shell.execute_reply.started":"2021-07-27T16:32:57.705602Z","shell.execute_reply":"2021-07-27T16:34:01.629398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:orange;\">\n    <h1><center>Extracting features using Xception</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"\ndef get_features(model_name, data_preprocessor,weight, input_size, data):\n    #Prepare pipeline.\n    input_layer = Input(input_size)\n    preprocessor = Lambda(data_preprocessor)(input_layer)\n    \n    base_model = model_name(weights=weight,\n                            include_top=False,\n                            input_shape=input_size)(preprocessor)\n    \n    avg = GlobalAveragePooling2D()(base_model)\n    feature_extractor = Model(inputs = input_layer, outputs = avg)\n    \n    #Extract feature.\n    feature_maps = feature_extractor.predict(data, batch_size=128, verbose=1)\n    print('Feature maps shape: ', feature_maps.shape)\n    return feature_maps","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:34:56.687046Z","iopub.execute_input":"2021-07-27T16:34:56.687436Z","iopub.status.idle":"2021-07-27T16:34:56.69481Z","shell.execute_reply.started":"2021-07-27T16:34:56.687403Z","shell.execute_reply":"2021-07-27T16:34:56.693997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting features using Xception\nXception_preprocessor = preprocess_input\nXception_features = get_features(Xception,\n                                  Xception_preprocessor,\n                                 '../input/keras-pretrained-models/Xception_NoTop_ImageNet.h5',\n                                  img_size, X)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:34:58.725528Z","iopub.execute_input":"2021-07-27T16:34:58.726028Z","iopub.status.idle":"2021-07-27T16:43:12.50238Z","shell.execute_reply.started":"2021-07-27T16:34:58.725996Z","shell.execute_reply":"2021-07-27T16:43:12.50123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:orange;\">\n    <h1><center>Model Building</center></h1>\n</div>","metadata":{}},{"cell_type":"code","source":"#Callbacks\nEarlyStop_callback = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nmy_callback=[EarlyStop_callback]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:43:20.08646Z","iopub.execute_input":"2021-07-27T16:43:20.086829Z","iopub.status.idle":"2021-07-27T16:43:20.092395Z","shell.execute_reply.started":"2021-07-27T16:43:20.086799Z","shell.execute_reply":"2021-07-27T16:43:20.091104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding the final layers to the above base models where the actual classification is done in the dense layers\n#Building Model\nmodel = Sequential()\nmodel.add(InputLayer(Xception_features.shape[1:]))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2,activation='sigmoid'))\n\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['AUC'])\nmodel.summary()\n\n# Training the CNN on the Train features and evaluating it on the val data\nhistory = model.fit(Xception_features,y,validation_split=0.20,callbacks=my_callback, epochs = 50, batch_size=128)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:43:22.805713Z","iopub.execute_input":"2021-07-27T16:43:22.806072Z","iopub.status.idle":"2021-07-27T16:43:30.798912Z","shell.execute_reply.started":"2021-07-27T16:43:22.806042Z","shell.execute_reply":"2021-07-27T16:43:30.797928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:43:37.179196Z","iopub.execute_input":"2021-07-27T16:43:37.179562Z","iopub.status.idle":"2021-07-27T16:43:37.353112Z","shell.execute_reply.started":"2021-07-27T16:43:37.179533Z","shell.execute_reply":"2021-07-27T16:43:37.352217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for AUC\nplt.plot(history.history['auc'])\nplt.plot(history.history['val_auc'])\nplt.title('model AUC')\nplt.ylabel('AUC')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T16:43:42.705038Z","iopub.execute_input":"2021-07-27T16:43:42.705436Z","iopub.status.idle":"2021-07-27T16:43:42.883035Z","shell.execute_reply.started":"2021-07-27T16:43:42.705403Z","shell.execute_reply":"2021-07-27T16:43:42.881748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\">\n<h4>If you like this notebook, please upvote it! \n     Thank you! :)</h4>\n</div>","metadata":{}}]}