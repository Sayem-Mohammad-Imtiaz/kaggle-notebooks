{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Import Libraries **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom PIL import Image\nimport chakin\nimport re\nimport os\nimport string\nimport nltk\nfrom nltk.tokenize import WordPunctTokenizer\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom sklearn.model_selection  import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional,Dropout\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read data and perform data cleaning**\n\nRead data"},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets = pd.read_csv('../input/Tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare data cleaning code:**\n1.  Remove links and domain name tags (those that are preceded with @)\n2.  Remove non-letter characters \n3.  Convert review to lower case\n4.  Tokenize tweets to words\n5.  Remove punctuation\n6.  Remove stop words\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"username = '@[A-Za-z0-9]+'\nurl = 'https?://[^ ]+'\nlink = 'www.[^ ]+'\ncombined_p = '|'.join((username, url, link))\n\n##  Cleaning Function \ndef tweet_cleaner( tweet ):\n    #\n    # 1. Remove non-informative text    \n    tweet = re.sub(combined_p, '', tweet)\n    #\n    # 2. Remove non-letters\n    tweet = re.sub(\"[^a-zA-Z]\",\" \", tweet)\n    #\n    # 3. Convert words to lower case\n    lower_tweet = tweet.lower()\n    #\n    # 4. Tokenize tweet \n    tok = WordPunctTokenizer()\n    tweet_words = [x for x in tok.tokenize(lower_tweet) if len(x) > 1]\n    #\n    # 5. remove punctuation from each word\n    table = str.maketrans('','', string.punctuation)\n    stripped = [w.translate(table) for w in tweet_words]\n    #\n    # 6. remove stop words\n    stops = set(stopwords.words(\"english\"))\n    words = [w for w in stripped if not w in stops]\n    #\n    # 7. combine words and return cleaned tweet\n    return (\" \".join(words)).strip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perform data cleaning on all tweets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_tweets = []\nfor tweet in airline_tweets.text:\n    normalized_tweets.append(tweet_cleaner(tweet))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some Exploratory Data Analysis**\n1.\tThe general sentiment of reviewers to airline services. Plot a pie-chart for the percentages of reviews sentiments (Figure 1).  The figure shows that customers would most probably write a review if they have a compliant. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nModify plot size\n\"\"\"\nplot_size = plt.rcParams[\"figure.figsize\"]\nplot_size[0] =8\nplot_size[1] = 8\nplt.rcParams[\"figure.figsize\"] = plot_size\nsentiment = airline_tweets.airline_sentiment.value_counts().to_frame()\nsentiment.columns = ['count']\nprint(sentiment)\n# print(airline_tweets.airline_sentiment.value_counts().to_frame())\nairline_tweets.airline_sentiment.value_counts().plot(kind='pie',autopct='%1.0f%%')\nplt.title('Fig 1. Distribution of tweets sentiments', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.\tWe should be more interested in negative reviews because they give the feedback about the main reasons of bad flights. We first explore the airlines that received the most negative feedback (figure 2). From the table and the figure, we can observe that *United* has the most negative reviews, followed by *US Airways* and *American*."},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_sentiment = airline_tweets[airline_tweets.airline_sentiment == 'negative'].groupby(['airline']).airline_sentiment.count()\np = pd.DataFrame(airline_sentiment.sort_values(ascending=False))\np.columns = ['Negative Count']\nprint (p)\nairline_sentiment = airline_sentiment.plot(kind='bar', color=sns.color_palette('hls'))\nairline_sentiment.set_xlabel('Airline')\nairline_sentiment.set_ylabel('Negative tweets count')\nplt.title('Fig 2. Negative Tweets per Airline', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, the negative reviews count might be misleading since bigger companies would receive more reviews and subsequently more negative reviews. A more indicative insight would be to explore the ratio of negative/positive/neutral reviews per airline (figure 3). We can see here that *US Airways* and *American* have higher ratio of negative reviews than *United*  "},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_sentiment = (airline_tweets.groupby(['airline', 'airline_sentiment']).size()/ airline_tweets.groupby('airline').size()*100).unstack()\nairline_sentiment=airline_sentiment.plot(kind='bar')\nairline_sentiment.set_xlabel('Airline')\nairline_sentiment.set_ylabel('Percentage (100%)')\nplt.title('Fig 3. Distribution of sentiment per Airline', fontsize=18)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.\tAny customer service would want to gain insight about the most issues that concerns airlines' customers in order to improve their services to tailor to customers concerns. Customer issues are presented as a pie-chart that shows the distribution of complains with respect to each (Figure 4). Clearly, 50% of complains are due to customer service problems and late flights."},{"metadata":{"trusted":true},"cell_type":"code","source":"airline_tweets.negativereason.value_counts().plot(kind='pie', autopct='%1.0f%%')\nplt.title('Fig 4. Negative Reasons Percentages', fontsize=18)\nplt.ylabel('Negative Reasons')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.\tWhile Figure 4 shows the general concerns of customer across all airlines, individual airlines would be interested to know the top complains of their own customers and how they are satisfying customers concerns compared to competitor airlines. The next graph (Figure 5) shows the topics of customer complains per airline. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualize negaive reasons per airline\npd.crosstab(airline_tweets.airline, airline_tweets.negativereason).apply(lambda x: x / x.sum() * 100, axis=1).plot(kind='bar',stacked=True)\nplt.title('Fig 5. The Reasons Customers React Negatively to Each Airline in Frequency', fontsize=18)\nplt.xlabel('Airline')\nplt.ylabel('Percentage of Reason')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The previous figure is a bit cluttered and non-clear. This is a closer look of the negative reviews per airline for the top 4 complaint reasons. Once again, US Airways and American recieve load of complaints about customer services in particular.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"top_negative_reasons= airline_tweets.negativereason[airline_tweets.negativereason.isin([\"Customer Service Issue\", \"Late Flight\", \"Can't Tell\", \"Cancelled Flight\" ])]\nairline_tweets['top_negative_reasons']=top_negative_reasons\nairline_sentiment = (airline_tweets.groupby(['airline', 'top_negative_reasons']).size()/ airline_tweets.groupby('airline').size()*100).unstack()\nairline_sentiment=airline_sentiment.plot(kind='bar')\nairline_sentiment.set_xlabel('Airline')\nairline_sentiment.set_ylabel('Percentage (100%)')\nplt.title('Fig 3. Distribution of sentiment per Airline', fontsize=18)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chakin.search(lang='English')\nchakin.download(number=21, save_dir='./')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def divide_data(X,Y):\n    \"\"\"\n    This code divide data into training, validation, and testing datasets\n    \"\"\"\n     # Divide all tweets to 80%-20% training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X,\n        Y,\n        test_size=0.2,\n        shuffle=True,\n        stratify = Y,\n        random_state=42)\n\n    # Divide the training data to 80%-20% training and validation sets\n    X_train, X_valid, y_train, y_valid = train_test_split(\n        X_train,\n        y_train,\n        test_size=0.2,\n        shuffle=True,\n        stratify = y_train,\n        random_state=42)\n    return  X_train, X_valid, X_test, y_train, y_valid, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build the Deep Learning neural network model for classification:\n- Layer 1: Embedding layer. Rather than training an embedding layer to generate the vectorized representation of input text, we feed the embedding matrix that was built from the the pre-trained wor2vec word embeddings.\n- Layer 2: Bidirectional LSTM layer: LSTM is a sequence predication RNN model. When trained over a dataset of sentences, LSTM predicts the next word given a sequence of the N previous words. In our case, the last word it will be trained to predicate is the tweet sentiment label. LSTM model parameters (embedding_size:64, dropout=0.2, recurrent_dropout=0.2).\n- Layer 3: Dense layer of an output of 10 dimensions. Creates a fully connect network between the previous layer and next layer.\n- Layer 4: Dropout Layer: drop rate = 0.3. Drop out layer objective is to reduce model overfitting by knocking out a percentage of the input data. In that case, it penalizes the model if it relied (put most weights) on specific input instances. Instead, it forces the model to spread out the weights over all inputs.\n- Layer 5: Output layer. The output layer uses a softmax activation function that generate the probability of each of the possible outputs (negative, positive, neutral), the probabilities sum to 1. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_LSTM_model(max_fatures, out_size, input_len, embedding_matrix):\n    model = Sequential()\n    model.add(Embedding(input_dim=max_fatures, output_dim=out_size, \n                        input_length=input_len, weights = [embedding_matrix], trainable=False))\n    model.add(Dense(10))\n    model.add(Dropout(0.3))\n    model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n    model.add(Dense(3, activation='softmax'))\n    model.summary()\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load pre-trained google's word2vec embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef evaluate(y_test, y_predict):\n    roc = roc_auc_score(y_test, y_predict, average = 'weighted')\n    accu = accuracy_score(y_test, y_predict)\n    print (\"Area Under Curve: \", roc)\n    print (\"Accurcy on test set: \", accu)\n  \n    \ndef deep_learning_tarining():\n    \"\"\"\n    Data preparation: Prepare the tweets and embedding matrix that we will send to the neural network.\n    \"\"\"\n    max_fatures = 2000                                                     \n    tokenizer = Tokenizer(num_words=max_fatures, split=' ')          # use the most 2000 frequent words\n    tokenizer.fit_on_texts(normalized_tweets)                        # create a vocabulary index\n    all_tweets = tokenizer.texts_to_sequences(normalized_tweets)     # transform each review to a sequence of integers. Use only the most 200 frequent words\n    all_tweets = pad_sequences(all_tweets, padding='post')           # pad short tweets with 0s\n    \n    # Preparing word embedding matrix for words in tweets\n    vocab_size = len(tokenizer.word_index)+1\n    embedding_matrix = np.zeros((vocab_size, 300))\n    for  word, i in tokenizer.word_index.items():\n        if word in word2vec.wv:\n            embedding_vector = word2vec.wv[word]\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n            \n    \"\"\"\n    - Divide the data into training, testing and validation sets\n    - Convert catagorical output to one-hot vectors\n    - Cast the classification problem as a multi-class classification task\n    - Train the model for 20 epochs, and batch size of 200 instances\n    \n    \"\"\"\n    X = all_tweets\n    Y = pd.get_dummies(airline_tweets['airline_sentiment']).values    # converts the catagorical outputs to one-hot vectors for each instance\n    \n    X_train, X_valid, X_test, y_train, y_valid, y_test = divide_data(X, Y)  # Split the data to three datasets\n    \n    model = create_LSTM_model(vocab_size, 300, X.shape[1], embedding_matrix)\n    \n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                                patience=3, \n                                                verbose=1, \n                                                factor=0.5, \n                                                min_lr=0.00001)\n    \n    history = model.fit(X_train, y_train, epochs = 20, batch_size = 200, validation_data=(X_valid, y_valid),callbacks = [learning_rate_reduction])\n\n \n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accurcy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    y_test_pred = model.predict(X_test)\n    predictions = (y_test_pred == y_test_pred.max(axis=1, keepdims=1)).astype(int)\n    evaluate(y_test, predictions)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_learning_tarining()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build a Random Forest classification model: Random forest is a powerful ensemble machine learning method that can be used for classification. The main principle behind ensemble methods is that a group of “weak learners” can come together to form a “strong learner”. In the case of random forests weak learners are decision trees. A decision tree is a flowchart-like structure in which each internal node represents a “test” on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. The paths from root to leaf represent classification rules. These trees are normally prone to overfitting. Random Forests build decsion trees such that they use random sample of the instances from the dataset and a random extraction of the features. Not every tree sees all the features or all the instances, and this guarantees that the trees are de-correlated and therefore less prone to overfitting. In addition, random forests are easy interpretable and fast to train.\n\n- Instead of using all words as features, we extract bi-grams as features to encode some contextual (word order) information.\n- We use 200 basic learners"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decision_tree_training():\n    splitted_tweets = []\n    for tweet in normalized_tweets:\n        bigramFeatureVector = []\n        tweet = tweet.split()\n        if len(tweet) == 1:\n            splitted_tweets.append(str(tweet))\n        else:\n            for item in nltk.bigrams(tweet):\n                bigramFeatureVector.append(' '.join(item))    \n            splitted_tweets.append(str(bigramFeatureVector))\n    \n  \n    X = splitted_tweets\n    Y = pd.get_dummies(airline_tweets['airline_sentiment']).values              # converts the catagorical outputs to one-hot vectors for each instance\n\n    X_train, X_valid, X_test, y_train, y_valid, y_test = divide_data(X, Y)      # Split the data to three datasets\n    \n    vectorizer = CountVectorizer(max_features=2000, min_df=5, max_df=0.8)       # Create CountVectorizer object that would tokenize a collection of text documents and build a vocabulary of known words\n                                                                                # It would Keep only the most 2000 frequent terms that have occured at least 5 times, and at most in 80% of documents. \n            \n                                                                                \n                                                                                \n    X_train = vectorizer.fit_transform(X_train).toarray()                       # Convert the tweets to a matrix of terms(features) counts, each row corrosponds to a tweet\n                                                                                # and each column corrosponds to a word in the vocabulary. The fit part learns the vocabulary based on the CountVectorizer parameters\n                                                                                # and the transform part encodes each tweet into a vector.\n    X_test = vectorizer.transform(X_test).toarray() \n        \n    text_classifier = RandomForestClassifier(n_estimators=200, max_features= 200, random_state=0)  \n    text_classifier.fit(X_train, y_train) \n    \n       \n    predictions = text_classifier.predict(X_test)  \n    evaluate(y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree_training()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}