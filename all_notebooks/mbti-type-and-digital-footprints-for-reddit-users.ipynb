{"cells":[{"metadata":{},"cell_type":"markdown","source":"                                               MBTI Types\n\n![](https://www.verywellmind.com/thmb/h3i7cVWZ0vZPHQ--hn-WDXmRU6g=/1500x782/filters:fill(ABEAC3,1)/the-myers-briggs-type-indicator-2795583_FINAL-5c4b6112c9e77c00014af95f.png)"},{"metadata":{},"cell_type":"markdown","source":"# Objective\n\nOur objectf is to make a predictive model that classifies reddit users as either extraverts or introverts based on the data recording their interaction with various subreddits."},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries Needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n!pip install seaborn==0.11.0\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ndf = pd.read_csv(\"../input/mbti-type-and-digital-footprints-for-reddit-users/reddit_psychometric_data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data contain 3586 rows and 27091 columns (sooo large isn't it)"},{"metadata":{},"cell_type":"markdown","source":"**Checking the Missing Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of missing data in the Total data is :\", df.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Types of data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Personalities Types\n\n![The Personalities Types](https://upload.wikimedia.org/wikipedia/commons/1/1f/MyersBriggsTypes.png)"},{"metadata":{},"cell_type":"markdown","source":"Our data personalities are distributed as followed : "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mbti_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.set(font_scale=1.4)\ndf['mbti_type'].value_counts().plot(kind='bar', figsize=(12, 6), rot=0)\nplt.xlabel(\"MBTI_Personality_Type\", labelpad=10)\nplt.ylabel(\"Count of People\", labelpad=10)\nplt.title(\"Count of People Who Received Tips by their Personality\", y=1.02);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see clearly that the most existing personality type is INPT :\n\n![](https://www.verywellmind.com/thmb/f53mBgKUJGSHvpymqOtfMPVxlAY=/700x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/intp-introverted-intuitive-thinking-perceiving-2795989-5c2e4533c9e77c0001cb80e9.png)\n\n\nINTP (introverted, intuitive, thinking, perceiving) is one of the 16 personality types described by the Myers-Briggs Type Indicator (MBTI).1\n\n﻿ People who score as INTP are often described as quiet and analytical. They enjoy spending time alone, thinking about how things work and coming up with solutions to problems. INTPs have a rich inner world and would rather focus their attention on their internal thoughts rather than the external world. They typically do not have a wide social circle, but they do tend to be close to a select group of people. \n  \n* **Popular INTP Careers**\nChemist\nPhysicist\nComputer programmer\nForensic scientist\nEngineer\nMathematician\nPharmacist\nSoftware developer\nGeologist"},{"metadata":{},"cell_type":"markdown","source":"# Understand Our Data"},{"metadata":{},"cell_type":"markdown","source":"\nIntroversion : INTP\n\n               INFP\n               INTJ\n               INFJ\n               ISTP    \n               ISFP\n               ISTJ\n               ISFJ\n    \n  \nExtraversion : ENTP \n\n               ENFP    \n               ENTJ         \n               ENFJ         \n               ESTP          \n               ESFP     \n               ESTJ     \n               ESFJ \n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"lIntro = ['INTP','INFP','INTJ','INFJ','ISTP','ISFP','ISTJ','ISFJ']\nfor i in lIntro:\n    df.mbti_type = df.mbti_type.replace(i, \"Introversion\")\nlExtra = ['ENTP','ENFP','ENTJ','ENFJ','ESTP','ESFP','ESTJ','ESFJ']\nfor i in lExtra:\n    df.mbti_type = df.mbti_type.replace(i, \"Extraversion\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will consider that :\n\n    Extraversion  -->  1\n        &\n    Introversion  -->  0"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.mbti_type = df.mbti_type.replace({'Extraversion':1,'Introversion':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the data"},{"metadata":{},"cell_type":"markdown","source":"The first preprocessing step is to divide the dataset into a features set and corresponding personality type. The following script performs this task:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,1:].values\ny = df.iloc[:,0].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standard Scaler :**\n\nStandardScaler transform your data such that its distribution will have a mean value 0 and standard deviation of 1.\n\n* Standard Scaler is useful for classification.\n* it's transform the data between [-1,1]"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Principal Component Analysis**\n\nThis method combines highly correlated variables together to form a smaller number of an artificial set of variables which is called \"principal components\" that account for most variance in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA()\nX = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA’s goal is to reduce the curse of dimensionality. It will reduce the features in such a way that it retains most principal information of the features in its principal components."},{"metadata":{},"cell_type":"markdown","source":"**Spliting the data into train & test sets**"},{"metadata":{},"cell_type":"markdown","source":"The next preprocessing step is to divide data into training and test sets. Execute the following script to do so:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Model Algorithms"},{"metadata":{},"cell_type":"markdown","source":"# 1. Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression is used when the dependent variable(target) is categorical.\n\nLike in our case : Extraversion:1, Introversion:0."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlrg = LogisticRegression()\nlrg.fit(X_train, y_train)\ny_pred = lrg.predict(X_test)\nprint('Logistic Regression Accuracy' , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Linear Discriminant Analysis"},{"metadata":{},"cell_type":"markdown","source":"Linear discriminant analysis is used as a tool for classification, dimension reduction, and data visualization. It has been around for quite some time now. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train, y_train)\ny_pred = lda.predict(X_test)\nprint('LinearDiscriminantAnalysis Accuracy' , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Gaussian Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes is a classification algorithm for binary and multi-class classification problems. The technique is easiest to understand when described using binary or categorical input values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngb = GaussianNB()\ngb.fit(X_train, y_train)\ny_pred = gb.predict(X_test)\nprint('Gaussian Naîve Bayes Accuracy' , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Random Forest Classifier"},{"metadata":{},"cell_type":"markdown","source":"The Random Forest Classifier is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(max_depth=2, random_state=0)\nrfc.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = rfc.predict(X_test)\nprint('Random Forest Classifier Accuracy' , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Support Vector Classifier"},{"metadata":{},"cell_type":"markdown","source":"Support vector machines Classifier (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n\nI will use it because of its advantages:\n\n* Effective in high dimensional spaces.\n\n* Still effective in cases where number of dimensions is greater than the number of samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nprint('Accuracy Suport Vector Classifier' , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"Decision Trees Classifiers are a type of Supervised Machine Learning meaning we build a model, we feed training data matched with correct outputs and then we let the model learn from these patterns. Then we give our model new data that it hasn't seen before so that we can see how it performs. And because we need to see what exactly is to be trained for a Decision Tree, let's see what exactly a decision tree is.\n\n[To see More](https://programmerbackpack.com/decision-tree-explained/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\ny_pred = dtc.predict(X_test)\nprint('Accuracy Decision Tree Classifier' , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. KNeighbor Classifier"},{"metadata":{},"cell_type":"markdown","source":"K-Nearest Neighbor is a supervised learning algorithm that can be used for regression as well as classification problems. But KNN is widely used for classification problems in machine learning. KNN works on a principle assuming that every data point falling near to each other will fall in the same class. That means similar things are near to each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier()\nknc.fit(X_train, y_train)\ny_pred = knc.predict(X_test)\nprint('Accuracy KNeighbors Classifier' , accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that the accuracy of our classification algorithms does not exceed the accuracy of 70%. And if we try to create a deeper model then what will be our precision !"},{"metadata":{},"cell_type":"markdown","source":"# Neural Network With Keras"},{"metadata":{},"cell_type":"markdown","source":"Before we begin on building our model, we need to know the input dimension of our feature vectors. This happens only in the first layer since the following layers can do automatic shape inference. In order to build the Sequential model, you can add layers one by one in order as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\n\ninput_dim = X_train.shape[1]\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so with .compile() will help me to specify my optimizer which will be adam ,and the loss function. In order to configure the learning process.\n\nKeras also includes a handy .summary() function to give an overview of the model and the number of parameters available for training just as below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', \n               optimizer='adam', \n               metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is time for training with the .fit() function.\n\nSince the training in neural networks is an iterative process, the training won’t just stop after it is done. You have to specify the number of iterations you want the model to be training. Those completed iterations are commonly called epochs. We want to run it for 100 epochs to be able to see how the training loss and accuracy are changing after each epoch.\n\nAnother parameter you have to your selection is the batch size. The batch size is responsible for how many samples we want to use in one epoch, which means how many samples are used in one forward/backward pass. This increases the speed of the computation as it need fewer epochs to run, but it also needs more memory, and the model may degrade with larger batch sizes. Since we have a small training set, we can leave this to a low batch size:"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train,\n                     epochs= 30,\n                     verbose=False,\n                     validation_data=(X_test, y_test),\n                     batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time for evaluation .evaluate() method measure the accuracy of the model. You can do this both for the training data and testing data. We expect that the training data has a higher accuracy then for the testing data. The longer you would train a neural network, the more likely it is that it starts overfitting.\n\nNote that if you rerun the .fit() method, you’ll start off with the computed weights from the previous training. Make sure to compile the model again before you start training the model again. Now let’s evaluate the accuracy model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Access Model Training History in Keras\n\nKeras provides the capability to register callbacks when training a deep learning model.\n\nOne of the default callbacks that is registered when training all deep learning models is the History callback. It records training metrics for each epoch. This includes the loss and the accuracy (for classification problems) as well as the loss and accuracy for the validation dataset.\n\nThe history object is returned from calls to the fit() function used to train the model. Metrics are stored in a dictionary in the history member of the object returned.\n\nFor example, you can list the metrics collected in a history object using the following snippet of code after a model is trained:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a model trained on a classification problem with a validation dataset, this might produce the listing above :"},{"metadata":{},"cell_type":"markdown","source":"You can use this little helper function to visualize the loss and the accuracy for the training and testing data both based on the History callback. This callback, which is automatically applied to each Keras model, records the loss and additional metrics that can be added in the .fit() method. In this case, we are only interested in the accuracy. We will try to complete the task by using the matplotlib plotting library:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(14, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's analys the **accuracy model** : As you can see in the diagram, the accuracy increases unbelievably in the first epoch, indicating that the network is learning fast. Afterwards, the curve flattens indicating that not too many epochs are required to train the model further. Generally, if the training data accuracy (“accuracy”) keeps improving and the validation data accuracy (“val_acc”) gets decreasing which is a good thing.\n\n"},{"metadata":{},"cell_type":"markdown","source":"A good way to see when the model starts overfitting is when the loss of the validation data starts rising again. This tends to be a good point to stop the model."},{"metadata":{},"cell_type":"markdown","source":"# Prepare Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)\nprint(y_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'mbti': y_test[:], 'predicted_value': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\nmy_submission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.predicted_value.value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}