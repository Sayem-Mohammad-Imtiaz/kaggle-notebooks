{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\n![](https://i.pinimg.com/originals/40/b1/3d/40b13d00e57b21a195217db15e03403e.png)\n\nThis kernel will introduce you to the basics of text processing with spaCy. If you are a beginner like me then for sure you will find this guide helpful. \n\nThe idea will be to learn basic preprocessing using spaCy which is an Industrial-Strength Natural Language Processing library and then level up based on the problems we are trying to solve. As this is my first kernel I hope I don't mess up a lot.\n\nYou can also take this course in detail by spaCy : [Advanced NLP with spaCy](https://course.spacy.io/)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\n\nfrom spacy.lang.en import English\nnlp = English()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use News Headlines dataset for Sarcasm Detection Dataset.\nYou can find the Dataset here : [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\nprint(\"Loading data...\")\ntrain = pd.read_json(\"../input/news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json\", lines=True)\ntrain = train.drop(['article_link'], axis=1)\n\nprint(\"Train shape:\", train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Before we can apply machine learning algorithms we have to preprocess our text data. Here's how to clean your text data.\n\n- Remove all irrelevant characters such as any non alphanumeric characters\n- Tokenize your text by separating it into individual words\n- Remove words that are not relevant, such as “@” twitter mentions or urls(if any)\n- Convert all characters to lowercase(**Case folding**), in order to treat words such as “hello”, “Hello”, and “HELLO” the same. ******\n- Consider combining misspelled or alternately spelled words to a single representation (e.g. “cool”/”kewl”/”cooool”)\n- Consider lemmatization (reduce words such as “am”, “are”, and “is” to a common form such as “be”)\n- Consider removing stopwords (such as a, an, the, be)etc.\n\n**Note** : For tasks like speech recognition and information retrieval, everything is mapped to lower case. For sentiment analysis and other text classification tasks, information extraction, and machine translation, by contrast, case is quite helpful and case folding is generally not done (losing the difference, for example, between **US the country and us the pronoun** can outweigh the advantage in generality that case folding provides)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the first review\n\nprint('The first review is:\\n\\n',train[\"headline\"][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create cleanData function. \nThis function will remove stopwords, punctuations, convert text to lowercase and preform lemmatization.\n\n**Stopwords**: Stop words are words that are particularly common in a text corpus and thus considered as rather un-informative.\n\n**Lemmatization**: Lemmatization refers to normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to clean data\n\ndef cleanData(doc,stemming = False):\n    doc = doc.lower()\n    doc = nlp(doc)\n    tokens = [tokens.lower_ for tokens in doc]\n    tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n    tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n    final_token = [token.lemma_ for token in tokens]\n    \n    return \" \".join(final_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_review = cleanData(train['headline'][0])\nclean_review","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean description\nprint(\"Cleaning train data...\\n\")\ntrain[\"headline\"] = train[\"headline\"].map(lambda x: cleanData(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basics of spaCy\n\n**spaCy** is a free, open-source library for advanced Natural Language Processing (NLP) in Python. \n\nspaCy is designed specifically for **production use** and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to **pre-process text for deep learning**.\n\nLet's get started"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_text = \"\"\"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. “I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I wasn’t worth talking to,” said Thrun, now the co-founder and CEO of online higher education startup Udacity, in an interview with Recode earlier this week.\n\nA little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.\"\"\"\ndoc = nlp(sample_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, what is really happening here is that  we pass a string of text to the nlp object, and receive a Doc object.\n\n![](https://course.spacy.io/pipeline.png)\n\nDuring processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. After tokenization, spaCy can parse and tag a given Doc.\nThere are several preprocessing tasks which we will go through one by one.\nThe best thing about spaCy pipeline is that you can always **add custom functions** in this pipeline depending on your problem.\n\nSome of the basic functions are:\n\n* Tokenization\n* Part-of-speech (POS) Tagging\n* Named Entity Recognition (NER) etc."},{"metadata":{},"cell_type":"markdown","source":"# Tokenization\n\n> Tokenization is the process of converting a sequence of characters into a sequence of tokens.\n\n![](https://raw.githubusercontent.com/theainerd/MLInterview/master/images/Screenshot%20from%202018-10-04%2014-14-08.png)\n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# print each token\nfor token in doc:\n    print(token.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Parts-of-Speech (POS) tagging\n\n> *Part-of-speech tagging (POS tagging)* is the task of tagging a word in a text with its part of speech. A part of speech is a category of words with similar grammatical properties. Common English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc.\n\nthe *\"en_core_web_sm\"* package is a small English model that supports all core capabilities and is trained on web text. The package provides the **binary weights** that enable spaCy to make predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(sample_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for token in doc:\n    # Print the text and the predicted part-of-speech tag\n    print(token.text, token.pos_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Named Entity Recognition (NER)\n\n> In the Named Entity Recognition (NER) task, systems are required to recognize the Named Entities occurring in the text. More specifically, the task is to find Person (PER), Organization (ORG), Location (LOC) and Geo-Political Entities (GPE). For instance, in the statement ”Shyam lives in India”, NER system extracts Shyam which refers to name of the person and India which refers to name of the country.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate over the predicted entities\nfor ent in doc.ents:\n    # Print the entity text and its label\n    print(ent.text, ent.label_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get quick definitions of the most common tags and labels\n\nprint(spacy.explain('GPE'))\nprint(spacy.explain('ORG'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.displacy.render(doc, style='ent', jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding custom functions to pipelines"},{"metadata":{},"cell_type":"markdown","source":"Can be added using the `nlp.add_pipe` method. You can also mention where you want to add the component using argumenets.\n\nLet's add a component of word counts to the pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a custom component\ndef custom_component(doc):\n    # Print the doc's length\n    print('Doc length:', len(doc))\n    # Return the doc object\n    return doc\n\n# Add the component first in the pipeline\nnlp.add_pipe(custom_component, first=True)\n\n# Print the pipeline component names\nprint('Pipeline:', nlp.pipe_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now everytime you create a new nlp object it will always print the document length."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process a text\ndoc = nlp(sample_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Document similarity\n\nspaCy can compare two objects and predict similarity.  In order to use similarity, you need a larger spaCy model that has word vectors included.\n\nFor example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\". "},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc1 = nlp(\"My name is shyam\")\ndoc2 = nlp(\"My name is Ram\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The documents similarity is:\" ,doc1.similarity(doc2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This was a very basic guide to get started on NLP for beginners, we saw data cleaning for text data and how to use spaCy for different Natural Language Processing Tasks.\n\nReferences:\n\n[Advanced NLP with spaCy](https://course.spacy.io/)\n\nKernels you can explore:\n\n[Hitchhiker's Guide to NLP in spaCy](https://www.kaggle.com/nirant/hitchhiker-s-guide-to-nlp-in-spacy/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}