{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # visualization tool\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#get the data from csv file to dataframe\n\ndata = pd.read_csv('/kaggle/input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking for data columns to have an idea about data type and data content\n\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to arrange the data columns name \ndata = data.rename(columns={\"Volume_(BTC)\": \"Volume_BTC\", \"Volume_(Currency)\": \"Volume_Currency\"})\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to make lower_case all_columns\n\ndata.columns= data.columns.str.lower()\ndata.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to see first five data in dataframe\n\ndata.head() # if you want to see more or less than five data, you need to use number in brackets..like->data.head(10) \n\n# data.tail() # to see last five data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#there are null(NaN) values and we need to clean the missing data\n\ndata = data.dropna(how='any',axis=0)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode the date to period column and delete timestamp column\n\ndata['period'] = pd.to_datetime(data['timestamp'],unit='s').dt.to_period('M') # monthly period\ndata = data.drop([\"timestamp\"],axis=1)   # column drop with column name\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data = data.groupby('period')\n#data.head()\n\ndata = data.groupby('period').agg({'weighted_price': ['mean'], 'volume_btc': ['sum'], 'volume_currency': ['sum']})\ndata.columns = ['weighted_price_mean', 'volume_btc_sum', 'volume_currency_sum']\ndata.head(10)\n\n\"\"\"\ngrouped_multiple = data.groupby(['period', 'weighted_price']).agg({'volume_btc': ['mean', 'min', 'max']})\ngrouped_multiple.columns = ['volume_btc_mean', 'volume_btc_min', 'volume_btc_max']\ngrouped_multiple = grouped_multiple.reset_index()\ngrouped_multiple\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Line Plot\n# color = color, label = label, linewidth = width of line, alpha = opacity, grid = grid, linestyle = sytle of line\n#data.volume_btc_sum.plot(color = 'g',label = 'volume_btc_sum',linewidth=1, alpha = 0.5,grid = True,linestyle = ':')\n#data.volume_currency_sum.plot(color = 'r',label = 'volume_currency_sum',linewidth=1, alpha = 0.5,grid = True,linestyle = '-.')\ndata.weighted_price_mean.plot(kind = 'line', color = 'g',label = 'weighted_price_mean',linewidth=1,alpha = 0.9,grid = True,linestyle = ':')\nplt.legend(loc='upper right')     # legend = puts label into plot\nplt.xlabel('x axis')              # label = name of label\nplt.ylabel('y axis')\nplt.title('Line Plot')            # title = title of plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter Plot \n# as you can in the Scatter Plot, we can say when volume currency increase, weighted price also increse\n\n# x = volume_currency_sum, y = weighted_price_mean\ndata.plot(kind='scatter', x='volume_currency_sum', y='weighted_price_mean',alpha = 0.5,color = 'red')\nplt.xlabel('volume_currency_sum')  # label = name of label\nplt.ylabel('weighted_price_mean') \nplt.title('volume_currency_sum & weighted_price_mean Scatter Plot')            # title = title of plot\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram\n# bins = number of weighted_price_mean in figure \ndata.weighted_price_mean.plot(kind = 'hist',bins = 50,figsize = (10,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data filtering for price > 11K\n\nx = data['weighted_price_mean']>11000\ndata[x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data filtering with more than one conditions\n\ndata[np.logical_and(data['weighted_price_mean']>5, data['weighted_price_mean']<10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation map \n# We can say volume_currency_sum and weighted_price_mean are positively correlated (when values close to 1)\n# And there is no negative correlation (when values close to -1)\n\nf,ax = plt.subplots(figsize=(18,18))\nsns.heatmap(data.corr(),annot=True,linewidths=.5,fmt='.1f',ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# AND SOME EXAMPLES FOR DICTIONARY, PANDAS series and dataframe, COMPARISON, WHILE AND FOR LOOPS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create dictionary and look its keys and values\ndictionary = {'spain' : 'madrid','usa' : 'vegas'}\nprint(dictionary.keys())\nprint(dictionary.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keys have to be immutable objects like string, boolean, float, integer or tubles\n# List is not immutable\n# Keys are unique\n\ndictionary['spain'] = \"barcelona\"    # update existing entry\nprint(dictionary)\n\n\ndictionary['france'] = \"paris\"       # Add new entry\nprint(dictionary)\n\ndel dictionary['spain']              # remove entry with key 'spain'\nprint(dictionary)\n\nprint('france' in dictionary)        # check include or not, returns boolean\n\ndictionary.clear()                   # remove all entries in dict\nprint(dictionary)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del dictionary         # delete entire dictionary     \n\nprint(dictionary)       # when delete ::: it gives error because dictionary is deleted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PANDAS series and dataframe \n\nseries = data['weighted_price_mean']        # data['weighted_price_mean'] = series\nprint(type(series))\n\ndata_frame = data[['weighted_price_mean']]  # data[['weighted_price_mean']] = data frame\nprint(type(data_frame))\n\nprint(series)\nprint(data_frame)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparison operator\nprint(5 > 2)\nprint(1!=2)\n\n\n# Boolean operators\nprint(True and False)\nprint(True or False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WHILE and FOR LOOPS\n# Stay in loop if condition( i is not equal 5) is true\n\ni = 0\nwhile i != 5 :\n    print('i is: ',i)\n    i +=1\nprint(i,' is equal to 5')\n\n\n\nfor j in range(5):\n    print('j is:',j)\n    j+=1\nprint(j,' is equal to 5')\n\n\nlist1 = [0,1,2,3,4]\nfor i in list1:\n    print('i is: ',i)\nprint(i,' is equal to 5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Enumerate index and value of list\n# index : value = 0:1, 1:2, 2:3, 3:4, 4:5\nlist2 = [1,2,3,4,5]\nfor index, value in enumerate(list2):\n    print(index,\" : \",value)\nprint('')   \n\n# For dictionaries\n# We can use for loop to achive key and value of dictionary. We learnt key and value before with examples\ndictionary = {'spain':'madrid','france':'paris'}\nfor key,value in dictionary.items():\n    print(key,\" : \",value)\nprint('')\n\n# For pandas we can achieve index and value\nfor index,value in data[['weighted_price_mean']][0:2].iterrows():\n    print(index,\" : \",value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# USER DEFINED FUNCTION\n\"\"\"\ntuple: sequence of immutable python objects.\ncant modify values\ntuple uses paranthesis like tuble = (1,2,3)\nunpack tuple into several variables like a,b,c = tuple\n\"\"\"\n\ndef tuple_ex():\n    \"\"\" return defined t tuple\"\"\"\n    t = (data.agg({'weighted_price_mean': ['min']}),data.agg({'weighted_price_mean': ['max']}))\n    return t\n\nmin_mean,max_mean = tuple_ex()\n\nprint(min_mean)\nprint(max_mean)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NESTED function\ndef min_max_average():\n    \n    def tuple_ex():\n        \"\"\" return defined t tuple\"\"\"\n        t = data.weighted_price_mean.min(),data.weighted_price_mean.max()\n        print(\"t is a tuple and values are : \",t)\n        return t\n\n    min_mean,max_mean = tuple_ex()\n    print(\"minimum mean is : \", min_mean)\n    print(\"maximum mean is : \", max_mean)\n    \n    return (min_mean+max_mean)/2\n\nprint(\"Average price is : \", min_max_average())\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# flexible arguments *args --> we can send any count of parameters\ndef f(*args):\n    for i in args:\n        print(i)\n        \nf(data.weighted_price_mean.sum()) #we can send one or more(below) parameters\nprint(\" \")\nf(data.weighted_price_mean.min(),data.weighted_price_mean.max(),data.weighted_price_mean.mean())\nprint(\"\")\n\n\n# flexible arguments **kwargs that is dictionary --> --> Again we can send any count of parameters\ndef g(**kwargs):\n    \"\"\" print key and value of dictionary\"\"\"\n    i = 0\n    for key, value in kwargs.items():  \n        i = i+1\n        print(i)\n        print(key, \" \", value)\n        if i==3: #as you can see there is no 3 output data, all of it fetch and after that writes one time\n            break\n\ng(montly_weighted_price_mean = data.weighted_price_mean.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lambda function\nsquare = lambda x: x**2     # where x is name of argument\nprint(square(5))\ntot = lambda x,y,z: x-y+z   # where x,y,z are names of arguments\nprint(tot(3,4,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ANONYMOUS FUNCTİON\n#Like lambda function but it can take more than one arguments.\n\n#map(func,seq) : applies a function to all the items in a list\n    \nnumber_list = [1,2,3]\ny = map(lambda x:x**2,number_list)\nprint(list(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ITERATORS\n# iteration example\n\nname = \"ronaldo\"\nit = iter(name)\nprint(next(it))    # print next iteration\nprint(next(it))    # print next iteration\nprint(next(it))    # print next iteration\n\nprint(*it)         # print remaining iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zip example\nlist1 = [1,2,3,4]\nlist2 = [5,6,7,8]\nz = zip(list1,list2)\nprint(z)  # it keeps an address\n\nz_list = list(z)\nprint(z_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"un_zip = zip(*z_list)\nun_list1,un_list2 = list(un_zip) # unzip returns tuple\nprint(un_list1)\nprint(un_list2)\nprint(type(un_list2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of list comprehension\nnum1 = [1,2,3]\nnum2 = [i + 1 for i in num1 ]\nprint(num2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conditionals on iterable\nnum1 = [5,10,15,20]\nnum2 = [i**2 if i == 10 else i-5 if i < 7 else i+5 for i in num1]\nprint(num2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets return btc csv data and make one more list comprehension example\n# lets classify btc mohtly_mean whether they have high or low price according to all_time mean. \n# Our threshold is all_time mean.\nthreshold = sum(data.weighted_price_mean)/len(data.weighted_price_mean)\ndata['threshold'] = sum(data.weighted_price_mean)/len(data.weighted_price_mean)\ndata[\"weighted_price_level\"] = [\"higher\" if i > threshold else \"lower\" for i in data.weighted_price_mean]\ndata.loc[:,[\"weighted_price_mean\",\"threshold\",\"weighted_price_level\"]] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CLEANING DATA\n\n#DIAGNOSE DATA for CLEANING","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head() #first five data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail() #last five data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns gives column names of features\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape gives number of rows and columns in a tuble\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# info gives data type like dataframe, number of sample or row, number of feature or column, feature types and memory usage\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#EXPLORATORY DATA ANALYSIS\n#value_counts(): Frequency counts\n#outliers: the value that is considerably higher or lower from rest of the data\n\n# For example lets look frequency of Volume_(BTC)\ndata\nprint(data['weighted_price_level'].value_counts(dropna =False))  # if there are nan values that also be counted\n\n# As it can be seen below there are 1241716 NaN values in the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can see sum basic information about data with describe() method \n\ndata.describe() #ignore null entries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VISUAL EXPLORATORY DATA ANALYSIS\n\n# Box plots: visualize basic statistics like outliers, min/max or quantiles\n\n#What is quantile?\n#1,4,5,6,8,9,11,12,13,14,15,16,17\n#The median is the number that is in middle of the sequence. In this case it would be 11.\n#The lower quartile is the median in between the smallest number and the median i.e. in between 1 and 11, which is 6.\n#The upper quartile, you find the median between the median and the largest number i.e. between 11 and 17, which will be 14 according to the question above.\n\n# For example: compare weighted_price_mean of BTC that are weighted_price_level is Higher or Lower\n# Black line at top is max\n# Blue line at top is 75%\n# Green line is median (50%)\n# Blue line at bottom is 25%\n# Black line at bottom is min\n# There are no outliers\n\ndata.boxplot(column='weighted_price_mean',by = 'weighted_price_level')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TIDY DATA\n# We tidy data with melt(). Describing melt is confusing. Therefore lets make example to understand it.\n\ndata_new = data.head()    # I only take 5 rows into new data\ndata_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets melt\n# id_vars = what we do not wish to melt\n# value_vars = what we want to melt\nmelted = pd.melt(frame=data_new,id_vars = 'weighted_price_mean', value_vars= ['volume_btc_sum','volume_currency_sum'])\nmelted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PIVOTING DATA\n#Reverse of melting.\n\n# Index is name\n# I want to make that columns are variable\n# Finally values in columns are value\n\nmelted.pivot(index = 'weighted_price_mean', columns = 'variable',values='value')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CONCATENATING DATA\n# We can concatenate two dataframe\n\n# Firstly lets create 2 data frame\ndata1 = data.head()\ndata2= data.tail()\nconc_data_row = pd.concat([data1,data2],axis =0,ignore_index =True) #axis=0 : adds dataframes in row \nconc_data_row","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = data['weighted_price_mean'].head()\ndata2= data['volume_btc_sum'].head()\nconc_data_col = pd.concat([data1,data2],axis =1) # axis = 1 : adds dataframes in column\nconc_data_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# DATA TYPES\n# There are 5 basic data types: object(string),boolean, integer, float and categorical.\n# We can make conversion data types like from str to categorical or from int to float\n# Why is category important:\n# make dataframe smaller in memory\n# can be utilized for anlaysis especially for sklearn(we will learn later)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# type(data) this is about all data like DataFrame\n\ndata.dtypes # this is about columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets convert object(str) to categorical and float to int.\ndata['weighted_price_level'] = data['weighted_price_level'].astype('category')\ndata['threshold'] = data['threshold'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As you can see weighted_price_level is converted from object to category\n# And threshold is converted from float to int\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MISSING DATA and TESTING WITH ASSERT\n\n# If we encounter with missing data, what we can do:\n\n# leave as is\n# drop them with dropna()\n# fill missing value with fillna()\n# fill missing values with test statistics like mean\n# Assert statement: check that you can turn on or turn off when you are done with your testing of the program","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at does btc data have nan value\n# As you can see there are 106 entries. However there is no null object because of i clean them before\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check weighted_price_mean\ndata[\"threshold\"].value_counts(dropna =False)\n# As you can see, there is no NAN value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"test\"] = [None if i<10 else 1 for i in data.weighted_price_mean] #added sum NaN values for test column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"test\"].value_counts(dropna = False)  #lets see count of values with NaN values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Lets check with assert statement\n# Assert statement:\nassert 1==1 # return nothing because it is true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to run all code, we need to make this line comment\n# assert 1==2 # return error because it is false you can check and see error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to run all code, we need to make this line comment\n#assert data['test'].notnull().all() # returns error  because it is false\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert data['weighted_price_mean'].notnull().all() # returns nothing because we drop nan values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets fill drop NaN values \n# data = data[\"test\"].dropna(inplace = True)  # inplace = True means we do not assign it to new variable. Changes automatically assigned to data\n\n# Lets fill nan values with empty\ndata[\"test\"].fillna('empty',inplace = True)\ndata[\"test\"].value_counts(dropna = False)  #lets see count of empty values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert data['test'].notnull().all() # returns nothing  because it is true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can build data frames from csv as we did earlier.\n# Also we can build dataframe from dictionaries\n\n# data frames from dictionary\ncountry = [\"Spain\",\"France\",\"Germany\",\"Turkey\"]\nteam = [\"Barcelona\",\"PSG\",\"Bayern\",\"Fenerbahce\"]\nlist_label = [\"country\",\"team\"]\nlist_col = [country,team]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped)\ndf = pd.DataFrame(data_dict)\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add new columns\ndf[\"fan\"] = [\"11\",\"12\",\"13\",\"35\"]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Broadcasting\ndf[\"income\"] = 0 #Broadcasting entire column\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VISUAL EXPLORATORY DATA ANALYSIS\n\n* Plot\n* Subplot\n* Histogram:\n    * bins: number of bins\n    * range(tuble): min and max values of bins\n    * normed(boolean): normalize or not\n    * cumulative(boolean): compute cumulative distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting all data \ndata1 = data.loc[:,[\"weighted_price_mean\",\"volume_btc_sum\",\"volume_currency_sum\"]]\ndata1.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# subplots\ndata1.plot(subplots = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatter plot  \ndata1.plot(kind = \"scatter\",x=\"volume_currency_sum\",y = \"weighted_price_mean\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram plot  \ndata1.plot(kind = \"hist\",y = \"weighted_price_mean\",bins = 50,range= (0,20000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\ndata1.plot(kind = \"hist\",y = \"weighted_price_mean\",bins = 50,range= (0,20000),ax = axes[0]) #non cumuşative\ndata1.plot(kind = \"hist\",y = \"weighted_price_mean\",bins = 50,range= (0,20000),ax = axes[1],cumulative = True) #cumulative\n#plt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# INDEXING PANDAS TIME SERIES\n* datetime = object\n* parse_dates(boolean): Transform date to ISO 8601 (yyyy-mm-dd hh:mm:ss ) format"},{"metadata":{"trusted":true},"cell_type":"code","source":"time_list = [\"1992-03-08\",\"1992-04-12\"]\nprint(type(time_list[1])) # As you can see date is string\n\n# however we want it to be datetime object\ndatetime_object = pd.to_datetime(time_list)\nprint(type(datetime_object))\n\nprint(\"\")\nprint(datetime_object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as you can see we have PeriodIndex\ntype(data.index)\n\n# you can see here PeriodIndex again at head of output\n# data.info() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[\"2018-09\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# In order to practice lets take head of btc data and add it a time list \ndata2 = data.head()\ndate_list = [\"2012-01-30\",\"2012-01-31\",\"2013-01-31\",\"2013-02-28\",\"2013-04-30\"]\ndatetime_object = pd.to_datetime(date_list)\ndata2[\"date\"] = datetime_object\n\n# lets make date as index\ndata2= data2.set_index(\"date\")\ndata2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can select according to our DateIndex\n\nprint(data2.loc[\"2012-01-30\"])\nprint(data2.loc[\"2012-01-31\":\"2013-02-28\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RESAMPLING PANDAS TIME SERIES\n* Resampling: statistical method over different time intervals\n    *   Needs string to specify frequency like \"M\" = month or \"A\" = year\n* Downsampling: reduce date time rows to slower frequency like from daily to weekly\n* Upsampling: increase date time rows to faster frequency like from daily to hourly\n* Interpolate: Interpolate values according to different methods like ‘linear’, ‘time’ or index’\n    *   https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.interpolate.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use data2 that we create at previous part \ndata2.resample(\"A\").mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets resample with month\ndata2.resample(\"M\").mean()\n# As you can see there are lots of NaN because data2 does not include all months","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In real life (data is real. Not created from us like data2) we can solve this problem with interpolate\n# For example we can interpolete from with mean()\ndata2.resample(\"M\").mean().interpolate(\"linear\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [0,1,2,3,4]\n\na[0]\n\nfor a[0] in a:\n\n    print(a[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MANIPULATING DATA FRAMES WITH PANDAS\n  INDEXING DATA FRAMES\n    * Indexing using square brackets\n    * Using column attribute and row label\n    * Using loc accessor\n    * Selecting only some columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info() # we have PeriodIndex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#then and we want to change the index to number 0-105\ndata[\"new_index\"] = 0\n\n\ni=0 \nfor i in range(106):\n    data[\"new_index\"][i] = i\n    \ndata.head() # we added new column to assign new index (new_index column)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data= data.set_index(\"new_index\") #we are changing the index to new_index column\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexing using square brackets\ndata[\"weighted_price_mean\"][1]   #second value will turn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexing using column attribute and row label\ndata.weighted_price_mean[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using loc accessor\ndata.loc[1,[\"weighted_price_mean\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting only some columns\ndata[[\"weighted_price_mean\",\"threshold\",\"weighted_price_level\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SLICING DATA FRAME\n*     Difference between selecting columns\n    *     Series and data frames\n*     Slicing and indexing series\n*     Reverse slicing\n*     From something to end"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Difference between selecting columns: series and dataframes\nprint(type(data[\"weighted_price_mean\"]))     # series\nprint(type(data[[\"weighted_price_mean\"]]))   # data frames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slicing and indexing series\ndata.loc[1:10,\"weighted_price_mean\":\"volume_currency_sum\"]   # 10 and \"volume_currency_sum\" are inclusive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reverse slicing \ndata.loc[10:1:-1,\"weighted_price_mean\":\"volume_currency_sum\"] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From something to end\ndata.loc[1:10,\"threshold\":] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# FILTERING DATA FRAMES\nCreating boolean series Combining filters Filtering column based others"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating boolean series\nboolean = data.weighted_price_mean > 10000  \n\ndata[boolean]  #returns true values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining filters\nfirst_filter = data.weighted_price_mean > 10000\nsecond_filter = data.volume_btc_sum > 400000\n\ndata[first_filter & second_filter]  # apply 2 filter with and condition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering column based others\ndata.weighted_price_mean[data.volume_btc_sum>800000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRANSFORMING DATA\n* Plain python functions\n* Lambda function: to apply arbitrary python function to every element\n* Defining column using other columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plain python functions\ndef div(n):\n    return n/2\n\ndata.weighted_price_mean.apply(div)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can use lambda function\ndata.weighted_price_mean.apply(lambda n : n/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining column using other columns\ndata[\"total_volume\"] = data.volume_btc_sum + data.volume_currency_sum\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# INDEX OBJECTS AND LABELED DATA\n* index: sequence of label"},{"metadata":{"trusted":true},"cell_type":"code","source":"# our index name is this:\nprint(data.index.name)\n\n# lets change it\ndata.index.name = \"index_name\"\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overwrite index\n\n# if we want to modify index, we need to change all of them.\ndata.head()\n\n# first copy of our data to data1 then change index \ndata1 = data.copy()\n\n# lets make index start from 100. It is not remarkable change but it is just example\ndata1.index = range(100,206,1)\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can make one of the column as index. We actually did it at the beginning of manipulating data frames with pandas section\n# It was like this\n# data= data.set_index(\"new_index\")\n# also you can use \n# data.index = data[\"new_index\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# HIERARCHICAL INDEXING\n* Setting indexing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets read data frame one more time to start from beginning\ndata.head()\n# As you can see there is index. However we want to set one or more column to be index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting index : test column is outer weighted_price_level is inner index\ndata1 = data.set_index([\"test\",\"weighted_price_level\"]) \ndata1.head(100)\n# data1.loc[\"Fire\",\"Flying\"] # howw to use indexes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# PIVOTING DATA FRAMES\n* pivoting: reshape tool"},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {\"treatment\":[\"A\",\"A\",\"B\",\"B\"],\"gender\":[\"F\",\"M\",\"F\",\"M\"],\"response\":[10,45,5,9],\"age\":[15,4,72,65]}\ndf = pd.DataFrame(dic)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivoting\ndf.pivot(index=\"treatment\",columns = \"gender\",values=\"response\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STACKING and UNSTACKING DATAFRAME\n* deal with multi label indexes\n* level: position of unstacked index\n* swaplevel: change inner and outer level index position"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.set_index([\"treatment\",\"gender\"])\ndf1\n# lets unstack it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# level determines indexes\ndf1.unstack(level=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.unstack(level=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change inner and outer level index position\ndf2 = df1.swaplevel(0,1)\ndf2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MELTING DATA FRAMES\n* Reverse of pivoting"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.pivot(index=\"treatment\",columns = \"gender\",values=\"response\")\npd.melt(df,id_vars=\"treatment\",value_vars=[\"age\",\"response\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CATEGORICALS AND GROUPBY"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# according to treatment take means of other features\ndf.groupby(\"treatment\").mean()   # mean is aggregation / reduction method\n# there are other methods like sum, std,max or min","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can only choose one of the feature\ndf.groupby(\"treatment\").age.max() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or we can choose multiple features\ndf.groupby(\"treatment\")[[\"age\",\"response\"]].min() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# as you can see gender is object\n# However if we use groupby, we can convert it categorical data. \n# Because categorical data uses less memory, speed up operations like groupby\n#df[\"gender\"] = df[\"gender\"].astype(\"category\")\n#df[\"treatment\"] = df[\"treatment\"].astype(\"category\")\n#df.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}