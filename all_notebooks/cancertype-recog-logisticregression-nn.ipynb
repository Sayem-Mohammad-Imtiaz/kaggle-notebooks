{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Notebook Intro\nIn this notebook, cancer type has been classified and predicted as **malignant or benign** based on Logistic Regression NN model.\n#### Dataset: [breastcancer](https://www.kaggle.com/jiuzhang/ninechapter-breastcancer)\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\nimport os\n#print(os.listdir(\"../input\"))\n\npath = \"../input/breastCancer.csv\"\ndf = pd.read_csv(path)\nprint(df.describe(),df.head(), df.columns)","execution_count":56,"outputs":[{"output_type":"stream","text":"                 id     ...       Unnamed: 32\ncount  5.690000e+02     ...               0.0\nmean   3.037183e+07     ...               NaN\nstd    1.250206e+08     ...               NaN\nmin    8.670000e+03     ...               NaN\n25%    8.692180e+05     ...               NaN\n50%    9.060240e+05     ...               NaN\n75%    8.813129e+06     ...               NaN\nmax    9.113205e+08     ...               NaN\n\n[8 rows x 32 columns]          id diagnosis     ...       fractal_dimension_worst  Unnamed: 32\n0    842302         M     ...                       0.11890          NaN\n1    842517         M     ...                       0.08902          NaN\n2  84300903         M     ...                       0.08758          NaN\n3  84348301         M     ...                       0.17300          NaN\n4  84358402         M     ...                       0.07678          NaN\n\n[5 rows x 33 columns] Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n      dtype='object')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# diagnosis column is an object, we simply transform it to integer in order to make categorical datatype\ndf.info()","execution_count":59,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\ndiagnosis                  569 non-null object\nradius_mean                569 non-null float64\ntexture_mean               569 non-null float64\nperimeter_mean             569 non-null float64\narea_mean                  569 non-null float64\nsmoothness_mean            569 non-null float64\ncompactness_mean           569 non-null float64\nconcavity_mean             569 non-null float64\nconcave points_mean        569 non-null float64\nsymmetry_mean              569 non-null float64\nfractal_dimension_mean     569 non-null float64\nradius_se                  569 non-null float64\ntexture_se                 569 non-null float64\nperimeter_se               569 non-null float64\narea_se                    569 non-null float64\nsmoothness_se              569 non-null float64\ncompactness_se             569 non-null float64\nconcavity_se               569 non-null float64\nconcave points_se          569 non-null float64\nsymmetry_se                569 non-null float64\nfractal_dimension_se       569 non-null float64\nradius_worst               569 non-null float64\ntexture_worst              569 non-null float64\nperimeter_worst            569 non-null float64\narea_worst                 569 non-null float64\nsmoothness_worst           569 non-null float64\ncompactness_worst          569 non-null float64\nconcavity_worst            569 non-null float64\nconcave points_worst       569 non-null float64\nsymmetry_worst             569 non-null float64\nfractal_dimension_worst    569 non-null float64\ndtypes: float64(30), object(1)\nmemory usage: 137.9+ KB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"id\",\"Unnamed: 32\"], axis=1, inplace = True)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":122,"outputs":[{"output_type":"execute_result","execution_count":122,"data":{"text/plain":"   diagnosis           ...             fractal_dimension_worst\n0          1           ...                             0.11890\n1          1           ...                             0.08902\n2          1           ...                             0.08758\n3          1           ...                             0.17300\n4          1           ...                             0.07678\n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>fractal_dimension_mean</th>\n      <th>radius_se</th>\n      <th>texture_se</th>\n      <th>perimeter_se</th>\n      <th>area_se</th>\n      <th>smoothness_se</th>\n      <th>compactness_se</th>\n      <th>concavity_se</th>\n      <th>concave points_se</th>\n      <th>symmetry_se</th>\n      <th>fractal_dimension_se</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>1.0950</td>\n      <td>0.9053</td>\n      <td>8.589</td>\n      <td>153.40</td>\n      <td>0.006399</td>\n      <td>0.04904</td>\n      <td>0.05373</td>\n      <td>0.01587</td>\n      <td>0.03003</td>\n      <td>0.006193</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>0.5435</td>\n      <td>0.7339</td>\n      <td>3.398</td>\n      <td>74.08</td>\n      <td>0.005225</td>\n      <td>0.01308</td>\n      <td>0.01860</td>\n      <td>0.01340</td>\n      <td>0.01389</td>\n      <td>0.003532</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>0.7456</td>\n      <td>0.7869</td>\n      <td>4.585</td>\n      <td>94.03</td>\n      <td>0.006150</td>\n      <td>0.04006</td>\n      <td>0.03832</td>\n      <td>0.02058</td>\n      <td>0.02250</td>\n      <td>0.004571</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>0.4956</td>\n      <td>1.1560</td>\n      <td>3.445</td>\n      <td>27.23</td>\n      <td>0.009110</td>\n      <td>0.07458</td>\n      <td>0.05661</td>\n      <td>0.01867</td>\n      <td>0.05963</td>\n      <td>0.009208</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>0.7572</td>\n      <td>0.7813</td>\n      <td>5.438</td>\n      <td>94.44</td>\n      <td>0.011490</td>\n      <td>0.02461</td>\n      <td>0.05688</td>\n      <td>0.01885</td>\n      <td>0.01756</td>\n      <td>0.005115</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if x == \"M\" else 0  for x in df.diagnosis]\nprint(df.info())","execution_count":60,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\ndiagnosis                  569 non-null int64\nradius_mean                569 non-null float64\ntexture_mean               569 non-null float64\nperimeter_mean             569 non-null float64\narea_mean                  569 non-null float64\nsmoothness_mean            569 non-null float64\ncompactness_mean           569 non-null float64\nconcavity_mean             569 non-null float64\nconcave points_mean        569 non-null float64\nsymmetry_mean              569 non-null float64\nfractal_dimension_mean     569 non-null float64\nradius_se                  569 non-null float64\ntexture_se                 569 non-null float64\nperimeter_se               569 non-null float64\narea_se                    569 non-null float64\nsmoothness_se              569 non-null float64\ncompactness_se             569 non-null float64\nconcavity_se               569 non-null float64\nconcave points_se          569 non-null float64\nsymmetry_se                569 non-null float64\nfractal_dimension_se       569 non-null float64\nradius_worst               569 non-null float64\ntexture_worst              569 non-null float64\nperimeter_worst            569 non-null float64\narea_worst                 569 non-null float64\nsmoothness_worst           569 non-null float64\ncompactness_worst          569 non-null float64\nconcavity_worst            569 non-null float64\nconcave points_worst       569 non-null float64\nsymmetry_worst             569 non-null float64\nfractal_dimension_worst    569 non-null float64\ndtypes: float64(30), int64(1)\nmemory usage: 137.9 KB\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define x and y values\ny = df.diagnosis.values\nx_data = df.drop([\"diagnosis\"],axis=1) #unnormalized data\n\nx_data.head()","execution_count":61,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"   radius_mean           ...             fractal_dimension_worst\n0        17.99           ...                             0.11890\n1        20.57           ...                             0.08902\n2        19.69           ...                             0.08758\n3        11.42           ...                             0.17300\n4        20.29           ...                             0.07678\n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>fractal_dimension_mean</th>\n      <th>radius_se</th>\n      <th>texture_se</th>\n      <th>perimeter_se</th>\n      <th>area_se</th>\n      <th>smoothness_se</th>\n      <th>compactness_se</th>\n      <th>concavity_se</th>\n      <th>concave points_se</th>\n      <th>symmetry_se</th>\n      <th>fractal_dimension_se</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>1.0950</td>\n      <td>0.9053</td>\n      <td>8.589</td>\n      <td>153.40</td>\n      <td>0.006399</td>\n      <td>0.04904</td>\n      <td>0.05373</td>\n      <td>0.01587</td>\n      <td>0.03003</td>\n      <td>0.006193</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>0.5435</td>\n      <td>0.7339</td>\n      <td>3.398</td>\n      <td>74.08</td>\n      <td>0.005225</td>\n      <td>0.01308</td>\n      <td>0.01860</td>\n      <td>0.01340</td>\n      <td>0.01389</td>\n      <td>0.003532</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>0.7456</td>\n      <td>0.7869</td>\n      <td>4.585</td>\n      <td>94.03</td>\n      <td>0.006150</td>\n      <td>0.04006</td>\n      <td>0.03832</td>\n      <td>0.02058</td>\n      <td>0.02250</td>\n      <td>0.004571</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>0.4956</td>\n      <td>1.1560</td>\n      <td>3.445</td>\n      <td>27.23</td>\n      <td>0.009110</td>\n      <td>0.07458</td>\n      <td>0.05661</td>\n      <td>0.01867</td>\n      <td>0.05963</td>\n      <td>0.009208</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>0.7572</td>\n      <td>0.7813</td>\n      <td>5.438</td>\n      <td>94.44</td>\n      <td>0.011490</td>\n      <td>0.02461</td>\n      <td>0.05688</td>\n      <td>0.01885</td>\n      <td>0.01756</td>\n      <td>0.005115</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalization of values of features\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\nx.head()","execution_count":62,"outputs":[{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"   radius_mean           ...             fractal_dimension_worst\n0     0.521037           ...                            0.418864\n1     0.643144           ...                            0.222878\n2     0.601496           ...                            0.213433\n3     0.210090           ...                            0.773711\n4     0.629893           ...                            0.142595\n\n[5 rows x 30 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>fractal_dimension_mean</th>\n      <th>radius_se</th>\n      <th>texture_se</th>\n      <th>perimeter_se</th>\n      <th>area_se</th>\n      <th>smoothness_se</th>\n      <th>compactness_se</th>\n      <th>concavity_se</th>\n      <th>concave points_se</th>\n      <th>symmetry_se</th>\n      <th>fractal_dimension_se</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.521037</td>\n      <td>0.022658</td>\n      <td>0.545989</td>\n      <td>0.363733</td>\n      <td>0.593753</td>\n      <td>0.792037</td>\n      <td>0.703140</td>\n      <td>0.731113</td>\n      <td>0.686364</td>\n      <td>0.605518</td>\n      <td>0.356147</td>\n      <td>0.120469</td>\n      <td>0.369034</td>\n      <td>0.273811</td>\n      <td>0.159296</td>\n      <td>0.351398</td>\n      <td>0.135682</td>\n      <td>0.300625</td>\n      <td>0.311645</td>\n      <td>0.183042</td>\n      <td>0.620776</td>\n      <td>0.141525</td>\n      <td>0.668310</td>\n      <td>0.450698</td>\n      <td>0.601136</td>\n      <td>0.619292</td>\n      <td>0.568610</td>\n      <td>0.912027</td>\n      <td>0.598462</td>\n      <td>0.418864</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.643144</td>\n      <td>0.272574</td>\n      <td>0.615783</td>\n      <td>0.501591</td>\n      <td>0.289880</td>\n      <td>0.181768</td>\n      <td>0.203608</td>\n      <td>0.348757</td>\n      <td>0.379798</td>\n      <td>0.141323</td>\n      <td>0.156437</td>\n      <td>0.082589</td>\n      <td>0.124440</td>\n      <td>0.125660</td>\n      <td>0.119387</td>\n      <td>0.081323</td>\n      <td>0.046970</td>\n      <td>0.253836</td>\n      <td>0.084539</td>\n      <td>0.091110</td>\n      <td>0.606901</td>\n      <td>0.303571</td>\n      <td>0.539818</td>\n      <td>0.435214</td>\n      <td>0.347553</td>\n      <td>0.154563</td>\n      <td>0.192971</td>\n      <td>0.639175</td>\n      <td>0.233590</td>\n      <td>0.222878</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.601496</td>\n      <td>0.390260</td>\n      <td>0.595743</td>\n      <td>0.449417</td>\n      <td>0.514309</td>\n      <td>0.431017</td>\n      <td>0.462512</td>\n      <td>0.635686</td>\n      <td>0.509596</td>\n      <td>0.211247</td>\n      <td>0.229622</td>\n      <td>0.094303</td>\n      <td>0.180370</td>\n      <td>0.162922</td>\n      <td>0.150831</td>\n      <td>0.283955</td>\n      <td>0.096768</td>\n      <td>0.389847</td>\n      <td>0.205690</td>\n      <td>0.127006</td>\n      <td>0.556386</td>\n      <td>0.360075</td>\n      <td>0.508442</td>\n      <td>0.374508</td>\n      <td>0.483590</td>\n      <td>0.385375</td>\n      <td>0.359744</td>\n      <td>0.835052</td>\n      <td>0.403706</td>\n      <td>0.213433</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.210090</td>\n      <td>0.360839</td>\n      <td>0.233501</td>\n      <td>0.102906</td>\n      <td>0.811321</td>\n      <td>0.811361</td>\n      <td>0.565604</td>\n      <td>0.522863</td>\n      <td>0.776263</td>\n      <td>1.000000</td>\n      <td>0.139091</td>\n      <td>0.175875</td>\n      <td>0.126655</td>\n      <td>0.038155</td>\n      <td>0.251453</td>\n      <td>0.543215</td>\n      <td>0.142955</td>\n      <td>0.353665</td>\n      <td>0.728148</td>\n      <td>0.287205</td>\n      <td>0.248310</td>\n      <td>0.385928</td>\n      <td>0.241347</td>\n      <td>0.094008</td>\n      <td>0.915472</td>\n      <td>0.814012</td>\n      <td>0.548642</td>\n      <td>0.884880</td>\n      <td>1.000000</td>\n      <td>0.773711</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.629893</td>\n      <td>0.156578</td>\n      <td>0.630986</td>\n      <td>0.489290</td>\n      <td>0.430351</td>\n      <td>0.347893</td>\n      <td>0.463918</td>\n      <td>0.518390</td>\n      <td>0.378283</td>\n      <td>0.186816</td>\n      <td>0.233822</td>\n      <td>0.093065</td>\n      <td>0.220563</td>\n      <td>0.163688</td>\n      <td>0.332359</td>\n      <td>0.167918</td>\n      <td>0.143636</td>\n      <td>0.357075</td>\n      <td>0.136179</td>\n      <td>0.145800</td>\n      <td>0.519744</td>\n      <td>0.123934</td>\n      <td>0.506948</td>\n      <td>0.341575</td>\n      <td>0.437364</td>\n      <td>0.172415</td>\n      <td>0.319489</td>\n      <td>0.558419</td>\n      <td>0.157500</td>\n      <td>0.142595</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","execution_count":65,"outputs":[{"output_type":"stream","text":"x_train:  (30, 455)\nx_test:  (30, 114)\ny_train:  (455,)\ny_test:  (114,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameter initialize\n# e.g. dimension = 30\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n# sigmoid function\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head\n\n# forward & back propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients\n\n# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\n\n# prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\n# logistic_regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","execution_count":105,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 4, num_iterations = 250)","execution_count":120,"outputs":[{"output_type":"stream","text":"Cost after iteration 0: 0.692977\nCost after iteration 10: 0.380300\nCost after iteration 20: 0.223346\nCost after iteration 30: 0.197934\nCost after iteration 40: 0.181022\nCost after iteration 50: 0.168641\nCost after iteration 60: 0.159030\nCost after iteration 70: 0.151270\nCost after iteration 80: 0.144827\nCost after iteration 90: 0.139364\nCost after iteration 100: 0.134655\nCost after iteration 110: 0.130544\nCost after iteration 120: 0.126914\nCost after iteration 130: 0.123680\nCost after iteration 140: 0.120776\nCost after iteration 150: 0.118150\nCost after iteration 160: 0.115762\nCost after iteration 170: 0.113579\nCost after iteration 180: 0.111572\nCost after iteration 190: 0.109721\nCost after iteration 200: 0.108005\nCost after iteration 210: 0.106410\nCost after iteration 220: 0.104923\nCost after iteration 230: 0.103530\nCost after iteration 240: 0.102224\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"stream","text":"test accuracy: 97.36842105263158 %\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sklearn with LR\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","execution_count":121,"outputs":[{"output_type":"stream","text":"test accuracy 0.9736842105263158\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n As a result, the forward & back propagation applied and sigmoid function used as an activation function on logistic regression classifier. Within appropriate learning rate and iteration, **about 97% accuracy level** can be seen on test data."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}