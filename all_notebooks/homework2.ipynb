{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"markdown","source":"First we preprocess the input file","metadata":{}},{"cell_type":"code","source":"with open('clean.txt','w') as clean_f:\n    with open('../input/coding2/stream_data.txt') as f:\n        line = f.readline()\n        cur_bits_list = line.split('\\t')\n        for bit in cur_bits_list:\n            line_clean = bit + '\\n'\n            clean_f.write(line_clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nimport math\nimport time\n\nbuckets = []\nwindowsize = 1000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"current_location = 5000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_over_windowsize(time_now):\n    if len(buckets)>0 and time_now-windowsize==buckets[0]['timestamp']:\n        # the leftest bucket is over windowsize\n        del buckets[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_buckets():\n    for i in range(len(buckets)-1,1,-1):\n        if buckets[i]['bit_sum']==buckets[i-2]['bit_sum']:\n            # have 3 same size buckets, then merge\n            buckets[i-2]['bit_sum']+=buckets[i-1]['bit_sum']\n            buckets[i-2]['timestamp']=buckets[i-1]['timestamp']\n            del buckets[i-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def DGIM():\n    total_sum = 0\n    # 读取文件\n    with open('clean.txt') as f:\n        # 从头读到指定位置\n        i = 0\n        for i in range(current_location):\n#             print(i)\n            warning_flag = 0\n            line = f.readline()\n            if line:\n                check_over_windowsize(i+1)\n                # 如果读到1 则加bucket\n                if line.strip('\\n') == \"1\":\n                    bucket = {\"timestamp\":i+1,\"bit_sum\":1}\n                    buckets.append(bucket)\n                    merge_buckets()\n    # 统计目前个数\n    for i in range(len(buckets)):\n        total_sum+=buckets[i]['bit_sum']\n    total_sum-=buckets[0]['bit_sum']/2\n    return total_sum if len(buckets)>0 else 0\n                ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time_DGIM = time.time()\nDGIM_sum = DGIM()\nend_time_DGIM = time.time()\nprint(\"DGIM method total time:\",end_time_DGIM - start_time_DGIM)\nprint(\"DGIM sum:\",DGIM_sum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\ndef count_exact_results():\n    total_num = 0\n    with open('clean.txt') as f:\n        f.seek(0 if windowsize >= current_location else 2*(current_location - windowsize))\n        for i in range(current_location if windowsize >= current_location else windowsize):\n            line = f.readline()\n#             print(line)\n            if line and line.strip('\\n') == '1':\n                total_num += 1\n    return total_num","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time_exact = time.time()\nexact_sum = count_exact_results()\nend_time_exact = time.time()\nprint(\"exact result's total time:\",end_time_exact - start_time_exact)\nprint(\"exact sum:\",exact_sum)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"compare two methods' space consume:","metadata":{}},{"cell_type":"code","source":"import sys\nDGIM_space = sys.getsizeof(buckets)\nprint(\"DGIM space:\",DGIM_space)\nexact_space = sys.getsizeof(\"1\")\nprint(\"exact finding uses space:\",exact_space)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can find that when taking the window size of 1000, getting the exact result have a faster speed and cost less space.","metadata":{}},{"cell_type":"markdown","source":"However, when we need to get a big window size: the DGIM method runs faster and have a less space cost.","metadata":{}},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nsignature_length = 25\nbands = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"origin_data = pd.read_csv('../input/coding2/docs_for_lsh.csv')\n# print(origin_data)\ninput_matrix = origin_data.values\nprint(input_matrix)\nprint(input_matrix.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def minhash(input_matrix,signature_length):\n    sign = np.zeros((signature_length,1000000),dtype=int)\n    # each row : a signature getting from one function \n    for i in range(signature_length):\n        cur = np.zeros((1,1000000),dtype = int)\n        a = random.randint(1,10)\n        b = random.randint(1,10)\n#         new_line_number = list()\n        m = 0 # current inset num\n        while m != 1000000:\n            for j in range(200):\n                cur_newline_num = ((j*a+b)%229)%200\n#                 new_line_number.append(cur_newline_num)\n                if input_matrix[m][cur_newline_num+1] == 1:\n                    cur[0][m] = cur_newline_num\n                    m += 1\n                    break\n        sign[i] = cur\n    return sign","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_hash_table = minhash(input_matrix,25)\n# each row: one signature for different files\nprint(min_hash_table)\nprint(min_hash_table.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_hash_table_reverse = min_hash_table.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jaccard_calculation(table,line_A,line_B):\n    A_hash_values = table[line_A]\n    B_hash_values = table[line_B]\n    return np.float(np.count_nonzero(A_hash_values == B_hash_values)) / np.float(len(A_hash_values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = jaccard_calculation(min_hash_table_reverse,24,10000)\nprint(test_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"traditional method (not big data!) just search! (need much time)","metadata":{}},{"cell_type":"code","source":"# best_score = 0\n# best_A = 0\n# best_B = 0\n# for i in range(1000000):\n#     for j in range(i+1,1000000):\n#         cur_jaccard = jaccard_calculation(min_hash_table,i,j)\n#         if cur_jaccard > best_score:\n#             best_score = cur_jaccard\n#             best_A = i\n#             best_B = j","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LSH method:","metadata":{}},{"cell_type":"code","source":"print(min_hash_table_reverse.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_pairs = {}\ntotal_cutting_num = int(signature_length / bands)\nH_table_lists = []\nfor i in range(total_cutting_num):\n# i = 0\n    cur = min_hash_table_reverse[:,i*bands:(i+1)*bands]\n    Hash_table = {}\n    cur_line_num = 0\n    for j in cur:\n        cur_key = ''\n        for m in j:\n            cur_key += str(m)\n            cur_key += ','\n        if cur_key in Hash_table.keys():\n            Hash_table[cur_key].append(cur_line_num)\n        else:\n            Hash_table[cur_key] = [cur_line_num]\n        cur_line_num += 1\n    H_table_lists.append(Hash_table)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(Hash_table.keys()))\n# print(Hash_table.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(Hash_table)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score = 0\nbest_a = 0\nbest_b = 0\n# Final_table = {}\nmax_flag = 0\nfor table in H_table_lists:\n    for bucket in table.values():\n        # 取每一个数组\n        bucket_length = len(bucket)\n        for i in range(bucket_length):\n            # 数组中两两配对\n            line_number_a = bucket[i]\n            for j in range(i+1,bucket_length):\n                line_number_b = bucket[j]\n#                 cur_tuple = (line_number_a,line_number_b)\n#                 if cur_tuple not in Final_table.keys():\n#                     Final_table[cur_tuple] = 1\n#                 else:\n#                     Final_table[cur_tuple] += 1\n                cur_score = jaccard_calculation(min_hash_table_reverse,line_number_a,line_number_b)\n                if cur_score > best_score:\n                    best_score = cur_score\n                    best_a = line_number_a\n                    best_b = line_number_b\n                    if best_score == 1:\n                        max_flag = 1\n                        break\n            if max_flag:\n                break\n        if max_flag:\n            break\n    if max_flag:\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_score)\nprint(best_a)\nprint(best_b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# Your code here, you can add cells if necessary\nthreshold = 0.8\n\ntotal_pairs = {}\ntotal_cutting_num = int(signature_length / bands)\nH_table_lists = []\nfor i in range(total_cutting_num):\n# i = 0\n    cur = min_hash_table_reverse[:,i*bands:(i+1)*bands]\n    Hash_table = {}\n    cur_line_num = 0\n    for j in cur:\n        cur_key = ''\n        for m in j:\n            cur_key += str(m)\n            cur_key += ','\n        if cur_key in Hash_table.keys():\n            if 0 in Hash_table[cur_key]:\n                Hash_table[cur_key].append(cur_line_num)\n        elif cur_line_num == 0:\n            Hash_table[cur_key] = [cur_line_num]\n        cur_line_num += 1\n    H_table_lists.append(Hash_table)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(Hash_table)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def take_score(elem):\n    return elem[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score_list = []\ncur_store_num = 0\ncur_min_tuple = (0,0)\n# Final_table = {}\nfor table in H_table_lists:\n    for bucket in table.values():\n        # 取每一个数组\n        bucket_length = len(bucket)\n#         for i in range(bucket_length):\n        i = 0\n        # 数组中和0两两配对\n        line_number_a = bucket[i]\n        for j in range(i+1,bucket_length):\n            line_number_b = bucket[j]\n#                 cur_tuple = (line_number_a,line_number_b)\n#                 if cur_tuple not in Final_table.keys():\n#                     Final_table[cur_tuple] = 1\n#                 else:\n#                     Final_table[cur_tuple] += 1\n            cur_score = jaccard_calculation(min_hash_table_reverse,line_number_a,line_number_b)\n            if cur_score > cur_min_tuple[0]:\n                cur_0_tuple = (cur_score,line_number_b)\n                best_score_list.append(cur_0_tuple)\n                cur_store_num += 1\n                if cur_store_num == 31:\n                    best_score_list.sort(key=take_score,reverse=True)\n#                     best_score_list.sort(reverse=True)\n                    best_score_list.pop(30)\n                    cur_min_tuple = best_score_list[29]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"打印出与0号文档相似度前30的结果","metadata":{}},{"cell_type":"code","source":"best_score_list.sort(key=take_score,reverse=True)\nprint(best_score_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}