{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"## Imports (code & data)\nimport re\nimport pandas as pd\nimport yake_helper_funcs as yhf\nfrom datetime import datetime, timedelta\nfrom math import sqrt, floor\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport itertools\nfrom matplotlib import pyplot as plt\nimport removing_polite_posts as rpp\nfrom flashtext.keyword import KeywordProcessor\nimport string\nimport nltk\nimport math\n\nforum_posts = pd.read_csv(\"../input/meta-kaggle/ForumMessages.csv\")\n\n# read in pre-tuned vectors\nvectors = pd.read_csv(\"../input/fine-tuning-word2vec-2-0/kaggle_word2vec.model\", \n                      delim_whitespace=True,\n                      skiprows=[0], \n                      header=None\n                     )\n\n# set words as index rather than first column\nvectors.index = vectors[0]\nvectors.drop(0, axis=1, inplace=True)\nprint(forum_posts.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install yake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"## Utility functions\n\n# get vectors for each word in post\n# TODO: can we vectorize this?\ndef vectors_from_post(post):\n    all_words = [] \n\n    for words in post:\n        all_words.append(words) \n        \n    return(vectors[vectors.index.isin(all_words)])\n\n\n# create document embeddings from post\ndef doc_embed_from_post(post):\n    test_vectors = vectors_from_post(post)\n\n    return(test_vectors.mean())\n\n# explore our posts by cluster\ndef get_keyword_set_by_cluster(number):\n    cluster_index = list(clustering.labels_ == number)\n    return(list(itertools.compress(keyword_sets, cluster_index)))\n\n# get sample post info by #\ndef get_post_info_by_cluster(number, \n                             data,\n                             cluster):\n    return(data[cluster.labels_ == number])\n\n# remove HTML stuff\n# https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\ndef remove_html_tags(text):\n    clean = re.compile('<.*?>')\n    return(re.sub(clean, '', text))\n\n# remove \"good\", \"nice\", \"thanks\", etc\ndef remove_thanks(text):\n    text = text.lower()\n    \n    text = re.sub(\"nice\", \"\", text)\n    text = re.sub(\"thank.*\\s\", \" \", text)\n    text = re.sub(\"good\",\"\", text)\n    text = re.sub(\"hi\", \"\", text)\n    text = re.sub(\"hello\", \"\", text)\n    \n    return(text)\n\ndef polite_post_index(forum_posts):\n    '''Pass in a list of fourm posts, get\n    back the indexes of short, polite ones.'''\n    \n    polite_indexes = []\n    \n    # create  custom stop word list to identify polite forum posts\n    stop_word_list = [\"no problem\", \"thanks\", \"thx\", \"thank\", \"great\",\n                      \"nice\", \"interesting\", \"awesome\", \"perfect\", \n                      \"amazing\", \"well done\", \"good job\"]\n\n    # create a KeywordProcess\n    keyword_processor = KeywordProcessor()\n    keyword_processor.add_keywords_from_list(stop_word_list)\n\n    # test our keyword processor\n    for i,post in enumerate(forum_posts):\n        post = post.lower().translate(str.maketrans({a:None for a in string.punctuation}))\n        \n        if len(post) < 100:\n            keywords_found = keyword_processor.extract_keywords(post.lower(), span_info=True)\n            if keywords_found:\n                polite_indexes.append(i)\n\n    return(polite_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hyperprameters\n\n# number of clusters currently based on the square root of the # of posts\nprint(len(vectors))\ndays_of_posts = 20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install llvmlite --ignore installed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall -y llvmlite\n!pip install llvmlite --ignore-installed\n!pip install numba==0.52.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install top2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing posts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For sample posts, get forum title and topic title\n# based on queries from https://www.kaggle.com/pavlofesenko/strategies-to-earn-discussion-medals\ntopics = pd.read_csv('../input/meta-kaggle//ForumTopics.csv').rename(columns={'Title': 'TopicTitle'})\nforums = pd.read_csv('../input/meta-kaggle/Forums.csv').rename(columns={'Title': 'ForumTitle'})\n\nprint(forum_posts.head())\ndf1 = pd.merge(forum_posts[['ForumTopicId', 'PostDate', 'Message']], topics[['Id', 'ForumId', 'TopicTitle']], left_on='ForumTopicId', right_on='Id')\ndf1 = df1.drop(['ForumTopicId', 'Id'], axis=1)\n\nforum_posts = pd.merge(df1, forums[['Id', 'ForumTitle']], left_on='ForumId', right_on='Id')\nforum_posts = forum_posts.drop(['ForumId', 'Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parse dates\nforum_posts['Date'] = pd.to_datetime(forum_posts.PostDate, format=\"%m/%d/%Y %H:%M:%S\")\n\n# posts from the last X days\nstart_time = datetime.now() + timedelta(days=-days_of_posts)  \n\n# forum posts from last week (remember to convert to str)\nsample_post_info = forum_posts.loc[forum_posts.Date > start_time]\nsample_posts = sample_post_info.Message.astype(str)\n\n# reindex from 0\nsample_posts.reset_index(drop=True)\nsample_post_info.reset_index(drop=True)\n\n# remove html tags\nsample_post_info.Message = sample_post_info.Message\\\n    .astype(str)\\\n    .apply(remove_html_tags)\nsample_posts = sample_posts.apply(remove_html_tags)\n\n# remove polite posts (make sure you remove HTML tags first)\npolite_posts = sample_posts.index[polite_post_index(sample_posts)]\n# posts aren't being dropped \nsample_posts = sample_posts.drop(polite_posts)\nsample_post_info = sample_post_info.drop(polite_posts)\n\n# number of posts\nnum_of_posts = sample_posts.shape[0]\n\n# Number of clusters is square root of the # of posts (rounded down)\nnumber_clusters = floor(sqrt(num_of_posts))\n\nprint(sample_posts.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# extact keywords & tokenize\n#keywords = yhf.keywords_yake(sample_posts, )\nkeywords_tokenized = yhf.tokenizing_after_YAKE(sample_posts)\nkeyword_sets = [set(post) for post in keywords_tokenized]\nprint(\"keywords_tokenized\\n\", keywords_tokenized[0])\nprint(\"keyword_sets\\n\", keyword_sets[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get word vectors for keywords in post"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create empty array for document embeddings\ndoc_embeddings = np.zeros([num_of_posts, 300])\n\n# get document embeddings for posts\nfor i in range(num_of_posts):\n    embeddings = np.array(doc_embed_from_post(keyword_sets[i]))\n    if np.isnan(embeddings).any():\n        doc_embeddings[i,:] = np.zeros([1,300])\n    else:\n        doc_embeddings[i,:] = embeddings\n    if(i==0):\n        print(embeddings.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bert Topic"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pip install awscli","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install responses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install flaky","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pandas==0.23.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install tensorflow==1.12.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install plotly==3.10.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install botocore==1.12.253","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install s3transfer==0.2.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install llvmlite --ignore-installed\n!pip install bertopic[visualization]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(sample_posts))\nprint(sample_posts.shape)\nlistSent = sample_posts.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall -y numba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install numba --ignore-installed\n!pip install umap-learn==0.4.6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nprint(sys.version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bertopic import BERTopic\nmodel = BERTopic()\ntopics, probabilities = model.fit_transform(listSent, doc_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.visualize_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.visualize_distribution(probabilities[153])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.get_topic(119)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top2Vec code"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(sample_posts))\nprint(sample_posts.shape)\nlistSent = sample_posts.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from top2vec import Top2Vec\nmodel1 = Top2Vec(documents=listSent, speed=\"learn\", workers=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.get_num_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"Neural\"], num_topics=5)\nfor topic in topic_nums:\n    model.generate_topic_wordcloud(topic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=35, num_docs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=12, num_docs=5)\nfor doc, score, doc_id in zip(documents, document_scores, document_ids):\n    print(f\"Document: {doc_id}, Score: {score}\")\n    print(\"-----------\")\n    print(doc)\n    print(\"-----------\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering!"},{"metadata":{"trusted":true},"cell_type":"code","source":"clustering = SpectralClustering(n_clusters=number_clusters, \n                                assign_labels=\"discretize\",\n                                n_neighbors=number_clusters).fit_predict(doc_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(doc_embeddings.shape)\nplt.scatter(doc_embeddings[:,0], doc_embeddings[:, 1], c=clustering,\n            s=50, cmap='viridis');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(number_clusters):\n    \n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, \n                                   data = sample_post_info,\n                                   cluster = clustering))\n    print(\"\\n\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(number_clusters):\n    \n#     print(f\"Cluster {i}:\\n\")\n#     print(get_keyword_set_by_cluster(i))\n#     print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clustering using KMeans**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=number_clusters, random_state=0).fit(doc_embeddings)\nfor i in range(number_clusters):\n    \n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, \n                                   data = sample_post_info,\n                                   cluster = kmeans))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"kmeans.labels_\\n\")\npd.Series(kmeans.labels_).value_counts()\nprint(\"sample_post\")\nprint(type(sample_posts))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Elbow methos for optimal K**"},{"metadata":{"trusted":true},"cell_type":"code","source":"distortions = []\nsize = int(number_clusters/2)\nK = range(1, size)\nfor i in K:\n    kmeanModel = KMeans(i, random_state=0).fit(doc_embeddings)\n    distortions.append(kmeanModel.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"after_elbow_size = 4\nkmeans = KMeans(4, random_state=0).fit(doc_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(after_elbow_size):\n    \n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, \n                                   data = sample_post_info,\n                                   cluster = kmeans))\n    print(\"\\n\")\nprint(\"kmeans.labels_\\n\")\npd.Series(kmeans.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testMessage = \"What is deep learning and neural network. Is RNN a higher version of NN? what courses to learn from. Support vector machine are a part of neural nets, and LSTM comes under RNN\"\n#testMessage = \"Neural network courses and competitions. Can i use LSTM for sentiment analysis\"\ntest_sample = [testMessage]\nkeywords_tokenized = yhf.tokenizing_after_YAKE(test_sample)\nkeyword_sets = [set(post) for post in keywords_tokenized]\nembeddings = np.array(doc_embed_from_post(keyword_sets[0]))\nprint(\"embeddings\\n\")\nprint(embeddings.shape)\nembeddingsT = embeddings.transpose();\nprint(\"trans\")\nprint(embeddingsT.shape)\nif np.isnan(embeddings).any():\n    doc_embeddings1 = np.zeros([1,300])\nelse:\n    doc_embeddings1 = embeddings\nnew_embeddings = embeddings.reshape(1, -1)\nclust = kmeans.predict(new_embeddings)\nprint(clust)\nprint(get_post_info_by_cluster(clust[0], \n                                   data = sample_post_info,\n                                   cluster = kmeans))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TEST PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_blobs\n# create blobs\ndata = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)\n# create np array for data points\npoints = data[0]\n# create scatter plot\nplt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')\nplt.xlim(-15,15)\nplt.ylim(-15,15)\n\nX = data[0]\nX[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"trying SK learn for k means"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nn_clusters = 5\nsklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(doc_embeddings)\nkmeans = KMeans(n_clusters= n_clusters, max_iter=600, algorithm = 'auto')\n%time fitted = kmeans.fit(Y_sklearn)\nprediction = kmeans.predict(Y_sklearn)\n\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=50, cmap='viridis')\n\ncenters2 = fitted.cluster_centers_\nplt.scatter(centers2[:, 0], centers2[:, 1],c='black', s=300, alpha=0.6);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"trying PCA for spectral clustering? will it work??"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nn_clusters = 5\nsklearn_pca = PCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform()\n\nspectralClusters = SpectralClustering(n_clusters=number_clusters, \n                                assign_labels=\"discretize\",\n                                n_neighbors=number_clusters)\nprediction = spectralClusters.fit_predict(Y_sklearn)\n\nplt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=50, cmap='viridis')\n\n#centers2 = fitted.cluster_centers_\n#plt.scatter(centers2[:, 0], centers2[:, 1],c='black', s=300, alpha=0.6);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"trying just k means for few sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n  \nfrom nltk.cluster import KMeansClusterer\nimport nltk\nimport numpy as np \n  \nfrom sklearn import cluster\nfrom sklearn import metrics\n  \n# training data\n  \nsentences = [['this', 'is', 'the', 'one','good', 'machine', 'learning', 'book'],\n            ['this', 'is',  'another', 'book'],\n            ['one', 'more', 'book'],\n            ['weather', 'rain', 'snow'],\n            ['yesterday', 'weather', 'snow'],\n            ['forecast', 'tomorrow', 'rain', 'snow'],\n            ['this', 'is', 'the', 'new', 'post'],\n            ['this', 'is', 'about', 'more', 'machine', 'learning', 'post'],  \n            ['and', 'this', 'is', 'the', 'one', 'last', 'post', 'book']]\n  \nsentences = sample_posts.tolist()    \n \nmodel = Word2Vec(sentences, min_count=1)\n \n  \ndef sent_vectorizer(sent, model):\n    sent_vec =[]\n    numw = 0\n    for w in sent:\n        try:\n            if numw == 0:\n                sent_vec = model[w]\n            else:\n                sent_vec = np.add(sent_vec, model[w])\n            numw+=1\n        except:\n            pass\n     \n    return np.asarray(sent_vec) / numw\n  \n  \nX=[]\nfor sentence in sentences:\n    X.append(sent_vectorizer(sentence, model))   \n \nprint (\"========================\")\nprint (X)\n  \n \n# note with some version you would need use this (without wv) \n#  model[model.vocab] \nprint (model[model.wv.vocab])\n \n \n  \n \nprint (model.similarity('post', 'book'))\nprint (model.most_similar(positive=['machine'], negative=[], topn=2))\n  \n  \n \n  \n  \nNUM_CLUSTERS=5\nkclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\nassigned_clusters = kclusterer.cluster(X, assign_clusters=True)\nprint (assigned_clusters)\n  \n  \n  \nfor index, sentence in enumerate(sentences):    \n    print (str(assigned_clusters[index]) + \":\" + str(sentence))\n \n     \n     \n     \nkmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\nkmeans.fit(X)\n  \nlabels = kmeans.labels_\ncentroids = kmeans.cluster_centers_\n  \nprint (\"Cluster id labels for inputted data\")\nprint (labels)\nprint (\"Centroids data\")\nprint (centroids)\n  \nprint (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\nprint (kmeans.score(X))\n  \nsilhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n  \nprint (\"Silhouette_score: \")\nprint (silhouette_score)\n \n \nimport matplotlib.pyplot as plt\n \nfrom sklearn.manifold import TSNE\n \nmodel = TSNE(n_components=2, random_state=0)\nnp.set_printoptions(suppress=True)\n \nY=model.fit_transform(X)\n \n \nplt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n \n \nfor j in range(len(sentences)):    \n   plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n   print (\"%s %s\" % (assigned_clusters[j],  sentences[j]))\n \n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(Y[:, 0], Y[:, 1], c=assigned_clusters, s=290,alpha=.5)\n \n \nfor j in range(len(sample_posts)):    \n   plt.annotate(assigned_clusters[j],xy=(Y[j][0], Y[j][1]),xytext=(0,0),textcoords='offset points')\n   print (\"%s %s\" % (assigned_clusters[j],  sample_posts[j]))\n \n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Refining clustering\n\nSteps:\n\n1. Drop empty clusters\n2. Identify large clusters (2 times more than expected)\n3. Recluster those clusters (# clusters = sqrt # posts)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count of posts/cluster\ncluster_counts = pd.Series(clustering.labels_).value_counts()\n\n# get clusters bigger than expected\nmax_cluster_size = number_clusters * 2\nbig_clusters = cluster_counts[cluster_counts > max_cluster_size]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub-cluster first (biggest) cluster\ncluster_label = big_clusters.index[0]\n\nsub_sample = sample_post_info[clustering.labels_ == cluster_label]\nsub_cluster_embeddings = doc_embeddings[clustering.labels_ == cluster_label]\n\nnumber_sub_clusters = floor(sqrt(sub_sample.shape[0]))\n\nsub_cluster = SpectralClustering(n_clusters=number_sub_clusters, \n                                 assign_labels=\"discretize\", \n                                 n_neighbors=number_sub_clusters).fit(sub_cluster_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see how it looks\nfor i in range(number_sub_clusters):\n\n    print(f\"Cluster {i}:\\n\")\n    print(get_post_info_by_cluster(i, data = sub_sample, \n                                   cluster = sub_cluster))\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(sub_cluster.labels_).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word clouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import path\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO why do I see thank you?\nposts_as_string = sample_post_info\\\n    .Message\\\n    .to_string(index=False)\n\n# shouldn't have to do this b/c I removed polite posts earlier\nposts_as_string = remove_thanks(posts_as_string)\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(posts_as_string)\n\n# Display the generated image:\n# the matplotlib way:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Going forward\n\nBiggest problem: redundent clusters\n\nPossible solutions: \n\n* Remove very short posts\n* Don't include posts on kernels\n* Build filter for removing short \"thanks!\" type posts\n* Start w/ sentiment analys & put all very high sentiment posts in a single bin"},{"metadata":{},"cell_type":"markdown","source":"# Visualization brain storming\n\nSlides on text visualizatoin: https://courses.cs.washington.edu/courses/cse512/15sp/lectures/CSE512-Text.pdf\n\n* Bigram based method, reporting the two terms with the median freuquency\n* term saliency, normalize by freq of most common term log(tf_w) / log(tf_the) (and then some sort of regression?)\n* Termite-based model: Topics as columns, terms as rows and weight visualiation of term distinctivenes as KL divergence p(T|term)/p(T|any_term)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# next week: get saliency measure by day, \n# look at shift between sliency on day & in corpus as whole pick summary words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: \n# make sure to match preprocessing (lower cased)\n# for each cluster, find the nomralized saliencey measure \n# rank words based on difference in normalizd saliency in whole corpus\n\n# edge cases:\n# OOV words, add smoothing or set corpus freq. to 0\n# ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"frequency_table = pd.read_csv(\"../input/kaggle-forum-term-frequency-unstemmed/kaggle_lex_freq.csv\",\n                             error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cluster_saliency_dict(cluster_number):\n    # create corpus from a cluster\n    text = get_post_info_by_cluster(cluster_number, data = sub_sample, cluster = sub_cluster)\\\n        .Message.astype(str).str.cat(sep=' ')\n\n    # tokenize\n    words = nltk.word_tokenize(text)\n\n    # Remove single-character tokens (mostly punctuation)\n    words = [word for word in words if len(word) > 1]\n\n    # Remove numbers\n    words = [word for word in words if not word.isnumeric()]\n\n    # remove non-breaking space\n    words = [word for word in words if word != \"nbsp\"]\n\n    # Lowercase all words (default_stopwords are lowercase too)\n    words = [word.lower() for word in words]\n\n    # Calculate frequency distribution\n    fdist = nltk.FreqDist(words)\n\n    cluster_dict = dict() \n\n    # get saliency measures\n    for word, frequency in fdist.most_common():\n        saliency_measure_smoothed = math.log(frequency + 0.0001)/(math.log(fdist.most_common(1)[0][1] + 0.0001))\n        cluster_dict[word] = saliency_measure_smoothed\n        \n    return(cluster_dict, fdist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_surprising_words(cluster_number, frequency_table):\n    cluster_dict, fdist = get_cluster_saliency_dict(cluster_number)\n    \n    words = []\n    surprisal = []\n\n    for word, freq in fdist.most_common():\n        words.append(word)\n        surprisal_measure = cluster_dict[word] - frequency_table.saliency[frequency_table.word == word]\n        if surprisal_measure.empty:\n            surprisal.append(cluster_dict[word] - .0001)\n        else:\n            surprisal.append(surprisal_measure.values[0])\n\n    cluster_surprisal_measures = pd.DataFrame(list(zip(words, surprisal)), \n                                              columns =['Words', 'Surprisal']) \n\n    suprising_words = cluster_surprisal_measures.Words[cluster_surprisal_measures.Surprisal > 0]\n    \n    return(suprising_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_surprising_words(1, frequency_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_post_info_by_cluster(1, data = sub_sample, cluster = sub_cluster).Message","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_surprising_words(0, frequency_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_post_info_by_cluster(0, data = sub_sample, cluster = sub_cluster).Message","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}