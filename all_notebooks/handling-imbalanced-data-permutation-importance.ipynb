{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nfrom sklearn.model_selection import StratifiedKFold , KFold\nimport math\nimport shap\nimport joblib\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade git+https://github.com/stanfordmlgroup/ngboost.git\nfrom ngboost import NGBRegressor, NGBClassifier\nfrom ngboost.ngboost import NGBoost\nfrom ngboost.learners import default_tree_learner\nfrom ngboost.scores import CRPS, MLE , LogScore\nfrom ngboost.distns import LogNormal, Normal\nfrom ngboost.distns import k_categorical, Bernoulli\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data fork from previous EDA kernel \nhttps://www.kaggle.com/possiblemanjr/handling-imbalanced-data-eda-small-fe\n\n#### trained model from following kernels\n\nhttps://www.kaggle.com/possiblemanjr/handling-imbalanced-data-supervised-learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## import & functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_pickle(\"../input/handling-imbalanced-data-eda-small-fe/df_for_use.pkl\")\ndf_fe = pd.read_pickle(\"../input/handling-imbalanced-data-eda-small-fe/df_fe.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_clf = joblib.load('../input/handling-imbalanced-data-supervised-learning/lgbm_clf.pkl')\nrf_clf = joblib.load('../input/handling-imbalanced-data-supervised-learning/rf_clf.pkl')\nxgb_clf = joblib.load('../input/handling-imbalanced-data-supervised-learning/xgb_clf.pkl')\nngb_clf = joblib.load('../input/handling-imbalanced-data-supervised-learning/ngb_clf.pkl')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_cpu_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\nrf_roc_score = roc_auc_score(y_test, rf_clf.predict_proba(X_test)[:,1], average = 'macro')\nxgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average = 'macro')\nNGb_roc_score = roc_auc_score(y_test, ngb_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nprint( 'RandomForest_ROC_AUC : {0:.4f}'.format(rf_roc_score))\nprint( 'Lightgbm_ROC_AUC : {0:.4f}'.format(lgbm_cpu_roc_score))\nprint( 'Xgboost_ROC_AUC : {0:.4f}'.format(xgb_roc_score))\nprint( 'Ngboost_ROC_AUC : {0:.4f}'.format(NGb_roc_score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}'.format(accuracy , precision, recall, f1))\n    print('------------------------------------------------------------------------------')\n    \n\nthresholds = {0.1,0.15, 0.2,0.25, 0.3,0.35, 0.4 , 0.45 , 0.5}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Random Forest\n\nftr_ims_values = rf_clf.feature_importances_\nftr_ims = pd.Series(ftr_ims_values, index = X_train.columns)\nftr_top20 = ftr_ims.sort_values(ascending = False)[:20]\n\nplt.figure(figsize = (10,8))\nplt.title('Feature importance')\nsns.barplot(x = ftr_top20, y = ftr_top20.index)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### XGboost\n\nfrom xgboost import plot_importance\nfig, ax = plt.subplots(1,1, figsize= (10,8))\nplot_importance(xgb_clf, ax= ax, max_num_features = 20 , height = 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### LightGBM\n\nfrom lightgbm import plot_importance\nfig, ax = plt.subplots(1,1, figsize= (10,8))\nplot_importance(lgbm_clf, ax= ax, max_num_features = 20 , height = 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### NGboost\n\n\n## Feature importance for loc trees\nfeature_importance_loc = ngb_clf.feature_importances_[0]\n\n# ## Feature importance for scale trees\n# feature_importance_scale = ngb_clf.feature_importances_[1]\n\ndf_loc = pd.DataFrame({'feature': X_train.columns, \n                       'importance':feature_importance_loc})\\\n    .sort_values('importance',ascending=False)\n# df_scale = pd.DataFrame({'feature':X_train.columns, \n#                        'importance':feature_importance_scale})\\\n#     .sort_values('importance',ascending=False)\n\nfig, ax1 = plt.subplots(1,1, figsize=(10,8))\nfig.suptitle(\"Feature importance plot for distribution parameters\", fontsize=17)\nsns.barplot(x='importance',y='feature',ax=ax1,data=df_loc, color=\"skyblue\").set_title('loc param')\n# sns.barplot(x='importance',y='feature',ax=ax2,data=df_scale, color=\"skyblue\").set_title('scale param')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation Importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Apply to LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"perm_lgbm = PermutationImportance(lgbm_clf, random_state=2020).fit(X_test, y_test)\neli5.show_weights(perm_lgbm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pi_features = eli5.explain_weights_df(perm_lgbm, feature_names = X_train.columns.tolist())\npi_features = pi_features.loc[pi_features['weight'] >= 0.005]['feature'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pi = X[pi_features]\nX_train, X_test, y_train, y_test = train_test_split(X_pi, y, test_size = 0.2 , random_state = 2020, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nlgbm_clf = LGBMClassifier(n_estimators = 3000, random_state = 2020)\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = 'auc' , eval_set = evals, verbose = 50)\n\n\nlgbm_cpu_runtime = time.time() - start\n\nget_eval_by_threshold(y_test, lgbm_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\nlgbm_cpu_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nprint( 'LightGBM_cpu_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(lgbm_cpu_roc_score ,lgbm_cpu_runtime ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM with Stratified 5Fold","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nfrom time import time\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'auc'\n           }\n\nk_fold=5\nkf=StratifiedKFold(n_splits=k_fold,shuffle=True, random_state=2020)\ntraining_start_time = time()\naucs=[]\ny_preds = np.zeros(X_test.shape[0])\n\nfor fold, (trn_idx,val_idx) in enumerate(kf.split(X_train,y_train)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n    clf = lgb.train(params_lgb, trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds=200)\n    aucs.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    y_preds += clf.predict(X_test) / 5\n    \n    \n    \nprint('-' * 30)\nprint('Training is completed!.')\nprint(\"\\n## Mean CV_AUC_Score : \", np.mean(aucs))\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print(clf.best_params_)\nprint('-' * 30)\n\n\n# pred_rf = clf.predict(X_test)\nauc = roc_auc_score(y_test,y_preds)\nprint(' ROC_AUC_Score : {0:.4f}'.format (auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### AUC got improved after applying permutation importance\n#### Also more improved with CV stratified 5 folds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}