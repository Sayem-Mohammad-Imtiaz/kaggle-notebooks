{"cells":[{"metadata":{},"cell_type":"markdown","source":"Um sistema de classificação de tópicos consiste em um modelo capaz de identificar a qual domínio uma determinada sentença ou texto pertence. Crie um classificador que receba uma frase em inglês e indique se ela faz parte de uma das seguintes categorias: aventura, ficção científica, religião ou governo, utilizando o corpus em anexo.\n\nUse as etapas de pré-processamento que vimos em sala e representações de texto diferentes se desejar (bag-of-words/LSA, Skip-gram, CBOW, GloVe). Utilize os algoritmos de classificação que desejar, e faça seleção de hiperparâmetros. Siga sempre as boas práticas para experimentos de aprendizado de máquina para evitar underfitting e overfitting.\n\nO grupo que obtiver o melhor resultado no corpus de teste (de posse somente do professor) receberá 1 ponto a mais na média de disciplina. Se mais de um grupo tiver o mesmo melhor resultado, nenhum grupo ganhará essa pontuação.\n\nA entrega deve ser feita por meio de script no kaggle compartilhado com o usuário do professor. Basta copiar o link na submissão da solução."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n\n# Any results you write to the current directory are saved as output.\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Demais imports necessários para o projeto**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nfrom nltk.corpus import wordnet\n\n\n\n\nfrom sklearn import neighbors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import neighbors\n\nfrom sklearn.model_selection import RandomizedSearchCV\nimport scipy\n\n\nfrom sklearn.metrics import *\nfrom sklearn.decomposition import TruncatedSVD\nfrom gensim.models import Word2Vec\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/corpus_categorias_treino.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = df['category'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l = df['category'].value_counts()\nl \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['words'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nstopwords_list = stopwords.words('english')\n\nlemmatizer = WordNetLemmatizer()\n\ndef my_tokenizer(doc):\n    \n    words = word_tokenize(doc)\n    \n    pos_tags = pos_tag(words)\n    \n    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n    \n    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n    \n    lemmas = []\n    for w in non_punctuation:\n        if w[1].startswith('J'):\n            pos = wordnet.ADJ\n        elif w[1].startswith('V'):\n            pos = wordnet.VERB\n        elif w[1].startswith('N'):\n            pos = wordnet.NOUN\n        elif w[1].startswith('R'):\n            pos = wordnet.ADV\n        else:\n            pos = wordnet.NOUN\n        \n        lemmas.append(lemmatizer.lemmatize(w[0].lower(), pos))\n\n    return lemmas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = []\nfor i in df['words']:\n    t.append(my_tokenizer(i))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#t\nprint(my_tokenizer(df['words'][0]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#frase_processada.append(' '.join(nova_frase))\nfrase_processada = list()\nfor i in t:\n    frase_processada.append(' '.join(i))\ndf['words_tratada'] = frase_processada\ndf.category.replace(['adventure', 'government','religion','science_fiction'], [0,1,2,3], inplace=True)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frase_processada[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvetorizar = CountVectorizer(lowercase=False, max_features=50)\nbag_of_words = vetorizar.fit_transform(df[\"words\"])\n\n\ntreino, teste , classe_treino, classe_teste = train_test_split(bag_of_words, df[\"category\"], random_state = 42)\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\n\n\nclf.fit(treino, classe_treino)\nprevisao_teste  = clf.predict(teste)\nacuracia = clf.score(teste, classe_teste)\nprint(acuracia)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#https://www.kaggle.com/sermakarevich/sklearn-pipelines-tutorial\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\nmy_pipeline = Pipeline([('clf', clf)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npar = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n\n#cross validation = cv\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)#teste, classe_teste","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nfor param_name in sorted(par.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ny_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Utilizando o tfidf e verificando a melhora**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#aplicando LSA ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n\n#tfs = tfidf_vectorizer.fit_transform(df[\"words_tratada\"])\n\n#svd_transformer = TruncatedSVD(n_components=1000)\n\n#svd_transformer.fit(tfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cummulative_variance = 0.0\n#k = 0\n#for var in sorted(svd_transformer.explained_variance_ratio_)[::-1]:\n#    cummulative_variance += var\n#    if cummulative_variance >= 0.5:\n#        break\n#    else:\n#        k += 1\n        \n#print(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#svd_transformer = TruncatedSVD(n_components=k)\n#svd_transformer.fit(tfs)\n#svd_data = svd_transformer.transform(tfs)\n#print(sorted(svd_transformer.explained_variance_ratio_)[::-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#treino, teste , classe_treino, classe_teste = train_test_split( svd_data, df[\"category\"], random_state = 42)\n#clf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\n#clf.fit(treino, classe_treino)\n#previsao_teste  = clf.predict(teste)\n#acuracia = clf.score(teste, classe_teste)\n#print(acuracia)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n#clf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\n#my_pipeline = Pipeline([('clf', clf)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n#par = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n\n#cross validation = cv\n#hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#hyperpar_selector.fit(X=treino, y=classe_treino)#teste, classe_teste","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\n#print(\"Best parameters set:\")\n#best_parameters = hyperpar_selector.best_estimator_.get_params()\n#for param_name in sorted(par.keys()):\n#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n#y_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\n#print(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LSA no pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass SVDDimSelect(object):\n    def fit(self, X, y=None):        \n        try:\n            self.svd_transformer = TruncatedSVD(n_components=round(X.shape[1]/2))\n            self.svd_transformer.fit(X)\n        \n            cummulative_variance = 0.0\n            k = 0\n            for var in sorted(self.svd_transformer.explained_variance_ratio_)[::-1]:\n                cummulative_variance += var\n                if cummulative_variance >= 0.5:\n                    break\n                else:\n                    k += 1\n                \n            self.svd_transformer = TruncatedSVD(n_components=k)\n        except Exception as ex:\n            print(ex)\n            \n        return self.svd_transformer.fit(X)\n    \n    def transform(self, X, Y=None):\n        return self.svd_transformer.transform(X)\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\nmy_pipeline = Pipeline([('tfidf', TfidfVectorizer(tokenizer=my_tokenizer)),\\\n                       ('svd', SVDDimSelect()), \\\n                       ('clf', clf)])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\npar = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n\n#cross validation = cv\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#treino, teste , classe_treino, classe_teste = train_test_split(df['words'], df['category'], random_state = 42)\ntreino, teste , classe_treino, classe_teste = train_test_split(df[\"words\"], df['category'], random_state = 42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)#teste, classe_teste","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nfor param_name in sorted(par.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\ny_pred = hyperpar_selector.predict(teste)\n#treino, teste , classe_treino, classe_teste\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"##Skip gram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n\n\nclass Word2VecTransformer(object):\n    \n    ALGO_SKIP_GRAM=1\n    ALGO_CBOW=2    \n    \n    def __init__(self, algo=1):    \n        self.algo = algo\n    \n    def fit(self, X, y=None):     \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        self.word2vec = Word2Vec(X, min_count=2, sg=self.algo)\n        \n        # Pegamos a dimensão da primeira palavra, para saber quantas dimensões estamos trabalhando,\n        # assim podemos ajustar nos casos em que aparecerem palavras que não existirem no vocabulário.\n        first_word = next(iter(self.word2vec.wv.vocab.keys()))\n        self.num_dim = len(self.word2vec[first_word])       \n        \n        return self\n    \n    def transform(self, X, Y=None):        \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        return np.array([np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.num_dim)], axis=0) \n                         for words in X])\n\n        \n    def get_params(self, deep=True):\n        return {}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frase_processada = list()\nfor i in t:\n    frase_processada.append(' '.join(i))\ndf['words_tratada2'] = frase_processada","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino, teste , classe_treino, classe_teste = train_test_split( df[\"words\"], df[\"category\"], random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\nmy_pipeline = Pipeline([('w2vt', Word2VecTransformer()), ('clf', clf)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"par = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n\n#cross validation = cv\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nfor param_name in sorted(par.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import *\n\ny_pred = hyperpar_selector.predict(teste)\n\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#cbow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n\n\nclass Word2VecTransformer(object):\n    \n    ALGO_SKIP_GRAM=1\n    ALGO_CBOW=2    \n    \n    def __init__(self, algo=2):    \n        self.algo = algo\n    \n    def fit(self, X, y=None):     \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        self.word2vec = Word2Vec(X, min_count=2, sg=self.algo)\n        \n        # Pegamos a dimensão da primeira palavra, para saber quantas dimensões estamos trabalhando,\n        # assim podemos ajustar nos casos em que aparecerem palavras que não existirem no vocabulário.\n        first_word = next(iter(self.word2vec.wv.vocab.keys()))\n        self.num_dim = len(self.word2vec[first_word])       \n        \n        return self\n    \n    def transform(self, X, Y=None):        \n        X = [nltk.word_tokenize(x) for x in X]\n        \n        return np.array([np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.num_dim)], axis=0) \n                         for words in X])\n\n        \n    def get_params(self, deep=True):\n        return {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\nmy_pipeline = Pipeline([('w2vt', Word2VecTransformer()), ('clf', clf)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npar = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n\n#cross validation = cv\nhyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperpar_selector.fit(X=treino, y=classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = hyperpar_selector.best_estimator_.get_params()\nfor param_name in sorted(par.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ny_pred = hyperpar_selector.predict(teste)\n\nprint(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#glove\n#from gensim.scripts.glove2word2vec import glove2word2vec\n#from glove import Corpus, Glove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#class GloveTransformer(object):\n#    \n#    def __init__(self, algo=2):\n#        self.algo = algo\n#    \n#    def fit(self, X, y=None):     \n#        X = [nltk.word_tokenize(x) for x in X]\n#        \n#        self.corpus = Corpus()\n#        self.corpus.fit(X, window=10)\n#        \n#        self.num_dim = 100       \n#        self.glove = Glove(no_components=self.num_dim, learning_rate=0.05)\n#        \n#        self.glove.fit(corpus.matrix, epochs=30, no_threads=1, verbose=True)\n#        self.glove.add_dictionary(self.corpus.dictionary)\n#        \n#        return self\n#    \n#    def transform(self, X, Y=None):        \n#        X = [nltk.word_tokenize(x) for x in X]\n#        \n#        return np.array([np.mean([self.glove.word_vectors[self.glove.dictionary[w]] for w in words if w in self.corpus.dictionary.keys()] or [np.zeros(self.num_dim)],axis=0) \n#                         for words in X])\n#\n#        \n#    def get_params(self, deep=True):\n#        return {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf = neighbors.KNeighborsClassifier(n_neighbors=10, weights='uniform')\n\n#my_pipeline = Pipeline([('gt', GloveTransformer()),('clf', clf)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#par = {'clf__n_neighbors': range(1, 60), 'clf__weights': ['uniform', 'distance']}\n\n#hyperpar_selector = RandomizedSearchCV(my_pipeline, par, cv=3, scoring='accuracy', n_jobs=1, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hyperpar_selector.fit(X=treino, y=classe_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Best score: %0.3f\" % hyperpar_selector.best_score_)\n#print(\"Best parameters set:\")\n#best_parameters = hyperpar_selector.best_estimator_.get_params()\n#for param_name in sorted(par.keys()):\n#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = hyperpar_selector.predict(teste)\n\n#print(accuracy_score(classe_teste, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}