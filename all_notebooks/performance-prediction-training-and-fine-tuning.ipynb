{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Students Performance in Exams\n","metadata":{}},{"cell_type":"markdown","source":"This notebook mainly describes the process of training models and fine-tuning models.","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/students-performance-in-exams/StudentsPerformance.csv')\ndata","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Dataset","metadata":{}},{"cell_type":"markdown","source":"Split the dataset to two parts - training set and testing set, the training set is used for training and cross validating, the testing set is used for the final testing.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nsplit = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n\nfor train_index,test_index in split.split(data,data[\"race/ethnicity\"]):\n    train_set = data.loc[train_index]\n    test_set = data.loc[test_index]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Data","metadata":{}},{"cell_type":"markdown","source":"Try to find how does each feature impact the scores.","metadata":{}},{"cell_type":"markdown","source":"Distribution of feature values.","metadata":{}},{"cell_type":"code","source":"def add_attr_trace(fig, dataset, attr, r, c):\n    fig.add_trace(go.Histogram(x=dataset[attr].sort_values(), histnorm='probability', name=attr), row=r, col=c)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=5, shared_yaxes=True,\n                    subplot_titles=(\"entire data\",\"entire data\", \"entire data\", \"entire data\", \"entire data\",\n                                   \"testing data\",\"testing data\", \"testing data\", \"testing data\", \"testing data\"))\n\nattrs = data.columns[:5]\n\nfor i in range(len(attrs)):\n    add_attr_trace(fig, data, attrs[i], 1, i + 1)\n    add_attr_trace(fig, test_set, attrs[i], 2, i + 1)\n\nfig.update_layout(showlegend=False, height=900)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Impaction of feature values on scores.","metadata":{}},{"cell_type":"code","source":"def add_score_trace(fig, dataset, impact_attr, attr_value, score_type, r, c):\n    fig.add_trace(\n        go.Histogram(x=dataset[dataset[impact_attr] == attr_value][score_type],\n                     histnorm='probability',\n                     name=attr_value + ' - ' + score_type),\n        row=r, col=c)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=5, cols=3, shared_yaxes=True,\n                    subplot_titles=(\"gender - math\", \"gender - reading\", \"gender - writing\",\n                                    \"race/ethnicity - math\", \"race/ethnicity - reading\", \"race/ethnicity - writing\", \n                                    \"parental level - math\", \"parental level - reading\", \"parental level - writing\",\n                                    \"lunch - math\", \"lunch - reading\", \"lunch - writing\",\n                                    \"test preparation - math\", \"test preparation - reading\", \"test preparation - writing\"))\n\nscore_types = data.columns[5:8]\n\nfor i in range(len(attrs)):\n    attr_values = data[attrs[i]].unique()\n    for j in range(len(attr_values)):\n        for k in range(len(score_types)):\n            add_score_trace(fig, data, attrs[i], attr_values[j], score_types[k], i + 1, k + 1)\n            \nfig.update_layout(showlegend=False, barmode='stack', height=1500)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data","metadata":{}},{"cell_type":"markdown","source":"Transform the category data to numerics.","metadata":{}},{"cell_type":"code","source":"X_train = train_set.drop(['math score', 'reading score', 'writing score'], axis=1)\ny_train_math = train_set['math score'].copy()\ny_train_reading = train_set['reading score'].copy()\ny_train_writing = train_set['writing score'].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncategory_attrs = attrs\n\nfull_pipeline = ColumnTransformer([('category', OneHotEncoder(), category_attrs)])\n\nX_train = full_pipeline.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Models","metadata":{}},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training linear regression models.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_regr_math = LinearRegression()\nlin_regr_math.fit(X_train, y_train_math)\n\nlin_regr_reading = LinearRegression()\nlin_regr_reading.fit(X_train, y_train_reading)\n\nlin_regr_writing = LinearRegression()\nlin_regr_writing.fit(X_train, y_train_writing)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree Regressor","metadata":{}},{"cell_type":"markdown","source":"Training decision tree regression models.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_regr_math = DecisionTreeRegressor(random_state=42)\ntree_regr_math.fit(X_train, y_train_math)\n\ntree_regr_reading = DecisionTreeRegressor(random_state=42)\ntree_regr_reading.fit(X_train, y_train_reading)\n\ntree_regr_writing = DecisionTreeRegressor(random_state=42)\ntree_regr_writing.fit(X_train, y_train_writing)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot tree\n# from sklearn import tree\n# tree.plot_tree(tree_regr_math)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# export tree\n# import graphviz \n# dot_data = tree.export_graphviz(tree_regr_math, out_file=None) \n# graph = graphviz.Source(dot_data)\n# graph.render(\"math\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Regressor","metadata":{}},{"cell_type":"markdown","source":"Training random forest regression models.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_regr_math = RandomForestRegressor(random_state=42)\nforest_regr_math.fit(X_train, y_train_math)\n\nforest_regr_reading = RandomForestRegressor(random_state=42)\nforest_regr_reading.fit(X_train, y_train_math)\n\nforest_regr_writing = RandomForestRegressor(random_state=42)\nforest_regr_writing.fit(X_train, y_train_math)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Training Data","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef predict(model, X, y, tag):\n    predictions = model.predict(X)\n    mse = mean_squared_error(y, predictions)\n    rmse = np.sqrt(mse)\n    print('prediction for ' + tag + ': rmse = ', rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Linear models","metadata":{}},{"cell_type":"code","source":"print('Linear Regression ----------------')\npredict(lin_regr_math, X_train, y_train_math, 'math score')\npredict(lin_regr_reading, X_train, y_train_reading, 'reading score')\npredict(lin_regr_writing, X_train, y_train_writing, 'writing score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision tree models","metadata":{}},{"cell_type":"code","source":"print('Decision Tree Regressor  ----------------')\npredict(tree_regr_math, X_train, y_train_math, 'math score')\npredict(tree_regr_reading, X_train, y_train_reading, 'reading score')\npredict(tree_regr_writing, X_train, y_train_writing, 'writing score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest models","metadata":{}},{"cell_type":"code","source":"print('Random Forest Regressor ----------------')\npredict(forest_regr_math, X_train, y_train_math, 'math score')\npredict(forest_regr_reading, X_train, y_train_reading, 'reading score')\npredict(forest_regr_writing, X_train, y_train_writing, 'writing score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross validate","metadata":{}},{"cell_type":"markdown","source":"Cross validation is used to test a model's ability to predict new data that was not used in training it.","metadata":{}},{"cell_type":"code","source":"def display_scores(scores):\n    print('Scores:', scores)\n    print('Mean:', scores.mean())\n    print('Standard deviation:', scores.std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_cross_validation(estimator, X, y, tag):\n    scores = cross_val_score(estimator, X, y, scoring='neg_mean_squared_error', cv=10)\n    rmse_scores = np.sqrt(-scores)\n    print()\n    print('********** ' + tag + ' **********')\n    display_scores(rmse_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nprint()\nprint('cross validation for linear regressions -----------------------------')\napply_cross_validation(lin_regr_math, X_train, y_train_math, 'math score')\napply_cross_validation(lin_regr_reading, X_train, y_train_reading, 'reading score')\napply_cross_validation(lin_regr_writing, X_train, y_train_writing, 'writing score')\n\nprint()\nprint('cross validation for decision tree regressors -----------------------------')\napply_cross_validation(tree_regr_math, X_train, y_train_math, 'math score')\napply_cross_validation(tree_regr_reading, X_train, y_train_reading, 'reading score')\napply_cross_validation(tree_regr_writing, X_train, y_train_writing, 'writing score')\n\nprint()\nprint('cross validation for random forest regressors -----------------------------')\napply_cross_validation(forest_regr_math, X_train, y_train_math, 'math score')\napply_cross_validation(forest_regr_reading, X_train, y_train_reading, 'reading score')\napply_cross_validation(forest_regr_writing, X_train, y_train_writing, 'writing score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Turning","metadata":{}},{"cell_type":"markdown","source":"Get better models by using Grid Search CV method. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef grid_search_cv(estimator, param_grid, X, y, tag):\n    print()\n    print(tag + ' ----------------------------------')\n    \n    grid_search = GridSearchCV(estimator, param_grid, verbose=1, cv=10,\n                              scoring='neg_mean_squared_error',\n                              return_train_score=True, refit=True)\n\n    grid_search.fit(X, y)\n    print()\n    print('best_params_: ')\n    print(grid_search.best_params_)\n    \n    results = grid_search.cv_results_\n    for mean_score, params in zip(results['mean_test_score'], results['params']):\n        print(np.sqrt(-mean_score), params)\n        \n    feature_importances = grid_search.best_estimator_.feature_importances_\n    print()\n    print('feature_importances: ')\n    print(feature_importances)\n    \n    return grid_search","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Grid search for decision tree regressors -------------------------------')\nparam_grid = {'max_depth': list(range(2, 10)), 'min_samples_split': [2, 3, 4, 5, 6]}\ngrid_search_tree_math = grid_search_cv(DecisionTreeRegressor(random_state=42), param_grid, X_train, y_train_math, 'math score')\ngrid_search_tree_reading = grid_search_cv(DecisionTreeRegressor(random_state=42), param_grid, X_train, y_train_reading, 'reading score')\ngrid_search_tree_writing = grid_search_cv(DecisionTreeRegressor(random_state=42), param_grid, X_train, y_train_writing, 'writing score')\n\n\nprint()\nprint('Grid search for random forest regressors -------------------------------')\nparam_grid = {'max_depth': list(range(2, 10)), 'min_samples_split': [2, 3, 4, 5, 6]}\ngrid_search_forest_math = grid_search_cv(RandomForestRegressor(random_state=42), param_grid, X_train, y_train_math, 'math score')\ngrid_search_forest_reading = grid_search_cv(RandomForestRegressor(random_state=42), param_grid, X_train, y_train_reading, 'reading score')\ngrid_search_forest_writing = grid_search_cv(RandomForestRegressor(random_state=42), param_grid, X_train, y_train_writing, 'writing score')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Testing","metadata":{}},{"cell_type":"code","source":"def final_predict(grid_search, X, y, tag):\n    predictions = grid_search.best_estimator_.predict(X)\n    mse = mean_squared_error(y, predictions)\n    rmse = np.sqrt(mse)\n    print('final predict for ' + tag + ': rmse = ', rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test_set.drop(['math score', 'reading score', 'writing score'], axis=1)\ny_test_math = test_set['math score'].copy()\ny_test_reading = test_set['reading score'].copy()\ny_test_writing = test_set['writing score'].copy()\nX_test = full_pipeline.transform(X_test)\n\nprint('Linear Regression  ----------------')\npredict(lin_regr_math, X_test, y_test_math, 'math score')\npredict(lin_regr_reading, X_test, y_test_reading, 'reading score')\npredict(lin_regr_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Decision Tree Regressor  ----------------')\npredict(tree_regr_math, X_test, y_test_math, 'math score')\npredict(tree_regr_reading, X_test, y_test_reading, 'reading score')\npredict(tree_regr_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Fine-tuned Decision Tree Regressor  ----------------')\nfinal_predict(grid_search_tree_math, X_test, y_test_math, 'math score')\nfinal_predict(grid_search_tree_reading, X_test, y_test_reading, 'reading score')\nfinal_predict(grid_search_tree_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Random Forest Regressor ----------------')\npredict(forest_regr_math, X_test, y_test_math, 'math score')\npredict(forest_regr_reading, X_test, y_test_reading, 'reading score')\npredict(forest_regr_writing, X_test, y_test_writing, 'writing score')\n\nprint()\nprint('Fine-tuned Random Forest Regressor  ----------------')\nfinal_predict(grid_search_forest_math, X_test, y_test_math, 'math score')\nfinal_predict(grid_search_forest_reading, X_test, y_test_reading, 'reading score')\nfinal_predict(grid_search_forest_writing, X_test, y_test_writing, 'writing score')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"We explored three different models:\n* Linear Regression\n* Decision Tree\n* Random Forest\n\nIt seems like linear regression is better than decision tree regressor and random forest regression for this dataset.","metadata":{}},{"cell_type":"markdown","source":"# Reference","metadata":{}},{"cell_type":"markdown","source":"1. https://github.com/ageron/handson-ml2\n2. https://en.wikipedia.org/wiki/Cross-validation_(statistics)#:~:text=Cross%2Dvalidation%2C%20sometimes%20called%20rotation,to%20an%20independent%20data%20set.","metadata":{}}]}