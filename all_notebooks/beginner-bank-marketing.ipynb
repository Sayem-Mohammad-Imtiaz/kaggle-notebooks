{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bank Marketing"},{"metadata":{},"cell_type":"markdown","source":"#### Task\n\nThe data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\n\n#### Data description\nDataset `bank-additional-full.csv` has 41188 examples and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Overview"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/bank-marketing-analysis/bank-additional-full.csv', header=0,sep=\";\")\ndf.head(10)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"#### Find Missing Values (NaN)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing value found"},{"metadata":{},"cell_type":"markdown","source":"#### Find Features with One Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in df.columns:\n    print(column,df[column].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No feature with only one value"},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing the categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [col for col, dtype in df.dtypes.items() if dtype == 'object']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check count based on categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25, 80), facecolor='white')\nplotnumber =1\nfor cat_feature in cat_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.countplot(y=cat_feature, data=df, palette='pastel')\n    plt.xlabel(cat_feature)\n    plt.title(cat_feature)\n    plotnumber+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find out the relationship between categorical variable and target value"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25, 80), facecolor='white')\nplotnumber =1\nfor cat_feature in cat_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.countplot(y=cat_feature, hue='y', palette='pastel', edgecolor='.6', data=df)\n    plt.xlabel(cat_feature)\n    plt.title(cat_feature)\n    plotnumber+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<li>Customers who work as admin, technician and blue-collar are more inclined towards a term deposit;</li>\n<li>Married customers have high interest on deposit;</li>\n<li>Customers with university_degree are more inclined towards a term deposit;</li>\n<li>Customers who don't have credit in default are more inclined towards a term deposit;</li>\n<li>During the summer seasons (May to August) customers show high interest to deposit;</li>\n<li>Customers who has personal loan seems to be less interested on deposit;</li>\n<li>Customers who were contacted via 'cellular' are more inclined towards a term deposit.</li>"},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing the numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = [col for col, dtype in df.dtypes.items() if dtype == 'int64' or dtype == 'float64']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,60), facecolor='white')\nplotnumber =1\nfor num_feature in num_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.kdeplot(df[num_feature], bw=1.5)\n    plt.xlabel(num_feature)\n    plotnumber+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Find Outliers in numerical features"},{"metadata":{},"cell_type":"markdown","source":"Boxplot on numerical features to find outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,60))\nplotnumber =1\nfor num_feature in num_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.boxplot(data = df, x = num_feature, palette='pastel')\n    plt.xlabel(num_feature)\n    plotnumber+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age, duration, compaign, pdays, previous and cons.conf.idx have some outliers"},{"metadata":{},"cell_type":"markdown","source":"#### Check if the Data set is balanced or not based on target values"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (6, 4))\nsns.countplot(data = df, x = 'y')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['y'].groupby(df['y']).count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given dataset seems to be highly imbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('y').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"#### Categorical Feature Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical boolean mask\ncategorical_feature_mask = df.dtypes==object\n# filter categorical columns using mask\ncategorical_cols = df.columns[categorical_feature_mask].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply le on categorical feature columns\ndf[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf[categorical_cols].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 20))\ncorr = df.corr()\nsns.heatmap(corr, fmt='.2f',annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this correlation matrix we can see, that duration,pdays,emp.var.rate,euribor3m and nr.employed are more correlated to target columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove Outliers"},{"metadata":{},"cell_type":"markdown","source":"Removing outliers in feature 'pdays'"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df1.groupby(['y','pdays']).size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping pdays-column as it has 999 value (means client was not previously contacted) for around 90%+ "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.drop(['pdays'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing outliers in feature 'campaign'"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df1.groupby(['y','campaign'],sort=True)['campaign'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assuming campaign count greater than 37 as outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = df1[df1['campaign'] < 37]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### X and y preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df2.iloc[:, :-1]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df2.iloc[:, -1]\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80, max_features='auto')\nrf.fit(X, y)\nprint('Training done using Random Forest')\n\nranking = np.argsort(-rf.feature_importances_)\nf, ax = plt.subplots(figsize=(11, 11))\nsns.barplot(x=rf.feature_importances_[ranking], y=X.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Important note: duration highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_main = X.iloc[:,ranking[1:11]]\ny_main = y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Handling Imbalanced Dataset"},{"metadata":{},"cell_type":"markdown","source":"Oversampling Using SMOTE Methode"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_sm, y_sm = smote.fit_sample(X_main, y_main)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(x = y_main, palette='pastel')\nplt.title('Reparition before SMOTE')\nplt.subplot(1, 2, 2)\nsns.countplot(x = y_sm, palette='pastel')\nplt.title('Reparition after SMOTE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Spliting Data On Traing And Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictive Model Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def k_fold_fit_and_evaluate(X, y, model, scoring_method, n_splits=5):\n    # define evaluation procedure\n    cv = KFold(n_splits=5, random_state=42, shuffle=True)\n    # evaluate model\n    scores = cross_validate(model, X_train, y_train, scoring=scoring_method, cv=cv, n_jobs=-1)\n    return scores[\"test_score\"]\n\nscoring_method_f1 = make_scorer(lambda true_target, prediction: f1_score(true_target, prediction, average=\"weighted\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Average F1-Score For Different Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\nmodels = {\n    \"GaussianNB\": GaussianNB(),\n    \"DummyClassifier\": DummyClassifier(strategy=\"most_frequent\"),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(max_depth=32, min_samples_leaf=1, random_state=random_state),\n    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=1, weights=\"uniform\"),   \n    \"LogisticRegression\": LogisticRegression(C=8, random_state=random_state),\n    \"GradientBoostingClassifier\": GradientBoostingClassifier(loss = 'deviance', n_estimators = 20),\n    \"XGBClassifier\": XGBClassifier(objective='binary:logistic', learning_rate=0.1, max_depth=22, n_estimators=300)\n}\n\n\ndict_f1 = {}\nfor name, model in models.items():\n    metrics_f1 = k_fold_fit_and_evaluate(X_train, y_train, model, scoring_method_f1, n_splits=5) \n    dict_f1[name] = np.mean(metrics_f1)\n\nval = []\nfor i in dict_f1.values():\n    val.append(i)\n\nkeys = []\nfor i in dict_f1.keys():\n    keys.append(i)\n\nplt.figure(figsize=(13,5))\nplt.barh(keys, val)\nfor index, value in enumerate(val):\n    plt.text(value, index, str(round(value,3)))\nplt.title(\"mean F1\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grid-Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nrandom_state = 42\nn_splits = 5\nscoring_method = make_scorer(lambda prediction, true_target: f1_score(true_target, prediction, average=\"weighted\"))\n\nmodel_parameters = {\n    \"GaussianNB\": {\n    \n    },\n    \"DummyClassifier\": {\n        'strategy':['stratified','most_frequent','prior','uniform']\n    },\n    \"DecisionTreeClassifier\": {\n        'max_depth': [20, 22, 28, 32, 37, 38, 42, 45, 50, 70],\n        'min_samples_leaf':[1, 2, 3, 4, 5]\n    },\n    \"KNeighborsClassifier\": {\n        'n_neighbors':[1, 2, 3, 4], \n        'weights':[\"uniform\", \"distance\"]\n    },\n    \"LogisticRegression\": {\n        'C':[7, 8, 10, 15, 30, 40, 50, 70],\n        'max_iter':[1000]\n        \n    },\n    \"GradientBoostingClassifier\": {\n        'loss': [\"deviance\", \"exponential\"],\n        'n_estimators': [1, 2, 10, 20]\n    }\n}\n\nfor model_name, parameters in model_parameters.items():\n    model = models[model_name]\n    \n    cv = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    grid_search = GridSearchCV(model, parameters, cv=cv, n_jobs=-1, verbose=False, scoring=scoring_method).fit(X_train, y_train)\n\n    best_score = grid_search.best_score_\n    best_params = grid_search.best_params_\n    \n    print(model_name)\n    print(\"- best_score =\", best_score)\n    print(\"best paramters:\")\n    for k,v in best_params.items():\n        print(\"-\", k, v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictive Model Application"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\n\ny_true = y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_gbc = GradientBoostingClassifier(loss = 'deviance', n_estimators = 20, random_state=42)\n\nclf_gbc.fit(X_train, y_train)\ny_predicted_gbc = clf_gbc.predict(X_test)\nprint(classification_report(y_true, y_predicted_gbc, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_predicted_gbc)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = clf_gbc.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_dt = DecisionTreeClassifier(max_depth=32, min_samples_leaf=1, random_state=0)\nclf_dt.fit(X_train, y_train)\n\ny_predicted_dt = clf_dt.predict(X_test)\nprint(classification_report(y_true, y_predicted_dt, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_predicted_dt)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = clf_dt.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNeighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_knn=KNeighborsClassifier(n_neighbors=1, weights=\"uniform\")\nclf_knn.fit(X_train, y_train)\n\ny_predicted_knn = clf_knn.predict(X_test)\n\nprint(classification_report(y_true, y_predicted_knn, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_predicted_knn)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = clf_knn.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Normalization*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nnorm = MinMaxScaler().fit(X_train)\nX_train_norm = norm.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm = MinMaxScaler().fit(X_test)\nX_test_main_norm = norm.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Standardization*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nstd_scale = preprocessing.StandardScaler().fit(X_train)\nX_train_std = std_scale.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scale = preprocessing.StandardScaler().fit(X_test)\nX_test_main_std = std_scale.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNeighbors With Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_knn=KNeighborsClassifier(n_neighbors=1, weights=\"uniform\")\nclf_knn.fit(X_train_norm, y_train)\n\ny_predicted_knn = clf_knn.predict(X_test_main_norm)\n\nprint(classification_report(y_true, y_predicted_knn, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_predicted_knn)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = clf_knn.predict_proba(X_test_main_norm)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNeighbors With Standardization"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_knn=KNeighborsClassifier(n_neighbors=1, weights=\"uniform\")\nclf_knn.fit(X_train_std, y_train)\n\ny_predicted_knn = clf_knn.predict(X_test_main_std)\n\nprint(classification_report(y_true, y_predicted_knn, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test,y_predicted_knn)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"probs = clf_knn.predict_proba(X_test_main_std)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XG Boost "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_xgb=XGBClassifier(objective='binary:logistic',learning_rate=0.1,max_depth=22,n_estimators=300)\nclf_xgb.fit(X_train, y_train)\n\ny_predicted_xgb = clf_xgb.predict(X_test)\n\nprint(classification_report(y_true, y_predicted_xgb, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test,y_predicted_xgb)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = clf_xgb.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'loss_function':'Logloss', # objective function\n          'eval_metric':'AUC', # metric\n          'verbose': 1000,\n         }\n\nclf_cat = CatBoostClassifier(**params)\nclf_cat.fit(X_train, y_train)\n\ny_predicted_cat = clf_cat.predict(X_test)\n\nprint(classification_report(y_true, y_predicted_cat, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test,y_predicted_cat)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = clf_cat.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Model with the best score is <em>XGBoost Classifier</em>  \nAchieved roc-auc: 0.97"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}