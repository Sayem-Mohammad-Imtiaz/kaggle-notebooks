{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(color_codes=True)\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_df = pd.read_csv(\"../input/vehicle/vehicle.csv\")\n\nprint('Dataframe Shape : ', vehicle_df.shape);\nprint('\\n')\nvehicle_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = vehicle_df.isnull().sum()\ndf = pd.DataFrame({'columns': vehicle_df.columns, 'missing_count': data.values})\ndf = df[df['missing_count'] > 0]\nprint(df.sort_values(['missing_count'], ascending=False))\nprint()\nprint('Missing data in ', df['columns'].size, ' columns.')\nprint('Missing data columns : ', df[df['missing_count'] > 0]['columns'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial data analysis summary\n- There are 846 rows with 19 columns.\n- The categorical column 'class' represents the category of vehicles.\n- The null values are present in 14 columns listed above.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# d. 5 point summary of numerical attributes\nvehicle_df.describe().round(2).T\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_df2 = vehicle_df.copy()\nvehicle_df2.fillna(vehicle_df2.mean(), inplace=True)\nvehicle_df2.drop(\"class\", axis=1, inplace=True)\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(25, 25))\nfor i, column in enumerate(vehicle_df2.columns):\n    sns.distplot(vehicle_df2[column],ax=axes[i//3,i%3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class column data distribution\nvehicle_df['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.subplots(figsize=(100, 100))\n#sns.boxplot(data=vehicle_df2, orient=\"h\")\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(25, 25))\n#fivepoint = pd.DataFrame(columns=['Model Name', 'Accuracy', 'Recall', 'Precision'])\nfor i, column in enumerate(vehicle_df2.columns):\n    sns.boxplot(vehicle_df2[column],ax=axes[i//3,i%3], dodge=False, whis=1.5)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Max value based on boxplot to filter outliers of 8 columns where outliers are identified. \nmax_df = pd.DataFrame([[255,77,13,288,990,87,19,40]],columns=['radius_ratio', 'pr.axis_aspect_ratio', 'max.length_aspect_ratio', 'scaled_variance', 'scaled_variance.1', 'scaled_radius_of_gyration.1', 'skewness_about', 'skewness_about.1'])\n\ntotal_outliers = 0\nfor i, column in enumerate(max_df.columns):\n    #print(column, max_df[column][0], vehicle_df[column][vehicle_df[column] > max_df[column][0]].size)\n    total_outliers += vehicle_df[column][vehicle_df[column] > max_df[column][0]].size\n    \nprint('Total Outliers ', total_outliers)\nprint('Total Outliers %', round((total_outliers/len(vehicle_df.index))*100) )\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### Statistical Summary\n- The data is not distributed equally for the 3 vehicle classes. The 50% of the data belongs to car class.\n- As seen in distribution graph, most of the columns have bimodal distribution of data and some are multimodal.\n- radius_ratio, axis_aspect_ration, length_aspect_ratio & scaled_radius_of_gyration.1 columns have very long right tail.\n- The scale of the columns are very different so would need normalization. \n- As per boxplot, 8 columns has outliers and 5 columns has many number of outliers. \n- The outliers are approx 6%."},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"vehicle_df_new = vehicle_df.copy();\n\n# Fill null\nvehicle_df_new.fillna(vehicle_df_new.mean(), inplace=True)\n\n# Remove outliers based on max value identified earlier from boxplot\nfor i, column in enumerate(max_df.columns):\n    vehicle_df_new = vehicle_df_new[vehicle_df_new[column] < max_df[column][0]]\n    \n# Convert class column to categorical \nvehicle_df_new['class'] = pd.Categorical(vehicle_df_new['class']).codes\n\n## rest the index post cleaning the outliers\nvehicle_df_new = vehicle_df_new.reset_index(drop=True)\n\nvehicle_df_new.info()\nvehicle_df_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Attribute Relationship Analysis\n\n#### - Find groups with correlated columns for feature selection & PCA\n#### - Find low correlated columns to ignore from PCA\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# independant variables\nX = vehicle_df_new.drop(['class'], axis=1)\n# the dependent variable\ny = vehicle_df_new[['class']]\n\nsns.pairplot(X, diag_kind='kde')   # plot density curve instead of histogram on the diag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = vehicle_df_new.corr().round(2)\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns,cmap='RdBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### - Pair plot & Heatmap Summary\n- 11 columns are highly correlated to each other considered as group1. PCA will be performed for Dimensionality Reduction.\n- 3 other columns are highly correlated to each other considered as group2. PCA will be performed for Dimensionality Reduction.\n- In group 2, scaled_radius_of_gyration.1 and hollows_ratio has -ve correlation. Whereas skewness_about.2 and hollows_ratio has +ve correlation.\n- 4 columns ( pr.axis_aspect_ratio, max.length_aspect_ratio, skewness_about, skewness_about.1 ) do not have correlation to any other columns which will be directly considered for model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = ['class']\n\n# High correlation columns (Group 1)\ncols_hc1 = ['compactness','circularity','distance_circularity','radius_ratio','scatter_ratio','elongatedness',\n           'pr.axis_rectangularity','max.length_rectangularity','scaled_variance','scaled_variance.1',\n           'scaled_radius_of_gyration']\nsns.pairplot(vehicle_df_new[[*cols_hc1, *c]], diag_kind='kde', hue='class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# High correlated columns (Group 2)\ncols_hc2 = ['hollows_ratio', 'scaled_radius_of_gyration.1', 'skewness_about.2']\n\nsns.pairplot(vehicle_df_new[[*cols_hc2, *c]], diag_kind='kde', hue='class')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Low correlation columns\ncols_lc = ['pr.axis_aspect_ratio','max.length_aspect_ratio','skewness_about','skewness_about.1']\n\nsns.pairplot(vehicle_df_new[[*cols_lc, *c]], diag_kind='kde', hue='class')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attribute Relationship Summary\n- Following columns are highly correlated hence PCA will be done\n    compactness,circularity, distance_circularity, radius_ratio,scatter_ratio, elongatedness, pr.axis_rectangularity, max.length_rectangularity, scaled_variance, scaled_variance.1, scaled_radius_of_gyration\n\n- Following 3 columns are highly correlated to each other but not to other columns so either we can keep 1 column and drop other 2 or we can do PCA separatly and merge the PCA feature to final list. \n    hollows_ratio, scaled_radius_of_gyration.1, skewness_about.2\n\n- Following columns are very low correlated to other columns hence will be ignore from PCA. These will be merged to PCA columns for model building.\n    pr.axis_aspect_ratio, max.length_aspect_ratio, skewness_about, skewness_about.1\n\n"},{"metadata":{},"cell_type":"markdown","source":"## 3. PCA - Dimensionality Reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale the values\nfrom scipy.stats import zscore\nXScaled=X.apply(zscore)\nXScaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply PCA on Group 1 of high Corelation columns\nX1 = XScaled[cols_hc1]\npca1 = PCA(n_components=len(cols_hc1), whiten=False)\npca1.fit(X1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original number of features:', len(cols_hc1))\n#print('Reduced number of features:', pca1.shape[1])\nprint()\nprint('Eigen Values', pca1.explained_variance_)\nprint()\n#print('Eigen Vector', pca1.components_)\n#print()\n#print('Percentage  ', pca1.explained_variance_ratio_)\n\npercent_variance = np.asarray([float(format(num, '.3f')) for num in pca1.explained_variance_ratio_])\npercent_variance = np.round(np.asarray(percent_variance) * 100, decimals =2)\nprint('Percentage  ', percent_variance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 5))\n\nax1.bar(list(range(0,len(cols_hc1))),pca1.explained_variance_ratio_,align='center')\nax1.set(xlabel='Eigen Value', ylabel='Variation explained')\n\nax2.step(list(range(0,len(cols_hc1))),np.cumsum(pca1.explained_variance_ratio_), where='mid')\n#ax2.plot(pca1.explained_variance_)\nax2.set(xlabel='Eigen Value', ylabel='Cumulative of variation explained')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With 4 variables we can explain over 95% of the variation in the original data of group1 columns. \n# And with 5 variables we can explain more than 98%\npca1_95 = PCA(n_components=0.95, whiten=True)\nX_pca1_95 = pca1_95.fit_transform(X1)\nprint('Original number of features:', len(cols_hc1))\nprint('Reduced number of features:', X_pca1_95.shape[1])\nprint(X_pca1_95.shape)\nsns.pairplot(pd.DataFrame(X_pca1_95))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply PCA on Group 2 of high Corelation columns\nX2 = XScaled[cols_hc2]\n\npca2 = PCA(n_components=len(cols_hc2), whiten=False)\npca2.fit(X2)\nprint('Eigen Values', pca2.explained_variance_)\nprint('Percentage  ', np.round(pca2.explained_variance_ratio_ * 100, decimals =2))\n\n\n# With 2 variables we can explain over 95% of the variation in the original data of group2 columns\npca2 = PCA(n_components=0.95, whiten=True)\nX_pca2 = pca2.fit_transform(X2)\n\nprint('Original number of features:', len(cols_hc2))\nprint('Reduced number of features:', X_pca2.shape[1])\nprint(X_pca2.shape)\nsns.pairplot(pd.DataFrame(X_pca2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reduced group1 of 11 columns to 4 columns with 95% variance\nx_pca1_95_df = pd.DataFrame(data = X_pca1_95)\n\n#Reduced group1 of 3 columns to 2 columns \nx_pca2_df = pd.DataFrame(data = X_pca2)\n\n#Combind the 3 data frames a) Group1 PCA columns (95% variance), b) Group2 PCA columns, c) No correlation columns\nX_new = pd.merge(x_pca1_95_df,x_pca2_df,right_index=True, left_index=True);\nX_new = pd.merge(X_new,XScaled[cols_lc],right_index=True, left_index=True);\n\nprint('Final Shape', X_new.shape)\nX_new.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. SVM Classifier"},{"metadata":{},"cell_type":"markdown","source":"#### SVM model with original features"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split the train and test data into 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(XScaled, y, test_size = 0.3, random_state = 1)\n## build the SVM model on training data\nsvc_org = SVC()\nsvc_org.fit(X_train,y_train)\nprediction= svc_org.predict(X_test)\nprint(XScaled.shape)\n#print(\"Class Distribution:\\n\",y['class'].value_counts())\nprint(\"Train Data Score\", round(svc_org.score(X_train, y_train), 3))\nprint(\"Test Data Score \", round(svc_org.score(X_test,y_test), 3))\nprint(\"Confusion Matrix:\\n   bus car van\\n\",metrics.confusion_matrix(prediction,y_test))\ntarget_names = ['bus', 'car', 'van']\nprint(metrics.classification_report(y_test, prediction, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM model with features selected using PCA and visual analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split the train and test data into 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.3, random_state = 1)\n## build the SVM model on training data\nsvc_pca = SVC()\nsvc_pca.fit(X_train,y_train)\nprediction= svc_pca.predict(X_test)\nprint(X_new.shape)\nprint(svc_pca)\nprint(\"Train Data Score\", round(svc_pca.score(X_train, y_train), 3))\nprint(\"Test Data Score\", round(svc_pca.score(X_test,y_test),3))\nprint(\"Confusion Matrix:\\n   bus car van\\n\",metrics.confusion_matrix(prediction,y_test))\ntarget_names = ['bus', 'car', 'van']\nprint(metrics.classification_report(y_test, prediction, target_names=target_names))\nmetrics.classification_report(y_test, prediction, target_names=target_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM Summary\n\nCommon\n- There is a slight increase in test data accuracy from 97.1% to 97.5% when PCA variables are used.\n- The precision & recall performance of bus is decreased slightly. However not much change in case of car & bus.\n\nAs per confusion matrix comparasion\n- The accuracy of the modal with PCA variables is slightly different than original columns.\n- The correct prediction of bus reduced from 66 to 64 and no change in incorrect prediction for both car & van.\n- The correct prediction of car remains same but incorrect prediction increased for bus and reduced for van.\n- The correct prediction of van increased from 46 to 49 and incorrect prediction is unchanged.\n\nAs per classification metrics \n- The precision for bus is reduced by 1% but for car & van remains same. \n- The recall for van is increased by 6% but for bus reduced by 3% and not change for car.\n"},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## 5. Hyper Parameters Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Parameter Grid\nparam_grid = [{'kernel': ['linear'], 'C': [0.01, 0.05, 0.5, 1.0, 10, 25, 50]},\n              {'kernel': ['rbf'], 'C': [0.01, 0.05, 0.5, 1.0, 10, 25, 50]}\n             ] \n# Make grid search classifier\nclf_grid = GridSearchCV(SVC(), param_grid, verbose=1)\n \n# Train the classifier\nclf_grid.fit(X_train, y_train)\n \n# clf = grid.best_estimator_()\nprint(\"Best Parameters:\\n\", clf_grid.best_params_)\nprint(\"Best Estimators:\\n\", clf_grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using Best SVM Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split the train and test data into 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.3, random_state = 1)\n## build the SVM model on training data\nsvc_pca_tun = SVC(C=10, kernel = \"rbf\")\nsvc_pca_tun.fit(X_train,y_train)\nprediction= svc_pca_tun.predict(X_test)\nprint(X_new.shape)\nprint(\"Train Data Score\", round(svc_pca_tun.score(X_train, y_train), 3))\nprint(\"Test Data Score\", round(svc_pca_tun.score(X_test,y_test),3))\nprint(\"Confusion Matrix:\\n   bus car van\\n\",metrics.confusion_matrix(prediction,y_test))\ntarget_names = ['bus', 'car', 'van']\nprint(metrics.classification_report(y_test, prediction, target_names=target_names))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVM Parameter Tuning Summary\n\n- The test data accuracy increased further by 0.4%.\n- Accuracy of car is improved by 1 in correct prediction and by 1 in incorrect predication of bus.\n"},{"metadata":{},"cell_type":"markdown","source":"#### K-FOLD Cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_kfold = cross_val_score(svc_org, XScaled, y, cv=10) \nprint(\"Accuracy with SVM on original data: %0.2f (+/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))\n\npred_kfold = cross_val_score(svc_pca, X_new, y, cv=10) \nprint(\"Accuracy with SVM on PCA data: %0.2f (+/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))\n\npred_kfold = cross_val_score(svc_pca_tun, X_new, y, cv=10) \nprint(\"Accuracy with SVM with tuned params on PCA data: %0.2f (+/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}