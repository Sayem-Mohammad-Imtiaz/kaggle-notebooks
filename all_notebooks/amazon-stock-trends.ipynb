{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AMAZON STOCKS TREND","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import dates\n\nfrom pylab import rcParams\nimport statsmodels.api as sm\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T11:40:52.311024Z","iopub.execute_input":"2021-07-10T11:40:52.31145Z","iopub.status.idle":"2021-07-10T11:40:52.320225Z","shell.execute_reply.started":"2021-07-10T11:40:52.311414Z","shell.execute_reply":"2021-07-10T11:40:52.317441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used the ‘parse_dates’ parameter in the read_csv function to convert the ‘Date’ column to the DatetimeIndex format.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/amazon-stock-data/AMZN.csv\", parse_dates=True, index_col = \"Date\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:12:21.938964Z","iopub.execute_input":"2021-07-10T11:12:21.939382Z","iopub.status.idle":"2021-07-10T11:12:22.021191Z","shell.execute_reply.started":"2021-07-10T11:12:21.939347Z","shell.execute_reply":"2021-07-10T11:12:22.020089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FEATURE DESCRIPTION:","metadata":{}},{"cell_type":"markdown","source":"* Open = Price from the first transaction of a trading day\n* High = Maximum price in a trading day\n* Low = Minimum price in a trading day\n* Close = Price from the last transaction of a trading day\n* Adj Close = Closing price adjusted to reflect the value after accounting for any corporate actions\n* Volume = Number of units traded in a day**","metadata":{}},{"cell_type":"markdown","source":"> BASIC PLOT FOR CHECKING THE TRENDS OF VOLUME:","metadata":{}},{"cell_type":"code","source":"df['Volume'].plot(figsize=(10,6))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:48:05.276668Z","iopub.execute_input":"2021-07-10T11:48:05.277045Z","iopub.status.idle":"2021-07-10T11:48:05.530751Z","shell.execute_reply.started":"2021-07-10T11:48:05.277014Z","shell.execute_reply":"2021-07-10T11:48:05.52963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We can see alot of peaks and density between 2000 and 2010**","metadata":{}},{"cell_type":"markdown","source":"Lets look at how the other features are distributed","metadata":{}},{"cell_type":"code","source":"df.plot(subplots=True, figsize=(10,12))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:25:25.993859Z","iopub.execute_input":"2021-07-10T11:25:25.994317Z","iopub.status.idle":"2021-07-10T11:25:27.261754Z","shell.execute_reply.started":"2021-07-10T11:25:25.994275Z","shell.execute_reply":"2021-07-10T11:25:27.260586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The shape of the curve for ‘Open’, ‘Close’, ‘High’ and ‘Low’ data have the same shape. Only the ‘Volume’ has a different shape.**","metadata":{}},{"cell_type":"markdown","source":"# SEASONALITY:","metadata":{}},{"cell_type":"markdown","source":"Resampling for months or weeks and making bar plots is another very simple and widely used method of finding seasonality. Here I am making a bar plot of month data in 2020","metadata":{}},{"cell_type":"code","source":"df_month = df.resample(\"M\").mean()\nfig, ax = plt.subplots(figsize=(12, 6))\nax.xaxis.set_major_formatter(dates.DateFormatter('%Y-%m'))\nax.bar(df_month['2020':].index, df_month.loc['2020':, \"Volume\"], width=25, align='center')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T12:06:12.281155Z","iopub.execute_input":"2021-07-10T12:06:12.281638Z","iopub.status.idle":"2021-07-10T12:06:12.608788Z","shell.execute_reply.started":"2021-07-10T12:06:12.281593Z","shell.execute_reply":"2021-07-10T12:06:12.607663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each bar represents a month. A huge spike in April 2020. Otherwise, there is monthly seasonality after 2020 ended.","metadata":{}},{"cell_type":"markdown","source":"# RESAMPLING AND ROLLING:","metadata":{}},{"cell_type":"markdown","source":"Resampling is very common in time-series data. Most of the time resampling is done to a lower frequency.Though resampling of higher frequency is also necessary especially for modeling purposes. Not so much in data analysis purpose.\nIn the ‘Volume’ data we are working on right now, we can observe some big spikes here and there. These types of spikes are not helpful for data analysis or for modeling. normally to smooth out the spikes, resampling to a lower frequency and rolling is very helpful.","metadata":{}},{"cell_type":"code","source":"start, end = '2017-01', '2017-06'\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df.loc[start:end, 'Volume'],\nmarker='.', linestyle='-', linewidth=0.5, label='Daily')\nax.plot(df_month.loc[start:end, 'Volume'],\nmarker='o', markersize=8, linestyle='-', label='Monthly Mean Resample')\nax.set_ylabel('Volume')\nax.legend();","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:27:41.683982Z","iopub.execute_input":"2021-07-10T11:27:41.684415Z","iopub.status.idle":"2021-07-10T11:27:41.964566Z","shell.execute_reply.started":"2021-07-10T11:27:41.684379Z","shell.execute_reply":"2021-07-10T11:27:41.963143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WEEKLY RESAMPLE**","metadata":{}},{"cell_type":"code","source":"df_week = df.resample(\"W\").mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:26:41.338546Z","iopub.execute_input":"2021-07-10T11:26:41.339Z","iopub.status.idle":"2021-07-10T11:26:41.372574Z","shell.execute_reply.started":"2021-07-10T11:26:41.338957Z","shell.execute_reply":"2021-07-10T11:26:41.371717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start, end = '2020-01', '2020-08'\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df.loc[start:end, 'Volume'], marker='.', linestyle='-', linewidth = 0.5, label='Daily', color='black')\nax.plot(df_week.loc[start:end, 'Volume'], marker='o', markersize=8, linestyle='-', label='Weekly', color='coral')\nax.set_ylabel(\"Open\")\nax.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:27:48.336572Z","iopub.execute_input":"2021-07-10T11:27:48.336967Z","iopub.status.idle":"2021-07-10T11:27:48.623012Z","shell.execute_reply.started":"2021-07-10T11:27:48.336935Z","shell.execute_reply":"2021-07-10T11:27:48.621843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROLLING:","metadata":{}},{"cell_type":"markdown","source":"Rolling is another very helpful way of smoothing out the curve. It takes the average of a specified amount of data. If I want a 7-day rolling, it gives us the 7-day average data.","metadata":{}},{"cell_type":"markdown","source":"We are doing it on the above plot","metadata":{}},{"cell_type":"code","source":"df_7d_rolling = df.rolling(7, center=True).mean()\nstart, end = '2016-06', '2017-05'\nfig, ax = plt.subplots(figsize=(12, 6))\nax.plot(df.loc[start:end, 'Volume'], marker='.', linestyle='-', \n        linewidth=0.5, label='Daily')\nax.plot(df_week.loc[start:end, 'Volume'], marker='o', markersize=5, \n        linestyle='-', label = 'Weekly mean volume')\nax.plot(df_7d_rolling.loc[start:end, 'Volume'], marker='.', linestyle='-', label='7d Rolling Average')\nax.set_ylabel('Stock Volume')\nax.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:28:56.236843Z","iopub.execute_input":"2021-07-10T11:28:56.237244Z","iopub.status.idle":"2021-07-10T11:28:56.524624Z","shell.execute_reply.started":"2021-07-10T11:28:56.237211Z","shell.execute_reply":"2021-07-10T11:28:56.523863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" 7-d rolling average is a bit smoother than the weekly average.","metadata":{}},{"cell_type":"markdown","source":"# PLOTTING THE CHANGE:","metadata":{}},{"cell_type":"markdown","source":"# SHIFT:","metadata":{}},{"cell_type":"markdown","source":"The shift function shifts the data before or after the specified amount of time. It will shift the data by one day by default. That means you will get the previous day's data. In financial data like this one, it is helpful to see previous day data and today's data side by side.","metadata":{}},{"cell_type":"code","source":"df['Change'] = df.Close.div(df.Close.shift())\ndf['Change'].plot(figsize=(20, 8), fontsize = 16)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:39:42.068008Z","iopub.execute_input":"2021-07-10T11:39:42.069511Z","iopub.status.idle":"2021-07-10T11:39:42.443293Z","shell.execute_reply.started":"2021-07-10T11:39:42.069401Z","shell.execute_reply":"2021-07-10T11:39:42.441804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the code above, .div() helps to fill up the missing data. Actually, div() means division. df. div(6) will divide each element in df by 6. But here I used ‘df.Close.shift()’. So, Each element of df will be divided by each element of ‘df.Close.shift()’. We do this to avoid the null values that are created by the ‘shift()’ operation.","metadata":{}},{"cell_type":"markdown","source":"This is the plot of 2001 only.","metadata":{}},{"cell_type":"code","source":"df['2001']['Change'].plot(figsize=(10, 6))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:40:58.615739Z","iopub.execute_input":"2021-07-10T11:40:58.616163Z","iopub.status.idle":"2021-07-10T11:40:58.862766Z","shell.execute_reply.started":"2021-07-10T11:40:58.616125Z","shell.execute_reply":"2021-07-10T11:40:58.861855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PERCENTAGE CHANGE:","metadata":{}},{"cell_type":"markdown","source":"There is a percent change function available to get the percent_change data.","metadata":{}},{"cell_type":"markdown","source":"I've chose only the first 100 data entries.","metadata":{}},{"cell_type":"code","source":"df_month.loc[:, 'pct_change'] = df.Close.pct_change()*100\nfig, ax = plt.subplots(figsize=(20, 8))\ndf_month['pct_change' ].head(100).plot(kind='bar', color='violet', ax=ax)\nax.xaxis.set_major_locator(dates.WeekdayLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter('%b %d'))\nplt.xticks(rotation=45)\nax.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:41:51.784211Z","iopub.execute_input":"2021-07-10T11:41:51.784666Z","iopub.status.idle":"2021-07-10T11:41:52.542127Z","shell.execute_reply.started":"2021-07-10T11:41:51.784627Z","shell.execute_reply":"2021-07-10T11:41:52.540752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see the percentage change in the data.","metadata":{}},{"cell_type":"markdown","source":"# DIFFERENCING:","metadata":{}},{"cell_type":"markdown","source":"Differencing takes the difference in values of a specified distance.It is a popular method to remove the trend in the data. The trend is not good for forecasting or modeling.","metadata":{}},{"cell_type":"markdown","source":"I've used expanding window,an another way of transformation. It keeps adding the cumulative. For example, if you add an expanding function to the ‘High’ column first element remains the same. The second element becomes cumulative of the first and second element, the third element becomes cumulative of the first, second, and third element, and so on. You can use aggregate functions like mean, median, standard deviation, etc. on it too","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 8))\nax = df.High.plot(label='High')\nax = df.High.expanding().mean().plot(label='High expanding mean')\nax = df.High.expanding().std().plot(label='High expanding std')\nax.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:43:04.133791Z","iopub.execute_input":"2021-07-10T11:43:04.134231Z","iopub.status.idle":"2021-07-10T11:43:04.494427Z","shell.execute_reply.started":"2021-07-10T11:43:04.134193Z","shell.execute_reply":"2021-07-10T11:43:04.49319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# DECOMPOSITION:","metadata":{}},{"cell_type":"markdown","source":"Decomposition will show the observations and these three elements in the same plot:\n* Trend: Consistent upward or downward slope of a time series.\n* Seasonality: Clear periodic pattern of a time series\n* Noise: Outliers or missing values","metadata":{}},{"cell_type":"code","source":"rcParams['figure.figsize'] = 11, 9\ndecomposition = sm.tsa.seasonal_decompose(df_month['Volume'], model='Additive')\nfig = decomposition.plot()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T11:45:07.280778Z","iopub.execute_input":"2021-07-10T11:45:07.281277Z","iopub.status.idle":"2021-07-10T11:45:08.492704Z","shell.execute_reply.started":"2021-07-10T11:45:07.28123Z","shell.execute_reply":"2021-07-10T11:45:08.491376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here the trend is the moving average. To give you a high-level idea of residuals, here is the general formula:\n**Original observations = Trend + Seasonality + Residuals**","metadata":{}},{"cell_type":"markdown","source":"# REFERENCE:\n[https://towardsdatascience.com/a-complete-guide-to-time-series-data-visualization-in-python-da0ddd2cfb01](http://)","metadata":{}}]}