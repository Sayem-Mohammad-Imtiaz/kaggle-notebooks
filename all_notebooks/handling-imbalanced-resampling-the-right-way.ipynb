{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction"},{"metadata":{},"cell_type":"markdown","source":"This notebook is about how to do cross-validation once you have decided that oversampling is the right approach for imbalance problem.So  in this notebook i have focused on modeling , You can't fin feat eng , feat selection , hyper parameter optimization and so on.  If you want to play with the process yourself;you can complete the pipeline applying better algorithm(s) and feat eng as well. \n\nwe will be going through the following steps:\n\n- Getting a base model with random forest \n- Oversampling the wrong way: Do oversample, then oof cross-validate. Sounds fine, but results are overly optimistic.\n- Oversampling the right way: Make SMOTE a part of our oof cross validation and compare recall and confusion matrix\n\nalso i used answer.npy , the target values of test data to evaluate unseen data for models.\n\nOur goal will be to find a classifier with a good recall (i.e. we want our classifier to find as many positive cases as it can). So i didn't apply method to boost roc-auc for example. "},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:14.531286Z","iopub.status.busy":"2020-12-17T14:11:14.530428Z","iopub.status.idle":"2020-12-17T14:11:14.540043Z","shell.execute_reply":"2020-12-17T14:11:14.540633Z"},"papermill":{"duration":0.047782,"end_time":"2020-12-17T14:11:14.540845","exception":false,"start_time":"2020-12-17T14:11:14.493063","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file \\\\I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:14.601542Z","iopub.status.busy":"2020-12-17T14:11:14.600782Z","iopub.status.idle":"2020-12-17T14:11:17.500894Z","shell.execute_reply":"2020-12-17T14:11:17.500056Z"},"papermill":{"duration":2.931858,"end_time":"2020-12-17T14:11:17.501026","exception":false,"start_time":"2020-12-17T14:11:14.569168","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold, GridSearchCV, StratifiedKFold\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsRegressor, KernelDensity, KDTree\nfrom sklearn.metrics import *\n\nfrom imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler, SMOTENC, SVMSMOTE\nfrom imblearn.combine import SMOTETomek, SMOTEENN\nfrom imblearn.under_sampling import ClusterCentroids, TomekLinks, NearMiss, RandomUnderSampler, AllKNN, CondensedNearestNeighbour,\\\n                                        NeighbourhoodCleaningRule, OneSidedSelection, RepeatedEditedNearestNeighbours, InstanceHardnessThreshold\n\n\n\nimport sys, os\nimport random \n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nfrom IPython.display import display\n\n\ndef set_seed(seed=4242):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the data"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:17.565032Z","iopub.status.busy":"2020-12-17T14:11:17.564269Z","iopub.status.idle":"2020-12-17T14:11:18.409976Z","shell.execute_reply":"2020-12-17T14:11:18.409187Z"},"papermill":{"duration":0.880401,"end_time":"2020-12-17T14:11:18.410116","exception":false,"start_time":"2020-12-17T14:11:17.529715","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train  = pd.read_csv('/kaggle/input/imbalanced-data-practice/aug_train.csv')\ntest  = pd.read_csv('/kaggle/input/imbalanced-data-practice/aug_test.csv')\nanswers = np.load('/kaggle/input/answer/answer.npy')\nprint(train.shape, test.shape)\ndisplay(train.head())\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:18.474855Z","iopub.status.busy":"2020-12-17T14:11:18.474033Z","iopub.status.idle":"2020-12-17T14:11:18.619608Z","shell.execute_reply":"2020-12-17T14:11:18.618845Z"},"papermill":{"duration":0.180572,"end_time":"2020-12-17T14:11:18.619744","exception":false,"start_time":"2020-12-17T14:11:18.439172","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:18.798428Z","iopub.status.busy":"2020-12-17T14:11:18.700924Z","iopub.status.idle":"2020-12-17T14:11:19.146497Z","shell.execute_reply":"2020-12-17T14:11:19.145356Z"},"papermill":{"duration":0.497505,"end_time":"2020-12-17T14:11:19.146651","exception":false,"start_time":"2020-12-17T14:11:18.649146","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.031127,"end_time":"2020-12-17T14:11:19.914572","exception":false,"start_time":"2020-12-17T14:11:19.883445","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Categories"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:19.987688Z","iopub.status.busy":"2020-12-17T14:11:19.986526Z","iopub.status.idle":"2020-12-17T14:11:19.990981Z","shell.execute_reply":"2020-12-17T14:11:19.99035Z"},"papermill":{"duration":0.045207,"end_time":"2020-12-17T14:11:19.991104","exception":false,"start_time":"2020-12-17T14:11:19.945897","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"cats = [c for c in train.columns if train[c].dtypes =='object']\nprint('Categories', cats)\n\nnums = [c for c in train.columns if c not in cats]\nprint('Numerics', nums)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:20.064889Z","iopub.status.busy":"2020-12-17T14:11:20.063764Z","iopub.status.idle":"2020-12-17T14:11:22.299578Z","shell.execute_reply":"2020-12-17T14:11:22.298918Z"},"papermill":{"duration":2.27672,"end_time":"2020-12-17T14:11:22.29971","exception":false,"start_time":"2020-12-17T14:11:20.02299","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"for c in cats:\n    le=LabelEncoder()\n    le.fit(list(train[c].astype('str')) + list(test[c].astype('str')))\n    train[c] = le.transform(list(train[c].astype(str))) \n    test[c] = le.transform(list(test[c].astype(str))) \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:22.373165Z","iopub.status.busy":"2020-12-17T14:11:22.37245Z","iopub.status.idle":"2020-12-17T14:11:22.392808Z","shell.execute_reply":"2020-12-17T14:11:22.392069Z"},"papermill":{"duration":0.059798,"end_time":"2020-12-17T14:11:22.392942","exception":false,"start_time":"2020-12-17T14:11:22.333144","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"del train['id']\ndel test['id']\n\ntarget = train.pop('Response')\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:22.466255Z","iopub.status.busy":"2020-12-17T14:11:22.465391Z","iopub.status.idle":"2020-12-17T14:11:23.53988Z","shell.execute_reply":"2020-12-17T14:11:23.540429Z"},"papermill":{"duration":1.114588,"end_time":"2020-12-17T14:11:23.540596","exception":false,"start_time":"2020-12-17T14:11:22.426008","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 12))\nsns.heatmap(pd.concat([train, target], axis=1).corr(),annot=True , cmap='vlag') ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:11:23.618873Z","iopub.status.busy":"2020-12-17T14:11:23.617841Z","iopub.status.idle":"2020-12-17T14:11:23.800704Z","shell.execute_reply":"2020-12-17T14:11:23.801196Z"},"papermill":{"duration":0.223225,"end_time":"2020-12-17T14:11:23.801394","exception":false,"start_time":"2020-12-17T14:11:23.578169","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x = target ,palette=\"Set2\")\nsns.set(font_scale=1.5)\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfig = plt.gcf()\nfig.set_size_inches(10,5)\nax.set_ylim(top=700000)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(target)), (p.get_x()+ 0.3, p.get_height()+10000))\n\nplt.title('Distribution of Target')\n\nplt.ylabel('Frequency [%]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.037313,"end_time":"2020-12-17T14:11:23.876095","exception":false,"start_time":"2020-12-17T14:11:23.838782","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Resampling Methods"},{"metadata":{},"cell_type":"markdown","source":"you can find several resources and kaggle notebooks for understanding resampling methods  and fundamentals, but the summary is that:\n    \n>*The main motivation behind the need to preprocess imbalanced data before we feed them into a classifier is that typically classifiers are more sensitive to detecting the majority class and less sensitive to the minority class.* *Thus, if we don't take care of the issue, the classification output will be biased, in many cases resulting in always predicting the majority class. Many methods have been proposed in the past few years to deal with imbalanced data.*\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/725/0*FeIp1t4uEcW5LmSM.png)"},{"metadata":{},"cell_type":"markdown","source":"#### SMOTE"},{"metadata":{},"cell_type":"markdown","source":"In SMOTE (Synthetic Minority Oversampling Technique) we synthesize elements for the minority class, in the vicinity of already existing elements.\nAt a high level, SMOTE creates synthetic observations of the minority class [0] by:\n\n- Finding the k-nearest-neighbors for minority class observations (finding similar observations)\n- Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.\n\n![](https://miro.medium.com/max/734/0*_XpwY9GznmejI4WN.png)"},{"metadata":{},"cell_type":"markdown","source":"#### Tomek Links"},{"metadata":{},"cell_type":"markdown","source":"In this algorithm, we end up removing the majority element from the Tomek link, which provides a better decision boundary for a classifier.\n\n![](https://miro.medium.com/max/798/0*YWVxE7SbWKnTnbZi)"},{"metadata":{},"cell_type":"markdown","source":"You can find an overview of most popular methods and advantage and disadvantage here:\n\n[Imbalanced Data : How to handle Imbalanced Classification Problems](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/)"},{"metadata":{},"cell_type":"markdown","source":"### Base Model : Original Data"},{"metadata":{},"cell_type":"markdown","source":"Lets look how Random Forest models insurance imalanced data. As performance metrics I will use recall and ConfusionMatrix. \n\n*Note: The goal of your modeling in practical projects could be specified by the business. here our goal is better TP* \n"},{"metadata":{"papermill":{"duration":0.038651,"end_time":"2020-12-17T14:11:23.952202","exception":false,"start_time":"2020-12-17T14:11:23.913551","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"score_auc = []\nscore_recall = []\noof_rf = np.zeros(len(train))\npred_rf = np.zeros(len(test))\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    rf = RandomForestClassifier(n_estimators=150, max_depth=5, criterion='gini', max_features=0.8, n_jobs= -1, random_state=32)\n    rf.fit(trn_data, y_train)\n    oof_rf[val_ind] = rf.predict_proba(val_data)[:, 1]\n    y = rf.predict_proba(trn_data)[:, 1]\n    print('val auc:' , roc_auc_score(y_val, oof_rf[val_ind]))\n    print('val recall:' , recall_score(y_val, np.where(oof_rf[val_ind] > 0.5, 1, 0)))\n   \n    score_auc.append(roc_auc_score(y_val, oof_rf[val_ind]))\n    score_recall.append(recall_score(y_val, np.where(oof_rf[val_ind] > 0.5, 1, 0)))\n                        \n    pred_rf += rf.predict_proba(test)[:, 1]/folds.n_splits\n    \nprint(' Model auc: -------> ', np.mean(score_auc))\nprint(' Model recall: -------> ', np.mean(score_recall))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"oof_rf_01 = np.where(oof_rf > 0.5, 1, 0)\n\n\n\n\ncf_matrix = confusion_matrix(target, (oof_rf_01)) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='vlag_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\npred = np.where(pred_rf>0.50, 1, 0)\npred\n\nrecall_score(answers, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So as we can boost auc with different techniques but the result of Confusion Matrix is terrible (TP=0.0%) , so we will continue with resampling. "},{"metadata":{"papermill":{"duration":0.058993,"end_time":"2020-12-17T14:14:37.985795","exception":false,"start_time":"2020-12-17T14:14:37.926802","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### **Upsampling the wrong way !**"},{"metadata":{"papermill":{"duration":0.058036,"end_time":"2020-12-17T14:14:38.102225","exception":false,"start_time":"2020-12-17T14:14:38.044189","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\n\n\n>*If cross-validation is done on already upsampled data, the scores don't generalize to new data. In a real problem, you should only use the test set ONCE; we are reusing it to show that if we do cross-validation on already upsampled data, the results are overly optimistic and do not generalize to new data (or the test set).*"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:14:38.231193Z","iopub.status.busy":"2020-12-17T14:14:38.230482Z","iopub.status.idle":"2020-12-17T14:14:39.762771Z","shell.execute_reply":"2020-12-17T14:14:39.762081Z"},"papermill":{"duration":1.602224,"end_time":"2020-12-17T14:14:39.762906","exception":false,"start_time":"2020-12-17T14:14:38.160682","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"smote= SMOTE(sampling_strategy='minority', random_state=2020, k_neighbors=5) # used default imblearn parameters\n\ntrain_resampled, target_resampled = smote.fit_resample(train, target)\ntrain_resampled.shape, target_resampled.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_resampled)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:14:40.013047Z","iopub.status.busy":"2020-12-17T14:14:40.012331Z","iopub.status.idle":"2020-12-17T14:14:40.214902Z","shell.execute_reply":"2020-12-17T14:14:40.215456Z"},"papermill":{"duration":0.266066,"end_time":"2020-12-17T14:14:40.215641","exception":false,"start_time":"2020-12-17T14:14:39.949575","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"ax = sns.countplot(x = target_resampled ,palette=\"vlag\")\nsns.set(font_scale=1.5)\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfig = plt.gcf()\nfig.set_size_inches(10,5)\nax.set_ylim(top=700000)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(target_resampled)), (p.get_x()+ 0.3, p.get_height()+10000))\n\nplt.title('Distribution of Target')\n\nplt.ylabel('Frequency [%]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:14:40.484206Z","iopub.status.busy":"2020-12-17T14:14:40.479212Z","iopub.status.idle":"2020-12-17T14:25:44.197203Z","shell.execute_reply":"2020-12-17T14:25:44.198173Z"},"papermill":{"duration":663.794365,"end_time":"2020-12-17T14:25:44.19847","exception":false,"start_time":"2020-12-17T14:14:40.404105","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"score_auc = []\nscore_recall = []\n\noof_rf = np.zeros(len(train_resampled))\npred_rf = np.zeros(len(test))\n\nfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train_resampled, target_resampled)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train_resampled.iloc[train_ind], train_resampled.iloc[val_ind]\n    y_train, y_val = target_resampled.iloc[train_ind], target_resampled.iloc[val_ind]\n    \n    rf = RandomForestClassifier(n_estimators=150, max_depth=5, criterion='gini', max_features=0.8, min_samples_split=5, n_jobs= -1, random_state=32)\n    rf.fit(trn_data, y_train)\n    oof_rf[val_ind] = rf.predict_proba(val_data)[:, 1]\n    y = rf.predict_proba(trn_data)[:, 1]\n    #print('train:',roc_auc_score(y_train, y),'val :' , roc_auc_score(y_val, oof_rf[val_ind]))\n    \n    print('val auc:' , roc_auc_score(y_val, oof_rf[val_ind]))\n    print('val recall:' , recall_score(y_val, np.where(oof_rf[val_ind] > 0.5, 1, 0)))\n    \n    \n    \n    score_auc.append(roc_auc_score(y_val, oof_rf[val_ind]))\n    score_recall.append(recall_score(y_val, np.where(oof_rf[val_ind] > 0.5, 1, 0)))\n    pred_rf += rf.predict_proba(test)[:, 1]/folds.n_splits\n    \nprint(' Model auc: -------> ', np.mean(score_auc))\nprint(' Model recall: -------> ', np.mean(score_recall))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_rf_01 = np.where(oof_rf > 0.5, 1, 0)\ncf_matrix = confusion_matrix(target_resampled, (oof_rf_01)) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='vlag_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\npred = np.where(pred_rf>0.50, 1, 0)\n\nrecall_score(answers, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### *So our model is not generalized: cv recall = 0.94535924158125 , test recall= 0.9087221095334685*"},{"metadata":{"papermill":{"duration":0.093654,"end_time":"2020-12-17T14:25:47.415687","exception":false,"start_time":"2020-12-17T14:25:47.322033","status":"completed"},"tags":[]},"cell_type":"markdown","source":"To see why it happened, Let's say every data point from the minority class is copied 6 times before making the splits. If we did a 3-fold validation, each fold has (on average) 2 copies of each point! If our classifier overfits by memorizing its training set, it should be able to get a perfect score on the validation set! Our cross-validation will choose the model that overfits the most. We see that CV chose the deepest trees it could!\n\nInstead, we should split into training and validation folds. Then, on each fold:\n\n>1.Oversample the minority class\n>\n>2.Train the classifier on the training folds\n>\n>3.Validate the classifier on the remaining fold\n\n\n"},{"metadata":{"papermill":{"duration":0.094793,"end_time":"2020-12-17T14:25:47.60653","exception":false,"start_time":"2020-12-17T14:25:47.511737","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### **Upsampling the Right way !**"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:25:47.999672Z","iopub.status.busy":"2020-12-17T14:25:47.99892Z","iopub.status.idle":"2020-12-17T14:25:48.002526Z","shell.execute_reply":"2020-12-17T14:25:48.001877Z"},"papermill":{"duration":0.104789,"end_time":"2020-12-17T14:25:48.002676","exception":false,"start_time":"2020-12-17T14:25:47.897887","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"smote= SMOTE(sampling_strategy='minority', k_neighbors=5) # used default imblearn parameters\n\n\ntml = TomekLinks()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-17T14:25:48.204772Z","iopub.status.busy":"2020-12-17T14:25:48.203705Z","iopub.status.idle":"2020-12-17T14:35:57.246409Z","shell.execute_reply":"2020-12-17T14:35:57.245739Z"},"papermill":{"duration":609.148094,"end_time":"2020-12-17T14:35:57.246574","exception":false,"start_time":"2020-12-17T14:25:48.09848","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"score_auc = []\nscore_recall = []\n\n\noof_rf = np.zeros(len(train))\npred_rf = np.zeros(len(test))\n\nfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n\nfor fold_ , (train_ind, val_ind) in enumerate(folds.split(train, target)):\n    print('fold:', fold_, '  - Starting ...')\n    trn_data, val_data = train.iloc[train_ind], train.iloc[val_ind]\n    y_train, y_val = target.iloc[train_ind], target.iloc[val_ind]\n    \n    train_upsample, y_upsample = smote.fit_resample(trn_data, y_train)\n    rf = RandomForestClassifier(n_estimators=150, max_depth=5, criterion='gini', max_features=0.8, min_samples_split=5, n_jobs= -1, random_state=32)\n    rf.fit(train_upsample, y_upsample)\n    \n    oof_rf[val_ind] = rf.predict_proba(val_data)[:, 1]\n    \n    y = rf.predict_proba(train_upsample)[:, 1]\n    \n    print('val auc:' , roc_auc_score(y_val, oof_rf[val_ind]))\n    print('val recall:' , recall_score(y_val, np.where(oof_rf[val_ind] > 0.5, 1, 0)))\n    \n    score_auc.append(roc_auc_score(y_val, oof_rf[val_ind]))\n    score_recall.append(recall_score(y_val, np.where(oof_rf[val_ind] > 0.5, 1, 0)))\n            \n    \n    pred_rf += rf.predict_proba(test)[:, 1]/folds.n_splits\n    \nprint(' Model auc: -------> ', np.mean(score_auc))\nprint(' Model recall: -------> ', np.mean(score_recall))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noof_rf_01 = np.where(oof_rf > 0.5, 1, 0)\n\ncf_matrix = confusion_matrix(target, (oof_rf_01)) \ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nplt.figure(figsize=(8, 6))\nsns.set(font_scale=1.4)\nplt.style.use('seaborn-poster')\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='vlag_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.where(pred_rf>0.50, 1, 0)\nrecall_score(answers, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CV recall = 0.9057 , test recall = 0.9088\n\nSo not only  we have very fit model(cv and test recall are very consistent) but also we have better confusion matrix and recall in comparision with first model, and model upsampling the wrong way is completely overfit."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\n1. Considering appropriate metric is a key and also depends on your business use case and its goal if we want more TPs so auc wont be a good chice.\n2. Resampling is one of the techniques to handle imbalanced problem if we apply resampling the right way. some time othe techniques such as class weight or applying better algoritms or ensemble modeling could help more. \n3. We just focused on modeling,  feature engineering and selection in many cases could boost the performance regarding recall and confusion  matrix."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}