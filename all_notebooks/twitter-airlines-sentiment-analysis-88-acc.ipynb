{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport heapq\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport nltk\nimport keras\nfrom keras.models import Sequential \nfrom keras.preprocessing.text import one_hot\nfrom keras.layers import Dense,Dropout,LSTM,Embedding\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/twitter-airline-sentiment/Tweets.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.countplot(data=data,x='airline_sentiment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\nsns.countplot(data=data,x='airline')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(25,15)})\nsns.countplot(data=data,x='negativereason')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['text','airline_sentiment']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'][5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(inp_text):\n    stop = nltk.corpus.stopwords.words('english')\n    punc = string.punctuation\n    stop.append(punc)\n    whitelist = [\"n't\", \"not\", \"no\"]\n    clean_words = []\n    words = nltk.word_tokenize(inp_text)\n    for word in words:\n        if word not in stop or word not in whitelist and len(word)>1:\n            clean_words.append(word)\n    return \" \".join(clean_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_stopwords(data['text'][5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_mentions(input_text):\n        return re.sub(r'@ \\w+', '', input_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.text = data.text.apply(remove_stopwords).apply(remove_mentions)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2count = {}\n\nfor i in range(len(data['text'])):\n    words = nltk.word_tokenize(data['text'][i])\n    \n    for word in words:\n        if word not in word2count.keys():\n            word2count[word] = 1\n        else:\n            word2count[word] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Vocabluray of our corpus is: {}\".format(len(word2count)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = heapq.nlargest(10000,word2count,key=word2count.get)\nword_freq[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_text = []\nfor sentences in data['text']:\n    Z = one_hot(sentences,vocab_size)\n    onehot_text.append(Z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_text[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = 20\nembedded_sents = pad_sequences(onehot_text,padding='pre',maxlen=length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_sents[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = data['airline_sentiment']\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lb = LabelBinarizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = lb.fit_transform(labels)\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = embedded_sents\ny = labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.asarray(X)\ny = np.asarray(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X[:13000]\ny_train = y[:13000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X[13000:]\ny_test = y[13000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_training,X_valid,y_training,y_valid = train_test_split(X_train,y_train,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_training.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Traditional Deep Learning Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(512,activation='relu',input_shape=(20,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_training,y_training,validation_data=(X_valid,y_valid),epochs=100,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('loss'), ax=ax[0, 0])\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('accuracy'), ax=ax[0, 1])\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('val_loss'), ax=ax[1, 0])\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('val_accuracy'), ax=ax[1, 1])\nax[0, 0].set_title('Training Loss vs Epochs')\nax[0, 1].set_title('Training Accuracy vs Epochs')\nax[1, 0].set_title('Validation Loss vs Epochs')\nax[1, 1].set_title('Validation Accuracy vs Epochs')\nplt.suptitle('Traditional Deep learning model',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM RNN Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_feature_vectors = 40\nmodel1 = Sequential()\nmodel1.add(Embedding(vocab_size,embedding_feature_vectors,input_length=length))\nmodel1.add(Dropout(0.2))\nmodel1.add(LSTM(200,dropout=0.2,recurrent_dropout=0.3))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel1.add(Dense(3,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model1.fit(X_training,y_training,validation_data=(X_valid,y_valid),epochs=10,batch_size=32,verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('loss'), ax=ax[0, 0])\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('accuracy'), ax=ax[0, 1])\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('val_loss'), ax=ax[1, 0])\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('val_accuracy'), ax=ax[1, 1])\nax[0, 0].set_title('Training Loss vs Epochs')\nax[0, 1].set_title('Training Accuracy vs Epochs')\nax[1, 0].set_title('Validation Loss vs Epochs')\nax[1, 1].set_title('Validation Accuracy vs Epochs')\nplt.suptitle('LSTM RNN model',size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test accurcy using Traditional DL Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dl = model.evaluate(X_test,y_test)\nscore_dl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of Traditional DL model: {}\".format(score_dl[1]*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test Accuracy of LSTM RNN Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score_lstm = model1.evaluate(X_test,y_test)\nscore_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of LSTM RNN model: {}\".format(score_lstm[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}