{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Student Grade Prediction\n\nThis notebook consists of data analysis and prediction for the UCL Machine Learning Repository. [Link to the dataset](https://archive.ics.uci.edu/ml/datasets/student+performance)\n\nThe dataset's attributes include student grades, demographic, social and school-related features. The data was collected by using school reports and questionnaires.\n\nThere are three columns for the grades:\n- G1 - first-period grade\n- G2 - second-period grade\n- G3 - the final grade\n\nAll grades are numeric, from 0 to 20.\n\nThe goal of the notebook is to predict the final grade (G3). The G1 and G2 have high correlation to the final grade. For this purpose, it was decided to follow a suggestion on the kaggle competition page and predict the final grade without using periodic grades. This resolution allows to produce higher value to this notebook.\n\n\n#### This notebook consists of:\n- Data load\n- Data analysis\n- Data preparation\n- Regression using classical machine learning\n- Impact analysis\n- Regression using deep learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data load","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The first step of the research was loading the data and dropping the G1 and G2 columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd ../input/student-grade-prediction/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('student-mat.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['G1', 'G2'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step was checking if there is any missing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum(axis=0).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of samples is relatively low. For this reason, checking of the skewness of columns was performed. Correlation to the goal was checked as well.\n\nNote: to allow calculation of skewness and correlation to the final grade categorical data was LabelEncodered. The appropriate transformation (after investigation) will be performed later.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"labeled_data = data.apply(LabelEncoder().fit_transform)\nsummary = pd.concat([data.dtypes, data.nunique(), labeled_data.skew().abs(), labeled_data.corr()['G3']], axis=1).sort_values(2, ascending=False).head(10)\nsummary.columns=['type', 'unique_values', 'skewness', 'correlation_to_G3']\nsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Later the average skewness of all the columns was calculated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labeled_data.skew().abs().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most of features are highly skewed. This was expected as the number of samples is quite low. Wherefore, inspection all the features with skewness higher than 2 was executed.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"for column in summary[summary['skewness']>2].index:\n    sns.countplot(data[column])\n    plt.ylabel('Count')\n    plt.xlabel(column.capitalize())\n    plt.title('Distribution of the {} column'.format(column))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['higher'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The higher column contains information if a particular student desire to take higher education. Since the feature is highly skewed (as one option is ten times more represented then the second) the feature will be dropped as it might lead to false generalization.\nThe rest of the boolean categories have slightly better distribution, so no further action was needed.\n\nThe Dalc column (workday alcohol consumption) and failures (number of past class failures) have underrepresented options. So they will be grouped in more general options to reduce skewness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['higher'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of failures was regrouped into two options to reduce skewness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['failures'])\nplt.title('Distribution of the number of failed past classes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['failures'] = data['failures'].apply(lambda x: 'No' if x == 0  else 'Yes')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.countplot(data['failures'])\nplt.title('Distribution of the number of failed past classes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['Dalc'])\nplt.title('Distribution of workday alcohol consumption by students')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regroup_dalc(x):\n    if x == 1:\n        return 'very low'\n    elif x == 2:\n        return 'low'\n    else:\n        return 'considerable'\n    \ndata['Dalc'] = data['Dalc'].apply(regroup_dalc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['Dalc'])\nplt.title('Distribution of workday alcohol consumption by students')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final skewness are as follows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[['Dalc', 'failures']].apply(LabelEncoder().fit_transform).skew().abs()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the age distribution are as follows:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data['age'])\nplt.title('Distribution of students age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the low number of samples of students older than 19 y.o., they were removed from the dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['age'] < 20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data analysis\n\nThe data analysis was initiated\nby investigating the final grade (goal) distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\n\nplt.figure(figsize=(8, 6))\nsns.distplot(data['G3'])\nmu, sigma = norm.fit(data['G3'])\n\nplt.xlabel('G3 score')\nplt.ylabel('Frequency')\nplt.title('Final grade distribution')\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot shows a gap of just over 0 scores, which is the cause of bimodality of the distribution. The column will be transformed in the next paragraph to increase the performance of ML models.\n\nThe dataset contains data from two schools, the scores distribution was compared.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['school'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ax = sns.violinplot(x='school', y='G3', data=data)\nax.set_xticklabels(['Gabriel Pereira', 'Mousinho da Silveira'])\nplt.title('Comparison of scores distribution between the schools')\nplt.xlabel('School')\nplt.ylabel('Final grade')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of students in the two schools varies considerably, but the score distribution seems to look similar. The comparison has been made in function of students sex.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x='sex', y='G3', data=data)\nplt.xlabel('Sex')\nplt.ylabel('Final grade')\nplt.title('Comparison between male and female students')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Women tend to have better grades and have a lower number of 0 scores.\n\nThe next part of the study was a comparison of the scores in the function of the age of a student.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x='age', y='G3', data=data)\nplt.xlabel('Age')\nplt.ylabel('Final grade')\nplt.title('Comparison between male and female students')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of students, mean score, and dispersion of scores all decrease with age.\n\nThe next step was the analysis of the three highest correlating attributes to the G3 score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labeled_data = data.apply(LabelEncoder().fit_transform)\nsummary = pd.concat([labeled_data.corr()['G3']], axis=1).sort_values('G3', ascending=False).head(4)\nsummary.columns=['Correlation to G3']\nsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Medu and Fedu columns contain information about students Mother and Father education, respectively. Whereas, the reason column contains information about the reason for selecting a particular school.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x='Medu', y='G3', data=data)\nplt.xlabel(\"Mother's education\")\nplt.ylabel('Final grade')\nplt.title(\"Impact of mother's education on the students final grade\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_group_Medu = data.groupby('Medu')['G3']\ndata_Medu = pd.DataFrame([data_group_Medu.count(), data_group_Medu.mean()])\ndata_Medu = data_Medu.T\ndata_Medu.columns = ['Count', 'Average']\ndata_Medu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x='Fedu', y='G3', data=data)\nplt.xlabel(\"Fathers's education\")\nplt.ylabel('Final grade')\nplt.title(\"Impact of fathers's education on the students final grade\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_group_Fedu = data.groupby('Fedu')['G3']\ndata_Fedu = pd.DataFrame([data_group_Fedu.count(), data_group_Fedu.mean()])\ndata_Fedu = data_Fedu.T\ndata_Fedu.columns = ['Count', 'Average']\ndata_Fedu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An increase in students grades in the function of parent's education can be easily seen. The high average on 0s can be ignored as the number of samples is low.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.swarmplot(x='reason', y='G3', data=data, order=['course', 'home', 'reputation', 'other'])\nax.set_xticklabels(['Courses ', '  Close to home  ', \"  School's rep.\", 'Other'])\nplt.xlabel(\"Reson of choosing the school\")\nplt.ylabel('Final grade')\nplt.title(\"The students final grade in function of reason of selecting particular school\")\nplt.figure(figsize=(10, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_group_reason = data.groupby('reason')['G3']\ndata_reason = pd.DataFrame([data_group_reason.count(), data_group_reason.mean()])\ndata_reason = data_reason.T\ndata_reason.columns = ['Count', 'Average']\ndata_reason","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The students, which selected the school because it was near their home or the school had a preferred course, have lower scores.\n\nThe prepared correlation matrix is shown below.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"labeled_data = data.apply(LabelEncoder().fit_transform)\nplt.figure(figsize=(14, 10))\nsns.heatmap(labeled_data.corr().abs(), vmax=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heatmap legend has the upper boundary set to 0.4 to increase readability.\n\nThe heatmap shows a high correlation between features: \n- mother's and father's education and job\n- alcohol consumption (both workday and weekend) and amount of going out with friends\n- address (urban or rural) and travel time","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data preparation for ML models\n\nFirst, there was investigated if all numerical attributes are ordered.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns = data.dtypes[data.dtypes != 'object'].index\nfor column in numerical_columns.drop('G3'):\n    sns.countplot(data[column])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All these columns are ordered (double-checked with the dataset description).\n\nThe distances might not be exactly equal between the fields where students selected values from 1 to 5. But taking the number of samples into account, the values were decided to remain numeric.\n\nNextly it has been checked whether the object features are not continuous. If so they should be labelled.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"object_columns = data.dtypes[data.dtypes == 'object'].index\nfor column in object_columns:\n    sns.countplot(data[column])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All these values except Dalc column, are not ordered. The Dalc column was decided to be labelled as the distance between low and considerable is higher than between low and very low.\n\nSo all the object columns can be labelled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following step, the goal distribution was analyzed. Furthermore, an attempt was made to normalize the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.distplot(data['G3'])\nmu, sigma = norm.fit(data['G3'])\n\nplt.xlabel('G3 score')\nplt.ylabel('Frequency')\nplt.title('Final grade distribution')\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['G3'] = np.log1p(data['G3'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.distplot(data['G3'])\nmu, sigma = norm.fit(data['G3'])\n\nplt.xlabel('G3 score')\nplt.ylabel('Frequency')\nplt.title('Final grade distribution')\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The log1p succeded in normalizing the data. But the gap still remains as any of transformation cannot remove it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dividing the data into train and test set:\n\n85% - train\n\n15% - test\n\nCV set will be divided from train set when it is needed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, test_X, train_Y, test_Y = train_test_split(data.drop(['G3'], axis=1), data['G3'], train_size=0.85, shuffle=True, random_state=1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having all features in a similar range can improve the performance of most of ML models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ntrain_X = scaler.fit_transform(train_X)\ntest_X = scaler.transform(test_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression using classical machine learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This chapter starts with a simple linear regression before checking more robust models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinear_model = LinearRegression()\nlinear_model.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Later, the scorer was created. MSE (mean squared error) was selected as in this metric high errors are punished more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nscorer = make_scorer(mean_squared_error, greater_is_better=True, squared=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scorer(linear_model, test_X, test_Y)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def regression_graph(y_true, y_pred, model_name):\n    plt.figure(figsize=(12, 8))\n    n = len(y_true)\n    ax = sns.scatterplot(x=range(n), y=y_true)\n    ax = sns.scatterplot(x=range(n), y=y_pred, marker=\"s\", s=45)\n    ax = plt.vlines(range(n), y_true, y_pred, linestyles='dotted')\n\n    plt.legend(title='', loc='upper right', labels=['Actual values', 'Predicted values'])\n    plt.xlabel('Number of the sample')\n    plt.ylabel('Final score')\n    plt.title('Predicted final scores using the {} model'.format(model_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_graph(np.expm1(test_Y), np.expm1(linear_model.predict(test_X)), 'linear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The errors are quite high in most of the cases, but there are some accurate predictions as well. The model avoids predicting 0s because predicting 0s leads to high penalty if a student score is a positive value.\n\nThe further move was trying more robust models. The best configurations of hyperparameters were found by using the GridSearch.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_scores = pd.DataFrame(columns=['CV score', 'Test score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import ensemble\nfrom sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV\n\n\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 10, 30, 60, 100, 300, 600, 1000], scoring = scorer)\nridge.fit(train_X, train_Y)\nresult = cross_val_score(ridge, train_X, train_Y, scoring = scorer, cv = 10, n_jobs=-1).mean()\nridge.fit(train_X, train_Y)\nresult_test = scorer(ridge, test_X, test_Y)\nmodel_scores.loc['ridge'] = [result, result_test]\n\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1], \n                max_iter = 50000)\nlasso.fit(train_X, train_Y)\nresult = cross_val_score(lasso, train_X, train_Y, scoring = scorer, cv = 10, n_jobs=-1).mean()\nlasso.fit(train_X, train_Y)\nresult_test = scorer(lasso, test_X, test_Y)\nmodel_scores.loc['lasso'] = [result, result_test]\n\n\nparams_GBR_grid = {'n_estimators': (100, 500), 'max_depth': (2, 4), 'min_samples_split':(1, 2), 'learning_rate': (0.05,), 'min_samples_leaf': (3, 5, 10), 'max_features': ('sqrt',), 'loss': ('huber',)}\nGBR = ensemble.GradientBoostingRegressor()\nGBR_GS = GridSearchCV(GBR, params_GBR_grid, scoring=scorer, n_jobs=-1)\nGBR_GS.fit(train_X, train_Y)\nGBR = ensemble.GradientBoostingRegressor(**GBR_GS.best_params_)\nGBR.fit(train_X, train_Y)\nresult = cross_val_score(GBR, train_X, train_Y, scoring = scorer, cv = 10, n_jobs=-1).mean()\nresult_test = scorer(GBR, test_X, test_Y)\nmodel_scores.loc['GRB'] = [result, result_test]\n\n\n\nelastic_net = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelastic_net.fit(train_X, train_Y)\nresult = cross_val_score(elastic_net, train_X, train_Y, scoring = scorer, cv = 10, n_jobs=-1).mean()\nelastic_net.fit(train_X, train_Y)\nresult_test = scorer(elastic_net, test_X, test_Y)\nmodel_scores.loc['elastic net'] = [result, result_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBR_GS.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBR.get_params()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GBR model gave the best predictions on the test set. Rest of the models generalize just better than the linear regression model.\n\nBelow are presented regression graphs for all of the models.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for name, model in {'Ridge': ridge, 'Lasso': lasso, 'Gradient Boosting Regressor': GBR, 'Elastic Net': elastic_net}.items():\n    regression_graph(np.expm1(test_Y), np.expm1(model.predict(test_X)), name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The GBR model avoids making high errors which are highly penalized by the MSE loss function. The model predicted multiple samples correctly but on the same time there are many high errors.\n\nThe ridge model has many accurate predictions, but at the same time, it has a few predictions quite different from true values, which make the test score low.\n\nThe ridge, lasso and elastic net models generally predict scores in a narrow range in the average grade neighbourhood. This is expected as these models include parameters regularization.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The linear model was used for visualization of feature importance, as it's easier to retrieve coefficients than on the GBR model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fething coeficients, their name and rescalling to percents\nimportance = pd.Series(linear_model.coef_)\nimportance = importance / importance.abs().sum()*100\npd.concat([pd.Series(data.drop(['G3'], axis=1).columns), importance.abs()], axis=1).sort_values(1, ascending=False)\ngraph_data = pd.concat([pd.Series(data.drop(['G3'], axis=1).columns), importance], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ordering data\ngraph_data = graph_data.sort_values(1, ascending=True)\ngraph_data = graph_data.reset_index()\ngraph_data[0] = pd.Categorical(graph_data[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the graph\nfig = plt.figure(figsize=(12,14), dpi= 100)\nax = fig.add_subplot(111)\nax.yaxis.tick_right()\nplt.hlines(y=graph_data.index, xmin=0, xmax=graph_data[1], color='black', alpha=.8, linewidth=.8)\nplt.scatter(graph_data[1], graph_data.index, color='black')\nplt.yticks(graph_data.index, graph_data[0])\nplt.xticks(fontsize=12)\nplt.xlabel('Impact [%]', fontsize=12)\n\n# Decorate\nplt.title('Impact of the features on the final grade', fontdict={'size':20})\nplt.grid(linestyle='dotted', alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression using deep learning\n\nThe next step was checking if a neural network can beat the GBR model score. Due to the small number of samples, different configurations of NN were compared using the test dataset. This is not the best practise, but diving a CV set from the train data lead to highly variating results variating on the randomness of the split. \n\nBecause neural networks have more configurations available than the previous models, GridSearching was performed at multiple steps. The search consisted of finding the optimum number of:\n- Layers\n- Nodes in the layers\n- Dropout ratio\n- Optimizer\n- Activation function\n\n\nThe last iteration of gridsearching can be found below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras as tf\nfrom sklearn.model_selection import ParameterGrid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_deep_model(first_layer=50, second_layer=0, third_layer=0, dropout=0.1, optimizer='Adamax', activation='relu'):\n    n = len(train_X[0])\n    \n    if second_layer > 0:\n        deep_model = tf.models.Sequential([\n        tf.layers.Dense(first_layer, input_shape=(n,), activation=activation),\n        tf.layers.Dropout(dropout),\n        tf.layers.Dense(second_layer, activation=activation),\n        tf.layers.Dropout(dropout),\n        tf.layers.Dense(1, activation=None)\n        ])\n    else:\n        deep_model = tf.models.Sequential([\n        tf.layers.Dense(first_layer, input_shape=(n,), activation=activation),\n        tf.layers.Dropout(dropout),\n        tf.layers.Dense(1, activation=None)\n        ])\n    \n    deep_model.compile(optimizer=optimizer, loss='mse')\n    callbacks = [tf.callbacks.EarlyStopping(patience=4)]\n    iter_history = deep_model.fit(train_X, train_Y, validation_split=0.1, epochs=100, verbose=0, callbacks=callbacks)\n    val_score = scorer(deep_model, test_X, test_Y) #\n    \n    \n    return deep_model, val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deep_results = pd.DataFrame(columns=['Test score', 'Params'])\n\nparams = dict(first_layer=[30, 20, 10, 5], second_layer=[20, 15, 10, 5, 0], third_layer=[0, ], dropout=[0.0, 0.1], \n              optimizer=['RMSprop', 'Adamax', 'Adam'], activation=['relu', 'selu', 'elu'])\n\ndeep_params_grid = ParameterGrid(params)\n\n\nfor param in deep_params_grid:\n    _, result = create_deep_model(**param)\n    deep_results.loc[len(deep_results)] = [result, param]\n\npd.set_option('display.max_colwidth', -1)\ndeep_results.sort_values('Test score').head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters_deep = deep_results[deep_results['Test score']==deep_results['Test score'].min()]['Params'].values[0]\n\ndeep_model, res = create_deep_model(**best_parameters_deep)\nprint(res)\n\nregression_graph(np.expm1(test_Y), np.expm1(deep_model.predict(test_X)[:, 0]), 'deep learning')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The small number of samples caused the NN networks to produce different results each time a model was trained. This was caused by a random selection of samples for CV in each epoch of training. Which occurs by retrieving various test scores each time model is trained. The randomness makes the NN model less reliable than the GBR model.\n\n#### Summary:\nTaking into account:\n- the type of data (most of the data avaible has indirect impact on the final score)\n- removal of the periodic grades data\n- the small number of samples\n\nthe regression results are satisfactory.\n\n\nThe models generaly dend to avoid predicting students failing the class, as predicting a zero score for a student which acctualy passed generates high penalty. The next step could be including the gap between 0 and 4 scores into a model architecture to produce better results.\n\n\nCollecting more data, especially for the students with the less represented selections, would improve model and regression performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}