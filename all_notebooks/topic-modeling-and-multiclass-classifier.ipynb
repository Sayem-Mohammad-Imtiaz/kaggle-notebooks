{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# plt.style.use('g')\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/janatahack-independence-day-2020-ml-hackathon/train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/janatahack-independence-day-2020-ml-hackathon/test.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = pd.read_csv(\"../input/janatahack-independence-day-2020-ml-hackathon/sample_submission_UVKGLZE.csv\")\nsample_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test_df.ID.values\ntest_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = list(train_df.columns.values)[3:]\nvalues = list(train_df.iloc[:,3:].sum(axis =0).values)\n\nsns.set(font_scale =1)\nfig = plt.figure(figsize =(10,8))\n\nax = sns.barplot(categories,values)\nax.set_xlabel(\"categories\")\nax.set_ylabel(\"number of topics and abstracts\")\nax.set_title(\"total topics in each category\")\nplt.xticks(rotation=90)\n\n\nrects = ax.patches\nlabels = values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's visualize the word cloud for Computer Sciecne category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subset = train_df[train_df['Computer Science'] == True]\nsubset = subset.TITLE.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\ncse_wc = WordCloud(\n            background_color='black',\n            max_words=3000,\n            stopwords= stopwords,\n            width =600,height = 400).generate(\" \".join(subset))\n\nfig  = plt.figure(figsize=(14,8))\nplt.imshow(cse_wc)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\nimport warnings\nimport sys\n\ndata_train = train_df.copy()\ndata_test = test_df.copy()\n\n\ndata_train['TITLE'] = data_train[['TITLE','ABSTRACT']].apply(lambda x: \" \".join(x),axis =1)\ndata_test['TITLE'] = data_test[['TITLE','ABSTRACT']].apply(lambda x: \" \".join(x),axis =1)\n\ndel data_train['ABSTRACT']\ndel data_test['ABSTRACT']\n\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanPunctuation(sentence):\n    cl_sentence  = re.sub(r'[?|!|\\'|\"|#|\\_]',r'',sentence)\n    cl_sentence = re.sub(r'[.|,|)|(|\\|/]',r'',cl_sentence)\n    cl_sentence = re.sub(r'\\\\',r'',cl_sentence)\n    cl_sentence = cl_sentence.strip()\n    cl_sentence = cl_sentence.replace(\"\\n\",\" \")\n    return cl_sentence\n\ndef keepAlphabet(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-z]+',' ',word)\n        alpha_sent +=alpha_word\n        alpha_sent+=\" \"\n    alpha_sent =alpha_sent.strip()\n    \n    return alpha_sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['TITLE'] = data_train['TITLE'].str.lower()\ndata_train['TITLE'] = data_train['TITLE'].str.lower()\n\ndata_train['TITLE'] = data_train['TITLE'].apply(cleanPunctuation)\ndata_test['TITLE'] = data_test['TITLE'].apply(cleanPunctuation)\n\ndata_train['TITLE'] = data_train['TITLE'].apply(keepAlphabet)\ndata_test['TITLE'] = data_test['TITLE'].apply(keepAlphabet)\n\nprint(\"Train data \\n\",data_train['TITLE'].values[0],\"\\n\")\n\nprint(\"Test data \\n\",data_test['TITLE'].values[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(stopwords.words('english'))\n# print(stopwords)\nre_stop_words = re.compile(r\"\\b(\"+\"|\".join(stopwords) + \")\\\\W\", re.I)\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\" \", sentence)\n\ndata_train['TITLE'] = data_train['TITLE'].apply(removeStopWords)\ndata_test['TITLE'] = data_test['TITLE'].apply(removeStopWords)\n\nprint(\"Train data \\n\",data_train['TITLE'].values[0])\n\nprint(\"Test data \\n\",data_test['TITLE'].values[0])\n\n# data_train['TITLE'].values[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Stemming","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import wordnet\nfrom nltk.stem import WordNetLemmatizer\nword_len  = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new_train = data_train.copy()\ndata_new_test = data_test.copy()\n\ndef stemming(sentence):\n    stemSentence =[]\n    \n    for word in sentence.split():\n        stem  = word_len.lemmatize(word)\n        stemSentence.append(stem)\n        \n\n    return \" \".join(stemSentence)\n\ndata_new_train['TITLE'] = data_new_train['TITLE'].apply(stemming)\ndata_new_test['TITLE'] = data_new_test['TITLE'].apply(stemming)\n\nprint(\"Train data \\n\",data_train['TITLE'].values[0])\n\nprint(\"Test data \\n\",data_test['TITLE'].values[0])\n\n# data_new['TITLE'].head().values[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2_train = data_new_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain,dev = train_test_split(data2_train,random_state =4,test_size = 0.20,shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = data2_train.columns.values\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categories = col[2:]\nprint(categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = train['TITLE']\ndev_text = dev['TITLE']\ntest_text = data_new_test['TITLE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizor = TfidfVectorizer(strip_accents='unicode',analyzer='word',ngram_range=(1,2),norm = 'l2')\nvectorizor.fit(train_text)\nvectorizor.fit(test_text)\nvectorizor.fit(dev_text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx_train= vectorizor.transform(train_text)\nx_dev= vectorizor.transform(dev_text)\nx_test = vectorizor.transform(test_text)\n\ny_train = train.drop(labels=['ID','TITLE'],axis = 1)\ny_dev = dev.drop(labels=['ID','TITLE'],axis = 1)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(x_dev.shape)\nprint(y_dev.shape)\n\n# x_train.toarray()\n# x_dev = x_dev.toarray()\n# x_test = x_test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"1. # Use of oneVRest classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score,accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categories = [data]\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"logReg_pipeline = Pipeline([('oneVrest',OneVsRestClassifier(LogisticRegression(solver ='sag'),n_jobs = -1)),])\npred_arr = []\nfinal_pred = []\nfor cat in categories:\n    print(\".....processing.....{} category...\".format(cat))\n#     print(cat)\n    logReg_pipeline.fit(x_train,train[cat])\n    pred = logReg_pipeline.predict(x_dev)\n    f_pred = logReg_pipeline.predict(x_test)\n    pred_arr.append(pred)\n    final_pred.append(f_pred)\n    \n    print(\"f1 score = {}\".format(accuracy_score(dev[cat],pred)))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.array(pred_arr).T\ny_pred.shape\ny_dev.shape\nprint(\"f1 score for oneVRest classifier = \",f1_score(y_dev,y_pred,average ='micro'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = np.array(final_pred).T\nprint(lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\ndel data_new_train\ndel data_train\ndel test_df\ndel data_test\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Level Powerset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.problem_transform import LabelPowerset\nclassifier = LabelPowerset(LogisticRegression(solver= 'sag',n_jobs = -1,class_weight ='balanced'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Uncomment the below code to run it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifier.fit(x_train, y_train)\n# pred_lp = classifier.predict(x_dev)\n# lst = classifier.predict(x_test)\n# print(\"f1 score =\",f1_score(y_dev,pred_lp,average =\"micro\"))\n# lst =lst.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID = np.asanyarray(test_id)\ndf_submit = pd.DataFrame(lst,columns =categories)\ndf_submit['ID'] = ID\ndf_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = df_submit.columns.tolist()\ncol_n = col[-1:]\n\nfor ele in range(len(col)-1):\n    col_n.append(col[ele])\n    \ndf_submit = df_submit[col_n]\ndf_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compression_opts = dict(method='zip',\n                        archive_name='submission4.csv')  \ndf_submit.to_csv('out13.zip', index=False,\n          compression=compression_opts)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}