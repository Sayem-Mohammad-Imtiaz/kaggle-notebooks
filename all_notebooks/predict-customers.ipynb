{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\ndata = pd.read_csv(\"../input/Wholesale customers data.csv\")\ndisplay(data.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37f2d4c3a41bf03753eb69095968e2aad3aeb873"},"cell_type":"code","source":"pd.plotting.scatter_matrix(data, alpha=0.5, figsize=(14,8), diagonal = 'kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6013d52d03e7751a47d6f31e2944d35fed74cbdd"},"cell_type":"code","source":"new_data = data.drop([\"Channel\", \"Region\"], axis=1)\ndisplay(new_data.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"770b3ad7d7fd417cf0d1671eeb98efad14148b5b"},"cell_type":"code","source":"indexs = [1,10,90]\nsamples = pd.DataFrame(new_data.loc[indexs], columns = new_data.keys()).reset_index(drop = True)\ndisplay(samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f20ead5d8152bd6f7389176d8f7dbcd4f53a593"},"cell_type":"code","source":"#Feature scaling\nnew_data = np.log(new_data)\npd.plotting.scatter_matrix(new_data, alpha=0.5, figsize=(14,8), diagonal=\"kde\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41bc8adfc6b117075b8a56e3d88a5869df0d676e"},"cell_type":"code","source":"# Outlier detection\noutlier_index = []\nfor feature in new_data.keys():\n    q1 = np.percentile(new_data[feature], 25)\n    q3 = np.percentile(new_data[feature], 75)\n    step = (q3-q1)*1.5\n    print (\"Data points considered outliers for the feature '{}':\".format(feature))\n    outlier_data = new_data[~((new_data[feature] >= q1 - step) & (new_data[feature] <= q3 + step))]\n    outlier_index += list(outlier_data.index.values)\n    display(outlier_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0f4fdcaa56fca389406bdc019de9df0823dd782"},"cell_type":"code","source":"good_data = new_data.drop(list(set(outlier_index))).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"_uuid":"2d28254e1417b272eb14ea9f2ac14098d1e19f55"},"cell_type":"code","source":"# refer from udacity visual.py\n\n\n###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic('matplotlib', 'inline')\n###########################################\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport pandas as pd\nimport numpy as np\n\ndef pca_results(good_data, pca):\n\t'''\n\tCreate a DataFrame of the PCA results\n\tIncludes dimension feature weights and explained variance\n\tVisualizes the PCA results\n\t'''\n\n\t# Dimension indexing\n\tdimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n\t# PCA components\n\tcomponents = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n\tcomponents.index = dimensions\n\n\t# PCA explained variance\n\tratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n\tvariance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n\tvariance_ratios.index = dimensions\n\n\t# Create a bar plot visualization\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Plot the feature weights as a function of the components\n\tcomponents.plot(ax = ax, kind = 'bar');\n\tax.set_ylabel(\"Feature Weights\")\n\tax.set_xticklabels(dimensions, rotation=0)\n\n\n\t# Display the explained variance ratios\n\tfor i, ev in enumerate(pca.explained_variance_ratio_):\n\t\tax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n\n\t# Return a concatenated DataFrame\n\treturn pd.concat([variance_ratios, components], axis = 1)\n\ndef cluster_results(reduced_data, preds, centers, pca_samples):\n\t'''\n\tVisualizes the PCA-reduced cluster data in two dimensions\n\tAdds cues for cluster centers and student-selected sample data\n\t'''\n\n\tpredictions = pd.DataFrame(preds, columns = ['Cluster'])\n\tplot_data = pd.concat([predictions, reduced_data], axis = 1)\n\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# Color the points based on assigned cluster\n\tfor i, cluster in plot_data.groupby('Cluster'):   \n\t    cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i)*1.0/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);\n\n\t# Plot centers with indicators\n\tfor i, c in enumerate(centers):\n\t    ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \\\n\t               alpha = 1, linewidth = 2, marker = 'o', s=200);\n\t    ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);\n\n\t# Plot transformed sample points \n\tax.scatter(x = pca_samples[:,0], y = pca_samples[:,1], \\\n\t           s = 150, linewidth = 4, color = 'black', marker = 'x');\n\n\t# Set plot title\n\tax.set_title(\"Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\\nTransformed Sample Data Marked by Black Cross\");\n\n\ndef biplot(good_data, reduced_data, pca):\n    '''\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https://github.com/teddyroland/python-biplot\n    '''\n\n    fig, ax = plt.subplots(figsize = (14,8))\n    # scatterplot of the reduced data    \n    ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], \n        facecolors='b', edgecolors='b', s=70, alpha=0.5)\n    \n    feature_vectors = pca.components_.T\n\n    # we use scaling factors to make the arrows easier to see\n    arrow_size, text_pos = 7.0, 8.0,\n\n    # projections of the original features\n    for i, v in enumerate(feature_vectors):\n        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n                  head_width=0.2, head_length=0.2, linewidth=2, color='red')\n        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color='black', \n                 ha='center', va='center', fontsize=18)\n\n    ax.set_xlabel(\"Dimension 1\", fontsize=14)\n    ax.set_ylabel(\"Dimension 2\", fontsize=14)\n    ax.set_title(\"PC plane with original feature projections.\", fontsize=16);\n    return ax\n    \n\ndef channel_results(reduced_data, outliers, pca_samples):\n\t'''\n\tVisualizes the PCA-reduced cluster data in two dimensions using the full dataset\n\tData is labeled by \"Channel\" and cues added for student-selected sample data\n\t'''\n\n\t# Check that the dataset is loadable\n\ttry:\n\t    full_data = pd.read_csv(\"customers.csv\")\n\texcept:\n\t    print (\"Dataset could not be loaded. Is the file missing?\")\n\t    return False\n\n\t# Create the Channel DataFrame\n\tchannel = pd.DataFrame(full_data['Channel'], columns = ['Channel'])\n\tchannel = channel.drop(channel.index[outliers]).reset_index(drop = True)\n\tlabeled = pd.concat([reduced_data, channel], axis = 1)\n\t\n\t# Generate the cluster plot\n\tfig, ax = plt.subplots(figsize = (14,8))\n\n\t# Color map\n\tcmap = cm.get_cmap('gist_rainbow')\n\n\t# Color the points based on assigned Channel\n\tlabels = ['Hotel/Restaurant/Cafe', 'Retailer']\n\tgrouped = labeled.groupby('Channel')\n\tfor i, channel in grouped:   \n\t    channel.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n\t                 color = cmap((i-1)*1.0/2), label = labels[i-1], s=30);\n\t    \n\t# Plot transformed sample points   \n\tfor i, sample in enumerate(pca_samples):\n\t\tax.scatter(x = sample[0], y = sample[1], \\\n\t           s = 200, linewidth = 3, color = 'black', marker = 'o', facecolors = 'none');\n\t\tax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker='$%d$'%(i), alpha = 1, s=125);\n\n\t# Set plot title\n\tax.set_title(\"PCA-Reduced Data Labeled by 'Channel'\\nTransformed Sample Data Circled\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14add9b0cb6ec8c36dd1277a52b7f81742e0e763"},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=6, random_state=0)\npca.fit(good_data)\npca_results = pca_results(good_data, pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ae1fe9cb8e2f786d398cf203d64dc99682f8e5"},"cell_type":"code","source":"pca = PCA(n_components=2, random_state=0)\npca.fit(good_data)\nreduced_data = pca.transform(good_data)\nreduced_data = pd.DataFrame(reduced_data, columns=['Dimension 1', 'Dimension 2'])\nbiplot(good_data, reduced_data, pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"019a1fbd19137f9a1480dab32c994ea589e95c63"},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\ndef build_model(n, data):\n    gm = GaussianMixture(n_components=n, random_state=0)\n    gm.fit(data)\n    preds = gm.predict(data)\n    score = silhouette_score(data, preds)\n    return score, gm\n\nfor i in range(2, 11):\n    score, model = build_model(i, good_data)\n    print(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db7c32371941208b25e873d34566435a4fde6375"},"cell_type":"code","source":"score, best_model  = build_model(2, reduced_data)\npreds = best_model.predict(reduced_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21dda6ff37c13b268bc2d6fbde17883331bd9747"},"cell_type":"code","source":"log_samples = np.log(samples)\npca_samples = pca.transform(log_samples)\nsample_preds = best_model.predict(pca_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68cbf97abb53dc723399caf29848aa2edcbb734a"},"cell_type":"code","source":"cluster_results(reduced_data, preds, best_model.means_, pca_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0b9fb6603c4190d6af149f0f60dc82aeab4910a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}