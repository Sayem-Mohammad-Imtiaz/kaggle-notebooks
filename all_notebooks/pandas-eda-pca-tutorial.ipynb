{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LEARNING PANDAS ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load data file into pandas dataframe","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/pokemon/Pokemon.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read summary of a dataframe including dtype and non-null value count.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Shape function tells shape or dimension of dataframe i.e no. of rows & columns.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read headers i.e column name\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Change column name '#' to 'id'\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df.columns.tolist()\ncolumns[0] = 'id'\ndf.columns = columns\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Check total null values in each column of dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read any particular column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Name'][0:5]\n\n##Read more than one columns\ndf[['Name', 'Attack','Type 2']][0:4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read more than 1 column at a time and store in new dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df[['Name', 'Type 1', 'Attack']]\ndf_tmp.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read any row using iloc function using its index","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read a value from specific row & column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[3,2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Iterate through each row","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# for index, row in df.iterrows():\n#     ##print(index, row)\n#     print(index, row['Name'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Apply conditional statements and read only those who satisfies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['Type 1'] == 'Fire'][0:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sorting data with respect to a particular/specify column ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Sort dataframe in ascending order(default) through a particular column.\ndf.sort_values('Attack')\n\n##Sort dataframe in descending order.\ndf.sort_values('Attack', ascending =False)\n\n##Sort dataframe using multiple columns with different order\ndf.sort_values(['Attack', 'Defense'], ascending =(0,1))[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Manipulation :- Adding new column in dataframe \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Total'] = df['HP'] + df['Attack'] + df['Defense'] + df['Sp. Atk'] +df['Sp. Def'] +df['Speed']\ndf.head(3)\n\n##Add new column using iloc[row, column].sum(axis = 1 for horizontal sum & 0 for vertical sum) \ndf['Total_iloc_use'] = df.iloc[:, 4:10].sum(axis = 1)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop columns in a dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns =['Total_iloc_use'])\ndf[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save modified data in new data file(.csv, .xlsx)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Save your datsframe as new csv file without index\ndf.to_csv('modified_new.csv', index=False)\n\n##Save in excel format i.e xlsx\ndf.to_excel('modified_new_excel.xlsx', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filtering Dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[(df['Type 1'] =='Grass') & (df['Type 2']=='Poison') & (df['Attack'] >=69)][:4]\n\n##Save this filtered data in new dataframe\nnew_df = df.loc[(df['Type 1'] =='Grass') & (df['Type 2']=='Poison') & (df['Attack'] >=69)][:4]\n\n##new_df.head(3)\n\n##Reset index of new dataframe\nnew_df.reset_index(drop =True, inplace=True)\nnew_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Find value in 'Name' column which contains mega word\n\ndf.loc[df['Name'].str.contains('Mega')][:4]\n\n##Remove those Name from data which contains mega word  \n\ndf.loc[~df['Name'].str.contains('Mega')].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regex library for further filtering operations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n##Find 'Name' which starts with 'pi' in Name column\n\ndf.loc[df['Name'].str.contains('^pi[a-z]*', flags =re.I, regex= True)].head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conditional changes in dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Changing the value 'Fire' to 'Flamer' in the 'Name' column \n\ndf.loc[df['Type 1'] =='Fire', 'Type 1'] ='Flamer'\ndf\n\n##Make all fire type pokemon legendery\ndf.loc[df['Type 1'] =='Flamer', 'Legendary'] =True\ndf\n\n##Add new column and assign true value whose attack value is greater than 69\ndf.loc[df['Attack'] > 69, 'Beast'] = True\ndf.loc[df['Attack'] < 69, 'Beast'] = False\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Read modified data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mod = pd.read_csv('modified_new.csv')\ndf_mod.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aggregate functions i.e groupby(), sum(), count() and mean()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Aggregate Statistics will be done with groupby(), sum(), count() and mean() function.\n\n##Use groupby function to find mean of all Type 1 pokemon and sort in ascending order w.r.t Attack.\n\ndf_mod.groupby(['Type 1']).mean().sort_values('Attack', ascending=False)\n\n##Sum up all Type 1 pokemon features\ndf_mod.groupby(['Type 1']).sum()\n\n##Count number of Type 1 pokemon in dataframe\ndf_mod.groupby(['Type 1']).count()\n\n##Make another column count to count Type 1 pokemon more efficiently &  it is helpful in big data \ndf_mod['count'] =1\n\ndf_mod.groupby(['Type 1']).count()['count']\n\n##Apply multiple parameters \ndf_mod.groupby(['Type 1', 'Type 2']).count()['count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Read chunk size data when data is too big\n##For example read 5 rows at a time\n\n# for df_mod in pd.read_csv('modified_new.csv', chunksize=5):\n#     print(\"Chunk Df\")\n#     print(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA VISUALIZATION","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Find correlation between numeric variables in dataframe\nCorrelation :- Calculate relationship between two numerical variables.\n\nExcluding null valuees & excluding the categorical variables to find the Pearson's correlation\n\n• Positive correlation – the other variable has a tendency to also increase\n\n• Negative correlation – the other variable has a tendency to decrease\n\n• No correlation – the other variable does not tend to either increase or decreas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check number of variable available under numeric_data\nnumeric_data = df.select_dtypes(exclude = [object])\nnumeric_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = numeric_data.corr()\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,20))\nsns.heatmap(corr_matrix,annot=True,cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### In the heatmap of correlation matrix the darker the color of tile the correlation between the variables is highly positive.And lighter the color of tile the correlation between the variables is highly negative.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Pairplot between Legendary variable with other numeric variables\nPairwise Plot :- Used to plot relationship in a dataset\n\nCreates scatter plots for join relationship and histogram for univariate distributions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(numeric_data, kind='scatter',hue='Legendary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Drop Type 2 and Beast columns and chack info\ndf_clean = df.drop(columns = ['Type 2','Beast'])\ndf_clean.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot a graph of frequency distribution of 'Type 1' pokemon","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x ='Type 1', data=df_clean)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot a graph of frequency distribution of 'Legendary' pokemon","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Legendary', data=df_clean)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Box plot between 'Total' and 'Type 1' variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Type 1', y='Total', data = df_clean)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Boxplot between 'Legendary' and 'Total' variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Legendary', y='Total', data = df_clean)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Box whiskers and histogram plot of 'Total' variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Box whiskers plot & histogram on the same window \n## Split the plotting window into 2 parts\n\nf, (ax_box, ax_hist)= plt.subplots(2, gridspec_kw={\"height_ratios\": (.15, .85)})\n## Add and create  box plot\nsns.boxplot(df_clean['Total'], ax=ax_box)\n\nsns.distplot(df_clean[\"Total\"], ax=ax_hist)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA - Principal Component Analysis \n#### It is a linear dimensionality reduction technique. \n\n### Why PCA ?\n####  Because there are a large number of variables or dimensions along which the data is distributed, visualization can be a challenge. Hence, PCA can do that for you since it projects the data into a lower dimension, thereby allowing you to visualize the data in a 2D or 3D space with a naked eye.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##Principal Component Analysis(PCA) used for dimension reduction of dataset\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Selecting numeric columns in which PSA technique has to apply to reduce dimension ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_col =['HP','Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Standardizing the data since PCA's output is influenced based on the scale of the features of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(df_mod[pca_col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_data = scaler.transform(df_mod[pca_col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### While applying StandardScaler, each feature of your data should be normally distributed such that it will scale the distribution to a mean of zero and a standard deviation of one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(scaled_data), np.std(scaled_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It has been converted into array and this is based on Standard Deviation is 1 and Mean = 0.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's convert the normalized features into a tabular format with the help of DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = ['feature'+str(i) for i in range(scaled_data.shape[1])]\nnormalised_pokemon = pd.DataFrame(scaled_data,columns=feature_cols)\nnormalised_pokemon.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Use the sklearn library to import the PCA module","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Projecting the six-dimensional data to two-dimensional principal components. You will pass the number of components (n_components=0.6) and finally call fit_transform on the aggregate data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 0.6)\nprincipal_components =pca.fit(scaled_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pca = pca.transform(scaled_data)\nscaled_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Shape of dataframe after PCA reduction technique.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's create a DataFrame that will have the principal component values for all 800 samples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"principal_comp_df = pd.DataFrame(data = x_pca, columns = ['Principal_comp_1', 'Principal_comp_2'])\nprincipal_comp_df.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Make a correlation matrix of PCA dataframe\ncorrelation_mat = principal_comp_df.corr()\ncorrelation_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## PLot correlation matrix for better understanding\nsns.heatmap(correlation_mat, annot=True, cmap='YlGnBu' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, find the explained_variance_ratio. It will provide you with the amount of information or variance each principal component holds after projecting the data to a lower dimensional subspace.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labels = df[\"Name\"].copy()\ndf['Type 1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, plot PCA variables w.r.t Type 1 Pokemon","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.figure(figsize=(10,10))\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal_comp_1',fontsize=20)\nplt.ylabel('Principal_comp_2',fontsize=20)\nplt.title(\"Principal Component Analysis of Pokemon Type 1\",fontsize=20)\ntargets = ['Water','Normal','Grass','Bug','Psychic','Flamer','Electric','Rock','Dragon','Ground','Ghost',        \n'Dark', 'Poison', 'Steel','Fighting','Ice','Fairy','Flying']\ncolors = ['r', 'g', 'y','c','#2c3e50','#ee9ca7','#1565C0','#91EAE4','#654ea3','#ffd89b','#799F0C','#dd1818',\n         '#FFF200','#FF8C00','#30E8BF','#603813','#b29f94','#24FE41','#a80077']\nfor target, color in zip(targets,colors):\n    indicesToKeep = df['Type 1'] == target\n    plt.scatter(principal_comp_df.loc[indicesToKeep, 'Principal_comp_1']\n               , principal_comp_df.loc[indicesToKeep, 'Principal_comp_2'], c = color, s = 50)\n\nplt.legend(targets,prop={'size': 7})\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}