{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv')\ndf = df.sample(5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(axis = 'index' , how='any' , inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_texts = []\ntarget_texts = []\n\ninput_vocabulary = set()\noutput_vocabulary = set()\nstart_token = '\\t'\nstop_token = '\\n'\n#max_training_examples = len(df) - 1\nmax_training_examples = 15000\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for input_text , target_text in zip(df.english_sentence , df.hindi_sentence):\n    target_text = start_token + target_text + stop_token\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_vocabulary:\n            input_vocabulary.add(char)\n    for char in target_text:\n        if char not in output_vocabulary:\n            output_vocabulary.add(char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(input_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_vocabulary = sorted(list(input_vocabulary))\noutput_vocabulary = sorted(list(output_vocabulary))\n\ninput_vocabulary_size = len(input_vocabulary)\noutput_vocabulary_size = len(output_vocabulary)\n\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\ninput_token_index = dict([(char , i) for i,char in enumerate(input_vocabulary)])\ntarget_token_index = dict([(char , i) for i,char in enumerate(output_vocabulary)])\n\nreverse_input_char_index = dict([(i,char) for char,i in input_token_index.items()])\nreverse_target_char_index = dict([(i,char) for char,i in target_token_index.items()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data = np.zeros((len(input_texts) , max_encoder_seq_length , input_vocabulary_size) , dtype = 'float32')\ndecoder_input_data = np.zeros((len(target_texts) , max_decoder_seq_length , output_vocabulary_size) , dtype = 'float32')\ndecoder_target_data = np.zeros((len(target_texts) , max_decoder_seq_length , output_vocabulary_size) , dtype = 'float32')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i ,(input_text , target_text) in enumerate(zip(input_texts,target_texts)):\n    \n    for t,char in enumerate(input_text):\n        encoder_input_data[i , t , input_token_index[char]] = 1\n    \n    for t,char in enumerate(target_text):\n        decoder_input_data[i , t , target_token_index[char]] = 1\n        \n    for t,char in enumerate(target_text):\n        decoder_target_data[i , t-1 , target_token_index[char]] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.test.is_gpu_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''with open('encoder_input_data.npy', 'wb') as f:\n    np.save(f, encoder_input_data)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import pickle\nwith open('./encoder_input_data.pickle' , 'w') as f:\n    pickle.dump(encoder_input_data , f)'''\n\n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''with open('./decoder_input_data.pickle' , 'w') as f:\n    pickle.dump(decoder_input_data , f)\n    \nwith open('./decoder_target_data.pickle' , 'w') as f:\n    pickle.dump(decoder_target_data , f)\n    #done till here'''\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''with open('./input_vocabulary.pickle' , 'w') as f:\n    pickle.dump(input_vocabulary , f)\n\nwith open('./output_vocabulary.pickle' , 'w') as f:\n    pickle.dump(output_vocabulary , f)\n    \n    \nwith open('./input_token_index.pickle' , 'w') as f:\n    pickle.dump(input_token_index , f)\n    \n    \nwith open('./target_token_index.pickle' , 'w') as f:\n    pickle.dump(target_token_index , f)\n\nwith open('./reverse_input_char_index.pickle' , 'w') as f:\n    pickle.dump(reverse_input_char_index , f)\n    \nwith open('./reverse_target_char_index.pickle' , 'w') as f:\n    pickle.dump(reverse_target_char_index , f)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input , LSTM , Dense\nbatch_size = 64\nepochs = 100\nnum_neurons = 256\n\nencoder_inputs = Input(shape = (None , input_vocabulary_size))\nencoder = LSTM(num_neurons , return_state = True)\nencoder_outputs , state_h , state_c = encoder(encoder_inputs)\nencoder_states = [state_h , state_c]\n\ndecoder_inputs = Input(shape = (None , output_vocabulary_size))\ndecoder_lstm = LSTM(num_neurons , return_state = True , return_sequences = True)\ndecoder_outputs , _ , _ = decoder_lstm(decoder_inputs , initial_state = encoder_states)\ndecoder_dense = Dense(output_vocabulary_size , activation = 'softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\nmodel = Model([encoder_inputs , decoder_inputs] , decoder_outputs)\nmodel.compile(optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([encoder_input_data , decoder_input_data] , decoder_target_data , batch_size = 64 , epochs = epochs,\n         validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model = Model(encoder_inputs, encoder_states)\nthought_input = [\n     Input(shape=(num_neurons,)), Input(shape=(num_neurons,))]\ndecoder_outputs, state_h, state_c = decoder_lstm(\n        decoder_inputs, initial_state=thought_input)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\n\ndecoder_model = Model(\n      [decoder_inputs] + thought_input,\n        [decoder_outputs] + decoder_states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_seq):\n    thought = encoder_model.predict(input_seq)\n    target_seq = np.zeros((1 , 1 , output_vocabulary_size))\n    target_seq[0 , 0 , target_token_index[stop_token]] = 1\n    stop_condition = False\n    generated_sequence = ''\n    \n    while not stop_condition:\n        output_tokens , h , c = decoder_model.predict([target_seq] + thought)\n        generated_token_idx = np.argmax(output_tokens[0, -1, :])\n        generated_char = reverse_target_char_index[generated_token_idx]\n        generated_sequence += generated_char\n        \n        if (generated_char == stop_token) or (len(generated_sequence) > max_decoder_seq_length):\n            stop_condition = True\n            \n        target_seq = np.zeros((1 , 1 , output_vocabulary_size))\n        target_seq[0, 0, generated_token_idx] = 1\n        thought = [h, c]\n        \n    return generated_sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def response(input_text1):\n    input_seq = np.zeros((1 , max_encoder_seq_length , input_vocabulary_size) ,\n                                  dtype = 'float32')\n    for t, char in enumerate(input_text1):\n        input_seq[0, t, input_token_index[char]] = 1\n    decoded_sentence = decode_sequence(input_seq)\n    print('Bot Reply (Decoded sentence):', decoded_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"response('what is government doing')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model.save('./encodermodel.h5')\ndecoder_model.save('./decodermodel.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./mainmodel.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_encoder_seq_length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_vocabulary_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_vocabulary_size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''with open('./input_vocabulary.pickle' , 'w') as f:\n    pickle.dump(input_vocabulary , f)\n\nwith open('./output_vocabulary.pickle' , 'w') as f:\n    pickle.dump(output_vocabulary , f)\n    \n    \nwith open('./input_token_index.pickle' , 'w') as f:\n    pickle.dump(input_token_index , f)\n    \n    \nwith open('./target_token_index.pickle' , 'w') as f:\n    pickle.dump(target_token_index , f)\n\nwith open('./reverse_input_char_index.pickle' , 'w') as f:\n    pickle.dump(reverse_input_char_index , f)\n    \nwith open('./reverse_target_char_index.pickle' , 'w') as f:\n    pickle.dump(reverse_target_char_index , f)'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}