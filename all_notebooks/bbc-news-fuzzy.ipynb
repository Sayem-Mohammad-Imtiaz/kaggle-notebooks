{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# raries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nimport itertools\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow import keras\nlayers = keras.layers\nmodels = keras.models\n\n\n# This code was tested with TensorFlow v1.8\nprint(\"You have TensorFlow version\", tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd\nbbc_text = pd.read_csv(\"../input/bbc-fulltext-and-category/bbc-text.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"bbc_text.head()\ndata = bbc_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import logging\nimport pandas as pd\nimport numpy as np\nfrom numpy import random\nimport gensim\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport re\nfrom bs4 import BeautifulSoup\n%matplotlib inline\n\ndf = data\ndf = df[pd.notnull(df['category'])]\nprint(df.head(10))\nprint(df['text'].apply(lambda x: len(x.split(' '))).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data['category'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"my_tags = [cat for cat in data['category'].unique()]\nplt.figure(figsize=(10,4))\ndf.category.value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def print_plot(index):\n    example = df[df.index == index][['text', 'category']].values[0]\n    if len(example) > 0:\n        print(example[0])\n        print('Tag:', example[1])\nprint_plot(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print_plot(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n#     text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n    return text\n    \ndf['text'] = df['text'].apply(clean_text)\nprint_plot(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df['text'].apply(lambda x: len(x.split(' '))).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X = df.text\ny = df.category\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nnb = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB()),\n              ])\nnb.fit(X_train, y_train)\n\n%time\nfrom sklearn.metrics import classification_report\ny_pred = nb.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=my_tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n               ])\nsgd.fit(X_train, y_train)\n\n%time\n\ny_pred = sgd.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=my_tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\nlogreg.fit(X_train, y_train)\n\n%time\n\ny_pred = logreg.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=my_tags))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Load word embedding pretrained on word Google news corpus**","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from gensim.models import Word2Vec\nwv = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\nwv.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def word_averaging(wv, words):\n    all_words, mean = set(), []\n    \n    for word in words:\n        if isinstance(word, np.ndarray):\n            mean.append(word)\n        elif word in wv.vocab:\n            mean.append(wv.vectors_norm[wv.vocab[word].index])\n            all_words.add(wv.vocab[word].index)\n\n    if not mean:\n        logging.warning(\"cannot compute similarity with no input %s\", words)\n        # FIXME: remove these examples in pre-processing\n        return np.zeros(wv.vector_size,)\n\n    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n    return mean\n\ndef  word_averaging_list(wv, text_list):\n    return np.vstack([word_averaging(wv, post) for post in text_list ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def w2v_tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text, language='english'):\n        for word in nltk.word_tokenize(sent, language='english'):\n            if len(word) < 2:\n                continue\n            tokens.append(word)\n    return tokens\n    \ntrain, test = train_test_split(df, test_size=0.3, random_state = 42)\n\ntest_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\ntrain_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n\nX_train_word_average = word_averaging_list(wv,train_tokenized)\nX_test_word_average = word_averaging_list(wv,test_tokenized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg = logreg.fit(X_train_word_average, train['category'])\ny_pred = logreg.predict(X_test_word_average)\nprint('accuracy %s' % accuracy_score(y_pred, test.category))\nprint(classification_report(test.category, y_pred,target_names=my_tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nimport os\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils\n\ntrain_size = int(len(df) * .7)\ntrain_posts = df['text'][:train_size]\ntrain_tags = df['category'][:train_size]\n\ntest_posts = df['text'][train_size:]\ntest_tags = df['category'][train_size:]\n\nmax_words = 5000\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)\ntokenize.fit_on_texts(train_posts) # only fit on train\n\nx_train = tokenize.texts_to_matrix(train_posts)\nx_test = tokenize.texts_to_matrix(test_posts)\n\nencoder = LabelEncoder()\nencoder.fit(train_tags)\ny_train = encoder.transform(train_tags)\ny_test = encoder.transform(test_tags)\n\nnum_classes = np.max(y_train) + 1\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)\n\nbatch_size = 32\nepochs = 10\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n              \nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data_list = data['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import nltk \nimport string \nimport re \n\ndef text_lowercase(text): \n    return text.lower()\n\n# Remove numbers \ndef remove_numbers(text): \n    result = re.sub(r'\\d+', '', text) \n    return result \n\n# remove punctuation \ndef remove_punctuation(text): \n    translator = str.maketrans('', '', string.punctuation) \n    return text.translate(translator) \n\n# remove whitespace from text \ndef remove_whitespace(text): \n    return  \" \".join(text.split()) \n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n  \n# remove stopwords function \ndef remove_stopwords(text): \n    stop_words = set(stopwords.words(\"english\")) \n    word_tokens = word_tokenize(text) \n    filtered_text = [word for word in word_tokens if word not in stop_words] \n    return filtered_text \n  \nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.tokenize import word_tokenize \nstemmer = PorterStemmer() \n  \n# stem words in the list of tokenised words \ndef stem_words(text): \n    word_tokens = word_tokenize(text) \n    stems = [stemmer.stem(word) for word in word_tokens] \n    return stems \n\n\n\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize \nlemmatizer = WordNetLemmatizer() \n# lemmatize string \ndef lemmatize_word(text): \n    word_tokens = word_tokenize(text) \n    # provide context i.e. part-of-speech \n    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n    return lemmas \n\n\ndef preprocessing(text):\n    result = text_lowercase(text)\n#     result = remove_numbers(result)\n#     result = remove_punctuation(result)\n#     result = remove_whitespace(result)\n    result = remove_stopwords(result)\n    \n#     result = stem_words(result)\n#     result = lemmatize_word(result)\n    \n    \n    return result\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#using data_list we can preprocess the sentences here\nimport time\nsentences = [sentence for sentence in data_list]\nprint(len(sentences))\nstart_time  = time.time()\npreprocessed_sentences = [preprocessing(sentence) for sentence in sentences]\nend_time = time.time()\nprint(\"total time taken in preprocessing {}\".format(end_time-start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(len(preprocessed_sentences))\nprint(len(sentences[4]))\nprint(len(preprocessed_sentences[4]))\nprint(sentences[4])\nprint(preprocessed_sentences[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# dictionary for words according to frequency\nfrom collections import defaultdict\nword_freq = defaultdict(int)\n\nfor sent in preprocessed_sentences:\n    for i in sent:\n        word_freq[i] += 1\nprint(len(word_freq))\n\nword_freq['subject']\nmost_freq_words = sorted(word_freq, key=word_freq.get, reverse=True)[:2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"vocabulary = [(key,value) for key,value in word_freq.items() if(value>=5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\nprint(len(vocabulary))\nvocabulary[10]\n\ndef Convert(tup, di): \n    for a, b in tup: \n        di.setdefault(a, []).append(b) \n    return di \n\nvocabulary_dict = {}\nvocabulary_dict = Convert(vocabulary,vocabulary_dict)\nprint(len(vocabulary_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz'\n!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from gensim.models import KeyedVectors\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\nx = word2vec.word_vec(\"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import multiprocessing\n\nfrom gensim.models import Word2Vec\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n\ncores = multiprocessing.cpu_count()\n\nprint(cores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from gensim.test.utils import get_tmpfile\nfrom gensim.models.callbacks import CallbackAny2Vec\n\n\nclass EpochSaver(CallbackAny2Vec):\n#      '''Callback to save model after each epoch.'''\n\n    def __init__(self, path_prefix):\n        self.path_prefix = path_prefix\n        self.epoch = 0\n\n    def on_epoch_end(self, model):\n        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n        model.save(output_path)\n        self.epoch += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class EpochLogger(CallbackAny2Vec):\n    def __init__(self):\n        self.epoch = 0\n   \n    def on_epoch_begin(self, model):\n         print(\"Epoch #{} start\".format(self.epoch))\n\n    def on_epoch_end(self, model):\n        print(\"Epoch #{} end\".format(self.epoch))\n        self.epoch += 1\n\nepoch_logger = EpochLogger()\n# w2v_model = Word2Vec(common_texts, iter=5, size=10, min_count=0, seed=42, callbacks=[epoch_logger])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from gensim.models import Word2Vec\nimport time\nw2v_model = Word2Vec(min_count=5,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=4)\n\nstart = time.time()\nw2v_model.build_vocab(preprocessed_sentences, progress_per=10000/2)\ntotal_examples = w2v_model.corpus_count\nprint(total_examples)\n\nw2v_model.build_vocab([list(word2vec.vocab.keys())], update=True)\nw2v_model.intersect_word2vec_format(EMBEDDING_FILE, binary =True, encoding='utf8')\nw2v_model.train(preprocessed_sentences,total_examples=total_examples,epochs=30,callbacks=[epoch_logger])\n\nprint(\"total time taken {}\".format(time.time()-start))\n\n\nw2v_model.save('fine_tuned_30epochs_bbc_model')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import os\nos.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nw2v_model = Word2Vec.load(\"fine_tuned_30epochs_bbc_model\")\nvector  = w2v_model.wv['news']\nprint(vector.shape)\nw2v_model = w2v_model.wv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import time\nstart_time  = time.time()\npreprocessed_sentences = [preprocessing(sentence) for sentence in sentences]\nend_time = time.time()\nprint(\"total time taken in preprocessing {}\".format(end_time-start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"start = time.time()\ndata_matrix = []\nfor i in range(len(preprocessed_sentences)):\n    word_dict ={}\n    for index,j in enumerate(preprocessed_sentences[i]):\n            if(j not in word_dict):\n                word_dict[j]=1\n            else:\n                word_dict[j]+=1\n    arr = []\n    for word in w2v_model.vocab:\n        if(word in word_dict):\n            arr.append(word_dict[word])\n        else:\n            arr.append(0)\n        \n    \n    data_matrix.append(arr)\nend = time.time()\nprint('total time taken in building matrix {}'.format(end-start))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data_matrix = np.array(data_matrix)\nprint(data_matrix.shape)\nprint(data_matrix.dtype)\ndata_matrix = data_matrix.astype('int32')\nnp.save('data_matrix',data_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data_matrix = np.load('data_matrix.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"len(most_freq_words)\nmost_freq_words[101]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"H_matrix = []\nstart = time.time()\nfor i in w2v_model.vocab:\n    arr = []\n    for word in most_freq_words:\n        similar = w2v_model.similarity(word, i)\n        if(similar>0):\n            arr.append(similar)\n        else:\n            arr.append(0)\n    H_matrix.append(arr)\n\n\n\n\n\n\n\nprint('time taken in generation {}'.format(time.time()-start))\nstart = time.time()\nH_matrix = np.array(H_matrix)\n\n\nprint('time taken in saving H_matrix {}'.format(time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"type(w2v_model.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"print(H_matrix.dtype)\nH_matrix = H_matrix.astype('float16')\nnp.save(\"H_matrix\",H_matrix)\nH_matrix = np.load('H_matrix.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\nprint('data_matrix shape : {} and dtype of data : {}'.format(data_matrix.shape, data_matrix.dtype))\nprint('H_matrix shape : {} and dtype of H_matrix : {}'.format(H_matrix.shape, H_matrix.dtype))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import time\nstart = time.time()\nmatrix_z = data_matrix.dot(H_matrix)\nprint('time taken in saving H_matrix {}'.format (time.time() - start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"np.save('matrix_z_bbc_without_negSimilarity',matrix_z)\n\nprint('finally I get this matrix! huuu,,,')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"matrix_z = np.load('../input/mydata/matrix_z_bbc_without_negSimilarity.npy')\nmatrix_z = np.around(matrix_z,decimals=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve, classification_report\nfrom sklearn import ensemble, linear_model, neighbors, svm, tree, neural_network\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"data.replace(cleanup_nums,inplace = True)\ncleanup_nums = {\"category\":     {\"tech\": 0, \"business\": 1,\"sport\":2, 'entertainment':3, 'politics':4}}\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# y = data['category']\nX = matrix_z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"y = data['category']\ny.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.2, random_state=38)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.svm import SVC\nimport time\nstart = time.time()\n# defining parameter range \nparam_grid = {'C': [0.001,0.01, 0.1],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['linear']}  \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=4) \n  \n# fitting the model for grid search \ngrid.fit(X_train, y_train) \n\nprint('time taken in training SMV model is {}'.format(time.time()-start))\n# print best parameter after tuning \nprint(grid.best_params_) \n  \n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"grid_predictions = grid.predict(X_test)\n\nprint(classification_report(y_test, grid_predictions)) \nprint(confusion_matrix(y_test,grid_predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"tree_clf = tree.DecisionTreeClassifier()\nstart = time.time()\ntree_clf.fit(X_train,y_train)\nprint('time taken in training tree_clf model is {}'.format(time.time()-start))\npredictions_tree = tree_clf.predict(X_test)\nprint(confusion_matrix(y_test,predictions_tree))\nprint(classification_report(y_test,predictions_tree))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}