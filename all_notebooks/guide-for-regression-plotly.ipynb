{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hands On Machine Learning (Ed.2) - Chapter II\n\nCause currently I'm studying the awesome book of Hands On Machine Learning, I want to reproduced some examples using different libraries like plotly and structure the workflow in an easier way for me.\nThe data is directly from the open github and this notebook could differ in some parts with the code of the book.\n\nAll credits to Aurélien Géron. And thanks again for that wonderful book, really helpful.\n\n--\n\nThis notebook covers:\n\n* [0. Frame the problem](#section0)\n* [1. Performance Measure](#section1)\n* [2. Visualization](#section2)\n* [3. Test Creation](#section3)\n* [4. Correlations](#section4)\n* [5. Preparing the data for the algorithm + Feature Scaling](#section5)\n* [6. Select and Train the Model](#section6)\n* [7. Evaluate with Test Set](#section7)\n* [8. Others models and hyperparameters](#section8)\n\nThe model should learn from this data and be able to predict the median housing\nprice in any district, given all the other metrics.\n\n## 0. Frame the problem <a class=\"anchor\" id=\"section0\"></a>\n\n* Supervised learning task: Labeled dataset\n* Regression task (multiple): The system will use multiple features to make a prediction of a value\n* Univariate Regression:The model only tries to predict a single value for each district\n* Plain batch Learning: No continuous flow of data and the data is small enough to fit in memory.\n\n## 1. Performance Measure <a class=\"anchor\" id=\"section1\"></a>\n**Root Mean Square Error(RMSE)**: Typical performance measure for regression problems. How much error the system typically makes in its predictions with a higher weight for large errors\n![imagen.png](attachment:imagen.png)\n\nIs possible to use others performances measures like **Mean Absolute Error(MAE)** which is used in contexts with more outliers due to this measure is less sensitive to outliers than RMSE. For now we will use the RMSE","attachments":{"imagen.png":{"image/png":"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"}},"execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Visualization\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom kaggle_tools import *\n\npath = '../input/hands-on-machine-learning-housing-dataset/'\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read data\nhousing = pd.read_csv(path+'housing.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Glance to the data\n* This is a small dataset: 20640 instances(rows)\n* Some missing values in the column **total_bed_rooms**: 207 districs are missing the feature\n* All are numerical attributes except **ocean_proximity**: This is type object","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(housing.info())\nhousing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing['ocean_proximity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary of each numerical attribute\n* **STD:** How dispersed the values are\n* **PERCENTILES:** Indicates the value below which a given percentage of observations in a group of observations falls\n* * Percentile 25% in **housing_median_age**: Lower than 18 => 1st Quartile\n* * Percentile 50% in **housing_median_age**: Lower than 29 => MEDIAN\n* * Percentile 75% in **housing_median_age**: Lower than 37 => 3rd Quartile","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Visualization<a class=\"anchor\" id=\"section2\"></a>\n### Geographical Data\nTake advantage of longitude and latitude attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mapboxpath = '../input/mapbox/mapbox.txt'\npx.set_mapbox_access_token(open(mapboxpath).read())\n\nfig = px.scatter_mapbox(housing, lat=\"latitude\", lon=\"longitude\",     color=\"median_house_value\", size=housing[\"population\"],\n                  color_continuous_scale=px.colors.cyclical.IceFire, size_max=15, zoom=4.5)\n\nfig.update_layout(title_text=\"California Housing Pricing (Median)\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way to understand the data: **Histograms**\n* Vertical Axis: Number of instances\n* Horizontal Axis: Value Range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"check_num_col = housing.dtypes == 'float64'\nnum_col = sorted(check_num_col[check_num_col].index)\nprint('NUMERICAL COLUMNS: \\n ', num_col)\nprint('HOW MANNY NUMERICAL COLUMNS? ', len(num_col))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights from the **histogram**\n1. Median income attribute: Scale of 10k of dollars (that means, 3 actually means 30000 dollars). Also is capped at 0.4999 and 15.0001)\n2. The housing_median_age and median_house_value are also capped\n\n> Could be a problem that the attribute median_house_value was capped cause this is the target attribute. That means that the ML Algorithm may learn that prices never go beyond a limit.\n\nIf would be necessary in the future make predictions beyond $500,000 there are two main options:\n* Collect more data\n* Remove those districts from the training set\n\n\n3. Different scales. Feature scaling will be necessary\n4. Tail Heavy: They extend much farther to the right of the median than left. This distribution may make it a bit harder to detect pattern. It's easier with a bell-shaped or gaussian-shaped distributoin","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfplot = housing[num_col]\n\nfig = make_subplots(rows=3, cols=3, subplot_titles=(num_col))\n\nindex = 0\nfor i in range(1,4):\n    for j in range(1,4):      \n        data = dfplot[num_col[index]]\n        trace = go.Histogram(x=data, nbinsx=50)\n        fig.append_trace(trace, i, j)\n        index+=1\n    \nfig.update_layout(height=900, width=1250, title_text=\"Numerical Attributes\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Test Set Creation<a class=\"anchor\" id=\"section3\"></a>\nIf you estimate the error of the algorithm using the train set your error estimation will be too optimistic and won't perform as expected. This is because the model could overfit the data. This fact is called *data snooping bias*\n\nSo a test set is necessary. Usually a 20% of the dataset could be use for this goal but could be less if the original dataset is huge\n\nHowever, one special feature of this problem is that you *know* that the attribute median_income is very important for predict median housing prices:\n* You want to ensure that the test set is representative  of the various categories of incomes in the whole dataset\n* ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Create a income category attribute\n# More median income values are clustered around 1.5 to 6\n# There are some median incomes beyond 6\nbins_to_cut = [0., 1.5, 3.0, 4.5, 6., np.inf]\nlabels_to_cut = [1,2,3,4,5]\nhousing['income_cat'] = pd.cut(housing['median_income'], bins = bins_to_cut, labels=labels_to_cut )\n\n\nfig = px.histogram(housing, x=\"income_cat\")\nfig.update_layout(height=500, width=700, title_text=\"Median Income Categories\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into train and test using the stratified category\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]\n    \n# Check distribution in the test and train sets\nprint('DISTRIBUTION TRAIN SET \\n', strat_train_set['income_cat'].value_counts()/len(strat_train_set))\nstrat_train_set.drop('income_cat', axis=1, inplace=True)\nprint('-----------')\nprint('DISTRIBUTION TEST SET \\n', strat_test_set['income_cat'].value_counts()/len(strat_test_set))\nstrat_test_set.drop('income_cat', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Check the correlations <a class=\"anchor\" id=\"section4\"></a>\n\nIs important to see what attributes correlate more with the target. The correlation coefficient only measures linear correlations:\n>  If X goes up, then Y goes up/down\n\nThe most promising attribute to predict the median house value is the median income.\nIn the next chart is possible to see that:\n1. There is an upward trend and the points are not too dispersed\n2. Is clearly visible that the median_income is capped at $500,000. However there are more strange lines at: 450,000 dollars; 350,000 dollars; 280,000 dollars and 112,000 dollars","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = housing.corr()\ncorr[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(housing, x=\"median_income\", y=\"median_house_value\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Prepare the data for the algorithm + Feature Scaling<a class=\"anchor\" id=\"section5\"></a>\n\nRemember that\n* There are some data quirkcs that should be cleaned up\n* The median income has a interesting correlation with the target\n* Some attributes have a tail-heavy distribution and should be reshaped into a bell-shape\n* There are some attributes that they don't give us information but maybe they'll do it with some transformations\n\n\nSo this is the data preparation workflow\n1. Create new attributes and see correlations\n2. Imputer missing values for numerical attributes\n3. Encode categorical attributes\n4. Add a custom transformation that will work like the first point above but using hyperparamters\n5. Feature Scaling\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Engineering\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n\n# Check again the correlation matrix\ncorr = housing.corr()\ncorr[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = strat_train_set.drop('median_house_value', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate the data for applying transformations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical Attributes\nhousing_num = housing.drop('ocean_proximity', axis=1)\n# Categorical Attributes\nhousing_cat = housing[['ocean_proximity']]\n# Labels\nhousing_labels = strat_train_set['median_house_value'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling Numerical Attributes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# HANDLING NUMERICAL ATTRIBUTES\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')\nimputer.fit(housing_num)\nX= imputer.transform(housing_num)\n\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling Categorical Attributes\n\n\nThere are two ways in order to handle this type of attributes:\n1. Ordinal Encoder: Each category will be a number in the column\n2. One Hot Encoder: Each category will be a column in the dataset\n\nThe Ordinal Encoder is good if the categories represents a scale of values like [good, bad, normal] because the ML algorithms will assume that two nearby values are more similar thatn two distant values\n\nSo for this case, the best choice is to use the **One Hot Encoder**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# HANDLING CATEGORICAL ATTRIBUTES\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\n\nhousing_cat_encoder = encoder.fit_transform(housing_cat)\nprint('CATEGORIES ENCODED:\\n', encoder.categories_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Transformers\n\nThe next transformed explained in the chapter creates the next commented attributes but using hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n# housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n# housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self # nothing else to do\n    def transform(self, X, y=None):\n        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n        population_per_household = X[:, population_ix] / X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling\nIn general, ML Algorithms don't perform well when the input numerical data have very different scales(for example Neural Networks perform really bad in that case because they often expect an input in a range of (0,1). Example:\n* The total number of rooms ranges from 6 to 39320\n* The median incomes ranges from 0 to 15\n\n> IMPORTANT: Scaling the target values is generally not required !!!\n\nThe two common ways to scaling th e data is using:\n* Min-Max(Normalization): Values Shifted and rescaled in a range of (0,1)\n* Standardization: Don't bound values to a specific range. However is less affected by outliers\n\n> IMPORTANT: Fit the scalers ONLY to the Training Data. Only then you can transform the training set and test set\n\n\nIn order to make these transformations, is very useful to use *Transformations Pipelines*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pipeline for numerical attributes\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ncheck_num_col = housing.dtypes == 'float64'\nnum_col = sorted(check_num_col[check_num_col].index)\n\npipeline_numeric = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('attribs_adder', CombinedAttributesAdder()),\n    ('std_scaler', StandardScaler())\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pipeline for categorical attributes\ncheck_cat_col = housing.dtypes == 'object'\ncat_col = sorted(check_cat_col[check_cat_col].index)\nprint('CAT COLS AFTER PROCESSED \\n', cat_col)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pipeline applies each transformer to:\n* The appropiate column\n* Concatenate the outputs along the second axis\n\nSo the transformer/s must return the same number of rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Full pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    ('pip_num', pipeline_numeric, num_col),\n    ('pip_cat', OneHotEncoder(), cat_col)\n])\n\nhousing_ready = full_pipeline.fit_transform(housing)\nhousing_ready","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Select and Train the models <a class=\"anchor\" id=\"section6\"></a>\n\nIt's usually for a regression problem, use as first try the LinearRegression model. This model produces an RMSE of  ~69038. So if the **median_housing_values** has a range between 120k and 265k, the error is around 69k dollars, which is very big. That could be happening cause:\n* Not enough information\n* The model is underfitting the data => We need a more powerful","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\ndef try_model(modelClass):\n    model = modelClass\n    model.fit(housing_ready, housing_labels)\n\n    # Get the predictions\n    predictors = housing.iloc[:5]\n    labels = housing_labels.iloc[:5]\n\n    predictors_tf = full_pipeline.transform(predictors)\n\n    # See the predictions\n    print('Predictions: \\n', model.predict(predictors_tf))\n    print('Labels: \\n', list(labels))\n\n    # Measure the error\n    predictions_final = model.predict(housing_ready)\n    rmse = np.sqrt(mean_squared_error(housing_labels, predictions_final))\n    print('RMSE: ', round(rmse,2))\n    \ntry_model(LinearRegression())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Others models to try:\n* **Decision Tree Regressor:** The RMSE result is 0.0. That's because the model has overfit the data. Using the *cross_validation evaluation* we are able to see that we havce a 71k of error ±2k\n* **RandomForestRegressor:** Better than the others 2, but the score on the training set is lower than in the validations set. That's meaning that the model is still overfitting the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ntry_model(DecisionTreeRegressor())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross-Validation Evaluation\n*Use of the K-fold cross-validation feature*\n\nThe reason of using a *neg_mean_squared_error* is because cv features expect a utility function where greater is better. So scoring with CV is with the opposite of the MSE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\n\ndef try_with_cv(modelClass, modelname):\n    print('MODEL EVALUATION ', modelname)\n    \n    # Use model for calculating TRAINING ERROR\n    modelClass.fit(housing_ready, housing_labels)\n    housing_predictions = modelClass.predict(housing_ready)\n    training_error = np.sqrt(mean_squared_error(housing_labels, housing_predictions))\n    print('TRAINING ERROR: ', round(training_error,2),'\\n')\n    \n    # Validation error using Cross Validation\n    scores = cross_val_score(modelClass, housing_ready,housing_labels, scoring='neg_mean_squared_error', cv=10)\n    scores_final = np.sqrt(-scores)\n    \n    print('VALIDATION ERROR:')\n    print('RMSE(mean): ', scores_final.mean())\n    print('RMSE(sd): ', scores_final.std())\n    print('\\n\\n\\n')\n    \ntry_with_cv(DecisionTreeRegressor(), 'DECISION TREE REGRESSOR')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remember:** If training error < validation error => Overfitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try_with_cv(RandomForestRegressor(), 'RANDOM FOREST REGRESSOR')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fine-Tune the model (Grid Search)\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# 90 rounds of training\n# (3x4 + 2x3)x5\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\n\nforest_reg = RandomForestRegressor()\ngrid = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid.fit(housing_ready, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is possible to see what attributes were the mos important for the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the system on the test set<a class=\"anchor\" id=\"section7\"></a>\nAfter have worked with the training set and use the evaluation set to get the errors, it's time to use the model in the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = np.sqrt(mean_squared_error(y_test, final_predictions))\nprint('TEST SET MSE: ', final_mse)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_weight = grid.best_estimator_.feature_importances_\ncustom_att = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_encoder = full_pipeline.named_transformers_[\"pip_cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\n\nattributes = num_col + custom_att + cat_one_hot_attribs\n\nsorted(zip(features_weight, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other Models with others Hyperparameters<a class=\"anchor\" id=\"section8\"></a>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nparam_grid = [\n        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n    ]\n\n\ngrid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(housing_ready, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nprint('SUPPORT VECTOR MACHINE RMSE: ', rmse)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}