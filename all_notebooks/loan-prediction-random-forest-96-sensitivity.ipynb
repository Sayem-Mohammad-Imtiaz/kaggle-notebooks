{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Important Libraries \n\nimport numpy as np \nimport pandas as pd\nimport pprint\nimport matplotlib.pyplot as plt \nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the data \ndf=pd.read_csv(\"/kaggle/input/analytics-vidhya-loan-prediction/train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking if the dataset is Imbalanced or not \n\nprint(df.Loan_Status.value_counts()/df.shape[0]*100)\nplt.bar(df.Loan_Status.value_counts().keys(),df.Loan_Status.value_counts()/df.shape[0]*100)\nplt.xlabel(\"Loan Given (YES/NO)\")\nplt.ylabel(\"% of People\")\nplt.title(\"Check for Imbalanced Dataset\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checjking for the null values in dataframe\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definig a new column\n\nLst=list(zip(df[\"ApplicantIncome\"],df[\"CoapplicantIncome\"]))\nHasCoapplicant=[]\nfor i in Lst:\n    if i[1]!=0:\n        HasCoapplicant.append(1)\n    else:\n        HasCoapplicant.append(0)\n        \ndf[\"HasCoapplicant\"]=HasCoapplicant\n       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying the unique values and the data type of each features \nfor i in df.columns[1:]:\n    print(\"Unique labels for {} = ({},  {}) \".format(i,df[i].nunique(),df[i].dtype))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Handling the categorical variables and missing values \n\n# Replacing Male with value 1 and Female with value 0 and Nan values with the most occuring label among them \ndf.Gender.replace({\"Male\": 1, \"Female\": 0},inplace=True)\ndf.Gender.replace({np.nan: df.Gender.value_counts().keys()[0]},inplace=True)\n\n# Same as what done for the Gender feature \ndf.Married.replace({\"Yes\": 1, \"No\": 0},inplace=True)\ndf.Married.replace({np.nan: df.Married.value_counts().keys()[0]},inplace=True)\n\ndf.Self_Employed.replace({\"Yes\": 1, \"No\": 0},inplace=True)\ndf.Self_Employed.replace({np.nan: df.Self_Employed.value_counts().keys()[0]},inplace=True)\n\ndf.Education.replace({\"Graduate\": 1, \"Not Graduate\": 0},inplace=True)\n\ndf.Property_Area.replace({\"Urban\": 1, \"Rural\": 0,\"Semiurban\":2},inplace=True)\n\ndf.LoanAmount.replace({np.nan: df.LoanAmount.median()},inplace=True)\n\ndf.Credit_History.replace({np.nan:df.Credit_History.value_counts().keys()[0]},inplace=True)\n\ndf.Loan_Status.replace({\"Y\":1,\"N\":0},inplace=True)\n\ndf.Dependents.replace({\"0\":0,\"1\":1,\"2\":2,\"3+\":3},inplace=True)\ndf.Dependents.replace({np.nan:df.Dependents.value_counts()[0]},inplace=True)\n\n# For continous feature replacing the nan value with the median of the feature\ndf.Loan_Amount_Term.replace({np.nan:df.Loan_Amount_Term.median()},inplace=True)\n\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No missing values in the dataset so missing value handling is done \nsum(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y=df.Loan_Status                                            # Target\ndf.drop([\"Loan_ID\",\"Loan_Status\"],axis=1,inplace=True)      # Features\nX=df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardizing the training dataset\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\nX  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size=0.30, random_state=42,stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Hyperparameter Tuning <br>\n\n- n_estimators = number of trees in the foreset\n- max_features = max number of features considered for splitting a node\n- max_depth = max number of levels in each decision tree\n- min_samples_split = min number of data points placed in a node before the node is split\n- min_samples_leaf = min number of data points allowed in a leaf node\n- bootstrap = method for sampling data points (with or without replacement)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nn_estimators\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n\n#Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n#Minimum number of samples required to split a node\nmin_samples_split = randint(1,10)\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = randint(1,10)\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n#Method to select the tree building criterion\ncriterion = [\"gini\", \"entropy\"]\n\n\n\nrandom_grid={'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n                'criterion':criterion}\n\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# caution : This step takes time to complete based on computational power of the pc\n# Using the random grid to search for best hyperparameters \nrf = RandomForestClassifier()                                     # Defining the classifier object\n\n# Performing a 10 fold cross validation with 50 iteration so total 50*10=500 combinations \nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 10, verbose=2, random_state=42, n_jobs = -1)\n\n#Fit the random search model to get the best parameters \nrf_random.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking what parameters will perform the best for the RandomForest classifier model\n\nprint(rf_random.best_params_)\nprint(rf_random.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building  the model with optimal Parameters \nclassifier=RandomForestClassifier(bootstrap= True,criterion='entropy',max_depth=None,max_features='log2',min_samples_leaf= 6,min_samples_split=8,n_estimators=1800,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model fitting with the optimal parameters \nclassifier.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_idx = classifier.feature_importances_.argsort()\nplt.barh(df.columns[sorted_idx], classifier.feature_importances_[sorted_idx])\nplt.xlabel(\"Random Forest Feature Importance\")\nplt.title(\"Feature Importance for Random Forest classifier\")\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get numerical feature importances\nimportances = list(classifier.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(list(df.columns), importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the most important features that can be used for the classification \nImpFeatures=[]\nfor i in feature_importances:\n    if i[1]>= 0.1:\n        ImpFeatures.append(i[0])\nprint(\"Most Important Features are :\",ImpFeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-creating the dataframe with the important features only \nData={}\nfor i in ImpFeatures:\n    if i in df.columns:\n        Data[i]=df[i]\nfeatures=pd.DataFrame(Data)\n\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the recreated dataframe \n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nScaled_features = sc.fit_transform(features)\nScaled_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the features of the scaled Dataframe \n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(Scaled_features, Y, test_size=0.30, random_state=42,stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nn_estimators\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n\n#Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n#Minimum number of samples required to split a node\nmin_samples_split = randint(1,10)\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = randint(1,10)\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\ncriterion = [\"gini\", \"entropy\"]\n\n\nrandom_grid={'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n                'criterion':criterion}\n\nprint(random_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the best Parameters with the optimal hyperparameters \nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 10, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_random.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building  the model with optimal Parameters \nclassifier=RandomForestClassifier(bootstrap= True,criterion='gini',max_depth=110,max_features='log2',min_samples_leaf= 8,min_samples_split=5,n_estimators=800,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model fitting with the optimal parameters \nclassifier.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the output labels \ny_pred=classifier.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparison dataframe between test and predicted result \n\nDatas={\"Test_Data\":Y_test,\"Predicted_Data\":y_pred}\ncomparison_df=pd.DataFrame(Datas)\ncomparison_df.reset_index(inplace=True,drop=True)\ncomparison_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion Matrix \nconfusion = metrics.confusion_matrix(Y_test, y_pred)\nconfusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nlabels = ['True Neg','False Pos','False Neg','True Pos']\nsns.heatmap(confusion, annot=True,cmap='Blues',fmt='.0f')\nplt.title(\"Heatmap confusion matrix\")\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy of the model \nprint(\"Accuracy : {:.2f}\".format(metrics.accuracy_score(Y_test, y_pred)*100), \"%\")\n# sensitivity of the model\nprint(\"Sensitivity : {:.2f} %\".format((TP / float(TP+FN))*100))\n# specificity of the model \nprint(\"Specificity : {:.2f} %\".format((TN / float(TN+FP))*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classification Report \nprint(metrics.classification_report(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note:\nAlthough this dataset has a seperate test dataset to predict the output value but here the test set is splitted into train and test datasets so that the accuracy matrix can be evaluated.\nHowever, Having fitted the actual test data to the classification model, an accuracy of 0.778 is achieved."},{"metadata":{},"cell_type":"markdown","source":"<h3 align=\"center\"> _____Thank You____ </h3>\n<h3 align=\"center\"> constructive criticism is appriciated </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}