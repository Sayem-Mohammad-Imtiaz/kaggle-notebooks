{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-08T14:28:14.052171Z","iopub.execute_input":"2021-09-08T14:28:14.052809Z","iopub.status.idle":"2021-09-08T14:28:14.065403Z","shell.execute_reply.started":"2021-09-08T14:28:14.052767Z","shell.execute_reply":"2021-09-08T14:28:14.064388Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Amsterdam House Prices\nI will be predicting the House Prices in the Amsterdam House Prices Predictions dataset with the regression models that I am familiar with in this notebook.\n\nTable of Contents:\n- Data Preprocessing\n- Data Cleaning\n- Feature Engineering\n- Optional: Suggestions\n- More Feature Engineering \n- Modeling (Train Test Split + Model Cross Validation)\n- Parameter Tuning & Final Model\n","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing\nOkay - first things first. Let's take a small look at the data.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/amsterdam-house-price-prediction/HousingPrices-Amsterdam-August-2021.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:55:17.197865Z","iopub.execute_input":"2021-09-08T13:55:17.198433Z","iopub.status.idle":"2021-09-08T13:55:17.228717Z","shell.execute_reply.started":"2021-09-08T13:55:17.198396Z","shell.execute_reply":"2021-09-08T13:55:17.227948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can tell that there aren't a lot of features that are created here - but we can definitely create some features out of this - namely from the Address and Zip features. Let's look at the features at a deeper level! ","metadata":{}},{"cell_type":"code","source":"for i in df.columns:\n    fig_dims = (12,8)\n    fig, ax = plt.subplots(figsize = fig_dims)\n    sns.histplot(x=i, data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:29:45.009245Z","iopub.execute_input":"2021-09-07T13:29:45.009636Z","iopub.status.idle":"2021-09-07T13:30:22.994917Z","shell.execute_reply.started":"2021-09-07T13:29:45.009608Z","shell.execute_reply":"2021-09-07T13:30:22.993885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can tell that there are outliers in some of the features - we might want to remove those outliers. Let's decide later on and look at the correlation of the features first.","metadata":{}},{"cell_type":"markdown","source":"# Correlation of features\nLet's do a heatmap to see the correlation between these features. ","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr())","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:18:56.147614Z","iopub.execute_input":"2021-09-07T13:18:56.148124Z","iopub.status.idle":"2021-09-07T13:18:56.395277Z","shell.execute_reply.started":"2021-09-07T13:18:56.14807Z","shell.execute_reply":"2021-09-07T13:18:56.394312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can tell that there is a strong correlation between the price and area of the house itself, and a slightly weaker (but still strong) correlation between the price and rooms features and the price and area features. \nAight, let's start the data cleaning and feature engineering stages. ","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning\nLet's start off with looking at if there is any missing data. ","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:12:56.591081Z","iopub.execute_input":"2021-09-07T13:12:56.591544Z","iopub.status.idle":"2021-09-07T13:12:56.612924Z","shell.execute_reply.started":"2021-09-07T13:12:56.591498Z","shell.execute_reply":"2021-09-07T13:12:56.611936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are missing data points - mainly in the price column. I wouldn't necessarily fill in the prices as it is the metric we are predicting, therefore we will be removing the rows with no price instead.","metadata":{}},{"cell_type":"code","source":"df = df.dropna(axis = 0, inplace = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:55:21.834225Z","iopub.execute_input":"2021-09-08T13:55:21.834626Z","iopub.status.idle":"2021-09-08T13:55:21.849805Z","shell.execute_reply.started":"2021-09-08T13:55:21.834593Z","shell.execute_reply":"2021-09-08T13:55:21.848542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T13:12:56.626526Z","iopub.execute_input":"2021-09-07T13:12:56.627047Z","iopub.status.idle":"2021-09-07T13:12:56.646234Z","shell.execute_reply.started":"2021-09-07T13:12:56.627003Z","shell.execute_reply":"2021-09-07T13:12:56.645034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! We now have a complete dataset. However, if you remember, we were looking at the outliers and considering removing it. Let's take a look at the boxplot first before we decide on anything! ","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='Price', data = df)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T13:55:50.399117Z","iopub.execute_input":"2021-09-08T13:55:50.39977Z","iopub.status.idle":"2021-09-08T13:55:50.542902Z","shell.execute_reply.started":"2021-09-08T13:55:50.399732Z","shell.execute_reply":"2021-09-08T13:55:50.541831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh wow - there are a lot of outliers in this dataset. I would remove most of these data points to have a more accurate regression, but just for fun, let's see how much of the dataset we're removing.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:00:16.276667Z","iopub.execute_input":"2021-09-08T14:00:16.277044Z","iopub.status.idle":"2021-09-08T14:00:16.314206Z","shell.execute_reply.started":"2021-09-08T14:00:16.277006Z","shell.execute_reply":"2021-09-08T14:00:16.313003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the maximum tolerance for the boxplot\nq1 = df.describe()['Price']['25%']\nq3 = df.describe()['Price']['75%']\niqr = q3 - q1\nmax_price = q3 + 1.5 * iqr ","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:00:18.341612Z","iopub.execute_input":"2021-09-08T14:00:18.341999Z","iopub.status.idle":"2021-09-08T14:00:18.384214Z","shell.execute_reply.started":"2021-09-08T14:00:18.341967Z","shell.execute_reply":"2021-09-08T14:00:18.383103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create an outliers dataset so that we can find the count of outliers\noutliers = df[df['Price'] >= max_price]\n\n#Outlier and dataset count followed by percentage of dataset removed\noutliers_count = outliers['Price'].count()\ndf_count = df['Price'].count()\nprint('Percentage removed: ' + str(round(outliers_count/df_count * 100, 2)) + '%')","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:03:04.592088Z","iopub.execute_input":"2021-09-08T14:03:04.592453Z","iopub.status.idle":"2021-09-08T14:03:04.601077Z","shell.execute_reply.started":"2021-09-08T14:03:04.592421Z","shell.execute_reply":"2021-09-08T14:03:04.599823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To be honest, that's a small chunk of data being removed. However, since linear regressions are rather sensitive to outliers, it is best that we remove those. However, if we had more info about housing with the prices close to the outliers, we would definitely be able to train the model better.","metadata":{}},{"cell_type":"code","source":"# Repalcing the old dataframe with the new one  \ndf = df[df['Price'] <= max_price]","metadata":{"execution":{"iopub.status.busy":"2021-09-08T16:20:13.207863Z","iopub.execute_input":"2021-09-08T16:20:13.208305Z","iopub.status.idle":"2021-09-08T16:20:13.215438Z","shell.execute_reply.started":"2021-09-08T16:20:13.208264Z","shell.execute_reply":"2021-09-08T16:20:13.214163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There we go!\n","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\nFrom the data analysis above, you can tell that there are only 5 features, so we definitely need to do some feature engineering to create more features, which is important for training the model. ","metadata":{}},{"cell_type":"code","source":"df['Zip No'] = df['Zip'].apply(lambda x:x.split()[0])\ndf['Letters'] = df['Zip'].apply(lambda x:x.split()[-1])","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:09.653816Z","iopub.execute_input":"2021-09-08T14:08:09.654209Z","iopub.status.idle":"2021-09-08T14:08:09.664829Z","shell.execute_reply.started":"2021-09-08T14:08:09.654176Z","shell.execute_reply":"2021-09-08T14:08:09.663489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we look at the Zip column, we realise that there are 4 digits in the front, and 2 letters at the back. We can split it such that we get 2 new features from the Zip column. ","metadata":{}},{"cell_type":"code","source":"df['Address']","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:11.828126Z","iopub.execute_input":"2021-09-08T14:08:11.828472Z","iopub.status.idle":"2021-09-08T14:08:11.837406Z","shell.execute_reply.started":"2021-09-08T14:08:11.828444Z","shell.execute_reply":"2021-09-08T14:08:11.836345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that the back part of the address isn't important as it is just stating that the address is in Amsterdam. We replace the address column with a less redundant version instead :) ","metadata":{}},{"cell_type":"code","source":"df['Address'] = df['Address'].apply(lambda x:x.split(',')[0]) ","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:14.566448Z","iopub.execute_input":"2021-09-08T14:08:14.566871Z","iopub.status.idle":"2021-09-08T14:08:14.575035Z","shell.execute_reply.started":"2021-09-08T14:08:14.566836Z","shell.execute_reply":"2021-09-08T14:08:14.573844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is definitely not enough as the addresses are too varied. I decided that I will take the street of the address itself as a feature instead. \nHowever, the separation is more complicated than it seems, so I have created a function that allows me to extract the street name from the address itself. ","metadata":{}},{"cell_type":"code","source":"def word_separator(string):\n    list = string.split()\n    word = []\n    number = [] \n    for element in list:\n        if element.isalpha() == True: \n            word.append(element)\n        else:\n            break\n    word = ' '.join(word)\n    return word","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:15.908221Z","iopub.execute_input":"2021-09-08T14:08:15.908605Z","iopub.status.idle":"2021-09-08T14:08:15.91477Z","shell.execute_reply.started":"2021-09-08T14:08:15.908571Z","shell.execute_reply":"2021-09-08T14:08:15.913306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Street'] = df['Address'].apply(lambda x:word_separator(x))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:17.359336Z","iopub.execute_input":"2021-09-08T14:08:17.359759Z","iopub.status.idle":"2021-09-08T14:08:17.367612Z","shell.execute_reply.started":"2021-09-08T14:08:17.359717Z","shell.execute_reply":"2021-09-08T14:08:17.366608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:19.294298Z","iopub.execute_input":"2021-09-08T14:08:19.294675Z","iopub.status.idle":"2021-09-08T14:08:19.312384Z","shell.execute_reply.started":"2021-09-08T14:08:19.294639Z","shell.execute_reply":"2021-09-08T14:08:19.311616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks great to me, don't you think?\n","metadata":{}},{"cell_type":"markdown","source":"# Optional: Suggestions\nThe above features are things that I thought by myself, but there are definitely some other features that you can consider in the model that I believe would make the model more accurate!\n- I personally did not think of using [Price per meter square](https://www.kaggle.com/lennarthaupts/prediction-based-on-the-10-closest-neighbors) but Lennart thought of it, which I thought was pretty impressive! \n- I also did not think of[ putting the districts into bins](https://www.kaggle.com/laetitiafrost/amsterdam-house-price-linreg-randomforest-knn) but Letitia thought of using that, which I thought was a really innovative idea as well. ","metadata":{}},{"cell_type":"markdown","source":"# Further Feature Engineering and Data Processing\nWe split the features into numerical and categorical features so that we are able to convert the categorical features into numerical ones, before training the model with it.","metadata":{}},{"cell_type":"code","source":"numerical = ['Price', 'Area', 'Room', 'Lon', 'Lat']\ncategorical = ['Address', 'Zip No', 'Letters', 'Street']","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:45.70809Z","iopub.execute_input":"2021-09-08T14:08:45.708474Z","iopub.status.idle":"2021-09-08T14:08:45.713364Z","shell.execute_reply.started":"2021-09-08T14:08:45.708437Z","shell.execute_reply":"2021-09-08T14:08:45.712236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a few encoders that I considered using:\n- Label Encoding\n- One Hot Encoding\n- Ordinal Encoding\n\nOne Hot Encoding would not be effective if there were too many features and Ordinal Encoding would be useful if you had to preserve some order ofcategorical data but useless otherwise. Therefore, Label Encoding would be the best choice here. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfor c in categorical:\n    lbl = LabelEncoder() \n    lbl.fit(list(df[c].values)) \n    df[c] = lbl.transform(list(df[c].values))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:46.93004Z","iopub.execute_input":"2021-09-08T14:08:46.93044Z","iopub.status.idle":"2021-09-08T14:08:46.942985Z","shell.execute_reply.started":"2021-09-08T14:08:46.930407Z","shell.execute_reply":"2021-09-08T14:08:46.941823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We drop the more obvious 'features' that we do not need as they're either an index to the dataset or features have been extracted from the dataset. ","metadata":{}},{"cell_type":"code","source":"df.drop(['Zip', 'Unnamed: 0', 'Address'], axis =1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:48.338588Z","iopub.execute_input":"2021-09-08T14:08:48.338992Z","iopub.status.idle":"2021-09-08T14:08:48.347272Z","shell.execute_reply.started":"2021-09-08T14:08:48.338957Z","shell.execute_reply":"2021-09-08T14:08:48.345847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's do a correlation between our new features!","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr())","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:11:07.95868Z","iopub.execute_input":"2021-09-08T14:11:07.959117Z","iopub.status.idle":"2021-09-08T14:11:08.285395Z","shell.execute_reply.started":"2021-09-08T14:11:07.959077Z","shell.execute_reply":"2021-09-08T14:11:08.284175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is now a strong and negative correlation beteen the Zip numbers and the Latitude features!","metadata":{}},{"cell_type":"markdown","source":"# Train Test Split\nWe will split the dataset into two datasets, the train dataset and the test dataset. We will then use cross-validation with negative mean squared error as the scoring feature. We will then make the value positive and square root it to derive the Root Mean Squared Error, which is smaller. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop('Price', axis =1)\ny = df['Price']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:51.602715Z","iopub.execute_input":"2021-09-08T14:08:51.603096Z","iopub.status.idle":"2021-09-08T14:08:51.611884Z","shell.execute_reply.started":"2021-09-08T14:08:51.603063Z","shell.execute_reply":"2021-09-08T14:08:51.610452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:52.994461Z","iopub.execute_input":"2021-09-08T14:08:52.994855Z","iopub.status.idle":"2021-09-08T14:08:53.008397Z","shell.execute_reply.started":"2021-09-08T14:08:52.994821Z","shell.execute_reply":"2021-09-08T14:08:53.007071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression Cross Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\npredictions = linreg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:08:54.289285Z","iopub.execute_input":"2021-09-08T14:08:54.289711Z","iopub.status.idle":"2021-09-08T14:08:54.620613Z","shell.execute_reply.started":"2021-09-08T14:08:54.28968Z","shell.execute_reply":"2021-09-08T14:08:54.619711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(linreg, X_train, y_train, cv=20, scoring = 'neg_mean_squared_error')\nprint(cv)\nprint(abs(cv.mean())**0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:41.578719Z","iopub.execute_input":"2021-09-08T14:31:41.579132Z","iopub.status.idle":"2021-09-08T14:31:41.624079Z","shell.execute_reply.started":"2021-09-08T14:31:41.579094Z","shell.execute_reply":"2021-09-08T14:31:41.623187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lasso Regression Cross Validation\nLasso regression is a type of regression that only uses L1 regularisation.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso = Lasso()\nlasso.fit(X_train, y_train)\npredictions = lasso.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:43.332577Z","iopub.execute_input":"2021-09-08T14:31:43.332923Z","iopub.status.idle":"2021-09-08T14:31:43.341722Z","shell.execute_reply.started":"2021-09-08T14:31:43.332892Z","shell.execute_reply":"2021-09-08T14:31:43.340672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(lasso, X_train, y_train, cv=20, scoring = 'neg_mean_squared_error')\nprint(cv)\nprint(abs(cv.mean())**0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:45.247474Z","iopub.execute_input":"2021-09-08T14:31:45.248011Z","iopub.status.idle":"2021-09-08T14:31:45.296023Z","shell.execute_reply.started":"2021-09-08T14:31:45.247977Z","shell.execute_reply":"2021-09-08T14:31:45.295193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Elastic Net Regression Cross Validation\nElastic Net Regression is a type of regression that uses a ratio of L1 and L2 regularisation. ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nelasticnet = ElasticNet()\nelasticnet.fit(X_train, y_train)\npredictions = elasticnet.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:48.495586Z","iopub.execute_input":"2021-09-08T14:31:48.495952Z","iopub.status.idle":"2021-09-08T14:31:48.504018Z","shell.execute_reply.started":"2021-09-08T14:31:48.495921Z","shell.execute_reply":"2021-09-08T14:31:48.502811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(elasticnet, X_train, y_train, cv=20, scoring = 'neg_mean_squared_error')\nprint(cv)\nprint(abs(cv.mean())**0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:49.86389Z","iopub.execute_input":"2021-09-08T14:31:49.864375Z","iopub.status.idle":"2021-09-08T14:31:49.909952Z","shell.execute_reply.started":"2021-09-08T14:31:49.864344Z","shell.execute_reply":"2021-09-08T14:31:49.909247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ridge Regression Cross Validation\nLasso regression is a type of regression that only uses L2 regularisation.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge()\nridge.fit(X_train, y_train)\npredictions = ridge.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:51.589349Z","iopub.execute_input":"2021-09-08T14:31:51.589912Z","iopub.status.idle":"2021-09-08T14:31:51.595843Z","shell.execute_reply.started":"2021-09-08T14:31:51.589862Z","shell.execute_reply":"2021-09-08T14:31:51.594873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(ridge, X_train, y_train, cv=20, scoring = 'neg_mean_squared_error')\nprint(cv)\nprint(abs(cv.mean())**0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:53.497903Z","iopub.execute_input":"2021-09-08T14:31:53.498262Z","iopub.status.idle":"2021-09-08T14:31:53.544527Z","shell.execute_reply.started":"2021-09-08T14:31:53.498222Z","shell.execute_reply":"2021-09-08T14:31:53.543568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Cross Validation\nFor this case, we'll be using Random Forest regression as it is a regression task.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrandom_forest = RandomForestRegressor()\nrandom_forest.fit(X_train, y_train)\npredictions = random_forest.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:56.522858Z","iopub.execute_input":"2021-09-08T14:31:56.523227Z","iopub.status.idle":"2021-09-08T14:31:56.877708Z","shell.execute_reply.started":"2021-09-08T14:31:56.523195Z","shell.execute_reply":"2021-09-08T14:31:56.876625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(random_forest, X_train, y_train, cv=20, scoring = 'neg_mean_squared_error')\nprint(cv)\nprint(abs(cv.mean())**0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:58.088572Z","iopub.execute_input":"2021-09-08T14:31:58.088934Z","iopub.status.idle":"2021-09-08T14:32:04.711443Z","shell.execute_reply.started":"2021-09-08T14:31:58.088904Z","shell.execute_reply":"2021-09-08T14:32:04.710316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Cross Validation\n","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(X_train, y_train)\npredictions = xgb.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:12:34.633139Z","iopub.execute_input":"2021-09-08T14:12:34.633561Z","iopub.status.idle":"2021-09-08T14:12:35.000326Z","shell.execute_reply.started":"2021-09-08T14:12:34.633515Z","shell.execute_reply":"2021-09-08T14:12:34.999111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(xgb, X_train, y_train, cv=20, scoring = 'neg_mean_squared_error')\nprint(cv)\nprint(abs(cv.mean())**0.5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:31:23.203813Z","iopub.execute_input":"2021-09-08T14:31:23.204513Z","iopub.status.idle":"2021-09-08T14:31:25.347516Z","shell.execute_reply.started":"2021-09-08T14:31:23.204449Z","shell.execute_reply":"2021-09-08T14:31:25.346291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the different cross validation data, we can deduce that the Random Forest Regression model is the best model for this dataset. Of course, XGBoost is something else that we can also consider, since the RMSE differs by approximately 2000. We will continue with hyperparameter tuning using a random search before using GridSearchCV for parameter tuning. ","metadata":{}},{"cell_type":"markdown","source":"# Parameter Tuning\n- We first use a RandomizedSearchCV so that we are able to get a rough estimate for a range of parameters. \n- After getting the best parameters, we will use GridSearchCV to further tune the parameters itself before finally taking the best parameters for the model.","metadata":{}},{"cell_type":"markdown","source":"# RandomizedSearchCV Tuning \nWe will start with a RandomizedSearchCV so that we are able to get a general direction for the parameters.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nrandom_grid = {'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n\nrandom_cv = RandomizedSearchCV(estimator = random_forest, param_distributions = random_grid, n_iter = 100, cv = 10, verbose = 2, n_jobs = -1)\nrandom_cv.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:39:30.553899Z","iopub.execute_input":"2021-09-08T14:39:30.554409Z","iopub.status.idle":"2021-09-08T14:58:32.048599Z","shell.execute_reply.started":"2021-09-08T14:39:30.554377Z","shell.execute_reply":"2021-09-08T14:58:32.047188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_cv.best_params_ ","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:58:35.684093Z","iopub.execute_input":"2021-09-08T14:58:35.684485Z","iopub.status.idle":"2021-09-08T14:58:35.691613Z","shell.execute_reply.started":"2021-09-08T14:58:35.684449Z","shell.execute_reply":"2021-09-08T14:58:35.690173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! We've got a slight idea of what parameters would optimize the model itself.","metadata":{}},{"cell_type":"markdown","source":"# GridSearchCV \nNow, we will use GridSearchCV so that we are able to get a more specific set of parameters for the model itself.","metadata":{}},{"cell_type":"code","source":"param_grid = {'bootstrap': [True, False],\n'max_depth': [60,65,70,75,80],\n'min_samples_leaf':[1,2,3],\n'min_samples_split': [1,2,3],\n'n_estimators': [1750,1760,1770,1780,1790,1800,1810,1820,1830,1840,1850]}\ngrid_search = GridSearchCV(estimator = random_forest, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T14:59:53.662912Z","iopub.execute_input":"2021-09-08T14:59:53.663301Z","iopub.status.idle":"2021-09-08T16:09:03.133788Z","shell.execute_reply.started":"2021-09-08T14:59:53.66327Z","shell.execute_reply":"2021-09-08T16:09:03.132784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-09-08T16:12:31.284909Z","iopub.execute_input":"2021-09-08T16:12:31.28532Z","iopub.status.idle":"2021-09-08T16:12:31.292092Z","shell.execute_reply.started":"2021-09-08T16:12:31.285286Z","shell.execute_reply":"2021-09-08T16:12:31.291019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And there we have it! The best parameters for the model itself. ","metadata":{}},{"cell_type":"markdown","source":"# Final Model\nWe've come to the final step of the notebook itself. We will be implementing the best parameters into the model, and training the model with the training data. We will use cross validation again to get a RMSE value and see if there is any improvement to the model itself.","metadata":{}},{"cell_type":"code","source":"tuned_random_forest = RandomForestRegressor(n_estimators = 1750, max_depth = 80, min_samples_leaf = 1, min_samples_split = 2)\nrandom_forest.fit(X_train, y_train)\npredictions = random_forest.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T16:13:50.536262Z","iopub.execute_input":"2021-09-08T16:13:50.536712Z","iopub.status.idle":"2021-09-08T16:13:50.896214Z","shell.execute_reply.started":"2021-09-08T16:13:50.536667Z","shell.execute_reply":"2021-09-08T16:13:50.895074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(tuned_random_forest, X_train, y_train, cv=20, scoring = 'neg_mean_squared_error')\nprint(\"The Random Forest Regressor with tuned parameters has a RMSE of: \" + str(abs(cv.mean())**0.5))","metadata":{"execution":{"iopub.status.busy":"2021-09-08T16:13:51.998092Z","iopub.execute_input":"2021-09-08T16:13:51.998472Z","iopub.status.idle":"2021-09-08T16:15:47.399111Z","shell.execute_reply.started":"2021-09-08T16:13:51.998439Z","shell.execute_reply":"2021-09-08T16:15:47.397967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems like we've got a slight improvement with the model itself! We can optimise it even further, but with the amount of time it took for GridSearchCV, it might not necessarily be a good idea. ","metadata":{}},{"cell_type":"markdown","source":"# The End\nThank you so much for reading my notebook! I appreciate it :) \n","metadata":{}}]}