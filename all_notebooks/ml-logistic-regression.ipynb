{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Имеется датасет, полученный в ходе исследования факторов, способных повлиять на депрессию людей, живущих в сельской местности. Задача модели ML --- научиться определять склонность к депрессии (0 --- не страдает, 1 --- страдает). Это типичная задача **бинарной классификации**. В лекции мы рассмотрим одну из популярных моделей для этой задачи --- логистическую регрессию."},{"metadata":{},"cell_type":"markdown","source":"# Логистическая регрессия\n\n## Почему регрессия?\n\nПрежде всего может возникнуть вопрос: почему для решения задачи классификации применяется модель под названием \"регрессия\"? Давайте разберёмся.\n\nТак сложилось, что в задачах бинарной классификации мы большее значение придаём тому, чтобы научиться определять класс 1, нежели класс 0. Класс 1 представляет собой в каком-то смысле \"редкое\" событие: письмо со спамом, неплатёжеспособность клиента, наличие заболевания, склонность к депрессии. Будем называть его \"положительным\" классом, а класс 0 --- \"отрицательным\". Во многих ситуациях нам важнее знать **оценку вероятности** того, что объект попадёт в положительный класс (вероятность невозврата кредита, вероятность заболевания, вероятность склонности к депрессии). Тогда, задавая различные пороги для этой вероятности (в зависимости от задачи), мы будем таким образом разделять объекты.\n\nОбозначим вероятность положительного события через $P_+$. Очевидно, что вероятность противоположного класса тогда равна $(1-P_+)$. Составим величину, называемую odds ratio (\"отношение шансов\"):\n\n$$\nOR = \\frac{P_+}{1-P_+}.\n$$\n\n$OR$ показывает отношение вероятностей того, произойдёт наше событие или не произойдёт. При этом величины $P_+$ и $OR$ содержат, по сути, одинаковую информацию. Но если $P_+\\in(0; 1)$, то $OR\\in(0; +\\infty)$. Если теперь рассмотреть величину логарифма отношения шансов $\\log OR$, то мы придём к тому, что $\\log OR\\in(-\\infty; +\\infty)$. И вот мы получили, что можем прогнозировать **вероятность**, прогнозируя величину, лежащую от минус до плюс бесконечности! А эту величину можно предсказывать с помощью **регрессионной модели**. Voilà :)\n\n## Сигма-функция\n\nПусть теперь мы прогнозируем $\\log OR$ с помощью линейной регрессии: $\\log OR=\\mathbf{w}^T\\mathbf{x}$. Как из этой величины получить $P_+$?\n\n$$\nP_+ = \\frac{OR}{1+OR} = \\frac{e^{\\log OR}}{1+e^{\\log OR}}=\\frac{e^{\\mathbf{w}^T\\mathbf{x}}}{1+e^{\\mathbf{w}^T\\mathbf{x}}}=\\frac{1}{1+e^{-\\mathbf{w}^T\\mathbf{x}}}.\n$$\n\nИнтересно, что мы получили здесь довольно важную функцию в машинном обучении (нет, на этот раз не логарифм :)):\n\n$$\n\\sigma(z) = \\frac{1}{1+e^{-z}},\n$$\n\nназываемую сигма-функцией, или сигмоидой, или **логистической функцией**. Она ведёт себя так:\n![](https://www.researchgate.net/profile/John_Davis82/publication/234049070/figure/fig6/AS:300093912698893@1448559372458/Logistic-sigmoid-function-Maps-real-numbers-to-the-interval-between-0-and-1.png)\n\nИтак, $P_+=\\sigma(\\mathbf{w}^T\\mathbf{x})$. Это важный результат.\n\nТем самым, с помощью логистической регрессии мы будем прогнозировать **вероятность принадлежности объекта к классу 1**.\n\n## LogLoss, или логистическая функция потерь\n\nСледующий важный момент: **как считать ошибку модели в случае бинарной классификации**? Иными словами, как выбрать функцию потерь и составить функционал качества?\n\nВновь вспомним принцип максимального правдоподобия. Только что мы получили, что\n\n$$\nP_+ = P(y=1 | \\mathbf{x}_i, \\mathbf{w}) = \\sigma(\\mathbf{w}^T\\mathbf{x}).\n$$\n\nНо тогда\n\n$$\nP_- = P(y=-1 | \\mathbf{x}_i, \\mathbf{w}) = 1 - P_+ = 1 - \\sigma(\\mathbf{w}^T\\mathbf{x}_i).\n$$\n\nЗдесь мы обозначили класс \"0\" как \"-1\" для удобства выкладок. Далее можно заметить (проверьте!), что $1 - \\sigma(\\mathbf{w}^T\\mathbf{x}_i)=\\sigma(-\\mathbf{w}^T\\mathbf{x}_i)$: первое из поразительных свойств сигмоиды --- то ли ещё будет! Учитывая последнее, можно записать **общую формулу** для вероятности:\n\n$$\nP(y=y_i | \\mathbf{x}_i, \\mathbf{w}) = \\sigma(y_i\\mathbf{w}^T\\mathbf{x}_i).\n$$\n\nТогда вероятность всей выборки (взятой независимо и из одного распределения) запишется как произведение вероятностей:\n\n$$\nP(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}) = \\prod_{i=1}^{l} P(y=y_i | \\mathbf{x}_i, \\mathbf{w}).\n$$\n\nЭто **функция правдоподобия выборки**. Её нужно максимизировать. Как обычно, вместо максимизации произведения множества слагаемых нам бы хотелось максимизировать сумму. Поэтому возьмём логарифм правдоподобия:\n\n$$\n\\begin{multline*}\n\\log P(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}) = \\log \\prod_{i=1}^{l} P(y=y_i | \\mathbf{x}_i, \\mathbf{w}) = \n\\sum_{i=1}^{l} \\log P(y=y_i | \\mathbf{x}_i, \\mathbf{w}) = \\\\\n=\\sum_{i=1}^{l} \\log \\sigma(y_i\\mathbf{w}^T\\mathbf{x}_i) = \\sum_{i=1}^{l} \\log \\frac{1}{1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}}=-\\sum_{i=1}^{l}\\log \\left(1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}\\right) \\to \\max\\limits_\\mathbf{w},\n\\end{multline*}\n$$\n\nто есть максимизация прадоподобия эквивалентна минимизации функционала\n\n$$\nQ(\\mathbf{w}) = \\sum_{i=1}^{l}\\log \\left(1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}\\right) \\to \\min\\limits_\\mathbf{w}.\n$$\n\nПоследний функционал под знаком суммы содежит **логистическую функцию потерь**, или лог-лосс: $L_{log}(z)=\\log(1+e^{-z})$. Её график выглядит следующим образом:\n![](https://www.researchgate.net/profile/Thomas_Hofmann10/publication/2877976/figure/fig1/AS:650449568923660@1532090670890/Loss-values-of-0-1-exp-and-log-loss-functions-in-a-binary-classification-problem.png)\n\nи мажорирует обычную пороговую функцию потерь:\n$$\nL(y, \\widehat{y}) = \\begin{cases}0, & \\text{if } y=\\widehat{y}, \\\\ 1, & \\text{if } y\\neq\\widehat{y}. \\end{cases}\n$$\n\nИтак, мы только что показали, что, с точки зрения статистики и метода максимального правдоподобия, **оптимальным выбором функции потерь** для логистической регрессии является **лог-лосс**, или **логистическая функция потерь**. Минимизируя лог-лосс, мы тем самым уменьшаем число ошибок классификации.\n\n## Регуляризация логистической регрессии\n\nВ моделях логистической регрессии, как и в линейной регрессии, используют разные типы **регуляризации** ($L_1$, $L_2$, Elastic Net) для уменьшения переобучения модели. Функционал качества в случае $L_2$-регуляризации выглядит так:\n\n$$\nQ_{reg}(\\mathbf{w}) = C\\cdot \\sum_{i=1}^{l}\\log \\left(1+e^{-y_i\\mathbf{w}^T\\mathbf{x}_i}\\right)+\\|\\mathbf{w}\\|^2 \\to \\min\\limits_\\mathbf{w}.\n$$\n\nЗдесь гиперпараметр $C>0$ играет роль **обратной силы регуляризации**. Чем больше $C$, тем меньше \"штраф\" за увеличение весов модели. И наоборот, чем меньше $C$, тем эффект регуляризации больше."},{"metadata":{},"cell_type":"markdown","source":"## Почему логистическая регрессия --- это линейный классификатор?\n\nМы ещё не обсудили вопрос, почему же рассмотренная нами модель относится к **линейным моделям**. Для этого рассмотрим игрушечный пример с точками на плоскости."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"np.random.seed(0)\nimport matplotlib.pyplot as plt\nX = np.random.normal(loc=0.5, scale=0.25, size=(100, 2))\ny = (X[:, 1] > X[:, 0]).astype('int') # разделяющая граница: y=x (биссектриса первой четверти)\nplt.scatter(X[:, 0], X[:, 1], color=['red' if c==1 else 'blue' for c in y])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\nw = log_reg.coef_\nbb = log_reg.intercept_\n\nprint(w, bb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Построим разделяющую границу между классами, которую \"выучила\" наша модель. Для этого запишем уравнение\n$$\nw_1\\cdot x_1 + w_2\\cdot x_2 + b = 0\n$$\nи выразим из него $x_2$ для более привычного построения графика:\n$$\nx_2 = -\\frac{w_1}{w_2}x_1-\\frac{b}{w_2}.\n$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"w1 = w[0][0]\nw2 = w[0][1]\nb = bb[0]\n\nprint('w1 = '+str(w1), '\\nw2 = '+str(w2), '\\nb = '+str(b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X[:, 0], X[:, 1], color=['red' if c==1 else 'blue' for c in y])\nplt.plot(X[:, 0], -w1/w2*X[:, 0]-b/w2, color='green', linewidth=3, linestyle=\"dashed\")\n\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итак, модель логистической регрессии почти идеально восстановила нашу разделяющую границу. А что будет, если разделяющая граница нелинейна?"},{"metadata":{"trusted":true},"cell_type":"code","source":"yy = ((X[:, 0]-0.5)**2 + (X[:, 1]-0.5)**2 > 0.25**2).astype('int')\nplt.scatter(X[:, 0], X[:, 1], color=['red' if c==1 else 'blue' for c in yy])\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Теперь посмотрим, что выучит наша модель."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression()\nlog_reg.fit(X, yy)\n\nw1 = log_reg.coef_[0][0]\nw2 = log_reg.coef_[0][1]\nb = log_reg.intercept_[0]\n\nplt.scatter(X[:, 0], X[:, 1], color=['red' if c==1 else 'blue' for c in yy])\nplt.plot(X[:, 0], -w1/w2*X[:, 0]-b/w2, color='green', linewidth=3, linestyle=\"dashed\")\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, несмотря на изменения в исходных данных, модель по-прежнему \"учит\" только линейную границу между классами, причём в данном случае далеко от идеала.\n\nТем не менее, в многомерных пространствах картинка получается лучше, и даже линейный классификатор способен дать хороший результат. Кроме того, мы можем использовать приём, который называется **\"создание полиномиальных признаков\"**. Это означает, что, помимо $x_1, x_2, \\ldots, x_n$, модель также рассматривает их попарные произведения степени не выше $k$ (в случае $k=2$ это будут $x_1^2$, $x_1x_2$, $x_2^2$) в качестве признаков. Этот приём описан в [статье](https://habr.com/ru/company/ods/blog/323890/) OpenDataScience-сообщества (раздел 4). "},{"metadata":{},"cell_type":"markdown","source":"# Логистическая регрессия в действии"},{"metadata":{},"cell_type":"markdown","source":"## Загрузка и препроцессинг"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/depression/b_depressed.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Все столбцы числовые, однако данные всё ещё не готовы к построению модели. Нам необходимо:\n* убрать пропуски (в столбце **no_lasting_investmen**);\n* убрать лишний признак **Survey_id** (номер опроса);\n* перевести категориальные признаки в бинарные (get_dummies);\n* стандартизировать признаки с неудобным масштабом."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Удалим пропуски\ndf_1 = df.dropna()\n\n# Дропнем ненужные столбцы\ndf_2 = df_1.drop(['Survey_id', 'depressed'], axis=1)\n\n# Переведём признаки \"Номер виллы\" и \"Уровень образования\" в бинарные \n# * мы не уверены на 100 %, что уровень образования ранговый, поэтому считаем его категориальным\ndf_3 = pd.get_dummies(df_2, columns=['Ville_id', 'education_level'])\n\n# Масштабирование\ncol_names = df.columns.values # это имена всех столбцов\nlarge_numbers = [col for col in col_names if df[col].mean() > 10000] # имена тех, у кого среднее > 10000\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_st = scaler.fit_transform(df_3[large_numbers])\n\n# Переприсвоим старым колонкам новые\ndf_3[large_numbers] = X_st\n\ndf_3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Построение модели и валидация"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_3\ny = df_1['depressed']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_valid)\n\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Метрики на основе confusion matrix\n\nРазберёмся, так ли хороша наша модель, как кажется (accuracy=84.7 %). Для начала посмотрим на соотношение классов в задаче."},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts(normalize=True).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Мы видим, что объектов класса 0 в выборке значительно больше (83.3 %), чем объектов класса 1. Если бы мы решили построить константный классификатор, который предсказывает 0 ВСЕГДА, то accuracy такого классификатора была бы примерно 83.3 %. Из этого можно сделать вывод, что accuracy --- далеко не лучший выбор метрики качества для случая несбалансированных классов.\n\nКакие же есть альтернативы? Прежде всего, это матрица ошибок (confusion_matrix). В ней по строкам расположены истинные значения классов, а по столбцам --- предсказанные моделью. Их обычно обзначают так:\n$$\n\\text{Confusion_Matrix}=\n\\begin{array}{c|cc} \n& \\text{0_pred} & \\text{1_pred} \\\\ \\hline\n\\text{0_true} & TN & FP \\\\ \n\\text{1_true} & FN & TP\\end{array}\n$$\n\nЗдесь:\n* $TN$ --- True Negative (правильный отрицательный) --- количество объектов класса 0, которые модель опознала верно;\n* $TP$ --- True Positive (правильный положительный) --- количество объектов класса 1, которые модель опознала верно;\n* $FP$ --- False Positive (ложный положительный) --- количество объектов класса 0, которые модель ошибочно отнесла к классу 1;\n* $FN$ --- False Negative (ложный отрицательный) --- количество объектов класса 1, которые модель ошибочно отнесла к классу 0.\n\nВ идеальном случае данная матрица содержит нули вне главной диагонали. На практике же, в зависимости от задачи, мы бы хотели **минимизировать** значения $FN$ либо $FP$. К примеру, в задаче обнаружения спама нам бы не хотелось, чтобы модель помечала как спам нормальные письма, т. е. хотим как можно меньше **ложных срабатываний** (False Positive). В задаче же медицинской диагностики куда важнее не пропустить больных (т. е. нужно минимизировать **пропуски класса 1**, False Negative).\n\nПостроим такую матрицу в нашей задаче обнаружения склонности к депрессии."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix\nprint(confusion_matrix(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(log_reg, X_valid, y_valid, values_format='5g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Итак, наша модель определяет 4 человек как склонных к депрессии (в то время как они \"здоровы\"), а также 50 человек помечает \"здоровыми\", в то время как они склонны к депрессии. Последнее обстоятельство явно не очень хорошо. Ведь всего у нас было 52 случая склонности к депрессии, из которых модель сумела найти только два!"},{"metadata":{},"cell_type":"markdown","source":"### Точность, полнота и F-мера\n\nНа основе матрицы ошибок можно построить следующие три метрики, каждая из которых лучше отображает результаты классификации для несбалансированных классов. Это **точность** (precision), **полнота** (recall) и **F-мера** (f1_score).\n\n$$\n\\text{Confusion_Matrix}=\n\\begin{array}{c|cc} \n& \\text{0_pred} & \\text{1_pred} \\\\ \\hline\n\\text{0_true} & TN & FP \\\\ \n\\text{1_true} & FN & TP\\end{array}\n$$\n\n* $\\text{Precision}=\\dfrac{TP}{TP+FP}$: показывает, насколько можно \"доверять\" модели, если она показала класс 1;\n* $\\text{Recall}=\\dfrac{TP}{TP+FN}$: показывает, как хорошо модель умеет находить класс 1;\n* $F_1=\\dfrac{2\\cdot \\text{Precision}\\cdot \\text{Recall}}{\\text{Precision}+\\text{Recall}}$: гармоническое среднее между точностью и полнотой.\n\nТаким образом, \n* если мы хотим минимизировать $FP$, то лучше в качестве метрики брать precision;\n* если мы хотим минимизировать $FN$, то лучше в качестве метрики брать recall;\n* если мы сомневаемся или хотим минимизировать оба значения, лучше подойдёт f1_score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))\nprint('F1 score:', f1_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Настройка гиперпараметров логистической регрессии по f1_score\n\nПопробуем настроить параметр C (силу регуляризации) для каждого типа регуляризации."},{"metadata":{},"cell_type":"markdown","source":"### Регуляризация L2 (по умолчанию)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nlog_reg = LogisticRegression(solver='liblinear')\n\nC_values = {'C': np.logspace(-3, 3, 10)}\nlogreg_grid = GridSearchCV(log_reg, C_values, cv=5, scoring='f1')\nlogreg_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logreg_grid.best_params_)\nprint(logreg_grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = pd.DataFrame(logreg_grid.cv_results_)\nplt.plot(results_df['param_C'], results_df['mean_test_score'])\n\n# Подписываем оси и график\nplt.xlabel('C')\nplt.ylabel('Test accuracy')\nplt.title('Validation curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Регуляризация L1"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg = LogisticRegression(solver='liblinear', penalty='l1')\n\nC_values = {'C': np.logspace(-3, 3, 10)}\nlogreg_grid = GridSearchCV(log_reg, C_values, cv=5, scoring='f1')\nlogreg_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logreg_grid.best_params_)\nprint(logreg_grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = pd.DataFrame(logreg_grid.cv_results_)\nplt.plot(results_df['param_C'], results_df['mean_test_score'])\n\n# Подписываем оси и график\nplt.xlabel('C')\nplt.ylabel('Test accuracy')\nplt.title('Validation curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg_grid.best_estimator_.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Стала ли наша модель лучше? Однозначно сказать трудно :)\n\nОднако уже понятно, что модель недообучена (слабый результат на train + слабый результат на valid). Возможно, в неё стоит добавить полиномиальные фичи. Но тогда возникнет проблема: число признаков больше числа объектов. Другая возможная причина: у нас просто мало данных (всего около 1400 объектов)."},{"metadata":{},"cell_type":"markdown","source":"# Попытки улучшить модель"},{"metadata":{"trusted":true},"cell_type":"code","source":"# kNN (не помог)\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_params = {'n_neighbors': np.arange(1, 50, 2)}\nknn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='f1')\nknn_grid.fit(X_train, y_train)\n\ny_pred = knn_grid.best_estimator_.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest (не помог)\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Искусственное добавление объектов класса 1\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Проверим баланс\ny_ros.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Логистическая регрессия с добавлением класса 1\nlogreg_ros = LogisticRegression(solver='liblinear')\nlogreg_ros.fit(X_ros, y_ros)\ny_pred = logreg_ros.predict(X_valid)\n\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Подбор гиперпараметров\nlogreg_params = {'C': np.logspace(-3, 3, 10), 'penalty': ['l2', 'l1']}\nlogreg_grid = GridSearchCV(logreg_ros, logreg_params, cv=5, scoring='f1')\nlogreg_grid.fit(X_ros, y_ros)\n\ny_pred = logreg_grid.best_estimator_.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}