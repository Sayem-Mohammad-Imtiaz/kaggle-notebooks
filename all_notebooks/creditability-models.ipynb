{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.ovh.com/blog/wp-content/uploads/2020/05/7A7FC4A4-9AEB-4151-90CD-C7B544E3F189-1024x537.png)ovh.com"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Code by princessa https://www.kaggle.com/princessa/heart-disease-prediction-9-models-ann-cat-xgb/notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.naive_bayes import GaussianNB\nimport pandas_profiling as pp\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndata = pd.read_csv('../input/cusersmarildownloadsgermancsv/german.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndata.dataframeName = 'german.csv'\nnRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Creditability\"]\nX = data.drop('Creditability',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#DecisionTreeClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = DecisionTreeClassifier(criterion = 'entropy',random_state=1,max_depth = 5)\ntr.fit(X_train, y_train)\ntr_predicted = tr.predict(X_test)\ntr_acc_score = accuracy_score(y_test, tr_predicted)\nprint(\"Accuracy of DecisionTreeClassifier:\",tr_acc_score*100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=60, random_state=12,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#KNeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"KneiCl = KNeighborsClassifier(n_neighbors=7)\nKneiCl.fit(X_train, y_train)\nKneiCl_predicted = KneiCl.predict(X_test)\nKneiCl_acc_score = accuracy_score(y_test, KneiCl_predicted)\nprint(\"Accuracy of K-NeighborsClassifier:\",KneiCl_acc_score*100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#XGBClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=30, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#CatBoostClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\nclf = CatBoostClassifier(\n    iterations=30, \n    learning_rate=0.3,depth = 3 )\nclf.fit(X_train, y_train,  plot=True)\npredicted = clf.predict(X_test)\npredicted_proba = clf.predict(X_test)\nprint(\"Accuracy is: \"+ str(clf.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#GaussianNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#ExtraTreesClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"ext = ExtraTreesClassifier()\next.fit(X_train , y_train)\nextpred = ext.predict(X_test)\next_acc_score = accuracy_score(y_test , extpred)\nprint(\"Accuracy of ExtraTreesClassifier model:\" , ext_acc_score * 100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#AdaBoostClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostClassifier()\nada.fit(X_train, y_train)\nadapred = ada.predict(X_test)\nada_acc_score = accuracy_score(y_test, adapred)\nprint(\"Accuracy of AdaBoostClassifier model : \", ada_acc_score * 100,'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Artificial Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Attention snippet below: input dim=20 number I wrote 1 less 21, which is the shape(21) of the data. \n\nOn the original input dim was 13 because data has 14 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(128,activation=\"relu\",input_dim=20))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128,activation=\"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist=model.fit(X_train,y_train,batch_size=60,epochs= 83,validation_data=(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#According to that code ExtraTreesClassifier achieved the best \"Accuracy\":\n\nAccuracy of ExtraTreesClassifier model: 80.0 \n\nSince I'm not a DS I can't interpret what those models are showing. I've just run the cells."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('Thanks to princessa for all the code' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}