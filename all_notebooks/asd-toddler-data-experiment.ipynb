{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\n\nfrom sklearn.metrics import precision_recall_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pricision_recall(model,X_test,y_test,title_name):\n    # -------- get probability of Prediction for true result only using predict_proba func. --------\n    y_pred = model.predict_proba(X_test)[:,1]\n\n    # -------- get precision,recall and thresholds to Drawing the curve to clarify result ----------\n    precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n    \n    # --------------------- Intialize Graph ---------------------\n    plt.xlabel(\"Threshhold\")\n    plt.ylabel(\"Precision/Recall\")\n    plt.title('Precision/Recall '+title_name)\n\n    # --------------------- Drawing the graph ---------------------\n    plt.plot(thresholds,precision[:-1],label='Precision')\n    plt.plot( thresholds,recall[:-1], label='Recall')\n\n    # --------------------- Set Legend on the graph ---------------------\n    plt.legend(bbox_to_anchor=(1.28,1), loc='best', borderaxespad=0.)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# --------------------- Load Dataset using panda ---------------------\ndata = pd.read_csv(\"/kaggle/input/autism-screening-for-toddlers/Toddler Autism dataset July 2018.csv\")\n\n# --------------------- Remove missing values in panda using dropna func. ---------------------\ndata = data.dropna()\n\n# --------------------- Adjust columns names for dataset ---------------------\ndata.columns = [\n    \"Case_No\",\n    \"A1\",\n    \"A2\",\n    \"A3\",\n    \"A4\",\n    \"A5\",\n    \"A6\",\n    \"A7\",\n    \"A8\",\n    \"A9\",\n    \"A10\",\n    \"Age_Mons\",\n    \"Qchat-10-Score\",\n    \"Sex\",\n    \"Ethnicity\",\n    \"Jaundice\",\n    \"Family_mem_with_ASD\",\n    \"Who_completed_the_test\",\n    \"Class/ASD_Traits\",\n]\n\n# --------------------- Print first 10 examples in dataset to explore it ---------------------\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------- Filtering dataset fron case number column ---------------------\ndata = data.drop(columns=['Case_No','Qchat-10-Score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------- Print out the unique values for each column ---------------------\nfor column_name in data.columns:\n    print(\n        \"\"\"\n    {column_name}:\n    {unique_values}\"\"\".format(\n            column_name=column_name,\n            unique_values=\", \".join(\n                map(str, data[column_name].unique())\n            ),\n        )\n    )\n    \nprint(\n    \"\"\"\nNUMBER OF EXAMPLES:{}\nNUMBER OF COLUMNS: {}\n\"\"\".format(\n        data.shape[0], data.shape[1]\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# - Draw histogram for each column with class/asd to see which columns Influential in class/ASD -\nbin_data=pd.DataFrame.copy(data)\n\n# --------------------- Encode Class/ASD_Traits unique values to 1 or 0 ---------------------\nbin_data['Class/ASD_Traits']=bin_data['Class/ASD_Traits'].apply(lambda x:1 if x==\"Yes\" else 0)\n\nfor column_name in data.columns:\n    if(column_name == 'Class/ASD_Traits'):\n        continue;\n    group_by_modelLine = bin_data[[column_name,'Class/ASD_Traits']].groupby(by=[column_name])\n# --------------------- get mean to draw histogram ---------------------    \n    mean=group_by_modelLine.mean().reset_index()\n    mean.columns=[column_name,'positive']\n    mean['negative']=mean['positive'].apply(lambda x: 1-x)\n    mean=pd.melt(mean, id_vars=column_name, var_name=\"Pos/Neg\", value_name=\"Class/ASD_Traits\")\n    p=sns.barplot(x=column_name, y='Class/ASD_Traits', hue='Pos/Neg', data=mean.reset_index())\n    p.legend(loc='best', bbox_to_anchor=(1.28, 1), ncol=1)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------- The columns will not change ---------------------\norg_data = data[\n    [\n        \"A1\",\n        \"A2\",\n        \"A3\",\n        \"A4\",\n        \"A5\",\n        \"A6\",\n        \"A7\",\n        \"A8\",\n        \"A9\",\n        \"A10\",\n        \"Age_Mons\",\n    ]\n]\n\n# --------------------- The columns wich will be encoded using label encoding -------------------\nlabel_data = data[\n    [\n        \"Sex\",\n        \"Jaundice\",\n        \"Family_mem_with_ASD\",\n        \"Class/ASD_Traits\",\n    ]\n]\nlabel_data[\"Sex\"] = label_data[\"Sex\"].apply(\n    lambda x: 1 if x == \"m\" else 0\n)\nlabel_data[\"Jaundice\"] = label_data[\"Jaundice\"].apply(\n    lambda x: 1 if x == \"yes\" else 0\n)\nlabel_data[\"Family_mem_with_ASD\"] = label_data[\n    \"Family_mem_with_ASD\"\n].apply(lambda x: 1 if x == \"yes\" else 0)\nlabel_data[\"Class/ASD_Traits\"] = label_data[\n    \"Class/ASD_Traits\"\n].apply(lambda x: 1 if x == \"Yes\" else 0)\n\n# --------------------- The columns wich will be encoded using one hot encoding -----------------\none_hot_encoded_data = data[\n    [\"Ethnicity\", \"Who_completed_the_test\"]\n]\none_hot_encoded_data = pd.get_dummies(one_hot_encoded_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------- concatenate all columns ---------------------\nfinal_data = pd.concat([org_data,label_data,one_hot_encoded_data],axis = 1)\n\n# --------------------- Droping column Class/ASD_Traits from features ---------------------\nX = final_data.drop(columns=['Class/ASD_Traits'])\ny = final_data[['Class/ASD_Traits']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------- Splitting data for test data and train data ---------------------\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_train, X_val, y_train, y_val   = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------- Create LogisticRegression model and train it ---------------------\nclf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)\nplt.subplots(1,2,figsize=(15,5))\nplt.subplots_adjust(wspace=0.5)\nplt.subplot(121)\nplot_pricision_recall(clf,X_train,y_train,'LogisticRegression-Train')\nplt.subplot(122)\nplot_pricision_recall(clf,X_val,y_val,'LogisticRegression-Validation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------- RandomForestClassifier ---------------------\nrfc= RandomForestClassifier(n_estimators=500)\nrfc.fit(X_train,y_train)\n\nplt.subplots(1,2,figsize=(15,5))\nplt.subplots_adjust(wspace=0.5)\nplt.subplot(121)\nplot_pricision_recall(rfc,X_train,y_train,'RandomForest-Train')\nplt.subplot(122)\nplot_pricision_recall(rfc,X_val,y_val,'RandomForest-Validation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn= KNeighborsClassifier(n_neighbors=13)\nknn.fit(X_train,y_train)\n\nplt.subplots(1,2,figsize=(15,5))\nplt.subplots_adjust(wspace=0.5)\nplt.subplot(121)\nplot_pricision_recall(knn,X_train,y_train,'KNearestNeighbours-Train')\nplt.subplot(122)\nplot_pricision_recall(knn,X_val,y_val,'KNearestNeighbours-Validation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = svm.SVC(probability=True)\nsvm.fit(X_train, y_train) \n\nplt.subplots(1,2,figsize=(15,5))\nplt.subplots_adjust(wspace=0.5)\nplt.subplot(121)\nplot_pricision_recall(svm,X_train,y_train,'SVM-Train')\nplt.subplot(122)\nplot_pricision_recall(svm,X_val,y_val,'SVM-Validation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(1,2,figsize=(15,5))\nplt.subplots_adjust(wspace=0.5)\nplt.subplot(121)\nplot_pricision_recall(clf,X_test,y_test,'LogisticRegression-Test')\nplt.subplot(122)\nplot_pricision_recall(rfc,X_test,y_test,'RFC-Test')\nplt.show()\n\nplt.subplots(1,2,figsize=(15,5))\nplt.subplots_adjust(wspace=0.5)\nplt.subplot(121)\nplot_pricision_recall(knn,X_test,y_test,'KNN-Test')\nplt.subplot(122)\nplot_pricision_recall(svm,X_test,y_test,'SVM-Test')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}