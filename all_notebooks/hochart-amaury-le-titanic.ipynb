{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Le Titanic","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.scienceabc.com/wp-content/uploads/2016/04/titanic-jack-and-rose-plank-scene.webp\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Lecture des données","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas : librairie de manipulation de données\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avancés\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lecture des données d'apprentissage et de test\nt = pd.read_csv(\"../input/titanic/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Interprétation des paramètres","execution_count":null},{"metadata":{},"cell_type":"raw","source":"- survived - Survival (0 = No; 1 = Yes)\n- pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- name - Name\n- sex - Sex\n- age - Age\n- sibsp - Number of Siblings/Spouses Aboard\n- parch - Number of Parents/Children Aboard\n- ticket - Ticket Number\n- fare - Passenger Fare\n- cabin - Cabin\n- embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rose & Jack","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*value_counts* permet de compter le nombre d'éléments par catégorie d'une série","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"t.Sex.value_counts()      # nombre d'hommes et de femmes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.Sex.count()              # nombre total hommes+femmes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.Cabin.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.count()                  # Comptage par colonnes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t[np.isnan(t.Age)].Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque qu'il manque des valeurs pour 'age' et 'embarked' (présence de valeurs indéfinies 'NaN')","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On peut définir un booléen pour abréger une caractéristique :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hommes = (t.Sex==\"male\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t[hommes].head()        # t[hommes] est le tableau où on ne retient que lignes pour lesquelles hommes est True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut compter les hommes survivants ou non :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"t[hommes].Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exercice : quelle est la probabilité de survie de Rose et Jack ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Définir les booléens pour *femmes, classe1, classe2, classe3, survivant, ...*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"femmes = t.Sex==\"female\"\nclasse1 = t.Pclass == 1\nclasse2 = t.Pclass == 2\nclasse3 = t.Pclass == 3\nsurvivant = t.Survived == 1\nmort = ~ survivant","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Jack est un homme en 3ème classe, et Rose une femme en 1ère (définir les booléens *jack* et *rose*) :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jack = hommes & classe3\nrose = femmes & classe1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculer la probabilité de survie de Jack :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p_jack = t[jack & survivant].Sex.count()/t[jack].Sex.count()\nprint(p_jack)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculer la probabilité de survie de Rose :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p_rose = t[rose & survivant].Sex.count()/t[rose].Sex.count()\nprint(p_rose)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exercice : tester différentes visualisations sur le dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Tracer différentes représentations du dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"t.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sns.FacetGrid(t, hue=\"Survived\", aspect=5, palette=\"Set2\")\nfig.map(sns.kdeplot, \"Fare\", shade=True)\nfig.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(\"Age\", \"Pclass\", t, kind='kde');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tracer les courbes de distribution de l'âge selon la classe (utiliser *FacetGrid*)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"Age\", y=\"Pclass\", data=t, fit_reg=False, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(\"Pclass\", \"Age\", data=t)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Que peut-on dire des voyageurs de 1ere, 2eme et 3eme classe ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Les voyageurs de 1ere classe ont un age moyen autour de 40 ans, 2e classe de 30 ans et 3e classe de 25 ans.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conditionnement des données","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Eliminer les colonnes non pertinentes pour la prédiction (on peut utiliser une liste de colonnes dans *drop*), et placer le résultat dans la variable *titanic* :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"t.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On élimine les colonnes non pertinentes pour la prédiction\ntitanic = t.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il manque des valeurs, par exemple pour la colonne *age*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Les valeurs inconnues sont affichées comme **NaN** (*Not a Number*).  \nOn peut tester si une valeur est **NaN** avec la fonction *np.isnan(valeur)*  \nAfficher les lignes pour lesquelles l'âge est inconnu :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic[np.isnan(titanic.Age)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Données manquantes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On voit qu'il manque des données, en particulier pour la colonne *'age'*  \nIl existe plusieurs approches pour compléter les données manquantes :  \n- **suppression** des données manquantes (par exemple avec la fonction *dropna*). C'est une méthode simple, mais qui élimine de l'information\n- **remplacement** des données manquantes. Par exemple, on pourrait remplacer les informations manquantes pour l'âge par la moyenne de la colonne (mais on introduit un biais sur cette valeur), ou par un nombre aléatoire généré par une loi normale de même moyenne et variance ...\n- **estimation** des paramètres manquants avec une méthode de prédiction (par exemple avec une régression)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"La fonction *fillna* permet de compléter simplement les paramètres manquants. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic1 = titanic.fillna(value = {'Age':titanic.Age.mean()})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tracer l'histogramme des âges. Qu'observez-vous ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(titanic1.Age, bins=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic = titanic.fillna(method='pad')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'option *method='pad'* permet d'utiliser la précédente valeur non manquante :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic = titanic.fillna(method='pad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tracer l'histogramme pour *age* :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(titanic.Age, bins=80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La distribution des âges n'est pas significativement modifiée ...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Déséquilibre des distributions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Certaines distributions sont déséquilibrées, et éloignées d'une loi normale :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(titanic.Fare, color='blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dans ce cas, une transformation log peut améliorer l'équilibre :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic['log_fare'] = np.log(titanic.Fare+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(titanic.log_fare, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic = titanic.drop(['Fare'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mise à l'échelle des données quantitatives","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic[['Age','log_fare']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(titanic.log_fare, color='blue')\nsns.kdeplot(titanic.Age, color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit qu'il y a une forte différente de distribution entre les deux séries.  \nCertains algorithmes demandent une distribution normalisée. Pour une discussion détaillée sur ce sujet, cf par exemple :  \nhttp://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html  \nhttp://scikit-learn.org/stable/modules/preprocessing.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"La librairie *sklearn* comporte une librairie de prétraitement des données","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut normaliser les valeurs min et à max (valeurs ramenées entre 0 et 1) :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))\ntitanic[['Age', 'log_fare']] = minmax.fit_transform(titanic[['Age', 'log_fare']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(titanic.log_fare, color='blue')\nsns.distplot(titanic.Age, color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut également utiliser le *StandardScaler* pour ramener la moyenne à 0 et l'écart type à 1 :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\ntitanic[['Age', 'log_fare']] = scaler.fit_transform(titanic[['Age', 'log_fare']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(titanic.log_fare, color='blue')\nsns.kdeplot(titanic.Age, color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encodage binaire des données qualitatives (*one hot encoding*)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La plupart des algorithmes ont besoin de données numériques, et n'acceptent pas les chaînes de caractères :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.Sex = titanic.Sex.map({\"male\":0, \"female\":1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On utilise la fonction *get_dummies* de Pandas pour transformer les colonnes multimodales (par exemple 'embarked') en plusieurs colonnes binaires (par exemple 'embarked_C' dont les valeurs sont 1 si le passager a embarqué à Cherbourg et 0 sinon) :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic = pd.get_dummies(data=titanic, columns=['Pclass', 'Embarked'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Création des jeux d'apprentissage et de test","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Créer les jeux d'apprentissage et de test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = titanic.drop(['Survived'], axis=1)\ny = titanic.Survived","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On sépare le dataset en deux parties :\n- un ensemble d'apprentissage (entre 70% et 90% des données), qui va permettre d'entraîner le modèle\n- un ensemble de test (entre 10% et 30% des données), qui va permettre d'estimer la pertinence de la prédiction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Régression logistique","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Appliquer une régression logistique pour classifier sur l'ensemble de test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mesures de performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importation des méthodes de mesure de performances\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En comparant les valeurs prédites et les valeurs réelles, on a plusieurs possibilités :\n- *Vrais positifs* (VP ou TP) : on prédit \"oui\" et la valeur attendue est \"oui\"\n- *Vrais négatifs* (VN ou TN) : on prédit \"non\" et la valeur attendue est \"non\"\n- *Faux positifs* (FP) : on prédit \"oui\" et la valeur attendue est \"non\"\n- *Faux négatifs* (FN) : on prédit \"non\" et la valeur attendue est \"oui\"\n\nPar exemple, si veut prédire le décès, le nombre de vrais positifs est le nombre de fois où on a prédit 0 pour des passagers effectivement morts sur le Titanic (*survived = 0*)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Afficher la matrice de confusion :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,y_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La **matrice de confusion** permet de compter les vrais positifs, faux positifs, ...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.stack.imgur.com/gKyb9.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"La pertinence (ou *accuracy*) mesure le nombre de bonnes prédictions sur le nombre total d'observations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Néanmoins cette mesure peut être faussée dans certains cas, en particulier si le nombre de 0 et de 1 est déséquilibré.\nOn a donc d'autres estimateurs :\n- la **précision** est le nombre de prédictions positives correctes sur le nombre total de prédictions positives : *precision = VP/(VP+FP)*\n- la **sensibilité** (*recall*) est le nombre de prédictions positives sur le nombre effectif de \"oui\" : *recall = VP:(VP+FN)*\n- le **score F1** est la moyenne pondérée de la précision et de la sensibilité : *f1-score = 2xprecisionxrecall/(precision+recall)*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*predict_proba* donne un tableau de couples de probabilités : *[probabilité de prédiction 0, probabilité de prédiction 1]*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"probas = lr.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(probas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On met les probabilités de prédiction de la valeur 1 dans un dataframe, avec les valeurs effectives, pour faciliter la visualisation :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfprobas = pd.DataFrame(probas,columns=['proba_0','proba_1'])\ndfprobas['y'] = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfprobas","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On affiche la distribution des probabilités de prédiction de 1, et celle des non probabilités de prédiction de 0 :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.distplot(1-dfprobas.proba_0[dfprobas.y==0], bins=50)\nsns.distplot(dfprobas.proba_1[dfprobas.y==1], bins=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La distribution idéale permet de séparer totalement la prédiction des positifs et négatifs :  \n<img src=\"https://miro.medium.com/max/660/1*Uu-t4pOotRQFoyrfqEvIEg.png\">\nLe cas le plus défavorable consiste en une distribution équivalente pour les positifs et les négatifs :  \n<img src=\"https://miro.medium.com/max/538/1*iLW_BrJZRI0UZSflfMrmZQ.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On utilise ces distributions pour construire la **courbe ROC** (Receiving Operator Characteristic) qui représente le taux de vrais positifs par rapport aux taux de faux positifs.  \nLa mesure de l'aire sous la courbe **AUC** (Area Under Curve) est un bon indicateur de performance  \nPour plus de détails : http://www.xavierdupre.fr/app/mlstatpy/helpsphinx/c_metric/roc.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint (roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\nplt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ajustement des hyperparamètres (Random Forests)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On teste les forêts aléatoires :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parmi les hyperparamètres de l'algorithme qui peuvent avoir un impact sur les performances, on a :\n- **n_estimators** : le nombre d'arbres de décision de la forêt aléatoire\n- **min_samples_leaf** : le nombre d'échantillons minimum dans une feuille de chaque arbre\n- **max_features** : le nombre de caractéristiques à prendre en compte lors de chaque split\n\nPour chaque algorithme de *sklearn*, on peut trouver la liste des paramètres dans la documentation, avec des exemples :  \nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train, y_train)\ny_rf1 = rf.predict(X_test)\nprint(classification_report(y_test, y_rf1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*validation_curve* permet de tracer la courbe du score sur un ensemble d'apprentissage et sur un ensemble de test (*cross validation*), en faisant varier un paramètre, par exemple *n_estimators* :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'n_estimators', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('n_estimators')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exercice** : tracer les courbes de validation pour les paramètres *min_samples_leaf* et *max_features* (attention pour ce dernier, le nombre max est le nombre de caractéristiques / colonnes du tableau)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, 300,step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'min_samples_leaf', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('min_samples_leaf')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import validation_curve\nparams = np.arange(1, params/dfprobas.count(),step=30)\ntrain_score, val_score = validation_curve(rf, X, y, 'max_features', params, cv=7)\nplt.figure(figsize=(12,12))\nplt.plot(params, np.median(train_score, 1), color='blue', label='training score')\nplt.plot(params, np.median(val_score, 1), color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('max_features')\nplt.ylabel('score');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La méthode *GridSearchCV* permet de tester plusieurs combinaisons de paramètres (listés dans une grille de paramètres) et de sélectionner celle qui donne la meilleure pertinence","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n              'n_estimators': [10, 100, 500],\n              'min_samples_leaf': [1, 20, 50]\n             }\nestimator = ensemble.RandomForestClassifier()\nrf_gs = model_selection.GridSearchCV(estimator, param_grid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ici on a choisi des valeurs pour le nombres d'arbres dans la forêt aléatoire (*'n_estimators'*) et le nombre minimum d'échantillons pour une feuille. On pourrait tester d'autres valeurs, et d'autres paramètres, cf :  \nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"On lance l'entrainement :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut voir les paramètres sélectionnés et le score :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On sélectionne le meilleur estimateur :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf2 = rf_gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_rf2 = rf2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_rf2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On a amélioré la performance du modèle","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importance des caractéristiques","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"L'attribut *feature_importances_* renvoie un tableau du poids de chaque caractéristique dans la décision :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rf2.feature_importances_\nindices = np.argsort(importances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut visualiser ces degrés d'importance avec un graphique à barres par exemple :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.title('Importance des caracteristiques')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"La méthode XGBoost est dérivée des arbres de décision, et très efficace, en particulier pour de grandes quantités de données.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Sous Anaconda prompt :\n*pip install xgboost*  \n(déjà disponible sous Kaggle)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sous Jupyter, si xgboost n'est pas déjà installé\n!pip install xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as XGB\nxgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)\nprint(classification_report(y_test, y_xgb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exercice : explorer d'autres méthodes de classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src = \"http://scikit-learn.org/0.16/_static/ml_map.png\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"http://scikit-learn.org/0.16/tutorial/machine_learning_map/index.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exercice : appliquer les méthodes sur le dataset *Indian Diabete*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas : librairie de manipulation de données\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avancés\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lecture des données d'apprentissage et de test\nd = pd.read_csv(\"../input/pyms-diabete/diabete.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.glucose.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.n_pregnant.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.diabete.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#jamaisEnceinte = d.n_pregnant == 0\ndejaEnceinte = d.n_pregnant >= 1\n#glucosePlusDe100 = d.glucose >= 100\n\n#test_jamaisEnceinte_et_glucocePlusDe100 = jamaisEnceinte & glucosePlusDe100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# p_test_jamaisEnceinte_et_diabetique = d[jamaisEnceinte & d.diabete].age.count()/d.age.count()\n# print('Si t\\'as jamais été enceinte et diabetique :')\n# print(p_test_jamaisEnceinte_et_diabetique)\n\np_test_dejaEnceinte_et_diabetique = d[dejaEnceinte & d.diabete].age.count()/d.age.count()\nprint('Si t\\'as déjà été enceinte et diabetique :')\nprint(p_test_dejaEnceinte_et_diabetique)\n\n# p_test_glucocePlusDe100_et_diabetique = d[glucosePlusDe100 & d.diabete].age.count()/d.age.count()\n# print('Si tu as un glucose de plus de 100 et diabetique :')\n# print(p_test_glucocePlusDe100_et_diabetique)\n\n# p_test_jamaisEnceinte_et_glucocePlusDe100 = d[test_jamaisEnceinte_et_glucocePlusDe100 & d.diabete].age.count()/d.age.count()\n# print('Si t\\'as jamais été enceinte et tu as un glucose de plus de 100 et diabetique :')\n# print(p_test_jamaisEnceinte_et_glucocePlusDe100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"insulin\", y=\"pedigree\", data=d, fit_reg=False, hue='diabete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(\"insulin\", \"glucose\", d, kind='kde');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On élimine les colonnes non pertinentes pour la prédiction\n# diabet = d.drop(['age'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.n_pregnant, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.glucose, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.tension, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.thickness, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.insulin, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.bmi, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.pedigree, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d['log_thickness'] = np.log(d.thickness+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.log_thickness, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d['log_pedigree'] = np.log(d.pedigree+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(d.log_pedigree, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = d.drop(['pedigree', 'thickness'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(d.n_pregnant, color=\"blue\")\nsns.kdeplot(d.bmi, color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = d.drop(['diabete'], axis=1)\ny = d.diabete","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importation des méthodes de mesure de performances\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test,y_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test,y_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf1 = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=10, max_features=3)\nrf1.fit(X_train, y_train)\ny_rf1 = rf.predict(X_test)\nprint(classification_report(y_test, y_rf1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n              'n_estimators': [10, 100, 500],\n              'min_samples_leaf': [1, 20, 50]\n             }\nestimator = ensemble.RandomForestClassifier()\nrf_gs = model_selection.GridSearchCV(estimator, param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf2 = rf_gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_rf2 = rf2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_rf2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rf2.feature_importances_\nindices = np.argsort(importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), X_train.columns[indices])\nplt.title('Importance des caracteristiques')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}