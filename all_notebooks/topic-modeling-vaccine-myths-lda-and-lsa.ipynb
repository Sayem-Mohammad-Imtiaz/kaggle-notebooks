{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction\nVaccines today play an important role in sustaining community health, hence it is important to understant what kind of myths are circulated about them and where it originates. Categorising such myths into different topics can help us understand the origin of myth (eg mistrust of vaccine,religious stigma, psedosciece etc).Hence it is important for people to understand scientific and medical community to understand the core topics involvng such myhts to countert them effectively, and convince victims of such myths otherwise.\n\nTopic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic.\n\nHere we'll try to perform EDA on a the reddit-vaccine-myth dataset and further try and classify them in topics.","metadata":{}},{"cell_type":"markdown","source":"![](https://www.hopkinsmedicine.org/-/media/images/health/1_-conditions/coronavirus/vaccine-hero.ashx?h=500&la=en&mh=500&mw=1300&w=1297&hash=ED466B472CD015FF5CC3288E75EA37EB707B65D3)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/reddit-vaccine-myths/reddit_vm.csv', error_bad_lines=False);\ndata.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the data its clear thast many posts have thier body missing, lets try to find out which all data are missing.","metadata":{}},{"cell_type":"code","source":"data.isnull().sum(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(data.isnull(),cbar= False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From heatmap we can understand that url is missing in 69% of rows.","metadata":{}},{"cell_type":"markdown","source":"Since title is available in all rowns we'll use title to for preliminary topic modeling","metadata":{}},{"cell_type":"markdown","source":"#### Data Preprocessing\n1. Tokenizing : the sentences are splint into words.\n2. Stop word are removed\n3. Lemmatization : past and future tenses are changed to present \n4. Stemming : words are reduced into their root form\n","metadata":{}},{"cell_type":"markdown","source":"#### Impoting necessary libaries for topic modeling","metadata":{}},{"cell_type":"code","source":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk\nfrom nltk import word_tokenize, pos_tag\n\nnltk.download('wordnet')\nstemmer = PorterStemmer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = word_tokenize(\"hi this is ren\")\n\nnltk.pos_tag(tokens)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize_stemming(text):\n   \n    token=word_tokenize(text)\n  \n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, get_wordnet_pos(nltk.pos_tag(token)[0][1])))\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preProcessedText = data[\"title\"].apply(preprocess)\ntitlesWithoutDupes = data[\"title\"].drop_duplicates()\ntitlesWithoutDupes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preProcessedText\npreProcessedTextWithoutComments = titlesWithoutDupes.apply(preprocess)\npreProcessedTextWithoutComments ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(preProcessedText)\ndictionaryWithoutComents = gensim.corpora.Dictionary(preProcessedTextWithoutComments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LDA \nWe'll apply the  Latent Dirichlet Allocation (LDA) to our documents and classify them into topics. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.","metadata":{}},{"cell_type":"code","source":"bow_corpus = [dictionary.doc2bow(doc) for doc in preProcessedText]\nbow_corpusWithoutComments = [dictionaryWithoutComents.doc2bow(doc) for doc in preProcessedTextWithoutComments]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 4, \n                                   id2word = dictionary,                                    \n                                   passes = 20,\n                                   workers = 2)\nlda_model.show_topics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model_after_removing_dupes = gensim.models.LdaMulticore(bow_corpusWithoutComments,num_topics = 4,id2word = dictionaryWithoutComents,\n                                                           passes = 20,workers=2)\nlda_model_after_removing_dupes.show_topics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LDA visualization before removing duplicates from titltes","metadata":{}},{"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim_models\n\nLDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\npyLDAvis.display(LDAvis_prepared)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LDA visualization after removing duplicates from titltes","metadata":{}},{"cell_type":"code","source":"LDAvis_duples = pyLDAvis.gensim_models.prepare(lda_model_after_removing_dupes,bow_corpusWithoutComments,dictionaryWithoutComents)\npyLDAvis.display(LDAvis_duples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"it is evident that after removing duplicate titles from the corpus the model has perofrmed susbstantially better , the visualisation unlike the one with duplicates hs defined large, non overlaping circles , indicating that topics are different from each other and unique.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Activity in subreddit\nLets take a look at the activity changes in the subredditt","metadata":{}},{"cell_type":"code","source":"data[\"date\"]= data[\"timestamp\"].apply(lambda x: pd.Timestamp(x).strftime('%Y-%m-%d'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity = data[\"date\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity = pd.DataFrame(activity)\nactivity.reset_index(inplace=True)\nactivity[\"dates\"] =activity[\"index\"]\nactivity[\"count\"] = activity[\"date\"]\nactivity.drop(['index','date'],inplace=True,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.figure(figsize=(15,8))\n\nax=sns.countplot(data=data, x='date')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity.sort_values(by=\"dates\",inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\n\nlx= sns.lineplot(data= activity,x=\"dates\",y=\"count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity[\"year\"] = activity['dates'].apply(lambda x:x[0:4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"activity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yearlyActivity = activity[\"year\"].value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yearlyActivity = pd.DataFrame(yearlyActivity)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yearlyActivity.reset_index(inplace=True)\nyearlyActivity[\"years\"] = yearlyActivity[\"index\"]\nyearlyActivity[\"count\"] = yearlyActivity[\"year\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nyearlyActivity.sort_values(by=\"years\",inplace=True)\nlx= sns.lineplot(data= yearlyActivity,x=\"years\",y=\"count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph it is evident that the activity in subreddit had peaked in 2019 and reduced significantly after the onset of covid-19 in 2020 and further decreasing in 2021","metadata":{}},{"cell_type":"markdown","source":"## LSA\nLSA for natural language processing task was introduced by Jerome Bellegarda in 2005. The objective of LSA is reducing dimension for classification. The idea is that words will occurs in similar pieces of text if they have similar meaning. Now we'll try to use this also for classifying the titles.","metadata":{}},{"cell_type":"code","source":"\ndef detokenize(preProcessedTexts):\n    detokenized_doc = []\n    for i in range(len(preProcessedTexts)):\n        t = ' '.join(preProcessedTexts[i])\n        detokenized_doc.append(t)\n    return(detokenized_doc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\ntfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\nsvd = TruncatedSVD(n_components=4)\ndetokenized_doc = detokenize(preProcessedText)\nX= tfidf_vec.fit_transform((detokenized_doc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preProcessedText","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preProcessedTextWithoutComments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detokenized_doc_no_comments = detokenize(list(preProcessedTextWithoutComments))\nXWithoutComments= tfidf_vec.fit_transform((detokenized_doc_no_comments))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_svd = svd.fit_transform(X)\nx_svd_noComment = svd.fit_transform(XWithoutComments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd.fit(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd.components_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import umap\nembedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(x_svd)\n\nplt.figure(figsize=(7,5))\nplt.scatter(embedding[:, 0], embedding[:, 1], \n\ns = 10, # size\nedgecolor='none'\n)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(x_svd_noComment)\n\nplt.figure(figsize=(7,5))\nplt.scatter(embedding[:, 0], embedding[:, 1], \n\ns = 10, # size\nedgecolor='none'\n)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the visualizations developed using both the methods, we can classify the documents into 4 topics.\n\n1. vaccine causing autism and cancer\n2. vaccine myth debunked and anti-vaxxer's childer need courts order for vaccine\n3. big pharma paying scientist to promote vaccine\n4. emergency occuring due to anti vaxx","metadata":{}},{"cell_type":"markdown","source":"## WordCloud","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nstrings_no_comment = \" \".join(detokenized_doc_no_comments)\nwordcloud = WordCloud().generate(strings_no_comment)\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the wordcloud it cn be we get a clear idea of the words used by people in the subreddit. The frequency of the word \"autism\" shows the prevalnce of the idea of vaccines causing atism in the subreddit.","metadata":{}},{"cell_type":"markdown","source":"this notebook is still a work in progress , please give your valuable feedback so that I can improve it, do consider upvoting if you found it helpful.","metadata":{}},{"cell_type":"markdown","source":"credits:\n\nhttps://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n\nhttps://www.kaggle.com/abhishekvermasg1/analysis-topic-modelling","metadata":{}}]}