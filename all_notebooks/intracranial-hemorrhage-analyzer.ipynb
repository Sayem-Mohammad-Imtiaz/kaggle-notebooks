{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data Summary\n\n- 82 patients in total\n- 36 patients with intracranial hemorrhage\n- 46 normal patients\n- Approx. 30 CT slices for each patient (e.g. 34 for patient 58)\n- 46 males\n- 36 females\n\n- 318 of 2500 images have masks that show intracranial hemorrhage\n\n- Images: 650x650x1\n- Masks: 650x650x1\n\n- Two kinds of windowed images are available - brain window and bone window\n","metadata":{}},{"cell_type":"markdown","source":"## Approach\n\n- We will only use the brain window images. We will convert these to 3 channel images to suit the model.\n- Apply DWT\n- Resize images and masks to 256x256\n- Set aside 20 images as a holdout test set.\n- Split the remainder of the data into 85% train and 15% validation.\n- Use a Keras Densenet121 encoder with a Unet decoder - Adam optimizer and dice loss.\n- We won't use Densenet pre-procesing. Instead we will simply normalize the images by dividing by 255.\n- The dataset is quite small therefore, we will use data augmentation to reduce overfitting and to help the model generalize better.","metadata":{}},{"cell_type":"markdown","source":"## Please note...\n\nThe training results in this kernel are quite poor. Please use this notebook only as a guide that demonstrates the overall workflow. Strangely, I got much better results when I trained this model in Google Colab. The Colab model is the one that's used in the web app. The colab notebook can be found here:<br>\nhttps://github.com/vbookshelf/Intracranial-Hemorrhage-Analyzer/blob/master/Colab_Notebook_Intracranial_Hemorrhage_Analyzer.ipynb\n","metadata":{}},{"cell_type":"code","source":"!pip install keras==2.3.1\n!pip install tensorflow==2.1.0\n!pip install keras_applications==1.0.8\n!pip install image-classifiers==1.0.0\n!pip install efficientnet==1.0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy.random import seed\nseed(101)\n\nimport pandas as pd\nimport numpy as np\n\n\nimport os\nimport cv2\n\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\n\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"qWd1uWrVM6Kp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = '../input/computed-tomography-ct-images/computed-tomography-images-for-intracranial-hemorrhage-detection-and-segmentation-1.0.0/'\nos.listdir(base_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nIMAGE_HEIGHT_ORIG = 650\nIMAGE_WIDTH_ORIG = 650\n\nNUM_TEST_IMAGES = 10 # 10 with intracranial hem + 10 without intracranial hem\n\nIMAGE_HEIGHT = 256\nIMAGE_WIDTH = 256\nIMAGE_CHANNELS = 3\n\nBATCH_SIZE = 10\n","metadata":{"id":"GID-fNhiWBjs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"IPSEjvFRi_WP"}},{"cell_type":"code","source":"# Function to perform the augmentations\n\ndef augment_image_and_mask(augmentation, image, mask):\n    \n    \"\"\"\n    Uses the Albumentations library.\n    \n    Inputs: \n    1. augmentation - this is the instance of type of augmentation to do \n    e.g. aug_type = HorizontalFlip(p=1) \n    # p=1 is the probability of the transform being executed.\n    \n    2. image - image with shape (h,w)\n    3. mask - mask with shape (h,w)\n    \n    Output:\n    Augmented image as a numpy array.\n    Augmented mask as a numpy array.\n    \n    \"\"\"\n    # get the transform as a dict\n    aug_image_dict =  augmentation(image=image, mask=mask)\n    # retrieve the augmented matrix of the image\n    image_matrix = aug_image_dict['image']\n    \n    mask_matrix = aug_image_dict['mask']\n    \n    return image_matrix, mask_matrix\n","metadata":{"id":"H8-pgzbXT-OG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download Packages","metadata":{"id":"ikeiK5lijJ7E"}},{"cell_type":"markdown","source":"We will use the excellent segmentation_models package by Pavel Yakubovskiy <br>\nhttps://github.com/qubvel/segmentation_models\n\nMore info can be found here:<br>\nhttps://www.kaggle.com/c/severstal-steel-defect-detection/discussion/103367","metadata":{}},{"cell_type":"code","source":"! pip install segmentation-models","metadata":{"id":"IY0ED5Oni-6O","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"XUMgMzRzi-3R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data","metadata":{"id":"40Nmo0MLDhxh"}},{"cell_type":"code","source":"path = base_path + 'hemorrhage_diagnosis.csv'\ndf_diag = pd.read_csv(path)\n\n# The existing No_Hemorrhage target column is not intuitive. \n# Create a new target column to make the binary targets easier to understand.\n\ndef swap_target(x):\n    if x == 0:\n        return 1\n    else:\n        return 0\n\n# create a new target column\ndf_diag['Has_Hemorrhage'] = df_diag['No_Hemorrhage'].apply(swap_target)\n\n# drop the old target column\ndf_diag = df_diag.drop('No_Hemorrhage', axis=1)\n\nprint(df_diag.shape)\n\ndf_diag.head()","metadata":{"id":"nFqc_6ReDfQF","outputId":"7de2b2bd-a481-4661-92f3-7010699cbadf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Patient 84\n# Brain image 36.jpg exists but bone image 36.jpg is missing.\n\n# Therefore, we will drop this row from the dataframe.\n\nindex_to_drop = df_diag[(df_diag['PatientNumber'] == 84) & (df_diag['SliceNumber'] == 36)].index\n\nindex_to_drop = index_to_drop[0]\n\ndf_diag = df_diag.drop(index_to_drop, axis=0)\n\n\n# Check that the row that we dropped has been removed\ndf_diag[df_diag.index == index_to_drop]","metadata":{"id":"k_M8RUMaDhKg","outputId":"1dea25f8-6562-4a3b-d513-a77d9baf3048","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"JMoBerNvDhDG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creat new columns","metadata":{"id":"XxbmRXLEFs_e"}},{"cell_type":"markdown","source":"Here we will be creating new image and mask file names to make the data easier to handle later. These new names will be added as new columns in df_diag.","metadata":{}},{"cell_type":"code","source":"# Create new columns\n\n\ndef get_mask_fname(row):\n        \n    mask_id = str(row['SliceNumber']) + '_HGE_Seg.jpg'\n    return mask_id\n    \n\n# create a new column with mask file names\ndf_diag['mask_fname'] = df_diag.apply(get_mask_fname, axis=1)\n\n\n\n\ndef new_mask_fname(row):\n        \n    mask_id = str(row['PatientNumber']) + '_' + str(row['SliceNumber']) + '_HGE_Seg.jpg'\n    return mask_id\n\n\n# create a new column with a new mask file names\ndf_diag['new_mask_fname'] = df_diag.apply(new_mask_fname, axis=1)\n\n\n\n\ndef assign_image_fname(row):\n    \n    image_fname = str(row['SliceNumber']) + '.jpg'\n    \n    return image_fname\n\n\n# create a new column with image file names\ndf_diag['image_fname'] = df_diag.apply(assign_image_fname, axis=1)\n\n\n\ndef assign_new_fname(row):\n         \n    mask_id = str(row['PatientNumber']) + '_' + str(row['SliceNumber']) + '.jpg'\n    \n    return mask_id\n    \n# create a new column with new image file names\ndf_diag['new_image_fname'] = df_diag.apply(assign_new_fname, axis=1)\n\n\n\ndf_diag.head()","metadata":{"id":"RbVglB0VDgvu","outputId":"c076f0d1-6878-4a47-ee90-ed6a865a3d7c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the binary target distribution.\n# You will note that it is unbalanced - most images have no signs of hemorrhage.\n\ndf_diag['Has_Hemorrhage'].value_counts()","metadata":{"id":"TKAuEgXvDgqP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the total number of patients\n\ndf_diag['PatientNumber'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Move all masks into the same folder","metadata":{"id":"wZLXkaCcF8qO"}},{"cell_type":"code","source":"path = base_path + 'Patients_CT'\n\nfolder_list = os.listdir(path)\n\nlen(folder_list)","metadata":{"id":"xRIeKhnNDgkY","outputId":"dd5c6f2f-95a6-4771-abd3-92f4da12e2af","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a new mask dir\nmask_dir = 'mask_dir'\nos.mkdir(mask_dir)","metadata":{"id":"RYdPhwfoDgei","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For non blank masks i.e. masks showing intracranial hemorrhage\n\nfor folder_name in folder_list:\n    \n    # convert the folder name to integer\n    patient_num = int(folder_name)\n    \n    # filter by patient number\n    df = df_diag[df_diag['PatientNumber'] == patient_num]\n    \n    # filter by Has_Hemorrhage\n    df = df[df['Has_Hemorrhage'] == 1]\n    \n    # get the list of mask file names\n    mask_list = list(df['mask_fname'])\n    \n    for fname in mask_list:\n        \n        # add the patient number to the file name\n        new_fname = str(patient_num) + '_' + fname\n        \n        # Source path to mask.\n        # All masks are in the brain folder.\n        path = base_path + 'Patients_CT/' + folder_name + '/brain'\n        src = os.path.join(path, fname)\n        # destination path to mask\n        dst = os.path.join(mask_dir, new_fname)\n        # copy the mask from the source to the destination\n        shutil.copyfile(src, dst)\n        \n# Check how many masks are in the new folder. (Should be 318)\nlen(os.listdir('mask_dir'))","metadata":{"id":"uRbzaDxJDgYT","outputId":"1ede6133-88d8-4c73-8b4e-3a9b2354d26c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"_gQ97a83OglY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For blank masks i.e. masks without intracranial hemorrhage\n\nblank_mask = np.zeros((IMAGE_HEIGHT_ORIG, IMAGE_WIDTH_ORIG))\n\n\nfor folder_name in folder_list:\n    \n    # convert the folder name to integer\n    patient_num = int(folder_name)\n    \n    # filter by patient number\n    df = df_diag[df_diag['PatientNumber'] == patient_num]\n    \n    # filter by Has_Hemorrhage\n    df = df[df['Has_Hemorrhage'] == 0]  # <-- for empty masks change filter here\n    \n    # get the list of mask file names\n    mask_list = list(df['mask_fname'])\n    \n    for fname in mask_list:\n        \n        # add the patient number to the file name\n        new_fname = str(patient_num) + '_' + fname\n        \n        # set the destination where the file will be saved\n        dst = os.path.join(mask_dir, new_fname)\n      \n        # save the image\n        cv2.imwrite(dst, blank_mask)\n        \n# Check how many masks are in the new folder. (Should be 2500)\nlen(os.listdir('mask_dir'))\n\n","metadata":{"id":"OwOH7a_8DgRX","outputId":"9293cd2d-1566-48ac-d426-ca765852a516","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"_SG8nJGrNpHE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Move all brain window images into the same folder","metadata":{"id":"jvQKI4MLGbfG"}},{"cell_type":"code","source":"# create a new mask dir\nbrain_image_dir = 'brain_image_dir'\nos.mkdir(brain_image_dir)","metadata":{"id":"Jog2U3-bDgKF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for folder_name in folder_list:\n    \n    # convert the folder name to integer\n    patient_num = int(folder_name)\n    \n    # filter by patient number\n    df = df_diag[df_diag['PatientNumber'] == patient_num]\n    \n    \n    # get the list of image file names\n    image_list = list(df['image_fname'])\n    \n    for fname in image_list:\n        \n        # add the patient number to the file name\n        new_fname = str(patient_num) + '_' + fname\n        \n        # source path to image\n        path = base_path + 'Patients_CT/' + folder_name + '/brain'\n        src = os.path.join(path, fname)\n        # destination path to mask\n        dst = os.path.join(brain_image_dir, new_fname)\n        # copy the mask from the source to the destination\n        shutil.copyfile(src, dst)\n        \n        \n\n# Check how many images are in the new folder (Should be 2500)\nlen(os.listdir('brain_image_dir'))","metadata":{"id":"fPMOdH7fDgCX","outputId":"b63aee5d-ceae-4991-9962-8c13fc5d9951","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"6RdewngBGgau","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Move all bone window images into the same folder","metadata":{"id":"Ci3tZjbvG6Br"}},{"cell_type":"code","source":"# create a new mask dir\nbone_image_dir = 'bone_image_dir'\nos.mkdir(bone_image_dir)","metadata":{"id":"XcZwxF8QGgW6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for folder_name in folder_list:\n    \n    # convert the folder name to integer\n    patient_num = int(folder_name)\n    \n    # filter by patient number\n    df = df_diag[df_diag['PatientNumber'] == patient_num]\n    \n    \n    # get the list of image file names\n    image_list = list(df['image_fname'])\n    \n    for fname in image_list:\n        \n        # add the patient number to the file name\n        new_fname = str(patient_num) + '_' + fname\n        \n        # source path to image\n        path = base_path + 'Patients_CT/' + folder_name + '/bone'\n        src = os.path.join(path, fname)\n        # destination path to mask\n        dst = os.path.join(bone_image_dir, new_fname)\n        # copy the mask from the source to the destination\n        shutil.copyfile(src, dst)\n        \n        \n# Check how many images are in the new folder (Should be 2500)\nlen(os.listdir('bone_image_dir'))","metadata":{"id":"tAaKKxaiGgRS","outputId":"5ea4f614-305b-40be-dbbc-fe892cb47fd0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"N2LY6PM9GgMw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display Images and Masks","metadata":{"id":"Br69UhxkHeBD"}},{"cell_type":"code","source":"# brain image\n\nindex = 14\nfname = df_diag.loc[index, 'new_image_fname']\npath = 'brain_image_dir/' + fname\n# read the image as a matrix\nbrain_image = plt.imread(path)\n\nprint(brain_image.shape)\n\nplt.imshow(brain_image, cmap='gray')","metadata":{"id":"boNxPeywGgH4","outputId":"bf9709d1-8498-40a5-bf54-7f9a2b7d23dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bone image\n\nfname = df_diag.loc[index, 'new_image_fname']\npath = 'bone_image_dir/' + fname\n# read the image as a matrix\nbone_image = plt.imread(path)\n\nprint(bone_image.shape)\n\nplt.imshow(bone_image, cmap='gray')","metadata":{"id":"JDk_DX73GgCB","outputId":"4c5abaaf-98cb-4c97-8874-1c762c42ae6c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mask\n\nfname = df_diag.loc[index, 'new_mask_fname']\npath = 'mask_dir/' + fname\n# read the image as a matrix\nmask = plt.imread(path)\n\nprint(mask.shape)\n\nplt.imshow(mask, cmap='Blues', alpha=0.7)","metadata":{"id":"wohH-i0QGf72","outputId":"2bae27d1-7cbf-43bf-9c38-1cccb4e2e456","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(brain_image, cmap='gray')\nplt.imshow(mask, cmap='Blues', alpha=0.7)","metadata":{"id":"DWz04Da3Gf3E","outputId":"f4ff1142-f1c4-4be4-aed0-9c2fb0adecc2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up data augmentation","metadata":{"id":"9HvWvuP6jY3I"}},{"cell_type":"code","source":"# HOW TO DO MULTIPLE AUGMENTATIONS\n\nimport albumentations as albu\n\n# Define the augmentations\n\naug_types = albu.Compose([\n    albu.HorizontalFlip(p=0.5),\n    albu.OneOf([\n        albu.RandomContrast(),\n        albu.RandomGamma(),\n        albu.RandomBrightness(),\n        ], p=0.3),\n    albu.OneOf([\n        albu.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        albu.GridDistortion(),\n        albu.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n        ], p=0.3),\n    albu.ShiftScaleRotate(scale_limit=0.1, rotate_limit=10, shift_limit=0.1, p=0.5, border_mode=0),\n])\n\n\n# This how to call the function\n# aug_image, aug_mask = augment_image_and_mask(aug_types, image, mask)","metadata":{"id":"KnLTD5fVHsjM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# brain image\n\nindex = 14\nfname = df_diag.loc[index, 'new_image_fname']\npath = 'brain_image_dir/' + fname\n# read the image as a matrix\nbrain_image = cv2.imread(path)\n\nprint(brain_image.shape)\n\nplt.imshow(brain_image, cmap='gray')","metadata":{"id":"cUYOAaJXHsdb","outputId":"7459d240-96f8-46fd-9c24-841933f2d4c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mask\n\nfname = df_diag.loc[index, 'new_mask_fname']\npath = 'mask_dir/' + fname\n# read the image as a matrix\nmask = plt.imread(path)\n\nprint(mask.shape)\n\nplt.imshow(mask, cmap='Blues', alpha=0.7)","metadata":{"id":"KFjJStmVjebd","outputId":"8facf609-43f9-4b60-c8bb-b216e6a25e93","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(brain_image, cmap='gray')\nplt.imshow(mask, cmap='Blues', alpha=0.7)","metadata":{"id":"KRRp6BkhjeZj","outputId":"ada6e3bd-6d00-49eb-a592-8e62f84bd673","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example augmentation - image and mask\n\n# augment the image and mask\naug_image, aug_mask = augment_image_and_mask(aug_types, brain_image, mask)\n\n\n\nplt.imshow(aug_image, cmap='gray')\nplt.imshow(aug_mask, cmap='Blues', alpha=0.7)","metadata":{"id":"N-qTdyLVjeWN","outputId":"fd4b8412-16a2-4947-b3ac-d216c7000c59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8aSrBW1GjoXr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Display a ramdom sample of images from each dataset by target","metadata":{"_uuid":"6efe2e5de99c4bf92079b1a7d0b892d30fc9d518","id":"Kv42Eqe4M6NJ"}},{"cell_type":"code","source":"# source: https://www.kaggle.com/gpreda/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    \n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['new_image_fname']\n            im=imageio.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=14)\n               \n    plt.tight_layout()\n    plt.show()\n    \n   \n  \n  \ndef draw_category_masks(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['new_mask_fname']\n            im=imageio.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=14)  \n    plt.tight_layout()\n    plt.show()\n    ","metadata":{"_kg_hide-input":true,"_uuid":"1c5143f227da4262eafce8cf0210a02c8072fb8e","id":"PXIuNP6RM6NL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Brain images\n\nIMAGE_PATH = 'brain_image_dir/'\ndraw_category_images('Has_Hemorrhage',4, df_diag, IMAGE_PATH)","metadata":{"_uuid":"bd38bcfb5839975e4fee9e70b93d42c29c1b5d2e","id":"iDXPFoFAM6NO","outputId":"1bf7d124-5d39-4d9a-84e4-bded267c610c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bone images\n\nIMAGE_PATH = 'bone_image_dir/'\n\ndraw_category_images('Has_Hemorrhage',4, df_diag, IMAGE_PATH)","metadata":{"_uuid":"6d6b11f3b0f26f1e22993896a20cfd1d90d5c5d1","id":"w_E_zszfM6Nf","outputId":"d4de941c-a1cd-4573-bb02-80a7cd4ba329","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Masks\n\nIMAGE_PATH = 'mask_dir/'\n\ndraw_category_masks('Has_Hemorrhage',4, df_diag, IMAGE_PATH)","metadata":{"id":"w7gtey-COoXH","outputId":"47999a45-15cc-497c-ef53-e9cd73bfd33c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"aWNjG9-cOoQ2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a holdout test set\n\nThis will be set aside and won't be used during training and validation. We will use it later to check how the trained model performs on unseen data.","metadata":{"id":"gJku1HjNLTKo"}},{"cell_type":"code","source":"NUM_TEST_IMAGES = 10\n\n\n# get 10 images without hemorrhages\n\ndf = df_diag[df_diag['Has_Hemorrhage'] == 0]\n\ndf_no_hem = df.sample(NUM_TEST_IMAGES, random_state=101)\n\n# Reset the index.\ndf_no_hem = df_no_hem.reset_index(drop=True)\n\n# create a list of images\ntest_images_list = list(df_no_hem['new_mask_fname'])\n\n\n# Select only rows that are not part of the test set.\n# Note the use of ~ to execute 'not in'.\ndf_diag = df_diag[~df_diag['new_mask_fname'].isin(test_images_list)]\n\n\n# get 10 images with hemorrhages\n\ndf = df_diag[df_diag['Has_Hemorrhage'] == 1]\n\ndf_with_hem = df.sample(NUM_TEST_IMAGES, random_state=102)\n\n# Reset the index.\ndf_with_hem = df_with_hem.reset_index(drop=True)\n\n# create a list of images\ntest_images_list = list(df_with_hem['new_mask_fname'])\n\n\n# Select only rows that are not part of the test set.\n# Note the use of ~ to execute 'not in'.\ndf_diag = df_diag[~df_diag['new_mask_fname'].isin(test_images_list)]\n\n\n# create the test set\ndf_test = pd.concat([df_with_hem, df_no_hem], axis=0).reset_index(drop=True)\n\n\n\nprint(df_diag.shape)\nprint(df_test.shape)","metadata":{"id":"FpDBhRRCTN42","outputId":"ece7e5a4-d607-4872-a4bf-b7a77a88316c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"id":"nbQ8hj8aTN2M","outputId":"a0ea133e-18dc-4bc8-b1d7-651240d98681","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"8V0D2t6FLJ5c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{"id":"4ea5Cg6DLpWY"}},{"cell_type":"code","source":"# train_test_split\n\n\n# shuffle\ndf_diag = shuffle(df_diag)\n\n# reset the index\ndf_diag = df_diag.reset_index(drop=True)\n\n# We will stratify by target\ny = df_diag['Has_Hemorrhage']\n\ndf_train, df_val = train_test_split(df_diag, test_size=0.15, random_state=107, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","metadata":{"_uuid":"15ba9792e6a370b7560330af15b3cfe21185c1cb","id":"aHaknxDZM6O8","outputId":"b6761e30-6c68-4d1a-c053-c775bed327ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the target distribution in the train set\n\ndf_train['Has_Hemorrhage'].value_counts()","metadata":{"_uuid":"ca2b267a97f465a4fc7803b072689eb661890168","id":"Rtj5Hx4CM6PD","outputId":"90c1cd3e-2653-44e7-ffa6-94fefff14256","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the target distribution in the val set\n\ndf_val['Has_Hemorrhage'].value_counts()","metadata":{"_uuid":"392c0eea00be8e43a6e55438d1458650e842030b","id":"vHL9AFdXM6PH","outputId":"e1507234-1abd-408d-9f17-2866886f2044","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"qvhY2uBrpQEn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the dataframes as compressed csv files","metadata":{"id":"jZycSA3bt9zJ"}},{"cell_type":"markdown","source":"These csv files will allow us to use Pandas chunking to feed images into the generators.","metadata":{"id":"QCQAlOUTuZ4f"}},{"cell_type":"code","source":"df_diag.to_csv('df_data.csv.gz', compression='gzip', index=False)\n\ndf_train.to_csv('df_train.csv.gz', compression='gzip', index=False)\ndf_val.to_csv('df_val.csv.gz', compression='gzip', index=False)\n\ndf_test.to_csv('df_test.csv.gz', compression='gzip', index=False)\n","metadata":{"id":"39Ip_vbipPuR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if the files were saved\n!ls","metadata":{"id":"S4n5WcuupPmZ","outputId":"d0e63dd5-8241-470e-8290-202c064faefa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"id":"3nPf9x6GuUuc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Data Generators","metadata":{"id":"qAlBmd0vwX2M"}},{"cell_type":"code","source":"# Note:\n# We won't be using densenet101 pre-processing however,\n# this code would need to be run if we were going to use it in the generators.\n\nfrom segmentation_models import  get_preprocessing # this line has an error in the docs\n\nBACKBONE = 'densenet121'\npreprocess_input = get_preprocessing(BACKBONE)","metadata":{"id":"zwPKNLmKlfzr","outputId":"5f9f8514-0782-4bd2-a8ee-46177203d34c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [ 1 ] Train Generator","metadata":{"id":"UmVkK7vkwZr9"}},{"cell_type":"code","source":"# We are only using brain images for training.\n# These are originally single channel images but cv2 will read them with 3 channels.\n\ndef train_generator(batch_size=10):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_train.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['new_image_fname'])\n            mask_id_list = list(df['new_mask_fname'])\n            \n            # Create empty X matrix - 3 channels\n            X_train = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n            # create empty Y matrix - 1 channel\n            Y_train = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.bool)\n\n        \n            \n            # Create X_train\n            #================\n            \n            for i in range(0, len(image_id_list)):\n              \n              \n                # get the image and mask\n                image_id = image_id_list[i]\n                mask_id = mask_id_list[i]\n              \n                \n\n                # set the path to the image\n                path = 'brain_image_dir/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                \n            \n            \n            # Create Y_train\n            # ===============\n                \n \n\n                # set the path to the mask\n                path = 'mask_dir/' + mask_id\n\n                # read the mask\n                mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n                \n                # resize the mask\n                mask = cv2.resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                # expand dims from (800,600) to (800,600,1)\n                mask = np.expand_dims(mask, axis=-1)\n         \n                \n                \n              \n              \n              \n            # Augment the image and mask\n            # ===========================\n            \n                aug_image, aug_mask = augment_image_and_mask(aug_types, image, mask)\n              \n                # insert the image into X_train\n                X_train[i] = aug_image\n                \n                # insert the image into Y_train\n                Y_train[i] = aug_mask\n                \n                              \n                \n            # Normalize the images\n            X_train = X_train/255\n\n            yield X_train, Y_train","metadata":{"id":"8VvVDKZ1j6IV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the generator\n\n# initialize\ntrain_gen = train_generator(batch_size=10)\n\n# run the generator\nX_train, Y_train = next(train_gen)\n\nprint(X_train.shape)\nprint(Y_train.shape)","metadata":{"id":"l3hniWRIu1sd","outputId":"de95c8d4-462e-4494-93a5-fa9e1c34c02d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the first image in X_train\n\nimg = X_train[7,:,:,:]\nplt.imshow(img)","metadata":{"id":"ffbfyOhuu1p5","outputId":"9309be6f-e1b7-44fe-8948-ceca232a1ef2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the mask in Y_train\n\nmsk = Y_train[7,:,:,0]\nplt.imshow(msk)","metadata":{"id":"BkW153VbZipi","outputId":"7be96c82-f7a2-4669-8a04-ec846bebef38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(img, cmap='gray')\nplt.imshow(msk, cmap='Blues', alpha=0.7)","metadata":{"id":"Fz00UmkGZimt","outputId":"4342e8ec-170b-48b2-b91f-ef0797aa9868","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"UorQTjP1Zijt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [ 2 ] Val Generator","metadata":{"id":"Ij3dW3H-uNW0"}},{"cell_type":"code","source":"def val_generator(batch_size=10):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_val.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['new_image_fname'])\n            mask_id_list = list(df['new_mask_fname'])\n            \n            # Create empty X matrix - 3 channels\n            X_val = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n            # create empty Y matrix - 1 channel\n            Y_val = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.bool)\n\n        \n            \n            # Create X_val\n            #================\n            \n            for i, image_id in enumerate(image_id_list):\n                \n\n                # set the path to the image\n                path = 'brain_image_dir/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                # insert the image into X_train\n                X_val[i] = image\n            \n            \n            # Create Y_val\n            # ===============\n                \n            for j, mask_id in enumerate(mask_id_list):\n\n                # set the path to the mask\n                path = 'mask_dir/' + mask_id\n\n                # read the mask\n                mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n                \n                # resize the mask\n                mask = cv2.resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                # expand dims from (800,600) to (800,600,1)\n                mask = np.expand_dims(mask, axis=-1)\n                \n                \n                \n                \n                # insert the image into Y_train\n                Y_val[j] = mask\n                \n            \n            # Normalize the images\n            X_val = X_val/255\n            \n            yield X_val, Y_val","metadata":{"id":"Pk5DDEHKZigK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the generator\n\n# initialize\nval_gen = val_generator(batch_size=10)\n\n# run the generator\nX_val, Y_val = next(val_gen)\n\nprint(X_val.shape)\nprint(Y_val.shape)","metadata":{"id":"O5Mr2yLcZidT","outputId":"4ffe5946-ef0a-402d-fb7a-d075624ae3b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the image from X_val\n\nimg = X_val[7,:,:,:]\nplt.imshow(img)","metadata":{"id":"Y3qOQKGRZiaL","outputId":"869f15cd-c775-4adf-8f75-9444365fe22d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the mask from Y_val\n\nmsk = Y_val[7,:,:,0]\nplt.imshow(msk)","metadata":{"id":"ODM5m_IXuw8f","outputId":"a8251bbd-f8d3-4c9f-b729-468762caad8a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the mask and the image\n\nplt.imshow(img, cmap='gray')\nplt.imshow(msk, cmap='Blues', alpha=0.7)","metadata":{"id":"fC0YYkqcuw4P","outputId":"526bca8b-ad45-4326-a20c-ef6b3ae1b903","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Kko5ziSxuw0_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [ 3 ] Test Generator","metadata":{"id":"9EVZXAr8NpJc"}},{"cell_type":"markdown","source":"In this test generator we will output both the test images (X_test) and the test masks (Y_test). ","metadata":{"id":"zJFiJbCmSOzW"}},{"cell_type":"code","source":"def test_generator(batch_size=1):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_test.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['new_image_fname'])\n            mask_id_list = list(df['new_mask_fname'])\n            \n            # Create empty X matrix - 3 channels\n            X_test = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), dtype=np.uint8)\n            \n            # create empty Y matrix - 1 channel\n            Y_test = np.zeros((len(df), IMAGE_HEIGHT, IMAGE_WIDTH, 1), dtype=np.bool)\n            \n\n\n            \n            # Create X_test\n            #================\n            \n            for i, image_id in enumerate(image_id_list):\n                \n\n                # set the path to the image\n                path = 'brain_image_dir/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n           \n                \n                # convert to from BGR to RGB\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                \n                # resize the image\n                image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                # insert the image into X_train\n                X_test[i] = image\n                \n             \n            # Create Y_test\n            # ===============\n                \n            for j, mask_id in enumerate(mask_id_list):\n\n                # set the path to the mask\n                path = 'mask_dir/' + mask_id\n\n                # read the mask\n                mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n                \n                # resize the mask\n                mask = cv2.resize(mask, (IMAGE_HEIGHT, IMAGE_WIDTH))\n                \n                # expand dims from (800,600) to (800,600,1)\n                mask = np.expand_dims(mask, axis=-1)\n                \n                \n                \n                \n                # insert the image into Y_train\n                Y_test[j] = mask\n            \n            \n            # Normalize the images\n            X_test = X_test/255\n            \n            yield X_test, Y_test\n","metadata":{"id":"HvUDCzhxNocA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the generator\n\n# initialize\ntest_gen = test_generator(batch_size=15)\n\n# run the generator\nX_test, Y_test = next(test_gen)\n\nprint(X_test.shape)\nprint(Y_test.shape)","metadata":{"id":"kMR3qsCJNoYY","outputId":"6712757e-7a6a-4c11-9345-987d2c091864","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the image from X_test\n\nimg = X_test[14,:,:,:]\nplt.imshow(img)","metadata":{"id":"t4wMC6CANoTc","outputId":"f0ec6e0f-3260-46d8-8a36-e49c822a5367","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the mask from Y_test\n\nmsk = Y_test[14,:,:,0]\nplt.imshow(msk)","metadata":{"id":"zMI7afntOamI","outputId":"84eb94dc-b529-417c-c9ec-1ec6cf559073","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the mask and the image\n\nplt.imshow(img, cmap='gray')\nplt.imshow(msk, cmap='Blues', alpha=0.7)","metadata":{"id":"JqWAF4q1SOzm","outputId":"9d0bfaa4-55e6-47fa-9537-5d6f5a6089c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture","metadata":{"id":"tFWMN1InxNbn"}},{"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input, UpSampling2D\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nfrom keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n                                        ModelCheckpoint, CSVLogger, LearningRateScheduler)\n\n\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\n\nfrom keras.initializers import he_normal \n\nimport tensorflow as tf","metadata":{"id":"TtdRNebmu-N3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"WyKJIFfMTtuC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom segmentation_models import Unet, FPN\nfrom segmentation_models import  get_preprocessing # this line has an error in the docs\n\nfrom segmentation_models.losses import bce_jaccard_loss\nfrom segmentation_models.metrics import iou_score\n\nfrom segmentation_models.losses import dice_loss\n#from segmentation_models.metrics import dice_score\n\nfrom segmentation_models.utils import set_trainable","metadata":{"id":"bqm77A29Sgr8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocess = get_preprocessing('resnet101') # for resnet, img = (img-110.0)/1.0\n\nBACKBONE = 'densenet121'\npreprocess_input = get_preprocessing(BACKBONE)\n\n# Note that the model takes 3-channel images as input\nmodel = Unet(BACKBONE, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), \n             #freeze_encoder=False,\n             classes=1, \n             encoder_weights='imagenet',\n             activation='sigmoid')\n\n#model.summary()","metadata":{"id":"87IKb0VzSi58","outputId":"3c9a0187-a4b8-4596-8789-39a102f37562","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"doE9X1K8Sitd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create X_test\nHere we will use the test generator with a batch size of len(df_test) to create X_test and Y_test. Because the batch size is equal to the number of rows in df_test, the generator will ouput the entire\ntest set (100 rows) as a matrix.","metadata":{"id":"44aJce_1VwFO"}},{"cell_type":"code","source":"# initialize\ntest_gen = test_generator(batch_size=len(df_test))\n\n# run the generator\nX_test, Y_test = next(test_gen)\n\nprint(X_test.shape)\nprint(Y_test.shape)","metadata":{"id":"XGzT6j-WVwrC","outputId":"e9730068-a946-48ab-9a0c-b12b43ad3f61","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"92TTU4FhVwmf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model","metadata":{"id":"_krkC31sx81p"}},{"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = BATCH_SIZE\nval_batch_size = BATCH_SIZE\n\n# determine numtrain steps\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\n# determine num val steps\nval_steps = np.ceil(num_val_samples / val_batch_size)","metadata":{"id":"qxVDEJDfu-Hg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the generators\ntrain_gen = train_generator(batch_size=BATCH_SIZE)\nval_gen = val_generator(batch_size=BATCH_SIZE)\n\nmodel.compile(\n    Adam(lr=0.0001),\n    loss=dice_loss,\n    metrics=[iou_score],\n)\n\n\n\nfilepath = \"model.h5\"\n\nearlystopper = EarlyStopping(patience=5, verbose=1)\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, \n                                   verbose=1, mode='min')\n\n\n\nlog_fname = 'training_log.csv'\ncsv_logger = CSVLogger(filename=log_fname,\n                       separator=',',\n                       append=False)\n\ncallbacks_list = [checkpoint, earlystopper, csv_logger, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=40, \n                              validation_data=val_gen, validation_steps=val_steps,\n                             verbose=1,\n                             callbacks=callbacks_list)","metadata":{"id":"WqBtkIHiu-ER","outputId":"b21a30db-20c2-44e8-f7c8-8f1ee397ba41","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history\nwith open('history.txt','w') as f:\n    f.write(str(history.history))","metadata":{"id":"Qgmz_hbOSO0C","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make a test set prediction","metadata":{"id":"Rc84QAAzSO0E"}},{"cell_type":"code","source":"# Make a prediction\n\n# initialize the test generator\ntest_gen = test_generator(batch_size=1)\n\nmodel.load_weights('model.h5')\npredictions = model.predict_generator(test_gen, \n                                      steps=len(df_test),  \n                                      verbose=1)","metadata":{"id":"oCaMFPoXu9_5","outputId":"05385d08-eeab-475a-9435-ccf80e9559f5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"id":"3nIMYJunu98m","outputId":"1841ba75-465d-493b-f817-a922b6207385","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Threshold the predictions","metadata":{"id":"eS5-VgXxO-BA"}},{"cell_type":"markdown","source":"The predictions are actually probabilities that a pixel is either part of a lung or part of the background. Here we threshold the predictions to convert all values to either 0 or 1.\n\nWe will use a threshold of 0.7. I got this number by trial and error - try a threshold value and look at the quality of the test set segmentations. ","metadata":{"id":"zdvqWTLCPB6k"}},{"cell_type":"code","source":"preds_test_thresh = (predictions >= 0.7).astype(np.uint8)\n\npreds_test_thresh.shape\n\nprint(preds_test_thresh.min())\nprint(preds_test_thresh.max())","metadata":{"id":"P4IXBYDlu94b","outputId":"a8e8c760-78b1-4eb6-bb32-ed4e2e282348","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a predicted mask\n\nmask = preds_test_thresh[3,:,:,0]\nplt.imshow(mask, cmap='Reds', alpha=0.3)","metadata":{"id":"w7uFCxJvZiWM","outputId":"44b26db3-7132-4e32-a0cc-904a03565331","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is a true mask\n\ntrue_mask = Y_test[3,:,:,0]\nplt.imshow(true_mask, cmap='Blues', alpha=0.3)","metadata":{"id":"MN063pY2MEpK","outputId":"cda8b702-0519-4b4e-9bc5-037c1044980b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the x-ray image\n\nimage = X_test[3,:,:,:]\n\nplt.imshow(image)","metadata":{"id":"m91zEAY8SDDH","outputId":"3248d5f1-ef13-4e04-8518-2b7775db2fe6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is an overlay of the pred mask, true mask and \n# the x-ray image.\n\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)","metadata":{"id":"oQt1_pFJSDAU","outputId":"3ae1fe9f-0323-4fb1-e070-84fec76440e3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overlay pred masks, true masks and the x-ray image\n\nRed - True Mask<br>\nBlue - Pred Mask","metadata":{"id":"xsPyl91kX7QB"}},{"cell_type":"code","source":"# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nplt.axis('Off')\n\npredicted_masks = preds_test_thresh\n\n\ncount = 1\n# image\nplt.subplot(1,4,1)\nimage = X_test[1,:,:,:] \nmask = predicted_masks[1, :, :, 0]\ntrue_mask = Y_test[1, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,2)\nimage = X_test[2,:,:,:] \nmask = predicted_masks[2, :, :, 0]\ntrue_mask = Y_test[2, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,3)\nimage = X_test[3,:,:,:]\nmask = predicted_masks[3, :, :, 0]\ntrue_mask = Y_test[3, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,4)\nimage = X_test[4,:,:,:] \nmask = predicted_masks[4, :, :, 0]\ntrue_mask = Y_test[4, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n\n# ============ #\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nplt.axis('Off')\n\n\n# image\nplt.subplot(1,4,1)\nimage = X_test[5,:,:,:] \nmask = predicted_masks[5, :, :, 0]\ntrue_mask = Y_test[5, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,2)\nimage = X_test[6,:,:,:] \nmask = predicted_masks[6, :, :, 0]\ntrue_mask = Y_test[6, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,3)\nimage = X_test[7,:,:,:] \nmask = predicted_masks[7, :, :, 0]\ntrue_mask = Y_test[7, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,4)\nimage = X_test[8,:,:,:] \nmask = predicted_masks[8, :, :, 0]\ntrue_mask = Y_test[8, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# ============ #\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nplt.axis('Off')\n\n\n# image\nplt.subplot(1,4,1)\nimage = X_test[9,:,:,:] \nmask = predicted_masks[9, :, :, 0]\ntrue_mask = Y_test[9, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,2)\nimage = X_test[10,:,:,:] \nmask = predicted_masks[10, :, :, 0]\ntrue_mask = Y_test[10, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,3)\nimage = X_test[11,:,:,:] \nmask = predicted_masks[11, :, :, 0]\ntrue_mask = Y_test[11, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,4)\nimage = X_test[12,:,:,:] \nmask = predicted_masks[12, :, :, 0]\ntrue_mask = Y_test[12, :, :, 0]\nplt.imshow(image, cmap='gray')\nplt.imshow(true_mask, cmap='Reds', alpha=0.3)\nplt.imshow(mask, cmap='Blues', alpha=0.3)\nplt.savefig(str(count)+'.jpg')\ncount+=1\nplt.axis('off')\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"id":"Pp-Cm3jvSC9i","outputId":"4861e88e-2357-4a12-d53c-8e61c8129e26","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"B_zxKauJW7Cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## More Architectures to try...\n\nThese are three other architectures that I experimented with. They also produced good results in google colab. ","metadata":{}},{"cell_type":"markdown","source":"> ### - Segmentation using efficientnet","metadata":{}},{"cell_type":"code","source":"# Segmentation only\n# Encoder: efficientnetb0 \n# Decoder: Unet\n\n\nBACKBONE = 'efficientnetb0'\npreprocess_input = get_preprocessing(BACKBONE)\n\n# Note that the model takes 3-channel images as input\nmodel = Unet(BACKBONE, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), \n             #freeze_encoder=False,\n             classes=1, \n             encoder_weights='imagenet',\n             activation='sigmoid')\n\n#model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### - Segmentation and Classification combined\n\nThese models output both an image segmentation and an image binary classification (classifies whether intracranial hemorrhage is present or not).","metadata":{}},{"cell_type":"code","source":"# Segmentation and Binary Classification\n# Encoder: efficientnetb0 \n# Decoder: Unet\n\n\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n\n\nBACKBONE = 'efficientnetb0'\npreprocess_input = get_preprocessing(BACKBONE)\n\n# Note that the model takes 3-channel images as input\nmodel = Unet(BACKBONE, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), \n             #freeze_encoder=False,\n             classes=1, \n             encoder_weights='imagenet',\n             activation='sigmoid')\n\n\n# classif path\nx = GlobalAveragePooling2D()(model.layers[266].output)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nclassif_output = Dense(1, activation='sigmoid', name='classif_output')(x)\n\n\nmy_model = Model(inputs=[model.input], outputs=[model.output, classif_output])\n\n\n\n#my_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Segmentation and Binary Classification\n# Encoder: densenet121 \n# Decoder: Unet\n\nfrom keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D)\n\nBACKBONE = 'densenet121'\npreprocess_input = get_preprocessing(BACKBONE)\n\n# Note that the model takes 3-channel images as input\nmodel = Unet(BACKBONE, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), \n             #freeze_encoder=False,\n             classes=1, \n             encoder_weights='imagenet',\n             activation='sigmoid')\n\n\n# classif path\nx = GlobalAveragePooling2D()(model.layers[266].output) #layer 197 Resnet34, 266 for efficientnetb0\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nclassif_output = Dense(1, activation='sigmoid', name='classif_output')(x)\n\n\nmy_model = Model(inputs=[model.input], outputs=[model.output, classif_output])\n\n#my_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helpful Resources\n\n- CT Scan Basics<br>\nhttps://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/109335\n\n- Kaggle RSNA Intracranial Hemorrhage Detection Competition<br>\nhttps://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/overview\n\n\n- segmentation_models package by Pavel Yakubovskiy <br>\nhttps://github.com/qubvel/segmentation_models\n\n- Write up on segmentation models package by Chris Deotte<br>\nhttps://www.kaggle.com/c/severstal-steel-defect-detection/discussion/103367\n\n\n- Albumentations paper:<br>\nAlbumentations: fast and flexible image augmentations<br>\nhttps://arxiv.org/abs/1809.06839\n\n- If you would like to learn how to build apps like this I recommend this video tutorial:<br>\nhttps://www.youtube.com/watch?v=HEQDRWMK6yY\n\n\n- I've also included a few practical tips on the readme page in this repo:<br>\nhttps://github.com/vbookshelf/Skin-Lesion-Analyzer\n\n","metadata":{"id":"2rZoNP78u1Da","trusted":true}}]}