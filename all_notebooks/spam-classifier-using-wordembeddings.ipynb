{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Probelm Statement:","metadata":{}},{"cell_type":"markdown","source":"To build a classification model using word embeddings to classify email messages as Spam or Ham.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-11T07:43:22.460948Z","iopub.execute_input":"2021-07-11T07:43:22.461342Z","iopub.status.idle":"2021-07-11T07:43:22.491943Z","shell.execute_reply.started":"2021-07-11T07:43:22.461256Z","shell.execute_reply":"2021-07-11T07:43:22.49066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Required Libraries:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import KeyedVectors\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:54:54.702509Z","iopub.execute_input":"2021-07-11T07:54:54.702908Z","iopub.status.idle":"2021-07-11T07:54:54.71014Z","shell.execute_reply.started":"2021-07-11T07:54:54.702874Z","shell.execute_reply":"2021-07-11T07:54:54.708792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data:","metadata":{}},{"cell_type":"code","source":"data_spam=pd.read_csv(\"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")\ndata_spam.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:43:30.386757Z","iopub.execute_input":"2021-07-11T07:43:30.387181Z","iopub.status.idle":"2021-07-11T07:43:30.441937Z","shell.execute_reply.started":"2021-07-11T07:43:30.387137Z","shell.execute_reply":"2021-07-11T07:43:30.440582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace Spam with 1 and Ham with 0\ndata_spam['Category']=data_spam['Category'].replace({'ham':0,'spam':1})\ndata_spam.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:43:30.444292Z","iopub.execute_input":"2021-07-11T07:43:30.44475Z","iopub.status.idle":"2021-07-11T07:43:30.464218Z","shell.execute_reply.started":"2021-07-11T07:43:30.444705Z","shell.execute_reply":"2021-07-11T07:43:30.462551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_spam.shape    # 5572 rows and 2 columns available","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:43:30.466013Z","iopub.execute_input":"2021-07-11T07:43:30.466536Z","iopub.status.idle":"2021-07-11T07:43:30.478792Z","shell.execute_reply.started":"2021-07-11T07:43:30.466465Z","shell.execute_reply":"2021-07-11T07:43:30.477392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Pre-processing:","metadata":{}},{"cell_type":"code","source":"# Original sentence before text pre-processing\ndata_spam['Message'][2]","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:43:30.480618Z","iopub.execute_input":"2021-07-11T07:43:30.481085Z","iopub.status.idle":"2021-07-11T07:43:30.492007Z","shell.execute_reply.started":"2021-07-11T07:43:30.48104Z","shell.execute_reply":"2021-07-11T07:43:30.490632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words=stopwords.words('english')\nlemma=WordNetLemmatizer()\n\nsentence=data_spam['Message']\n\nnew_sentence=[]\nfor i in range(len(sentence)):\n    \n    # excluding the irrelavent characters\n    word=re.sub(\"[^a-zA-Z\\.\\!]\",\" \",sentence[i])\n    \n    #Lower casing all words\n    word=word.lower()\n    \n    #Splitting each sentence into words\n    word=word.split()\n    \n    #Performing lemmatization to retain the root word out of all the inflected words. Also eliminating stopwords\n    word=[lemma.lemmatize(w) for w in word if w not in stop_words]\n    word=\" \".join(word)\n    new_sentence.append(word)\n\n#Sentence after pre-processing\nnew_sentence[2]","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:43:30.493891Z","iopub.execute_input":"2021-07-11T07:43:30.494685Z","iopub.status.idle":"2021-07-11T07:43:33.216525Z","shell.execute_reply.started":"2021-07-11T07:43:30.494633Z","shell.execute_reply":"2021-07-11T07:43:33.215471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_spam['New_Message']=new_sentence\ndata_spam.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:43:33.219668Z","iopub.execute_input":"2021-07-11T07:43:33.220091Z","iopub.status.idle":"2021-07-11T07:43:33.233904Z","shell.execute_reply.started":"2021-07-11T07:43:33.220046Z","shell.execute_reply":"2021-07-11T07:43:33.232497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building a Word Embedding Matrix:","metadata":{}},{"cell_type":"code","source":"# Loading a pre-trained word2vec model \nembeddings=KeyedVectors.load_word2vec_format('../input/word2vec-google/GoogleNews-vectors-negative300.bin',binary=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:45:21.400471Z","iopub.execute_input":"2021-07-11T07:45:21.400992Z","iopub.status.idle":"2021-07-11T07:46:10.45905Z","shell.execute_reply.started":"2021-07-11T07:45:21.400948Z","shell.execute_reply":"2021-07-11T07:46:10.457822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting data into train and test\ntrain_X,test_X,train_y,test_y=train_test_split(data_spam['New_Message'],data_spam['Category'],test_size=0.3,random_state=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:46:44.132101Z","iopub.execute_input":"2021-07-11T07:46:44.132483Z","iopub.status.idle":"2021-07-11T07:46:44.143035Z","shell.execute_reply.started":"2021-07-11T07:46:44.13245Z","shell.execute_reply":"2021-07-11T07:46:44.141726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialising the Tokenizer\ntokenizer=Tokenizer()\n\n# Fitting the tokenizer on train_x\ntokenizer.fit_on_texts(train_X)\n\n# Number of words \nvocab_size=len(tokenizer.word_index)+1\n\n#Converting the words to sequence of their corresponding index\ntrain_X_seq=tokenizer.texts_to_sequences(train_X)\ntest_X_seq=tokenizer.texts_to_sequences(test_X)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:46:48.758489Z","iopub.execute_input":"2021-07-11T07:46:48.758923Z","iopub.status.idle":"2021-07-11T07:46:49.029083Z","shell.execute_reply.started":"2021-07-11T07:46:48.75889Z","shell.execute_reply":"2021-07-11T07:46:49.027911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Obtaining the length of each sequence list inoder to find the maximum length which is to be used for padding the train and test\nlen_doc=[]\nfor doc in train_X_seq:\n    doc_size=len(doc)\n    len_doc.append(doc_size)\n\nprint(\"Length of First 10 sequence:\",len_doc[:10])\nprint(\"Maximum length of sequence:\",max(len_doc))\n\nsns.boxplot(len_doc,color='green')\nplt.title(\"Distribution of length of sequence list\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:46:51.446154Z","iopub.execute_input":"2021-07-11T07:46:51.44658Z","iopub.status.idle":"2021-07-11T07:46:51.640196Z","shell.execute_reply.started":"2021-07-11T07:46:51.446503Z","shell.execute_reply":"2021-07-11T07:46:51.638818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Padding each list of sequence to ensure each of them have same dimension.Considering max length as 97\n\ntrain_X_pad=pad_sequences(train_X_seq,maxlen=97,padding='post')\ntest_X_pad=pad_sequences(test_X_seq,maxlen=97,padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:46:55.409201Z","iopub.execute_input":"2021-07-11T07:46:55.40964Z","iopub.status.idle":"2021-07-11T07:46:55.49206Z","shell.execute_reply.started":"2021-07-11T07:46:55.40958Z","shell.execute_reply":"2021-07-11T07:46:55.49095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the weight matrix \nembeddings_mat=np.zeros((vocab_size,300))\nwords_available=[]\nwords_not_available=[]\n\nfor w,wid in tokenizer.word_index.items():\n    if w in embeddings:\n        embeddings_mat[wid]=embeddings[w]\n        words_available.append(w)\n        \n    else:\n        words_not_available.append(w)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:51:20.431062Z","iopub.execute_input":"2021-07-11T07:51:20.431684Z","iopub.status.idle":"2021-07-11T07:51:20.509105Z","shell.execute_reply.started":"2021-07-11T07:51:20.431633Z","shell.execute_reply":"2021-07-11T07:51:20.50737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Amount of words not available in the pre-trained embeddings model\nlen(words_not_available)/vocab_size *100","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:52:04.886321Z","iopub.execute_input":"2021-07-11T07:52:04.886745Z","iopub.status.idle":"2021-07-11T07:52:04.893815Z","shell.execute_reply.started":"2021-07-11T07:52:04.886713Z","shell.execute_reply":"2021-07-11T07:52:04.89262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building a Classification model:","metadata":{}},{"cell_type":"code","source":"# Initialising the model\nmodel=Sequential()\n\n# Since we already have the weights matrix, there is not need to further train the weights and hence setting trainable parameter as False\nmodel.add(Embedding(vocab_size,300,weights=[embeddings_mat],input_length=97,trainable=False))\n\n# Flattening the layers\nmodel.add(Dense(units=16,activation='relu'))\n\n#Output Layer\nmodel.add(Dense(units=1,activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T08:00:08.011623Z","iopub.execute_input":"2021-07-11T08:00:08.011949Z","iopub.status.idle":"2021-07-11T08:00:08.154252Z","shell.execute_reply.started":"2021-07-11T08:00:08.01192Z","shell.execute_reply":"2021-07-11T08:00:08.153193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compiling the model:","metadata":{}},{"cell_type":"code","source":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T08:01:04.097665Z","iopub.execute_input":"2021-07-11T08:01:04.09806Z","iopub.status.idle":"2021-07-11T08:01:04.115657Z","shell.execute_reply.started":"2021-07-11T08:01:04.09803Z","shell.execute_reply":"2021-07-11T08:01:04.114546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting the model on Train data:","metadata":{}},{"cell_type":"code","source":"my_model=model.fit(x=train_X_pad,y=train_y,batch_size=32,epochs=50,validation_data=(test_X_pad,test_y))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T08:05:13.200149Z","iopub.execute_input":"2021-07-11T08:05:13.200583Z","iopub.status.idle":"2021-07-11T08:05:55.664532Z","shell.execute_reply.started":"2021-07-11T08:05:13.200532Z","shell.execute_reply":"2021-07-11T08:05:55.663355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing Performance:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nplt.plot(my_model.history['loss'],label='train_loss')\nplt.plot(my_model.history['val_loss'],label='test_loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.suptitle(\"Performance evaluation\")\n\nplt.subplot(122)\nplt.plot(my_model.history['accuracy'],label='train_accuracy')\nplt.plot(my_model.history['val_accuracy'],label='test_accuracy')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T08:27:06.559717Z","iopub.execute_input":"2021-07-11T08:27:06.560186Z","iopub.status.idle":"2021-07-11T08:27:06.917168Z","shell.execute_reply.started":"2021-07-11T08:27:06.560155Z","shell.execute_reply":"2021-07-11T08:27:06.915885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion:","metadata":{}},{"cell_type":"markdown","source":"- A spam classifier was built using  a pre-trained word embedding model\n\n- The model gave an accuracy of 87.4% on Train data and 87.8% on Test data.\n\n- The model gave a binaray cross-entropy loss of 0.37 on Train data and 0.36 on test data.\n\n- As per the scores, the model built exhibited exceptionally low over-fitting.","metadata":{}}]}