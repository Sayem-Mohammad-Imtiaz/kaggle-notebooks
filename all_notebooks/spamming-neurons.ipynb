{"cells":[{"source":"# Introduction\n\nIn this kernel we will try to build a model to classify messages as spam, we will work with Neural Networks instead of Old & Gold Machine Learning algorithms, just because NN (*Neural Networks*) are fancy now. In fact in this kernel I try to put some of the knowledge I acquired while doing Machine Learning and Deep Learning courses, it uses a not so hype dataset so we can be able to have a better understanding of the domain.\n\n\n## Outline:\n - The Data\n     - Raw Read\n     - Clean up\n     - Visualizations\n - The Model:\n     - Define goals\n     - Identify possible painpoints\n     - Build a not improved model\n     - Tune it!","metadata":{"_cell_guid":"f9ab1e3a-949d-4f1b-97cf-0de26b69d0ef","_uuid":"a773a504b747f723305d70697ff63ad33f3e6d2b"},"cell_type":"markdown"},{"source":"## The Data\n\nFirst we will load the data and clean up it a bit, even using UCI database the sets there still contains a bit of noise that can be done better, next we will perform some visualizations so we can understand better the distribution, so when we fall into some problem while training the model we might spot the problem faster.","metadata":{"_cell_guid":"0f183ec8-cd36-4209-ae0b-e6e8ebd5426b","_uuid":"497640f3f5550857ef75849e5b1c95cbd0ad4ed4"},"cell_type":"markdown"},{"source":"# data libs\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\nsns.set_palette('Pastel1')\nsns.set_style('whitegrid')\n\n# fix the seed to make this notebook reproducible\nnp.random.seed = 69","metadata":{"collapsed":true,"_cell_guid":"9512cc92-1142-455e-8cb1-042c8a29131a","_uuid":"1fbd3485e29fa6f14982e2827969c499b83014b0"},"cell_type":"code","outputs":[],"execution_count":1},{"source":"### Raw Read\n\nLet's read the data provided from the dataset using *pandas*, which offers a nice wrapper for data so us can work better with it.\n\nOne problem we might stop is that using `utf-8` encoding will crash the read, we can tell that is because some grapheme present, such as tilde.\n\n`UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte`\n\nTo overcome this we might want to use the `latin1` encoding, since it support graphemes.","metadata":{"_cell_guid":"b9fba51b-8491-4afd-a6fb-456880822c9f","_uuid":"46677a1314b547092c24e21f8c232b6717ab677f"},"cell_type":"markdown"},{"source":"data = pd.read_csv('../input/spam.csv', encoding='latin1')\ndata.head()","metadata":{"_cell_guid":"c3307f88-3b0a-4149-9c35-b3f93bb34858","_uuid":"22836b0bf9ba66faa490bf2b10e15057f87cb306"},"cell_type":"code","outputs":[],"execution_count":2},{"source":"### Clean Up\n\nWith the summary of the data printed above we can see that we have columns that does not add anything, they might ended up here after the processing data without removing indexes columns, also the headers does not help us using this dataframe.\n\nSo we will do this:\n  - Drop the Unnamed columns\n  - Rename the headers","metadata":{"_cell_guid":"b26bfc79-e2f1-49f0-a60e-758d105bbbd8","_uuid":"c1fb051d9871f7f8b56a0dd5c050b608f2ebed35"},"cell_type":"markdown"},{"source":"# drop unnamed columns\ndata.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\n# rename columns\ndata.columns = ['label', 'text']\ndata.head()","metadata":{"_cell_guid":"24ffdadd-adfc-47c8-889e-d6cf8e961a34","_uuid":"e2de78d2c2bfe4b4d81e3d47918c6c8605fc3c9b"},"cell_type":"code","outputs":[],"execution_count":3},{"source":"There you go, much nicer to work with.\n\n### Visualizations\n\nTo better undestand this data we can analyse it's content, for example, there is a distinction lenght from the spam messages from the normal ones ?","metadata":{"_cell_guid":"94faebdc-b9e4-42f1-9e6e-3e229900e5a5","_uuid":"7a5c7cfa88731890a0988e554c95c9ae6fe7891d"},"cell_type":"markdown"},{"source":"def get_lengths_of_texts(data):\n    texts = data['text'].values\n    return [len(text) for text in texts]\n\nlengths = get_lengths_of_texts(data)\n\nmean_length = np.mean(lengths)\nstd_length = np.std(lengths)\n\nspam_lengths = get_lengths_of_texts(data[data['label'] == 'spam'])\n\nmean_spam_length = np.mean(spam_lengths)\nstd_spam_length = np.std(spam_lengths)\n\nnormal_lengths = get_lengths_of_texts(data[data['label'] == 'ham'])\n\nmean_normal_length = np.mean(normal_lengths)\nstd_normal_length = np.std(normal_lengths)","metadata":{"_kg_hide-input":true,"_cell_guid":"19cd950b-3049-4677-8536-200faf464029","collapsed":true,"_uuid":"5f83757756fffb639461a959f48367bd395e8108"},"cell_type":"code","outputs":[],"execution_count":4},{"source":"def annotate_values(ax, values):\n    for react, val in zip(ax.patches, values):\n        h = react.get_height()\n        ax.text(react.get_x() + react.get_width() / 2, h - (h * 0.1),\n            \"{0:.2f}\".format(val), ha='center', style='italic', fontsize=13)\n\ngrid = GridSpec(1, 2)\nfig = plt.figure(figsize=(15,5))\nfig.suptitle('Stats from the length of text')\n\nax = plt.subplot(grid[0, 0])\nvalues = [mean_spam_length, std_spam_length]\nsns.barplot(x=['Mean', 'Std'], y=values, ax=ax)\nax.set_title('Mean & Std for Spam messages')\nannotate_values(ax, values)\n\nax = plt.subplot(grid[0, 1])\nvalues = [mean_normal_length, std_normal_length]\nsns.barplot(x=['Mean', 'Std'], y=values, ax=ax)\nax.set_title('Mean & Std for Normal messages')\nannotate_values(ax, values)\nplt.show()","metadata":{"_kg_hide-input":true,"_cell_guid":"aece8ca8-7146-44fb-9d3b-74bebcf45c43","_uuid":"be5ad094f33625d539179cc991d7c32660545c6e"},"cell_type":"code","outputs":[],"execution_count":5},{"source":"Looks like spam messages tend to fill all the possible characters on a SMS, resulting in higher mean value.\nThe limit of a SMS are *160* characters, the mean value of spams reach near it, around *138* character, in constrast, normal messages tend to have less characters, resulting in a mean lower than the spam, using this analyse of the mean one can be tempted to say \n> Oh with a simple if we can solve it!\n\nWell let's try:","metadata":{"_cell_guid":"e2ff75c0-1034-40ec-b1f6-6d63ef5cbf3c","_uuid":"0bfdd138b967eb71098e4df5cef4b56620140531"},"cell_type":"markdown"},{"source":"def isspam(message):\n    return len(message) >= 138\n\nprint(\"What am I?\")\nvld = data[data['label'] == 'spam'].sample(1)\nprint('>', 'Spam' if isspam(vld['text']) else 'Not Spam')\nprint(\"Actually I'm a\", *vld['label'].values)","metadata":{"_cell_guid":"9f62a458-e799-4fe2-a49b-8ecd60a0e3ba","_uuid":"bfad038ef247f461e5da237b71c8949e74e7cdba"},"cell_type":"code","outputs":[],"execution_count":6},{"source":"*Seems it did not work at all*\n\nBut why ? Well we can see that following the mean on the graph there is the standard deviation, this is in simple terms means that lower values tend to be close the the mean, that is, we don't have a lot of messages sparsed around the limits of a SMS. The *Std* (*Standard Deviation*) show us that the Spam messages tend to be alot more close to the mean but still have a higher deviation, that is why the `if` above fails. When looking at the *Std* of the normal messages it grows alot more than the spam, because we have messages with alot more variation on the lenght.\n\n------","metadata":{"_cell_guid":"a81ff84e-525d-4df1-8dee-b45bb5219a1b","_uuid":"a1f1cf81ee229e10ed0bbd52ad0945deaa3150e2"},"cell_type":"markdown"},{"source":"## The Model\n\nSince our attempt to use if's didn't get us far, let's try what we are supposed to try out, [Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network)!\n\n### Defining Goals\n\nAs start for every project on machine learning area, one should define what are the goals of the resulting model, for example: It should be fast to predict? How about resources, how much it can use from the CPU to calculate the prediction? Can we iterate every few days, do we need to iterate *n* times a day ? We define those so we can aim out optimization so that the resulting model can fit into the destination application.\n\nSo what is our goals ?\n\n- Small enought to fit into most of smarthphones\n- Use small cycles of computation, because of draning battery\n- Fit the training into 60min execution time of this kernel\n- Have a higher score on *false negatives* than *false positives*, meaning we might fail classifying as spam a spam message, but we can't classify as spam a legitimate message (such as code verifications).\n","metadata":{"_cell_guid":"5d26effe-c233-4288-9d62-acfe2a54de10","_uuid":"3ceb51796677d1301798d9406f49e8cdfb5ba8cd"},"cell_type":"markdown"},{"source":"### Identifying painpoints\n\nPrior start building our model, we might take a look back at the data we got:","metadata":{"_cell_guid":"e4e53ae6-9699-433b-843b-075b9aadf550","_uuid":"07567dafcae9be7b125787ed62662f03b86761b5"},"cell_type":"markdown"},{"source":"data.head()","metadata":{"_kg_hide-input":true,"_cell_guid":"0f885f4e-abf4-4784-8b39-4e4d3f3c1f40","_uuid":"43a242ec0de8658b0264cb8d62857615e8eacc9f"},"cell_type":"code","outputs":[],"execution_count":7},{"source":"As we can see our columns are with good names, thanks to what we have done before, but it has a hide range of length, so we need to think about how to encode the words into numbers, also our labels are strings insteads of `0 & 1`'s\n\nSo here what we need to do:\n\n - Convert the text into a number repesentation\n - Convert the label to numbers representing 1 for spam and 0 otherwise","metadata":{"_cell_guid":"1bd7c128-7d53-4a84-9faf-2a6bfb4c47f4","_uuid":"6e7301f580059f0cb00ef8e0d4ef2a5b97524cc5"},"cell_type":"markdown"},{"source":"# a simple generator function to compute the label\ndef encode_labels(labels):\n    return [1 if 'spam' in label else 0 for label in labels]","metadata":{"collapsed":true,"_cell_guid":"396bdeff-6bca-4f9f-8290-fbc1522a30c8","_uuid":"e6013f7dd3175dd8e96c67ccb29918aad2022c22"},"cell_type":"code","outputs":[],"execution_count":8},{"source":"from keras.preprocessing.text import Tokenizer\n\n\nclass TextEncoder:\n    def __init__(self):\n        self.tokenizer = Tokenizer()\n    \n    def fit(self, texts):\n        self.tokenizer.fit_on_texts(texts)\n    \n    def transform(self, texts):\n        return self.tokenizer.texts_to_matrix(texts)\n    \n    def fit_transform(self, texts):\n        self.fit(texts)\n        return self.transform(texts)\n    \n    @property\n    def dim(self):\n        return len(self.tokenizer.word_index) + 1\n    ","metadata":{"_cell_guid":"96ba0333-7530-4a9a-91b9-699bd465a287","_kg_hide-output":true,"_uuid":"c152ab3face280e4e104e5cb307036544da96bea"},"cell_type":"code","outputs":[],"execution_count":9},{"source":"texts = data['text'].values\nencoder = TextEncoder()\nX = encoder.fit_transform(texts)\ny = encode_labels(data['label'].values)","metadata":{"collapsed":true,"_cell_guid":"5f94b825-12a9-412a-ac27-77368f2325c1","_uuid":"a4c6e45b652e74123afb0b1fdfdab0e7af0e6f87"},"cell_type":"code","outputs":[],"execution_count":10},{"source":"Using Keras as our abstraction over backends of Neural Network frameworks, in particular Tensorflow we can use it's preprocessing modules to help us encoding the text into numbers, to make things easier I did a class that wraps the Tokenizer and expose it in a `scikit-learn` alike API, using `TextEncoder#fit` fit the text into the Tokenizer, so it can calculate the minimal and maximum size to pad the result so every example has the same length in the end, using `TextEncoder#transform` can transform a list of text into a matrix representation so the network can compute it.","metadata":{"_cell_guid":"2622ddb5-f40a-4094-a0eb-63cb4c7c3239","_uuid":"09b00358fc8ea4c882881ec057daed1a54b4a0a0"},"cell_type":"markdown"},{"source":"### Build a not improved Model\n\nTo start we will build a simple model, so we can then add more layers, units, tune parameters as we iterate for a better accuracy, because if we throw a complex network we can get awesome results with the cost of our goal definitions, so starting small anad growing is the best, controlled, way to do this.\n\nWe will use Keras again, with the preprocessed data from the previous cell, the `X` for the input and `y` as the output. The netwok will start with a shallow net, contaning just a single neuron with a sigmoid activation.","metadata":{"_cell_guid":"86c8f717-7897-426c-b588-cc265945e150","_uuid":"3f1563aab538be3e226ae804ec0cf4a85ffe5d25"},"cell_type":"markdown"},{"source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nmodel = Sequential()\n# dimensions calulated by our tokenizer\ndims = encoder.dim\n# add input layer\nmodel.add(Dense(2, input_dim=dims))\n# add output lauer\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n# fit it!\nhistory = model.fit(X, y, shuffle=True, validation_split=.2, epochs=100, batch_size=50)","metadata":{"_cell_guid":"b05c59d9-86f4-4806-b60c-c06d90279e07","_kg_hide-output":true,"_uuid":"9910ade62db5b5cd7d46fc9e7f1e5713c4c74d11"},"cell_type":"code","outputs":[],"execution_count":11},{"source":"We have fitted a simple neural network, we can see from the logs, that the loss is sooooo low so let's plot it to better understand it, because I'm smelling *overfitting*","metadata":{"_cell_guid":"8f371b8f-bfa4-41db-a389-a79b7331dd81","_uuid":"0e2189ba2866097830ae2890ae81997bbe47e972"},"cell_type":"markdown"},{"source":"grid = GridSpec(2, 2)\n\nfig = plt.figure(figsize=(13, 10))\nfig.suptitle('Model Metrics')\n\n# first plot\n\nax = plt.subplot(grid[0, :])\nax.set_title('Model Loss')\nax.plot(history.history['loss'], color='c', lw=2)\nax.plot(history.history['val_loss'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss')\n# annotate\nax.annotate('Start Overfitting', xy=(8, 0.06),\n            xytext=(10, 0.1),\n            arrowprops=dict(arrowstyle='->'))\n\nax.annotate('Look at this gape', xy=(85, 0.06),\n            xytext=(85, 0.1))\n\nax.legend(['Train Loss', 'Val Loss'], loc='best')\n\n# Second plot\n\nax = plt.subplot(grid[1, :])\nax.set_title('Model Accuracy')\nax.plot(history.history['acc'], color='c', lw=2)\nax.plot(history.history['val_acc'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Accuracy')\n# annotate\nax.annotate('Start Overfitting', xy=(8, 0.985),\n            xytext=(10, 0.97),\n            arrowprops=dict(arrowstyle='->'))\n\nax.annotate('Look at this gape', xy=(85, 0.06),\n            xytext=(85, 0.1))\n\nax.legend(['Train Accuracy', 'Val Accuracy'], loc='best')\n\nplt.show()","metadata":{"_cell_guid":"8c587bcb-e585-4732-b796-1e25d9a5a95d","_uuid":"45c49ede05317bccea3a6799b0782b1745a0b6b4"},"cell_type":"code","outputs":[],"execution_count":12},{"source":"We can see comparing the loss from training and validation that our model is overfitting, the loss drops quickly which is good, but around `10` epochs the validation loss starts increasing, that means that the model stops generalizing the learning data and failing to predict new examples and it only gets worse as the epochs increases.\n\nLooking at the accuracy plot we can also see the same effect occuring, when it reaches about `10` the accuracy of the validation set starts to drop while the training set accuracy keeps improving.\n\n### Tune it!\n\nSo what can we do about it? \n - We can't get more data\n - Maybe we can try lower the epochs ?\n \n> What is an epoch ?\n>\n> An epoch is how many times the network will *see* the data flowing through it.\n> NN starts with a random weight, a random learning as the forward and then backward propagation happens,\nthe weights are updated, but one time is usually not enough, this *time* is called epoch.\nGreater the value the epoch is more the NN will learn, which can lead to overfit.","metadata":{"_cell_guid":"dd668670-f7a2-44b3-89ef-5c333b4bba1d","_uuid":"1bde981af0bf8087c56e551c806b72a8f5e8f92b"},"cell_type":"markdown"},{"source":"model0 = Sequential()\n# dimensions calulated by our tokenizer\ndims = encoder.dim\n# add input layer\nmodel0.add(Dense(2, input_dim=dims))\n# add output lauer\nmodel0.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel0.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n# fit it!\nhistory = model0.fit(X, y, shuffle=True, validation_split=.2, epochs=10, batch_size=32)","metadata":{"_cell_guid":"d801dbf7-3813-4641-90a3-9e9cb67d6711","_kg_hide-output":true,"_uuid":"72c0811cbb4dda9fb61ec1889481e64150d3e53e"},"cell_type":"code","outputs":[],"execution_count":13},{"source":"grid = GridSpec(2, 2)\n\nfig = plt.figure(figsize=(13, 10))\nfig.suptitle('Model Metrics')\n\n# first plot\n\nax = plt.subplot(grid[0, :])\nax.set_title('Model Loss')\nax.plot(history.history['loss'], color='c', lw=2)\nax.plot(history.history['val_loss'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss')\n# annotate\nax.annotate('Start Overfitting', xy=(8, 0.06),\n            xytext=(10, 0.1),\n            arrowprops=dict(arrowstyle='->'))\n\nax.annotate('Look at this gape', xy=(85, 0.06),\n            xytext=(85, 0.1))\n\nax.legend(['Train Loss', 'Val Loss'], loc='best')\n\n# Second plot\n\nax = plt.subplot(grid[1, :])\nax.set_title('Model Accuracy')\nax.plot(history.history['acc'], color='c', lw=2)\nax.plot(history.history['val_acc'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Accuracy')\nax.legend(['Train Accuracy', 'Val Accuracy'], loc='best')\n\nplt.show()","metadata":{"_cell_guid":"69fc3854-fa0e-4da7-9e46-389ad4c6b80c","_uuid":"14e2e7149fac4de1f98ae34a1b4a993b1d4bc638"},"cell_type":"code","outputs":[],"execution_count":14},{"source":"We see an improvement over the overfitting, it still sightly overfitting near the end, we can see it better on the accuracy graph, the lines are spliting starting at epoch `8`.\n\nSe let's try with  `5`  epochs:","metadata":{"_cell_guid":"f6509fdc-09c2-4cbc-b1a5-24287bcbe40a","_uuid":"342e14c8ae579b9128f551348bc97855bdf13b17"},"cell_type":"markdown"},{"source":"model1 = Sequential()\n# dimensions calulated by our tokenizer\ndims = encoder.dim\n# add input layer\nmodel1.add(Dense(2, input_dim=dims))\n# add output lauer\nmodel1.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n# fit it!\nhistory = model1.fit(X, y, shuffle=True, validation_split=.2, epochs=5, batch_size=32)","metadata":{"_cell_guid":"2d0a9d3c-569e-4c64-9a18-b3dbd17cf44a","_kg_hide-output":true,"_uuid":"73c06a3a6c95d943ea160b860345665936f62adf"},"cell_type":"code","outputs":[],"execution_count":15},{"source":"grid = GridSpec(2, 2)\n\nfig = plt.figure(figsize=(13, 10))\nfig.suptitle('Model Metrics')\n\n# first plot\n\nax = plt.subplot(grid[0, :])\nax.set_title('Model Loss')\nax.plot(history.history['loss'], color='c', lw=2)\nax.plot(history.history['val_loss'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss')\nax.legend(['Train Loss', 'Val Loss'], loc='best')\n\n# Second plot\n\nax = plt.subplot(grid[1, :])\nax.set_title('Model Accuracy')\nax.plot(history.history['acc'], color='c', lw=2)\nax.plot(history.history['val_acc'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Accuracy')\nax.legend(['Train Accuracy', 'Val Accuracy'], loc='best')\n\nplt.show()","metadata":{"_cell_guid":"1e9320a5-05ec-4838-9b3a-1f3bb5acf858","_uuid":"fcb247a42b976d9b9940fcac53ab72c875d31a7e"},"cell_type":"code","outputs":[],"execution_count":16},{"source":"We got better resultings interrupting the learning process at epoch `5` becuase it does not iterate over the training data again, of course there are other ways to improve this, for example, more data, but since we have limited amount this is the best we got with this scenario.\n\nAs a bonus let's try to make our net deeper:","metadata":{"collapsed":true,"_cell_guid":"70bcfd77-bdee-4ec0-938c-cf3facb3c188","_uuid":"3f72308ce9550d0b8d92f61d9864292c7858101e"},"cell_type":"markdown"},{"source":"model2 = Sequential()\n# dimensions calulated by our tokenizer\ndims = encoder.dim\n# add input layer\nmodel2.add(Dense(2, input_dim=dims))\nmodel2.add(Dense(2, activation='relu'))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(2, activation='relu'))\n# add output lauer\nmodel2.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# fit it!\nhistory = model2.fit(X, y, shuffle=True, validation_split=.2, epochs=50, batch_size=50)","metadata":{"_cell_guid":"c469b91a-135b-459c-ac84-3d862783d68c","_uuid":"8e470839f87c97200b5189bb50dfedb96efaf83a"},"cell_type":"code","outputs":[],"execution_count":24},{"source":"grid = GridSpec(2, 2)\n\nfig = plt.figure(figsize=(13, 10))\nfig.suptitle('Model Metrics')\n\n# first plot\n\nax = plt.subplot(grid[0, :])\nax.set_title('Model Loss')\nax.plot(history.history['loss'], color='c', lw=2)\nax.plot(history.history['val_loss'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss')\nax.legend(['Train Loss', 'Val Loss'], loc='best')\n\n# Second plot\n\nax = plt.subplot(grid[1, :])\nax.set_title('Model Accuracy')\nax.plot(history.history['acc'], color='c', lw=2)\nax.plot(history.history['val_acc'], color='darkorange', lw=2)\nax.set_xlabel('Epochs')\nax.set_ylabel('Accuracy')\nax.legend(['Train Accuracy', 'Val Accuracy'], loc='best')\n\nplt.show()","metadata":{"_cell_guid":"eab1cb35-b6a2-4207-a3f1-ffa02da4869b","_uuid":"6d96f567da70cdf31fc3cbbc562fc26ee1c024a6"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"With one more layer, we can see that we got results a bit lower than our 'final' model, also, the accuracy show us a drop, small, but present when computing the last epoch.","metadata":{"_cell_guid":"5b6ea426-eb73-42a8-b6a3-847fe1a99a27","_uuid":"bbffd4a7f3c4cbfc85b8305fd18b77782d87e898"},"cell_type":"markdown"},{"source":"## That's all for now!\n\nBut in the future I might came back to implement the resulting, `model1`, model into raw Tensorflow outputting the Keras output!\n\n-------------\n\nThanks!","metadata":{"_cell_guid":"f8966b5c-74d1-4806-8abe-cf8a8069b953","_uuid":"4adfbf78ec75fca285ff926a58efa26c7d43615f"},"cell_type":"markdown"}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}