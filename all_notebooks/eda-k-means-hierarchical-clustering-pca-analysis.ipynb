{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Importing some useful libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing important libraries\n\nimport numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nimport pandas_profiling as pp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Detailed EDA of the dataset using pandas profiling"},{"metadata":{"trusted":true},"cell_type":"code","source":"pp.ProfileReport(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *As seen from the correlation chart,there is no true corelation between any variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping customerID as its useless for our clustering analysis\ndataset.drop('CustomerID',axis=1,inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g=sns.pairplot(dataset)\ng.fig.set_size_inches(15,15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Annual Income higher than 100k can be seen only for people in the age range 30-50\n### *Another intersing pattern of cluster formation can be seen between datapoints in the AnnualIncome-SpendingScore chart-Almost 5 clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Changing the gender column from categorical to numerical\ndataset.loc[dataset.Gender == 'Male' ,'Gender'] = 1\ndataset.loc[dataset.Gender == 'Female' ,'Gender'] = 0\n\ndataset.Gender= dataset.Gender.astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling as in a general sense,differnt scale might affect the cluster formation due to distance difference between features .In this specific dataset,even an unscaled feature set works(I have tried.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.preprocessing import StandardScaler\ncolumns = dataset.columns.values.tolist()\nfor i in columns:\n    if i != 'Gender':\n        ss = StandardScaler()\n        scaled = ss.fit_transform(dataset[[i]])\n        dataset[i] = scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cluster Number Selection Based on Inertia Method"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting the best number of cluster based on intertia scoring\nkm_list = list()\n\nfor clust in range(1,15):\n    km = KMeans(n_clusters=clust,init='k-means++', random_state=42)\n    km = km.fit(dataset)\n    \n    km_list.append(pd.Series({'clusters': clust, \n                              'inertia': km.inertia_,\n                              'model': km}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the effect of increased cluser numbers on inertia\nplot_data = (pd.concat(km_list, axis=1)\n             .T\n             [['clusters','inertia']]\n             .set_index('clusters'))\n\nax = plot_data.plot(marker='o',ls='-')\nax.set_xticks(range(0,15,1))\nax.set_xlim(0,16)\nax.axhline(y=190, c='black',ls='--')\nax.set(xlabel='Cluster', ylabel='Inertia');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *The dotted line shows that after 5 clusters are formed,there is no large decrease in inertia value.We can choose either 4n or 5 according to our threshold inertia value.I took 5 as it is evident from the earlier graph of Scoring and Income.Note:This if we want to cluster based on that specific features."},{"metadata":{},"cell_type":"markdown","source":"## Selecting the cluster number based on Hierarchical Clustering Distance Threshold "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n#Note the base n_cluster value of 2 is taken to show the whole tree\nag = AgglomerativeClustering(n_clusters=2, linkage='ward', compute_full_tree=True)\nag = ag.fit(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster import hierarchy\nZ = hierarchy.linkage(ag.children_, method='ward')\n\nfig, ax = plt.subplots(figsize=(15,5))\n\n# Some color setup\nred = 'red'\nblue = 'blue'\nthreshold = 480\n\nhierarchy.set_link_color_palette([red, 'green'])\n\nden = hierarchy.dendrogram(Z, orientation='top', \n                           p=30, truncate_mode='lastp',\n                           show_leaf_counts=True, ax=ax,\n                           above_threshold_color=blue)\nax.axhline(y=threshold, c='black',ls='--')\nax.set_ylabel('Distance')\nax.set_xlabel('datapoint_Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *From the dendrogram ,it is eveident from the dotted line that the distance doesnt decrease drastically if we increase the cluster number even further. Both these analysis points to taking the number of clusters as 5. "},{"metadata":{},"cell_type":"markdown","source":"## 1.Clustering with the entire dataset taken."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Kmeans with 5 clusters\nkm = KMeans(n_clusters=5,init='k-means++', random_state=36)\nkm = km.fit(dataset)\ndataset['km'] = km.fit_predict(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#AgglomerativeClustering with ward linkage\nag = AgglomerativeClustering(n_clusters=5, linkage='ward', compute_full_tree=True)\nag = ag.fit(dataset.iloc[:,:-1])\ndataset['agglom'] = ag.fit_predict(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the clusters predicted.\ncolor = 'brgym'\nalpha = 0.5\nlabels=['Cluster1','Cluster2','Cluster3','Cluster4','Cluster5']\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][km.labels_==i],dataset['Spending Score (1-100)'][km.labels_==i],c = color[i],alpha = alpha,s=20)\n    plt.scatter(km.cluster_centers_[i,2],km.cluster_centers_[i,3],c = color[i], marker = 'X', s = 200,label=labels[i])\nplt.ylabel('Annual Income')\nplt.xlabel('Spending Score')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *It is evident that the model could not predict these two features entirely.However this might be also due to the fact that it might have given other features(other than Annual Income or Spending Score) more importance.\n### *Cluster 5 and 3 are wrongly labelled or interchanged."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using agglomerative hierarchichal clustering labels\ncolor = 'brgcm'\nalpha = 0.5\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][ag.labels_==i],dataset['Spending Score (1-100)'][ag.labels_==i],c = color[i],alpha = alpha,s=20)\nplt.ylabel('Annual Income')\nplt.xlabel('Spending Score')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Almost exact result using agglomerative clustering too.\n### *We can either use this or give the selected two variables more importance by clustering just on the basis of these two variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"color = 'brgym'\nalpha = 0.5\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][ag.labels_==i],dataset['Age'][ag.labels_==i],c = color[i],alpha = alpha,s=20)\nplt.ylabel('Annual Income')\nplt.xlabel('Age')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The above graph cements our earlier insight that other features might have had more importane in pur clustering as the model almost succesfully clustered the datapoints in the above chart."},{"metadata":{},"cell_type":"markdown","source":"## 2.Clustering with just the two features taken"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking just 'Annual Income (k$)', 'Spending Score (1-100)' for clustering\ndataset_small = dataset[['Annual Income (k$)', 'Spending Score (1-100)']]\nkm = KMeans(n_clusters=5,init='k-means++', random_state=42)\nkm = km.fit(dataset_small)\ndataset_small['km'] = km.fit_predict(dataset_small)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting\ncolor = 'brgcm'\nalpha = 0.5\nfor i in range(5):\n    plt.scatter(dataset_small['Annual Income (k$)'][km.labels_==i],dataset_small['Spending Score (1-100)'][km.labels_==i],c = color[i],alpha = alpha,s=20)\n    plt.scatter(km.cluster_centers_[i,0],km.cluster_centers_[i,1],c = color[i], marker = 'X', s = 200,label=labels[i])\nplt.ylabel('Annual Income')\nplt.xlabel('Spending Score')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *As expected the model correclty formed 5 clusters without large number of outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the new model in the old dataset with different features\nfor i in range(5):\n    plt.scatter(dataset['Annual Income (k$)'][km.labels_==i],dataset['Age'][km.labels_==i],c = color[i],alpha = alpha,s=20,label=labels[i])\nplt.ylabel('Annual Income')\nplt.xlabel('Age')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So ,in this specifi dataset,selecting just the two variables and clustering is a more apt solution than takin the whole dataset\n#We will replce the predicted labels column of the old dataset with the new one\ndataset['km']=dataset_small['km']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA and Variance Explanation Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if we can reduce the number of features by using PCA\ndataset_pca = dataset.iloc[:,:-2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nfeature_weight = []\nvariance_explained = []\n\n# We can select upto 4 features as it is tha max\n\nfor n in range(1, 4):\n    \n    PCAmod = PCA(n_components=n)\n    PCAmod.fit(dataset_pca)\n    \n    # Store the model and variance\n    variance_explained.append(PCAmod.explained_variance_ratio_.sum())\n    \n    # Calculate and store feature importances\n    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n    feature_weight.append(pd.DataFrame({'n':n, \n                                             'features': dataset_pca.columns,\n                                             'values':abs_feature_values/abs_feature_values.sum()}))\nvar=pd.DataFrame(variance_explained)\nvar   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_df = (pd.concat(feature_weight)\n               .pivot(index='n', columns='features', values='values'))\n\nfeatures_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *From the above dataframe,it is clear that Gender as a feature have the least importance in all out PCA lists.Age and Gender forms the maximum importaent duo followed closely by Annual Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the variance explanation\nax = var.plot(kind='bar')\n\nax.set(xlabel='Number of dimensions',\n       ylabel='Percent explained variance',\n       title='Explained Variance vs Dimensions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *So just with 2 dimesions,we can capture almost 72% of the dataset variance.If it is increased to 3 ,more than 92 percent can be captured ,so that we can reduce with a single or two features from this dataset without losing much information."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}