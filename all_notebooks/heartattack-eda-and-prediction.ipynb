{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. BUSINESS UNDERSTANDING\n\n### Objective: Predict whether a person is prone to heart attack or not based on the information available.\n\n* a) Perform Exploratory Data Analysis on the information / dataset available to gather insights around it. \n* b) Additionally, perform predict if a person is prone to heart attack or not.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/heartpredictionimage/HeartAttackPrediction_Image.png\")","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-07-08T04:12:26.230475Z","iopub.execute_input":"2021-07-08T04:12:26.230896Z","iopub.status.idle":"2021-07-08T04:12:26.339534Z","shell.execute_reply.started":"2021-07-08T04:12:26.230821Z","shell.execute_reply":"2021-07-08T04:12:26.338048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. DATA UNDERSTANDING\n\n#### There are 2 files provided as inputs.\n* o2saturation.csv\n* heart.csv\n\n#### Description of dataset features are captured below.\n\n* age : Age of the patient\n* sex : Sex of the patient\n    * 1: Male\n    * 0: Female\n* cp : Chest Pain type\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n* trtbps : resting blood pressure (in mm Hg)\n* chol : cholestoral in mg/dl fetched via BMI sensor\n* fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n    * 1: True (i.e. Fasting Blood Sugar > 120mg/dl)\n    * 0: False (i.e. Fasting Blood Sugar < 120mg/dl)\n* rest_ecg : resting electrocardiographic results\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach : maximum heart rate achieved\n* exng: exercise induced angina (1 = yes; 0 = no)\n* oldpeak: Previous peak\n* slp: the slope of the peak exercise ST segment \n* caa: number of major vessels (0-4) - 0/1/2/3/4\n* thall: thallium stress result - 0/1/2/3 etc\n* output: (This is the TARGET variable)\n    * 0= less chance of heart attack \n    * 1= more chance of heart attack \n","metadata":{}},{"cell_type":"markdown","source":"# 2a. Get required libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport time\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:12:34.138931Z","iopub.execute_input":"2021-07-08T04:12:34.139446Z","iopub.status.idle":"2021-07-08T04:12:35.945414Z","shell.execute_reply.started":"2021-07-08T04:12:34.139387Z","shell.execute_reply":"2021-07-08T04:12:35.944733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2b. Read from Datasets\n\n### O2 Saturation Dataset","metadata":{}},{"cell_type":"code","source":"o2sat = pd.read_csv(\"../input/heart-attack-analysis-prediction-dataset/o2Saturation.csv\")\no2sat.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:12:39.784133Z","iopub.execute_input":"2021-07-08T04:12:39.784696Z","iopub.status.idle":"2021-07-08T04:12:39.81593Z","shell.execute_reply.started":"2021-07-08T04:12:39.784654Z","shell.execute_reply":"2021-07-08T04:12:39.81526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o2sat.mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:12:41.678531Z","iopub.execute_input":"2021-07-08T04:12:41.678903Z","iopub.status.idle":"2021-07-08T04:12:41.688092Z","shell.execute_reply.started":"2021-07-08T04:12:41.678871Z","shell.execute_reply":"2021-07-08T04:12:41.686986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o2sat.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:12:42.96508Z","iopub.execute_input":"2021-07-08T04:12:42.965532Z","iopub.status.idle":"2021-07-08T04:12:42.986096Z","shell.execute_reply.started":"2021-07-08T04:12:42.965502Z","shell.execute_reply":"2021-07-08T04:12:42.985029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o2sat.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:12:44.354384Z","iopub.execute_input":"2021-07-08T04:12:44.354727Z","iopub.status.idle":"2021-07-08T04:12:44.374302Z","shell.execute_reply.started":"2021-07-08T04:12:44.35469Z","shell.execute_reply":"2021-07-08T04:12:44.373402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.histplot(data=o2sat['98.6'])","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:12:45.648029Z","iopub.execute_input":"2021-07-08T04:12:45.649967Z","iopub.status.idle":"2021-07-08T04:12:45.90293Z","shell.execute_reply.started":"2021-07-08T04:12:45.649932Z","shell.execute_reply":"2021-07-08T04:12:45.902123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that approximately 89% values range between 96.5 to 98.6. Most common values are 98.6","metadata":{}},{"cell_type":"markdown","source":"### Heart Attack Dataset","metadata":{}},{"cell_type":"code","source":"heart = pd.read_csv (\"../input/heart-attack-analysis-prediction-dataset/heart.csv\")\nheart.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:12:53.211206Z","iopub.execute_input":"2021-07-08T04:12:53.211789Z","iopub.status.idle":"2021-07-08T04:12:53.2395Z","shell.execute_reply.started":"2021-07-08T04:12:53.211739Z","shell.execute_reply":"2021-07-08T04:12:53.238595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2c. Profile Report Analysis to understand features, distributions & correlations","metadata":{}},{"cell_type":"markdown","source":"This is just an example of one of AutoEDA (Automated Exploratory Data Analysis) considered for quick insights / analysis. We can always consider any such methods / approaches which we can use and see if it helps in our current context. For example other AutoEDA libraries could be - SweetViz, LUX, AutoViz, DataPrep, DTale etc.\n\nIn any case, these will provide some quick insights and save time for us. Post that, we can focus more in depth into certain areas such as correlation or interaction between features to understand more and take actions as part of our Data Preparation / Feature Engineering steps.","metadata":{}},{"cell_type":"code","source":"pp.ProfileReport(heart, explorative = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:13:00.017113Z","iopub.execute_input":"2021-07-08T04:13:00.017538Z","iopub.status.idle":"2021-07-08T04:13:16.961521Z","shell.execute_reply.started":"2021-07-08T04:13:00.017502Z","shell.execute_reply":"2021-07-08T04:13:16.96057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations in the Heart Dataset: \n    * 14 columns/features and 303 rows/observations\n    * It indicated no missing values\n    * 1 duplicate row\n    * A column \"oldpeak\" has almost 32.7% zero values\n    ","metadata":{}},{"cell_type":"markdown","source":"#### We observed from the Profile report about some of the features and data representations. Now we will further do EDA and Data Preparation to have pre-processing, more charts / visualization prior to making the data ready for model development phase.\n","metadata":{}},{"cell_type":"markdown","source":"# 3. DATA PREPARATION ","metadata":{}},{"cell_type":"markdown","source":"# 3a. Since there are duplicates, let's remove them.","metadata":{}},{"cell_type":"code","source":"heart.shape # Get quick snapshot of number of rows and features.","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:29.156819Z","iopub.execute_input":"2021-07-08T04:16:29.157215Z","iopub.status.idle":"2021-07-08T04:16:29.161791Z","shell.execute_reply.started":"2021-07-08T04:16:29.157157Z","shell.execute_reply":"2021-07-08T04:16:29.161118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"heart[heart.duplicated()] #understand which row is duplicate","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:30.683027Z","iopub.execute_input":"2021-07-08T04:16:30.683499Z","iopub.status.idle":"2021-07-08T04:16:30.697807Z","shell.execute_reply.started":"2021-07-08T04:16:30.683468Z","shell.execute_reply":"2021-07-08T04:16:30.69701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed = heart.copy()\n\ndf_processed.drop_duplicates(inplace = True)\ndf_processed.reset_index(drop = True, inplace = True)\ndf_processed.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:32.054718Z","iopub.execute_input":"2021-07-08T04:16:32.055042Z","iopub.status.idle":"2021-07-08T04:16:32.06494Z","shell.execute_reply.started":"2021-07-08T04:16:32.055007Z","shell.execute_reply":"2021-07-08T04:16:32.064058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed[df_processed.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:34.541607Z","iopub.execute_input":"2021-07-08T04:16:34.541942Z","iopub.status.idle":"2021-07-08T04:16:34.554806Z","shell.execute_reply.started":"2021-07-08T04:16:34.541914Z","shell.execute_reply":"2021-07-08T04:16:34.553589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### So, we have removed the 1 duplicate. We will proceed with this dataframe. (df_processed)","metadata":{}},{"cell_type":"markdown","source":"# 3b. Let's visualize through a histogram","metadata":{}},{"cell_type":"code","source":"df_processed.hist(figsize=(18,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:38.87023Z","iopub.execute_input":"2021-07-08T04:16:38.870555Z","iopub.status.idle":"2021-07-08T04:16:40.880049Z","shell.execute_reply.started":"2021-07-08T04:16:38.870528Z","shell.execute_reply":"2021-07-08T04:16:40.87907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3b. Correlation using Histogram","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.heatmap(df_processed.corr(),annot=True,cmap=\"PuBuGn\")","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:45.882754Z","iopub.execute_input":"2021-07-08T04:16:45.883112Z","iopub.status.idle":"2021-07-08T04:16:46.936754Z","shell.execute_reply.started":"2021-07-08T04:16:45.88308Z","shell.execute_reply":"2021-07-08T04:16:46.933361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, not much high correlation between variables. \n\nOutput (Target variable) - is correlated more relatively with cp, thalachh, slp (positively) and exng, oldpeak, caa (negatively).\n","metadata":{}},{"cell_type":"markdown","source":"# 3c. Analysis on \"Sex\" feature","metadata":{}},{"cell_type":"code","source":"df_processed.sex.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:53.313364Z","iopub.execute_input":"2021-07-08T04:16:53.313708Z","iopub.status.idle":"2021-07-08T04:16:53.321817Z","shell.execute_reply.started":"2021-07-08T04:16:53.313673Z","shell.execute_reply":"2021-07-08T04:16:53.320766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Male ~ 68.2%\n* Female ~ 31.8%","metadata":{}},{"cell_type":"markdown","source":"# 3d. Analysis on \"Age\" feature","metadata":{}},{"cell_type":"code","source":"df_processed.age.hist(figsize=(16,8),bins=30)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:16:57.588941Z","iopub.execute_input":"2021-07-08T04:16:57.58926Z","iopub.status.idle":"2021-07-08T04:16:57.774794Z","shell.execute_reply.started":"2021-07-08T04:16:57.589232Z","shell.execute_reply":"2021-07-08T04:16:57.774212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def age_category_values(df):\n    p=round(df.max()/5)*5\n    q=round(df.min()/5)*5\n    L=[i for i in range(q,p,5)]\n    dicts={}\n    M=[]\n    for a in range(len(L)):\n        dicts[L[a]]=0\n    for j in df:\n        for k in L:\n            if j<k:\n                dicts[k]+=1\n                break\n    for b in dicts:\n        M.append(([b-5,b],dicts[b]))\n    return M\n\nage_category_values(df_processed.age)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:00.75316Z","iopub.execute_input":"2021-07-08T04:17:00.753517Z","iopub.status.idle":"2021-07-08T04:17:00.76335Z","shell.execute_reply.started":"2021-07-08T04:17:00.753486Z","shell.execute_reply":"2021-07-08T04:17:00.762667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s=0\nfor i in df_processed.age:\n    if 40 <= i:\n        s+=1\nx = len(df_processed)\nprint(100*s/x)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:02.445441Z","iopub.execute_input":"2021-07-08T04:17:02.44591Z","iopub.status.idle":"2021-07-08T04:17:02.45165Z","shell.execute_reply.started":"2021-07-08T04:17:02.445879Z","shell.execute_reply":"2021-07-08T04:17:02.450842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interpretation: Around 95% of people above Age of 40 are having heart attack","metadata":{}},{"cell_type":"markdown","source":"# 3e. Analysis on \"CP\" feature\n\n### CP - Chest Pain Type\n\n* Value 0: Typical angina: chest pain related decrease blood supply to the heart\n* Value 1: Atypical angina: chest pain not related to heart\n* Value 2: Non-anginal pain: typically esophageal spasms (non heart related)\n* Value 3: Asymptomatic: chest pain not showing signs of disease","metadata":{}},{"cell_type":"code","source":"df_processed.cp.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:08.860036Z","iopub.execute_input":"2021-07-08T04:17:08.860392Z","iopub.status.idle":"2021-07-08T04:17:08.866904Z","shell.execute_reply.started":"2021-07-08T04:17:08.860351Z","shell.execute_reply":"2021-07-08T04:17:08.866254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed.cp.hist()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:10.162088Z","iopub.execute_input":"2021-07-08T04:17:10.162642Z","iopub.status.idle":"2021-07-08T04:17:10.321395Z","shell.execute_reply.started":"2021-07-08T04:17:10.162595Z","shell.execute_reply":"2021-07-08T04:17:10.320696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_processed.cp.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:13.187414Z","iopub.execute_input":"2021-07-08T04:17:13.187944Z","iopub.status.idle":"2021-07-08T04:17:13.195155Z","shell.execute_reply.started":"2021-07-08T04:17:13.187902Z","shell.execute_reply":"2021-07-08T04:17:13.194401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3f. Analysis on \"trtbps\" feature\n\n### trtbps - Resting Blood Pressure","metadata":{}},{"cell_type":"code","source":"df_processed.trtbps.hist()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:15.032675Z","iopub.execute_input":"2021-07-08T04:17:15.033207Z","iopub.status.idle":"2021-07-08T04:17:15.188256Z","shell.execute_reply.started":"2021-07-08T04:17:15.033147Z","shell.execute_reply":"2021-07-08T04:17:15.187347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def category_values(df, step):\n    p = round(df.max()/step)*step\n    q = round(df.min()/step)*step\n    L=[i for i in range(q,p+(2*step),step)]\n    dicts={}\n    M=[]\n    for a in range(len(L)):\n        dicts[L[a]]=0\n    for j in df:\n        for k in L:\n            if j<k:\n                dicts[k]+=1\n                break\n    for b in dicts:\n        M.append(([b-step,b],dicts[b]))\n    return M\n\ncategory_values(df_processed.trtbps,10)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:17.300045Z","iopub.execute_input":"2021-07-08T04:17:17.300451Z","iopub.status.idle":"2021-07-08T04:17:17.312813Z","shell.execute_reply.started":"2021-07-08T04:17:17.300416Z","shell.execute_reply":"2021-07-08T04:17:17.31174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data=df_processed,x=\"trtbps\", bins=(80,90,100,110,120,130,140,150,160,170,180,190,200))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:19.633448Z","iopub.execute_input":"2021-07-08T04:17:19.633789Z","iopub.status.idle":"2021-07-08T04:17:19.810335Z","shell.execute_reply.started":"2021-07-08T04:17:19.633761Z","shell.execute_reply":"2021-07-08T04:17:19.809368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Processing with Dummy variables\n\n### We observed that we need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. With regards to this, we will use the get_dummies method to create dummy columns for categorical variables.","metadata":{}},{"cell_type":"code","source":"df_processed.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:24.172161Z","iopub.execute_input":"2021-07-08T04:17:24.172528Z","iopub.status.idle":"2021-07-08T04:17:24.187431Z","shell.execute_reply.started":"2021-07-08T04:17:24.172496Z","shell.execute_reply":"2021-07-08T04:17:24.186124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_processed = pd.get_dummies(df_processed, columns = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall'])","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:27.064873Z","iopub.execute_input":"2021-07-08T04:17:27.065233Z","iopub.status.idle":"2021-07-08T04:17:27.078858Z","shell.execute_reply.started":"2021-07-08T04:17:27.065205Z","shell.execute_reply":"2021-07-08T04:17:27.078029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_processed.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:29.128947Z","iopub.execute_input":"2021-07-08T04:17:29.129266Z","iopub.status.idle":"2021-07-08T04:17:29.154889Z","shell.execute_reply.started":"2021-07-08T04:17:29.12924Z","shell.execute_reply":"2021-07-08T04:17:29.154228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling\n\n* We noticed that following features/columns are needed to be normalized/scaled.\n    * age\n    * trtbps\n    * chol\n    * thalachh\n    * oldpeak","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\n\ncolumns_for_scaling = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\n\ndf1_processed[columns_for_scaling] = standardScaler.fit_transform(df1_processed[columns_for_scaling])","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:33.596385Z","iopub.execute_input":"2021-07-08T04:17:33.59675Z","iopub.status.idle":"2021-07-08T04:17:33.675635Z","shell.execute_reply.started":"2021-07-08T04:17:33.596721Z","shell.execute_reply":"2021-07-08T04:17:33.674641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1_processed.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:36.287319Z","iopub.execute_input":"2021-07-08T04:17:36.287705Z","iopub.status.idle":"2021-07-08T04:17:36.313979Z","shell.execute_reply.started":"2021-07-08T04:17:36.287675Z","shell.execute_reply":"2021-07-08T04:17:36.313323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now, we can see that the features are scaled appropriately.","metadata":{}},{"cell_type":"markdown","source":"# 4. MODEL DEVELOPMENT","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n\nfrom sklearn.linear_model import LogisticRegression     # Logistic Regression\nfrom sklearn.neighbors import KNeighborsClassifier      # KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier     # Random Forest\nfrom sklearn.ensemble import GradientBoostingClassifier # GBM\nimport xgboost as xgb\nfrom xgboost import XGBClassifier                       # XGBoost\nfrom lightgbm import LGBMClassifier                     # Light GBM\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\n#from sklearn.metrics import mean_squared_error, r2_score","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:48.356658Z","iopub.execute_input":"2021-07-08T04:17:48.357202Z","iopub.status.idle":"2021-07-08T04:17:49.721229Z","shell.execute_reply.started":"2021-07-08T04:17:48.357145Z","shell.execute_reply":"2021-07-08T04:17:49.720234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's prepare Independent and Target variables ","metadata":{}},{"cell_type":"code","source":"SEED = 124\nx = df1_processed.drop(\"output\",axis=1)\ntarget = df1_processed[\"output\"]\n\nx_train,x_test,y_train,y_test = train_test_split(x,target,test_size=0.25,random_state = SEED)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:51.996942Z","iopub.execute_input":"2021-07-08T04:17:51.99729Z","iopub.status.idle":"2021-07-08T04:17:52.006535Z","shell.execute_reply.started":"2021-07-08T04:17:51.997257Z","shell.execute_reply":"2021-07-08T04:17:52.005338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:55.149297Z","iopub.execute_input":"2021-07-08T04:17:55.149654Z","iopub.status.idle":"2021-07-08T04:17:55.155412Z","shell.execute_reply.started":"2021-07-08T04:17:55.149622Z","shell.execute_reply":"2021-07-08T04:17:55.154209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:17:57.133948Z","iopub.execute_input":"2021-07-08T04:17:57.134737Z","iopub.status.idle":"2021-07-08T04:17:57.141408Z","shell.execute_reply.started":"2021-07-08T04:17:57.134689Z","shell.execute_reply":"2021-07-08T04:17:57.140315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4a. Classification Models - kNearestNeighbor","metadata":{}},{"cell_type":"code","source":"# For us, x --> Independent Feature Set, target --> Target feature\n# We will try with 10 fold Cross Validation of dataset\n\nknn_scores = []\nfor k in range(1,21): \n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    score = cross_val_score(knn_classifier,x,target,cv=10)\n    knn_scores.append(score.mean())","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:25.611341Z","iopub.execute_input":"2021-07-08T04:18:25.611679Z","iopub.status.idle":"2021-07-08T04:18:27.092418Z","shell.execute_reply.started":"2021-07-08T04:18:25.611652Z","shell.execute_reply":"2021-07-08T04:18:27.091373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([k for k in range(1, 21)], knn_scores, color = 'blue')\nfor i in range(1,21):\n    plt.text(i, round(knn_scores[i-1],3), (i, round(knn_scores[i-1],2)))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:29.985585Z","iopub.execute_input":"2021-07-08T04:18:29.985935Z","iopub.status.idle":"2021-07-08T04:18:30.250925Z","shell.execute_reply.started":"2021-07-08T04:18:29.985906Z","shell.execute_reply":"2021-07-08T04:18:30.249911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are trying to execute with the best k value found above\nknn_classifier = KNeighborsClassifier(n_neighbors = 10)\nscore = cross_val_score(knn_classifier,x,target,cv=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:34.31691Z","iopub.execute_input":"2021-07-08T04:18:34.317285Z","iopub.status.idle":"2021-07-08T04:18:34.399005Z","shell.execute_reply.started":"2021-07-08T04:18:34.317255Z","shell.execute_reply":"2021-07-08T04:18:34.398111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score.mean() ## Accuracy score from kNN","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:36.018845Z","iopub.execute_input":"2021-07-08T04:18:36.019379Z","iopub.status.idle":"2021-07-08T04:18:36.024371Z","shell.execute_reply.started":"2021-07-08T04:18:36.019345Z","shell.execute_reply":"2021-07-08T04:18:36.02356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4b. Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rf_classifier= RandomForestClassifier(n_estimators = 10)\n\nscore = cross_val_score(rf_classifier,x,target,cv=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:40.005278Z","iopub.execute_input":"2021-07-08T04:18:40.00563Z","iopub.status.idle":"2021-07-08T04:18:40.279378Z","shell.execute_reply.started":"2021-07-08T04:18:40.005601Z","shell.execute_reply":"2021-07-08T04:18:40.278598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score.mean() ## Accuracy score from RF","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:42.166589Z","iopub.execute_input":"2021-07-08T04:18:42.167047Z","iopub.status.idle":"2021-07-08T04:18:42.172121Z","shell.execute_reply.started":"2021-07-08T04:18:42.167019Z","shell.execute_reply":"2021-07-08T04:18:42.171318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4c. Light GBM Classifier","metadata":{}},{"cell_type":"code","source":"lgbm_classifier = LGBMClassifier()\n\nscore = cross_val_score(lgbm_classifier,x,target,cv=5)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:45.801307Z","iopub.execute_input":"2021-07-08T04:18:45.801764Z","iopub.status.idle":"2021-07-08T04:18:46.047288Z","shell.execute_reply.started":"2021-07-08T04:18:45.801736Z","shell.execute_reply":"2021-07-08T04:18:46.046461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score.mean() ## Accuracy score from LightGBM","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:47.141666Z","iopub.execute_input":"2021-07-08T04:18:47.143564Z","iopub.status.idle":"2021-07-08T04:18:47.149882Z","shell.execute_reply.started":"2021-07-08T04:18:47.143527Z","shell.execute_reply":"2021-07-08T04:18:47.148964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. MODEL TUNING\n\nLet's focus on the fine tuning of hyper parameters and explore which combinations works in an optimum manner.\nBased on that, we will consider those parameter values and re-execute our model and evaluate the performance.","metadata":{}},{"cell_type":"code","source":"def print_score(classifier, x_train, y_train, x_test, y_test, train=True):\n    if train:\n        pred = classifier.predict(x_train)\n        classifier_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n--------------------------------------------\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{classifier_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = classifier.predict(x_test)\n        classifier_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n---------------------------------------------\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{classifier_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:51.427507Z","iopub.execute_input":"2021-07-08T04:18:51.427835Z","iopub.status.idle":"2021-07-08T04:18:51.436265Z","shell.execute_reply.started":"2021-07-08T04:18:51.427809Z","shell.execute_reply":"2021-07-08T04:18:51.435231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:52.747949Z","iopub.execute_input":"2021-07-08T04:18:52.748477Z","iopub.status.idle":"2021-07-08T04:18:52.772509Z","shell.execute_reply.started":"2021-07-08T04:18:52.748432Z","shell.execute_reply":"2021-07-08T04:18:52.77156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:54.312395Z","iopub.execute_input":"2021-07-08T04:18:54.312947Z","iopub.status.idle":"2021-07-08T04:18:54.320419Z","shell.execute_reply.started":"2021-07-08T04:18:54.312897Z","shell.execute_reply":"2021-07-08T04:18:54.319359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.1 Hyperparameter Tuning for kNN","metadata":{}},{"cell_type":"code","source":"knn_classifier = KNeighborsClassifier(n_neighbors=20)\nknn_classifier.fit(x_train, y_train)\n\nprint_score(knn_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(knn_classifier, x_train, y_train, x_test, y_test, train=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:18:58.535584Z","iopub.execute_input":"2021-07-08T04:18:58.536279Z","iopub.status.idle":"2021-07-08T04:18:58.611907Z","shell.execute_reply.started":"2021-07-08T04:18:58.536229Z","shell.execute_reply":"2021-07-08T04:18:58.60959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = accuracy_score(y_test, knn_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, knn_classifier.predict(x_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Tuned k-Nearest Neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:19:01.895708Z","iopub.execute_input":"2021-07-08T04:19:01.896067Z","iopub.status.idle":"2021-07-08T04:19:01.940927Z","shell.execute_reply.started":"2021-07-08T04:19:01.896036Z","shell.execute_reply":"2021-07-08T04:19:01.93999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.2 Hyperparameter Tuning for Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams = {\"C\": np.logspace(-4, 4, 20),\n          \"solver\": [\"liblinear\"]}\n\nlr_classifier = LogisticRegression()\n\nlr_cv = GridSearchCV(lr_classifier, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5)\nlr_cv.fit(x_train, y_train)\nbest_params = lr_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\nlr_classifier = LogisticRegression(**best_params)\n\nlr_classifier.fit(x_train, y_train)\n\nprint_score(lr_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(lr_classifier, x_train, y_train, x_test, y_test, train=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:19:06.478849Z","iopub.execute_input":"2021-07-08T04:19:06.479178Z","iopub.status.idle":"2021-07-08T04:19:08.705216Z","shell.execute_reply.started":"2021-07-08T04:19:06.479152Z","shell.execute_reply":"2021-07-08T04:19:08.704002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = accuracy_score(y_test, lr_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, lr_classifier.predict(x_train)) * 100\n\ntuning_results_df = pd.DataFrame(data=[[\"Tuned Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\n\nz = results_df.append(tuning_results_df, ignore_index=True)\nz","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:19:12.947918Z","iopub.execute_input":"2021-07-08T04:19:12.948282Z","iopub.status.idle":"2021-07-08T04:19:12.968328Z","shell.execute_reply.started":"2021-07-08T04:19:12.948248Z","shell.execute_reply":"2021-07-08T04:19:12.967322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.3 Hyperparameter Tuning for Random Forest","metadata":{}},{"cell_type":"code","source":"n_estimators = [100]\nmax_features = ['auto', 'sqrt']\nmax_depth = [5]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nparams_rf = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nrf_classifier = RandomForestClassifier(random_state = SEED)\n\nrf_cv = GridSearchCV(rf_classifier, params_rf, scoring=\"accuracy\", cv=3, verbose=2, n_jobs=-1)\n\n\nrf_cv.fit(x_train, y_train)\nbest_params = rf_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\n\nrf_classifier = RandomForestClassifier(**best_params)\nrf_classifier.fit(x_train, y_train)\n\nprint_score(rf_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(rf_classifier, x_train, y_train, x_test, y_test, train=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:19:20.927844Z","iopub.execute_input":"2021-07-08T04:19:20.928237Z","iopub.status.idle":"2021-07-08T04:19:28.53891Z","shell.execute_reply.started":"2021-07-08T04:19:20.928185Z","shell.execute_reply":"2021-07-08T04:19:28.537999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = accuracy_score(y_test, rf_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, rf_classifier.predict(x_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Tuned Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nz = z.append(results_df, ignore_index=True)\nz","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:19:37.879828Z","iopub.execute_input":"2021-07-08T04:19:37.880161Z","iopub.status.idle":"2021-07-08T04:19:37.922306Z","shell.execute_reply.started":"2021-07-08T04:19:37.880132Z","shell.execute_reply":"2021-07-08T04:19:37.921471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.4 Hyperparameter Tuning for XGBoost","metadata":{}},{"cell_type":"code","source":"n_estimators = [100]\nmax_depth = [2, 3, 5]\nbooster = ['gbtree', 'gblinear']\nbase_score = [0.99]\nlearning_rate = [0.05]\nmin_child_weight = [1, 2, 3]\n\nparams = {\n    'n_estimators': n_estimators, 'max_depth': max_depth,\n    'learning_rate' : learning_rate, 'min_child_weight' : min_child_weight, \n    'booster' : booster, 'base_score' : base_score\n                      }\n\nxgb_classifier = XGBClassifier()\n\nxgb_cv = GridSearchCV(xgb_classifier, params, cv=3, scoring = 'accuracy',n_jobs =-1, verbose=1)\n\n\nxgb_cv.fit(x_train, y_train)\nbest_params = xgb_cv.best_params_\nprint(f\"Best paramters: {best_params}\")\n\nxgb_classifier = XGBClassifier(**best_params)\nxgb_classifier.fit(x_train, y_train)\n\nprint_score(xgb_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(xgb_classifier, x_train, y_train, x_test, y_test, train=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:19:44.580664Z","iopub.execute_input":"2021-07-08T04:19:44.581016Z","iopub.status.idle":"2021-07-08T04:22:56.409683Z","shell.execute_reply.started":"2021-07-08T04:19:44.580977Z","shell.execute_reply":"2021-07-08T04:22:56.408564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_score = accuracy_score(y_test, xgb_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, xgb_classifier.predict(x_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Tuned XGBoost Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nz = z.append(results_df, ignore_index=True)\nz","metadata":{"execution":{"iopub.status.busy":"2021-07-08T04:23:09.046853Z","iopub.execute_input":"2021-07-08T04:23:09.047248Z","iopub.status.idle":"2021-07-08T04:23:09.072535Z","shell.execute_reply.started":"2021-07-08T04:23:09.047216Z","shell.execute_reply":"2021-07-08T04:23:09.071571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The tuned model with Training and Testing Accuracy percetage points are captured above.\n\n#### We will continue to explore further with multiple experiments.","metadata":{}},{"cell_type":"markdown","source":"# 6. Conclusion / Interpretations:\n\n* Since our objective is to predict whether a person is prone to heart attack or not based on the dataset and information available, we have approached it accordingly and explored with initial data analysis followed by feature engineering and few methods.\n* We analyzed few algorithms and compared with their accuracy percentage points. Both training and testing p.p are compared to just get a feel of how they are performing (though we will be only interested in the testing accuracy p.p.)\n* We will further experiment more with additional feature engineering and models to be analyzed with various options to see what works better and why.\n* In any business problem solving, we will have to see data and context/need and then only can state which algorithm will perform better given the scenario. Time is also important and we will have to consider trade off between time and optimum solution accordingly.\n* More effort will always be towards EDA and Feature Engineering which are important.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}