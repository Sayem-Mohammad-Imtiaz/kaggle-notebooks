{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SHAP Values\n\nYou've seen (and used) techniques to extract general insights from a machine learning model. But what if you want to break down how the model works for an individual prediction?\n\nSHAP Values (an acronym from **SHapley Additive exPlanations**) break down a prediction to show the impact of each feature. Where could you use this?\n\n- A model says a bank shouldn't loan someone money, and the bank is legally required to explain the basis for each loan rejection\n- A healthcare provider wants to identify what factors are driving each patient's risk of some disease so they can directly address those risk factors with targeted health interventions\n\n# How They Work\nSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\n\nWe'll use the soccer/football example to predict whether a team would have a player win the *Man of the Game* award.\n\nWe could ask:\n- How much was a prediction driven by the fact that the team scored 3 goals?\n    \nBut it's easier to give a concrete, numeric answer if we restate this as:\n- How much was a prediction driven by the fact that the team scored 3 goals, **instead of some baseline number of goals.**\n\nOf course, each team has many features. So if we answer this question for `number of goals`, we could repeat the process for all other features.\n\nSHAP values do this in a way that guarantees a nice property. When we make a prediction.\n\n```sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values```\n\nThat is, the SHAP values of all features sum up to explain why my prediction was different from the baseline. This allows us to decompose a prediction in a graph like this:\n\n![Imgur](https://i.imgur.com/JVD2U7k.png)\n\n*If you want a larger view of this graph, [here is a link](https://i.imgur.com/JVD2U7k.png)*","metadata":{"_uuid":"1ec9909dc4a7de166acec00d3bb42104457f1881"}},{"cell_type":"markdown","source":"\nHow do you interpret this?\n\nWe predicted 0.7, whereas the base_value is 0.4979.  Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect.  Feature values decreasing the prediction are in blue.  The biggest impact comes from `Goal Scored` being 2.  Though the ball possession value has a meaningful effect decreasing the prediction. If you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output\n\nThere is some complexity to the technique, to ensure that the baseline plus the sum of individual effects adds up to the prediction (which isn't as straightforward as it sounds.) We won't go into that detail here, since it isn't critical for using the technique. [This blog post](https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d) has a longer theoretical explanation.\n\n# Code to Calculate SHAP Values\nWe calculate SHAP values using the wonderful [Shap](https://github.com/slundberg/shap) library.","metadata":{"_uuid":"cfb16f6b30cee9f6f0977efa86479075f83059f3"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","metadata":{"_uuid":"5cdc61edd0b6964731b04e84eabf899f1355e280","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap  # package used to calculate the Shap values.\n\nrow_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","metadata":{"_uuid":"296bd5153772faea1c1b34ef03eb86606226cf3c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shap_values object above is a list with two arrays. The first array in the list is the SHAP values for a negative outcome (don't win the award.) . The second array is the list of SHAP values for the positive outcome, which is how we usually think about predictions. It's cumbersome to review a raw array, but the shap package has a nice way to visualize the results.  Before doing that, here are the predicted probabilities of each outcome","metadata":{"_uuid":"ba168eb00fdc893850a94204122f2e9a973cc311"}},{"cell_type":"code","source":"my_model.predict_proba(data_for_prediction_array)","metadata":{"_uuid":"1a4f7bb273aa9126df93da8a963979949057bd1c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The team is 70% likely to have a player win the award. Now we visualize the SHAP values.","metadata":{"_uuid":"ec8b25e417059f9c42942a3328e462e2c6d05de9"}},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","metadata":{"_uuid":"1d5e4c7d3d76dd45dc2149db3e862d4151d0167f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in `shap.TreeExplainer(my_model)`.  But the SHAP package has explainers for every type of model.\n\n- `shap.DeepExplainer` works with Deep Learning models. \n- `shap.KernelExplainer` works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n\nHere is an example using KernelExplainer to get the same results you saw above.","metadata":{"_uuid":"532b1df7a4efc648b363ec8784c792df5c75718f"}},{"cell_type":"code","source":"# use Kernel SHAP to explain test set predictions\nk_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\nk_shap_values = k_explainer.shap_values(data_for_prediction)\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","metadata":{"_uuid":"731e78387e2cecfedf0a2daed32f039ac08de7cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exercises\n\nAt this point, you have enough tools to put together compelling solutions to real-world problems.\n\n**Run the following cell to set up our feedback system.**","metadata":{"_uuid":"acee780dfd425967fe6bec4de49132833b7c620c"}},{"cell_type":"code","source":"import sys\nsys.path.append('../input/ml-insights-tools')\nfrom ex4 import *\nprint(\"Setup Complete\")","metadata":{"_uuid":"9ed57a52fe847cea321a3dcaa3817e49f5566149","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Scenario\nA hospital has struggled with \"readmissions,\" where they release a patient before the patient has recovered enough, and the patient returns with health complications. The hospital wants your help identifying patients at highest risk of being readmitted. Our model will highlight issues the doctors should consider when releasing a patient. Here is a list of columns in the data:\n","metadata":{"_uuid":"72c1f96b853ed387e24cb573c395699d1c063593"}},{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('../input/hospital-readmissions/train.csv')\ndata.columns","metadata":{"_uuid":"b6631ea8d8f3067f39ef5367c9d37ae7d2fe6576","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are some quick hints at interpreting the field names:\n\n- Your prediction target is `readmitted`\n- Columns with the word `diag` indicate the diagnostic code of the illness or illnesses the patient was admitted with. For example, `diag_1_428` means the doctor said their first illness diagnosis is number \"428\".  What illness does 428 correspond to? You could look it up in a codebook, but without more medical background it wouldn't mean anything to you anyway.\n- A column names like `glimepiride_No` mean the patient did not have the medicine `glimepiride`. If this feature had a value of False, then the patient did take the drug `glimepiride`\n- Features whose names begin with `medical_specialty` describe the specialty of the doctor seeing the patient. The values in these fields are all `True` or `False`.\n","metadata":{"_uuid":"c088c1c04d3f0317b0b2f6be61a68851b994b6c9"}},{"cell_type":"markdown","source":"## Your Code Library\nAs you write code to work through this scenario, these code snippets from previous tutorials may be useful. You'll still need to modify them, but we've copied them here to save you from having to look them up.\n\n**Calculate and show permutation importance:**\n```\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())\n```\n\n**Calculate and show partial dependence plot:**\n```\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=my_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()\n```\n\n**Calculate and show Shap Values for One Prediction:**\n```\nimport shap  # package used to calculate Shap values\n\ndata_for_prediction = val_X.iloc[0,:]  # use 1 row of data here. Could use multiple rows if desired\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\nshap_values = explainer.shap_values(data_for_prediction)\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0], data_for_prediction)\n```\n\n## Step 1:\nYou have built a simple model, but the doctors say they don't know how to evaluate a model, and they'd like you to show them some evidence the model is doing something in line with their medical intuition. Create any graphics or tables that will show them a quick overview of what the model is doing? They are very busy. So they want you to condense your model overview into just 1 or 2 graphics, rather than a long string of graphics.","metadata":{"_uuid":"e2be06e0266b2a2d1339f36af23bfaf06dbc5f32"}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/hospital-readmissions/train.csv')\n\ny = data.readmitted\nbase_features = [c for c in data.columns if c != \"readmitted\"]\nX = data[base_features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(train_X, train_y)","metadata":{"_uuid":"b14a9667ad6b84c647f0ef4144c8a309dcc35951","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now use the following cell to create the materials for the doctors.","metadata":{"_uuid":"26e690fdf73206744e9852d800f9dc5a51eb0458"}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","metadata":{"_uuid":"9f9cb04f7d63144b301a84405f697d5af4b815bd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For our idea of what to show, run the cell below.  ","metadata":{"_uuid":"6977f6387113e451e301e192270ae465fda24d66"}},{"cell_type":"code","source":"#q_1.solution()","metadata":{"_uuid":"75ca5c423dc28d25155c4b2f9b07f972cb948770","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you'd like to discuss your approach or see what others have done, we have a discussion forum [here](https://www.kaggle.com/learn-forum/66267#latest-390149).\n\n","metadata":{"_uuid":"72868ba6579e5db147bf8aec3c755daa9d5c69ab"}},{"cell_type":"markdown","source":"## Step 2\n\nIt appears `number_inpatient` is a really important feature. The doctors would like to know more about that. Create a graph for them that shows how `num_inpatient` affects the model's predictions.","metadata":{"_uuid":"52289166dc40cc0fe592ce01d5b1167b1581d949"}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=my_model, dataset=val_X, model_features=base_features, feature='number_inpatient')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'number_inpatient')\nplt.show()","metadata":{"_uuid":"1361d41757902d5768698b7c2459473eca849ac0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For our solution, uncomment the line below.","metadata":{"_uuid":"2c9c593158c3537bc1ddc8ce1dfcfef1b124a16c"}},{"cell_type":"code","source":"#q_2.solution()","metadata":{"_uuid":"c3091a6dd0bad4b454c54066aef645794c9d82a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3\n\nThe doctors think it's a good sign that increasing the number of inpatient procedures leads to increased predictions.  But they can't tell from this plot whether that change in the plot is big or small. They'd like you to create something similar for `time_in_hospital` to see how that compares.","metadata":{"_uuid":"4ba5bfcbc8442ca29db06f9a0c77cced9d0f4225"}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=my_model, dataset=val_X, model_features=base_features, feature='time_in_hospital')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'number_inpatient')\nplt.show()","metadata":{"_uuid":"6a58de6492d05ea652eaf653114b566477c42487","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment the relevant line below to see one solution.","metadata":{"_uuid":"782faa61247181917a625fa325ceb9a13019a5e6"}},{"cell_type":"code","source":"#q_3.solution()","metadata":{"_uuid":"cff6cdd7fe649633e04c02a16b1c631cc04ca6de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4\n\nWoah!  It seems like `time_in_hospital` doesn't matter at all.  The difference between the lowest value on the partial dependence plot and the highest value is about 5%. If that is what your model concluded, the doctors will believe it. But it seems so low. Could  the data be wrong, or is your model doing something more complex than they expect? \n\nThey'd like you to show them the raw readmission rate for each value of `time_in_hospital` to see how it compares to the partial dependence plot.\n\n- Make that plot. \n- Are the results similar or different?","metadata":{"_uuid":"b34a99a16fd0df0bb4cc841622bede9c383de36f"}},{"cell_type":"code","source":"# A simple pandas groupby showing the average readmission rate for each time_in_hospital.\n# Do concat to keep validation data separate, rather than using all original data\nall_train = pd.concat([train_X, train_y], axis=1)\n\nall_train.groupby(['time_in_hospital']).mean().readmitted.plot()\nplt.show()","metadata":{"_uuid":"0abd881dbf0b959b0aa7a9b9f5d206b17c4c9434","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a hint or solution, uncomment the relevant line below.","metadata":{"_uuid":"897ec6df9eebd3dc8e10a931458196b8c7325941"}},{"cell_type":"code","source":"#q_4.hint()\n#q_4.solution()","metadata":{"_uuid":"c7f98c899f5b066ada675979e25caa8f2fddc1ec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5\nNow the doctors are convinced you have the right data, and the model overview looked reasonable.  It's time to turn this into a finished product they can use. Specifically, the hospital wants you to create a function `patient_risk_factors` that does the following\n- Takes a single row with patient data (of the same format you as your raw data)\n- Creates a visualization showing what features of that patient increased their risk of readmission, what features decreased it, and how much those features mattered.\n\nIt's not important to show every feature with every miniscule impact on the readmission risk.  It's fine to focus on only the most important features for that patient.","metadata":{"_uuid":"d1593f15b60d2e020e768c2fcff046230c202ef7"}},{"cell_type":"code","source":"# Use SHAP values to show the effect of each feature of a given patient\n\nimport shap  # package used to calculate Shap values\n\nsample_data_for_prediction = val_X.iloc[0].astype(float)  # to test function\n\ndef patient_risk_factors(model, patient_data):\n    # Create object that can calculate shap values\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(patient_data)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], patient_data)\n\npatient_risk_factors(my_model,sample_data_for_prediction)","metadata":{"_uuid":"76bc0dadfcfd1301362f62769e82e8e7b0e8b73b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment the relevant line below for a hint or solution.","metadata":{"_uuid":"8786a6219d42d065ffce080884752788152f680a"}},{"cell_type":"code","source":"#q_5.hint()\n#q_5.solution()","metadata":{"_uuid":"68474bd5aee6e6230d759d901016d3d12bdf6139","trusted":true},"execution_count":null,"outputs":[]}]}