{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Let's import all the necessary Libraries required for Data Wrangling, Analysis and Classification**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\n\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pprint import pprint\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **IMPROTING DATASET - PIMA INDIANS DIABETES DATASET**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **DATA WRANGLING**","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding Null/NaN values...","metadata":{}},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Here we can see that there is no null values but many columns in the data have zero values indicating missing values.****","metadata":{}},{"cell_type":"code","source":"df.corr()['Outcome'].sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ****The correlation also tells us to find if any of the attributes contribute positively or negatively towards the diabetes. It utilizes the Pearson Coefficient.****","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), cmap=\"Dark2\", annot= True,)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram Plot\n\n**Histogram gives us the frequency of occurrence per value in the dataset. The distribution\ndoes not occur as a bell curve in all attributes therefore, the signal is not normally\ndistributed.**\n\n**The Histogram appears Skewed and major values of 'Insulin', 'BMI', 'Glucose','Blood Pressure ', 'Skin thickness' appears zero is visualized here**","metadata":{}},{"cell_type":"code","source":"his = df.hist(figsize = (20,20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IDENTIFYING AND REPLACING THE NULL VALUES","metadata":{}},{"cell_type":"markdown","source":"**Replacing the zero values to NaN and evaluating the null values in the attributes obtained**","metadata":{}},{"cell_type":"code","source":"df1 = df.copy(deep = True)\ndf1[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df1[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\n## showing the count of Nans\nprint(df1.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## IMPUTATION\n\n**Imputing the Median values in the place of null values to obatin proper value distribution. Imputing can be done in both mean and median values. But since the variable is skewed, the mean is biased by the values at the far end of the distribution. Therefore, the median is a better representation of the majority of the values in the variable**","metadata":{}},{"cell_type":"code","source":"df1['Glucose'].fillna(df1['Glucose'].median(), inplace = True)\ndf1['BloodPressure'].fillna(df1['BloodPressure'].median(), inplace = True)\ndf1['SkinThickness'].fillna(df1['SkinThickness'].median(), inplace = True)\ndf1['Insulin'].fillna(df1['Insulin'].median(), inplace = True)\ndf1['BMI'].fillna(df1['BMI'].median(), inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Again plotting the Histogram and Correlation plot after the imputation of data to see the variation**","metadata":{}},{"cell_type":"code","source":"his1 = df1.hist(figsize = (20,20))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df1.corr(), cmap=\"Dark2\", annot= True,)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.corr()['Outcome'].sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"### DATA ANALYSIS AND VISUALIZATION\n\nPairplot helps us to identify the relation between the attributes how one of them influces the other. ","metadata":{}},{"cell_type":"code","source":"plt.style.use('seaborn-dark')\nsns.pairplot(df1,hue='Outcome', palette='husl', diag_kind=\"hist\");\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATA VIZUALISATION AND INTERPRETATION\n\n**First we find the number of diabetic and non-diabetic patients. Followed by range in which diabetes occur e.g. what are possible age group or glucose level are more susceptible to diabetes**\n","metadata":{}},{"cell_type":"code","source":"df1['Outcome'].value_counts().to_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsns.distplot(df1[df1['Outcome'] == 0][\"Glucose\"],color='purple' ) # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"Glucose\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df1[df1['Outcome'] == 0][\"BMI\"], color='purple') # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"BMI\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df1[df1['Outcome'] == 0][\"Insulin\"], color='purple') # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"Insulin\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df1[df1['Outcome'] == 0][\"Age\"], color='purple') # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"Age\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df1[df1['Outcome'] == 0][\"Pregnancies\"], color='purple') # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"Pregnancies\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df1[df1['Outcome'] == 0][\"BloodPressure\"], color='purple') # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"BloodPressure\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df1[df1['Outcome'] == 0][\"SkinThickness\"], color='purple') # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"SkinThickness\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df1[df1['Outcome'] == 0][\"DiabetesPedigreeFunction\"], color='purple') # Healthy - purple\nsns.distplot(df1[df1['Outcome'] == 1][\"DiabetesPedigreeFunction\"], color='yellow') # Diabetic - yellow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interpretation of the Data Analysis\n\n**Glucose**: Diabetic range occurs when the glucose levels at 75 mg/dl and reaches a peak at ~ 125mg/dl the data sampled. And a standard high levels of 145mg/dl glucose is observed in diabetes patients compared to the normal. Since the glucose represents the plasma glucose concentration a 2 hours in an oral glucose tolerance test. Values less than 140mg/dl are considered normal. Glucose range over 145mg/dl in the popluation sampled indicates the presence of diabetes.\n\n**BMI**: The normal BMI range for an healthy adult is between 18.5 to 24.9. Keeping that in mind we can see that the Diabetic population has BMI range from 22 to 50, Highest number no people having ~32 BMI (30-40 BMI has higher diabetes). hence we can conclude that Obesity being a major factor.\n\n**Insulin**: The insulin value given in dataset is 2-Hour serum insulin (mu U/ml). We can see that even though the healthy individuals as well as diabetic both have crossover and have sharp decrease after 200 mu U/ml, Diabetic patient have a sharp peak between (~ 160 -180)mu U/ml range.\n\n**Age**: The ages between 40 to 45 years have higher chances of diabetes even though the curve starts from the age of 20 and slowly drops after the 46-47 years of age. This indicates the sedentary lifestyle and well as the living conditions are a major reason foe diabetes more than the age attribute.\n\n**Pregnancies**: Even though women having no kids equally had a higher chance of diabetes, many of them remained healthy also. So we can conclude that pregnancy may not be a pri e factor to consider.\n\n**Blood Pressure**: The peak values for the diabetic patients occurs at ~72 to 76 mm Hg and the range of diabetic range falls between 62 to 95 mm Hg while healthy people have till 80 mmHg \n\n**Skin Thickness aqnd Diabetes Pedigree function (DPF)**: We can see that the healthy and diabetic samples have similar skin thickness (Triceps skin fold thickness (mm)) range the highest being 32mm and similarly the overall mean curve of the DPF is higher for healthy compared to the diabetic. Thus these two factors dont play a greater role in predicting diabetes.","metadata":{}},{"cell_type":"markdown","source":"## MACHINE LEARNING MODELS USING HYPER PARAMETER TUNING\n\n**Hyper parameter tuning is done to increase accuracy with the limited data. The following classifiers are tunes based on the estimators obtained.**\n\n1. K- NEAREST NEIGHBOR\n2. LOGISTIC REGRESSION\n3. RANDOM FOREST\n\n\nWe are now trying to predict and find accuracy for all Data Attributes \n","metadata":{}},{"cell_type":"markdown","source":"## ACURACY AND OTHER METRICS TABLE \n\n**Accuracy:**  Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. Accuracy is a great measure but only for symmetric datasets where values of false positive and false negatives are almost same. Therefore, other parameters are estimated to evaluate the performance of your model.\n\n**Precision:** Precision is the ratio of correctly predicted positive observations to the total predicted positive observations\n\n**Recall:** Recall is the ratio of correctly predicted positive observations to the all observations in actual class \n\n**F1 Score:** F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if the dataset has an uneven class distribution. Hence F1 score is calculated here\n\n**Jaccard Index:** The Jaccard similarity index compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0% to 100%. \n\n**Cohen’s Kappa:** Cohen’s kappa statistic measures interrater reliability (sometimes called interobserver agreement). Interrater reliability, or precision, happens when your data raters (or collectors) give the same score to the same data item.\n0 = agreement equivalent to chance.\n0.1 – 0.20 = slight agreement.\n0.21 – 0.40 = fair agreement.\n0.41 – 0.60 = moderate agreement.\n0.61 – 0.80 = substantial agreement.\n0.81 – 0.99 = near perfect agreement\n1 = perfect agreement.\n\n**ROC - AUC:** AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. \n\n**Confusion Matrix:** A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class.\n\n**LogLoss:** Logarithmic loss measures the performance of a classification model where the prediction input is a probability value between 0 and 1. A perfect model would have a log loss of 0. \n","metadata":{}},{"cell_type":"code","source":"x = df1.iloc[:, :-1].values\ny = df1.iloc[:, -1].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into training set and test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 4)\n# Scaling to bring values to the same range\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\nprint ('Train set:', x_train.shape,  y_train.shape)\nprint ('Test set:', x_test.shape,  y_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################### KNN MODEL ############################################################\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    neigh = KNeighborsClassifier(n_neighbors = n).fit(x_train,y_train)\n    yhat = neigh.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1] = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1,Ks),mean_acc,'m')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build model with best accuracy, K=6\nknn_model = KNeighborsClassifier(n_neighbors=6).fit(x_train, y_train)\nyhat = knn_model.predict(x_test)\nmean = metrics.accuracy_score(y_test, yhat)\nmean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Receiving Operating Characteristic Curve\n    # Create true and false positive rates\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, yhat)\nprint('roc_auc_score: ', roc_auc_score(y_test, yhat))\n# Plot ROC curves\nplt.subplots(1, figsize=(10,10))\nplt.title('Receiver Operating Characteristic- KNN Classifier')\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix-KNN Classifier\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(knn_model, x_test, y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################################## LOGISTIC REGRESSION ##################################################\n\nsolvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\nregularisations = [1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001]\nsolver_mean_acc = {}\nsolver_std_acc = {}\nsolver_best_reg = {}\nfor solver in solvers:\n    best_mean = 0\n    best_std = 0\n    best_reg = 0\n    for reg in regularisations:\n        lr = LogisticRegression(C=reg, solver=solver).fit(x_train, y_train)\n        yhat = lr.predict(x_test)\n        mean = metrics.accuracy_score(y_test, yhat)\n        std = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n        if mean > best_mean:\n            best_mean = mean\n            best_std = std\n            best_reg = reg\n    solver_mean_acc[solver] = best_mean\n    solver_std_acc[solver] = best_std\n    solver_best_reg[solver] = best_reg\n\nsolver_mean_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"solver_best_reg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model = LogisticRegression(C=1, solver='liblinear', max_iter=200).fit(x_train, y_train)\nyhat = lr_model.predict(x_test)\nmean = metrics.accuracy_score(y_test, yhat)\nmean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Receiving Operating Characteristic Curve\n    # Create true and false positive rates\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, yhat)\nprint('roc_auc_score: ', roc_auc_score(y_test, yhat))\n# Plot ROC curves\nplt.subplots(1, figsize=(10,10))\nplt.title('Receiver Operating Characteristic- Logistic Regression')\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix- Logistic Regression\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(lr_model, x_test, y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################################### RANDOM FOREST ##################################################################\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, x_test, y_test):\n    yhat = model.predict(x_test)\n   # accuracy: (tp + tn) / (p + n)\n    accuracy = accuracy_score(y_test, yhat.round())\n    print('Accuracy: %f' % accuracy)\n    # precision tp / (tp + fp)\n    precision = precision_score(y_test, yhat.round())\n    print('Precision: %f' % precision)\n    # recall: tp / (tp + fn)\n    recall = recall_score(y_test, yhat.round())\n    print('Recall: %f' % recall)\n    # f1: 2 tp / (2 tp + fp + fn)\n    f1 = f1_score(y_test, yhat.round(),'weighted')\n    print('F1 score: %f' % f1)\n    # Jaccard Index\n    jaccard=jaccard_score(y_test, yhat.round(),'weighted')\n    print('Jaccard: %f' % jaccard)\n    # kappa\n    kappa = cohen_kappa_score(y_test, yhat.round())\n    print('Cohens kappa: %f' % kappa)\n    # ROC AUC\n    auc = roc_auc_score(y_test, yhat.round())\n    print('ROC AUC: %f' % auc)\n    # confusion matrix\n    matrix = confusion_matrix(y_test, yhat.round())\n    print(matrix)\n    \n    return accuracy,precision,recall,f1,jaccard,kappa,auc,matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Receiving Operating Characteristic Curve\n    # Create true and false positive rates\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, yhat)\nprint('roc_auc_score: ', roc_auc_score(y_test, yhat))\n# Plot ROC curves\nplt.subplots(1, figsize=(10,10))\nplt.title('Receiver Operating Characteristic-Random Forest')\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\ntitles_options = [(\"Confusion matrix- Random Forest\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(rf_random, x_test, y_test,\n                                 cmap=plt.cm.Blues)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.DataFrame(index=['KNN','Logistic Regression','Random Forest', ], \n                  columns=['Accuracy','Precision','Recall','Kappa','Jaccard', 'F1-score', 'ROC','Confusion Matrix', 'LogLoss'])\n\n# -------------------------------------------TABLE CREATION--------------------------------------------------------\n\n# -------------------------------------------------KNN----------------------------------------------------------------\nyhat = knn_model.predict(x_test)\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat.round())\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat.round())\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat.round())\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat.round(),'weighted')\nprint('F1 score: %f' % f1)\n# Jaccard Index\njaccard=jaccard_score(y_test, yhat.round(),'weighted')\nprint('Jaccard: %f' % jaccard)\n# kappa\nkappa = cohen_kappa_score(y_test, yhat.round())\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat.round())\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat.round())\nprint(matrix)\n\ndf2.loc['KNN'] = [accuracy, precision, recall, kappa, jaccard, f1, auc, matrix, np.nan]\n\n\n\n# ----------------------------------------------------LOGISTIC REGRESSION ---------------------------------------------------------\n\nyhat = lr_model.predict(x_test)\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat.round())\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat.round())\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat.round())\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat.round(),'weighted')\nprint('F1 score: %f' % f1)\n# Jaccard Index\njaccard=jaccard_score(y_test, yhat.round(),'weighted')\nprint('Jaccard: %f' % jaccard)\n# kappa\nkappa = cohen_kappa_score(y_test, yhat.round())\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat.round())\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat.round())\nprint(matrix)\nyhat_prob = lr_model.predict_proba(x_test)\nll = log_loss(y_test, yhat_prob)\ndf2.loc['Logistic Regression'] = [accuracy, precision, recall, kappa, jaccard, f1, auc, matrix,ll]\n\n# ----------------------------------------------------- RANDOM FOREST ---------------------------------------------------------\nyhat = rf_random.predict(x_test)\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat.round())\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat.round())\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat.round())\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat.round(),'weighted')\nprint('F1 score: %f' % f1)\n# Jaccard Index\njaccard=jaccard_score(y_test, yhat.round(),'weighted')\nprint('Jaccard: %f' % jaccard)\n# kappa\nkappa = cohen_kappa_score(y_test, yhat.round())\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat.round())\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat.round())\nprint(matrix)\ndf2.loc['Random Forest'] = [accuracy, precision, recall, kappa, jaccard, f1, auc, matrix,np.nan]\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ML MODELS FOR SELECTED ATTRIBUTES\n\nHere we have excluded the attributes Age, Pregnancy, Diabetes Pedigree Function and Skin Thickness as these do not contribute much to the detecting Diabetes from our analysis.\n\nLet's see how the accuracy varies from the previous models.","metadata":{}},{"cell_type":"code","source":"data = df1.copy(deep = True)\ndata.drop(data.columns[[ 0 , 3 , 6 , 7 ]], axis = 1 , inplace = True )\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = data.iloc[:, :-1].values\ny1 = data.iloc[:, -1].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split dataset into training set and test set\nx_train, x_test, y_train, y_test = train_test_split(x1, y1, test_size=0.3, random_state = 4)\n# Scaling to bring values to the same range\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\nprint ('Train set:', x_train.shape,  y_train.shape)\nprint ('Test set:', x_test.shape,  y_test.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################### KNN MODEL ############################################################\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    neigh = KNeighborsClassifier(n_neighbors = n).fit(x_train,y_train)\n    yhat = neigh.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1] = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nmean_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1,Ks),mean_acc,'m')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build model with best accuracy, K=6\nknn_model = KNeighborsClassifier(n_neighbors=8).fit(x_train, y_train)\nyhat = knn_model.predict(x_test)\nmean = metrics.accuracy_score(y_test, yhat)\nmean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################################## LOGISTIC REGRESSION ##################################################\n\nsolvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\nregularisations = [1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001]\nsolver_mean_acc = {}\nsolver_std_acc = {}\nsolver_best_reg = {}\nfor solver in solvers:\n    best_mean = 0\n    best_std = 0\n    best_reg = 0\n    for reg in regularisations:\n        lr = LogisticRegression(C=reg, solver=solver).fit(x_train, y_train)\n        yhat = lr.predict(x_test)\n        mean = metrics.accuracy_score(y_test, yhat)\n        std = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n        if mean > best_mean:\n            best_mean = mean\n            best_std = std\n            best_reg = reg\n    solver_mean_acc[solver] = best_mean\n    solver_std_acc[solver] = best_std\n    solver_best_reg[solver] = best_reg\n\nsolver_mean_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"solver_best_reg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model = LogisticRegression(C=0.3, solver='saga', max_iter=200).fit(x_train, y_train)\nyhat = lr_model.predict(x_test)\nmean = metrics.accuracy_score(y_test, yhat)\nmean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################################### RANDOM FOREST ##################################################################\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, x_test, y_test):\n    yhat = model.predict(x_test)\n   # accuracy: (tp + tn) / (p + n)\n    accuracy = accuracy_score(y_test, yhat.round())\n    print('Accuracy: %f' % accuracy)\n    # precision tp / (tp + fp)\n    precision = precision_score(y_test, yhat.round())\n    print('Precision: %f' % precision)\n    # recall: tp / (tp + fn)\n    recall = recall_score(y_test, yhat.round())\n    print('Recall: %f' % recall)\n    # f1: 2 tp / (2 tp + fp + fn)\n    f1 = f1_score(y_test, yhat.round(),'weighted')\n    print('F1 score: %f' % f1)\n    # Jaccard Index\n    jaccard=jaccard_score(y_test, yhat.round(),'weighted')\n    print('Jaccard: %f' % jaccard)\n    # kappa\n    kappa = cohen_kappa_score(y_test, yhat.round())\n    print('Cohens kappa: %f' % kappa)\n    # ROC AUC\n    auc = roc_auc_score(y_test, yhat.round())\n    print('ROC AUC: %f' % auc)\n    # confusion matrix\n    matrix = confusion_matrix(y_test, yhat.round())\n    print(matrix)\n    \n    return accuracy,precision,recall,f1,jaccard,kappa,auc,matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = pd.DataFrame(index=['KNN','Logistic Regression','Random Forest', ], \n                  columns=['Accuracy','Precision','Recall','Kappa','Jaccard', 'F1-score', 'ROC','Confusion Matrix', 'LogLoss'])\n\n# -------------------------------------------TABLE CREATION--------------------------------------------------------\n\n# -------------------------------------------------KNN----------------------------------------------------------------\nyhat = knn_model.predict(x_test)\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat.round())\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat.round())\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat.round())\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat.round(),'weighted')\nprint('F1 score: %f' % f1)\n# Jaccard Index\njaccard=jaccard_score(y_test, yhat.round(),'weighted')\nprint('Jaccard: %f' % jaccard)\n# kappa\nkappa = cohen_kappa_score(y_test, yhat.round())\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat.round())\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat.round())\nprint(matrix)\n\ndf3.loc['KNN'] = [accuracy, precision, recall, kappa, jaccard, f1, auc, matrix, np.nan]\n\n\n\n# ----------------------------------------------------LOGISTIC REGRESSION ---------------------------------------------------------\n\nyhat = lr_model.predict(x_test)\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat.round())\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat.round())\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat.round())\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat.round(),'weighted')\nprint('F1 score: %f' % f1)\n# Jaccard Index\njaccard=jaccard_score(y_test, yhat.round(),'weighted')\nprint('Jaccard: %f' % jaccard)\n# kappa\nkappa = cohen_kappa_score(y_test, yhat.round())\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat.round())\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat.round())\nprint(matrix)\nyhat_prob = lr_model.predict_proba(x_test)\nll = log_loss(y_test, yhat_prob)\ndf3.loc['Logistic Regression'] = [accuracy, precision, recall, kappa, jaccard, f1, auc, matrix,ll]\n\n# ----------------------------------------------------- RANDOM FOREST ---------------------------------------------------------\nyhat = rf_random.predict(x_test)\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, yhat.round())\nprint('Accuracy: %f' % accuracy)\n# precision tp / (tp + fp)\nprecision = precision_score(y_test, yhat.round())\nprint('Precision: %f' % precision)\n# recall: tp / (tp + fn)\nrecall = recall_score(y_test, yhat.round())\nprint('Recall: %f' % recall)\n# f1: 2 tp / (2 tp + fp + fn)\nf1 = f1_score(y_test, yhat.round(),'weighted')\nprint('F1 score: %f' % f1)\n# Jaccard Index\njaccard=jaccard_score(y_test, yhat.round(),'weighted')\nprint('Jaccard: %f' % jaccard)\n# kappa\nkappa = cohen_kappa_score(y_test, yhat.round())\nprint('Cohens kappa: %f' % kappa)\n# ROC AUC\nauc = roc_auc_score(y_test, yhat.round())\nprint('ROC AUC: %f' % auc)\n# confusion matrix\nmatrix = confusion_matrix(y_test, yhat.round())\nprint(matrix)\ndf3.loc['Random Forest'] = [accuracy, precision, recall, kappa, jaccard, f1, auc, matrix,np.nan]\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here we can see that the accuracy difference between all the attributes and the ones with the selected attributes as displayed in the table shown in df3 and df2**","metadata":{}}]}