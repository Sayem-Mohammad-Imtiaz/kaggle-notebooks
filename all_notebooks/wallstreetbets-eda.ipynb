{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nimport matplotlib.pyplot as plt\nimport multiprocessing\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models import Word2Vec\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom yellowbrick.text import freqdist\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/reddit-wallstreetsbets-posts/reddit_wsb.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Title is mentioning one of the following stocks:"},{"metadata":{"trusted":true},"cell_type":"code","source":"stocks = ['GME', 'AMC', 'NOC', 'BB', 'TR', 'BBW', 'KOSS']\n\nfor stock in stocks:\n    name = f'is_{stock.lower()}'\n    df[name] = np.where(df['title'].str.contains(stock, case=False), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['timestamp'] = pd.to_datetime(df['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['date'] = df['timestamp'].dt.date\ndf['hour'] = df['timestamp'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stocks_bydate = df.groupby('date').agg({'is_gme':'sum',\n                        'is_amc':'sum',\n                        'is_noc':'sum',\n                        'is_bb':'sum',\n                        'is_koss':'sum',\n                        'is_tr':'sum',\n                       }).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=stocks_bydate['date'].values, y=stocks_bydate['is_gme'].values,\n                    mode='lines+markers',\n                    name='GME'))\n\nfig.add_trace(go.Scatter(x=stocks_bydate['date'].values, y=stocks_bydate['is_amc'].values,\n                    mode='lines+markers',\n                    name='AMC'))\n\nfig.add_trace(go.Scatter(x=stocks_bydate['date'].values, y=stocks_bydate['is_bb'].values,\n                    mode='lines+markers',\n                    name='BB'))\n\nfig.add_trace(go.Scatter(x=stocks_bydate['date'].values, y=stocks_bydate['is_tr'].values,\n                    mode='lines+markers',\n                    name='TR'))\n\nfig.update_layout(\n    title='Number of Mentions in Post Title per Day',\n    xaxis_tickfont_size=14,\n    yaxis=dict(\n        title='Number of Posts',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hourly Mentions in title during Jan 29th - February 1st Peak"},{"metadata":{"trusted":true},"cell_type":"code","source":"at_peak = df[(df['date']<=pd.to_datetime('2021-02-01'))].copy()\nat_peak.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"atpeak_byhour = at_peak.groupby('hour').agg({'is_gme':'sum',\n                                             'is_bb':'sum',\n                             'is_amc':'sum',\n                             'is_noc':'sum',\n                             'is_tr':'sum'}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=atpeak_byhour['hour'].values, y=atpeak_byhour['is_gme'].values,\n                    mode='lines+markers',\n                    name='GME'))\n\nfig.add_trace(go.Scatter(x=atpeak_byhour['hour'].values, y=atpeak_byhour['is_amc'].values,\n                    mode='lines+markers',\n                    name='AMC'))\n\nfig.add_trace(go.Scatter(x=atpeak_byhour['hour'].values, y=atpeak_byhour['is_bb'].values,\n                    mode='lines+markers',\n                    name='BB'))\n\nfig.add_trace(go.Scatter(x=atpeak_byhour['hour'].values, y=atpeak_byhour['is_tr'].values,\n                    mode='lines+markers',\n                    name='TR'))\n\nfig.update_layout(\n    title='Number of Mentions in Post Title per Hour',\n    xaxis_tickfont_size=14,\n    yaxis=dict(\n        title='Number of Posts',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze Body Data During Peak"},{"metadata":{"trusted":true},"cell_type":"code","source":"has_body = at_peak[at_peak['body'].notnull()].copy()\nhas_body.shape, np.round(has_body.shape[0]/at_peak.shape[0]*100, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"has_body['body'] = has_body['body'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Post Sample at Peak"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = has_body.sample(3000, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_token(token):\n    \"\"\"\n    Check whether token is valid\n    \"\"\"\n    if (not token or not token.string.strip or\n       token.is_stop or not token.is_alpha or token.is_punct):\n        return False\n    return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardize_text(df, column, nlp):\n    \"\"\"\n    MUCH FASTER than .apply\n    Check \n    https://stackoverflow.com/questions/44395656/applying-spacy-parser-to-pandas-dataframe-w-multiprocessing\n    \"\"\"\n    clean_docs = []\n    for doc in nlp.pipe(df[column].astype('unicode').values, batch_size=50,\n                    n_threads=3):\n        clean_comment =  [token.lemma_.strip() for token in doc if is_token(token)]\n        clean_docs.append(\" \".join(clean_comment))\n    return clean_docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['clean_body'] = standardize_text(sample, 'body', nlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['clean_title'] = standardize_text(sample, 'title', nlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv('sample_reddit_wallstreetsbets_posts.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_freq_distribution(df, column, ngram=2, top=20):\n    \"\"\"\n    Show Frequency Distribution for\n    Ngrams\n    \"\"\"\n    vectorizer = CountVectorizer(analyzer='word', ngram_range=(ngram, ngram))\n    matrix = vectorizer.fit_transform(df[column].values)\n    ngrams = pd.DataFrame(matrix.toarray())\n    ngrams.columns = vectorizer.get_feature_names()\n    freqdist(ngrams.columns, matrix, orient='h', n=top)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 20 Bigrams for Post Body"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_freq_distribution(sample, 'clean_body')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nmatrix = vectorizer.fit_transform(sample['clean_body'].values)\nngrams = pd.DataFrame(matrix.toarray())\nngrams.columns = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check = sample.reset_index(drop=True).merge(ngrams, left_index=True, right_index=True, how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_20 = ngrams.sum().to_frame().sort_values(0, ascending=False).head(20).index.tolist()\ntop_20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How did posts evolve during the peak?"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbigrams_bydate = check.groupby('date').agg({'hedge fund':'sum',\n                           'buy gme':'sum',\n                           'retail investor':'sum',\n                           'financial advice':'sum',\n                           'market manipulation':'sum',\n                           'wall street': 'sum',\n                           'hold line':'sum',\n                           'short squeeze':'sum'}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(x=bigrams_bydate['date'].values,\n                y=bigrams_bydate['hedge fund'].values,\n                name='Hedge Fund',\n                marker_color='rgb(55, 83, 109)'\n                ))\n\nfig.add_trace(go.Bar(x=bigrams_bydate['date'].values,\n                y=bigrams_bydate['buy gme'].values,\n                name='Buy GME',\n                marker_color='rgb(51, 185, 176)'\n                ))\n\n\nfig.add_trace(go.Bar(x=bigrams_bydate['date'].values,\n                y=bigrams_bydate['financial advice'].values,\n                name='Financial Advice'\n                ))\n\nfig.add_trace(go.Bar(x=bigrams_bydate['date'].values,\n                y=bigrams_bydate['market manipulation'].values,\n                name='Market Manipulation'\n                ))\n\nfig.add_trace(go.Bar(x=bigrams_bydate['date'].values,\n                y=bigrams_bydate['wall street'].values,\n                name='Wall Street'\n                ))\n\nfig.add_trace(go.Bar(x=bigrams_bydate['date'].values,\n                y=bigrams_bydate['hold line'].values,\n                name='Hold Line'\n                ))\n\n\nfig.update_layout(\n    title='Posts for Selected Bigrams During Peak',\n    xaxis_tickfont_size=14,\n    yaxis=dict(\n        title='Number of Posts',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Word2Vec to Find Similarities in the Posts\n\nThe idea of using Word2Vec is to find which words are more likely to appear together. This could shed some light into the overall sentiment of the post."},{"metadata":{"trusted":true},"cell_type":"code","source":"cores = multiprocessing.cpu_count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"has_body['clean_body'] = standardize_text(has_body, 'body', nlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [row.split() for row in has_body['clean_body']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = Phrases(sentences, min_count=30, progress_per=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_model = Phraser(phrases)\nmodel_sentences = word_model[sentences]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     size=400,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.build_vocab(model_sentences, progress_per=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_top_positive(model, word, top):\n    df = pd.DataFrame(model.wv.most_similar(positive=word)).loc[:top-1]\n    df = df.rename(columns={0:'words', 1:'prob'})\n    df.plot(x='words', y='prob', kind='barh', alpha=0.5, color='k')\n    plt.legend(loc='upper right',  bbox_to_anchor=(1.02, 1.0))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_top_positive(w2v_model, ['buy', 'gme'], 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_top_positive(w2v_model, ['hedgefund'], 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is interesting that this naive way of looking at the data is showing some relations that confirm what has been reported in the news:\n\nFor example, the words **market** or **manipulation** are likely to be related with fraud, collusion, clearly or blatant.\n\nThe same seems to be true for **hedgefund**, which is likely to be associated with words that express a negative sentiments."},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_top_positive(w2v_model, ['market', 'manipulation'], 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_top_positive(w2v_model, ['hold', 'line'], 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}