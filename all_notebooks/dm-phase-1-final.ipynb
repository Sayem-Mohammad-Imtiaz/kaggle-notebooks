{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CSE 571(Data Mining) Fall 19 Project- PHASE 1\n## Project AIM- Given a TS, predict if a MEAL was taken or not\n## Phase AIM- Extract features from training data, pass through PCA & prove their importance\n### TASKS:\n#### a) Extract 4 (one for each student) different types of time series features from only the CGM data cell array and CGM timestamp cell array (10 points each) total 40\n#### b) For each time series explain why you chose such feature (5 points each) total 20\n#### c) Show values of each of the features and argue that your intuition in step b is validated or disproved? (5 points each ) total 20\n#### d) Create a feature matrix where each row is a collection of features from each time series. So if there are 75 time series and your feature length after concatenation of the 4 types of features is 17 then the feature matrix size will be 75 X 17 (10 points)\n#### e) Provide this feature matrix to PCA and derive the new feature matrix. Choose the top 5 features and plot them for each time series. (5 points)\n#### f) For each feature in the top 5 argue why it is chosen as a top five feature in PCA? (3 points each) total 15\n\n### STEPS:\n#### 1. Load data(CGM values & time-series)\n#### 2. Convert TS values and reverse data into chronological order\n#### 3. EDA & plots\n#### 4. Feature extraction- Windowed Mean, Maximum Windowed Velocity, FFT & Entropy\n#### 6. PCA\n#### 7. Prove results & plots"},{"metadata":{},"cell_type":"markdown","source":"*  ### Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import packages\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom pandas import read_csv\nfrom pandas import concat\nfrom pandas import DataFrame\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport scipy.stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"KaggleInput = \"/kaggle/input/\"\nCGMTimeCSV = KaggleInput + \"continuous-blood-glucose-monitor-data/CGMDatenumLunchPat1.csv\"\nCGMValueCSV = KaggleInput + \"continuous-blood-glucose-monitor-data/CGMSeriesLunchPat1.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display data sample\nCGMDatenum = pd.read_csv(CGMTimeCSV)\nCGMSeries = pd.read_csv(CGMValueCSV)\n\nCGMDatenum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CGMSeries.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Convert TS type & timeline to chronological order"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert TS datatype and reverse indexing for chronological order\nCGMDatenum = CGMDatenum.applymap(lambda i : pd.to_datetime(i - 719529, unit='D'))\nCGMDatenum = CGMDatenum.iloc[::-1]\nCGMDatenum = CGMDatenum.iloc[:, ::-1]\n\nCGMDatenum_updated = CGMDatenum.copy()\nCGMDatenum_updated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reverse indexing for chronological order\n# Missing values- Linear interpolation\nCGMSeries = CGMSeries.iloc[::-1]\nCGMSeries = CGMSeries.iloc[:, ::-1]\n\nCGMSeries_updated = CGMSeries.copy()\nCGMSeries_updated.interpolate(method='linear', inplace=True)\n\nrow, col = CGMSeries_updated.shape\n\nCGMSeries_updated.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FEATURE EXTRACTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Matrix\nNewFeatureMatrix = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Feature 1 - CGM Velocity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Windowed velocity(non-overlapping)- 30 mins intervals\nvelocityDF = pd.DataFrame()\nfor i in range(0,26):\n     velocityDF['Vel_'+str(i)] = (CGMSeries_updated.iloc[:,i+5]-CGMSeries_updated.iloc[:,i])\nNewFeatureMatrix['Window_Velocity_Max']=velocityDF.max(axis = 1, skipna=True)\nNewFeatureMatrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting\nplt.plot(NewFeatureMatrix['Window_Velocity_Max'],'r-')\nplt.ylabel('Window_Velocity_Max')\nplt.xlabel('Days')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Feature 2 - Windowed Mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting\nfig = plt.figure(figsize = (12,8))\nax = fig.add_subplot(1,1,1) \nax.set_ylabel('Mean')\nax.set_xlabel('Days')\nax.set_title('Windowed Means')\nax.plot(NewFeatureMatrix.iloc[:,1:7],'-')\nax.legend(('Mean_0', 'Mean_6', 'Mean_12','Mean_18','Mean_24','Mean_30'),loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Windowed mean interval - 30 mins(non-overlapping)\nfor i in range(0,31,6):\n    NewFeatureMatrix['Mean_'+str(i)] = CGMSeries_updated.iloc[:,i:i+6].mean(axis = 1)\n    \nNewFeatureMatrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Feature 3 - FFT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FFT- Finding top 8 values for each row\ndef get_fft(row):\n    cgmFFTValues = abs(scipy.fftpack.fft(row))\n    cgmFFTValues.sort()\n    return np.flip(cgmFFTValues)[0:8]\n\nFFT = pd.DataFrame()\nFFT['FFT_Top2'] = CGMSeries_updated.apply(lambda row: get_fft(row), axis=1)\nFFT_updated = pd.DataFrame(FFT.FFT_Top2.tolist(), columns=['FFT_1', 'FFT_2', 'FFT_3', 'FFT_4', 'FFT_5', 'FFT_6', 'FFT_7', 'FFT_8'])\n\n#FFT_updated.head()\n\nNewFeatureMatrix = NewFeatureMatrix.join(FFT_updated)\n\nNewFeatureMatrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Feature 4 - Entropy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculates entropy(from occurences of each value) of given series\ndef get_entropy(series):\n    series_counts = series.value_counts()\n    entropy = scipy.stats.entropy(series_counts)  \n    return entropy\n\nNewFeatureMatrix['Entropy'] = CGMSeries_updated.apply(lambda row: get_entropy(row), axis=1) \nNewFeatureMatrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Final Feature Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final feature matrix\nNewFeatureMatrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA\nrows,cols = NewFeatureMatrix.shape\n\n# Standardizes feature matrix\nNewFeatureMatrix = StandardScaler().fit_transform(NewFeatureMatrix)\n\npca = PCA(n_components=5)\nprincipalComponents = pca.fit(NewFeatureMatrix)\nprint(principalComponents.components_) # Principal Components vs Original Features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(principalComponents.explained_variance_ratio_.cumsum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"principalComponentsTrans = pca.fit_transform(NewFeatureMatrix)\nPC_TimeSeries=pd.DataFrame(data=principalComponentsTrans,columns = ['principal component 1', 'principal component 2','principal component 3', 'principal component 4','principal component 5'])\nPC_TimeSeries.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting explained variance versus principle componenets\npcs = ['PC1','PC2','PC3','PC4','PC5']\nplt.bar(pcs,principalComponents.explained_variance_ratio_*100)\nplt.savefig('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #plotting top 5 principle components against each time series\nax = PC_TimeSeries.plot.bar(y='principal component 1', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 2', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 3', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 4', rot=0)\nax = PC_TimeSeries.plot.bar(y='principal component 5', rot=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting top 5 principle components against each time series\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### RESULTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plots & prove assumptions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# patient-by-patient analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}