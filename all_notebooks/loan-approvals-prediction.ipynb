{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loan approvals prediction ","metadata":{}},{"cell_type":"markdown","source":"### Data Exploration ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set()\n\nfrom ipywidgets import widgets\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/loan-predication/train_u6lujuX_CVtuZ9i (1).csv\")\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Loan_ID: Unique Loan ID\n- Gender: Male/ Female\n- Married: Applicant married (Y/N)\n- Dependents: Number of dependents\n- Education: Applicant Education (Graduate/ Under Graduate)\n- Self_Employed: Self employed (Y/N)\n- ApplicantIncome: Applicant income\n- CoapplicantIncome: Coapplicant income\n- LoanAmount: Loan amount in thousands\n- Loan_Amount_Term:\tTerm of loan in months\n- Credit_History: credit history meets guidelines\n- Property_Area: Urban/ Semi Urban/ Rural\n- Loan_Status: Loan approved (Y/N)","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Features engineering ","metadata":{}},{"cell_type":"code","source":"def missing_values(data):\n    temp = [feature for feature in data.columns if data[feature].isnull().sum()>1]\n    for feature in temp:\n        print(feature, \": Number of missing values ==> \",data[feature].isnull().sum(),\n             \"  --- missing data percentage ==> \" ,np.round(data[feature].isnull().mean(), 4),\"%\")\nmissing_values(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop('Loan_ID', axis = 1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoricalValues = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Loan_Amount_Term',\n                     'Credit_History', 'Property_Area']\nnumericalValues = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(data['Loan_Status'])\nfor p in ax.patches:\n    plt.title('loan approvement status', fontsize = 16)\n    ax.annotate(p.get_height(), (p.get_x()+p.get_width()/2.5, p.get_height()), fontsize=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotFunction(column):\n    ax = sns.countplot(x=column, data=data, hue='Loan_Status')\n    plt.ylabel('Count')\n    for p in ax.patches:\n        ax.annotate(p.get_height(), (p.get_x()+p.get_width()/2.6, p.get_height()), fontsize=12)\n\ndropdown_menu = {i:i for i in categoricalValues}\n\nwidgets.interact(plotFunction, column=dropdown_menu);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotFunction(column):\n    sns.distplot(data[column])\n    plt.xlabel(str(column))\n    plt.ylabel('Count')\n\ndropdown_menu = {i:i for i in numericalValues}\n\nwidgets.interact(plotFunction, column=dropdown_menu);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(y=numericalValues[2], data=data, x = 'Loan_Status')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Imputing Missing Values","metadata":{}},{"cell_type":"code","source":"# Categorical variables will be filled by the most frequent class\ndata['Gender'].fillna(data['Gender'].value_counts().idxmax(), inplace=True)\ndata['Married'].fillna(data['Married'].value_counts().idxmax(), inplace=True)\ndata['Dependents'].fillna(data['Dependents'].value_counts().idxmax(), inplace=True)\ndata['Self_Employed'].fillna(data['Self_Employed'].value_counts().idxmax(), inplace=True)\ndata['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].value_counts().idxmax(), inplace=True)\ndata['Credit_History'].fillna(data['Credit_History'].value_counts().idxmax(), inplace=True)\n\n# We use the median to fill missing numerical value\ndata[\"LoanAmount\"].fillna(data[\"LoanAmount\"].median(skipna=True), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encode categorical variables using weight of evidence (WOE)","metadata":{}},{"cell_type":"markdown","source":"WOE = ln (p(1) / p(0))\n\np(1) is the probability of the target being 1.<br>\np(0) is the probability of the target being 0.\n\n**Advantages of WOE Encoding:**\n- Creates a monotonic relationship between the target and the variables.\n- Orders the categories on a logistic scale, which is natural for logistic regression.\n- Determine which variable is more predictive.","metadata":{}},{"cell_type":"code","source":"data['Loan_Status'] = data['Loan_Status'].map({'N': 0, 'Y': 1})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation Matrix (only numerical variables)\nsns.heatmap(data.corr(),annot=True,linewidths=0.2,annot_kws={'size':12})\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data['Loan_Status'], test_size  = 0.2, \n                                                    random_state=42, stratify = data['Loan_Status'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = X_train.copy()\ntest = X_test.copy()\ntrain['target'] = y_train\ntest['target'] = y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for variable in ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed','Credit_History', 'Property_Area']: \n    dataframe = pd.DataFrame(train.groupby([variable])['target'].mean())\n    dataframe['non-target'] = 1 - dataframe['target']\n    dataframe['woe'] = np.log(dataframe['target'] / dataframe['non-target'])\n    ratio_mapping = dataframe['woe'].to_dict()\n    train[variable] = train[variable].map(ratio_mapping)\n    test[variable] = test[variable].map(ratio_mapping)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train.corr(),annot=True,linewidths=0.2,annot_kws={'size':12})\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"==> The credit history and the applicant income are the most positively correlated variables with our target","metadata":{}},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"X_train, X_test = train.iloc[:,:-1], test.iloc[:,:-1]\ny_train, y_test = train['target'], test['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler().fit(X_train)\nX_train_std = scaler.transform(X_train)\nX_test_std = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBClassifier\nimport graphviz\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, roc_curve","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are dealing with **binary** classification where  $y_j$ can take only 0 or 1. \n\n$$ y_j = f(X_j). $$\n\nFirstly, we are going to use the logistic regression model which is a classifier version of linear regression. It is a probabilistic model (predict probability values that can then be used to assign class labels) <br>\n$$ p_j = \\sigma \\left( \\sum_{i} X_{ji}\\beta_i  + \\beta_0 \\right), $$\nwhere\n$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$","metadata":{}},{"cell_type":"markdown","source":"**Performance Metrics**\n\n|                     | Positive Observation     | Negative Observation    |\n|---------------------|:------------------------:|:-----------------------:|\n| Positive Prediction |     True Positive (TP)   | False Positive (FP)     |\n| Negative Prediction | False Negative (FN)      |     True Negative (TN)  |\n\n$$ \\text{accuracy} = \\frac{\\text{number of correct observations}}{\\text{number of observations}}.$$\n\n$$ \\text{precision} = \\frac{\\text{TP}}{TP + FP}$$\n\n$$ \\text{recall} = \\frac{\\text{TP}}{TP + FN}. $$\n\n$$ \\text{F1-Score} = \\frac{\\text{2.precision.recall}}{precision + recall}. $$","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(max_iter=1000, solver='liblinear')\nlr.fit(X_train_std, y_train)\ny_pred_lr = lr.predict(X_test_std)  \nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred_lr),\"****   F1_score: \", f1_score(y_test, y_pred_lr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test) \nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred_lr),\"****   F1_score: \", f1_score(y_test, y_pred_lr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_probs = lr.predict_proba(X_test)[:, 1]\nlr_auc = roc_auc_score(y_test, lr_probs)\nprint('Logistic: ROC AUC= ' ,lr_auc)\n# calculate roc curves\n\nfpr, tpr, _ = roc_curve(y_test, lr_probs)\nplt.plot(fpr, tpr, label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Changing threshold\ny_pred_lr = (lr.predict_proba(X_test)[:,1]>0.75).astype(np.int32)\nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred_lr),\"****   F1_score: \", f1_score(y_test, y_pred_lr))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n# DummyClassifier is a baseline classifier that makes predictions using simple rule\n# we will use it as benchmark to evaluate our model performance ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\ny_pred_dummy = dummy_clf.predict(X_test) \nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred_dummy),\"****   F1_score: \", f1_score(y_test, y_pred_dummy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"xgboost classifier","metadata":{}},{"cell_type":"code","source":"# xgboost_parameters = {\n#     'max_depth' : [3,5,7,9,12,15,17,25],\n#     'learning_rate' : [0.01,0.015,0.025,0.05,0.1],\n#     'gamma' : [0.05,0.1,0.3,0.5,0.7,0.9,1],\n#     'min_child_weight' : [1,3,5,7], \n#     'subsample' : [0.6,0.7,0.8,0.9,1],\n#     'colsample_bytree' : [0.6,0.7,0.8,0.9,1],\n#     'reg_alpha' : [0.01,0.1,1],\n#     'reg_lambda' : [0.1,0.5,1],\n# }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgbmodel = XGBClassifier()\n# gs = GridSearchCV(xgbmodel, xgboost_parameters)\n# gs.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"gs.best_params_<br>\n{'colsample_bytree': 0.6,\n 'gamma': 0.1,\n 'learning_rate': 0.05,\n 'max_depth': 7,\n 'min_child_weight': 5,\n 'reg_alpha': 0.01,\n 'reg_lambda': 0.5,\n 'subsample': 1}","metadata":{}},{"cell_type":"markdown","source":"| hyperparameter | description |\n| --- | --- |\n| learning_rate | step size shrinkage used to prevent overfitting. Range is [0,1] |\n| max_depth | determines how deeply each tree is allowed to grow during any boosting round |\n| subsample | percentage of samples used per tree. Low value can lead to underfitting. |\n| colsample_bytree | percentage of features used per tree. High value can lead to overfitting. |\n| n_estimators | number of trees you want to build. |\n| objective | determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision, binary:logistic for classification problems with probability. |\n| gamma | controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners. |\n| alpha | L1 regularization on leaf weights. A large value leads to more regularization. |\n| lambda | L2 regularization on leaf weights and is smoother than L1 regularization. |","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"PATH\"] += os.pathsep + 'C:/Users/youssef.amdouni/Anaconda3/Library/bin/graphviz'\nimport xgboost as xgb\nimport graphviz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbmodel = XGBClassifier(colsample_bytree = 0.6,\n                        gamma= 0.1,\n                        learning_rate = 0.05,\n                        max_depth = 7,\n                        min_child_weight = 5,\n                        reg_alpha = 0.01,\n                        reg_lambda = 0.5,\n                        subsample = 1)\nxgbmodel.fit(X_train, y_train)\ny_pred_xgb = xgbmodel.predict(X_test) \nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred_xgb),\"****   F1_score: \", f1_score(y_test, y_pred_xgb))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_tree(xgbmodel, num_trees=0)\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(xgbmodel)\nplt.rcParams['figure.figsize'] = [16, 6]\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Classification using neural network ","metadata":{}},{"cell_type":"markdown","source":"Good tutorial to build neural network with pytorch [Tutorial link.](https://uvadlc-notebooks.readthedocs.io/en/latest/index.html)","metadata":{}},{"cell_type":"code","source":"X_train_std = pd.DataFrame(X_train_std, columns=X_train.columns) \nX_test_std = pd.DataFrame(X_test_std, columns=X_train.columns)\nprint(X_train_std.shape, X_test_std.shape, data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(250)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class loadData(Dataset):\n    \n    def __init__(self, X_data, y_data):\n        self.X_data = X_data\n        self.y_data = y_data\n        \n    def __getitem__(self, index):\n        return self.X_data[index], self.y_data[index]\n        \n    def __len__ (self):\n        return len(self.X_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = loadData(torch.FloatTensor(X_train_std.values), \n                       torch.FloatTensor(y_train.values))\ntest_data = loadData(torch.FloatTensor(X_test_std.values), \n                       torch.FloatTensor(y_test.values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_data, batch_size=64)\ntest_loader = DataLoader(dataset=test_data, batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class binaryClassification(nn.Module):\n    def __init__(self, num_inputs, num_hidden, num_outputs):\n        super(binaryClassification, self).__init__()\n        \n        self.layer_1 = nn.Linear(num_inputs, num_hidden) \n        self.layer_2 = nn.Linear(num_hidden, num_hidden)\n        self.layer_out = nn.Linear(num_hidden, num_outputs) \n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(64)\n        self.batchnorm2 = nn.BatchNorm1d(64)\n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.layer_out(x)\n        \n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = binaryClassification(num_inputs=11, num_hidden=64, num_outputs=1)\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_module = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\ndef train_model(model, optimizer, data_loader, loss_module, num_epochs=80):\n    model.train()\n\n    # Training loop\n    for epoch in tqdm(range(num_epochs)):\n        for data_inputs, data_labels in data_loader:\n\n            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n\n            ## Run the model on the input data\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1) \n            \n            ## loss\n            loss = loss_module(preds, data_labels.float())\n\n            ## Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n\n            ## Update parameters\n            optimizer.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model, optimizer, train_loader, loss_module)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_dict = model.state_dict()\n#print(state_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, data_loader):\n    model.eval() \n    true_preds, num_preds = 0., 0.\n\n    with torch.no_grad(): \n        for data_inputs, data_labels in data_loader:\n\n            # Determine prediction of model on dev set\n            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n            preds = model(data_inputs)\n            preds = preds.squeeze(dim=1)\n            preds = torch.sigmoid(preds) \n            pred_labels = (preds >= 0.5).long() \n\n            true_preds += (pred_labels == data_labels).sum()\n            num_preds += data_labels.shape[0]\n\n    acc = true_preds / num_preds\n    print(\"Accuracy of the model: %4.2f%%\" % (100.0*acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_model(model, test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}