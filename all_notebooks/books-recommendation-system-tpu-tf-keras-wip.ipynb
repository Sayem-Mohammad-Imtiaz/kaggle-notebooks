{"cells":[{"metadata":{},"cell_type":"markdown","source":"## RNN books recommendation system with TPU training"},{"metadata":{},"cell_type":"markdown","source":"Welcome to this notebook\n\nWe assume that there is an interaction between an item (book) if there is a rating on the BX-Book-Ratings.csv file. We don't take into account the rate the user gives to the book.\n\nNote: \"items\" in this notebook will refer to books\n\nYou want to improve your knowledge in recommendation systems (and the possible application for ecommerce)? You can check it out this [Medium blog post](https://medium.com/decathlondevelopers/personalization-strategy-and-recommendation-systems-at-d%C3%A9cathlon-canada-d9cb3d37f675) from [DÃ©cathlon Canada](https://www.decathlon.ca/en/) (world's largest sporting good retailer)\n\nNote2: this notebook is in progress, if you like it or you would like the analysis to be deeper, please upvote it!"},{"metadata":{},"cell_type":"markdown","source":"# 1. Notebook setup"},{"metadata":{},"cell_type":"markdown","source":"## Libraries importation and setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# installation of keras self attention layer\n!pip install keras-self-attention\n\nimport os\nfrom collections import Counter\nfrom random import choice\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport requests\n\nfrom IPython.display import Image\n\nimport keras\nfrom keras_self_attention import SeqSelfAttention\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import models\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_convergence\nfrom skopt.utils import use_named_args","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tensorflow version checking\nprint(\"Tensorflow version \" + tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define some parameters\n\n## size of the train set\ntrain_percent_split = 0.9\n\n# first hyperparameters of the RNN model\nepochs = 1\ndropout = 0\nembedding_size = 128\nhidden_size_lstm = 128\nlearning_rate = 0.00276\nattention_width = 20\nadd_dense_layer = True\nhidden_size_dense = 128\n\nvalidation_split = 0.1\nn_reco = 20\nn_min_interactions = 20\nmax_length_seq = 20\nnb_last_item = 20\nmax_users = 5000\n\n# iterations number for optimization with scikit-optimize lib\nn_random_starts = 2\nn_calls = 15","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data importation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# paths definition\npath = '/kaggle/input/bookcrossing-dataset/Book reviews/'\nbook_fp = os.path.join(path, 'BX-Books.csv')\nuser_fp = os.path.join(path, 'BX-Users.csv')\nrating_fp = os.path.join(path, 'BX-Book-Ratings.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# files reading\ndef read_file(fp):\n    data = pd.read_csv(fp,\n                       sep=';',\n                       encoding='latin-1',\n                       low_memory=False,\n                       header=0,\n                       error_bad_lines=False)\n    return data\n    \nuser_df = read_file(user_fp)\nitem_df = read_file(book_fp)\nrating_df = read_file(rating_fp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Datasets inspection"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(user_df.shape)\nprint(user_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(item_df.shape)\nprint(item_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rating_df.shape)\nprint(rating_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rows seem sorted by User-ID, let's shuffle the rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df = rating_df.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have NA on rating_df! Good point.\n\n# 3. Data preparation\n\nLet's rename the columns for simplicity."},{"metadata":{"trusted":true},"cell_type":"code","source":"colname_mapping = {\n    'ISBN': 'item',\n    'User-ID': 'user',\n    'Book-Rating': 'rating',\n    'Book-Title': 'name',\n    'Book-Author': 'author',\n    'Image-URL-M': 'image',\n    'Publisher': 'publisher',\n    'Year-Of-Publication': 'year'\n}\nrating_df = rating_df.rename(columns=colname_mapping)\nitem_df = item_df.rename(columns=colname_mapping)\nuser_df = user_df.rename(columns=colname_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenization: converting books and users \"real\" ids to integer for efficiency of manipulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"items = list(set(item_df.item.unique().tolist() + rating_df.item.unique().tolist()))\nusers = user_df.user.unique() \nprint(f'number of unique items: {len(items)}\\nnumber of unique users: {len(users)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_df = rating_df[rating_df.item.isin(items)]\nitem_df = item_df[item_df.item.isin(items)]\nrating_df.shape, item_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a huge number of different books!"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_to_token = {user: int(token) for token, user in enumerate(users)}\ntoken_to_item = {token: user for user, token in user_to_token.items()}\nitem_to_token = {item: int(token) for token, item in enumerate(items)}\ntoken_to_item = {token: item for item, token in item_to_token.items()}\n\nrating_df['user_id'] = rating_df['user'].map(user_to_token)\nrating_df['item_id'] = rating_df['item'].map(item_to_token).dropna().astype(int)\n\nitem_df['item_id'] = item_df['item'].map(item_to_token)\nuser_df['user_id'] = user_df['user'].map(user_to_token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%timeit\n#diff = set(item_df.item_id).difference(set(rating_df.item_id))\n#len(diff), diff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_df.item_id.nunique(), rating_df.item_id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining functions to print books characteristics such as name, author, publisher, year of publication and display the cover image for vizualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_single_item_characteristics(item_id=None, item_df=item_df):\n    if not item_id:\n        item_id = choice(item_df.item_id)\n    if item_id not in set(item_df.item_id):\n        print(f'item_id {item_id} not in df')\n        return None\n\n    item_df = item_df[item_df['item_id'] == item_id]\n    url = item_df.image.values[0]\n    response = requests.get(url)\n\n    print(f'item_id: {item_id}; '\n          f'name: {item_df.name.values[0]}; '\n          f'author: {item_df.author.values[0]} '\n          f'publisher: {item_df.publisher.values[0]} '\n          f'year: {item_df.year.values[0]}')\n    return Image(url)\n\ndef print_items_characteristics(item_id_list):\n    for item_id in item_id_list:\n        display(print_single_item_characteristics(item_id=item_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rating_by_user = rating_df.user_id.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_rating_by_user.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's display some popular books characteristics for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"top3_item_id = list(rating_df.item_id.value_counts().index.values[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top3_item_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_items_characteristics(item_id_list=top3_item_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count the number of \"interactions\" per user (i.e. ratings per user)"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_occurence = Counter(rating_df.user).most_common()\nprint(user_occurence[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keep users with at least n interactions, to reduce the data size and probably expect better \"qualitative users\""},{"metadata":{"trusted":true},"cell_type":"code","source":"def user_with_n_interaction(data, n):\n    print(f'length before filtering: {len(data)}.')\n    user_occurence = Counter(data.user)\n\n    user_to_keep = [\n        user\n        for user, occ in user_occurence.items()\n        if occ >= n\n    ]\n\n    data_filtered = data[data['user'].isin(user_to_keep)]\n    print(f'length after filtering: {len(data_filtered)}.')\n    return data_filtered\n\nrating_df = user_with_n_interaction(data=rating_df, n=n_min_interactions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_ind = int(len(rating_df) * train_percent_split)\ntrain, test = rating_df[:split_ind], rating_df[split_ind:]\nprint(f'shape of train: {train.shape}\\nshape of test: {test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Sequences preparation for the RNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to prepare sequence data\ndef prepare_sequences(data, users, item_to_token, max_length=20, \n                      one_hot_encoding=False):\n    \n    print('preparing sequences')\n    \n    #generate sequences - see https://stackoverflow.com/questions/36864699/pandas-pivot-dataframe-with-unequal-columns        \n    data = pd.concat([\n        pd.DataFrame(\n            {\n                g:[0] * (max_length+1-len(d['item_id'].tolist()[-max_length-1:])) + d['item_id'].tolist()[-max_length-1:]\n            }\n        )\n        for g,d in data.groupby('user_id')], axis=1)\n \n    \n    #from pandas dataframe to numpy array\n    data = data.transpose().values\n        \n    #transpose and build the arrays\n    x = np.array([i[:-1] for i in data])\n    y = np.array([i[1:] for i in data])\n            \n    #build the one-hot encoding, if we want\n    if one_hot_encoding:\n        y = np_utils.to_categorical(y, len(item_to_token)+1)\n    else:\n        y = np.expand_dims(y, -1)\n    \n    print('sequences prepared')\n        \n    return (x, y)\n\n#function to extract prediction from keras model at last timestep\ndef predict_last_timestep(model, data):\n    #calculate the model output\n    prediction = model.predict(data)\n    #keep only the prediction at the final timestep\n    return prediction[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = prepare_sequences(data=train, users=users, item_to_token=item_to_token, max_length=max_length_seq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Keras model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"def keras_model(hidden_size_lstm=hidden_size_lstm, \n                learning_rate=learning_rate, \n                dropout=dropout,\n                attention_width=attention_width,\n                embedding_size=embedding_size,\n                add_dense_layer=add_dense_layer,\n                hidden_size_dense=hidden_size_dense,\n                embedding_matrix=None,\n                item_to_token=item_to_token):\n    \n    with strategy.scope():\n        embedding_layer = Embedding(len(item_to_token)+1,\n                                    embedding_size,\n                                    weights=embedding_matrix,\n                                    mask_zero=True)\n\n        model = Sequential()\n        model.add(embedding_layer)\n        model.add(LSTM(units=hidden_size_lstm,\n                       activation='tanh', dropout=dropout,\n                       return_sequences=True))\n        model.add(SeqSelfAttention(attention_activation='sigmoid',\n                                   attention_width=attention_width,\n                                   history_only=True))\n        if add_dense_layer:\n            model.add(Dense(units=hidden_size_dense, activation='relu'))\n        model.add(Dense(units=len(item_to_token)+1, activation='softmax'))\n        optimizer = Adam(lr=learning_rate)\n    # Compile model\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer,\n                  metrics=['sparse_categorical_accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the model\nmodel = keras_model(hidden_size_lstm=hidden_size_lstm, \n                    learning_rate=learning_rate, \n                    dropout=dropout, \n                    embedding_size=embedding_size)  \n\nhistory = model.fit(x, y,\n                    epochs=epochs,\n                    validation_split=validation_split,\n                    batch_size=64,\n                    verbose=1)\n\nprint(history.history['val_sparse_categorical_accuracy'][-1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Recommendations prediction"},{"metadata":{},"cell_type":"markdown","source":"We make a list of users which are both present in train and test set. We will use these users to compute the performance metrics of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"common_user = list(set(train.user_id).intersection(test.user_id))\ncommon_user[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'There are {len(common_user)} common users between the train and test set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reco_from_item_id_interacted(item_id_interacted, model=model, n_reco=10):\n    predictions = predict_last_timestep(model=model,\n                                        data=[item_id_interacted]).argsort()[0][:n_reco]\n    return list(predictions)\n\ndef build_user_to_interacted(user_id, train=train):\n    return train[train['user_id'] == user_id]['item_id'].dropna().unique().tolist()\n\n#item_id_interacted = build_user_to_interacted(user_id=choice(user_train))\n#len(item_id_interacted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def user_to_last_visited_item_id_dict(train: pd.DataFrame,\n                                      user_list: list,\n                                      nb_last_item: int=None) -> dict:\n    \"\"\"\n    Return a dictionary mapping user to last visited items id.\n    input:\n            :train: pd.DataFrame, training set\n            :user_list: list, head_visitor_id\n            :nb_last_item: int, number of last interacted items\n                           to use to predict recommendation\n    output:\n            :: dict, mapping the user (head_visitor_id) to the last\n               visited items\n    \"\"\"\n    if nb_last_item:\n        return train.groupby('user_id')['item_id'].apply(lambda g: g.values\n                                                      .tolist()[-nb_last_item:]).to_dict()\n    else:\n        return train.groupby('user_id')['item_id'].apply(lambda g: g.values\n                                                      .tolist()).to_dict()\n\nuser_to_last_visited_item_id_dict = user_to_last_visited_item_id_dict(train=train,\n                                                                      user_list=train.user_id.unique().tolist(),\n                                                                      nb_last_item=nb_last_item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(user_to_last_visited_item_id_dict.items())[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_for_one(\n    model,\n    user_id,\n    n_reco,\n    user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict\n):\n    item_id_interacted = user_to_last_visited_item_id_dict[user_id]\n    reco_id = reco_from_item_id_interacted(item_id_interacted,\n                                           model=model,\n                                           n_reco=n_reco)\n    return reco_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(\n    model,\n    user_id_list,\n    n_reco,\n    user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict,\n    max_users=None, # reduce computation time\n):\n    start = time.time()\n    print(f'recommendation computation for {len(user_id_list[:max_users])} users.')\n    print(f'n_reco={n_reco}')\n    reco_dict = {\n        user_id: predict_for_one(\n            model=model,\n            user_id=user_id,\n            n_reco=n_reco,\n            user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict\n        )\n        for user_id in user_id_list[:max_users]\n    }\n    print(f'predict for {len(user_id_list[:max_users])} spent {round(time.time()-start, 2)} s.')\n    return reco_dict\n#max_users=100\n#n_reco=50\nreco_dict = predict(\n    model=model,\n    user_id_list=common_user,\n    user_to_last_visited_item_id_dict=user_to_last_visited_item_id_dict,\n    max_users=max_users,\n    n_reco=n_reco)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(reco_dict.items())[:3]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Performance metrics computation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to calculate, precision, recall and coverage\ndef statistics_at_k(reco_dict,\n                    test_df,\n                    train_df,\n                    calculate_precision=True,\n                    calculate_recall=True, \n                    calculate_coverage=True): \n    '''\n    reco_dict: dictionary with the uid as key, list of items recommended as attribute\n    test_df: dataframe of user-item interactions\n    '''     \n    #calculate precision\n    if calculate_precision:\n        k_relevant = 0\n        k_total = 0\n        for uid, iid in reco_dict.items():\n            iid_test = set(test_df[test_df['user_id'] == uid]['item_id'])\n            for j in iid:\n                k_total += 1\n                if j in iid_test:\n                    k_relevant += 1\n        if not k_total:\n            precision = 0\n        else:\n            precision = k_relevant/k_total\n        print(f'precision={precision}')    \n    else:\n        precision = None\n        \n    #calculate precision\n    if calculate_recall:\n        k_relevant = 0\n        k_total = 0\n        for uid, iid in reco_dict.items():\n            for j in list(test_df[test_df['user_id'] == uid]['item_id']):\n                k_total += 1\n                if j in set(iid):\n                    k_relevant += 1\n        \n        if not k_total:\n            recall = 0\n        else:\n            recall = k_relevant/k_total\n        print(f'recall={recall}')\n    else:\n        recall = None\n        \n    #calculate coverage\n    if calculate_coverage:\n        nb_recommended = len(set(sum(reco_dict.values(), [])))\n        nb_total = len(train_df['item_id'].unique())\n        coverage = nb_recommended/nb_total\n        print(f'coverage={coverage}')\n    else:\n        coverage = None\n    \n    return precision, recall, coverage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# statistics with RNN recommendation\nstatistics_at_k(reco_dict=reco_dict,\n                test_df=test,\n                train_df=train,\n                calculate_precision=True,\n                calculate_recall=True, \n                calculate_coverage=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topn = list(train.item_id.value_counts().index.values[:n_reco])\ntopn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reco_topn_dict = {\n    user_id: topn\n    for user_id in reco_dict.keys() \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(reco_topn_dict.items())[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# statistics with top n recommendation\nstatistics_at_k(reco_dict=reco_topn_dict,\n                test_df=test,\n                train_df=train,\n                calculate_precision=True,\n                calculate_recall=True, \n                calculate_coverage=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Visualization example"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_id_interacted = [197383]\nitem_id_pred = predict_last_timestep(\n    model=model,data=[item_id_interacted]\n).argsort()[0][:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# past interactions\nitem_df[item_df['item_id'].isin(item_id_interacted)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization of the pas\nprint_items_characteristics(item_id_list=item_id_interacted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\nitem_df[item_df['item_id'].isin(item_id_pred)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_id_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_items_characteristics(item_id_list=item_id_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del history\ndel model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. RNN model hyperparameters optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the dimension to search\ndim_epochs = Integer(low=1, high=15, name='epochs')\ndim_hidden_size_lstm = Categorical(categories=[32, 64, 128, 256], name='hidden_size_lstm')\ndim_learning_rate = Real(low=1e-4, high=5e-1, prior='log-uniform',\n                         name='learning_rate')\ndim_attention_width = Integer(low=1, high=50, name='attention_width')\ndim_dropout = Real(low=0, high=0.9, name='dropout')\ndim_embedding_size = Categorical(categories=[64, 128, 256, 512], name='embedding_size')\ndim_add_dense_layer = Categorical(categories=[True, False], name='add_dense_layer')\ndim_hidden_size_dense = Categorical(categories=[32, 64, 128, 256], name='hidden_size_dense')\n\ndimensions = [dim_epochs,\n              dim_hidden_size_lstm,\n              dim_learning_rate,\n              dim_attention_width,\n              dim_dropout,\n              dim_embedding_size,\n              dim_add_dense_layer,\n              dim_hidden_size_dense]\n\nepochs = 3\ndropout = 0\nembedding_size = 256\nhidden_size_lstm = 64\nlearning_rate = 0.00276","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize(dimensions=dimensions, n_calls=15, n_random_starts=3, verbose=1, x0=None):\n    print(dimensions)\n    @use_named_args(dimensions=dimensions)\n    def fitness(**params):\n        print(f'params={params}')\n        model = keras_model(hidden_size_lstm=params['hidden_size_lstm'],\n                            learning_rate=params['learning_rate'],\n                            attention_width=params['attention_width'],\n                            dropout=params['dropout'],\n                            embedding_size=params['embedding_size'],\n                            add_dense_layer=params['add_dense_layer'],\n                            hidden_size_dense=params['hidden_size_dense'])  \n\n        history = model.fit(x, y,\n                            epochs=params['epochs'],\n                            validation_split=validation_split,\n                            batch_size=64,\n                            verbose=verbose)\n        sca = history.history['val_sparse_categorical_accuracy'][-1]\n        print(f'##sca={sca}## with params={params}')\n        del history\n        del model\n        return -1.0 * sca\n    \n    res = gp_minimize(func=fitness,\n                      dimensions=dimensions,\n                      acq_func='EI', # Expected Improvement.\n                      n_calls=n_calls,\n                      n_random_starts=n_random_starts,\n                      x0=x0)\n    print(f'best accuracy={-1.0 * res.fun} with {res.x}')\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = optimize(dimensions=dimensions,\n               n_calls=n_calls,\n               n_random_starts=n_random_starts,\n               x0=[3, 256, 0.001, 40, 0, 512, True, 128])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}