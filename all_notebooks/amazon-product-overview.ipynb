{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\n#from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.layers import Dense,LSTM\nfrom keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/amazon-music-reviews/Musical_instruments_reviews.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets get the exactly columns names so to avoid mismatching strings checks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for \"Not a Numbers\" values is always a good procedure before any data munipulation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the column \"Review Text\" has 27 NaN values. Lets replace those by just an empty string \"\". For that we'll use the method *fillna()*:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reviewText.fillna(\"\",inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, as we are only interested in predicting the rating of a user based on his review, we can delete all the other informations of our data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['reviewerID']\ndel df['asin']\ndel df['reviewerName']\ndel df['helpful']\ndel df['unixReviewTime']\ndel df['reviewTime']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our new data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"quality\"] = df.loc[:,\"overall\"].apply(lambda x : \"good\" if x >= 4 else (\"neutral\" if x==3 else \"bad\" ))\ndf[\"strQuality\"] = df.loc[:,\"quality\"].apply(lambda x : 2 if x == \"good\" else (1 if x== \"neutral\" else 0 ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the \"Summary\" can add some useful information about the overall review, we gonna simply merge this column info with the \"Review Text\". ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['reviewText'] + ' ' + df['summary']\ndel df['reviewText']\ndel df['summary']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets get some statistical info: how many users rated their bought as 5.0? How many as 1.0? We can do that simply running the *value_counts()* on the desired column:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.overall.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,each in enumerate(df.overall.value_counts()):\n    print(f\"Percentage of {df.overall.value_counts().index[i]} stars : {(each*100/len(df.overall)):.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have a lot rating their bought with 5 stars (68%) and only a few with 1 (2%).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To perform NLP is interesting to remove unecessary words and symbols that may just overload our model and don't add meaningful information (this is the case of ponctuation and some words connecting phrases) This words are called as *STOPWORDS* and it is a pre-built list. For each language it is composed of specific words, evidently. So firts we get that list and store it in our variable, \"stop\". Next, we get a similar list but this time with the punctuation! Finally, we can update our \"stop\" list with also this punctuation. We are creating this \"stop\" list as we'll need to remove them from our \"reviews\" in order to clean it up and proceed to our training!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation) ### adding the punctioation as stopwords as well!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look in what is our \"stop\" list so far:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point you may be asking if some of those words could not be RELEVANT to understand the user's review!! Well, it depends on how you gonna analyse the data. If you want to use a LSTM algorithm, we may lose important information if we just delete the \"binding\" words. As we will also train a LSTM model, I'll create a list called \"rem\" to remove from my \"stop\" list the words that could be meaningful for the LSTM model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rem = [\"aren't\", \"aren\", \"but\", \"couldn\", \"couldn't\", \"don\", \"don't\",\"didn\", \"didn't\", \"doesn\", \"doesn't\", \"wouldn\", \"wouldn't\", \"won\", \"won't\", \"weren\", \"weren't\", \"wasn\", \"wasn't\", \"should\", \"shouldn't\", \"needn\", \"needn't\", \"mustn\", \"mustn't\", \"mightn\", \"mightn't\", \"isn\", \"isn't\", \"haven\", \"haven't\", \"hasn\", \"hasn't\", \"hadn\", \"hadn't\",\"not\", \"no\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll *LEMMATIZE*!!! What??!! Yes, lemmatize is the process of getting inflected words and treated them as the same. For instace, the word, \"rocks\", \"rocky\" will be seen as \"rock\". This is a way to schrink our data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        #print(\"word : \", i)\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            #print(\"pos : \", pos)\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            #print(\"Lemma word : \", word)\n            final_text.append(word.lower())\n    return \" \".join(final_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check it with a small example. I create a list \"z\" with 3 sentences. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"z = [\"I don't knowing know know, but don't care\",\"I would like you know\",\"Don't care care cares\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, when we call \"lemmatize_words(each)\" we'll first get off the \"stop\" words and them build our new lemmatized phares:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for each in z:\n    r = lemmatize_words(each)\n    print(\"our r : \", r)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that for our 1st sentence, \"knowing\" and \"know\" are seen as the same word - \"kwno\". The 3rd one tells us that \"care\" and \"cares\" are seen as \"care\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you want to take a look deeper in what these \"Reviews\" look like, uncomment the 2 cells below (for the \"good\" and \"neutral\" rating respectvely)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(100):\n#     if df.loc[i,\"quality\"] == \"good\":\n#         print(i,\"\\n\", df.loc[i,\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(100):\n#     if df.loc[i,\"quality\"] == \"neutral\":\n#         print(i,\"\\n\", df.loc[i,\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets do it in our \"text\" column (it can take some time, as we are doing that in a not that small data):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.apply(lemmatize_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the data now:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets just take a list of each of our possible ratings (\"good\", \"neutral\" and \"bad\"). (This will be useful for creating the WordPlotting that comes next):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"good = df.text[df.quality == \"good\"]\nneutral = df.text[df.quality == \"neutral\"] #.drop(columns = \"overall\")\nbad = df.text[df.quality == \"bad\"] # .drop(columns = \"overall\")\ngood.shape,bad.shape,neutral.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Plotting WordClouds: ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,30))\nqual = {0 : [\"neutral\",neutral], 1 : [\"bad\", bad], 2 : [\"good\",good]}\nqual[0][0]\nfor i in range(3):\n    ax = fig.add_subplot(1,3,i+1)\n    wc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800).generate(\" \".join(qual[i][1]))\n    #wc.recolor(color_func = grey_color_func)\n    ax.imshow(wc,interpolation = 'bilinear')\n    plt.xlabel(qual[i][0])\n    #ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"WordPlot can always be a cool and interesting way to visualize word frequency in a data. But lets be more precise and check a histogram, because we cannot exactly tell \"how much\" they are bigger or smaller within each case and compared to the others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.datasets import load_hobbies\n\nfor i in range(3):  \n    fig = plt.figure(figsize=(15,3))\n    corpus = qual[i][1]\n    vectorizer = CountVectorizer()\n    docs       = vectorizer.fit_transform(corpus)\n    features   = vectorizer.get_feature_names()\n\n    visualizer = FreqDistVisualizer(features=features, orient='v',n=10, title=[\"Frequency of 10 words for : \" + qual[i][0]])\n    visualizer.fit(docs)    \n    visualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What if we consider \"words\" composed of 2 to 3 words? This can be achieved changing a parameter in the CountVectorizer funciont (further explanation can be seen in the next steps)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(3):  \n    fig = plt.figure(figsize=(15,3))\n    corpus = qual[i][1]\n    vectorizer = CountVectorizer(min_df=0,binary=False,ngram_range=(2,3)) ### We changed this parameter!!\n    docs       = vectorizer.fit_transform(corpus)\n    features   = vectorizer.get_feature_names()\n\n    visualizer = FreqDistVisualizer(features=features, orient='v',n=10, title=[\"Frequency of 10 words for : \" + qual[i][0]])\n    visualizer.fit(docs)    \n    visualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that \"work well\" is not a good vocabulary to distinguish btw the rates as it appears with high frequency in all the 3 cases, specially because we have a very UNBALANCED data. Could we penalize our model if he sees it when training?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# CountVectorizer and TFI:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we're gonna to transform each of our sentence in a Matrix!!! Yes!! This is how we gonna treat those complex words expressed by humans: with NUMBERS!! For that we call the method \"CountVectorizer\"! ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Take a look at our small example from our \"z\" list of 3 sentences that we created previously:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Remember our \"z\" list:\n#### z = [\"I don't knowing know know, but don't care\",\"I would like you know\",\"Don't care care cares\"]\ncvz=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(2,3))\n#cv=CountVectorizer(ngram_range=(2,3))\n\ncv_testz=cvz.fit_transform(z)\ncvz.get_feature_names() ### Take a look at what this method does:","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can play with the *ngram_range* parameter and see what happens!! Roughly it says to our method to consider only a bag of words that has between 2 and 3 words!! Next, lets see in terms of numbers and matrix what that represents and how the ML model will see our data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_testz.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, for our 1st sentence (the first vector of our matrix), it says that the first elements considered in our bag \"but don\" appears in our sentence 1 times - this is the first \"1\" of our vector. After, he says that the second element of our bag \"but don care\" appears once also. The thirds element (\"care care\" doesn't appear anytime). \"care\" appears, but NOT \"care care\". And so on...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"What if we want to emphazise a vocabulary? Lets say that the combination \"you know\" is important. To highlight this, we can make our model increase its weight. To do so, lets first see the index term of \"you know\" in our cv matrix:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cvz.vocabulary_[\"you know\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so now lets say to our matrix that we want a doubled weight to its value. So lets multiply its value by 2:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a = cv_testz.toarray()\nfor i in range(3):\n       a[i][18] = a[i][18]*2 \ndisplay(cv_testz.toarray())\ndisplay(a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, we can observe that our last element of the 2nd vector changed to 2. The others didn't because multiplying any number to ZERO is still ZERO nowadays... -,-","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"What about TfidVectorizer???","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#z = [\"I don't know know know, but don't care\",\"I like you know\",\"Don't care care care\"]\ntvt=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(2,3))\n#transformed reviews\ntvt_test=tvt.fit_transform(z)\n\ntvt.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far, we don't see any difference btw CountVect and TfdiVect. But what about our matrix?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tvt_test.toarray()\n#z = [\"I don't know know know, but don't care\",\"I like you know\",\"Don't care care care\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quite different!! This is because the Tdfi will take the inverse of the frequence of each element considered. It's a way to normalize our data! So, the more commom is a \"word\" in our document, the higher its frequency, and so the lower its score. Inversely, words that are \"unique\" will have a lower frequency, thus higher score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After that we prepared our data, it's time to TRAIN our models!!! YEAHHH","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, lets split our data!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(df.text,df.quality,test_size = 0.2 , random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv=CountVectorizer(min_df=0,binary=False,ngram_range=(2,3))\n#cv=CountVectorizer(ngram_range=(2,3))\n#transformed train reviews\ncv_train_reviews=cv.fit_transform(x_train)\n#transformed reviews\ncv_test_reviews=cv.transform(x_test)\n#display(cv_train_reviews.toarray())\nprint('cv_train:',cv_train_reviews.shape)\nprint('cv_test:',cv_test_reviews.shape)\n\ntv=TfidfVectorizer(min_df=0,use_idf=True,ngram_range=(2,3))\n#transformed reviews\ntv_train_reviews=tv.fit_transform(x_train)\n\ntv_test_reviews=tv.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=0)\n#Fitting\nlr_bow=lr.fit(cv_train_reviews,y_train)\nprint(lr_bow)\n\nlr_tfidf=lr.fit(tv_train_reviews,y_train)\nprint(lr_tfidf)\n\n#Predicting \nlr_bow_predict=lr.predict(cv_test_reviews)\n\nlr_tfidf_predict=lr.predict(tv_test_reviews)\n\n#Accuracy score \nlr_bow_score=accuracy_score(y_test,lr_bow_predict)\nprint(\"lr_bow_score :\",lr_bow_score)\n\nlr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got a score of 89%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#report\nlr_bow_report=classification_report(y_test,lr_bow_predict,target_names=['good','neutral','bad'])\nprint(lr_bow_report)\n\n\nlr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['good','neutral','bad'])\nprint(lr_tfidf_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_bow_report=classification_report(y_test,lr_bow_predict,target_names=['good','neutral','bad'])\nprint(lr_bow_report)\n\n\nlr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['good','neutral','bad'])\nprint(lr_tfidf_report)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before moving to the LSTM model, lets see what happen if we change some vocabulary weight. \"sound like\" and \"planet wave\", \"they re\" and \"sound good\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv.vocabulary_[\"sound good\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tuning(pen, inc):\n    global tr, te\n    tr = cv_train_reviews.toarray()\n    te = cv_test_reviews.toarray()\n\n    voc = [\"planet waves\", \"sound like\", \"work well\", \"sound good\", \"they re\"] ## work well we're goind to penalize\n\n    for each in voc:\n        idx = cv.vocabulary_[each]\n        if each == \"work well\": #### PENALIZING\n            for i in range(cv_train_reviews.shape[0]):\n                tr[i][idx] = int(tr[i][idx]//pen) \n            for i in range(cv_test_reviews.shape[0]):\n                te[i][idx] = int(te[i][idx]//pen)\n        else:##### INCREASING THE WEIGHT\n            for i in range(cv_train_reviews.shape[0]):\n                tr[i][idx] = tr[i][idx]*inc \n            for i in range(cv_test_reviews.shape[0]):\n                te[i][idx] = te[i][idx]*inc\n    tr_sm = sparse.csr_matrix(tr)\n    te_sm = sparse.csr_matrix(te)\n    \n    return tr_sm, te_sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(tr_sm)\ndisplay(cv_train_reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = {}\n\nfor inc in range(2,23,10):\n    for pen in range(2,3):\n        tr_sm, te_sm = tuning(pen,inc)\n        Lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=0) ## We are already penalizing!\n\n        lr_bow=Lr.fit(tr_sm,y_train)\n        print(lr_bow)\n\n        lr_bow_predict=Lr.predict(te_sm)\n\n        #Accuracy score \n        lr_bow_score=accuracy_score(y_test,lr_bow_predict)\n        print(\"lr_bow_score :\",round(lr_bow_score,6))\n        mod = \"Model: increase \" + str(inc) + \", penalize in \" + str(pen)\n        results[mod] = round(lr_bow_score,6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This didn't increase too much our performance (only 0.002). As we have a ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As we said previously, I'll remove certain words (in the \"rem\" list) from \"stop\". For that, we'll discad each of them from \"stop\":","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/amazon-music-reviews/Musical_instruments_reviews.csv\")\ndf.reviewText.fillna(\"\",inplace = True)\ndel df['reviewerID']\ndel df['asin']\ndel df['reviewerName']\ndel df['helpful']\ndel df['unixReviewTime']\ndel df['reviewTime']\n\ndf[\"quality\"] = df.loc[:,\"overall\"].apply(lambda x : \"good\" if x >= 4 else (\"neutral\" if x==3 else \"bad\" ))\ndf[\"strQuality\"] = df.loc[:,\"quality\"].apply(lambda x : 2 if x == \"good\" else (1 if x== \"neutral\" else 0 ))\n\ndf['text'] = df['reviewText'] + ' ' + df['summary']\ndel df['reviewText']\ndel df['summary']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for each in rem:\n    stop.discard(each)\nstop ## check the new list to see if it's smaller:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.apply(lemmatize_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final(X_data_full):\n    \n    cv = CountVectorizer(min_df = 0, max_features=1000, ngram_range =(2,3))\n    X_full_vector = cv.fit_transform(X_data_full).toarray()    \n    \n    full = X_full_vector\n    print(\"our full: \", full)\n    voc = [\"planet waves\", \"sound like\", \"work well\", \"sound good\", \"they re\"] ## work well we're goind to penalize\n    \n    try:\n        for each in voc:\n            idx = cv.vocabulary_[each]\n            if each == \"work well\": #### PENALIZING\n                for i in range(X_full_vector.shape[0]):\n                    full[i][idx] = int(full[i][idx]//2) \n            else:##### INCREASING THE WEIGHT\n                for i in range(X_full_vector.shape[0]):\n                    full[i][idx] = full[i][idx]*inc\n    except:\n        print(\"didn't work!\")\n    full_sm = sparse.csr_matrix(full)\n    \n    tfidf = TfidfTransformer()\n    X_data_full_tfidf = tfidf.fit_transform(full_sm).toarray()\n    \n    return X_data_full_tfidf\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = final(df.text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obs: to our ydata, we're getting the \"strQuality\" column as for the LSTM model we will use a \"categorical-crossentropy\" analyse. So we need our target as number classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,df.strQuality,test_size = 0.2 , random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XX = x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n\nembedding_size=32\nmax_words=5000\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_size, input_length=1000 )) #x_train.shape[0]))\nmodel.add(Bidirectional(LSTM(16, return_sequences = True)))\nmodel.add(Bidirectional(LSTM(16)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3,activation='softmax'))\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\ny_train_dummies = pd.get_dummies(y_train).values\nprint('shape label tensor: ', y_train_dummies.shape)\n\n#trainingggg the model\nmodel.fit(XX, y_train, epochs=2, batch_size=32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display(XX.shape)\n# display(XX[:int(len(XX)/5),:].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting categorical var in y_train to numerical var\ny_test_dummies = pd.get_dummies(y_test).values\nprint('Shape of Label tensor: ', y_test_dummies.shape)\n\n#model = load_model('../output/MusicalInstrumentReviews_correct.h5')\nscores = model.evaluate(XX[:int(len(XX)/4)+1,:], y_test)\n\nLSTM_accuracy = scores[1]*100\n\nprint('Test accuracy: ', scores[1]*100, '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test accuracy: 88.89%\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusions ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. NLP is not a trivial task!!\n2. when using LSTM, we may take a carefully look at the words that we want to remove from our data, or we can miss important and meaningful information\n3. we may have a lot of room to improvement, specially regarding the bag of words to be used. Playing with the ngram_range is definetly an important key to do so.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}