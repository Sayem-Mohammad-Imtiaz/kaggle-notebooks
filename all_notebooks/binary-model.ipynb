{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n!pip install bert-for-tf2\n!pip install h5py\n!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!unzip uncased_L-12_H-768_A-12.zip","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport keras\nfrom keras.models import model_from_json\nimport bert\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight as cw\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs(\"./model\", exist_ok=True)\n!mv uncased_L-12_H-768_A-12/ ./model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING_PATH = \"../input/sarcasm-preprocessed/sarcastic_preprocessed_train.csv\"\nTESTING_PATH = \"../input/sarcasm-preprocessed/sarcastic_preprocessed_test.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentAnalysisData:\n    DATA_COLUMN = \"text\"\n    LABEL_COLUMN = \"label\"\n    SAMPLE_WEIGHT = \"sample_weight\"\n\n    def __init__(self, train, test, tokenizer: FullTokenizer, max_seq_len=192):\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n\n        self.train_x, self.train_y, self.train_sample_weights = self._prepare_train(train)\n        self.test_x, self.test_y = self._prepare_test(test)\n\n        print(\"sample_weights\", self.train_sample_weights.shape)\n        print(\"max seq_len\", self.max_seq_len)\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n\n    def _prepare_test(self, df):\n        x, y = [], []\n        for _, row in tqdm(df.iterrows()):\n            try:\n                text, label = row[SentimentAnalysisData.DATA_COLUMN], row[SentimentAnalysisData.LABEL_COLUMN]\n                tokens = self.tokenizer.tokenize(text)\n                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n                x.append(token_ids)\n                y.append(label)\n            except:\n                pass\n        return np.array(x), np.array(y)\n    \n    def _prepare_train(self, df):\n        x, y, z = [], [], []\n        for _, row in tqdm(df.iterrows()):\n            try:\n                text, label, sample_weight= row[SentimentAnalysisData.DATA_COLUMN], row[SentimentAnalysisData.LABEL_COLUMN], row[SentimentAnalysisData.SAMPLE_WEIGHT]\n                tokens = self.tokenizer.tokenize(text)\n                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n                x.append(token_ids)\n                y.append(label)\n                z.append(sample_weight)\n            except:\n                pass\n        return np.array(x), np.array(y), np.array(z)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n            x.append(np.array(input_ids))\n        return np.array(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_NAME=\"uncased_L-12_H-768_A-12\"\nbert_ckpt_dir = os.path.join(\"./model/\", BERT_MODEL_NAME)\nBERT_CKPT_FILE = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")\nVOCAB_PATH =os.path.join(bert_ckpt_dir, \"vocab.txt\")\ntokenizer = FullTokenizer(vocab_file = \"./model/uncased_L-12_H-768_A-12/vocab.txt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \n# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef create_model(max_seq_len, num_classes, bert_ckpt_file = BERT_CKPT_FILE):\n    \n    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n        bc = StockBertConfig.from_json_string(reader.read())\n        bert_params = map_stock_config_to_params(bc)\n        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n    bert_output = bert(input_ids)\n\n    print(\"bert shape\", bert_output.shape)\n    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n    cls_out = keras.layers.Dropout(0.5)(cls_out)\n    logits = keras.layers.Dense(units=1024, activation=\"tanh\")(cls_out)\n    logits = keras.layers.Dropout(0.2)(logits)\n    logits = keras.layers.Dense(units=num_classes, activation=\"softmax\")(logits)\n\n    model = keras.Model(inputs=input_ids, outputs=logits)\n    bert.trainable = False\n    model.build(input_shape=(None, max_seq_len))\n    load_stock_weights(bert, bert_ckpt_file)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_filepath = '/tmp/checkpoint'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_acc',\n    mode='max',\n    save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with tpu_strategy.scope():\nMAX_SEQ_LEN = 100\ntokenizer = FullTokenizer(vocab_file=VOCAB_PATH)\n\ntrain_val = pd.read_csv(TRAINING_PATH)\ntest = pd.read_csv(TRAINING_PATH)\ndata = SentimentAnalysisData(train_val, test, tokenizer, max_seq_len=MAX_SEQ_LEN)\n\nmodel = create_model(data.max_seq_len, 5)\nmodel.summary()\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n)\n\n\n# Model weights are saved at the end of every epoch, if it's the best seen\n# so far.\nhistory = model.fit(\n    x=data.train_x,\n    y=data.train_y,\n    validation_split=0.2,\n    batch_size= 32,\n    sample_weight = data.train_sample_weights,\n    shuffle=True,\n    epochs=12,\n    verbose=1,\n    callbacks=[model_checkpoint_callback]\n)\n    \n# The model weights (that are considered the best) are loaded into the model.\nmodel.load_weights(checkpoint_filepath)\n\n_, test_acc = model.evaluate(data.test_x, data.test_y)\n_, train_acc = model.evaluate(data.train_x, data.train_y)\nprint(\"Test Accuracy:\" + str(test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"json_file = model.to_json()\njson_file_path = \"model_json.json\"\nwith open(json_file_path, \"w\") as saved_model:\n   saved_model.write(json_file)\n# serialize weights to HDF5\nmodel.save_weights(h5_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}