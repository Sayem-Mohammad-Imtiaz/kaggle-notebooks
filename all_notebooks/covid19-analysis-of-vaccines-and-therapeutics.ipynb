{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"def update_word_count_presence_in_text(fileId,inputText):\n    global key2DocumentCount,keyColumnNames\n    for keyName in keyColumnNames :\n        count = inputText.count(keyName)\n        if (count > 0): \n            existingValue = key2DocumentCount.get_value(fileId,keyName)\n            updatedValue = existingValue + count \n            key2DocumentCount.set_value(fileId,keyName,updatedValue) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fetch_eligible_document(inputKey): # Return a set of matching documents \n    global key2DocumentCount\n    columnList = key2DocumentCount[inputKey].to_list()\n    nonZeroIndexes = pd.Series(columnList).nonzero()\n    dataList = nonZeroIndexes[0] \n    dataList = dataList + 1\n    finalSetOfDocuments = set(dataList)\n    return finalSetOfDocuments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_shortlisted_textId_using_sentence_encoder(questionId,individualDocumentId,contextSentencesListPerQuestion) : # returns list of text ids\n    global fileList,embed\n    \n    textcount = 0\n    finalBatchOfSentence = []\n    textSet = {}\n    finalShortListedTextIds = []\n    \n    #Open the specific document file\n    filePath = fileList[individualDocumentId][1]\n    with open(filePath,'r') as f:\n        fileContent = json.load(f)\n    body_text = fileContent['body_text']\n    for textData in body_text :\n        actualText = textData['text'].lower()\n        sentencesInText = actualText.split('.')\n        finalBatchOfSentence =  contextSentencesListPerQuestion + sentencesInText\n        sentenceEmbedding = embed(finalBatchOfSentence)\n        decision = should_batch_be_considered(sentenceEmbedding,len(contextSentencesListPerQuestion))\n        if (decision) :\n            finalShortListedTextIds.append(textcount)\n        textcount = textcount + 1 \n    return finalShortListedTextIds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def should_batch_be_considered(sentenceEmbedding , countOfRefSentences) :\n    match_batching = False\n    corelation = np.inner(sentenceEmbedding,sentenceEmbedding)\n    related_entries = np.array(corelation[countOfRefSentences:,:countOfRefSentences])\n    grade_A_match = len(np.where(related_entries[:] >= 0.4)[0])\n    if (grade_A_match > 1) :\n        match_batching = True\n    return match_batching    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_result():\n    global finalResultPerOpenQuestions,referenceContextSentences,key2OpenQuestions\n    updatedFinalListOfData = []\n    shortlistedDocumentPerQuestion = {}\n    #Step 1 - Prepare the list of probable Documents for each of the Question Ids\n    for questionId in key2OpenQuestions.keys():\n        keysListPerQuestionId =  key2OpenQuestions[questionId]\n        for key in keysListPerQuestionId:\n            #Derive the documentId against the keys \n            shortlistedDocumentSet = {} \n            shortlistedDocumentSet =  fetch_eligible_document(key)\n            updatedDocumentSet = {}\n            if (questionId in shortlistedDocumentPerQuestion.keys() ):\n                updatedDocumentSet = shortlistedDocumentPerQuestion[questionId]\n                updatedDocumentSet.update(shortlistedDocumentSet)\n            else :\n                updatedDocumentSet = shortlistedDocumentSet\n            shortlistedDocumentPerQuestion[questionId] = updatedDocumentSet\n    #Step 2 - For each of the shorlisted document score the body_text against the reference sentences and prepare the finallist\n    for questionId in  shortlistedDocumentPerQuestion.keys() :\n        contextSentencesListPerQuestion = referenceContextSentences[questionId]\n        documentcnt = 0\n        print('Step2 Working on Question Id {}'.format(questionId))\n        documentSet = shortlistedDocumentPerQuestion[questionId]\n        totalNumberOfDocument = len(documentSet)\n        for individualDocumentId in documentSet :\n            documentcnt = documentcnt + 1 \n            print('Step2 - Processing document {}/{}'.format(documentcnt,totalNumberOfDocument))\n            shortlistTextIdsWithinDocument = []\n            shortlistTextIdsWithinDocument = get_shortlisted_textId_using_sentence_encoder(questionId,individualDocumentId,contextSentencesListPerQuestion)            \n            if (len(shortlistTextIdsWithinDocument) > 0 ) :\n                recordToBeInserted = {}\n                recordToBeInserted[individualDocumentId] = shortlistTextIdsWithinDocument\n                #Update the Master finalResultPerOpenQuestions \n                if (questionId in finalResultPerOpenQuestions.keys()) : \n                    updatedFinalListOfData = finalResultPerOpenQuestions[questionId]\n                    updatedFinalListOfData.append(recordToBeInserted)\n                else : \n                     updatedFinalListOfData.append(recordToBeInserted)\n                finalResultPerOpenQuestions[questionId] = updatedFinalListOfData\n    print(\"finalResultPerOpenQuestions-->\",finalResultPerOpenQuestions)\n            ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport json\nfrom IPython.display import FileLink\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfinalResultPerOpenQuestions = {} # Format -->{1(OpenQuestionId): [{1(document id):[number within the body_text eg- 1,5,6]}]}\nreferenceContextSentences = {} \nfileList = {}\nkeyColumnNames = ['naproxen','clarithromycin','minocycline','antibody-dependent enhancement','animal models','therapeutic',\n                  'antiviral agents','models','prioritize','distribute scarce','population','vaccine','standardize','prophylaxis',\n                 'enhanced disease','corona','response']\ninitializedData = {'naproxen':0,'clarithromycin':0,'minocycline':0,'antibody-dependent enhancement':0,\n                  'animal models':0,'therapeutic':0,\n                  'antiviral agents':0,'models':0,'prioritize':0,'distribute scarce':0,\n                  'population':0,'vaccine':0,'standardize':0,'prophylaxis':0,\n                 'enhanced disease':0,'corona':0,'response':0}\n\ncnt = 0 \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n       # print(os.path.join(dirname, filename))\n        if(filename.find('.json') >=0):\n            cnt = cnt + 1 \n            fileList[cnt] = [filename,os.path.join(dirname, filename)]\n\nreferenceContextSentences = {1:['Drugs experimented against COVID.','Clinical and bench trials of naproxen against COVID','Clinical and bench trials of clarithromycin against COVID','Clinical and bench trials of minocycline  against COVID-19'],\n                             2: ['Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients','Methods evaluating potential effect of vaccine on recipient.'],\n                            3: ['Exploration of use of best animal models and their predictive value for a human vaccine','Animal models and their value for a human vaccine'],\n                            4: ['Capabilities to discover a therapeutic  for the disease.','Cinical effectiveness studies to discover therapeutics, to include antiviral agents.'],\n                            5: ['Models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics',' Approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.'],\n                            6: ['Efforts at a universal coronavirus vaccine','Preparing an universal coronavirus vaccine'],\n                            7: ['developing animal models and standardize challenge studies for corona','developing models on animals','developing animal models and standardize challenge studies for covid19'],\n                            8: ['develop prophylaxis clinical studies'],\n                            9: ['Evaluate risk for enhanced disease after vaccination.','Post vaccination risks on new diseases'],\n                            10: ['Evaluate vaccine immune response','Response to Corona Vaccine']}\nkey2OpenQuestions = {1: ['naproxen','clarithromycin','minocycline'], \n                     2: ['antibody-dependent enhancement'],\n                   3: ['animal models'] , 4: ['therapeutic','antiviral agents'],\n                   5: ['models','prioritize','distribute scarce'],\n                   6: ['vaccine','corona'],7: ['standardize'] , \n                   8: ['prophylaxis'] , 9:['enhanced disease'] , 10: ['response','corona']}\n\n#key2OpenQuestions = {1: ['naproxen','clarithromycin','minocycline']}\n\nisDir = os.path.isdir('./model')\nprint(\"isDir->\",isDir)\nif (isDir != True):\n   !mkdir model\n   !curl -L \"https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed\" | tar -zxvC ./model\n\nembed = hub.load(\"./model\")    \nkey2DocumentCount = pd.DataFrame(initializedData,columns=keyColumnNames,index=fileList.keys())\n\n#Step1 - Prepare the initial list of shorlisted Document \nto_be_executed = True \nif(to_be_executed) :\n    for fileId in fileList.keys(): \n        fileData = {}\n        fileName = fileList[fileId][0]\n        filePath = fileList[fileId][1]\n        with open(filePath) as f:\n            fileData = json.load(f)\n        body_text = fileData['body_text']\n        for textData in body_text :\n            actualText = textData['text'].lower()\n            update_word_count_presence_in_text(fileId,actualText)\nkey2DocumentCount.head(1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step2 - Prepare the final list of shorlisted document \nprepare_result()\n\n#Step3 - Prepare for final dump to a csv file Format - Question  ,DocumentName ,Author Name ,Relevent Contexts\nprepare_final_result()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_final_result() :\n    global finalResultPerOpenQuestions,fileList\n    \n    is_first_record_per_question = True\n    is_first_record_per_document = True\n    area_focus = \" \"\n    documentName = \" \"\n    documentTitle = \" \"\n    relatedText =  \" \"\n    \n    resultantColumns = {'Area Under Focus','Document Name' , 'Title' , 'Related Context within document'}\n    initialData = {}\n    resultantDataFrame = pd.DataFrame(initialData,columns=resultantColumns)\n    \n    actual_opening_questions = {1: \"Effectiveness of drugs being developed and tried to treat COVID-19 patients.\",\n                                  2: \"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\",\n                                  3: \"Exploration of use of best animal models and their predictive value for a human vaccine.\",\n                                  4: \"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\",\n                                  5: \"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up.\",\n                                  6: \"Efforts targeted at a universal coronavirus vaccine.\",\n                                  7: \"Efforts to develop animal models and standardize challenge studies.\",\n                                  8: \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers.\",\n                                  9: \"Approaches to evaluate risk for enhanced disease after vaccination.\",\n                                  10: \"Assays to evaluate vaccine immune response and process development for vaccines.\"}\n    \n    for questionId in finalResultPerOpenQuestions.keys():\n        print(\"Preparing for Question Id\",questionId)\n        finalDocumentList = finalResultPerOpenQuestions[questionId]\n        is_first_record_per_question = True \n        for documentData in finalDocumentList :\n            is_first_record_per_document = True\n            documentId = list(documentData.keys())[0]\n            print(\"Preparing for documentId\",documentId)\n            documentTextList = documentData[documentId]\n            filePath = fileList[documentId][1]\n            with open(filePath ,'r') as f :\n                 documentContent = json.load(f)\n            documentBodyText = documentContent['body_text']\n            for individualTextIds in documentTextList : \n                relatedTextWithinDocument = documentBodyText[individualTextIds]['text']\n                if(is_first_record_per_question or is_first_record_per_document ) :\n                    area_focus = actual_opening_questions[questionId]\n                    documentName = fileList[documentId][0]\n                    documentTitle = documentContent['metadata']['title']\n                    relatedText =  relatedTextWithinDocument\n                    is_first_record_per_question = False\n                    is_first_record_per_document = False\n                else :\n                    area_focus = \" \"\n                    documentName = \" \"\n                    documentTitle = \" \"\n                    relatedText =  relatedTextWithinDocument\n                recordToInsert = {'Area Under Focus' : area_focus,'Document Name' : documentName, \n                                  'Title': documentTitle , 'Related Context within document' : relatedText}\n                \n                recordDataFrame = pd.DataFrame([recordToInsert])\n                print(\"record to insert -->\",recordDataFrame)\n                resultantDataFrame = pd.concat([resultantDataFrame,recordDataFrame],ignore_index=True,sort=False)\n        #print (\"Final Result -->\",resultantDataFrame.head(800))\n        #Moving the data to csv \n        isDir = os.path.isdir('./result')\n        print(\"isDirpresent->\",isDir)\n        if (isDir != True):\n           !mkdir result\n        resultantFileName = str(questionId) + \"_\" + \"result.csv\"\n        resultantDataFrame.to_csv('./result/' + resultantFileName) \n        resultantDataFrame = pd.DataFrame(initialData,columns=resultantColumns)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lrt ./result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,11)\n    fileName = \"./result/\" + str(i) + \"_result.csv\"\n    FileLink(fileName)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lrt ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}