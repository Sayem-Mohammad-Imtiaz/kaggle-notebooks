{"cells":[{"cell_type":"markdown","source":"Greetings! This is an auto generated kernel template, below here, you may find delightful insights regarding the dataset allowing you to jump start on your kernel creation!","metadata":{},"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\r\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\nimport matplotlib.pyplot as plt # plotting\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nimport os\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.preprocessing import StandardScaler\r\n# There is 1 csv file in the current version of the dataset:\nprint(os.listdir('../input'))","metadata":{},"outputs":[],"execution_count":0},{"cell_type":"code","source":"# Plot the PCA with either 2 or 3 reduced components\r\ndef plotPCA(df, nComponents):\n\tdf = df.select_dtypes(include =[np.number]) # keep only numerical columns\r\n\tdf = df.dropna('columns') # drop columns with NaN\r\n\tif df.shape[1] < nComponents:\r\n\t\tprint(f'The number of numeric columns ({df.shape[1]}) is less than the number of PCA components ({nComponents})')\r\n\t\treturn\r\n\tdf = df.astype('float64') # Cast to float for sklearn functions\r\n\tdf = StandardScaler().fit_transform(df) # Standardize features by removing the mean and scaling to unit variance\r\n\tpca = PCA(n_components = nComponents)\r\n\tprincipalComponents = pca.fit_transform(df)\r\n\tprincipalDf = pd.DataFrame(data = principalComponents, columns = ['Principal Component ' + str(i) for i in range(1, nComponents + 1)])\r\n\tfig = plt.figure(figsize = (8, 8))\r\n\tif (nComponents == 3):\r\n\t\tax = fig.add_subplot(111, projection = '3d')\r\n\t\tax.set_xlabel('Principal Component 1', fontsize = 15)\r\n\t\tax.set_ylabel('Principal Component 2', fontsize = 15)\r\n\t\tax.set_zlabel('Principal Component 3', fontsize = 15)\r\n\t\tax.set_title('3 component PCA', fontsize = 20)\r\n\t\tax.scatter(xs = principalDf.iloc[:, 0], ys = principalDf.iloc[:, 1], zs = principalDf.iloc[:, 2])\r\n\telse:\r\n\t\tax = fig.add_subplot(111)\r\n\t\tax.set_xlabel('Principal Component 1', fontsize = 15)\r\n\t\tax.set_ylabel('Principal Component 2', fontsize = 15)\r\n\t\tax.set_title('2 component PCA', fontsize = 20)\r\n\t\tax.scatter(x = principalDf.iloc[:, 0], y = principalDf.iloc[:, 1])\r\n","metadata":{},"outputs":[],"execution_count":0},{"cell_type":"code","source":"# Histogram of column data\r\ndef plotHistogram(df, nHistogramShown, nHistogramPerRow):\n\tnRow, nCol = df.shape\r\n\tcolumnNames = list(df)\r\n\tnHistRow = (nCol + nHistogramPerRow - 1) / nHistogramPerRow\r\n\tplt.figure(num=None, figsize=(6*nHistogramPerRow, 5*nHistRow), dpi=80, facecolor='w', edgecolor='k')\n\tfor i in range(min(nCol, nHistogramShown)):\r\n\t\tplt.subplot(nHistRow, nHistogramPerRow, i+1)\r\n\t\tdf.iloc[:,i].hist()\r\n\t\tplt.ylabel('counts')\r\n\t\tplt.title(f'{columnNames[i]} (column {i})')\r\n\tplt.show()\r\n","metadata":{},"outputs":[],"execution_count":0},{"cell_type":"code","source":"# Correlation matrix\r\ndef plotCorrelationMatrix(df, graphWidth):\n\tplt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\r\n\tcorr = df.corr()\r\n\tcorrMat = plt.matshow(corr, fignum = 1)\r\n\tplt.xticks(range(len(corr.columns)), corr.columns)\r\n\tplt.yticks(range(len(corr.columns)), corr.columns)\r\n\tplt.colorbar(corrMat)\r\n\tplt.title(f'Correlation Matrix for {df.name}')\r\n\tplt.show()\r\n","metadata":{},"outputs":[],"execution_count":0},{"cell_type":"code","source":"# Scatter and density plots\r\ndef plotScatterMatrix(df, plotSize, textSize):\n\tdf = df.select_dtypes(include =[np.number]) # keep only numerical columns\r\n\t# Remove rows and columns that would lead to df being singular\r\n\tdf = df.dropna('columns')\r\n\tdf = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r\n\tcolumnNames = list(df)\r\n\tif len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n\t\tcolumnNames = columnNames[:10]\n\tdf = df[columnNames]\r\n\tax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\r\n\tcorrs = df.corr().values\r\n\tfor i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\r\n\t\tax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\r\n\tplt.suptitle('Scatter and Density Plot')\r\n\tplt.show()\r\n","metadata":{},"outputs":[],"execution_count":0},{"cell_type":"code","source":"nRowsRead = 100 # specify 'None' if want to read whole file\ndf0 = pd.read_csv('../input/benhamner.csv', delimiter=',', nrows = nRowsRead)\ndf0.name = 'benhamner.csv'\n# benhamner.csv has 2702 rows in reality, but we are only loading/previewing the first 100 rows\nnRow, nCol = df0.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ncolumnNames = list(df0)\n\n# Histogram of sampled columns\r\nplotHistogram(df0, 10, 5)\n\n# Correlation matrix\r\nplotCorrelationMatrix(df0, 8)\n\n# Scatter and density plots\r\nplotScatterMatrix(df0, 15, 6)\n\n# PCA Analysis\r\nplotPCA(df0, 2) # 2D PCA\nplotPCA(df0, 3) # 3D PCA\n","metadata":{},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}