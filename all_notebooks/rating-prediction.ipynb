{"cells":[{"metadata":{"id":"FpcmOaAzDSza"},"cell_type":"markdown","source":"# Final Project\n### Anoop Kunjumon Scariah\n### video demo [click here](https://youtu.be/yQJyGSp7OLc)\n### Hosted Application [click here](https://ratingprediction-ml-nlp.herokuapp.com/)\n### Github [click here](https://github.com/AnoopKunju/RatingPredictor)\n"},{"metadata":{"id":"eycK10kgtUsL","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport spacy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport nltk\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"id":"SdipW6cH1ii8","outputId":"7b94afa7-fa16-4296-ae96-95929d74ffdb","trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"id":"KxWrWRr7D5Q2","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/boardgamegeek-reviews/bgg-15m-reviews.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"xwxdvqatFLHj"},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"id":"aFRhNFEvEAIx","outputId":"19de38be-c599-404d-bbf0-61e26d1752d6","trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"my1qz5d6FTrl"},"cell_type":"markdown","source":"*The dataset consist of 6 columns the columns ID, name and user is not taken into consideration for this projects as we are preparing a model which can predict rating using NLP*"},{"metadata":{"id":"xkLtAKCbER6W","outputId":"da789d6d-1f75-430b-c9cb-9f94eedf9b6c","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"WE1FKIQ1Gw-5","outputId":"7d4a69b4-25ab-419a-d913-1c5578127744","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"DFRauR2dGauG","outputId":"7ca34736-94a5-4e6c-a4fb-a4e12bc11843","trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 0', 'user','ID','name'], axis=1, inplace= True) #dropping columns which is not fruitful for this project\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"C08uFfM8GFPc"},"cell_type":"markdown","source":"As we can see in the comment column it consist of Null data which doesnot make any contribution to our prediction so we will first clean the data"},{"metadata":{"id":"cpvOaYoDIhJD","outputId":"cb81980d-f186-4b46-ae33-bb4da6b27269","trusted":true},"cell_type":"code","source":"df.dropna(inplace= True)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"RP0tGGxhOaxe","outputId":"55e6548f-d4e5-4a92-e15f-cec5b2906cfa","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"WE4DJCI8QFNf"},"cell_type":"markdown","source":"*We can see that after doing the cleaning for NULL value we have found that the data size srink from 15 million to almost 3 million*"},{"metadata":{"id":"uKJLJIZgWyZ6"},"cell_type":"markdown","source":"### Undersatnding Data  "},{"metadata":{"id":"FUauIpJcTCBU","outputId":"0b6a48e7-655b-4f0f-9c98-93175cd155cd","trusted":true},"cell_type":"code","source":"plt.hist(df['rating'], bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"07MpozA8XJ4M"},"cell_type":"markdown","source":"*From the above plot we can see that the ratings are not just integer value they in float too so we the model genration purpose we need to round the rating to nearest integer*"},{"metadata":{"id":"QKRey6XqU8Rx","trusted":true},"cell_type":"code","source":"df.rating = df['rating'].round()","execution_count":null,"outputs":[]},{"metadata":{"id":"bsubP9xkZQNS","outputId":"19edaf21-1047-4f82-9a7b-2ef8d35a06a7","trusted":true},"cell_type":"code","source":"plt.hist(df['rating'], bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"EGqLX0BnfyM7"},"cell_type":"markdown","source":"*From the above plot we can understand that the dataset consist of comments which are rated 6 to 8 are more in number* "},{"metadata":{"id":"LAgUvlVpnbaI"},"cell_type":"markdown","source":"### Data Reduciton \n*As the data consist of almost 3 million data and requires a huge amount of computation power therefore for carrying out preprocessing and model generation im using 10% of the data create the model and carry out testing on it.*"},{"metadata":{"id":"T9NkEE6Wr225","outputId":"913db4b7-e93b-4ed0-fe3d-bb47e461fc24","trusted":true},"cell_type":"code","source":"df_final = df.sample(frac=0.1, replace=False, random_state=1)\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"8BfIP57SrQ2b","outputId":"b42710b2-6da6-4530-b4e9-cc7f85dffc54","trusted":true},"cell_type":"code","source":"plt.hist(df_final['rating'], bins = 50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"BqkYPlADsTpu"},"cell_type":"markdown","source":"*From the above graph we can see that the data is split in the same fashion as the orginal data of 3 million , therefore we can be sure there is no loss of information*"},{"metadata":{"id":"dleG-knhxs8t","trusted":true},"cell_type":"code","source":"X = df_final['comment'].values\nY = df_final['rating'].values","execution_count":null,"outputs":[]},{"metadata":{"id":"vhBmEm6mitUj"},"cell_type":"markdown","source":"# Removing StopWords, Punctuatio and Integers from Comment"},{"metadata":{"id":"FwiaUoTN9Fwo","trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30)","execution_count":null,"outputs":[]},{"metadata":{"id":"zVavK-ldDplB","outputId":"d15ebda8-115a-46bf-ce09-37769c29b282","trusted":true},"cell_type":"code","source":"print(np.where(Y_train == 3))","execution_count":null,"outputs":[]},{"metadata":{"id":"9hwyNK6Pirhg","trusted":true},"cell_type":"code","source":"exclude_list = string.digits + string.punctuation #removing the punctuation & digits\nstopwords = nltk.corpus.stopwords.words(\"english\")\n\ndef cleaning(text):\n  nlp.max_length = len(text)\n  # Preprocessing by removing Uppercase and removing the punctuation & digits\n  raw_text = text.lower() #coverting to lower case\n  # Lemmatization\n  table = str.maketrans(exclude_list,len(exclude_list)*\" \")\n  raw_text = raw_text.translate(table)\n  doc = nlp(raw_text, disable = ['ner', 'parser']) #Loading into the 'en model' Lemmatization\n  lemmatized_output = \" \".join([token.lemma_ for token in doc]) #Lemmatize list of words and join\n  # Removing Stopwords\n  words = lemmatized_output.split()\n  clean_text = \" \".join([w for w in words if w not in stopwords])\n  return clean_text","execution_count":null,"outputs":[]},{"metadata":{"id":"Bu8pnajxhSA7","trusted":true},"cell_type":"code","source":"Xtrain_clean = list()\nfor text in X_train:\n  Xtrain_clean.append(cleaning(text))","execution_count":null,"outputs":[]},{"metadata":{"id":"Ce7_qQAnD9yo","trusted":true},"cell_type":"code","source":"Xtrain_clean = list(map(lambda st: str.replace(st,'-PRON-',''), Xtrain_clean)) #removing string -PRON-","execution_count":null,"outputs":[]},{"metadata":{"id":"6EODienQOFl9"},"cell_type":"markdown","source":"# Model Creation and Evaluation"},{"metadata":{"id":"PujLcp2qQ2E1","trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words = 'english', max_df = 0.5 , max_features= 15000)\ndtm = vectorizer.fit_transform(Xtrain_clean)\ndtm_test = vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"WB7wUvGdBicd"},"cell_type":"markdown","source":"### MultiNomial Naive Bayes"},{"metadata":{"id":"YfjP4ZDZVuQP","outputId":"e72f4d8f-e030-4955-ceec-fab9d175753b","trusted":true},"cell_type":"code","source":"NaiveBayes = MultinomialNB()\nNaiveBayes.fit(dtm, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"Smz_VnwUXGkA","trusted":true},"cell_type":"code","source":"Y_pred = NaiveBayes.predict(dtm_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"n7xTJGGjZgBW","outputId":"6068ec67-823d-4349-ed13-b999c6e03c8c","trusted":true},"cell_type":"code","source":"print(\"MUltinomial Naive Bayes Test Accuracy is {} %\".format(accuracy_score(Y_test, Y_pred)*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"KxIRbUwJFZ4M"},"cell_type":"markdown","source":"### Linear SVC"},{"metadata":{"id":"qr3Rny1dmM0P","outputId":"d1a8a4e0-1de7-47d0-a48b-d70b3796d831","trusted":true},"cell_type":"code","source":"SVC = LinearSVC()\nSVC.fit(dtm,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"sZKihbb4nCup","trusted":true},"cell_type":"code","source":"Y_pred_svm = SVC.predict(dtm_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"HbC8vwmGwcOA","outputId":"a5a8a7b1-2a68-420a-90de-3ac12eef4426","trusted":true},"cell_type":"code","source":"print(\"Linear SVC Test Accuracy is {} %\".format(accuracy_score(Y_test, Y_pred_svm)*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"y-NW16naZ4Ud"},"cell_type":"markdown","source":"# Hyperparameter tunning "},{"metadata":{"id":"xE0yXqGLf__0"},"cell_type":"markdown","source":"Hypeparameter for Multinomial Naive Bayes"},{"metadata":{"id":"jKzs_xwfi5EA","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"id":"ugcGt6gWrdfs","outputId":"b2943107-9a93-402f-84f8-e33de2dcd93b","trusted":true},"cell_type":"code","source":"alpha = list(range(1, 11, 1))\naccuracy = dict()\nfor x in alpha:\n  MNBC  = MultinomialNB(alpha= x)\n  scores = cross_val_score(MNBC, dtm, Y_train, cv=3, scoring='accuracy')\n  accuracy[x] = scores.mean()\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"vQTdGm6Qk1Ah","outputId":"4a9ec922-0d2d-4200-ba9a-9957d428eca8","trusted":true},"cell_type":"code","source":"naiveDF = pd.DataFrame.from_dict(accuracy,orient='index')\nnaiveDF.sort_values(0,ascending= False)","execution_count":null,"outputs":[]},{"metadata":{"id":"jLvT5O39ZGuF"},"cell_type":"markdown","source":"*from above we can see that the alpha was 1 outperform others in the case of Multinomial Naive Bayes therefore keeping alpha 1 as the hyperparameter for the Naive Bayes model*"},{"metadata":{"id":"SfRtwO9Z7L1S"},"cell_type":"markdown","source":"Hypeparameter for Linear SVC"},{"metadata":{"id":"qgtRzR77aA4p","trusted":true},"cell_type":"code","source":"SVC_acc = dict()\nSVC_10 = LinearSVC(C= 10)\nscores = cross_val_score(SVC, dtm, Y_train, cv=3, scoring='accuracy')\nSVC_acc[10] = scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"lD6ZS42q4d-f","trusted":true},"cell_type":"code","source":"SVC_100 = LinearSVC(C= 100)\nscores100 = cross_val_score(SVC, dtm, Y_train, cv=3, scoring='accuracy')\nSVC_acc[100] = scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"oJKvePWf41JH","trusted":true},"cell_type":"code","source":"SVC_1 = LinearSVC()\nscores = cross_val_score(SVC, dtm, Y_train, cv=3, scoring='accuracy')\nSVC_acc[1] = scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"QrSTQXIz6ly4","outputId":"3d7d0e0a-3c8e-47ae-9277-0d73e77f1e9a","trusted":true},"cell_type":"code","source":"SVCDF = pd.DataFrame.from_dict(SVC_acc,orient='index')\nSVCDF.sort_values(0,ascending= False)","execution_count":null,"outputs":[]},{"metadata":{"id":"J5rP1iel63yt"},"cell_type":"markdown","source":"*from the above comparison we can see that the hyperparameter C= 100 performs better for Linear SVC model*"},{"metadata":{"id":"V_0_zJQW75w_"},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"id":"ZlzHExmt9XWt","outputId":"c17077aa-f888-4de0-aeda-681f5525c0ec","trusted":true},"cell_type":"code","source":"review = list()\ncomment = \"I hate the game\"\nreview.append(comment)\ndtm_rev = vectorizer.transform(review)\npred_naive = NaiveBayes.predict(dtm_rev)\npred_SVC = SVC.predict(dtm_rev)\nprint(\"prediction for NAIVE Bayes:\",pred_naive)\nprint(\"prediction for SVC:\",pred_SVC)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ktk5ejcH8Eei"},"cell_type":"markdown","source":"*As the model is been trained on partial amount of data due to computation limitation and as we can see that the data is not balance in the case of rating which we can understand by seeing the visulization of rating that the amount of comments with rating 8 is more in the dataset as compared to others, therefore which has lead to  error of overfitting the model for the class 8 rating data in Navie Bayes model where as SVC has performed in a proper fashion. In theroy and statics wise the Naive Bayes does perform good on the data but after observation we can understand that Naive Bayes model is Overfitted*"},{"metadata":{"id":"AnhA3yGP_y7i"},"cell_type":"markdown","source":"*Therefore for the application development I'm using Linear SVC model for prediction*"},{"metadata":{"id":"SI6MIIq2Z-gU"},"cell_type":"markdown","source":"# Saving the model"},{"metadata":{"id":"rIJnKwFxeXnL","trusted":true},"cell_type":"code","source":"# import pickle","execution_count":null,"outputs":[]},{"metadata":{"id":"_A6LH1eBeeIc","trusted":true},"cell_type":"code","source":"# pickle.dump(NaiveBayes,open('modelNaiveBayes.pkl','wb'))","execution_count":null,"outputs":[]},{"metadata":{"id":"ZKPOQRg1eiaC","trusted":true},"cell_type":"code","source":"# pickle.dump(SVC,open('modelSVM.pkl','wb'))","execution_count":null,"outputs":[]},{"metadata":{"id":"qdc_ZxlrewST","trusted":true},"cell_type":"code","source":"# pickle.dump(vectorizer,open('modelvectorizer.pkl','wb'))","execution_count":null,"outputs":[]},{"metadata":{"id":"dj6RLurNAVJZ"},"cell_type":"markdown","source":"# Reference \nhttps://monkeylearn.com/text-classification/\n\nhttps://www.kaggle.com/jvanelteren/collaborative-filtering-defining-similar-games\n\nhttps://www.kaggle.com/jvanelteren/exploring-the-13m-reviews-bgg-dataset\n"},{"metadata":{"id":"Pwlbsum5HQq5"},"cell_type":"markdown","source":"# Challenge \n The amount of data was very huge 15 million data the preprocessing itself took the life of my personal computer, because of which I implemented the whole  project using Google Colab still it was very time consuming for some algorithms to obtain fitting example SVM.\n \n Therotically the Naive bayes model was outperforming Lineasr SVC, but at the end when i used it for manual testing by passing the comments my self i understood it has been overfitted for the class 8 comments. \n\n"},{"metadata":{"id":"9JZWBKUODJlu","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}