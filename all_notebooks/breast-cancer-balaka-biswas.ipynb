{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib as ml\nimport matplotlib.pyplot as plt\n%matplotlib inline\nml.style.use('ggplot')\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nwarnings.filterwarnings('ignore')\n\n# Models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,recall_score,f1_score,confusion_matrix,roc_auc_score,roc_curve,auc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\nprint(data.shape)\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data processing and related EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unnamed: 32 is probably an erronous column in the dataset with all 569 values being NaN. So we drop it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['Unnamed: 32'],axis=1,inplace=True)\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding the labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['diagnosis_enc']=np.nan\nrep_dict = {'M': int(1), 'B': int(0)}\n\n\nfor i in rep_dict.keys():\n    data.loc[data['diagnosis']==i,'diagnosis_enc'] = int(rep_dict[i])\n    \n# Drop diagnosis\ndata.drop(columns=['diagnosis'],inplace=True)\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.hist(figsize=(30,20), color='green', bins=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Proportion of labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['lightslategray',] * 2\ncolors[1] = 'crimson'\nfig = go.Figure([go.Bar(x=['malignant','benign'], y=data.diagnosis_enc.value_counts().values, marker_color=colors)])\nfig.update_layout(title_text='Proportion Overview')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,20))\nsns.heatmap(data.corr(),annot=True,linewidth=0.1,linecolor='white')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_corr():\n    corr = pd.DataFrame(data.corr())\n    corr_dict = {}\n    for c in corr.columns:\n        max_corr = sorted(corr[c].values)[-2]\n        m_i = corr[corr[c] == max_corr].index[0]\n        corr_dict[c] = m_i\n    return corr_dict\n\n# For this dataset\nc_d = max_corr()\nprint(\"HIGHLY CORRELATED FEATURE FOR EACH FEATURE : \\n\")\nfor i in c_d.keys():\n    print(i,\" ; \",c_d[i],\"\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAINING AND PREDICTIONS","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. KNN\n### 2. Random Forest\n### Thresholding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(data.iloc[:,1:30].values)\ny = np.array(data.iloc[:,-1].values)\n# Scaling\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n# Split\ntrainx,testx,trainy,testy = train_test_split(X,y,test_size=0.2,random_state=4)\nprint(\"For X : trainx = {} , testx = {}\".format(trainx.shape,testx.shape))\nprint(\"For y : trainy = {} , testy = {}\".format(trainy.shape,testy.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Optimum value of n_neigbors for KNN classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy,recall = [],[]\nfor i in range(1,31):\n    knn = KNeighborsClassifier(n_neighbors=i,p=2)\n    knn.fit(trainx,trainy)\n    y_pred=knn.predict(testx)\n    accuracy.append(accuracy_score(y_pred,testy))\n    recall.append(recall_score(y_pred,testy))\n\n# Plot and find best hyperparameter\nplt.figure(figsize=(20,10))\nplt.plot(range(1,31),accuracy,marker=\"o\")\nplt.plot(range(1,31),recall,marker=\"o\")\nplt.legend([\"Accuracy\",\"Recall_Score\"])\nplt.title(\"Accuracy vs Recall_score\")\nplt.show()\nprint(\"Best n_neighbors value(accuracy) : \",np.argmax(accuracy)+1)\nprint(\"Best n_neighbors value(recall) : \",np.argmax(recall)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"CHOOSING THE BEST HYPERPARAMETER : \\n\")\n# Choosing n_neigbors = 17\nprint(\"n_neighbors = 17\")\nknn_n = KNeighborsClassifier(n_neighbors=17,p=2,algorithm='auto')\nknn_n.fit(trainx,trainy)\ny_pred_n=knn_n.predict(testx)\ntn,fp,fn,tp = confusion_matrix(y_pred_n,testy).ravel()\nprint(\"Accuracy score = \",(tp+tn)/(tp+fn+fp+tn))\nprint(confusion_matrix(y_pred_n,testy))\n\n# Choosing n_neighbors = 2\nprint(\"\\nn_neighbors = 2\")\nknn_n = KNeighborsClassifier(n_neighbors=2,p=2,algorithm='auto')\nknn_n.fit(trainx,trainy)\ny_pred_n=knn_n.predict(testx)\ntn,fp,fn,tp = confusion_matrix(y_pred_n,testy).ravel()\nprint(\"Accuracy score = \",(tp+tn)/(tp+fn+fp+tn))\nprint(confusion_matrix(y_pred_n,testy))\nacc_knn = (tp+tn)/(tp+fn+fp+tn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see from the two confusion matrices that number of False Positives(FP) for n_neighbors = 17 is lesser than that for n_neighbors = 2. So we conclude that n_neighbors = 17 is the best hyperparameter for this model/algorithm on this data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Finding optimum max_depth\n### 2. Finding optimum n_estimators","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy2,recall2 = [],[]\nfor i in range(1,11):\n    rf = RandomForestClassifier(max_depth = i,criterion='gini',max_features='auto')\n    rf.fit(trainx,trainy)\n    y_pred=rf.predict(testx)\n    accuracy2.append(accuracy_score(y_pred,testy))\n    recall2.append(recall_score(y_pred,testy))\n\n# Plot and find best hyperparameter\nplt.figure(figsize=(20,10))\nplt.plot(range(1,11),accuracy2,marker=\"o\")\nplt.plot(range(1,11),recall2,marker=\"o\")\nplt.legend([\"Accuracy\",\"Recall_Score\"])\nplt.title(\"Accuracy vs Recall_score\")\nplt.show()\nprint(\"Best max_depth value(accuracy) : \",np.argmax(accuracy2)+1)\nprint(\"Best max_depth value(recall) : \",np.argmax(recall2)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy3,recall3,choice = [],[],[]\nfor i in range(60,101,5):\n    choice.append(i)\n    rf = RandomForestClassifier(n_estimators = i,max_depth = 5,criterion='gini',max_features='auto')\n    rf.fit(trainx,trainy)\n    y_pred=rf.predict(testx)\n    accuracy3.append(accuracy_score(y_pred,testy))\n    recall3.append(recall_score(y_pred,testy))\n\n# Plot and find best hyperparameter\nplt.figure(figsize=(20,10))\nplt.plot(range(60,101,5),accuracy3,marker=\"o\")\nplt.plot(range(60,101,5),recall3,marker=\"o\")\nplt.legend([\"Accuracy\",\"Recall_Score\"])\nplt.title(\"Accuracy vs Recall_score\")\nplt.show()\nprint(\"Best n_estimators value(accuracy) : \",choice[np.argmax(accuracy3)])\nprint(\"Best n_estimators value(recall) : \",choice[np.argmax(recall3)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_n = RandomForestClassifier(n_estimators = choice[np.argmax(accuracy3)],max_depth = 5,criterion='gini',max_features='auto')\nrf_n.fit(trainx,trainy)\ny_pred_n=rf_n.predict(testx)\ntn,fp,fn,tp = confusion_matrix(y_pred_n,testy).ravel()\nprint(\"Accuracy score = \",(tp+tn)/(tp+fn+fp+tn))\nprint(\"Recall = \",recall_score(y_pred_n,testy),\"\\n\")\nprint(confusion_matrix(y_pred_n,testy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For n_estimators = 90 and max_depth = 5, the maximum accuracy achieved by the RandomForestClassifier model on this data is 93.85 %. Number of False Positives(FP) for this model is the same as in KNN(for n_neighbors = 17). But, number of False Negatives(FN) is higher than in KNN. So, for this data, RandomForestClassifier is not a better choice over KNN.\n# Plotting the ROC curves","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN\ny_scores = knn_n.predict_proba(testx)\nfptpk = y_scores[:,1]\nknn_score = roc_auc_score(testy,fptpk)\nprint(\"FOR KNN :\\nROC AUC score = \",knn_score)\nknn_fp,knn_tp,tk = roc_curve(testy,fptpk)   # Returns FPR, TPR and thresholds.\nauc_knn = auc(knn_fp,knn_tp)\nplt.figure(figsize=(20,10))\nplt.plot(knn_fp,knn_tp,marker='o',label=\"KNN ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC Curve for KNN\")\nplt.show()\n\n# Random Forest\ny_scoresf = rf_n.predict_proba(testx)\nfptp_rf = y_scoresf[:,1]\nrf_score = roc_auc_score(testy,fptp_rf)\nprint(\"FOR RANDOM FOREST :\\nROC AUC score = \",rf_score)\nrf_fp,rf_tp,_ = roc_curve(testy,fptp_rf)   # Returns FPR, TPR and thresholds.\nauc_rf = auc(rf_fp,rf_tp)\nplt.figure(figsize=(20,10))\nplt.plot(rf_fp,rf_tp,marker='o',label=\"Random Forest ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC Curve for Random Forest\")\nplt.show()\n\n# Comparing AUC\nprint(\"AUC for KNN = \",auc_knn)\nprint(\"AUC for Random Forest = \",auc_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the graphs and also by metrics.auc()(using trapezoidal rule), AUC of KNN > AUC of Random Forest. Therefore, KNN with n_neighbors = 17 is better suited to this data.\n# OBTAINING OPTIMAL THRESHOLDS FOR KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before optimization\nprint(\"BEFORE THRESHOLDING : \\n\")\nprint(\"Accuracy score = \",acc_knn)\nprint(\"F1 score = \",f1_score(knn_n.predict(testx),testy))\nprint(\"ROC AUC score = \",knn_score)\nprint(\"\\nConfusion Matrix : \")\nprint(confusion_matrix(knn_n.predict(testx),testy))\nprint(\"\\nROC curve :\")\nplt.plot(knn_fp,knn_tp,marker='o',label=\"KNN ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC Curve for KNN before Threshold Optimization\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We use Youden's J Statistic to compute optimal threshold value\n##### --> It is computed as : J = True_Posiitve_Rate + True_Negative_Rate - 1 = True_Positive_Rate - (1 - True_Negative_Rate) = True_Positive_Rate - False_Positive_Rate\n\n##### --> We find the maximum value of J. The threshold value corresponding to that value of J is the required threshold we're looking for.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"optimal = sorted(list(zip(np.abs(knn_tp - knn_fp),tk)),key = lambda i: i[0],reverse = True)[0][1]    # Sort on basis of thresholds in descending order\ny_pred_optimal = [1 if i >= optimal else 0 for i in fptpk]\n\n# After optimization\nprint(\"AFTER THRESHOLDING : \\n\")\nprint(\"Accuracy score = \",accuracy_score(testy,y_pred_optimal))\nprint(\"F1 score = \",f1_score(testy,y_pred_optimal))\nprint(\"\\nConfusion Matrix : \")\nprint(confusion_matrix(testy,y_pred_optimal))\nprint(\"\\nROC curve :\")\nknn_fp_n,knn_tp_n,_ = roc_curve(testy,y_pred_optimal)\nplt.plot(knn_fp_n,knn_tp_n,marker='o',label=\"KNN ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC Curve for KNN after Threshold Optimization\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}