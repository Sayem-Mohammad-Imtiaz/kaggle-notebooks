{"cells":[{"metadata":{"_uuid":"5f9946a6ea9de8917cbf72a02f0945588838de2b"},"cell_type":"markdown","source":"## Text EDA using lyrics from Taylor Swift :)\n\n**The dataset contains follwing fields\n**\n* Album name\n* Track title\n* Track number\n* Lyric text\n* Line number of the lyric in the track\n* Year of release of the album"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport os\nimport pandas as pd\nimport datetime as dt\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nwidth = 0.75\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nimport string\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nsns.set_palette(sns.color_palette('tab20', 20))\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom datetime import date, timedelta\nimport operator \nimport re\nimport spacy\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding\nimport spacy #load spacy\nnlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n#stops = stopwords.words(\"english\")\nfrom tqdm import  tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"../input\"))\nfrom IPython.display import IFrame\nfrom IPython.core.display import display, HTML\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/taylor_swift_lyrics.csv\",encoding = \"latin1\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8743354ff6ff34c7f7d51fe36b036e452071b8e5"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d7e6f813990c9327866b6864ecbf88f19c20775"},"cell_type":"markdown","source":"Basic Lyric Understanding "},{"metadata":{"trusted":true,"_uuid":"cf84aea56cccc0d225a6335911a596097d7852dd"},"cell_type":"code","source":"def get_features(df):    \n    data['lyric'] = data['lyric'].apply(lambda x:str(x))\n    data['total_length'] = data['lyric'].apply(len)\n    data['capitals'] = data['lyric'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n                                axis=1)\n    data['num_words'] = data.lyric.str.count('\\S+')\n    data['num_unique_words'] = data['lyric'].apply(lambda comment: len(set(w for w in comment.split())))\n    data['words_vs_unique'] = data['num_unique_words'] / df['num_words']  \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abaa1b60f95167929f9c56488913153af34fdde8"},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\ny1 = data[data['year'] == 2017]['lyric'].str.len()\nsns.distplot(y1, label='2017')\ny2 = data[data['year'] == 2014]['lyric'].str.len()\nsns.distplot(y2, label='2014')\ny3 = data[data['year'] == 2012]['lyric'].str.len()\nsns.distplot(y3, label='2012')\ny4 = data[data['year'] == 2010]['lyric'].str.len()\nsns.distplot(y4, label='2010')\ny5 = data[data['year'] == 2008]['lyric'].str.len()\nsns.distplot(y5, label='2008')\ny6 = data[data['year'] == 2006]['lyric'].str.len()\nsns.distplot(y6, label='2006')\nplt.title('Year Wise - Lyrics Lenght Distribution (Without Preprocessing)')\nplt.legend();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ef29a454102f8f82b1a98d9bf1e62cd7029bdc4"},"cell_type":"code","source":"train = get_features(data)\ndata_pair = data.filter(['year','total_length','capitals','caps_vs_length','num_words','num_unique_words','words_vs_unique'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbdb94dfa6e467aaded47428c729448a13b83e86"},"cell_type":"code","source":"data.head().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b0a9f415bc85b14fbdb66e64755b6cbf9197575"},"cell_type":"markdown","source":"### Pairplot allows us to see both distribution of single variables and relationships between two variables."},{"metadata":{"trusted":true,"_uuid":"bac3c9d591c3746c5c913922ecef85ac276b5157"},"cell_type":"code","source":"sns.pairplot(data_pair,hue='year',palette=\"husl\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a51733e4d4fe93d05b8fd4b2c2761db78245f71"},"cell_type":"markdown","source":"## Expanding English language contractions "},{"metadata":{"trusted":true,"_uuid":"285673db80a45dc48142ed76812a20f384b7bd21"},"cell_type":"code","source":"contraction_mapping_1 = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n                       \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" ,\n                       \"Isn't\":\"is not\", \"\\u200b\":\"\", \"It's\": \"it is\",\"I'm\": \"I am\",\"don't\":\"do not\",\"did't\":\"did not\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n                       \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n                       \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f19f90a4f1cabdee054808aeb127df1a667a1e54"},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48055b6d2d0ff1543a9a4bf845c984ea98fccbd0"},"cell_type":"code","source":"def get_features(df):    \n    data['Clean_Lyrics'] = data['Clean_Lyrics'].apply(lambda x:str(x))\n    data['total_length'] = data['Clean_Lyrics'].apply(len)\n    data['capitals'] = data['Clean_Lyrics'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n                                axis=1)\n    data['num_words'] = data.lyric.str.count('\\S+')\n    data['num_unique_words'] = data['Clean_Lyrics'].apply(lambda comment: len(set(w for w in comment.split())))\n    data['words_vs_unique'] = data['num_unique_words'] / df['num_words']  \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b38ccd1660f4d45feb3ba17901e5f0c0ea449386"},"cell_type":"code","source":"data['Clean_Lyrics'] = data['lyric'].apply(lambda x: clean_contractions(x, contraction_mapping_1))\n#Stopwords\ndata['Clean_Lyrics'] = data['Clean_Lyrics'].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\n#Re-calculate the features\ntrain = get_features(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed3d781d05ab17beecc466bcf8bed6b5392294a7","scrolled":false},"cell_type":"code","source":"data.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eada0896e076b870b37cb399d24f4705987a0c2"},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\ny1 = data[data['year'] == 2017]['Clean_Lyrics'].str.len()\nsns.distplot(y1, label='2017')\ny2 = data[data['year'] == 2014]['Clean_Lyrics'].str.len()\nsns.distplot(y2, label='2014')\ny3 = data[data['year'] == 2012]['Clean_Lyrics'].str.len()\nsns.distplot(y3, label='2012')\ny4 = data[data['year'] == 2010]['Clean_Lyrics'].str.len()\nsns.distplot(y4, label='2010')\ny5 = data[data['year'] == 2008]['Clean_Lyrics'].str.len()\nsns.distplot(y5, label='2008')\ny6 = data[data['year'] == 2006]['Clean_Lyrics'].str.len()\nsns.distplot(y6, label='2006')\nplt.title('Year Wise - Lyrics Lenght Distribution (After Preprocessing)')\nplt.legend();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b66529ad917c3756551530c4514158c7ddf3cd26"},"cell_type":"code","source":"data['year'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b51552d63005416ab54278337a8579f8a0094ac"},"cell_type":"code","source":"def ngram_extractor(text, n_gram):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Function to generate a dataframe with n_gram and top max_row frequencies\ndef generate_ngrams(df, col, n_gram, max_row):\n    temp_dict = defaultdict(int)\n    for question in df[col]:\n        for word in ngram_extractor(question, n_gram):\n            temp_dict[word] += 1\n    temp_df = pd.DataFrame(sorted(temp_dict.items(), key=lambda x: x[1])[::-1]).head(max_row)\n    temp_df.columns = [\"word\", \"wordcount\"]\n    return temp_df\n\ndef comparison_plot(df_1,df_2,col_1,col_2, space):\n    fig, ax = plt.subplots(1, 2, figsize=(20,10))\n    \n    sns.barplot(x=col_2, y=col_1, data=df_1, ax=ax[0], color=\"skyblue\")\n    sns.barplot(x=col_2, y=col_1, data=df_2, ax=ax[1], color=\"skyblue\")\n\n    ax[0].set_xlabel('Word count', size=14, color=\"green\")\n    ax[0].set_ylabel('Words', size=18, color=\"green\")\n    ax[0].set_title('Top words in 2017 Lyrics', size=18, color=\"green\")\n\n    ax[1].set_xlabel('Word count', size=14, color=\"green\")\n    ax[1].set_ylabel('Words', size=18, color=\"green\")\n    ax[1].set_title('Top words in 2008 Lyrics', size=18, color=\"green\")\n\n    fig.subplots_adjust(wspace=space)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80be65c8b159eb662281d773e3b466f54cb9a5ed"},"cell_type":"markdown","source":"## Ngram Lyrics Anaysis 2017 vs 2008 "},{"metadata":{"trusted":true,"_uuid":"0b8987fc93c9a99c927e0725d796f3b40f064e1c"},"cell_type":"code","source":"Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 1, 10)\nLyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 1, 10)\ncomparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d440f12743488731181fe5fbd5531f4c9abad572"},"cell_type":"markdown","source":"## Bigram Lyrics Anaysis 2017 vs 2008 "},{"metadata":{"trusted":true,"_uuid":"843ca29018c51d5f585ee8dd760277ab30152e01"},"cell_type":"code","source":"Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 2, 10)\nLyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 2, 10)\ncomparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae5bbfbd2f616222af2aac2b8c7b4c36c1fa559d"},"cell_type":"markdown","source":"## Trigram Lyrics Anaysis 2017 vs 2008 "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7af08b5855a10f365a97f14a8a8508943a0fc29f"},"cell_type":"code","source":"Lyrics_2017 = generate_ngrams(train[train[\"year\"]==2017], 'Clean_Lyrics', 3, 10)\nLyrics_2008 = generate_ngrams(data[data[\"year\"]==2008], 'Clean_Lyrics', 3, 10)\ncomparison_plot(Lyrics_2017,Lyrics_2008,'word','wordcount', 0.25)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2599ef735470e295c38a8409801c58d356f17382"},"cell_type":"markdown","source":"Scattertext is an open source tool for visualizing linguistic variation between document categories in a language-independent way. The tool presents a scatterplot, where each axis corresponds to the rank-frequency a term occurs in a category of documents. Through a tie-breaking strategy, the tool is able to display thousands of visible term-representing points and find space to legibly label hundreds of them. Scattertext also lends itself to a query-based visualization of how the use of terms with similar embeddings differs between document categories, as well as a visualization for comparing the importance scores of bag-of-words features to univariate metrics. "},{"metadata":{"trusted":true,"_uuid":"6184950c3114cb02b058b9c951be0ba68c9ee4ce"},"cell_type":"code","source":"import scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\ndata['parsed'] = data.Clean_Lyrics.apply(nlp)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"547a4201fbef834b039e355e5cdf2c9344febd8d"},"cell_type":"code","source":"corpus = st.CorpusFromParsedDocuments(data,\n                             category_col='album',\n                             parsed_col='parsed').build()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c5b8085f4adeb3b04cc6846bdacbedf0148bfc5"},"cell_type":"code","source":"html = st.produce_scattertext_explorer(corpus,\n          category='reputation',\n          category_name='reputation',\n          not_category_name='1989',\n          width_in_pixels=600,\n          minimum_term_frequency=5,\n          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c97a9964690b7671c3d595801db2c3f81f6443a"},"cell_type":"code","source":"filename = \"reputation-vs-1989.html\"\nopen(filename, 'wb').write(html.encode('utf-8'))\nIFrame(src=filename, width = 800, height=700)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1513ef7a02e3cf5e80d913cda51f9950084d6e2"},"cell_type":"markdown","source":"Thank you :)"},{"metadata":{"trusted":true,"_uuid":"4ae877cc53d43d17341ab94748267fc5974e6af6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}