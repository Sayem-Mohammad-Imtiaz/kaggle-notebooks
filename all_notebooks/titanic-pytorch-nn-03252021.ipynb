{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read data\ntrain_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print first five rows of train data\ntrain_data.head(n=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of rows in training data \", train_data.shape[0])\nprint(\"Total number of columns in training data \", train_data.shape[1])\nprint(\"Total number of rows in test data \", test_data.shape[0])\nprint(\"Total number of columns in test data \", test_data.shape[1])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization and Analysis\n# Visualizing the number of null values in both data\n\nplt.figure(figsize = (13,5))\nplt.bar(train_data.columns, train_data.isna().sum())\nplt.xlabel(\"Columns name\")\nplt.ylabel(\"Number of missing values in training data\")\nplt.show()\n# from the bar plot of missing value we can conclude that Cabin, Embarked and Cabin column has null value so, we \n# can either drop the entire row or can fill the nan value with some values like mean, meadian. \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 5))\nplt.bar(test_data.columns, test_data.isnull().sum().values, color='red')\nplt.xlabel(\"Columns name\")\nplt.ylabel(\"Number of missing values in test data\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing the Number of survived passenger\n\nsns.countplot('Survived', data=train_data)\nplt.show()\n# here we plot only for train_data as we donot have Survived column for test data,\n# This plot show that around 600 people died while around 300 survived","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing the number of passenger from different embarked column in train_data\nsns.countplot('Embarked', data = train_data)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing whether gender affect the survival rate or not\nsns.countplot('Survived', hue = 'Sex', data = train_data)\nplt.plot()   # show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing whether embarked place affects the survival rate or not\nsns.countplot('Survived', hue = 'Embarked', data = train_data)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualizing whether pclass affect the survial rate or not\nsns.countplot('Survived', hue = 'Pclass', data = train_data)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box and whisker plot\nsns.boxplot('Fare', data = train_data)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot('Age', data = train_data)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram\ninterval = 10\nvalue_for_bin = np.ceil((train_data.Age.max() - train_data.Age.min()) / interval).astype(int)\nplt.hist(train_data.Age, bins = value_for_bin)\nplt.xlabel('Age')\nplt.ylabel('Number')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10, 4))\nplt.hist(train_data.Fare, bins = 10, color = 'red')\nplt.xlabel('Fare')\nplt.ylabel('Number')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = sns.FacetGrid(train_data, col = 'Survived', row = 'Pclass', size = 2.2, aspect = 1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple Data Analysis¶\n\ncorr_train = train_data.corr()\nsns.heatmap(corr_train)\nplt.show()\n# this shows that SibSp and Parch columns are releted , so we can combine this two column to reduce the dimension\n# of our data.. this plot only works for columns with numercal data \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_data.groupby(['Sex', 'Survived']).Survived.count() * 100) / train_data.groupby('Sex').Survived.count()\n# this shows that female have around 74% chance of survival while male have around 81% chance of death","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train_data.groupby(['Pclass','Survived']).Survived.count() * 100) / train_data.groupby('Pclass').Survived.count()\n# this shows that people belonging to third class are likely to die while people in class one are likely to survive","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Survival rate on the basis of Embarked place\n(train_data.groupby(['Embarked','Survived']).Survived.count() * 100) / train_data.groupby('Embarked').Survived.count()\n# this shows that people who embarked from Southampton are likely to die\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.groupby(by=['Survived']).mean()[\"Age\"]\n# this show that average age of people who survived was around 28 years old","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Dealing with the Missing values¶\n\n# before filling the missing values, let's drop Cabin column from both data.\ntrain_data.drop('Cabin', axis = 1, inplace = True)\ntest_data.drop('Cabin', axis = 1, inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_data = [train_data, test_data]\n\nfor data in combined_data:\n    print(data.isnull().sum())\n    print (\"*\" * 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#filling the nan values fo Age and fare column with the mean while Embarked column with most_frequent value\n\nfor data in combined_data:\n    data.Age.fillna(data.Age.mean(), inplace = True)\n    data.Fare.fillna(data.Fare.mean(), inplace = True)\n\n    # from visualization we know that Southamptom is most frequent Embarked place so, filling the missing value \n# with 'S'\ntrain_data.Embarked.fillna('S', inplace = True)\n\n# we simply can use SimpleImputer class form the sklearn to deal with the missing value\n# from sklearn.impute import SimpleImputer\n# impute = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n# train_data[['Age']] = impute.fit_transform(train_data[['Age']])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting a categorical feature¶  label encoding\n# Let's start by converting Sex feature to categorical female=1 and male=0\n\ndef change_gender(x):\n    if x == 'male':\n        return 0\n    elif x == 'female':\n        return 1\ntrain_data.Sex = train_data.Sex.apply(change_gender)\ntest_data.Sex = test_data.Sex.apply(change_gender)\n\n# we simply can use mapfunction to change the gender\n# train_data.Sex = train_data.Sex.map({'female':1, 'male':0})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using map funcion to change the Embarked column S = 1, C = 2, Q = 0\nchange = {'S':1, 'C':2, 'Q':0}\ntrain_data.Embarked = train_data.Embarked.map(change)\ntest_data.Embarked = test_data.Embarked.map(change)\n\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Extraction**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#while visualizing the correlation heatmap we came to know that Sibsp and Parch columns were closely related so lets created new column called Alone using this two columns -------> 1 = Alone , 0 = not Alone\n\ntrain_data['Alone'] = train_data.SibSp + train_data.Parch\ntest_data['Alone'] = test_data.SibSp + test_data.Parch\n\ntrain_data.Alone = train_data.Alone.apply(lambda x: 1 if x == 0 else 0)\ntest_data.Alone = test_data.Alone.apply(lambda x: 1 if x == 0 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop SibSp and Parch\ntrain_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nCreating new feature Title extracting from existing feature Name\n","metadata":{}},{"cell_type":"code","source":"train_data.Name.str.extract('([A-Za-z]+)\\.', expand = False).unique().size\n\n# create the Title feature which contain the title of the passenger and drop Name column\nfor data in combined_data:\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand = False)\n    data.drop('Name', axis = 1, inplace = True)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.Title.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.Title.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace least occuring title in the data with rare\nleast_occuring = [ 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Dona',\n       'Jonkheer']\nfor data in combined_data:\n    data.Title = data.Title.replace(least_occuring, 'Rare')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#perform title mapping in order to change to ordinal\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor data in combined_data:\n    data['Title'] = data['Title'].map(title_mapping)\n\ntrain_data.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# columns_to_drop = ['PassengerId','Ticket']\ncolumns_to_drop = ['PassengerId', 'Ticket']\ntrain_data.drop(columns_to_drop, axis = 1, inplace = True)\ntest_data.drop(columns_to_drop[1], axis = 1, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Binning Age and Fare columns**","metadata":{}},{"cell_type":"code","source":"for dataset in combined_data:\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for data in combined_data:\n    data.loc[data['Fare'] < 30, 'Fare'] = 1\n    data.loc[(data['Fare'] >= 30) & (data['Fare'] < 50),'Fare'] = 2\n    data.loc[(data['Fare'] >= 50) & (data['Fare'] < 100),'Fare'] = 3\n    data.loc[(data['Fare'] >= 100),'Fare'] = 4\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_train = train_data.corr()\nsns.heatmap(corr_train)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**preparing training and testing data **","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu\nUSE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create ANN Model\nclass ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct / float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}