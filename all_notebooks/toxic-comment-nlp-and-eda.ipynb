{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bs4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nimport pickle\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read dataset\ndf = pd.read_csv('../input/jigsaw-toxic-comment-train-and-test/train.csv')\n\n# first few rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So in this notebook we are going to focus on weather a comment is toxic or not.We only need toxic and commment column so we will going to others drop"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop columns\ndf.drop(['id','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of the dataset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see weather our dataset is balanced or imbalanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['toxic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['toxic'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can clearly see that our dataset is imbalanced dataset.We will later handle with it till then we will do data visualization after all first we need to understand our data"},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Number_of_words'] = df['comment_text'].apply(lambda x:len(str(x).split()))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can see that we have maximum 1411 words in our sentence and average length is 67.The minimum words in sentence is 1,let's see what are those sentences and how many sentences are there"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of sentences having one word are',len(df[df['Number_of_words']==1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['Number_of_words']==1]['comment_text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So basically there are link,random words and numbers, so there are no one word sentence having some meaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(df['Number_of_words'],kde = False,color=\"red\",bins=200)\nplt.title(\"Frequency distribution of number of words for each text extracted\", size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see toxic and non-toxic comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# toxic comments\ntoxic_comments = df[df['toxic'] ==1]['comment_text']\ntoxic_comments.reset_index(inplace=True,drop=True)\nfor i in range(5):\n    print(toxic_comments[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non toxic comments\nnon_toxic_comments = df[df['toxic'] ==0]['comment_text']\nnon_toxic_comments.reset_index(inplace=True,drop=True)\nfor i in range(5):\n    print(non_toxic_comments[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of characters in sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['toxic']==1]['comment_text'].str.len()\nax1.hist(text_len,color='orange')\nax1.set_title('Toxic Comment')\ntext_len=df[df['toxic']==0]['comment_text'].str.len()\nax2.hist(text_len,color='yellow')\nax2.set_title('Non-Toxic Commet')\nfig.suptitle('Characters in Sentence')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of words in each text"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['toxic']==1]['comment_text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Toxic Comments')\ntext_len=df[df['toxic']==0]['comment_text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='b')\nax2.set_title('Non-Toxic Comment')\nfig.suptitle('Words in Sentence')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tri-gram"},{"metadata":{"trusted":true},"cell_type":"code","source":"# toxic\ntoxic_text = ' '.join(df.loc[df.toxic == 1, 'comment_text'].values)\ntoxic_text_trigrams = [i for i in ngrams(toxic_text.split(), 3)]\nCounter(toxic_text_trigrams).most_common(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non-toxic\nnon_toxic_text = ' '.join(df.loc[df.toxic == 0, 'comment_text'].values)\nnon_toxic_text_trigrams = [i for i in ngrams(non_toxic_text.split(), 3)]\nCounter(non_toxic_text_trigrams).most_common(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud of toxic and non-toxic comment\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(toxic_comments))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Toxic Comments',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_toxic_comments))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Toxic Comments',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"It's time to clean our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n            final_text.append(i.strip().lower())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['comment_text']=df['comment_text'].apply(denoise_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see our cleaned data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ORIGINAL SENTENCE :',non_toxic_comments[0])\nprint('-'*100)\nprint('CLEANED SENTENCE :',df['comment_text'][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dependent and independent variable\nX = df['comment_text']\ny = df['toxic']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# countvectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smote = SMOTE(random_state = 402)\nX_smote, Y_smote = smote.fit_resample(X,y)\n\n\nsns.countplot(Y_smote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_smote, Y_smote, test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\n#Fitting the model \nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, Precision,f1 and Recall\nscore1 = accuracy_score(y_test,y_pred_lr)\nscore2 = precision_score(y_test,y_pred_lr)\nscore3= recall_score(y_test,y_pred_lr)\nscore4 = f1_score(y_test,y_pred_lr)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))\nprint(\"F1 Score score is: {}\".format(round(score4,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred_nb = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, Precision,f1 and Recall\nscore1 = accuracy_score(y_test,y_pred_nb)\nscore2 = precision_score(y_test,y_pred_nb)\nscore3 = recall_score(y_test,y_pred_nb)\nscore4 = f1_score(y_test,y_pred_nb)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))\nprint(\"F1 Score score is: {}\".format(round(score4,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XgbClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgbClassifier\nclf = XGBClassifier()\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the Test set results\ny_pred_xg = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy, Precision,f1 and Recall\nscore1 = accuracy_score(y_test,y_pred_xg)\nscore2 = precision_score(y_test,y_pred_xg)\nscore3= recall_score(y_test,y_pred_xg)\nscore4 = f1_score(y_test,y_pred_nb)\nprint(\"---- Scores ----\")\nprint(\"Accuracy score is: {}%\".format(round(score1*100,2)))\nprint(\"Precision score is: {}\".format(round(score2,2)))\nprint(\"Recall score is: {}\".format(round(score3,2)))\nprint(\"F1 Score score is: {}\".format(round(score4,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# open a file, where you want to store the data\nfile = open('toxic_comments.pkl', 'wb')\n\n# dump information to that file\npickle.dump(clf, file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(cv, open('transform.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Flask application\nSo basically I have also made a flask application for this problem by writing whole code in colab.If you want to know how to run a flask application in colab,then click on this link: https://www.kaggle.com/dikshabhati2002/run-flask-in-colab?scriptVersionId=55081927"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}