{"cells":[{"metadata":{"_uuid":"64b16d308eebca5b450c4b261852568d13f7448d"},"cell_type":"markdown","source":"<h1> Indian Startup Funding </h1>"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"201f1c87192dacf629518e72d9389b6ccf4739df"},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np ","execution_count":1,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5883ad800ec04ddd4f430e4c354951a86fdd8439"},"cell_type":"code","source":"ds = pd.read_csv(\"../input/startup_funding.csv\")\nds.head()","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"23bf26bb7815e9bbf9e995bc45918badebab2d6d"},"cell_type":"code","source":"ds.shape","execution_count":139,"outputs":[]},{"metadata":{"_uuid":"5add54951589b5ea877c6c11ea53fa54525c6e69"},"cell_type":"markdown","source":"<h3> Funding with respect to cities </h3>"},{"metadata":{"trusted":false,"_uuid":"94bc47dfb5b8742dfe817f50f45415c4b42e58e6"},"cell_type":"code","source":"city = ds['CityLocation'].value_counts()[:15]\nsns.barplot(city.index, city.values, alpha=0.8, color=\"red\")\nplt.xticks(rotation='vertical')\nplt.xlabel('City location', fontsize=14)\nplt.ylabel('Startups funded', fontsize=14)\nplt.title(\"Funding with respect to cities\", fontsize=16)\nplt.show()","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"99c7f8c5eb02ea9fe643284ffe00f53761023121"},"cell_type":"markdown","source":"<p>Based on the plot it is safe to assume that bangalore is a major startup hub in India followed by Mumbai and New Delhi<p>\n<p> Bangalore has funded more than 600 startups and would be a favorable choice for startups to acquire funding <p>"},{"metadata":{"_uuid":"c8ea1e38cfed789aa901bf9db5504aa1e4a04458"},"cell_type":"markdown","source":"<h3> Funding with respect to the Industry </h3>"},{"metadata":{"trusted":false,"_uuid":"f2a040f45ebdccecafa89df811fd8a2759a5d4fb"},"cell_type":"code","source":"city = ds['IndustryVertical'].value_counts()[:15]\nsns.barplot(city.index, city.values, alpha=0.8, color=\"blue\")\nplt.xticks(rotation='vertical')\nplt.xlabel('Industry', fontsize=14)\nplt.ylabel('Startups funded', fontsize=14)\nplt.title(\"Funding with respect to the industry\", fontsize=16)\nplt.show()","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"615016ab08a341b917aad760eaf6acfb5a0f72d5"},"cell_type":"markdown","source":"<p> Consumer Internet based startups have acquired the most funding from investors followed by technology ,e-commerce and healthcare<p>\n<p> Personally I am amazed by this trend as I have always assumed that there are more startups focussing on Machine learning, Big data analytics, Artificial intelligence and Blockchains<p> "},{"metadata":{"trusted":false,"_uuid":"011056230253cf578e0d449bb09d980c1fbeea6a"},"cell_type":"code","source":"inv = ds['InvestorsName'].value_counts()\nx = inv.values\nnp.where(x>=10)","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"d9864ab1ddc6153f94b2593588b71a8226f725f5"},"cell_type":"markdown","source":"<p> Only the first 10 investors are funding more than or equal to 10 startups <p>"},{"metadata":{"_uuid":"ed22e3848ad00b5e64ff150ecab369e2de748c02"},"cell_type":"markdown","source":"<h3> Funding with respect to the Investors </h3>"},{"metadata":{"trusted":false,"_uuid":"fa37cba3f9f4cea09ff8c1f98561774c808b886d"},"cell_type":"code","source":"inv = ds['InvestorsName'].value_counts()[2:10]\nsns.barplot(inv.index, inv.values, alpha=0.8, color=\"green\")\nplt.xticks(rotation='vertical')\nplt.xlabel('Investor', fontsize=14)\nplt.ylabel('Startups funded', fontsize=14)\nplt.title(\"Number of startups funded\", fontsize=16)\nplt.show()","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"7fc030d7abcf8938f2335c80090fef14f562d10e"},"cell_type":"markdown","source":"Indian angel network and Ratan Tata are the top investors followed by Kalaari capital"},{"metadata":{"trusted":false,"_uuid":"6468af405de3b1041b892f22c3b040c553f0b23a"},"cell_type":"code","source":"#I had used replace to add the blankspaces in investment type\ntyp = ds['InvestmentType'].value_counts()\ntyp","execution_count":124,"outputs":[]},{"metadata":{"_uuid":"80ea5bab19da304db913c03b53d31821bfe03f18"},"cell_type":"markdown","source":"<h3> Funding with respect to the Type of Investment </h3>"},{"metadata":{"trusted":false,"_uuid":"d3000b6532a99b73534332943c9c7ab330f533fa"},"cell_type":"code","source":"sns.barplot(typ.index, typ.values, alpha=0.8, color=\"black\")\nplt.xticks(rotation='vertical')\nplt.xlabel('Investment type', fontsize=14)\nplt.ylabel('Number of startups', fontsize=14)\nplt.title(\"Type of investment\", fontsize=16)\nplt.show()","execution_count":134,"outputs":[]},{"metadata":{"_uuid":"8eb290a26dbbc4e58c059f2b5149af88e400105c"},"cell_type":"markdown","source":"For most of the investors **Seed funding** is the favored kind of investment followed by private equity"},{"metadata":{"trusted":false,"_uuid":"4aefff57da4b28f7657d4eeee976367fa233b248"},"cell_type":"code","source":"amt = ds['AmountInUSD']\namt","execution_count":190,"outputs":[]},{"metadata":{"_uuid":"f5d7740eda0b45506ed7d8b269c3fb8b6e8d9fee"},"cell_type":"markdown","source":"We use log to make the distribution more analyzable"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"51e975126da0fc871c585ca6d547d12a3be352cb"},"cell_type":"code","source":"data = ds[\"AmountInUSD\"].apply(lambda x: float(str(x).replace(\",\",\"\")))","execution_count":196,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ade9e6bc407e8e4e08e85e5340125733c0b3aeb3"},"cell_type":"code","source":"ds[\"AmountInUSD\"] = ds[\"AmountInUSD\"].apply(lambda x: float(str(x).replace(\",\",\"\")))\nds['AmountInUSD_log'] = np.log(ds[\"AmountInUSD\"] + 1)\nplt.figure(figsize=(8,5))\nsns.distplot(ds['AmountInUSD_log'].dropna())\nplt.xlabel('log of Amount in USD', fontsize=12)\nplt.title(\"Log Hist of investment in USD\", fontsize=16)\nplt.show()","execution_count":193,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a4e9333006dfffe27a8581468fc769ba75af43cf"},"cell_type":"code","source":"ds.head()","execution_count":230,"outputs":[]},{"metadata":{"_uuid":"627a07291b930e5db0a46a28e75af311763b04d8"},"cell_type":"markdown","source":"<h1> Generate a random startup name based on the given data using LSTM networks <h1> "},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"3a1ee33fbcfdcd62e8246d843e2d61809b0efb3e"},"cell_type":"code","source":"data = ds['StartupName'].values\ndata","execution_count":285,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"9b8210a6d2595ffd1be3635f7cd1c4712b7eb7eb"},"cell_type":"code","source":"data = '\\n'.join(data)\ndata","execution_count":287,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"4f90b716a391b15ca0fa1c0cec75adba0783e830"},"cell_type":"code","source":"data = data.replace(\" \", \"\")\ndata = data.replace(\"(\", \"\")\ndata = data.replace(\")\", \"\")\ndata = data.replace(\"!\", \"\")\ndata = data.replace(\".\", \"\")\ndata = data.replace(\"-\", \"\")\ndata = data.replace(\"/\", \"\")\ndata = data.replace(\"\\xa0\", \"\")\ndata = data.replace(\"â€™\", \"\")\ndata = data.replace(\"#\", \"\")\ndata = data.replace(\"0\", \"\")\ndata = data.replace(\"1\", \"\")\ndata = data.replace(\"2\", \"\")\ndata = data.replace(\"3\", \"\")\ndata = data.replace(\"4\", \"\")\ndata = data.replace(\"5\", \"\")\ndata = data.replace(\"6\", \"\")\ndata = data.replace(\"8\", \"\")\ndata = data.replace(\"9\", \"\")","execution_count":290,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"8493aadcbd5a3144f8f57c7e59dc5f91fa8fdd70"},"cell_type":"code","source":"data= data.lower()\nchars = list(set(data))\ndata_size, vocab_size = len(data), len(chars)\nprint('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))","execution_count":291,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fc5a7fed4899503e18153584c9ef530820a7db2b"},"cell_type":"code","source":"char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\nix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\nprint(ix_to_char)","execution_count":292,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"92071ed9d853acf90de1ca488cf658744a27ed55"},"cell_type":"code","source":"example = data.split('\\n')\nexample","execution_count":314,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8f22da18077080b42553570bc923b14f8824667d"},"cell_type":"code","source":"import numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef smooth(loss, cur_loss):\n    return loss * 0.999 + cur_loss * 0.001\n\ndef print_sample(sample_ix, ix_to_char):\n    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n    txt = txt[0].upper() + txt[1:]  # capitalize first character \n    print ('%s' % (txt, ), end='')\n\ndef get_initial_loss(vocab_size, seq_length):\n    return -np.log(1.0/vocab_size)*seq_length\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef initialize_parameters(n_a, n_x, n_y):\n\n    np.random.seed(1)\n    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n    b = np.zeros((n_a, 1)) # hidden bias\n    by = np.zeros((n_y, 1)) # output bias\n    \n    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n    \n    return parameters\n\ndef rnn_step_forward(parameters, a_prev, x):\n    \n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n    \n    return a_next, p_t\n\ndef rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n    \n    gradients['dWya'] += np.dot(dy, a.T)\n    gradients['dby'] += dy\n    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n    gradients['db'] += daraw\n    gradients['dWax'] += np.dot(daraw, x.T)\n    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n    return gradients\n\ndef update_parameters(parameters, gradients, lr):\n\n    parameters['Wax'] += -lr * gradients['dWax']\n    parameters['Waa'] += -lr * gradients['dWaa']\n    parameters['Wya'] += -lr * gradients['dWya']\n    parameters['b']  += -lr * gradients['db']\n    parameters['by']  += -lr * gradients['dby']\n    return parameters\n\ndef rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n    \n    # Initialize x, a and y_hat as empty dictionaries\n    x, a, y_hat = {}, {}, {}\n    \n    a[-1] = np.copy(a0)\n    \n    # initialize your loss to 0\n    loss = 0\n    \n    for t in range(len(X)):\n        \n        # Set x[t] to be the one-hot vector representation of the t'th character in X.\n        # if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. \n        x[t] = np.zeros((vocab_size,1)) \n        if (X[t] != None):\n            x[t][X[t]] = 1\n        \n        # Run one step forward of the RNN\n        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n        \n        # Update the loss by substracting the cross-entropy term of this time-step from it.\n        loss -= np.log(y_hat[t][Y[t],0])\n        \n    cache = (y_hat, a, x)\n        \n    return loss, cache\n\ndef rnn_backward(X, Y, parameters, cache):\n    # Initialize gradients as an empty dictionary\n    gradients = {}\n    \n    # Retrieve from cache and parameters\n    (y_hat, a, x) = cache\n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    \n    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n    gradients['da_next'] = np.zeros_like(a[0])\n    for t in reversed(range(len(X))):\n        dy = np.copy(y_hat[t])\n        dy[Y[t]] -= 1\n        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n    return gradients, a\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9fb50714a776d728590d00dc1c4a4b6e0cd1e59b"},"cell_type":"code","source":"def clip(gradients, maxValue):\n\n    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n   \n    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (â‰ˆ2 lines)\n    for gradient in [dWax, dWaa, dWya, db, dby]:\n         np.clip(gradient, -maxValue, maxValue, out=gradient)\n    \n    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n    \n    return gradients\n\n# GRADED FUNCTION: sample\n\ndef sample(parameters, char_to_ix, seed):\n    \n    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n    vocab_size = by.shape[0]\n    n_a = Waa.shape[1]\n    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (â‰ˆ1 line)\n    x = np.zeros((vocab_size, 1))\n    # Step 1': Initialize a_prev as zeros (â‰ˆ1 line)\n    a_prev = np.zeros((n_a,1))\n    \n    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (â‰ˆ1 line)\n    indices = []\n    # Idx is a flag to detect a newline character, we initialize it to -1\n    idx = -1 \n    \n    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n    # trained model), which helps debugging and prevts entering an infinite loop. \n    counter = 0\n    newline_character = char_to_ix['\\n']\n    \n    while (idx != newline_character and counter != 50):\n        \n        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n        z = np.dot(Wya, a) + by\n        y = softmax(z)\n        \n        # for grading purposes\n        np.random.seed(counter+seed) \n        \n        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n\n        # Append the index to \"indices\"\n        indices.append(idx)\n        \n        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n        x = np.zeros((vocab_size, 1))\n        x[idx] = 1\n        \n        # Update \"a_prev\" to be \"a\"\n        a_prev = a\n        \n        # for grading purposes\n        seed += 1\n        counter +=1\n        \n    if (counter == 50):\n        indices.append(char_to_ix['\\n'])\n    \n    return indices\n\ndef optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n\n    # Forward propagate through time (â‰ˆ1 line)\n    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n    \n    # Backpropagate through time (â‰ˆ1 line)\n    gradients, a =  rnn_backward(X, Y, parameters, cache)\n    \n    # Clip your gradients between -5 (min) and 5 (max) (â‰ˆ1 line)\n    gradients = clip(gradients, 5)\n    \n    # Update parameters (â‰ˆ1 line)\n    parameters = update_parameters(parameters, gradients, learning_rate)\n        \n    return loss, gradients, a[len(X)-1]","execution_count":296,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"20b1bc45a349221b92f7bbd64d36d457420d305d"},"cell_type":"code","source":"def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n    \n    \n    # Retrieve n_x and n_y from vocab_size\n    n_x, n_y = vocab_size, vocab_size\n    \n    # Initialize parameters\n    parameters = initialize_parameters(n_a, n_x, n_y)\n    \n    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n    loss = get_initial_loss(vocab_size, dino_names)\n    \n    # Build list of all dinosaur names (training examples).\n\n\n    examples = example\n\n    \n    # Shuffle list of all dinosaur names\n    np.random.seed(0)\n    np.random.shuffle(examples)\n    \n    # Initialize the hidden state of your LSTM\n    a_prev = np.zeros((n_a, 1))\n    \n    # Optimization loop\n    for j in range(num_iterations):\n                \n        # Use the hint above to define one training example (X,Y) (â‰ˆ 2 lines)\n        index =  j % len(examples)\n        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n        Y = X[1:] + [char_to_ix[\"\\n\"]]\n        \n        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n        # Choose a learning rate of 0.01\n        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n                \n        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n        loss = smooth(loss, curr_loss)\n\n        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n        if j % 2000 == 0:\n            \n            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n            \n            # The number of dinosaur names to print\n            seed = 0\n            for name in range(dino_names):\n                \n                # Sample indices and print them\n                sampled_indices = sample(parameters, char_to_ix, seed)\n                print_sample(sampled_indices, ix_to_char)\n                \n                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n      \n            print('\\n')\n        \n    return parameters","execution_count":315,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e0d76c9eb0e577dd038bb82509ae5ba64bd0b1a4"},"cell_type":"code","source":"parameters = model(data, ix_to_char, char_to_ix)","execution_count":316,"outputs":[]},{"metadata":{"_uuid":"5106d8d4011a2882ddf93289657218108a8febab"},"cell_type":"markdown","source":"![](http://)<h3> Initially the names make no sense but as as the iteration increases we see some probable company names (Names may be changed as I am training several times)<h3>\n<h2> The name IND or INDA comes almost in all iterations indicates that its a common startup name in India, which makes sense <h2>\n<ul>\n<li>Mabacora </li> <br>\n<li>  Votbigo</li><br>\n<li>Zystcleri</li><br>\n<li>Feagora</li><br>\n<li>Inda</li><br>\n<li>Voreedom</li><br>\n<li>Mevstock and so on....</li>\n \n<h2> Thank you\n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}