{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', 50)\n\nfrom matplotlib import pyplot\n# import matplotlib.pylab as plt\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime,timedelta\n\nimport numpy as np\nfrom numpy import *\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"white\")\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method to convert ordinal timeseries format to\n## YYYY-mm-dd HH:MM:ss.SSSSSS"},{"metadata":{"trusted":false},"cell_type":"code","source":"def _from_ordinal(x, tz=None):\n    ix = int(x)\n    dt = datetime.fromordinal(ix)\n    remainder = float(x) - ix\n    hour, remainder = divmod(24 * remainder, 1)\n    minute, remainder = divmod(60 * remainder, 1)\n    second, remainder = divmod(60 * remainder, 1)\n    microsecond = int(1e6 * remainder)\n    if microsecond < 10:\n        microsecond = 0  # compensate for rounding errors\n    dt = datetime(dt.year, dt.month, dt.day, int(hour), int(minute),\n                  int(second), microsecond)\n    if tz is not None:\n        dt = dt.astimezone(tz)\n\n    if microsecond > 999990:  # compensate for rounding errors\n        dt += timedelta(microseconds=1e6 - microsecond)\n\n    return dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data(added pre-processing)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# READING DATA - Patient 1\npat1_values_original = pd.read_csv('/home/varun/Desktop/dm/assignment1/DataFolder/CGMSeriesLunchPat1.csv')\n\npat1_ts_original = pd.read_csv('/home/varun/Desktop/dm/assignment1/DataFolder/CGMDatenumLunchPat1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# REPLACING MISSING DATA WITH N/A AND CONVERTING TIMESERIES FORMAT\npat1_ts_original.fillna(1,inplace=True)\n# pat1_values_df.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pat1_values_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# CONVERTING TIMESERIES FORMAT\npat1_ts_parsed = pat1_ts_original.applymap(lambda ts : _from_ordinal(ts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pat1_ts_parsed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# ROWS & COLUMNS(Original)\npat1_values_original.shape\npat1_ts_parsed.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# TS - \n# 1. DECREASES TO THE RIGHT IN A SINGLE DAY\n# 2. DECREASES TO THE BOTTOM IN A SINGLE MONTH(ish)\npat1_ts_updated = pat1_ts_parsed.iloc[::-1]\npat1_ts_updated = pat1_ts_updated.iloc[:, ::-1]\npat1_ts_updated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# CGM VALUES - \n# 1. DECREASES TO THE RIGHT IN A SINGLE DAY\n# 2. DECREASES TO THE BOTTOM IN A SINGLE MONTH(ish)\npat1_values_updated = pat1_values_original.iloc[::-1]\npat1_values_updated = pat1_values_updated.iloc[:, ::-1]\npat1_values_updated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# ROWS & COLUMNS(Updated)\npat1_values_updated.shape\npat1_ts_updated.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample Day 1 & 2 Plots"},{"metadata":{"trusted":false},"cell_type":"code","source":"# PATIENT 1, DAY 1\nplt.xlim( max(pat1_ts_updated.iloc[:1,:].values.flatten())- timedelta(minutes=150), max(pat1_ts_updated.iloc[:1,:].values.flatten()))\nplt.plot(pat1_ts_updated.iloc[:1,:].values.flatten(),pat1_values_updated.iloc[:1,:].values.flatten())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# PATIENT 1, DAY 2\nplt.xlim( max(pat1_ts_updated.iloc[1:2,:].values.flatten())- timedelta(minutes=150), max(pat1_ts_updated.iloc[1:2,:].values.flatten()))\nplt.plot(pat1_ts_updated.iloc[1:2,:].values.flatten(),pat1_values_updated.iloc[1:2,:].values.flatten())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross-Correlation"},{"metadata":{"trusted":false},"cell_type":"code","source":"corr = pat1_values_updated.corr()\npat1_values_updated.corr().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# GENERATE A MASK FOR THE UPPER TRIANGLE\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# SET UP THE MATPLOTLIB FIGURE\nf, ax = plt.subplots(figsize=(11, 9))\n\n# GENERATE A CUSTOM DIVERGING COLORMAP\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# DRAW THE HEATMAP WITH THE MASK & CORRECT ASPECT RATIO\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# CREATING DF COPY\npat1_values_df_features = pat1_values_df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Max - Min"},{"metadata":{"trusted":false},"cell_type":"code","source":"pat1_values_df_features['max'] = pat1_values_df.max(axis = 1, skipna=True)\npat1_values_df_features['min'] = pat1_values_df.min(axis = 1, skipna=True)\npat1_values_df_features['max-min'] = pat1_values_df_features['max'] - pat1_values_df_features['min']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Mean"},{"metadata":{"trusted":false},"cell_type":"code","source":"pat1_values_df_features['mean'] = pat1_values_df.mean(axis = 1, skipna=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Standard-deviation"},{"metadata":{"trusted":false},"cell_type":"code","source":"pat1_values_df_features['std'] = pat1_values_df.std(axis = 1, skipna=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Skewness"},{"metadata":{"trusted":false},"cell_type":"code","source":"pat1_values_df_features['skew'] = pat1_values_df.skew(axis = 1, skipna=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FEATURES DATAFRAME"},{"metadata":{"trusted":false},"cell_type":"code","source":"# OBSERVATIONS\npat1_values_df_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ADDING TARGET VARIABLE"},{"metadata":{"trusted":false},"cell_type":"code","source":"# ADDING TARGET - 'meal', since all labels are positive\npat1_values_df_features['target'] = 'meal'\npat1_values_df_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## STANDARDIZATION"},{"metadata":{"trusted":false},"cell_type":"code","source":"features = ['max', 'min', 'max-min', 'mean', 'std', 'skew']\n# SEPARATING FEATURES\nfeatures_unstd = pat1_values_df_features.loc[:, features].values\n# SEPARATING TARGET\ntarget = pat1_values_df_features.loc[:,['target']].values\n# STANDARDIZING FEATURES\nfeatures_std = StandardScaler().fit_transform(features_unstd)\nprint(features_std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA"},{"metadata":{"trusted":false},"cell_type":"code","source":"# APPLYING PCA with n=None\npca = PCA(n_components=None)\nprincipal_components = pca.fit(features_std)\n\nprint(principal_components.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# APPLYING PCA with n=2\npca = PCA(n_components=2)\nprincipal_components_2 = pca.fit(features_std)\n\nprint(principal_components_2.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# TODO: convert array to DF\nprincipal_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# CONCATENATING PRINCIPLE COMPONENTS WITH TARGET\ndf_final = pd.concat([principal_df, pat1_values_df_features[['target']]], axis = 1)\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RESULTS"},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = ['meal', 'not-meal']\ncolors = ['r', 'g', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = df_final['target'] == target\n    ax.scatter(df_final.loc[indicesToKeep, 'principal component 1']\n               , df_final.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing - Covariance v/s Correlation(un-standardized) v/s Correlation(standardized)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# CORRELATION MATRICES\ncorr_matrix_std = np.corrcoef(features_std.T)\nprint('Correlation matrix using standardized data\\n\\n', corr_matrix_std)\n\ncorr_matrix_unstd = np.corrcoef(features_unstd.T)\nprint('\\n\\nCorrelation matrix using base unstandardized data\\n\\n', corr_matrix_unstd)\n\n# COVARIANCE MATRIX(Standardized data)\nmean_vec = np.mean(features_std, axis=0)\ncov_matrix = (features_std - mean_vec).T.dot((features_std - mean_vec)) / (features_std.shape[0]-1)\nprint('\\n\\nCovariance matrix \\n\\n', cov_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## OBSERVATION - \n1. Correlation on both dataset yields same result(standardizing the data-set and then computing the covariance and correlation matrices will yield the same results)\n2. Covariance matrix produces similar results to correlation matrices"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}