{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Poverty Prediction: From Visualization to Model Stacking (My First Kernel!)\n**By Johnny Yiu**\n<a href=\"https://www.worldbank.org/en/news/video/2018/10/17/new-ways-of-looking-at-poverty\"><img src=\"https://i.imgur.com/jIVQ94c.png\" title=\"Source: World Bank\" /></a>\n\n## Overview\nThe motivation of creating this notebook is a data science competition hosted by **Microsoft** and **DrivenData**, which is a part of the Capstone project of the **Microsoft Professional Certificate in Data Science** and also my very first competition related to machine learning. By the end of the competition, I was at the **top 4%** on the leaderboard, and I am happy to share with you my approach, especially to those who are just beginning their journey on data science.\n\n<a href=\"https://datasciencecapstone.org/competitions/15/predicting-poverty/leaderboard/\"><img src=\"https://i.imgur.com/Yo4Oeya.png\" title=\"Leaderboard\" /></a>\n\n<a href=\"https://datasciencecapstone.org/competitions/15/predicting-poverty/leaderboard/\"><img src=\"https://i.imgur.com/ItktCZr.png\" title=\"Best Score\" /></a>\n\n## Goal\nParticipants are to predict the probability that individuals across 7 different countries live below the poverty line at the $2.50/day threshold, given other socioeconomic indicators. The probability of being in poverty was calculated using the **Poverty Probability Index (PPI)**, which estimates an individual's poverty status using 10 questions about a householdâ€™s characteristics and asset ownership. The remaining data comes from the **Financial Inclusion Insights** household surveys conducted by InterMedia.\n\n<a href=\"https://www.povertyindex.org\"><img src=\"https://www.povertyindex.org/sites/default/files/PPI-logo-RGB-header-image.png\" title=\"PPI\" /></a>\n<a href=\"http://finclusion.org\"><img src=\"https://i.imgur.com/jjPjT06.png\" title=\"Fii\" /></a>\n\n## Data\nThe dataset was retrieved from datasciencecapstone.org and contains the PPI along with 58 features of 12,600 individuals across 7 different countries.\n\n**Information on the competition and the data:**\nhttps://datasciencecapstone.org/competitions/15/predicting-poverty/page/47/\n\n## Model\nI have hand-picked 3 regression-based models (Gboost, XGBoost and LightGBM) and used a 5-fold cross validation to evaluation their performance. A stacked model of the 3 is then tested for prediction results. The final r2 score was 0.4213, resulting a top 4% on the leaderboard. \n\n**The model stacking approach here is inspired by Serigne, make sure to check it out at:**\n[Serigne's Stacked Regressions Notebook](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)."},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing\nWe start by first preparing the data into suitable formats for analysis.\n\n\nThe required packages and data sets are loaded. I have merged the features and labels data sets into one dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport numpy.random as nr\nfrom sklearn import preprocessing\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nfrom sklearn import feature_selection as fs\nimport sklearn.decomposition as skde\nimport sklearn.metrics as sklm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport scipy.stats as ss\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.linear_model import ElasticNet, Lasso,  Ridge, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import r2_score\nimport lightgbm as lgb\nimport xgboost as xgb\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = pd.read_csv('../input/train_values_wJZrCmI.csv')\ndf_labels = pd.read_csv('../input/train_labels.csv')\ndf = df_features.merge(df_labels, on='row_id')\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop duplicates\nNo duplicates detected"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = df.drop_duplicates(keep = 'last')\ndf = df.drop_duplicates(subset = 'row_id', keep = 'last')\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Treat missing values\nDropping columns with mostly missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.isnull().sum())\n\ndf.drop('bank_interest_rate', axis = 1, inplace = True)\ndf.drop('mm_interest_rate', axis = 1, inplace = True)\ndf.drop('mfi_interest_rate', axis = 1, inplace = True)\ndf.drop('other_fsp_interest_rate', axis = 1, inplace = True)\n\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Replace missing values   \nAssigning missing values to a new category for both `education_level` and `share_hh_income_provided` gives with best R-squared for our regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['education_level'].fillna(df['education_level'].median(), inplace=True)\n# df['share_hh_income_provided'].fillna(df['share_hh_income_provided'].median(), inplace=True)\n\n# df.dropna(subset=['education_level'], inplace=True)\n# df.dropna(subset=['share_hh_income_provided'], inplace=True)\n\ndf['education_level'].fillna(4, inplace=True)\ndf['share_hh_income_provided'].fillna(0, inplace=True)\n\ndf.isnull().sum()\n# print(df.loc[df['education_level'] == 'Unknown'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing true/false with 1/0 for categorical features isn't necessary for the new OneHotEncoder, as it takes strings as arguments.\n\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# def replace_boolean(data):\n#     for col in data:\n#         data[col].replace(True, 1, inplace=True)\n#         data[col].replace(False, 0, inplace=True)\n        \n# replace_boolean(df)\n# df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing outliers decreased our R-squared."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = df[df.poverty_probability > 0]\n# df = df[df.poverty_probability < 1]\n# df = df[df.num_financial_activities_last_year < 10]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis\n### Exploring the label\n\nSquare transformation on the label produces a less skewed distribution, but decreases our R-squared. Other transformations did not make the distribution more normal."},{"metadata":{"trusted":true},"cell_type":"code","source":"# df['poverty_probability'].describe().to_csv('D:\\\\Users\\\\user\\\\Desktop\\\\MPP DS Cert\\DAT102x - Microsoft Professional Capstone Data Science\\\\label_summary.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hist_plot(vals, lab):\n    ## Distribution plot of values\n    sns.set(style=\"whitegrid\", palette='Blues_r')\n    sns.distplot(vals)\n    plt.title('Histogram of ' + lab)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n    \nsns.set_style(\"whitegrid\")\nhist_plot(df['poverty_probability'], 'PPI')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FUNCTION FOR returning 0 for ln(0)\nfrom numpy import errstate,isneginf\na = df['poverty_probability']\nwith errstate(divide='ignore'):\n    res = np.log(a)\nres[isneginf(res)]=0\nhist_plot(res, 'ln PPI')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#CUBE ROOT LABELS\ndf['cbrt_poverty_probability'] = np.cbrt(df['poverty_probability'])\nhist_plot(df['cbrt_poverty_probability'], 'cube root PPI')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#SQUARE LABELS\ndf['squared_poverty_probability'] = np.square(df['poverty_probability'])\nhist_plot(df['squared_poverty_probability'], 'squared PPI')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring features"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"list(df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The feautre age_group is created from age:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_age_group(data):\n    age_conditions = [\n    (data['age'] < 30 ),\n    (data['age'] >= 30) & (data['age'] < 45),\n    (data['age'] >= 45) & (data['age'] < 60),\n    (data['age'] >= 60)\n    ]\n    age_choices = ['Under 30', '30 to 44', '45 to 59', '60 or Over']\n    data['age_group'] = np.select(age_conditions, age_choices)\n\ncreate_age_group(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploring categorical features\nCounting unique values for categorical features - we see that some features, like religion, have values with very little data. Feature engineering will be performed to solve the problem.\n\nNote that some discrete numerical features are treated as categorical features."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def count_unique(df, cols):\n    for col in cols:\n        print('\\n' + 'For column ' + col)\n        print(df[col].value_counts())\n\ncat_cols = ['age_group','country','is_urban','female','married','religion','relationship_to_hh_head',\n 'education_level','literacy','can_add','can_divide','can_calc_percents','can_calc_compounding',\n 'employed_last_year','employment_category_last_year','employment_type_last_year',\n 'income_ag_livestock_last_year','income_friends_family_last_year','income_government_last_year',\n 'income_own_business_last_year','income_private_sector_last_year','income_public_sector_last_year',\n 'borrowing_recency','formal_savings','informal_savings','cash_property_savings',\n 'has_insurance','has_investment','borrowed_for_emergency_last_year','borrowed_for_daily_expenses_last_year',\n 'borrowed_for_home_or_biz_last_year','phone_technology','can_call','can_text','can_use_internet',\n 'can_make_transaction','phone_ownership','advanced_phone_use','reg_bank_acct',\n 'reg_mm_acct','reg_formal_nbfi_account','financially_included','active_bank_user',\n 'active_mm_user','active_formal_nbfi_user','active_informal_nbfi_user','nonreg_active_mm_user', 'share_hh_income_provided', \n'num_times_borrowed_last_year','num_shocks_last_year','num_formal_institutions_last_year',\n            'num_informal_institutions_last_year']\n\ncount_unique(df, cat_cols)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating violin plots for all categorical features against our label PPI - only some features have values that are distinctively different in PPI distributions."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def plot_violin(df, cols, col_y, title):\n    for col in cols:\n        sns.set(style=\"whitegrid\")\n        sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n        sns.violinplot(col, col_y, data=df)\n        plt.xlabel(col) # Set text for the x axis\n        plt.ylabel(col_y)# Set text for y axis\n        plt.title(title + ' by ' + col)\n        plt.show()\n        \nplot_violin(df, cat_cols, 'poverty_probability', 'PPI')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Explore numeric features\nCreating combined kernel density estimation and histogram plots for all numeric features:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"num_cols = ['age', 'avg_shock_strength_last_year', \n            'num_financial_activities_last_year', \n            'poverty_probability', 'poverty_probability'] \n\ndef plot_density_hist(df, cols, bins = 10, hist = False):\n    for col in cols:\n        sns.set(style=\"whitegrid\", palette='Blues_r')\n        sns.distplot(df[col], bins = bins, rug=True, hist = hist)\n        plt.title('Histogram of ' + col) # Give the plot a main title\n        plt.xlabel(col) # Set text for the x axis\n        plt.ylabel('Frequency')# Set text for y axis\n        plt.show()\n        \nplot_density_hist(df, num_cols, bins = 20, hist = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A pairplot and a correlation matrix of numeric features is created - apparent relationships are neither found within numeric features, and between the label and these features."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"num_cols = ['age', 'avg_shock_strength_last_year', \n            'num_financial_activities_last_year', 'poverty_probability'] \n\nsns.set(style=\"whitegrid\", palette='Blues_r')\nsns.pairplot(df[num_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, on average, the amount of financial activities seems to decrease with age."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Compute the correlation matrix\ncorrs = df[num_cols].corr()\n\n## Create the hierarchical clustering model\ndist = sch.distance.pdist(corrs)   # vector of pairwise distances using correlations\nlinkage = sch.linkage(dist, method='complete') # Compute the linkages for the clusters\nind = sch.fcluster(linkage, 0.5*dist.max(), 'distance')  # Apply the clustering algorithm\n\n## Order the columns of the correlaton matrix according to the hierarchy\ncolumns = [corrs.columns.tolist()[i] for i in list((np.argsort(ind)))]  # Order the names for the result\ncorrs_clustered = corrs.reindex(columns) ## Reindex the columns following the heirarchy \n\n## Correlation Plot\ncorrs_clustered.style.background_gradient().set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"num_corrs = df[num_cols].corr()\nf,ax = plt.subplots(figsize=(5, 5))\nsns.heatmap(num_corrs, annot=True, square=True, linewidths=.1, fmt= '.2f',ax=ax, \n           cmap=\"RdBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering\n#### Aggregate categorical features\n\nThe aggregation of categorical features was performed to reduce the number of categories. For discrete variables, rare values are combined to form a range of values. For categorical variables, rare values are combined with common values that share a more similar distribution in PPI.\n\n- Religion O is combined with religion P to form religion O_P, and religion N is combined with religion Q to form religion â€˜N_Qâ€™\n- The number of shocks of 4 or above are combined to form â€˜4_5â€™\n- The number of formal financial institutions used of 3 or above are combined to form â€˜3_4_5_6â€™\n- The number of informal financial institutions used of 2 or above are combined to form â€˜2_3_4â€™\n- The number of shocks of 4 or above are combined to form â€˜4_5â€™ \n- The category â€˜Unknownâ€™ for relationship to the head of the household is combined with â€˜Otherâ€™"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_box_1(df, col, col_y, title):\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n    sns.boxplot(col, col_y, data=df)\n    plt.xlabel(col) # Set text for the x axis\n    plt.ylabel(col_y)# Set text for y axis\n    plt.title(title + ' by ' + col)\n    plt.show()\n    \ndef plot_violin_1(df, col, col_y, title):\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n#     fig, ax = plt.subplots(figsize=(11,8))\n    sns.violinplot(col, col_y, data=df)\n    plt.xlabel(col) # Set text for the x axis\n    plt.ylabel(col_y)# Set text for y axis\n    plt.title(title + ' by ' + col)\n    plt.show()\n\nplot_violin_1(df, 'religion', 'poverty_probability', 'PPI')\nplot_violin_1(df, 'num_shocks_last_year', 'poverty_probability', 'PPI')\nplot_violin_1(df, 'num_formal_institutions_last_year', 'poverty_probability', 'PPI')\nplot_violin_1(df, 'num_informal_institutions_last_year', 'poverty_probability', 'PPI')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n    fig, ax = plt.subplots(figsize=(11,8))\n    sns.violinplot('relationship_to_hh_head', 'poverty_probability', data=df) #order=['Under 30', '30 to 44', '45 to 59', '60 or Over']\n    plt.xlabel('relationship_to_hh_head') # Set text for the x axis\n    plt.ylabel('poverty_probability')# Set text for y axis\n    plt.title('PPI' + ' by ' + 'relationship_to_hh_head')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"religion_categories = {'N':'N_Q', 'O':'O_P',\n                       'P':'O_P', 'Q':'N_Q','X':'X'}\ndf['religion'] = [religion_categories[x] for x in df['religion']]\nprint(df['religion'].value_counts())\n\n#num_shocks_last_year 4_5\nnum_shocks_last_year_categories = {0:'0', 1:'1', 2:'2',\n                       3:'3', 4:'4_5', 5:'4_5'}\ndf['num_shocks_last_year'] = [num_shocks_last_year_categories[x] for x in df['num_shocks_last_year']]\nprint(df['num_shocks_last_year'].value_counts())\n\n#num_formal_institutions_last_year 3_or_over\nnum_formal_institutions_last_year_categories = {0:'0', 1:'1', 2:'2',\n                       3:'3_4_5_6', 4:'3_4_5_6', 5:'3_4_5_6', 6:'3_4_5_6'}\ndf['num_formal_institutions_last_year'] = [num_formal_institutions_last_year_categories[x] for x in df['num_formal_institutions_last_year']]\nprint(df['num_formal_institutions_last_year'].value_counts())\n\n#num_informal_institutions_last_year 2_or_over\nnum_informal_institutions_last_year_categories = {0:'0', 1:'1', 2:'2_3_4',\n                       3:'2_3_4', 4:'2_3_4'}\ndf['num_informal_institutions_last_year'] = [num_informal_institutions_last_year_categories[x] for x in df['num_informal_institutions_last_year']]\nprint(df['num_informal_institutions_last_year'].value_counts())\n\nrelationship_to_hh_head_categories = {'Other':'Other', 'Spouse':'Spouse',\n                                      'Head':'Head',\n                                      'Son/Daughter':'Son/Daughter',\n                                      'Sister/Brother':'Sister/Brother',\n                                      'Father/Mother': 'Father/Mother',\n                                      'Unknown':'Other'}\ndf['relationship_to_hh_head'] = [relationship_to_hh_head_categories[x] for x in df['relationship_to_hh_head']]\nprint(df['relationship_to_hh_head'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create violin plots for the new features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_violin_1(df, 'religion', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'num_shocks_last_year', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'num_formal_institutions_last_year', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'num_informal_institutions_last_year', 'poverty_probability', 'PPI') \n\nplot_violin_1(df, 'relationship_to_hh_head', 'poverty_probability', 'PPI') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing values named 'Unknown' for relationship_to_hh_head was attempted, but it worsened our model."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# df = df[df.relationship_to_hh_head != 'Unknown']\n# df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transforming numeric features\n\nTransformed numeric features worsened our model."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# df['ln_age'] = np.log(df['age'])\n# hist_plot(df['ln_age'], 'natural log age')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# df['cbrt_num_times_borrowed_last_year'] = np.cbrt(df['num_times_borrowed_last_year'])\n# hist_plot(df['cbrt_num_times_borrowed_last_year'], 'cube root num_times_borrowed_last_year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Matrix Preparation\n\nWe are predicting the label `poverty_probability` with both categorical and numeric features. Categorical features are one-hot encoded, and joint back with the numeric features."},{"metadata":{"trusted":true},"cell_type":"code","source":"Labels = np.array(df['poverty_probability'])\n# Labels = np.array(df['poverty_probability'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def encode_string(cat_features):\n    ## Now, apply one hot encoding\n    ohe = preprocessing.OneHotEncoder(categories='auto')\n    encoded = ohe.fit_transform(cat_features.values.reshape(-1,1)).toarray()\n    pdfn = ohe.get_feature_names()\n    print(pdfn)\n    return encoded\n\nfeatures_cat_cols = ['country','is_urban','female','married','religion','relationship_to_hh_head',\n 'education_level','literacy','can_add','can_divide','can_calc_percents','can_calc_compounding',\n 'employed_last_year','employment_category_last_year','employment_type_last_year',\n 'income_ag_livestock_last_year','income_friends_family_last_year','income_government_last_year',\n 'income_own_business_last_year','income_private_sector_last_year','income_public_sector_last_year',\n 'borrowing_recency','formal_savings','informal_savings','cash_property_savings',\n 'has_insurance','has_investment','borrowed_for_emergency_last_year','borrowed_for_daily_expenses_last_year',\n 'borrowed_for_home_or_biz_last_year','phone_technology','can_call','can_text','can_use_internet',\n 'can_make_transaction','phone_ownership','advanced_phone_use','reg_bank_acct',\n 'reg_mm_acct','reg_formal_nbfi_account','financially_included','active_bank_user',\n 'active_mm_user','active_formal_nbfi_user','active_informal_nbfi_user','nonreg_active_mm_user', 'share_hh_income_provided', \n'num_times_borrowed_last_year','num_shocks_last_year','num_formal_institutions_last_year',\n            'num_informal_institutions_last_year']\n\nFeatures = encode_string(df['age_group'])\nfor col in features_cat_cols:\n    temp = encode_string(df[col])\n    Features = np.concatenate([Features, temp], axis = 1)\n    \nprint(Features.shape)\nprint(Features[:2, :])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"features_num_cols = ['avg_shock_strength_last_year',\n                     'num_financial_activities_last_year']\nFeatures = np.concatenate([Features, np.array(df[features_num_cols])], axis = 1)\nprint(Features.shape)\nprint(Features[:2, :])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model selection\n\n#### Eliminate low variance features\n\nWe eliminate features with low variance using the scikit-learn `VarianceThreshold` based function. A p = 0.95 was found to optimize the model."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Features selection\nprint(Features.shape)\n\n## Define the variance threhold and fit the threshold to the feature array.\nsel = fs.VarianceThreshold(threshold=(.95 * (1 - .95)))\nFeatures_reduced = sel.fit_transform(Features)\nprint(sel.get_support())\n\n## Print the support and shape for the transformed features\nprint(Features_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recursive feature elimination\n\nWe used the scikit-learn `RFECV` function to determine which features to retain using a cross validation method, with the ridge regression model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select k best features with Recursive Feature Eliminiation\n## Reshape the Label array\nLabels = Labels.reshape(Labels.shape[0],)\n## Set folds for nested cross validation\nnr.seed(562)\nfeature_folds = ms.KFold(n_splits=10, shuffle = True)\n## Define the model\nlin_mod_l2 = linear_model.Ridge()\n## Perform feature selection by CV with high variance features only\nnr.seed(265)\nselector = fs.RFECV(estimator = lin_mod_l2, cv = feature_folds,\n                      scoring = 'r2')\nselector = selector.fit(Features_reduced, Labels)\nprint(selector.support_)\nprint(selector.ranking_)\n\nFeatures_reduced = selector.transform(Features_reduced)\nprint(Features_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the Dataset\n\nWith the model matrix constructed, we now create randomly sampled training and test data sets in a 8:2 ratio:"},{"metadata":{"trusted":true},"cell_type":"code","source":"nr.seed(265)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 0.2)\nx_train = Features_reduced[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nx_test = Features_reduced[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescale numeric features\n\nWe use the `StandardScaler` to z-score scale the numeric features."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"scaler = preprocessing.StandardScaler().fit(x_train[:,104:])\nx_train[:,104:] = scaler.transform(x_train[:,104:])\nx_test[:,104:] = scaler.transform(x_test[:,104:])\nprint(x_train[:2,])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Construct the Regression Models\n\nDifferent regression models are tested with optimized hyperparameters. We first fit the models with pre-selected parameters to our train data, and fine-tune them recursively. Here we will test Gradient Boosting Regressor, XGboost Regressor and LGB Regressor, the current best-performing algorithms for data science competitions."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"GBoost = GradientBoostingRegressor()\nmodel_xgb = xgb.XGBRegressor()\nmodel_lgb = lgb.LGBMRegressor(objective='regression', num_leaves = 32,\n                              learning_rate=0.01)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tuning\n\nHyperparameters are manually and recursively selected with a grid search, here is an example of finding the optimized value for XGB Regressor's `n_estimators`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"nr.seed(265)\ninside = ms.KFold(n_splits=5, shuffle = True)\nnr.seed(562)\noutside = ms.KFold(n_splits=5, shuffle = True)\n\nnr.seed(2652)\n## Define the dictionary for the grid search and the model object to search on\nparam_grid = {'n_estimators': [2000, 3000]}\n\n## Perform the grid search over the parameters\ngsearch = ms.GridSearchCV(estimator = model_lgb, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'r2',\n                      return_train_score = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit the cross validated grid search over the data and print the best parameter value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch.fit(Features_reduced, Labels)\ngsearch.best_params_, gsearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimized parameters are plugged into the models:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"GBoost = GradientBoostingRegressor(n_estimators= 2000, learning_rate=0.01,\n                                   max_depth=5, max_features='sqrt',\n                                   min_samples_leaf=7, min_samples_split=15, \n                                   loss='ls', random_state = 1)\n\nmodel_xgb = xgb.XGBRegressor(max_depth = 5, min_child_weight = 0, gamma = 0, \n                           subsample = 0.8, colsample_bytree = 0.8, \n                           scale_pos_weight = 1, reg_lambda = 1,\n                           learning_rate =0.01, n_estimators=2000, \n                           objective = 'reg:squarederror', seed = 14)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression', num_leaves = 32,\n                              learning_rate=0.01, n_estimators=2100, \n                              bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.4,\n                              min_data_in_leaf = 5,  \n                              feature_fraction_seed=3, bagging_seed=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate models with cross validation\nWe define a cross validation strategy to evaluate the r-square of our models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validation function\nn_folds = 5\n\ndef r2_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\n    r2 = cross_val_score(model, x_train, y_train, scoring=\"r2\", cv = kf)\n    return(r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = r2_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = r2_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = r2_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Stacking\n\nHere, 2 model stacking approaches are applied - \n1. averaging base models, and \n2. stacking averaged models and adding a meta-model\n\nFor more information, read [Serigne's Stacked Regressions Notebook](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)."},{"metadata":{},"cell_type":"markdown","source":"### Averaging base models\nWe build a new class to extend scikit-learn with our model and leverage inheritance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Averaging XBG and LGB and print out their cross-validation r-square score:"},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_models = AveragingModels(models = (model_xgb, model_lgb))\n\n# score = r2_cv(averaged_models)\n# print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding a meta-model\nLet's create a class for stacking average models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacking GBR on top of the averaged XBG and LGB and print out their cross-validation r-square score:"},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (model_lgb, model_xgb),\n                                                 meta_model = GBoost)\n\nscore = r2_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, it looks like simply averaging XGB and LGB gives a better score."},{"metadata":{},"cell_type":"markdown","source":"We then create a function to evaluate the models' performances on the test data after fitting to the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rsquare(y, y_pred):\n    return r2_score(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_models.fit(x_train, y_train)\navg_pred = averaged_models.predict(x_test)\nprint(rsquare(y_test, avg_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models.fit(x_train, y_train)\nstacked_pred = stacked_averaged_models.predict(x_test)\nprint(rsquare(y_test, stacked_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GBoost.fit(x_train, y_train)\ngb_pred = GBoost.predict(x_test)\nprint(rsquare(y_test, gb_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(x_train, y_train)\nxgb_pred = model_xgb.predict(x_test)\nprint(rsquare(y_test, xgb_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb.fit(x_train, y_train)\nlgb_pred = model_lgb.predict(x_test)\nprint(rsquare(y_test, lgb_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For meaningful interpretation, predicted scores with negative values are converted to 0, and scores larger than 1 are converted to 1:"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_pred[avg_pred < 0] = 0\navg_pred[avg_pred > 1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_pred[stacked_pred < 0] = 0\nstacked_pred[stacked_pred > 1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_pred[gb_pred < 0] = 0\ngb_pred[gb_pred > 1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred[xgb_pred < 0] = 0\nxgb_pred[xgb_pred > 1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_pred[lgb_pred < 0] = 0\nlgb_pred[lgb_pred > 1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = avg_pred*0.55 + lgb_pred*0.45\nprint('R2 score on test data:')\nprint(rsquare(y_test, ensemble))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Print metrics and evaluations\n\nThe model's performance metrics on the locally splitted test data is printed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_metrics(y_true, y_predicted, n_parameters):\n    ## First compute R^2 and the adjusted R^2\n    r2 = sklm.r2_score(y_true, y_predicted)\n    r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n    \n    ## Print the usual metrics and the R^2 values\n    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n    print('R^2                    = ' + str(r2))\n    print('Adjusted R^2           = ' + str(r2_adj))\n   \nprint_metrics(y_test, lgb_pred, 105)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A feature importance plot is created find out what are the top features, here a sample plot for XGB regressor is printed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fimp = xgb.plot_importance(model_xgb)\nfig = fimp.figure\nfig.set_size_inches(20, 20)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# hist_plot(y_score, 'Predicted PPI')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a residual histogram plot, a residual q-q plot, and a residuals vs. predicted values plot:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def hist_resids(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    sns.set(style = 'whitegrid', palette='Blues_r')\n    sns.distplot(resids)\n    plt.title('Histogram of residuals')\n    plt.xlabel('Residual value')\n    plt.ylabel('count')\n    \nhist_resids(y_test, lgb_pred)    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def resid_qq(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ss.probplot(resids.flatten(), plot = plt)\n    ax.get_lines()[0].set_marker('+')\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n    \nresid_qq(y_test, lgb_pred)   ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def resid_plot(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    sns.regplot(y_score, resids, fit_reg=False, marker=\"+\", scatter_kws={'alpha':0.5})\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n\nresid_plot(y_test, lgb_pred) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# External Test Data Preparation\n\nA seperated file containing test data was provided by DrivenData for testing and submission. We prepare these test features the same way the train data was prepared in order to fit the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/test_values.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop duplicates, uninformative features, and impute missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.drop_duplicates(keep = 'last')\ntest_df = test_df.drop_duplicates(subset = 'row_id', keep = 'last')\n\ntest_df.shape\ntest_df.drop('bank_interest_rate', axis = 1, inplace = True)\ntest_df.drop('mm_interest_rate', axis = 1, inplace = True)\ntest_df.drop('mfi_interest_rate', axis = 1, inplace = True)\ntest_df.drop('other_fsp_interest_rate', axis = 1, inplace = True)\n\n\ntest_df['education_level'].fillna(4, inplace=True)\ntest_df['share_hh_income_provided'].fillna(0, inplace=True)\n# test_df.dropna(subset=['education_level'], inplace=True)\n# test_df.dropna(subset=['share_hh_income_provided'], inplace=True)\n\ntest_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# replace_boolean(test_df)\n# test_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df = test_df[test_df.relationship_to_hh_head != 'Unknown']\n# test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature engineering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"create_age_group(test_df)\ntest_df['religion'] = [religion_categories[x] for x in test_df['religion']]\ntest_df['num_shocks_last_year'] = [num_shocks_last_year_categories[x] for x in test_df['num_shocks_last_year']]\ntest_df['num_formal_institutions_last_year'] = [num_formal_institutions_last_year_categories[x] for x in test_df['num_formal_institutions_last_year']]\ntest_df['num_informal_institutions_last_year'] = [num_informal_institutions_last_year_categories[x] for x in test_df['num_informal_institutions_last_year']]\ntest_df['relationship_to_hh_head'] = [relationship_to_hh_head_categories[x] for x in test_df['relationship_to_hh_head']]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(test_df['relationship_to_hh_head'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df.loc[test_df.education_level.isnull(), :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Onehotencode categorical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Features = encode_string(test_df['age_group'])\nfor col in features_cat_cols:\n    test_temp = encode_string(test_df[col])\n    test_Features = np.concatenate([test_Features, test_temp], axis = 1)\n\nprint(test_Features.shape)\nprint(test_Features[:2, :])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Joining numeric features:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"test_Features = np.concatenate([test_Features, np.array(test_df[features_num_cols])], axis = 1)\nprint(test_Features.shape)\nprint(test_Features[:2, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_Features.shape)\ntest_Features_reduced = sel.fit_transform(test_Features)\nprint(test_Features_reduced.shape)\n\ntest_Features_reduced = selector.transform(test_Features_reduced)\nprint(test_Features_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The whole training dataset was used to train the model, and the model is used to predict the separate test dataset:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"nr.seed(265)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = None)\nx_train = Features_reduced[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nx_test = test_Features_reduced\n# y_test = np.ravel(Labels[indx[1]])\n\nscaler = preprocessing.StandardScaler().fit(x_train[:,104:])\nx_train[:,104:] = scaler.transform(x_train[:,104:])\nx_test[:,104:] = scaler.transform(x_test[:,104:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first fit the selected models to the training data and fit them to the test data, then ensemeble the models with the assigned weights:"},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_models.fit(x_train, y_train)\navg_pred = averaged_models.predict(x_test)\n# model_xgb.fit(x_train, y_train)\n# xgb_pred = model_xgb.predict(x_test)\nmodel_lgb.fit(x_train, y_train)\nlgb_pred = model_lgb.predict(x_test)\nensemble = avg_pred*0.55 + lgb_pred*0.45","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It turned out that I achieved my highest score (0.4213) by submitting predictions using the LGB model, instead of the ensembled / averaged model. The discrepancy between the r2 score (0.4690) for the local test data and the external test data might be due to overfitting."},{"metadata":{},"cell_type":"markdown","source":"Create the final submission file:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test_df['row_id']\nsub = pd.DataFrame({ 'row_id': test_id,\n                            'poverty_probability': lgb_pred })\nsub.to_csv('prediction_result_lgb_3.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}