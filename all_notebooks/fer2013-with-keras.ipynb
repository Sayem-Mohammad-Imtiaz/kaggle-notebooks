{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hello,\n\nIn this work, I will try to recognize emotions in photographs through a convolutional neural network using a keras library."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA**"},{"metadata":{},"cell_type":"markdown","source":"\nOver 35,000 photographs collected in the data in 2013, some of the existing photographs were taken from various media publications and some of them consist of stock photographs. Our goal here is to recognize 7 different emotions (angry, happy, scared, natural, etc.) through photographs with a convolutional neural network."},{"metadata":{},"cell_type":"markdown","source":"[](http://)**Determine a Road Map**"},{"metadata":{},"cell_type":"markdown","source":"First, we will load our data and examine the various information in it.\n\nLater, we will make our train and test data to be able to model with convolutional neural network and build a model and expect to get a good result."},{"metadata":{},"cell_type":"markdown","source":"**UPLOAD DATA**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FIRST LOOK**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Datanın satır ve sütün sayıları = \", data.shape)\nprint(\"Sütünların ismi = \", data.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The emotion variable represents emotions and is our target variable.\nPixels variable expresses the value per pixel in the photos.\nUsage, on the other hand, shows which set the row it belongs to (such as training and testing).\n\nLet's look at our training and test sets and separate these sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Usage\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training = data.loc[data[\"Usage\"] == \"Training\"]\npublic_test = data.loc[data[\"Usage\"] == \"PublicTest\"]\nprivate_test = data.loc[data[\"Usage\"] == \"PrivateTest\"]\n\nprint(\"Eğitim seti = \", training.shape)\nprint(\"Genel test seti = \", public_test.shape)\nprint(\"Özel test seti = \", private_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Her bir veri setimizde hangi duygudan ne kadar olduğunu görmek istersek;"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"========================= Emotion Adetleri ===========================\")\nprint(\"train adet = \\n{}, \\npublic adet = \\n{}, \\nprivate adet = \\n{}\".format(training[\"emotion\"].value_counts(),\n      public_test[\"emotion\"].value_counts(), private_test[\"emotion\"].value_counts()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yine her bir setimizde \"emotion\", \"pixels\" ve \"usage\" değişkenlerimiz var şimdi eğitim ve test setlerimizi düzenleme işlemlerine geçelim."},{"metadata":{},"cell_type":"markdown","source":"**PREPROCESSING**"},{"metadata":{},"cell_type":"markdown","source":"We will do the same for each set. These:\n\n    1) We will categorize it by taking the \"emotion\" column which is our answers. In other words, we will output a transaction with 0 in all the remaining lines except the class it is in.\n    \n    2) We will take the pixels column in our set and turn it into a tensor and standardize it.\n     "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = training[\"emotion\"]\ntrain_labels = to_categorical(train_labels)\n\ntrain_pixels = training[\"pixels\"].str.split(\" \").tolist()\ntrain_pixels = np.uint8(train_pixels)\ntrain_pixels = train_pixels.reshape((28709, 48, 48, 1))\ntrain_pixels = train_pixels.astype(\"float32\") / 255\n\n\nprivate_labels = private_test[\"emotion\"]\nprivate_labels = to_categorical(private_labels)\n\nprivate_pixels = private_test[\"pixels\"].str.split(\" \").tolist()\nprivate_pixels = np.uint8(private_pixels)\nprivate_pixels = private_pixels.reshape((3589, 48, 48, 1))\nprivate_pixels = private_pixels.astype(\"float32\") / 255\n\n\npublic_labels = public_test[\"emotion\"]\npublic_labels = to_categorical(public_labels)\n\npublic_pixels = public_test[\"pixels\"].str.split(\" \").tolist()\npublic_pixels = np.uint8(public_pixels)\npublic_pixels = public_pixels.reshape((3589, 48, 48, 1))\npublic_pixels = public_pixels.astype(\"float32\") / 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at some photos"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(0, figsize=(12,6))\nfor i in range(1, 13):\n    plt.subplot(3,4,i)\n    plt.imshow(train_pixels[i, :, :, 0], cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Graf --> [https://www.kaggle.com/omarensaj/fer-emotion-detection-psd07](http://)"},{"metadata":{},"cell_type":"markdown","source":"So far, we have processed the data and made it ready to build a model, now we can start building the model."},{"metadata":{},"cell_type":"markdown","source":"**MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = models.Sequential()\n\n# Conv (evrişim katmanı)\nmodel_1.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n#Ortaklama katmanı\nmodel_1.add(layers.MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n\nmodel_1.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel_1.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel_1.add(layers.AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n\nmodel_1.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel_1.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel_1.add(layers.AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n\nmodel_1.add(layers.Flatten())\n\n# Tam bağlantı katmanı\nmodel_1.add(layers.Dense(1024, activation='relu'))\nmodel_1.add(layers.Dropout(0.2))\nmodel_1.add(layers.Dense(1024, activation='relu'))\nmodel_1.add(layers.Dropout(0.2))\n\nmodel_1.add(layers.Dense(7, activation='softmax'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of transactions in the model output is almost 1.5 million. Let's train the model now"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nhist = model_1.fit(train_pixels, train_labels, batch_size = 256, epochs = 30,\n                validation_data = (private_pixels, private_labels))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = hist.history[\"accuracy\"]\nval_acc = hist.history[\"val_accuracy\"]\nloss = hist.history[\"loss\"]\nval_loss = hist.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, \"bo\", label = \"Eğitim Başarısı\")\nplt.plot(epochs, val_acc, \"b\", label = \"Doğrulama Başarısı\")\nplt.title(\"Eğitim ve Doğrulama Başarısı\")\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, \"bo\", label = \"Eğitim Kaybı\")\nplt.plot(epochs, val_loss, \"b\", label = \"Doğrulama Kaybı\")\nplt.legend()\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Teşekkürler!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nModel 16-17. After the epoch (1 full round on the data), the training success increases and the test success decreases, that is, it overfit.\n\nIn this study, we were able to predict 7 different emotions by 58-60%. Data Diversification can be applied to photos in different filter sizes, more layers or training set to improve the model.\n\nSee you...\n\nThanks."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}