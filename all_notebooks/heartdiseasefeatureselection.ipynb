{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Feature Selection**\n\nUsing feature selection, we can select the set of features that are most relevant to the target variable. This ends up reducing the complexity of the model, as well as minimizing the resources required for training and inference.","metadata":{"id":"QSC_EVQOnWy7"}},{"cell_type":"code","source":"# Import packages\nimport pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE, SelectKBest, SelectFromModel, chi2, f_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt","metadata":{"id":"wYD1hdL4hzuh","execution":{"iopub.status.busy":"2021-07-06T05:08:31.32363Z","iopub.execute_input":"2021-07-06T05:08:31.323993Z","iopub.status.idle":"2021-07-06T05:08:31.331042Z","shell.execute_reply.started":"2021-07-06T05:08:31.323961Z","shell.execute_reply":"2021-07-06T05:08:31.330017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_filepath = \"../input/heart-disease-uci/heart.csv\"","metadata":{"id":"d9PS-fItnaa4","execution":{"iopub.status.busy":"2021-07-06T05:08:31.343393Z","iopub.execute_input":"2021-07-06T05:08:31.343738Z","iopub.status.idle":"2021-07-06T05:08:31.348369Z","shell.execute_reply.started":"2021-07-06T05:08:31.343709Z","shell.execute_reply":"2021-07-06T05:08:31.347094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(csv_filepath, na_values=['na', '--'])\n\n# See the dataset\ndisplay(df.head())\n\n# Check info\ndisplay(df.info())    ","metadata":{"id":"OEkrHgssnadH","outputId":"dd70f7fa-3f0e-4910-d204-cb43757c0d38","execution":{"iopub.status.busy":"2021-07-06T05:08:31.365975Z","iopub.execute_input":"2021-07-06T05:08:31.366384Z","iopub.status.idle":"2021-07-06T05:08:31.406125Z","shell.execute_reply.started":"2021-07-06T05:08:31.366348Z","shell.execute_reply":"2021-07-06T05:08:31.404706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Describe columns\nprint(df.describe(include='all'))","metadata":{"id":"CLmYWu-Cnp7A","outputId":"b6527d0a-8141-4122-e558-34a69c32d51c","execution":{"iopub.status.busy":"2021-07-06T05:08:31.408335Z","iopub.execute_input":"2021-07-06T05:08:31.408798Z","iopub.status.idle":"2021-07-06T05:08:31.458481Z","shell.execute_reply.started":"2021-07-06T05:08:31.408747Z","shell.execute_reply":"2021-07-06T05:08:31.457194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check NaN values and remove the unwanted features**","metadata":{"id":"A2mSSRbVnucE"}},{"cell_type":"code","source":"# To identify the total missing values \nprint(df.isnull().sum())","metadata":{"id":"K94vddyanp-B","outputId":"c59a736e-bdc9-44fe-df4e-205d30e93cee","execution":{"iopub.status.busy":"2021-07-06T05:08:31.459789Z","iopub.execute_input":"2021-07-06T05:08:31.460141Z","iopub.status.idle":"2021-07-06T05:08:31.467523Z","shell.execute_reply.started":"2021-07-06T05:08:31.460108Z","shell.execute_reply":"2021-07-06T05:08:31.466316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split the data**\n\nNow, we split the dataset into feature vectors X and target vector (stroke) Y to fit a RandomForestClassifier. ","metadata":{"id":"_OgnsLD6oBlz"}},{"cell_type":"code","source":"# Split feature and target vectors\nX = df.drop([\"target\"], axis=1)\nY = df[\"target\"]","metadata":{"id":"gVzLRE8Pn-rL","execution":{"iopub.status.busy":"2021-07-06T05:08:31.469783Z","iopub.execute_input":"2021-07-06T05:08:31.470155Z","iopub.status.idle":"2021-07-06T05:08:31.485016Z","shell.execute_reply.started":"2021-07-06T05:08:31.47012Z","shell.execute_reply":"2021-07-06T05:08:31.48331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest Model**","metadata":{"id":"Skvd5LlqoH14"}},{"cell_type":"code","source":"def RF_model(X, Y):\n  # define the model \n  model = RandomForestClassifier(criterion=\"entropy\", random_state=42)\n  # Train the model\n  model.fit(X, Y)\n\n  return model\n\n# Calculate metrics\ndef cal_accuracy(model, X_test_scaled, Y_test):\n  # Predict model\n  y_pred = model.predict(X_test_scaled)\n\n  # Calculate metrics for evaluating the model\n  roc = roc_auc_score(Y_test, y_pred)\n  print('roc score is : {}'.format(roc))\n\n  accuracy = accuracy_score(Y_test, y_pred)\n  print('Accuracy score is : {}'.format(accuracy))\n\n  precision = precision_score(Y_test, y_pred)\n  print('Precision score is : {}'.format(precision))\n\n  recall = recall_score(Y_test, y_pred)\n  print('Recall score is : {}'.format(recall))\n\n  f1 = f1_score(Y_test, y_pred)\n  print('f1 score is : {}'.format(f1))\n\n  return accuracy, roc, precision, recall, f1\n\ndef train_model(X, Y):\n  # Split data to train and test\n  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n\n  # Normalize feature's values\n  scaler = StandardScaler().fit(X_train)\n  X_train_scaled = scaler.transform(X_train)\n  X_test_scaled = scaler.transform(X_test)\n\n  # Call the RF_model \n  model = RF_model(X_train_scaled, Y_train)\n\n  # Make predictions on test dataset and calculate metrics.\n  accuracy, roc, precision, recall, f1 = cal_accuracy(model, X_test_scaled, Y_test)\n\n  return accuracy, roc, precision, recall, f1\n\ndef evaluate_model(X, Y):\n  # Train the model and compute metrics\n  accuracy, roc, precision, recall, f1 = train_model(X, Y)\n\n  # Display all metrics in a dataframe\n  metrics_df = pd.DataFrame([[accuracy, roc, precision, recall, f1, X.shape[1]]], \n                            columns=[\"Accuracy\", \"ROC\", \"Precision\", \"Recall\", \"F1 Score\", \"Feature Count\"])\n\n  return metrics_df","metadata":{"id":"vMwL7gfbn-t6","execution":{"iopub.status.busy":"2021-07-06T05:08:31.487692Z","iopub.execute_input":"2021-07-06T05:08:31.488201Z","iopub.status.idle":"2021-07-06T05:08:31.503102Z","shell.execute_reply.started":"2021-07-06T05:08:31.488153Z","shell.execute_reply":"2021-07-06T05:08:31.501706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df = evaluate_model(X, Y) \nmetrics_df.index = [\"All features\"]\n\nresults = metrics_df\ndisplay(metrics_df.head())","metadata":{"id":"P-cvcYChn-w9","outputId":"b646a86e-4fce-467e-a962-7c58a9567ce4","execution":{"iopub.status.busy":"2021-07-06T05:08:31.505111Z","iopub.execute_input":"2021-07-06T05:08:31.505589Z","iopub.status.idle":"2021-07-06T05:08:31.769064Z","shell.execute_reply.started":"2021-07-06T05:08:31.50554Z","shell.execute_reply":"2021-07-06T05:08:31.767937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Calculate and Visualize the Correlation Matrix**\n\nTo find which feature has the highest correlation.","metadata":{"id":"b_B2Nk63ob0K"}},{"cell_type":"code","source":"# correlation matrix\ncor = df.corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\nplt.show()","metadata":{"id":"nIAvVH3vn-y5","outputId":"f1618812-6855-41c9-ad17-47b6d95f8d74","execution":{"iopub.status.busy":"2021-07-06T05:08:31.771169Z","iopub.execute_input":"2021-07-06T05:08:31.771459Z","iopub.status.idle":"2021-07-06T05:08:33.040829Z","shell.execute_reply.started":"2021-07-06T05:08:31.771431Z","shell.execute_reply":"2021-07-06T05:08:33.036586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation with the target variable (Stroke)**","metadata":{"id":"4RWamlqyokBv"}},{"cell_type":"code","source":"# Get value of the correlation\ntarget_correlation = abs(cor[\"target\"])\n\n# Select highly correlated features (thresold = 0.2)\nhigh_corr_feature = target_correlation[target_correlation > 0.2]\n\n# Determine the name of features\nnames = []\nfor idx, value in high_corr_feature.iteritems():\n  names.append(idx)\n\n# Remove the name of target value\nnames.remove(\"target\")\nprint(\"features are strongly correlated with the target : {}\".format(names))","metadata":{"id":"cQDXhlnhohIw","outputId":"ef83e9ee-7beb-45bf-b071-0727aa88854f","execution":{"iopub.status.busy":"2021-07-06T05:08:33.043469Z","iopub.execute_input":"2021-07-06T05:08:33.043833Z","iopub.status.idle":"2021-07-06T05:08:33.06844Z","shell.execute_reply.started":"2021-07-06T05:08:33.043797Z","shell.execute_reply":"2021-07-06T05:08:33.067391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the new features with the model\ndf_strong_feature = evaluate_model(df[names], Y)\ndf_strong_feature.index = [\"Strong Features\"]\n\n# Add to the previous results\nresults = results.append(df_strong_feature)\ndisplay(results.head())","metadata":{"id":"4JHHPFjfohLP","outputId":"37fcd920-d27a-43a5-9283-e269a5108cfd","execution":{"iopub.status.busy":"2021-07-06T05:08:33.070703Z","iopub.execute_input":"2021-07-06T05:08:33.07103Z","iopub.status.idle":"2021-07-06T05:08:33.325015Z","shell.execute_reply.started":"2021-07-06T05:08:33.071001Z","shell.execute_reply":"2021-07-06T05:08:33.324097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation with strong features\n# correlation matrix\ncor_features = df[names].corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(cor_features, annot=True, cmap=plt.cm.PuBu)\nplt.show()","metadata":{"id":"B67-jLp4ohNf","outputId":"87660fbe-d945-4de5-9564-368e35a382a7","execution":{"iopub.status.busy":"2021-07-06T05:08:33.326108Z","iopub.execute_input":"2021-07-06T05:08:33.326391Z","iopub.status.idle":"2021-07-06T05:08:34.010238Z","shell.execute_reply.started":"2021-07-06T05:08:33.326363Z","shell.execute_reply":"2021-07-06T05:08:34.009326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Subset of features**","metadata":{"id":"9DftWPDZou34"}},{"cell_type":"code","source":"# Select a subset of features\nnew_feature_corr = df[['cp', 'thalach', 'exang', 'oldpeak', 'slope']].corr()\n\nplt.figure(figsize=(12,10))\nsns.heatmap(new_feature_corr, annot=True, cmap=plt.cm.Blues)\nplt.show()","metadata":{"id":"gxiWLshXohQa","outputId":"46714523-e6a1-4c8a-e013-076829cff098","execution":{"iopub.status.busy":"2021-07-06T05:08:34.011283Z","iopub.execute_input":"2021-07-06T05:08:34.011692Z","iopub.status.idle":"2021-07-06T05:08:34.346183Z","shell.execute_reply.started":"2021-07-06T05:08:34.011663Z","shell.execute_reply":"2021-07-06T05:08:34.345264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the name of subsets in highl correlation with other features list and then remove them\nsubset_features = [i for i in names if i not in ['cp', 'thalach', 'exang', 'oldpeak']]\n\n# Check the new features with the model\nsubset_features_df = evaluate_model(df[subset_features], Y)\nsubset_features_df.index= [\"Subset Features\"]\n\nresults = results.append(subset_features_df)\nresults.head()","metadata":{"id":"fjfIVpVZoxC_","outputId":"a0d470f4-733a-44d4-f396-441a06cd17dd","execution":{"iopub.status.busy":"2021-07-06T05:08:34.347274Z","iopub.execute_input":"2021-07-06T05:08:34.347682Z","iopub.status.idle":"2021-07-06T05:08:34.701221Z","shell.execute_reply.started":"2021-07-06T05:08:34.347654Z","shell.execute_reply":"2021-07-06T05:08:34.700214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Filter methods for feature selection**\n\nThere are three ways to filter the features.","metadata":{"id":"QD4zST-ao1nM"}},{"cell_type":"markdown","source":"**1- Univariate Selection with Sci-Kit Learn**\n\nScikit learn have several methods which can be used for feature selection/dimensionality reduction on sample sets. For more information, you can visit [this](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) website.\n\nIn this section, we are going to use SelectKBest() method to select the top 10 features.","metadata":{"id":"mVjr8fIyo20D"}},{"cell_type":"code","source":"def univariate_selection():\n  # Split data to train and test\n  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n\n  # Normalize feature's values\n  scaler = StandardScaler().fit(X_train)\n  X_train_scaled = scaler.transform(X_train)\n  X_test_scaled = scaler.transform(X_test)\n\n  # Using SelectKBest method we can select top 7 features based on f-test\n  selector = SelectKBest(f_classif, k=7)\n\n  # Fit selector to scaled data, then transform it\n  X_new = selector.fit_transform(X_train_scaled, Y_train)\n  \n  # See the results\n  feature_index = selector.get_support()\n  # Romove the target value fro dataframe\n  df_new = df.drop(\"target\", axis=1)\n  for name, idx in zip(df_new.columns, feature_index):\n    print(\"%s: %s\" % (name, idx))\n\n  # Drop the target variable\n  feature_names = df.drop(\"target\", axis=1).columns[feature_index]\n\n  return feature_names","metadata":{"id":"P-QR6wbnoxFQ","execution":{"iopub.status.busy":"2021-07-06T05:08:34.702903Z","iopub.execute_input":"2021-07-06T05:08:34.703246Z","iopub.status.idle":"2021-07-06T05:08:34.712055Z","shell.execute_reply.started":"2021-07-06T05:08:34.703209Z","shell.execute_reply":"2021-07-06T05:08:34.71071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names_univariate_feature = univariate_selection()\n\n# Check the univariate features with the model\ndf_univariate_feature = evaluate_model(df[names_univariate_feature], Y)\ndf_univariate_feature.index = [\"F-test\"]\n\n# Add to the previous results\nresults = results.append(df_univariate_feature)\ndisplay(results.head())","metadata":{"id":"eLoq8E1loxHm","outputId":"497c9bbf-a29d-4408-a2d5-1494dc77012c","execution":{"iopub.status.busy":"2021-07-06T05:08:34.713434Z","iopub.execute_input":"2021-07-06T05:08:34.713796Z","iopub.status.idle":"2021-07-06T05:08:34.989515Z","shell.execute_reply.started":"2021-07-06T05:08:34.71376Z","shell.execute_reply":"2021-07-06T05:08:34.988443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2- Wrapper Methods**\n\nWrapper methods use a specific machine learning algorithm for feature selection process trying to measure the effectiveness of a particular subset of features.\n\nMost commonly used techniques under wrapper methods are:\n\n1- Forward selection : uses k-fold cross validation scores to decide which features to add or remove\n\n2- Backward elimination: starts with all predictors and eliminates one-by-one iteratively. One of the most popular algorithms is Recursive Feature Elimination (RFE) which eliminates less important predictors based on feature importance ranking.\n\n3- Bi-directional elimination(Stepwise Selection): is based on a combination of forward selection and backward elimination.","metadata":{"id":"GEfW1h0wo_gv"}},{"cell_type":"markdown","source":"**Recursive Feature Elimination**\n\nWe are going to use Recursive Feature Elimination, which wraps around the selected model (random forest in this case) to perform feature selection.\n\nOne of the popular libraries in Python which can be used to perform wrapper method for feature selection is Recursive Feature Elimination from Scikit-learn.\n\nFor more details, you can visit [official doc](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) ","metadata":{"id":"a1qiCGLkpDau"}},{"cell_type":"code","source":"def recursive_feature_selection():\n  # Split data to train and test\n  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n\n  # Normalize feature's values\n  scaler = StandardScaler().fit(X_train)\n  X_train_scaled = scaler.transform(X_train)\n  X_test_scaled = scaler.transform(X_test)\n\n  # Set the model\n  RF_model = RandomForestClassifier(criterion=\"entropy\", random_state=42)\n\n  # Wrap RFE around the model\n  rfe = RFE(RF_model, 7)\n\n  # Train the RFE\n  rfe.fit(X_train_scaled, Y_train)\n  feature_names = df.drop(\"target\", axis=1).columns[rfe.get_support()]\n\n  return feature_names","metadata":{"id":"GD6_sR8woxMA","execution":{"iopub.status.busy":"2021-07-06T05:08:34.991028Z","iopub.execute_input":"2021-07-06T05:08:34.991327Z","iopub.status.idle":"2021-07-06T05:08:34.999273Z","shell.execute_reply.started":"2021-07-06T05:08:34.991298Z","shell.execute_reply":"2021-07-06T05:08:34.997471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names_rfe = recursive_feature_selection()\n\n# Check the rfe features with the model\ndf_rfe_feature = evaluate_model(df[feature_names_rfe], Y)\ndf_rfe_feature.index = [\"RFE\"]\n\n# Add to the previous results\nresults = results.append(df_rfe_feature)\ndisplay(results.head())","metadata":{"id":"z3R44nakpG-B","outputId":"f75d8ac3-8eb3-46f6-eaa6-4c1eb3cb161f","execution":{"iopub.status.busy":"2021-07-06T05:08:35.001464Z","iopub.execute_input":"2021-07-06T05:08:35.002003Z","iopub.status.idle":"2021-07-06T05:08:36.870761Z","shell.execute_reply.started":"2021-07-06T05:08:35.001947Z","shell.execute_reply":"2021-07-06T05:08:36.869529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Embedded Methods**\n\nEmbedded methods using the construction of the machine learning algorithm complete the feature selection process.\\\nIn fact, embedded methods tackle those problems encountering with the filter and wrapper methods.","metadata":{"id":"9hlIVH99sTiA"}},{"cell_type":"markdown","source":"In this section, we want to explore two embedded feature selection namely tree-based methods and regularization.\n\n**1- Tree-based methods**\n\nTree-based algorithms and models, such as random forest, are well-established algorithms that are able to specify the feature importance to select features.\n\nIn order to select features from the trained model, we can use [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html).","metadata":{"id":"gwfrFUUssUO-"}},{"cell_type":"code","source":"# Get Feature importance from RandomForestClassifier model\ndef tree_based_feature_importance_fn(X, Y):\n  # Split data to train and test\n  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n\n  # Normalize feature's values\n  scaler = StandardScaler().fit(X_train)\n  X_train_scaled = scaler.transform(X_train)\n  X_test_scaled = scaler.transform(X_test)\n\n  # Set the model\n  rf_model = RandomForestClassifier()\n  rf_model = rf_model.fit(X_train_scaled, Y_train)\n\n  # Plot feature importance\n  plt.figure(figsize=(12,10))\n  feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\n  feature_importance.sort_values(ascending=False).plot(kind='barh')\n  plt.show()\n\n  return rf_model\n\ndef select_features(model):\n  \n  # Set the model\n  selection = SelectFromModel(model, prefit=True, threshold=0.013)\n\n  # see the selected features\n  selected_features = selection.get_support()\n  feature_names = df.drop(\"target\", axis=1).columns[selected_features]\n\n  return feature_names","metadata":{"id":"nyucOeDZpHAW","execution":{"iopub.status.busy":"2021-07-06T05:08:36.872477Z","iopub.execute_input":"2021-07-06T05:08:36.872906Z","iopub.status.idle":"2021-07-06T05:08:36.88279Z","shell.execute_reply.started":"2021-07-06T05:08:36.872845Z","shell.execute_reply":"2021-07-06T05:08:36.881605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_based_model = tree_based_feature_importance_fn(X, Y)\nfeature_names = select_features(tree_based_model)","metadata":{"id":"DqfHiZ53saj9","outputId":"14609f25-fe3d-4d12-a026-e201957a7b42","execution":{"iopub.status.busy":"2021-07-06T05:08:36.884626Z","iopub.execute_input":"2021-07-06T05:08:36.885094Z","iopub.status.idle":"2021-07-06T05:08:37.36963Z","shell.execute_reply.started":"2021-07-06T05:08:36.885047Z","shell.execute_reply":"2021-07-06T05:08:37.368466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the rfe features with the model\ndf_feature_importance = evaluate_model(df[feature_names], Y)\ndf_feature_importance.index = [\"Feature Importance\"]\n\n# Add to the previous results\nresults = results.append(df_feature_importance)\ndisplay(results.head(n=10))","metadata":{"id":"3O-xZAnxpHCl","outputId":"851eb23d-b1bd-4fef-b03f-30bbe692610d","execution":{"iopub.status.busy":"2021-07-06T05:08:37.372466Z","iopub.execute_input":"2021-07-06T05:08:37.372804Z","iopub.status.idle":"2021-07-06T05:08:37.62282Z","shell.execute_reply.started":"2021-07-06T05:08:37.372772Z","shell.execute_reply":"2021-07-06T05:08:37.621522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2- Regularization**\n\nRegularization introduces a penalty  to the different parameters of a model to reduce its freedom. \n\nThere are three main types of regularization for linear models which we are going to use **lasso regression or L1 regularization** in this stage.","metadata":{"id":"uhtJyVPksgaw"}},{"cell_type":"markdown","source":"**L1 Regularization**\n\nL1 regularization adds a penalty to the loss function which leads to the least important features being eliminated.\n\nFor learning algorithm, we can use Linear Support Vector Classification or LinearSVC (for more information, see [this](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) document). \n\nMoreover, after training the LinearSVC model, we use SelectFromModel() to select features.","metadata":{"id":"LVHAfkTashDl"}},{"cell_type":"code","source":"def l1_regularization_fn(X, Y):\n  # Split data to train and test\n  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n\n  # Normalize feature's values\n  scaler = StandardScaler().fit(X_train)\n  X_train_scaled = scaler.transform(X_train)\n  X_test_scaled = scaler.transform(X_test)\n\n  # Select L1 regulated features from LinearSVC model\n  model = LinearSVC(C=1, penalty=\"l1\", dual=False)   # Prefer dual=False when n_samples > n_features\n  selection = SelectFromModel(model)\n  selection.fit(X_train_scaled, Y_train)\n\n  # Determine feature selected\n  feature_selected = selection.get_support()\n  feature_names = df.drop(\"target\", axis=1).columns[feature_selected]\n\n  return feature_names","metadata":{"id":"oMVtzow3pHGL","execution":{"iopub.status.busy":"2021-07-06T05:08:37.624467Z","iopub.execute_input":"2021-07-06T05:08:37.624768Z","iopub.status.idle":"2021-07-06T05:08:37.632075Z","shell.execute_reply.started":"2021-07-06T05:08:37.62474Z","shell.execute_reply":"2021-07-06T05:08:37.631138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_regularization_feature_names = l1_regularization_fn(X, Y)\n\n# Check the rfe features with the model\ndf_l1_regularization = evaluate_model(df[lr_regularization_feature_names], Y)\ndf_l1_regularization.index = [\"L1 Regularization\"]\n\n# Add to the previous results\nresults = results.append(df_l1_regularization)\ndisplay(results.head(n=10))","metadata":{"id":"mW56uU3NoxOH","outputId":"acc769c6-cc54-4194-e896-500afde732f6","execution":{"iopub.status.busy":"2021-07-06T05:08:37.633615Z","iopub.execute_input":"2021-07-06T05:08:37.634033Z","iopub.status.idle":"2021-07-06T05:08:37.914071Z","shell.execute_reply.started":"2021-07-06T05:08:37.633996Z","shell.execute_reply":"2021-07-06T05:08:37.912858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"dUVQwVa3oxP4"},"execution_count":null,"outputs":[]}]}