{"cells":[{"metadata":{"_uuid":"c708b7e1020677625d97ad8082cf4a79b9d78800"},"cell_type":"markdown","source":"Here I am using sklearn library to implement all the major algorithms from Regression part of Supervised Learning. Will use Matplotlib to draw the outcomes or model predictions."},{"metadata":{"_uuid":"b09786c8d26169fade8a2b1b1fb6be0492bf1b2b"},"cell_type":"markdown","source":"Global Imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9964675d469a41b959ea1ee45326904ca97611f"},"cell_type":"markdown","source":"<h1>1. Simple Linear Regression</h1>"},{"metadata":{"_uuid":"cb8a329f10825ecb4c047b705c92d72beb4b706a"},"cell_type":"markdown","source":"Algorithm specific imports"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6ac535ec463cc43295d57cf352f7fcc498106bc4"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7c2b5c97b1d9a9aa53589492f25e3c5aec66fd4"},"cell_type":"markdown","source":"Extracting training and test dataset "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/salary-data/Salary_Data.csv')\n\nX = dataset.iloc[:,0].values\ny = dataset.iloc[:,1].values\n\n# We are going to keep 20% of the dataset in test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/5, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcb2a0a57c15a43df74f81a8e4edbfb8621fd2d9"},"cell_type":"markdown","source":"Training model and making prediction"},{"metadata":{"trusted":true,"_uuid":"4d1fbe0ee22a4347d7cfb130ef01fb3dacf7c33d","collapsed":true},"cell_type":"code","source":"linear_regressor = LinearRegression()\nlinear_regressor.fit(np.array(X_train.reshape(-1, 1)), y_train.reshape(-1, 1))\n\ny_predict = linear_regressor.predict(X_test.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2c7a2114d599ac481374e36390f454ac1dd8b89","collapsed":true},"cell_type":"code","source":"# Plot points and fit line for training data\nplt.scatter(X_train, y_train, color='teal', edgecolors='black', label='Training-set observation points')\nplt.plot(X_train.reshape(-1, 1), linear_regressor.predict(X_train.reshape(-1, 1)), color='grey', label='Fit Regression Line')\nplt.title('Salary vs Experience')\nplt.xlabel('Experience (in years)')\nplt.ylabel('Salary (in USD)')\n\n# plot scatter points and line for test data\nplt.scatter(X_test, y_test, color='red', edgecolors='black', label='Test-set observation points')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3cddc6e5b947a6033c584ab5531509e8bbd77ad","collapsed":true},"cell_type":"markdown","source":"\n<h1>2. Multivarite Linear Regression</h1>"},{"metadata":{"_uuid":"85e4828457dc131cbaded191c31c5d7bc9727064"},"cell_type":"markdown","source":"Import libraries"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2475338e5c31e89d35fbb1828286a60e8077bee2"},"cell_type":"code","source":"# We already have imported LinearRegression in above algorithm, going to use the same\nimport statsmodels.formula.api as sm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af693cc6520093540b5c425ad092acb5abdf382c"},"cell_type":"markdown","source":"Extracting training and test dataset"},{"metadata":{"trusted":true,"_uuid":"b98570c2ceff3936f3c88aa667ef42730f51a60a","collapsed":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/m-50-startups/50_Startups.csv')\n\nX = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1].values\n\n# Handle categorical variable - State column\nlabelencoder_X = LabelEncoder()\nX[:,3] = labelencoder_X.fit_transform(X[:, 3])\nhotonencoder_X = OneHotEncoder(categorical_features = [3])\nX = hotonencoder_X.fit_transform(X).toarray()\n\n# Avoiding the dummy trap\nX = X[:, 1:]\n\n# We are going to keep 20% of the dataset in test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/5, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"126bcac242105b50048c6460ae62ae182397384a","collapsed":true},"cell_type":"markdown","source":"Algorithm execution"},{"metadata":{"trusted":true,"_uuid":"5ae45ec7d5d12d82e61c319003ccb0ecb311bf83","collapsed":true},"cell_type":"code","source":"multiple_linear_regressor = LinearRegression()\nmultiple_linear_regressor.fit(X_train, y_train)\n\n# y_pred contains all the values predicted by trained model\ny_pred = multiple_linear_regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a371d695e78cd2039aa1ea31acc52310e8410b0"},"cell_type":"markdown","source":"*  <h2> Backward Elimination for model optimization</h2>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"79904bba0fda15a3ad84204d0b592bbda0ad8548"},"cell_type":"markdown","source":"Our current model is not optimal, since we don't know the impact of every predictor (or feature) on target variable. So we are going to use Backward Elimanation technique to make it optimal by removing insignificant predictors."},{"metadata":{"trusted":true,"_uuid":"4ba570b50b7c4094b961a353e3515e98493e08ec","collapsed":true},"cell_type":"code","source":"# adding const X0 at the very start of matrix\nX = np.append(arr=np.ones((50, 1)).astype(int), values=X, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"525c8558f6df3d7b261e3ca498734a26cf410f92","collapsed":true},"cell_type":"code","source":"# Starting Backword Elimination Steps\n# Step 1 - Taking all the predictors(features) in X_opt. Our X_opt will contain only most optimal predictors. We will keep optimizing it\nX_opt = X[:, [0, 1, 2, 3, 4, 5]]\n# Step 2 - fitting the full model with all predictors\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n# Step 3 - analysing & fetching the predictor with value more than 0.05 \nregressor_OLS.summary()\n# Step 4 - removing the predictor [In this case column with index 2]\nX_opt = X[:, [0, 1, 3, 4, 5]]\n# Step 5 - Re-fit the model with removed predictor\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n\n### Repeating 3, 4, 5 till we don't get all the predictor with Significant level < 0.5\n# Removed 2nd index column and done fitting.\n\n# Step 3 - analysing & fetching the predictor with value more than 0.05 \nregressor_OLS.summary()\n# Step 4 - removing the predictor [In this case column with index 1]\nX_opt = X[:, [0, 3, 4, 5]]\n# Step 5 - Re-fit the model with removed predictor\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n# Removed 1st index column and done fitting.\n\n# Step 3 - analysing & fetching the predictor with value more than 0.05 \nregressor_OLS.summary()\n# Step 4 - removing the predictor [In this case column with index 4]\nX_opt = X[:, [0, 3, 5]]\n# Step 5 - Re-fit the model with removed predictor\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n# Removed 4th index column and done fitting.\n\n# Step 3 - analysing & fetching the predictor with value more than 0.05 \nregressor_OLS.summary()\n# Step 4 - removing the predictor [In this case column with index 5]\nX_opt = X[:, [0, 3]]\n# Step 5 - Re-fit the model with removed predictor\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n# Removed 5th index column and done fitting.\nregressor_OLS.summary()\n\n# FINISHED","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bb5bb24257308099af3856c024a5481b059818b"},"cell_type":"markdown","source":"Now all predictors are below 0.05 significant level and our model is optimized as per Backward Elimination"},{"metadata":{"trusted":true,"_uuid":"47316eb8921980f9bb122851fd02894ca17d4487","collapsed":true},"cell_type":"markdown","source":"<h1>3. Polynomial Regression</h1>"},{"metadata":{"_uuid":"f76b43600873cf71f016469fab0d9413b07c352f"},"cell_type":"markdown","source":"Import algorithm specific modules"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"69ebea208f6c8c4edd21c5190cad2f1928a37d3c"},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a696cd80a5e4ee1be6070815dfab0af3adb856f","collapsed":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\n\nX = dataset.iloc[:,1:2].values\ny = dataset.iloc[:,-1].values\n\n# We don't have enough record in our dataset, so we are going to skip the step of spliting our X, y into test and train dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6baa326e4940d1a61eb4d1d49b5ecfca7450f0b6"},"cell_type":"markdown","source":"From here. we are simply generating the matrix for X^0, X^1 and X^2"},{"metadata":{"trusted":true,"_uuid":"a953382cb685f5197635957b1b8f7b05957b84e0","collapsed":true},"cell_type":"code","source":"poly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd9fe11e11442d4005c6d4284d5058e40c48eafb","collapsed":true},"cell_type":"code","source":"# linear regression model\nlinear_reg_model = LinearRegression()\nlinear_reg_model.fit(X, y)\n\n# polynomial regression model\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_poly, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a340365b36547baf1d92c8a47605914872e6ca7a"},"cell_type":"markdown","source":"We have trained both models linear and polynomial. Now we are going to compare the plots of both models\n"},{"metadata":{"trusted":true,"_uuid":"2a9f5ebcf6abb3b1b3bd14606d3d125c76b65324","collapsed":true},"cell_type":"code","source":"plt.scatter(X, y, color='red', label='Actual observation points')\nplt.plot(X, linear_reg_model.predict(X), label='Linear regressor fit curve')\nplt.plot(X, poly_reg_model.predict(poly_reg.fit_transform(X)), label='Polynmial regressor fit line')\nplt.title('Truth or bluff (Linear Regression)')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c503d40858b4757ad066fe4daa40fab0a3537678"},"cell_type":"markdown","source":"<h1>4. Simple Vector Regression (SVR)</h1>"},{"metadata":{"_uuid":"f5e2dbfc36f0ce278219bac898ba766d4df8b317"},"cell_type":"markdown","source":"Import model specific libraries"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e4482e63e16e04e192a2d001224cbad0215cb651"},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c56cac57440ff82c7f77a052aaaad56e3b48729e","collapsed":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\n\nX = dataset.iloc[:,1:2].values\ny = dataset.iloc[:,-1].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"778dad88db623b1be710ab10a85e162e1cefa346"},"cell_type":"markdown","source":"Performing feature scaling, since it is not an inbuild feature of SVR class in sk-learn"},{"metadata":{"trusted":true,"_uuid":"1ff45a8de538a40aa2cd97d50d3ab465b4275381","collapsed":true},"cell_type":"code","source":"scale_X = StandardScaler()\nscale_y = StandardScaler()\n\nX = scale_X.fit_transform(X)\ny = scale_y.fit_transform(y.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"718b1e0730771b117192eee0895c26b2d3f2b88d"},"cell_type":"markdown","source":"Algorithm execution"},{"metadata":{"trusted":true,"_uuid":"626e01e5a3e57359fbb447a515b9efdf39e926ec","collapsed":true},"cell_type":"code","source":"svr_regressor = SVR(kernel='rbf', gamma='auto')\nsvr_regressor.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55966707cd13b2ca6178ffe807015f5c87f6a047"},"cell_type":"markdown","source":"Visualizing the SVR predictions"},{"metadata":{"trusted":true,"_uuid":"1fd1ae05d36d9ae02c81dd90cd564a7087db1978","collapsed":true},"cell_type":"code","source":"plt.scatter(X, y, color='red', label='Actual observation points')\nplt.plot(X, svr_regressor.predict(X), label='SVR regressor')\nplt.title('Truth or bluff (SVR Regression)')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cf9c3ea710e6288036f5e9d71a7222b44ac62a1"},"cell_type":"markdown","source":"Predicting salary for someone who has 6.5 years of experience."},{"metadata":{"trusted":true,"_uuid":"c68ea22d7d6dc36e7428c51b5fa598a00c669bf4","collapsed":true},"cell_type":"code","source":"scale_y.inverse_transform(svr_regressor.predict(scale_X.transform(np.array([[6.5]]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5acda1d15c065c3a8e5fb5ebfdd66011c76afc27","collapsed":true},"cell_type":"markdown","source":"<h1>5. Decision Tree - Regression</h1>"},{"metadata":{"_uuid":"4ccd8fe73781c54215766ecb32531c8b19292ac0"},"cell_type":"markdown","source":"Algorithm specific imports"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0e97230a83909f1d85ca4fcf8c348db4e8ef0372"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2f3ebfdf45e27ed05c9271f7e532a5978cb6859"},"cell_type":"markdown","source":"Read dataset and extract feature and target variable from that."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5cb51b5e93b8718ec81f1ca238e5b54013cc2f07"},"cell_type":"code","source":"dataset = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\n\nX = dataset.iloc[:, 1:2].values\ny = dataset.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06279b7454c8796d05cb9b4dbeea73769b7621ef"},"cell_type":"markdown","source":"No need to perform feature scaling. Since it will get taken care by the library itself."},{"metadata":{"_uuid":"7c19a60f93d2a1fbcd25ad61d5e8187ec9e0048f"},"cell_type":"markdown","source":"Creating regressor"},{"metadata":{"trusted":true,"_uuid":"49fe1152437a63da95305f0a5211b01237d43551","collapsed":true},"cell_type":"code","source":"tree_regressor = DecisionTreeRegressor(random_state = 0)\ntree_regressor.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8e8bbb05a20a7521e8512811f1e0f87a7d59797"},"cell_type":"markdown","source":"Visualizing the graph"},{"metadata":{"trusted":true,"_uuid":"97cb731fe91bf004a20df10a05bdf8b3431efef2","collapsed":true},"cell_type":"code","source":"X_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape(len(X_grid), 1)\n\nplt.scatter(X, y, color='red', label='Actual observation points')\nplt.plot(X_grid, tree_regressor.predict(X_grid), label='Tree regressor')\nplt.title('Truth or bluff (Tree Regression)')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06f6a5b158ba091eefbed1718a7b5ff9de2b2390"},"cell_type":"markdown","source":"Predicting salary for someone who has 6.5 years of experience."},{"metadata":{"trusted":true,"_uuid":"7e7a6e3e3d11b71718f20443991bdc8ec4ac55c1","collapsed":true},"cell_type":"code","source":"tree_regressor.predict([[6.5]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"44f68eed0aac9163e9b4d7e211370e83aa1fdf75"},"cell_type":"markdown","source":"Here we are not getting the good prediction, may be because of 2D plain (or single feature). It should give better predictions for 3d plan."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8bbff81837a185916225282a71a1fafb81cb1e73"},"cell_type":"markdown","source":"<h1>6. Random Forest Regression</h1>"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e6c0ff62a46af9b0083e22168a07fd52e941efdc"},"cell_type":"markdown","source":"Algorithm specific imports"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"971122fab95855f6103c14bf647d56bd2ec37e4a"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f689705eb6bc542cf2db9278e563e510975d552b"},"cell_type":"code","source":"dataset = pd.read_csv('../input/polynomial-position-salary-data/Position_Salaries.csv')\n\nX = dataset.iloc[:, 1:2].values\ny = dataset.iloc[:, -1].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97db86896e3af6dfa32c52d6a146eee02f11c8de"},"cell_type":"markdown","source":"Regressor creation"},{"metadata":{"trusted":true,"_uuid":"13aa60e252dae42ec0cb0d5b66f77baa98eecb28","collapsed":true},"cell_type":"code","source":"forest_regressor = RandomForestRegressor(n_estimators = 300, random_state = 0)\nforest_regressor.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9997a329698cd332ae107141aab17d569d06f7a0"},"cell_type":"markdown","source":"Plot and visualize graph"},{"metadata":{"trusted":true,"_uuid":"4e5984714310ab6ef6f5f676cc6c24ef23d6d566","collapsed":true},"cell_type":"code","source":"X_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape(len(X_grid), 1)\n\nplt.scatter(X, y, color='red', label='Actual observation points')\nplt.plot(X_grid, forest_regressor.predict(X_grid), label='Random Forest regressor')\nplt.title('Truth or bluff (Random Forest Regression)')\nplt.xlabel('Position Level')\nplt.ylabel('Salary')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2f9451915fdfc3fbf9c8fb83cc87994289545df"},"cell_type":"markdown","source":"Predict salary for 6.5 year experienced employee, we are taking same value of x so that we can compare the predicted value form Polynomial's and Decision Tree's prediction"},{"metadata":{"trusted":true,"_uuid":"25562de259e14729c2986968f2241a3d6c20b36e","collapsed":true},"cell_type":"code","source":"forest_regressor.predict([[6.5]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ca89b2476ea1fe27a7ae93483cfc05de6725fea"},"cell_type":"markdown","source":"This prediction is much better than the ones obtained from other regression algorithms"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}