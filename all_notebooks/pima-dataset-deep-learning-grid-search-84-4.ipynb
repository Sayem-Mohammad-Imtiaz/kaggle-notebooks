{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Deep Learning Grid Search\n\nIn this project, we will learn how to use the scikit-learn grid search capability.\n\nWe are going to learn the following topics:\n\n* How to use Keras models in scikit-learn.\n* How to use grid search in scikit-learn.\n* How to tune batch size and training epochs.\n* How to tune learning rate\n* How to tune network weight initialization.\n* How to tune activation functions.\n* How to tune dropout regularization.\n* How to tune the number of neurons in the hidden layer.","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import sys\nimport pandas\nimport numpy\nimport sklearn\nimport keras\n\nprint('Python: {}'.format(sys.version))\nprint('Pandas: {}'.format(pandas.__version__))\nprint('Numpy: {}'.format(numpy.__version__))\nprint('Sklearn: {}'.format(sklearn.__version__))\nprint('Keras: {}'.format(keras.__version__))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68a60e83a77164333e1643cafed703843a344e15"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nnames = ['n_pregnant', 'glucose_concentration', 'blood_pressuer (mm Hg)', 'skin_thickness (mm)', 'serum_insulin (mu U/ml)',\n        'BMI', 'pedigree_function', 'age', 'class']\n#df = pd.read_csv('../input/diabetes.csv', names = names)\ndf = pd.read_csv('../input/diabetes.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc9ad941f9ab6e6299ff254f36c074f8fd96bd8f"},"cell_type":"code","source":"# Describe the dataset\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8df79be80598e9d8b97956f1dd66cb416867a182"},"cell_type":"code","source":"df[df['Glucose'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess the data, mark zero values as NaN and drop\ncolumns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n\nfor col in columns:\n    df[col].replace(0, np.NaN, inplace=True)\n    \ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows with missing values\ndf.dropna(inplace=True)\n\n# summarize the number of rows and columns in df\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert dataframe to numpy array\ndataset = df.values\nprint(dataset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into input (X) and an output (Y)\nX = dataset[:,0:8]\nY = dataset[:, 8].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(Y.shape)\nprint(Y[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the data using sklearn StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scaler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform and display the training data\nX_standardized = scaler.transform(X)\n\ndata = pd.DataFrame(X_standardized)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary sklearn and keras packages\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\n\n# Do a grid search for the optimal batch size and number of epochs\n# import necessary packages\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(8, input_dim = 16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = 0.01)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, verbose = 0)\n\n# define the grid search parameters\nbatch_size = [16, 32, 64,128]\nepochs = [2, 5, 10]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_batch_size = 64\nbest_epochs = 10 # 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Do a grid search for learning rate and dropout rate\n# import necessary packages\nfrom keras.layers import Dropout\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(learn_rate, dropout_rate):\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(16, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(8, input_dim = 16, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = best_epochs, batch_size = best_batch_size, verbose = 0)\n\n# define the grid search parameters\nlearn_rate = [0.001, 0.01, 0.1]\ndropout_rate = [0.0, 0.2, 0.4,0.6]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(learn_rate=learn_rate, dropout_rate=dropout_rate)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_dropout_rate = 0.0\nbest_learn_rate = 0.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Do a grid search to optimize kernel initialization and activation functions\n# import necessary packages\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(activation, init):\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer= init, activation= activation))\n    model.add(Dense(16, input_dim = 8, kernel_initializer= init, activation= activation))\n    model.add(Dense(8, input_dim = 16, kernel_initializer= init, activation= activation))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = best_learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = best_epochs, batch_size = best_batch_size, verbose = 0)\n\n# define the grid search parameters\nactivation = ['softmax', 'relu', 'tanh', 'linear']\ninit = ['uniform', 'normal', 'zero']\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(activation = activation, init = init)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_activation = 'relu'\nbest_init = 'normal'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Do a grid search to find the optimal number of neurons in each hidden layer\n# import necessary packages\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(neuron1, neuron2, neuron3):\n    # create model\n    model = Sequential()\n    model.add(Dense(neuron1, input_dim = 8, kernel_initializer= best_init, activation= best_activation))\n    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= best_init, activation= best_activation))\n    model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= best_init, activation= best_activation))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = best_learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = best_epochs, batch_size = best_batch_size, verbose = 0)\n\n# define the grid search parameters\nneuron1 = [8, 16, 32]\nneuron2 = [16, 32, 64]\nneuron3 = [8, 16, 32]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(neuron1 = neuron1, neuron2 = neuron2, neuron3 = neuron3)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), refit = True, verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_neuron1 = 16\nbest_neuron2 = 16\nbest_neuron3 = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#best model\nmodel = Sequential()\nmodel.add(Dense(best_neuron1, input_dim = 8, kernel_initializer= best_init, activation= best_activation))\nmodel.add(Dense(best_neuron2, input_dim = best_neuron1, kernel_initializer= best_init, activation= best_activation))\nmodel.add(Dense(best_neuron3, input_dim = best_neuron2, kernel_initializer= best_init, activation= best_activation))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nadam = Adam(lr = best_learn_rate)\nmodel.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\nckpt_model = 'pima-weights_best_t.hdf5'\ncheckpoint = ModelCheckpoint(ckpt_model, \n                            monitor='val_acc', \n                            verbose=1,\n                            save_best_only=True,\n                            mode='max')\ncallbacks_list = [checkpoint]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_test, y_test),\n                    nb_epoch=best_epochs,\n                    batch_size=best_batch_size,\n                    callbacks=callbacks_list,\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"pima-weights_best_t.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.3f%%\" % (model.metrics_names[1], scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n# Predict the values from the validation dataset\ny_pred = model.predict(X_test)\ny_final = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_test, y_final) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_test, y_final, target_names=['0','1'])\n\nprint(report)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}