{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Import libraries\nimport math\nimport random\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report \nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom imblearn.over_sampling import SMOTE\nimport scikitplot as skplt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Got 90ish AUC score? Check your test set!\nI want to discuss and illustrate a common problem some of the beginners face - target leak (I've been there myself as well). It appears that in this particular challenge it happens over and over again. So I think it'd be helpful to point out the most obvious mistake causing the leakage.<br>\nShort version - you must have a clean separate test set put aside before you do upsampling/oversampling. Otherwise you soil your test set with synthetic data that your model just has learnt to predict very well. Hence the incredibly amazing score. <br>\nI provide the example below.<br>\n<a style = 'color : darkgray'>And just to be clear - I'm not doing any EDA here...</a>"},{"metadata":{},"cell_type":"markdown","source":"Lets have a look at our data:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# data loading and hypermarameters setup\nSEED = 1970\nrandom.seed(SEED)\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npath = '../input/health-insurance-cross-sell-prediction/'\ndf_train = pd.read_csv(path + \"train.csv\").set_index('id')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = df_train['Response'].value_counts()\nprint(targets)\nprint('Percentage of Positive class - %.0f'%(targets[1]/(targets.sum())*100), '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The major issue here is that we only have about 12% of the people who bought the insurance, 2 classes are really imbalanced. Therefore any model would have a tough time making confident prediction.<br>\nIn general we'd like to have evenly represented classes. Otherwise we'd need to upsample the minority class, or downsample the majority. There are different approches and techniques for achieving it. One of them is SMOTE, and I choose it here since it is really easy to use. But you might want to [check it later](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE)."},{"metadata":{},"cell_type":"markdown","source":"I'm encoding categorical features as labels (for simplicity I leave one-hot encoding out as it is not crucial for the problem at hands):"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"col_list = df_train.columns.to_list()[1:]\ndf_train_corr = df_train.copy()\n\ncategorical_features = ['Gender', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage','Policy_Sales_Channel']\ntext_features = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n\n# encode text categorical features\nle = preprocessing.LabelEncoder()\nfor f in text_features :\n    df_train_corr[f] = le.fit_transform(df_train_corr[f])\nfor f in categorical_features :\n    df_train_corr[f] = df_train_corr[f].astype('int32')\n\ndf_train_corr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets put aside 10% of the samples as our final test set (and FYI this is that most crucial moment many are missing). Any <a style = ' color : brown'>Feature Engineering</a> must be done <a style = ' color : red'>before</a> this split by the way."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp, X_test, y_temp, y_test = train_test_split(df_train_corr, df_train_corr['Response'], train_size=0.9, random_state = SEED)\nX_test = X_test.drop(columns = ['Response'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets do upsampling using SMOTE:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ovrsmpl = SMOTE(n_jobs=-1, k_neighbors=5, random_state = SEED)\nX_balanced, y_balanced = ovrsmpl.fit_resample(df_temp.drop(['Response'], axis=1), df_temp['Response'])\n# sanity check\nprint(X_balanced.shape)\nX_balanced.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the results of the upsampling:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Number of duplicates in the original train set :',df_temp.duplicated().sum())\nprint('Original class distribution:')\nprint(y_temp.value_counts())\nprint('-'*40)\nprint('Number of duplicates in the oversampled train set :',X_balanced.duplicated().sum())\nprint('New class distribution:')\nprint(y_balanced.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are quite a few duplicates produced (about 2.4%), and we might check later if they skew our results.<br>\nAnd now lets split our newly **upsampled and balanced** data set into **train/validation sets**."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_balanced, y_balanced, train_size=0.9, random_state = SEED)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_ROC(fpr, tpr, m_name):\n    roc_auc = sklearn.metrics.auc(fpr, tpr)\n    plt.figure(figsize=(6, 6))\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.5)\n    \n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--', alpha=0.5)\n    \n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.grid(True)\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.title('Receiver operating characteristic for %s'%m_name, fontsize=20)\n    plt.legend(loc=\"lower right\", fontsize=16)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are ready to train a basic model, and make predictions on our validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def predict_display(df,target):\n    rfc_preds_proba = rfc.predict_proba(df)\n    rfc_pred = rfc.predict(df)\n    rfc_preds_proba = rfc.predict_proba(df)\n    rfc_score = roc_auc_score(target, rfc_preds_proba[:,1])\n    (fpr, tpr, thresholds) = roc_curve(target, rfc_preds_proba[:,1])\n    plot_ROC(fpr, tpr,'RandomForestClassifier')\n    print('ROC AUC score for RandomForestClassifier model with over-sampling: %.4f'%rfc_score)\n    print('F1 score: %0.4f'%f1_score(target, rfc_pred))\n    print(f\"Accuracy score is {100*accuracy_score(target,rfc_pred).round(2)}\")\n    skplt.metrics.plot_confusion_matrix(target, rfc_pred,figsize=(8,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare to be <a style = 'font-weight: bold; color : red'>amazed</a> :)<br>\nThis Random Forest classifier with default settings will provide us with quite remarkable results with no efforts. And of course we can get it much better with tuning parameters, or using boosted trees, like XGBoost, for example. We can get to almost ***perfect 100%*** :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing prediction on the validation set we got AFTER oversampling\npredict_display(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see we got really decent and **balanced** prediction.<br>\n<br>\nHowever if we check the same model on the **preserved test set** that was not affected by the oversampling algorithm the result is drastically different.<br>\nIt is most apparent when you compare the confusion matrices - the model is not really capable of predicting <a style='color:red'>True Positives</a>, giving almost 50% <a style='color:red'>False Negatives</a> and significant number of <a style='color:red'>False Positives</a>. The score mostly defined by the prediction of <a style='color:red'>True Negatives</a> which is much more easier to do due to the class imbalance in the test set. And the **F1 score is just 40%** in this case.<br>\nHowever I need to point out that the balancing classes by SMOTE has helped a lot because the same model trained on the original imbalanced dataset predicts almost no <a style='color:red'>True Positives</a>, giving **F1 score below 2%** as the result (I checked it when started working with this challenge)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing prediction on the validation set we put aside BEFORE oversampling\npredict_display(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This does not look as good, but it is a good starting point! <BR>\nKnowing that oversampling helps us we can improve further by doing feature engineering, choosing different models with better perfomance, ensembling different models etc."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nSo as you can see it is a very simple mistake that can easily be avoided - just take care of your validation/test set before doing any type of oversampling/upsampling.<br>\nHowever keep in mind that there are more trickier ways to get the leakage, and I'd advise to learn about the issue, maybe starting with this [Kaggle tutorial 'Data Leakage'](https://www.kaggle.com/alexisbcook/data-leakage)"},{"metadata":{},"cell_type":"markdown","source":"<p style = 'font-size : medium; font-weight: bold; color : brown'> Please Upvote this notebook if you found it useful. </p>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}