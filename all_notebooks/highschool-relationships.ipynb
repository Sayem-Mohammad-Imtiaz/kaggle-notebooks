{"cells":[{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"2cd82c57-683c-4284-b92d-2158ea8d3940","collapsed":true,"_uuid":"f721a772383902566c4d3ca12512428c5a8013e0"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b97e1a9a-97c6-4163-9e16-af6c37bef09e","collapsed":true,"_uuid":"e1526d1fc621da17c1c8107f70af39eae56416e2"},"source":"data_mat = pd.read_csv('../input/student-mat.csv')","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"b2cda065-77b0-464b-8f5f-afaeabed9d63","_uuid":"4974a02c99f8393d1907ae0f482ac17eb4c54c92"},"source":"**Characterizing the High School Relationship**"},{"cell_type":"markdown","metadata":{"_cell_guid":"e42078d8-81b2-473a-bf7d-bb3b5a545382","_uuid":"24d24b6c8327ca7a0b8cb52bb0d09784f3d974c2"},"source":"let's see the data columns"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"ee2bfa90-1e3f-4346-a274-9ec3a494a291","collapsed":true,"_uuid":"9e5937ebe6ef427adab81570907978e2231d1d67"},"source":"data_mat.dtypes","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"17412989-b920-4c91-8111-9f8b32a491d2","_uuid":"bf59f806456e54f6314bee819346e950b92edec7"},"source":"note that a large portion of the data is not in the correct format (object columns). I will choose a subset of features to work with - address,studytime,failures,activities,nursery,higher,internet,famrel,freetime,absences,G3 "},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"1e37abf2-5d78-4d1d-b869-5c0189f13b5e","collapsed":true,"_uuid":"70bfcfbd762b4c957b608ba47758357e02df2c95"},"source":"data_mat = data_mat[['address','studytime','failures','activities','nursery','higher','internet','famrel','freetime','absences','G3','romantic']]","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"473adc9a-d2fd-4044-ac0a-b0c3b3acd058","_uuid":"308e86904f1a9fcc152bd03ceaab7ce03ab40591"},"source":"let's transform the object columns into a workable format"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"e176d33e-f047-46ad-a854-dcc6a7d3dbd2","collapsed":true,"_uuid":"105e8dc2542cfbef67e544f248b9f075af92b2cd"},"source":"binary_features = ['address','activities','nursery','higher','internet','romantic']\nfor column in binary_features:\n    print(column,\"-\",data_mat[column].unique())","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"0a0be61c-1400-4577-9143-12d8a9f8e9d3","collapsed":true,"_uuid":"46200f94d0ce04df8755e55b0491bf28aab91dfa"},"source":"for column in binary_features:\n    if (column == 'address'):\n        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x == 'U') else 0)\n    else:\n        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x =='yes') else 0)\n    print(column,\"-\",data_mat[column].unique())","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"aea855f2-0cb5-422d-9e86-5de504724aa9","_uuid":"3b62b28462220feba3f9749d816ffd1580552e00"},"source":"Great, now we can look at the Pearson correlation coefficents to find the most correlated features"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"4eb0490d-66af-4e93-80db-3bb5e15fc011","collapsed":true,"_uuid":"449c118ca72ff1a601df1174ba30d3ddfff1352d"},"source":"plt.figure(figsize=(15,15))\nsns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)\nplt.xticks(rotation=90)\nplt.yticks(rotation = 0)","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"2aab81d9-a63d-4441-a93c-f3d893de5bed","_uuid":"2b51582152c94f0dbf89704172f7746f80d605af"},"source":"It seems like the address (living in an urban or rural environment) is barely correlated to being in a relationship for a highschool student. The same is true for activities (participating in extra-curricular activities) and nursery (attended nursery school). We will drop those columns together with the free time column (numerical variable describing the free time of a student) -"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"4fb0f772-99e2-4de9-88ff-c75a75d6edf4","collapsed":true,"_uuid":"04dc48c18e4615d143f458a4fcaab3da998d45a7"},"source":"data_mat = data_mat[data_mat.columns.drop(['address','activities','nursery','freetime'])]","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"960b84bc-8e15-4d2a-8ab4-0e256abc40c0","_uuid":"59d1d0afaeaed87fd0217d791084aeb105f323a1"},"source":"Let's look at the data, now with 7 features (besides the relationship status)"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"8c4b4de7-1f2d-41a9-8d19-3e317b51d93d","collapsed":true,"_uuid":"66f9b753ecea8d5b627574df17f24a55e6fff45f"},"source":"plt.figure(figsize=(15,15))\nsns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)\nplt.xticks(rotation=90)\nplt.yticks(rotation = 0)","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"d3fab19e-9f8c-4337-b199-3469b796d144","_uuid":"79ccb86b943b7f9f7e408eb52385f7f5254cd48f"},"source":"One can say that if a highschool student is in a relationship, he or she is among other things, less likely to attend his/her lectures, less likely to preform well on his/her exams, less likely to desire a higher education and more likely to have already failed the class."},{"cell_type":"markdown","metadata":{"_cell_guid":"e735a57a-e78a-4632-8879-f56b40e3e422","_uuid":"98d791c4a4668a5f97a9e6313c4999f6104e2de1"},"source":"**Decision Tree Prediction**"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2b0ff82-ae87-4255-b810-3783990f90bf","_uuid":"2818f421e07238bad703d8e6835cd31056efad4c"},"source":"Lets see how well a decision tree might be able to make a prediction whether a high school student is in a relation based on the following 3 kinds of features - 2 binary features: higher and internet, 3 categorical features: absences, failures and studytime and 2 integer valued features: absences and G3"},{"cell_type":"markdown","metadata":{"_cell_guid":"19d53dd5-d905-4c54-8c02-38b6d2998a57","_uuid":"43f858ac6351873f08e76ef1791c17fbe1518281"},"source":"let's start by transform the categorical variables into a dummy indicator form"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"008589cb-9a7e-4e77-9054-3fdb73e49fcf","collapsed":true,"_uuid":"2076dfaef4ca23dac56d036dddccf5c229b1513b"},"source":"absences = pd.get_dummies(data_mat['absences'], drop_first = True)\nfailures = pd.get_dummies(data_mat['failures'],drop_first = True)\nstudytime = pd.get_dummies(data_mat['studytime'],drop_first = True)\n\ndata_mat.drop(['absences','failures','studytime'], axis =1, inplace = True)\ndata_mat = pd.concat([data_mat,absences, failures,studytime],axis = 1)\ndata_mat.head()","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"dfa5dde7-1b66-4756-aa57-ace41b2950d0","collapsed":true,"_uuid":"b6bf7cd2981dbf69a19d175199cce4c7dffc3abd"},"source":"next, we will split the data into labels and features"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"c0704802-ac2f-4f52-aa0c-f9fb90fd0c56","collapsed":true,"_uuid":"c9847a5f715c002528a5fe7f4f3c6eb8fb9a5485"},"source":"data_matf = data_mat.drop('romantic', axis = 1)\ndata_matl = data_mat['romantic']","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"262fd8aa-3eba-40f5-8267-8fe61229803c","_uuid":"48a48b74d6550fd6fbd542e92754c708c2287cc7"},"source":"in order to evaluate the performance of our predictor we will use 10 fold cross validation -  splitting the data into 10 folds, training the decision tree on 9 of them and evaluating its perforance on the  last fold, for each of the folds."},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"e0978139-b803-42ac-b08e-cfef4686136f","collapsed":true,"_uuid":"810106c75f726f21b5e593d050b182bb9030d547"},"source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nkf=KFold(n_splits=10, shuffle=True, random_state=False)\ndtree = DecisionTreeClassifier()\n\noutcomesDt = []\nfor train_id, test_id in kf.split(data_matf,data_matl):\n    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n    dtree.fit(X_train,y_train)\n    predictions = dtree.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    outcomesDt.append(accuracy)\nplt.plot(range(10),outcomesDt)\nplt.show()\naverage_error_Dt = np.mean(outcomesDt)\nprint(\"the average error is equal to \",average_error_Dt)","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"f87778d3-61b3-45ef-8cdf-4c59acd81c6d","_uuid":"4a3b6386205841148fedcd330afc96cd6b40a56d"},"source":"Therefore, we will estimate the probability for error for a decision tree classifier to be {{average_error_Dt}}, regardless of the type of error (false negative and false postive are treated the same)."},{"cell_type":"markdown","metadata":{"_cell_guid":"9c4b9fe8-408d-4d34-b998-892885854eec","_uuid":"4c4e61cdb0269a551de60a9ad0b37d69ed2e31b5"},"source":"**Random Forest Prediction**"},{"cell_type":"markdown","metadata":{"_cell_guid":"e4f92a0a-dddd-4961-98e7-c018169c4253","_uuid":"94ba76106db5e861bb0283d29acf41ee75f2de3f"},"source":"let's see whether a random forest performs better than a decision tree for the prediction we are interested at"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"2e095ca8-c2dc-4700-a144-65109232216d","collapsed":true,"_uuid":"d36208653a9de2384682347ea77bd7673c0abd2d"},"source":"from sklearn.ensemble import RandomForestClassifier\nRf=RandomForestClassifier(n_estimators=10)\noutcomesRf=[]\nfor train_id, test_id in kf.split(data_matf,data_matl):\n    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n    Rf.fit(X_train,y_train)\n    predictions = Rf.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    outcomesRf.append(accuracy)\nplt.plot(range(10),outcomesRf)\nplt.show()\nprint(\"the average error is equal to \",np.mean(outcomesRf))","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"49385222-ad03-48b0-a7d0-647f736c7754","_uuid":"e0e957dfc6fec689841d6e94afabeed219e6f75a"},"source":"which means that an ensemble of 10 trees does not perform better than a single decision tree with respect to the probability of error. Let's see how the performance is dependent on the number of trees and attempt to decide which method makes a better decision with respect to the probability of error -"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"5bb37d42-1762-4e14-bfdb-371196b60eb1","collapsed":true,"_uuid":"86ab6316a2289dc6ff38da482fdd5fc301aebd70"},"source":"ForestTreesPerformance = []\nfor n_trees in range(1,11,1):\n    pRf=RandomForestClassifier(n_estimators=n_trees)\n    outcomesRfs=[]\n    for train_id, test_id in kf.split(data_matf,data_matl):\n        X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n        y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n        Rf.fit(X_train,y_train)\n        predictions = Rf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        outcomesRfs.append(accuracy)\n    ForestTreesPerformance.append(np.mean(outcomesRfs))\nplt.plot(range(1,11,1),ForestTreesPerformance)\nplt.show()\nif (min(ForestTreesPerformance) > average_error_Dt):\n    print(\"A decision tree works better than a random forest with respect to the probability error\")\nelse:\n    print(\"A random forest with\",np.argmin(ForestTreesPerformance)+1,\"trees works better than a decision tree with respect to the probability error\")","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"faaa3fc8-7c86-4407-b13f-2ce21d5e9966","collapsed":true,"_uuid":"da2280a9402cd275d1e771a6903578805ea9f7ab"},"source":"","execution_count":null}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","version":"3.6.3","nbconvert_exporter":"python","mimetype":"text/x-python","file_extension":".py"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}