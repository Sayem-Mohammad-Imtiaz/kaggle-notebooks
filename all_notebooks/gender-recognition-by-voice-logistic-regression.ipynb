{"cells":[{"metadata":{"_uuid":"fafd0c8ba158ab1e1bd97002e62d093ae9746053"},"cell_type":"markdown","source":"** <h1>What is \"Logistic Regression\"?</h1> **\n\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. <br></br>\nIn logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).<br></br>\n\n<h2>Where we use Logistic Regression?</h2>\nTo predict whether an email is spam (1) or not (0)<br></br>\nWhether the tumor is malignant (1) or not (0)<br></br>\nTo predict  whether a voice/face man (1) or woman (0)<br></br>\nLogistic regression is generally used where the dependent variable is Binary or Dichotomous. That means the dependent variable can take only two possible values such as “Yes or No”, “Default or No Default”, “Living or Dead”, \"Man or Woman\", “Responder or Non Responder”, “Yes or No” etc. Independent factors or variables can be categorical or numerical variables."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/voice.csv\") #read data\ndata.head() #first five datas","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6a1291773b1ee95675b9d823688199d232f85eb"},"cell_type":"markdown","source":"As you see, the \"label\" column has binary data. It has \"male\" and \"female\". So, we can use logistic regression in here.\nBut, predicted value can't be an object. It must be integer or category type. We must convert label column' s type to integer. "},{"metadata":{"trusted":true,"_uuid":"bfc35600dab8b5d4940b0c6179650234410598c7"},"cell_type":"code","source":"print(data.label.unique())\ndata.label = [1 if i =='female' else 0 for i in data.label ]\ny = data.label.values.reshape(-1,1)\nx_data = data.drop([\"label\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a082ea770c7dd3512577b05eb53cfd44e29f4b23"},"cell_type":"markdown","source":"<h2>Normalization</h2>\nWe must normalize features into same scalar. Because each feature can have different type of measures. Formula of normalization: ![](http://nichea.sourceforge.net/images/figures/functions/standardization/formula1.png)\n"},{"metadata":{"trusted":true,"_uuid":"82788e5fdc9c41fbcb6e082132277e24b2cca58b"},"cell_type":"code","source":"x = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data)).values\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69fff7bf320643c67b7dbeb6a1159e9617e5cf54"},"cell_type":"code","source":"x.head()\n# Do you see the difference?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7ca4f7210738997b5f0d246d25f527f4f3021dd"},"cell_type":"markdown","source":"<h2> Train Test Split </h2>\nWe split our data for test and train our regression. We use sklearn library for that. \nI use %20 for test my regression and %80 for train my regression."},{"metadata":{"trusted":true,"_uuid":"c0c519b5f66e0f83fe7b182762e8cc69b85b60fd","scrolled":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42)\n# y => label\n#x => feature\n#test_size => %20\n#random state => something like id\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train shape: \",x_train.shape )\nprint(\"y_train shape: \",y_train.shape )\nprint(\"x_test shape: \",x_test.shape )\nprint(\"y_test shape: \",y_test.shape )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf8b11534c1e270bf935fd4ba1906fdfa8495052"},"cell_type":"markdown","source":"<h2> Application of Logistic Regression </h2>\n1. Initialize parameters (weight and bias)\n2. Forward propagation\n3. Update\n4. Predict <br>\n<h3>Initializing Parameters </h3>\n Parameters are weights and bias. <br> </br>\n Bias: intercept"},{"metadata":{"trusted":true,"_uuid":"51646c660c3a2988260b94890a3af9d2bd3c79a5"},"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    #make dimensionx1 matrix full of 0.01. We use 0.01 but if we write 0, 0*x = 0 and that will cause our\n    #code can't learn\n    b = 0.0 # we want float\n    return w,b\n    \n    #b is initial bias","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10ac88956f2791ab3eea55ff6235e211c6101a65"},"cell_type":"markdown","source":"<h3> Forward Propagation </h3>\nz = (w.T)x + b => x is array, w is weights and  b is bias <br></br>\nWe put z into \"sigmoid function\" that returns y_head. What is sigmoid function?<br></br>\n<b>Sigmoid function</b> makes z between zero and one so that is probability.It gives probabilistic result. It is derivative so we can use it in gradient descent algorithm. You can see formula and graph below.<br></br>\n![](https://cdn-images-1.medium.com/max/1600/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\nThen we calculate \"loss (error) function\"\n"},{"metadata":{"trusted":true,"_uuid":"5fedbbed8ebc1e131d72f54e389588b1447c8105"},"cell_type":"code","source":"def sigmoid(z):    \n    y_head = 1/(1 + np.exp(-z)) #formula of sigmoid \n    return y_head\n\n# If z = 0, function must give us 0.5 mathematically\nsigmoid(0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75054b1ca46c6aaf2b7b2eba4d827051f7ac1c8a"},"cell_type":"markdown","source":"Now we should calculate loss function. Formula of loss function is: <br></br> ![](https://image.ibb.co/eC0JCK/duzeltme.jpg)\n\nIt says that, if you are making wrong prediction, loss becomes big. After that, the cost function is summation of loss function. Each weight creates loss function. Cost function is summation of loss functions that is created by each input. We should find min cost.\n"},{"metadata":{"trusted":true,"_uuid":"0ab9689a0e2808e12c25157cd985076aa7ab2d96"},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    \n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]\n    \n    #backward prop.\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] # derivative weight\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1] #derivative bias\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias} #parameters\n    \n    return cost,gradients\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00d8d7dc2bb0fe1035d97c148395d8d246dc1f00"},"cell_type":"markdown","source":"<h3>Updating Parameters</h3>"},{"metadata":{"trusted":true,"_uuid":"9a47dbab27e7484c140933d634ff95774b839c18"},"cell_type":"code","source":"#number of it. = how many time backward-forward\ndef update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #update parameters is number-of-iter. times\n    for i in range(number_of_iteration):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        #we need cost for  know how many time make iteration\n        cost_list.append(cost)\n        \n        #update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b= b - learning_rate * gradients[\"derivative_bias\"]\n        #stop when derivatives approach to zero\n\n        if i % 10 == 0:\n            cost_list2.append(cost) \n            index.append(i)\n            print(\"cost after iteration %i: %f\" %(i,cost))\n\n    parameters = {\"weight\": w, \"bias\": b} #important part\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation = 'vertical')\n    plt.xlabel(\"number of cost iteration\")\n    plt.ylabel(\"cost\")\n    plt.show()\n    \n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54751512d418051d86cfdf86ea6820e78479f51a"},"cell_type":"markdown","source":"<h3>Prediction Method</h3>"},{"metadata":{"trusted":true,"_uuid":"95685b7d5d80c65fa9d77fea6fe83a059b6020db"},"cell_type":"code","source":"def predict(weight, bias, x_test):\n    #x_test input for forward propagation    \n    z = sigmoid(np.dot(weight.T,x_test)+bias)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z > 0.5, prediction = 1 (y_head = 1)\n    # if z < 0.5, prediction = 0 (y_head = 0)\n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c599e81e6dcb48676d6b734c2acca0bb8b1e789d"},"cell_type":"markdown","source":"<h3> Logistic Regression Method ~ Last </h3> "},{"metadata":{"trusted":true,"_uuid":"afc5277308805fa3960346077ef28df6d47fa5dc"},"cell_type":"code","source":"def logistic_reg(x_train, y_train, x_test, y_test, learning_rate, num_iteration):\n    dimension = x_train.shape[0] #need for initialize weight, that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,num_iteration)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"], x_test)\n    \n    #print errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_reg(x_train, y_train, x_test, y_test,learning_rate = 1, num_iteration = 500)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63a2cd8ce4e500a7e968b953c41380837a2d6a8c"},"cell_type":"markdown","source":"<h2>Simple Logistic Regression Method</h2>\nOf course we don' t write that long code for all time we need. Python has a simple method for that. Love Python <33"},{"metadata":{"trusted":true,"_uuid":"d1e7dcda38129dda89eab31e88b057a1a9de8e5a"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}