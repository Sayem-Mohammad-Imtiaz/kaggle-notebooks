{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text Processing"},{"metadata":{},"cell_type":"markdown","source":"1. Preprocessing:-\n\nPreprocessing is one of the major steps when we are dealing with any kind of text models. During this stage we have to look at the distribution of our data, what techniques are needed and how deep we should clean.\n\n\na. Lowercase -\n\nDuring the text processing each sentence is split to words and each word is considered as a token after preprocessing.\nProgramming languages consider textual data as sensitive, which means that The is different from the. we humans know that those both belong to same token but due to the character encoding those are considered as different tokens. Converting to lowercase is a very mandatory preprocessing step. As we have all our data in list, numpy has a method which can convert the list of lists to lowercase at once.\n\n\nb. Stop words :-\n\nStop words are the most commonly occurring words which don’t give any additional value to the document vector. in-fact removing these will increase computation and space efficiency. nltk library has a method to download the stopwords, so instead of explicitly mentioning all the stopwords ourselves we can just use the nltk library and iterate over all the words and remove the stop words. There are many efficient ways to do this, but ill just give a simple method.\nwe are going to iterate over all the stop words and not append to the list if it’s a stop word\nnew_text = \"\"\nfor word in words:\n    if word not in stop_words:\n        new_text = new_text + \" \" + word\n\n\nc. Punctuation:-\n\nPunctuation are the unnecessary symbols that are in our corpus documents, we should be little careful with what we are doing with this. There might be few problems such as U.S — us “United Stated” being converted to “us” after the preprocessing. hyphen and should usually be dealt little carefully. but for this problem statement we are just going to remove these\nsymbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n    \nd. Apostrophe:-\n\nNote that there is no ‘ apostrophe in the punctuation symbols. Because when we remove punctuation first it will convert don’t to dont, and it is a stop word which wont be removed. so what we are doing is we are first removing the stop words, and then symbols and then finally stopwords because few words might still have a apostrophe which are not stop words.\n\n\ne. Single Characters:-\n\nSingle characters are not much useful in knowing the importance of the document and few final single characters might be irrelevant symbols, so it is always good be remove the single characters.\nnew_text = \"\"\nfor w in words:\n    if len(w) > 1:\n       new_text = new_text + \" \" + w\nWe just need to iterate to all the words and not append the word if the length is not greater than 1.\n\nf. Lemmatisation:-\n\nLemmatisation is a way to reduce the word to root synonym of a word. Unlike Stemming, Lemmatisation makes sure that the reduced word is again a dictionary word (word present in the same language). WordNetLemmatizer can be used to lemmatize any word.\n\n#####Stemming vs Lemmatization\nstemming — need not be a dictionary word, removes prefix and affix based on few rules\nlemmatization — will be a dictionary word. reduces to a root synonym.\n\ng. Stemming:-\n\nThis is the final and most important part of the preprocessing. stemming converts words to its stem.\nFor example playing and played are the same type of words which basically indicate an action play. Stemmer does exactly this, it reduces the word to its stem. we are going to use a library called porter-stemmer which is a rule based stemmer. Porter-Stemmer identifies and removes the suffix or affix of a word. The words given by the stemmer need note be meaningful few times, but it will be identified as the same for the model.\n\n####Note: A better efficient way to proceed is to first lemmatise and then stem, but stemming alone is also fine for few problems statements, in this problem statement we are not going to lemmatise.\n\n\ni. Converting Numbers:-\n\nWhen user gives a query such as 100 dollars or hundred dollars. For the user both those search terms are same. but out IR model treats them separately, as we are storing 100, dollar, hundred as different tokens. so to make our IR mode little better we need to convert 100 to hundred. To achieve this we are going to use a library called num2word."},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport nltk\nimport nltk.corpus\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the corpora and displaying it\nf = open('/kaggle/input/sms-spam-collection-dataset/spam.csv', mode='r', encoding='latin-1')\nmessage = []\nfor line in f.readlines():\n    message.append(line.rstrip('\\n'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of messages are {}'.format(len(message)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the dataset\ndata= pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\n\ndata = data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\ndata= data.rename(columns = {'v1':'label','v2':'message'})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot for label Data\nprint(data['label'].value_counts())\nsns.set_style(style='darkgrid')\nsns.countplot(data['label'], hue=data['label'])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gropuby for label\ndata.groupby(by='label').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for missing values\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Engineering\ndata['length'] = data['message'].apply(len)\n\nspam_data= data.loc[data['label']=='spam']\nham_data= data.loc[data['label']=='ham']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the length of spam and ham messages\nplt.figure(figsize=(10,4))\nplt.subplot(121)\nsns.distplot(ham_data['length'], label='ham_length')\nplt.legend()\nplt.subplot(122)\nsns.distplot(spam_data['length'], label= 'spam_lenth', color='orange')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_data['length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_data['length'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Preprocessing"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#LowerCase\nmsg = data['message'][0]\nmsg = msg.lower()\n\n#Stopwords\nfrom nltk.tokenize import word_tokenize\nmsg = word_tokenize(msg, preserve_line=False)\n\n#Stop word removal\nfrom nltk.corpus import stopwords\nmsg = [words for words in msg if words not in stopwords.words('english')]\n\n#punctuations removal\nimport string\nmsg = \" \".join(msg)\nnopunc = [c for c in msg if c not in string.punctuation]\nnopunc = ''.join(nopunc)\n\n# apostrope removal\n\n#single character removal\nmsg = [words for words in nopunc.split() if len(words) >1]\n\n#Lemmatization\nfrom nltk import stem\nword_lem = stem.WordNetLemmatizer()\nmsg = [word_lem.lemmatize(words) for words in msg]\n\n#Stemming\nfrom nltk.stem import PorterStemmer\npst = PorterStemmer()\nmsg = [pst.stem(word) for word in msg]\nmsg = ' '.join(msg)\nmsg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"msg = data['message'][0]\n#loweing the sentence\nmsg = msg.lower()\n#removal of punctuations\nmsg = re.sub(r'[^A-Za-z]+', ' ', msg)\nmsg = msg.split()\nmsg = [word for word in msg if word not in stopwords.words('english')]\nmsg = [word for word in msg if len(word)>1]\nwlem = WordNetLemmatizer()\nmsg = [wlem.lemmatize(word) for word in msg] \npst = PorterStemmer()\nmsg = [pst.stem(word) for word in msg] \nmsg = ' '.join(msg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk import stem\nfrom nltk.stem import PorterStemmer\n\ndef text_process(msg):\n    #LowerCase\n    msg = msg.lower()\n\n    #Stopwords\n    msg = word_tokenize(msg, preserve_line=False)\n\n    #Stop word removal\n    msg = [words for words in msg if words not in stopwords.words('english')]\n\n    #punctuations removal\n    msg = \" \".join(msg)\n    nopunc = [c for c in msg if c not in string.punctuation]\n    nopunc = ''.join(nopunc)\n\n    # apostrope removal\n\n    #single character removal\n    msg = [words for words in nopunc.split() if len(words) >1]\n\n    #Lemmatization\n    word_lem = stem.WordNetLemmatizer()\n    msg = [word_lem.lemmatize(words) for words in msg]\n\n    #Stemming\n    pst = PorterStemmer()\n    msg = [pst.stem(word) for word in msg]\n    msg = ' '.join(msg)\n    \n    return msg\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply text processing to each message and calculating length again\ndata['message']= data['message'].apply(text_process)\ndata['new_length'] = data['message'].apply(len)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After text processing lets calculate the new_length \nspam_data_new= data.loc[data['label']=='spam']\nham_data_new= data.loc[data['label']=='ham']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot\nplt.figure(figsize=(10,4))\nplt.subplot(121)\nsns.distplot(ham_data_new['new_length'], label='ham_length')\nplt.legend()\nplt.subplot(122)\nsns.distplot(spam_data_new['new_length'], label= 'spam_lenth', color='orange')\nplt.legend()\nplt.show()\n\ndata.groupby(by = 'label')['new_length'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bag Of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Vectorization\nfrom sklearn.feature_extraction.text import CountVectorizer\n#Binary bag of words\ncount_vector = CountVectorizer(binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binary_bow= count_vector.fit(data['message'])\nbinary_bow.get_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print total number of vocab words\nprint(len(binary_bow.vocabulary_))\n[v for v in binary_bow.vocabulary_.items()][0:5]\n\n### we have 7776 words in our vocabulary ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### lets take the 4th message from our message dataframe \nmessage4 = data['message'][3]\nprint(message4)\nprint('\\n')\n\n### use the bow_transformer and call transform function on the test message \"message4\"\nbow4 = binary_bow.transform([message4])\nprint(bow4)\nprint('\\n')\n### .transform outputs the sparse matrix of indexes along with the number of times each word occurs in that index.\n\nprint(type(bow4))\nprint('\\n')\nprint(bow4.ndim)\nprint(bow4.shape)\nprint('\\n')\n\n#Checking the colums are correct for words or not\nprint(binary_bow.get_feature_names()[1103])\nprint(binary_bow.get_feature_names()[2602])\nprint(binary_bow.get_feature_names()[2618])\nprint(binary_bow.get_feature_names()[3590])\nprint(binary_bow.get_feature_names()[5954])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can use **.transform** on our Bag-of-Words (bow) transformed object and transform the entire DataFrame of messages.\n\nLet's go ahead and check out how the bag-of-words counts for the entire SMS corpus is a large, sparse matrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming\nbag_of_words= binary_bow.transform(data['message'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### check the shape of the sparse matrix using .shape\nprint('Shape of Sparse Matrix: ', bag_of_words.shape)\n\n### check the amount of non zero occurrences using .nnz\nprint('Amount of Non-Zero occurences: ', bag_of_words.nnz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparsity = (100.0 * bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1]))\n\nprint('sparsity: {}'.format((sparsity)))\n## sparsity counts the number of non zero messages vs the total number of messages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect = TfidfVectorizer()\ntfidf_vect= tfidf_vect.fit(data['message'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print total number of vocab words\nprint(len(tfidf_vect.vocabulary_))\n[v for v in tfidf_vect.vocabulary_.items()][0:5]\n\n### we have 7776 words in our vocabulary ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### lets take the 4th message from our message dataframe \nmessage4 = data['message'][3]\nprint(message4)\ntfidf4 = tfidf_vect.transform([data['message'][3]])\nprint(tfidf4)\nprint(type(tfidf4))\nprint(tfidf4.ndim)\nprint(tfidf4.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming\ntfidf= tfidf_vect.transform(data['message'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### check the shape of the sparse matrix using .shape\nprint('Shape of Sparse Matrix: ', tfidf.shape)\n\n### check the amount of non zero occurrences using .nnz\nprint('Amount of Non-Zero occurences: ', tfidf.nnz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparsity = (100.0 * tfidf.nnz / (tfidf.shape[0] * tfidf.shape[1]))\n\nprint('sparsity: {}'.format((sparsity)))\n## sparsity counts the number of non zero messages vs the total number of messages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive bayes for Bag of word data\n\n#Train test split for bag_of_words data\nX_train, X_test, Y_train, Y_test = train_test_split(bag_of_words, data['label'], test_size = 0.25, random_state=1)\n\n#Building naive bayes model for BOW data\nfrom sklearn.naive_bayes import MultinomialNB\nnaive_bayes_bow= MultinomialNB()\nnaive_bayes_bow.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive bayes for Tfidf data data\n\n#Train test split for tfidf data\nx_train, x_test, y_train, y_test = train_test_split(tfidf, data['label'], test_size = 0.25, random_state=1)\n\n#Building naive bayes model for tfidf data\nnaive_bayes_tfidf= MultinomialNB()\nnaive_bayes_tfidf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metrics for Bag of words model\nprint('The shape of X_train is {}'.format(X_train.shape))\nprint('The shape of X_test is {}'.format(X_test.shape))\nprint('\\n')\nprint('The accuracy for Binary BOW model is {}'.format(accuracy_score(Y_test, naive_bayes_bow.predict(X_test))))\nprint('\\n')\nprint('The confusion matrix for Binary BOW model is :')\nprint(confusion_matrix(Y_test, naive_bayes_bow.predict(X_test)))\nprint('\\n')\nprint('The classification report for Binary BOW model is :')\nprint(classification_report(Y_test, naive_bayes_bow.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metrics for TFIDF Naive Bayes model\nprint('The shape of x_train is {}'.format(x_train.shape))\nprint('The shape of x_test is {}'.format(x_test.shape))\nprint('\\n')\nprint('The accuracy for TFIDF Naive Bayes model is {}'.format(accuracy_score(y_test, naive_bayes_tfidf.predict(x_test))))\nprint('\\n')\nprint('The confusion matrix for TFIDF Naive Bayes model is :')\nprint(confusion_matrix(y_test, naive_bayes_tfidf.predict(x_test)))\nprint('\\n')\nprint('The classification report for TFIDF Naive Bayes model is :')\nprint(classification_report(y_test, naive_bayes_tfidf.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest model for TFIDF data\nfrom sklearn.ensemble import RandomForestClassifier\nrf_tfidf = RandomForestClassifier(n_estimators=200, criterion='entropy', random_state=1)\nrf_tfidf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metrics for TFIDF Random Forest model\nprint('The shape of x_train is {}'.format(x_train.shape))\nprint('The shape of x_test is {}'.format(x_test.shape))\nprint('\\n')\nprint('The accuracy for TFIDF Random Forest model is {}'.format(accuracy_score(y_test, rf_tfidf.predict(x_test))))\nprint('\\n')\nprint('The confusion matrix for TFIDF Random Forest model is :')\nprint(confusion_matrix(y_test, rf_tfidf.predict(x_test)))\nprint('\\n')\nprint('The classification report for TFIDF Random Forest model is :')\nprint(classification_report(y_test, rf_tfidf.predict(x_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## here for the test train split we are just grabbing the text messages.\n\n1) the ideal way to do is after train test split, run the bag of wods, count vectorization, transformation and tf-idf process , run multinomial Naive Bayes Process and predict. \n\n2) but python's scikit learn offers a very simple solution called data pipeline for this purpose. this is basically a pipeline\n   of our workflow . Enterprise solutions are offered using pipeline feature\n   \n   "},{"metadata":{},"cell_type":"markdown","source":"## Creating a Data Pipeline"},{"metadata":{},"cell_type":"raw","source":"The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.\nSequentially apply a list of transforms and a final estimator.Intermediate steps of the pipeline must be 'transforms', that is, they must implement fit and transform methods. The final estimator only needs to implement fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msg_train, msg_test, label_train, label_test = train_test_split(data['message'], data['label'], test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### observe we have summarized all the steps we did in the Pipleine which takes a list of every process we did so far\nestimators =[('bow', CountVectorizer(analyzer=text_process)), ('log_reg', LogisticRegression())]\npipeline = Pipeline(estimators)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we can directly pass message text data and the pipeline will do our pre-processing \n##We can treat it as a model/estimator API:\npipeline.fit(msg_train,label_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metrics for Pipeline model\nprint('The shape of msg_train is {}'.format(msg_train.shape))\nprint('The shape of msg_test is {}'.format(msg_test.shape))\nprint('\\n')\nprint('The accuracy for Pipeline Log Reg model is {}'.format(accuracy_score(label_test, pipeline.predict(msg_test))))\nprint('\\n')\nprint('The confusion matrix for TFIDF Random Forest model is :')\nprint(confusion_matrix(label_test, pipeline.predict(msg_test)))\nprint('\\n')\nprint('The classification report for TFIDF Random Forest model is :')\nprint(classification_report(label_test, pipeline.predict(msg_test)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}