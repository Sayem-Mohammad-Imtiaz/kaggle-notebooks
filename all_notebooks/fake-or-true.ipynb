{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"true = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\nfake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image \ntext = str\ntext = \" \".join(review for review in true.text)\nmask = np.array(Image.open(\"/kaggle/input/image-file-for-fake-news/news.png\"))\nwc = WordCloud(background_color=\"white\", max_words=1000, mask=mask,max_font_size=200,contour_color='black')\nwc.generate(text)\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image \ntext = str\ntext = \" \".join(review for review in fake.text)\nmask = np.array(Image.open(\"/kaggle/input/image-file-for-fake-news/news.png\"))\nwc = WordCloud(background_color=\"white\", max_words=1000, mask=mask,max_font_size=200,contour_color='black')\nwc.generate(text)\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true['result'] = [1 for i in range(0,len(true))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake['result'] = [0 for i in range(0,len(fake))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# club both the files\ndata = true.append(fake,ignore_index = True,sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"info() shows that there is no null object. so this cuts down one of the steps of data preprocessing.\nsince columns are object type(strings) feature scaling is also obviously not required.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"lets have a look at our data subject wise","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['subject'].nunique()\n# this shows we have 8 subjects","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['subject'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**lets find out that which subject has maximum  true news**\nthis can not be done for fake news as subject for all news is 'news' there","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"true_politicnews=0\nworld_n=0\ngovt_n =0\nus_n =0\nmiddle_n=0\nfor i,j in zip(data['subject'],data['result']):\n    if i==('politicsNews' or 'politics') and j==1:\n        true_politicnews = true_politicnews +1\n    if i==('worldnews') and j==1:\n        world_n+=1\n    if i==('Government News') and j==1:\n        govt_n+=1\n    if i==('US_News') and j==1:\n        us_n+=1\n    if i==('Middle-east') and j==1:\n        middle_n+=1\nprint(true_politicnews)\nprint(world_n)\nprint(us_n)\nprint(govt_n)\nprint(middle_n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"wordcloud for political_news(true)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"str =\" \"\nfor i,j in zip(data['subject'],data['text']):\n    if i=='politicsNews':\n        str += j\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n# list_comp = [review if data[data['subject']==\"politicsnews\"] else ' ' for review in data['text']]\npolitics = str\nwordcloud = WordCloud( background_color=\"white\", max_words=1000).generate(politics)\n\nplt.figure(figsize=[7,7])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can have a look at the wordcloud for each of these subjects\n# so lets group the data acc to subjects and then make wordclods for fake and true news.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LETS WORK ON TEXT NOW**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CLEANING THE TEXT COLUMN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i will concat titles and text columns and that column would be pre processed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = pd.concat(data['title'],data['text'])\ndata['combines'] = data['title']+\" \"+data['text']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[['combines','result']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LOWERCASE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['combines'] = data['combines'].apply(lambda word:word.lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PUNCTUATIONS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nprint(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def punctuation_removal(str1):\n    list1 = [x for x in str1 if x not in string.punctuation]\n    str2 = ''.join(list1)\n    return str2\ndata['combines'] = data['combines'].apply(lambda word:punctuation_removal(word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STOPWORDS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\nprint(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['combines'].apply(lambda x: [word for word in x if word not in stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seperate each word by white space\ndata['combines'] = data['combines'].apply(lambda word:word.split(','))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['combines'].head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nltk.tokenize import word_tokenize\n# def to_words(text):\n#     tokens = word_tokenize(text)\n#     return tokens\n# data['combines'] = data['combines'].apply(lambda s:to_words(s))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert(lst):       \n    return ' , '.join(lst)\n\ndata['combines'] = data['combines'].apply(lambda s:convert(s))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **STEMMING**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # split into words\n# from nltk.tokenize import word_tokenize\n# def splitter(text):\n#     tokens = word_tokenize(text)\n#     return tokens\n# data['combines'] = data['combines'].apply(lambda x:splitter(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def convert(lst):       \n#     return ' , '.join(lst)\n\n# data['combines'] = data['combines'].apply(lambda s:convert(s))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nltk.stem.porter import PorterStemmer\n# porter = PorterStemmer()\n# def stem(tokens):\n#     stemmed = [porter.stem(word) for word in tokens]\n#     print(stemmed[:100])\n# data['combines'] = data['combines'].apply(lambda x:stem(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['combines'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncombine = CountVectorizer()\ncombine = combine.fit(data['combines'])\n\ncombined_vector = combine.transform(data['combines'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(combined_vector)\nnews_tfidf = tfidf_transformer.transform(combined_vector)\nprint(news_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['result']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain,xtest,ytrain,ytest = train_test_split(news_tfidf,y,test_size =0.25,random_state=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\nsvm = SGDClassifier().fit(xtrain, ytrain)\npredict_svm = svm.predict(xtest)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint (classification_report(ytest, predict_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfinal = confusion_matrix(predict_svm, ytest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Accuracy Score :',accuracy_score(ytest, predict_svm) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}