{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Helper function, used these for debugging purposes\n# detector2 build only succeeds if CUDA version is correct\n\n#!nvidia-smi\n#!nvcc --version\n\n#import torch\n#torch.__version__\n#import torchvision\n#torchvision.__version__\n\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html\n!pip install fastai","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-22T04:14:44.528328Z","iopub.execute_input":"2021-06-22T04:14:44.528708Z","iopub.status.idle":"2021-06-22T04:15:15.646927Z","shell.execute_reply.started":"2021-06-22T04:14:44.52863Z","shell.execute_reply":"2021-06-22T04:15:15.645963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Underneed you'll find the extra libraries we'll use in this notebook. More libraries will be added througout the notebook when needed.","metadata":{}},{"cell_type":"code","source":"# Base setup:\n# detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-22T04:15:15.650589Z","iopub.execute_input":"2021-06-22T04:15:15.650854Z","iopub.status.idle":"2021-06-22T04:15:17.536191Z","shell.execute_reply.started":"2021-06-22T04:15:15.650827Z","shell.execute_reply":"2021-06-22T04:15:17.535469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_path = \"/kaggle/input/dsta-brainhack-2021/c1_release/c1_release\"\ntrain_annotation = os.path.join(training_path, \"train.json\")\nval_annotation = os.path.join(training_path, \"val.json\")\nimage_path = os.path.join(training_path,\"images\")\n\n\n\nfrom detectron2.structures import BoxMode\n# if your dataset is in COCO format, this cell can be replaced by the following three lines:\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"train_data\", {}, train_annotation, image_path)\nregister_coco_instances(\"val_data\", {}, val_annotation, image_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:25:22.59336Z","iopub.execute_input":"2021-06-22T04:25:22.593727Z","iopub.status.idle":"2021-06-22T04:25:22.600372Z","shell.execute_reply.started":"2021-06-22T04:25:22.593696Z","shell.execute_reply":"2021-06-22T04:25:22.59924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=traincustom> </a>\n## 4.3. Training with a custom dataset\nLet's first check our training data! Ofcourse we'll use the **Visualizer** class again.","metadata":{}},{"cell_type":"code","source":"\n\n#visualize training data\nmy_dataset_train_metadata = MetadataCatalog.get(\"train_data\")\ndataset_dicts = DatasetCatalog.get(\"train_data\")\n\nmy_dataset_val_metadata = MetadataCatalog.get(\"val_data\")\nval_dicts = DatasetCatalog.get(\"val_data\")\n\nimport random\nfrom detectron2.utils.visualizer import Visualizer\nimport cv2\nimport matplotlib.pyplot as plt\n\nfor d in random.sample(dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n    vis = visualizer.draw_dataset_dict(d)\n    plt.imshow(vis.get_image()[:, :, ::-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:25:28.892006Z","iopub.execute_input":"2021-06-22T04:25:28.892398Z","iopub.status.idle":"2021-06-22T04:25:29.524088Z","shell.execute_reply.started":"2021-06-22T04:25:28.892367Z","shell.execute_reply":"2021-06-22T04:25:29.523156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4Training","metadata":{}},{"cell_type":"code","source":"# # DATA AUG\n\n# from detectron2.data import transforms as T\n# # Define a sequence of augmentations:\n# augs = T.AugmentationList([\n#     T.RandomBrightness(0.9, 1.1),\n#     T.RandomFlip(prob=0.5),\n#     T.RandomCrop(\"absolute\", (640, 640))\n# ])  # type: T.Augmentation\n\n# # Define the augmentation input (\"image\" required, others optional):\n# input = T.AugInput(image, boxes=boxes, sem_seg=sem_seg)\n\n# # Apply the augmentation:\n# transform = augs(input)  # type: T.Transform\n# image_transformed = input.image  # new image\n# sem_seg_transformed = input.sem_seg  # new semantic segmentation\n\n# # For any extra data that needs to be augmented together, use transform, e.g.:\n# image2_transformed = transform.apply_image(image2)\n# polygons_transformed = transform.apply_polygons(polygons)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Run training\n\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator\n\n\nclass CocoTrainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            os.makedirs(\"coco_eval\", exist_ok=True)\n            output_folder = \"coco_eval\"\n        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T06:32:41.451189Z","iopub.execute_input":"2021-06-21T06:32:41.451931Z","iopub.status.idle":"2021-06-21T06:32:41.45795Z","shell.execute_reply.started":"2021-06-21T06:32:41.451893Z","shell.execute_reply":"2021-06-21T06:32:41.45726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### FASTERCNN\n","metadata":{}},{"cell_type":"code","source":"# LOADING PREV FORMAT\n\nfrom detectron2.config.config import CfgNode as CN\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"train_data\",)\ncfg.DATASETS.TEST = (\"val_data\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.001\ncfg.SOLVER.WARMUP_ITERS = 1000\ncfg.SOLVER.MAX_ITER = 40000 #adjust up if val mAP is still rising, adjust down if overfit\ncfg.SOLVER.STEPS = [10000,20000,30000]\ncfg.SOLVER.GAMMA = 0.05\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n# cfg.TEST.EVAL_PERIOD = 1000\n\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = CocoTrainer(cfg) \n# trainer.resume_or_load(resume=False)\ntrainer.resume_or_load(resume=False)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:31:03.981205Z","iopub.execute_input":"2021-06-22T04:31:03.981556Z","iopub.status.idle":"2021-06-22T04:31:04.003915Z","shell.execute_reply.started":"2021-06-22T04:31:03.981521Z","shell.execute_reply":"2021-06-22T04:31:04.003207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RetinaNet","metadata":{}},{"cell_type":"code","source":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_data\",)\n# cfg.DATASETS.TEST = (\"val_data\",)\n# cfg.DATALOADER.NUM_WORKERS = 4\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\")  # Let training initialize from model zoo\n# cfg.SOLVER.IMS_PER_BATCH = 4\n# cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n# cfg.SOLVER.MAX_ITER = 300    # 300 iterations enough for this dataset; Train longer for a practical dataset\n# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, enough for this dataset (default: 512)\n# # cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  # classes for RCNN\n# cfg.MODEL.RETINANET.NUM_CLASSES = 5 # Classes for Retina\n# cfg.TEST.EVAL_PERIOD = 500\n\n\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = DefaultTrainer(cfg) \n# trainer.resume_or_load(resume=False)\n# trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"modelevaluation\" ></a>\n## 4.4. Model evaluation\nLet's check out the performance of our model!\n\nFirst of all let's make some predictions! We're going to use the [**DefaultPredictor**](https://detectron2.readthedocs.io/en/latest/modules/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor) class. Ofcourse we'll use the same cfg that we used during training. We'll change two parameters for our inferencing.","metadata":{}},{"cell_type":"markdown","source":"### Loading model","metadata":{}},{"cell_type":"code","source":"# RELOADING MODEL\n# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n# cfg.MODEL.WEIGHTS = os.path.join(\"/kaggle/input/detectronmodel20k\", \"model_final (1).pth\")  # path to the model we trained\n# cfg.DATASETS.TRAIN = (\"train_data\",)\n# cfg.DATASETS.TEST = (\"val_data\",)\n# cfg.DATALOADER.NUM_WORKERS = 4\n# cfg.SOLVER.IMS_PER_BATCH = 4\n# cfg.SOLVER.BASE_LR = 0.001\n# cfg.SOLVER.WARMUP_ITERS = 1000\n# cfg.SOLVER.MAX_ITER = 10000 #adjust up if val mAP is still rising, adjust down if overfit\n# # cfg.SOLVER.STEPS = [0,20000,40000]\n# cfg.SOLVER.GAMMA = 0.05\n# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5\n\n\n\n\n# model saved weights\n# cfg.MODEL.WEIGHTS = os.path.join(\"/kaggle/input/objectron-retinanetv1/model_final.pth\")\n\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a testing threshold\npredictor = DefaultPredictor(cfg)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:31:54.820804Z","iopub.execute_input":"2021-06-22T04:31:54.821154Z","iopub.status.idle":"2021-06-22T04:31:56.487083Z","shell.execute_reply.started":"2021-06-22T04:31:54.821123Z","shell.execute_reply":"2021-06-22T04:31:56.486323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that by using the [**ColorMode.IMAGE_BW**](https://detectron2.readthedocs.io/en/latest/modules/utils.html?highlight=ColorMode#module-detectron2.utils.visualizer) we we're capable of removing the colors from objects which aren't detected!","metadata":{}},{"cell_type":"code","source":"from detectron2.utils.visualizer import ColorMode\nval_dict = DatasetCatalog.get(\"val_data\")\n\nfor d in random.sample(val_dict, 3):    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im) \n    v = Visualizer(im[:, :, ::-1],\n                   metadata=my_dataset_train_metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. Only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.figure(figsize=(15,7))\n    plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:34:25.720715Z","iopub.execute_input":"2021-06-22T04:34:25.721117Z","iopub.status.idle":"2021-06-22T04:34:26.974569Z","shell.execute_reply.started":"2021-06-22T04:34:25.721077Z","shell.execute_reply":"2021-06-22T04:34:26.973435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we can see that our models performs pretty well! Let's now evaluate our custom model with [Evaluators](https://detectron2.readthedocs.io/en/latest/modules/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor). Two evaluators can be used:\n* [**COCOEvaluator**](https://detectron2.readthedocs.io/en/latest/modules/evaluation.html#detectron2.evaluation.COCOEvaluator) can evaluate AP (Average Precision) for box detection, instance segmentation and keypoint detection.\n* [**SemSegEvaluator**](https://detectron2.readthedocs.io/en/latest/modules/evaluation.html#detectron2.evaluation.SemSegEvaluator) can evaluate semantic segmentation metrics.\n\nAfterwards we'll use the [**build_detection_test_loader**](https://detectron2.readthedocs.io/en/latest/modules/data.html?highlight=build_detection_test_loader#detectron2.data.build_detection_test_loader) which returns a torch DataLoader, that loads the given detection dataset.\n\nAt last we'll use the model, evaluated and dataloader within the [inference_on_dataset](https://detectron2.readthedocs.io/en/latest/modules/evaluation.html#detectron2.evaluation.inference_on_dataset) function. It runs the model on the dataloader and evaluates the metric with the evaluator.","metadata":{}},{"cell_type":"code","source":"from detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\nfrom detectron2.modeling import build_model\n\n\nevaluator = COCOEvaluator(\"val_data\", None, False, output_dir=\"./output/\")\n# evaluator = COCOEvaluator(\"val_data\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n\n# Loading model\nmodel_uploaded = build_model(cfg)\n\nval_loader = build_detection_test_loader(cfg, \"val_data\")\n# print(inference_on_dataset(trainer.model, val_loader, evaluator))\nprint(inference_on_dataset(model_uploaded, val_loader, evaluator))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:33:45.168897Z","iopub.execute_input":"2021-06-22T04:33:45.169284Z","iopub.status.idle":"2021-06-22T04:34:25.71784Z","shell.execute_reply.started":"2021-06-22T04:33:45.169248Z","shell.execute_reply":"2021-06-22T04:34:25.716633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n# cfg.MODEL.WEIGHTS = os.path.join(\"/kaggle/input/objectron-retinanetv1/model_final.pth\")\n\n\ntest_img_path = \"/kaggle/input/dsta-brainhack-2021/c1_test_release/c1_test_release/images\" # extracted testing images path\ncfg.DATASETS.TEST = (\"my_dataset_test\", )\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\ntest_metadata = MetadataCatalog.get(\"my_dataset_test\")\n\nfrom detectron2.utils.visualizer import ColorMode\nimport glob\n\nou_test = []\nfor imageName in glob.glob('/kaggle/input/dsta-brainhack-2021/c1_test_release/c1_test_release/images/*.jpg'):\n  im = cv2.imread(imageName)\n  outputs = predictor(im)\n  ou_test.append(outputs)\n  v = Visualizer(im[:, :, ::-1],\n                metadata=my_dataset_train_metadata, \n                scale=0.8\n                 )\n  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n  plt.imshow(out.get_image()[:, :, ::-1])\n    \n    \n    # im = cv2.imread(\"/kaggle/input/dsta-brainhack-2021/c1_test_release/c1_test_release/images/1.jpg\")\n# outputs = predictor(im)\n# v = Visualizer(im[:, :, ::-1],\n#             metadata=my_dataset_train_metadata, \n#             scale=0.8\n#              )\n# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n# plt.imshow(out.get_image()[:, :, ::-1])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:52:11.749842Z","iopub.execute_input":"2021-06-22T04:52:11.750174Z","iopub.status.idle":"2021-06-22T04:52:12.086119Z","shell.execute_reply.started":"2021-06-22T04:52:11.750143Z","shell.execute_reply":"2021-06-22T04:52:12.085155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate detections on the folder of test images (this will be used for submission)\nfrom PIL import Image, ImageDraw\nfrom torchvision import transforms\nfrom torchvision.ops import batched_nms\nfrom torchvision.transforms import functional as F\nimport torch\n\ndetections = []\n\nfor imageName in glob.glob('/kaggle/input/dsta-brainhack-2021/c1_test_release/c1_test_release/images/*jpg'):\n\n        im = cv2.imread(imageName)\n        outputs = predictor(im)\n        classes = outputs[\"instances\"].pred_classes.tolist()\n        box_round = outputs[\"instances\"].pred_boxes.tensor.tolist()\n        score_output = outputs[\"instances\"].scores.tolist()\n        head, tail = os.path.split(imageName)\n        img_id = int(tail.split('.')[0])\n\n        for i in range(len(box_round)):\n\n            x1, y1, x2, y2 = box_round[i]\n            label = int(classes[i]) + 1\n            score = float(score_output[i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})\n\ntest_pred_json = os.path.join(\"/kaggle/working\", \"test_pred_2.json\")\nwith open(test_pred_json, 'w') as f:\n    json.dump(detections, f)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T04:59:34.386679Z","iopub.execute_input":"2021-06-22T04:59:34.387018Z","iopub.status.idle":"2021-06-22T05:00:01.482452Z","shell.execute_reply.started":"2021-06-22T04:59:34.386986Z","shell.execute_reply":"2021-06-22T05:00:01.481659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check \nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nsample_json_path = os.path.join(\"/kaggle/input/dsta-brainhack-2021/c1_test_release/c1_test_release\", \"c1_test_sample.json\")\n\ncoco_gt = COCO(sample_json_path)\ncoco_dt = coco_gt.loadRes(test_pred_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T05:00:01.483886Z","iopub.execute_input":"2021-06-22T05:00:01.484204Z","iopub.status.idle":"2021-06-22T05:00:01.505647Z","shell.execute_reply.started":"2021-06-22T05:00:01.484169Z","shell.execute_reply":"2021-06-22T05:00:01.504144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"othermodels\" ></a>\n# 5. Other models\n\nIt's possible to use other high-end object detection models aswell. Let's check it out!\n\n<a id=\"keypoint\" ></a>\n## 5.1. Keypoint detection","metadata":{}},{"cell_type":"markdown","source":"Reload the data.","metadata":{"trusted":true}},{"cell_type":"code","source":"# !wget http://images.cocodataset.org/val2017/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\"./input.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cfg = get_cfg()   # fresh config\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# outputs = predictor(im)\n# v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n# plt.figure(figsize=(15,7))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"panoptic\" ></a>\n## 5.2. Panoptic segmentation","metadata":{}},{"cell_type":"code","source":"# !wget http://images.cocodataset.org/val2017/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\"./input.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n# plt.figure(figsize=(25,15))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"semantic\" ></a>\n## 5.3. Semantic, Densepose, ...\n\nWill be added in a future version! Stay tuned!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"video\" ></a>\n# 6. Video\n\nSo up until now we've been working with images only. Can we quickly use the models for videos? The answer is YES!\n\n<a id=\"videolib\" ></a>\n## 6.1. Libraries\nAs you can see we actually don't need many other libraries. Lets import a library to handle the video.","metadata":{}},{"cell_type":"code","source":"# from IPython.display import YouTubeVideo, display, Video # for viewing the video\n# !pip install youtube-dl # for downloading the video","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"thevideo\" ></a>\n## 6.2. The video","metadata":{}},{"cell_type":"code","source":"# #video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)#7HaJArMDKgI\n# video = YouTubeVideo(\"7HaJArMDKgI\", width=750, height= 450)#\n# display(video)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downloading the video and cropping 6 seconds for processing\n","metadata":{}},{"cell_type":"code","source":"# !youtube-dl https://www.youtube.com/watch?v=7HaJArMDKgI -f 22 -o video.mp4\n# !ffmpeg -i video.mp4 -t 00:00:10 -c:v copy video-clip.mp4 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"videoinference\" ></a>\n## 6.3. Inference on the video\nLet's now run an panoptic model over the video above.\n\n*note: For now I'll be using some [demo](https://github.com/facebookresearch/detectron2/tree/master/demo) files, I'll later add the code implementations to this notebook.*","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/facebookresearch/detectron2\n# !python detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output 1video-output.mkv \\\n#   --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the result! \n\n*I've ran into some trouble with video encoding opencv and ffmpeg (fix in future version of this notebook).*","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/vandeveldemaarten/tempdetector2video.git\n# Video(\"./tempdetector2video/myvideo.mkv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# That's all for now!\n\nThank you for reading this notebook! If you enjoyed it, please upvote!\n\n*More coming soon!*","metadata":{"trusted":true}}]}