{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_colwidth', 200)\n\nimport tarfile\nimport scipy.io\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\n# !pip install -q efficientnet\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import activations as Ac\n\n\n# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# from efficientnet.tfkeras import EfficientNetB3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/fgvc-aircraft/train.csv')\nval = pd.read_csv('../input/fgvc-aircraft/val.csv')\ntest = pd.read_csv('../input/fgvc-aircraft/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpath = \"../input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images\"\ntrain_paths = train.filename.apply(lambda x: os.path.join(tpath, x))\ntrain_paths.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(train.Labels)\ntrain_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpath = \"../input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images\"\nval_paths = val.filename.apply(lambda x: os.path.join(tpath, x))\nval_paths.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to one-hot-encoding-labels\nval_labels = to_categorical(val.Labels)\nval_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tpath = \"../input/fgvc-aircraft/fgvc-aircraft-2013b/fgvc-aircraft-2013b/data/images\"\ntest_paths = test.filename.apply(lambda x: os.path.join(tpath, x))\ntest_paths.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(filename, label=None, image_size=(299, 299)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(augment, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nval_dataset = (tf.data.Dataset\n        .from_tensor_slices((val_paths, val_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(batch_size)\n)\n\ntest_dataset = (tf.data.Dataset\n        .from_tensor_slices(test_paths)\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(batch_size)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_addons as tfa\n\ndef Ranger(sync_period=6,\n           slow_step_size=0.5,\n           learning_rate=0.01,\n           beta_1=0.9,\n           beta_2=0.999,\n           epsilon=1e-7,\n           weight_decay=0.,\n           amsgrad=False,\n           sma_threshold=5.0,\n           total_steps=0,\n           warmup_proportion=0.1,\n           min_lr=0.,\n           name=\"Ranger\"):\n    \"\"\"\n        function returning a tf.keras.optimizers.Optimizer object\n        returned optimizer is a Ranger optimizer\n        Ranger is an optimizer combining RAdam (https://arxiv.org/abs/1908.03265) and Lookahead (https://arxiv.org/abs/1907.0861)\n        returned optimizer can be fed into the model.compile method of a tf.keras model as an optimizer\n        ...\n        Attributes\n        ----------\n        learning_rate : float\n            step size to take for RAdam optimizer (depending on gradient)\n        beta_1 : float\n            parameter that specifies the exponentially moving average length for momentum (0<=beta_1<=1)\n        beta_2 : float\n            parameter that specifies the exponentially moving average length for variance (0<=beta_2<=1)\n        epsilon : float\n            small number to cause stability for variance division\n        weight_decay : float\n            number with which the weights of the model are multiplied each iteration (0<=weight_decay<=1)\n        amsgrad : bool\n            parameter that specifies whether to use amsgrad version of Adam (https://arxiv.org/abs/1904.03590)\n        total_steps : int\n            total number of training steps\n        warmup_proportion : float\n            the proportion of updated over which the learning rate is increased from min learning rate to learning rate (0<=warmup_proportion<=1)\n        min_lr : float\n            learning rate at which the optimizer starts\n        k : int\n            parameter that specifies after how many steps the lookahead step backwards should be applied\n        alpha : float\n            parameter that specifies how much in the direction of the fast weights should be moved (0<=alpha<=1)\n    \"\"\"\n    # create RAdam optimizer\n    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n    # feed RAdam optimizer into lookahead operation\n    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n    return optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mish(x):\n    return x * K.tanh(Ac.softplus(x))\n\nmodel = tf.keras.Sequential([\n        DenseNet201(weights = 'imagenet', \n                       include_top = False,\n                       input_shape = (299, 299, 3)),\n    \n        GlobalAveragePooling2D(),\n        Dense(1024, activation = mish),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(512, activation = mish),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(100, activation = 'softmax')\n])\n    \noptimizer = SGD()\nmodel.compile(optimizer = optimizer,\n              loss = 'categorical_crossentropy',\n              metrics = ['accuracy'])\n    \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Train_model(model, batch_size, EPOCHS):\n    \n    n_steps = train_labels.shape[0] // batch_size\n    EPOCHS = EPOCHS\n    \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n                                                     factor = 0.5,\n                                                     patience = 3, \n                                                     min_lr = 0.0001)\n    \n    clr = CyclicLR(base_lr = 0.001, \n                   max_lr = 0.01, \n                   step_size = 2000, \n                   mode = 'triangular',)\n    \n    \n    Model = model\n    history = Model.fit(train_dataset, \n                    steps_per_epoch = n_steps,\n                    epochs = EPOCHS,\n                    validation_data = val_dataset,\n                    callbacks = [clr],\n                    verbose = 1)\n    \n    return Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\nEPOCHS = 40\n\ndef check(x, y):\n    if x == y:\n        return 1\n    else:\n        return 0\n    \nprint('Training')\nmodel = Train_model(model, batch_size, EPOCHS)\npred = model.predict(test_dataset, verbose=1)\n    \ntest['Prediction'] = pred.argmax(axis=1)\ntest['Score'] = test.apply(lambda x: check(x['Prediction'], x['Labels']), axis=1)\nprint('Test accuracy on iterations is ', 100 * test.Score.sum() / test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(test.Labels, test.Prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}