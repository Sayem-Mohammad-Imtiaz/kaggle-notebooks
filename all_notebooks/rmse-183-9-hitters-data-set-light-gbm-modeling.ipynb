{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Light GBM modeling for Hitters Data Set( RMSE: 183.9 )","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Library\n\n# Firstly used libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Warnings\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# Data Preprocessing\n\nfrom sklearn.neighbors import LocalOutlierFactor \nfrom sklearn import preprocessing\n\n# Modeling\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\n!pip install lightgbm\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Model Tuning\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# read the data\nhitters=pd.read_csv(\"../input/hitters/Hitters.csv\")\nhitters.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# DATA UNDERSTANDING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are 322 observations and int-float-object types of features in this data set.\n\ndf=hitters.copy()\nprint(df.shape)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are 59 null values in Hitters data set\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All these NA values comes from \"Salary\" feature\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are high correlated  features within themselves. However,being high correlated is not a problem in machine learning algorithms.\n# In addition, there isn't high correlation between independent features and target feature(Salary).\nplt.figure(figsize=(14,12))\nsns.heatmap(df.corr(), annot=True, cmap=\"BuPu\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If the missing values don't come from Salary(target feature), i would have thought to assign mean according to these results.\n# Because, there seems to be a relation between categoric variables and Salary values for example there is an important differences between being E Division and W Devision.\n\nprint(\"New League= A\" ,df[df[\"NewLeague\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"New League= N\" ,df[df[\"NewLeague\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= A\" ,df[df[\"League\"]==\"A\"].agg({\"Salary\":\"mean\"}))\nprint(\"League= N\" ,df[df[\"League\"]==\"N\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= E\" ,df[df[\"Division\"]==\"E\"].agg({\"Salary\":\"mean\"}))\nprint(\"Division= W\" ,df[df[\"Division\"]==\"W\"].agg({\"Salary\":\"mean\"}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA PREPROCESSING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1st Trial : df3\n\n* **df1-->df2-->df3**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* size= 322-->261","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop NA values\n\ndf1=df.dropna()\ndf1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# understanding skewness of the features ( It is acceptable if the skewness is btween -1 and 1)\n# When the value of the skewness is negative, the tail of the distribution is longer towards the left hand side of the curve.\n# When the value of the skewness is positive, the tail of the distribution is longer towards the right hand side of the curve.\ndf1.skew(axis = 0, skipna = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.skew(axis = 0, skipna = True)[(df1.skew(axis = 0, skipna = True) >1) | (df1.skew(axis = 0, skipna = True)< -1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying log transformation for right skewed features and applying exponential for left skewed features\nsns.distplot(df1[\"CAtBat\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"CAtBat\"]= np.log(df1[\"CAtBat\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df1[\"CHits\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"CHits\"]= np.log(df1[\"CHits\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df1[\"CHmRun\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"CHmRun\"]=np.log(df1[\"CHmRun\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df1[\"CRuns\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"CRuns\"]= np.log(df1[\"CRuns\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df1[\"CRBI\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"CRBI\"]= np.log(df1[\"CRBI\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df1[\"CWalks\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"CWalks\"]= np.log(df1[\"CWalks\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df1[\"PutOuts\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"PutOuts\"]= np.log(df1[\"PutOuts\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df1[\"Assists\"], hist=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[\"Assists\"]= np.log(df1[\"Assists\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get dummies\n\ndf1 =pd.get_dummies(df1,columns= [\"League\",\"Division\",\"NewLeague\"], drop_first=True)\ndf1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1=df1.loc[:, \"AtBat\":\"Errors\"]\ncat_df1=df1.loc[:, \"League_N\":\"NewLeague_N\"]\ny_df1= df1[\"Salary\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1.isin(['-inf']).any()==True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1[numeric_df1[\"CHmRun\"].astype(\"str\").str.get(1)==\"i\"].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1[numeric_df1[\"CHmRun\"].astype(\"str\").str.get(1)==\"i\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1[\"CHmRun\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign median to infinite values in CHmRun\n\nnumeric_df1.loc[[7, 188, 239],\"CHmRun\"]=3.688879\nnumeric_df1[\"CHmRun\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1[numeric_df1[\"PutOuts\"].astype(\"str\").str.get(1)==\"i\"].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1[\"PutOuts\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign median to infinite values in PutOuts\n\nnumeric_df1.loc[[9, 65, 132, 149, 186, 196, 198, 207, 249, 251, 267],\"PutOuts\"]=5.411646\nnumeric_df1[\"PutOuts\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1[numeric_df1[\"Assists\"].astype(\"str\").str.get(1)==\"i\"].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_df1[\"Assists\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign median to infinite values in Assists\n\nnumeric_df1.loc[[9, 65, 132, 149, 176, 186, 196, 198, 207, 249, 251, 255, 267, 304],\"Assists\"]=3.806662\nnumeric_df1[\"Assists\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=numeric_df1.copy()\n\n# LOF  Outlier Detection\n\nclf= LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nclf.fit_predict(df2)\n\ndf2_scores=clf.negative_outlier_factor_\nnp.sort(df2_scores)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df2_scores);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_indexes=df2.loc[df2_scores< -1.73878565]\noutlier_indexes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Throw away outliers from Salary feature also according to these indexes .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_df1=pd.DataFrame(y_df1).drop(index=[217,295])\ny_df1=y_df1.reset_index(drop=True)\nprint(y_df1.shape )\ny_df1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2= df2.loc[df2_scores> -1.73878565]\ndf2=df2.reset_index(drop=True)\nprint(df2.shape)\ndf2.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Throw away outliers from dummies also according to these indexes .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df1=pd.DataFrame(cat_df1).drop(index=[217,295])\ncat_df1=cat_df1.reset_index(drop=True)\nprint(cat_df1.shape)\ncat_df1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3= pd.concat([df2,y_df1,cat_df1], axis=1)\ndf3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2nd Trial : df4\n\n* **df2-->df10-->df4**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* standardize df2\n* size= 322-->261","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2_columns=df2.columns\nstandardized_df2=preprocessing.scale(df2)\nstandardized_df2=pd.DataFrame(standardized_df2, columns=df2_columns)\nstandardized_df2.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_df10=hitters.dropna()[\"Salary\"]\ny_df10=pd.DataFrame(y_df10).drop(index=[217,295])\ny_df10=y_df10.reset_index(drop=True)\ny_df10.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_df10=cat_df1.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4= pd.concat([standardized_df2,y_df10,cat_df10], axis=1)\ndf4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3rd Trial : df5\n\n* **df4-->df5**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* standardize df2\n* generating variables\n* size= 322-->261","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5=df4.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(df5.corr(), annot=True, cmap=plt.cm.Reds);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5[\"walks/cwalks\"]= df5[\"Walks\"]/df5[\"CWalks\"]\ndf5[\"CAtBat/Years\"]= df5[\"CAtBat\"]/df5[\"Years\"]\ndf5[\"CHits/Years\"]= df5[\"CHits\"]/df5[\"Years\"]\ndf5[\"CHmRun/Years\"]= df5[\"CHmRun\"]/df5[\"Years\"]\ndf5[\"Hits/CHits\"]= df5[\"Hits\"]/df5[\"CHits\"]\ndf5[\"Assists/Errors\"]= df5[\"Assists\"]/df5[\"Errors\"]\ndf5[\"CHits/CRBI\"]= df5[\"CHits\"]/df5[\"CRBI\"]\ndf5[\"HmRun/CHmRun\"]= df5[\"HmRun\"]/df5[\"CHmRun\"]\ndf5[\"CRBI/RBI\"]= df5[\"CRBI\"]/df5[\"RBI\"]\ndf5[\"CRuns/CHits\"]= df5[\"CRuns\"]/df5[\"CHits\"]\ndf5[\"AtBat/PutOuts\"]= df5[\"AtBat\"]/df5[\"PutOuts\"]\ndf5[\"Walks/Years\"]=df5[\"Walks\"]/df5[\"Years\"]\n\nplt.figure(figsize=(14,12))\nsns.heatmap(df5.corr(), annot=True, cmap=plt.cm.Blues);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4th Trial : df6\n\n* **df3-->df6**\n\n* drop NA values\n* log transformation\n* detect outliers and drop them\n* generating variables\n* size= 322-->261","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df6=df3.copy()\n\ndf6[\"walks/cwalks\"]= df6[\"Walks\"]/df6[\"CWalks\"]\ndf6[\"CAtBat/Years\"]= df6[\"CAtBat\"]/df6[\"Years\"]\ndf6[\"CHits/Years\"]= df6[\"CHits\"]/df6[\"Years\"]\ndf6[\"CHmRun/Years\"]= df6[\"CHmRun\"]/df6[\"Years\"]\ndf6[\"Hits/CHits\"]= df6[\"Hits\"]/df5[\"CHits\"]\ndf6[\"CHits/CRBI\"]= df6[\"CHits\"]/df6[\"CRBI\"]\ndf6[\"CRBI/RBI\"]= df6[\"CRBI\"]/df6[\"RBI\"]\ndf6[\"CRuns/CHits\"]= df6[\"CRuns\"]/df6[\"CHits\"]\ndf6[\"AtBat/PutOuts\"]= df6[\"AtBat\"]/df6[\"PutOuts\"]\ndf6[\"Walks/Years\"]=df6[\"Walks\"]/df6[\"Years\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## df3 modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df3[\"Salary\"]\nX=df3.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## df4 modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df4[\"Salary\"]\nX=df4.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## df5 modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df5[\"Salary\"]\nX=df5.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## df6 modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df6[\"Salary\"]\nX=df6.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nmodels = []\n\nmodels.append(('Regression', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('SVR', SVR()))\nmodels.append(('GBM', GradientBoostingRegressor()))\nmodels.append((\"XGBoost\", XGBRegressor()))\nmodels.append((\"LightGBM\", LGBMRegressor()))\nmodels.append((\"CatBoost\", CatBoostRegressor(verbose = False)))\n\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(name,rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TUNING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGB Feature Importance according to df6\n\n\ny=df6[\"Salary\"]\nX=df6.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nlgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n\nImportance = pd.DataFrame({'Importance':lgb_model.feature_importances_*100}, \n                          index = X_train.columns)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', figsize=(14,12))\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGB Feature Selection and LGBM : df7","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# df7\n\n# Feature Selection( throw away the features which are not so important for LGBM)\n\ndf7=df6.copy()\ndf7= df7.drop(\"Division_W\", axis=1)\ndf7= df7.drop(\"League_N\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df7[\"Salary\"]\nX=df7.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nlgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Default Parameters\n\nboosting_type='gbdt',\n    num_leaves=31,\n    max_depth=-1,\n    learning_rate=0.1,\n    n_estimators=100,\n    subsample_for_bin=200000,\n    objective=None,\n    class_weight=None,\n    min_split_gain=0.0,\n    min_child_weight=0.001,\n    min_child_samples=20,\n    subsample=1.0,\n    subsample_freq=0,\n    colsample_bytree=1.0,\n    reg_alpha=0.0,\n    reg_lambda=0.0,\n    random_state=None,\n    n_jobs=-1,\n    silent=True,\n    importance_type='split',\n    kwargs,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params= { \"boosting_type\" : [\"dart\"],\n              \"learning_rate\": [0.09, 0.1,0.11, 0.2],\n              \"n_estimators\": [90,100,110,150],\n              \"num_leaves\" :[30,31,32],\n              \"max_depth\": [7,10],\n              \"colsample_bytree\": [1,0.8,0.5,0.4]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_cv_model = GridSearchCV(lgb_model, \n                             lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose =2).fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_cv_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_lgbm= LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)\ny_pred= tuned_lgbm.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check train error to control overfitting\n\ntuned_lgbm2= LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)\ny_pred2= tuned_lgbm.predict(X_train)\nnp.sqrt(mean_squared_error(y_train, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGB Feature Importance according to final df and final model\n\n\ny=df7[\"Salary\"]\nX=df7.drop(\"Salary\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n\nlgb_model = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train, y_train)\ny_pred = lgb_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n\nImportance = pd.DataFrame({'Importance':lgb_model.feature_importances_*100}, \n                          index = X_train.columns)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', figsize=(14,12))\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REPORTING","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n## 1. Data Understanding\n> 1. Kütüphaneler import edildi.Hitters veri setindeki gözlem sayısı, değişken türleri, eksik değerler ve değişkenler arası korelasyonlar incelendi.\n\n> 2. Salary değişkenine atama yapılabilecek bir ilişki var mı diye araştırıldı, Division kategorilerinin maaşlarında anlamlı farklılık olduğu gözlemlense de, hedef değişken olması sebebiyle atama yapılmadı.\n\n\n## 2. Data Preprocessing\n> 1. Veri seti,birbirinden farklı 4 ön işleme sürecinden geçti. Nihai olarak 4 data frame oluşturuldu. (df3-df4-df5-df6)\n\n> 2. df3 -> NA değerleri atıldı.Skewness değeri -1 ile 1 arasında olmayanların sağa çarpık oldukları distplot ile gözlemlendi.\n\n> 3. df3 -> CAtBat - CHits - CHmRun - CRuns - CRBI - CWalks - PutOuts - Assists değişkenlerine log transformation yapıldı. -∞ (-inf) gelen değerlere median ataması yapıldı.Çarpıklıkları giderildi.\n\n> 4. df3 -> df1 veri seti, kategorik, numerik ve hedef değişkene göre split edildi. Kategorik değişkenlere dummy dönüşümü yapıldı.\n\n> 5. df3 -> LOC ile outlierlar tespit edildi. Boxplot ile incelendi, diğer skorlardan çok uzakta kalan 2 gözlemin indexi belirlendikten sonra kategorik, numerik ve hedef değişken içeren dflerden ayrı ayrı atıldı.\n\n> 6. df3 -> concat ile kategorik, numerik ve hedef değişken içeren df ler birleştirildi. 261 gözlem içeren nihai df oluşturuldu ve df3 ismi verildi.\n\n> 7. df4 -> Yukarıda oluşturulan df3 standardize edildi. (mean 0, std=1)\n\n> 8. df5 -> Yukarıda oluşturulan df4 üzerine yeni değişkenler eklendi. Bunlar: walks/cwalks, CAtBat/Years, CHits/Years , CHmRun/Years, Hits/CHits, Assists/Errors, CHits/CRBI, HmRun/CHmRun, CRBI/RBI, CRuns/CHits, AtBat/PutOuts, Walks/Years\n\n> 9. df6 -> Yukarıda oluşturulan df3 üzerine yeni değişkenler eklendi. Bunlar: walks/cwalks, CAtBat/Years, CHits/Years , CHmRun/Years, Hits/CHits, CHits/CRBI, CRBI/RBI, CRuns/CHits, AtBat/PutOuts, Walks/Years   \n\n\n## 3. Modeling\n\n> 1. Yukarıda belirtilen 4 data frame modellere fit ettirildi. ( Regression-Ridge- Lasso-Elastic Net- KNN - CART - RF- SVR - GBM - XGBoost - LightGBM - CatBoost)\n\n> 2. **df3 -> en iyi RMSE -> LightGBM 191.9**\n\n> 3. **df4 -> en iyi RMSE -> LightGBM 194.7**\n\n> 4. **df5 -> en iyi RMSE -> CatBoost 198.8**\n\n> 5. **df6 -> en iyi RMSE -> LightGBM 189.4**\n\n> 6. 4 data setinde genel olarak Light GBM ile daha düşük hata elde edildi. En iyi sonuç ise 189 ile df6ya fit edilen Light GBM ile elde edildi. \n\n\n## 4. Tuning\n\n> 1. Light GBM feature selection ile önemi çok az olan 2 değişkeni attım -> **df7-> LightGBM 186.8**\n\n> 2. Hiperparametrelerden, boosting_type, learning_rate, n_estimators, num_leaves, max_depth, colsample_bytree değerleri denenerek en iyi model bulunmaya çalışıldı -> **LightGBM 183.9**\n\n\n# Sonuç:\n> **Nihai RMSE 183.9 olarak hesaplandı.**\n\n> **Train verisi üzerinde overfitting kontrolü yapıldı ve olmadığı gözlemlendi.**\n\n> **Nihai modele göre en önemli değişkenler saptandı.**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}