{"cells":[{"metadata":{},"cell_type":"markdown","source":"#     Using Machine Learning to Diagnose Breast Cancer in Python\n## by:  Steven Smiley"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement:\n\nFind a Machine Learning (ML) model that accurately predicts breast cancer based on the 30 features described below."},{"metadata":{},"cell_type":"markdown","source":"# 1.  Background:\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\n\nAlso can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nAttribute Information:\n\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\n* a) radius (mean of distances from center to points on the perimeter) \n* b) texture (standard deviation of gray-scale values) \n* c) perimeter \n* d) area \n* e) smoothness (local variation in radius lengths) \n* f) compactness (perimeter^2 / area - 1.0) \n* g) concavity (severity of concave portions of the contour) \n* h) concave points (number of concave portions of the contour) \n* i) symmetry \n* j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant"},{"metadata":{},"cell_type":"markdown","source":"# 2. Abstract:\n\n  When it comes to diagnosing breast cancer, we want to make sure we don't have too many false-positives (you don't have cancer, but told you do and go on treatment) or false-negatives (you have cancer, but told you don't and don't get treatment). Therefore, the highest overall accuracy model is chosen.  \n  \n  The Data was split into 80% training (~455 people) and 20% testing (~114 people).\n  \n  Several different models were evaluated through k-fold Cross-Validation with GridSearchCV, which iterates on different algorithm's hyperparameters:\n  * Logistic Regression \n  * Support Vector Machine\n  * Neural Network\n  * Random Forest\n  * Gradient Boost\n  * eXtreme Gradient Boost\n\n\n All of the models performed well after fine tunning their hyperparameters, but the best model is the one the highest overall accuracy.  Out of the 20% of data witheld in this test (114 random individuals), only a handful were misdiagnosed.  No model is perfect, but I am happy about how accurate my model is here.  If on average less than a handful of people out of 114 are misdiagnosed, that is a good start for making a model.  Furthermore, the Feature Importance plots show that the \"concave points worst\" and \"concave points mean\" were the significant features.  Therefore, I recommend the concave point features should be extracted from each future biopsy as a strong predictor for diagnosing breast cancer.   \n\n"},{"metadata":{},"cell_type":"markdown","source":"# 3.  Import Libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport os # Get Current Directory\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport pandas as pd # data processing, CSV file I/O (e.i. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nfrom time import time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom scipy import stats\nimport subprocess\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.utils.multiclass import unique_labels\nimport itertools\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hide Warnings"},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\npd.set_option('mode.chained_assignment', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Current Directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"currentDirectory=os.getcwd()\nprint(currentDirectory)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Import and View Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data= pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\n#data=os.path.join(currentDirectory,'data.csv')\n#data= pd.read_csv(data)\ndata.head(10) # view the first 10 columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Import and View Data:  Check for Missing Values\n\nAs the background stated, no missing values should be present.  The following verifies that.  The last column doesn't hold any information and should be removed.  In addition, the diagnosis should be changed to a binary classification of 0= benign and 1=malignant."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop Unnamed: 32 variable that has NaN values.\ndata.drop(['Unnamed: 32'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Diagnosis for Cancer from Categorical Variable to Binary\ndiagnosis_num={'B':0,'M':1}\ndata['diagnosis']=data['diagnosis'].map(diagnosis_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify Data Changes, look at first 5 rows \ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2  Heatmap with Pearson Correlation Coefficient  for Features\nA strong correlation is indicated by a Pearson Correlation Coefficient value near 1.  Therefore, when looking at the Heatmap, we want to see what correlates most with the first column, \"diagnosis.\"  It appears that the features of \"concave points worst\" [0.79] has the strongest correlation with \"diagnosis\".  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#fix,ax = plt.subplots(figsize=(25,25))\nfix,ax = plt.subplots(figsize=(22,22))\nheatmap_data = data.drop(['id'],axis=1)\nsns.heatmap(heatmap_data.corr(),vmax=1,linewidths=0.01,square=True,annot=True,linecolor=\"white\")\nbottom,top=ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)\nheatmap_title='Figure 1:  Heatmap with Pearson Correlation Coefficient for Features'\nax.set_title(heatmap_title)\nplt.savefig('Figure1.Heatmap.png',dpi=300,bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5.  Split Data for Training  "},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Split Data for Training : Standardize and Split the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['id','diagnosis'], axis= 1)\ny = data.diagnosis\n\n#Standardize Data\nscaler = StandardScaler()\nX=StandardScaler().fit_transform(X.values)\nX = pd.DataFrame(X)\nX.columns=(data.drop(['id','diagnosis'], axis= 1)).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A good rule of thumb is to hold out 20 percent of the data for testing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 42)\n\n\n#Standardize Data\nscaler = StandardScaler()\n\n#Fit on training set only.\nscaler.fit(X_train)\n\n#Apply transform to both the training and test set\nX_train=scaler.transform(X_train)\nX_test=scaler.transform(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Split Data for Training: Feature Extraction with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Feature Extraction:  Principal Component Analysis: PC1, PC2\npca = PCA(n_components=2, random_state=42) \n# Only fit to the training set\npca.fit((X_train))\n# transform with PCA model from training\nprincipalComponents_train = pca.transform(X_train)\nprincipalComponents_test = pca.transform(X_test)\n\n# Use Pandas DataFrame\nX_train = pd.DataFrame(X_train)\nX_test=pd.DataFrame(X_test)\nX_train.columns=(data.drop(['id','diagnosis'], axis= 1)).columns\nX_test.columns=(data.drop(['id','diagnosis'], axis= 1)).columns\ny_train = pd.DataFrame(y_train)\ny_test=pd.DataFrame(y_test)\n\nX_train['PC1']=principalComponents_train[:,0]\nX_train['PC2']=principalComponents_train[:,1]\nX_test['PC1']=principalComponents_test[:,0]\nX_test['PC2']=principalComponents_test[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_features=X_train\ntr_labels=y_train\n\nval_features = X_test\nval_labels=y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Split Data for Training:  Verify the Split"},{"metadata":{},"cell_type":"markdown","source":"Verify the data was split correctly"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_train - length:',len(X_train), 'y_train - length:',len(y_train))\nprint('X_test - length:',len(X_test),'y_test - length:',len(y_test))\nprint('Percent heldout for testing:', round(100*(len(X_test)/len(data)),0),'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Machine Learning:\n\nIn order to find a good model, several algorithms are tested on the training dataset. A senstivity study using different Hyperparameters of the algorithms are iterated on with GridSearchCV in order optimize each model. The best model is the one that has the highest accuracy without overfitting by looking at both the training data and the validation data results. Computer time does not appear to be an issue for these models, so it has little weight on deciding between models."},{"metadata":{},"cell_type":"markdown","source":"## GridSearch CV\n\nclass sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, n_jobs=None, iid='deprecated', refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)[source]¶\n\nExhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid."},{"metadata":{},"cell_type":"markdown","source":"#### Function: print_results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_results(results,name,filename_pr):\n    with open(filename_pr, mode='w') as file_object:\n        print(name,file=file_object)\n        print(name)\n        print('BEST PARAMS: {}\\n'.format(results.best_params_),file=file_object)\n        print('BEST PARAMS: {}\\n'.format(results.best_params_))\n        means = results.cv_results_['mean_test_score']\n        stds = results.cv_results_['std_test_score']\n        for mean, std, params in zip(means, stds, results.cv_results_['params']):\n            print('{} {} (+/-{}) for {}'.format(name,round(mean, 3), round(std * 2, 3), params),file=file_object)\n            print('{} {} (+/-{}) for {}'.format(name,round(mean, 3), round(std * 2, 3), params))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(GridSearchCV)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.1 Machine Learning Models:  Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression:  Hyperparameter used in GridSearchCV\n### HP1, C:  float, optional (default=1.0)\nInverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n##### Details\nRegularization is when a penality is applied with increasing value to prevent overfitting.  The inverse of regularization strength means as the value of C goes up, the value of the regularization strength goes down and vice versa.  \n##### Values chosen\n'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_model_dir=os.path.join(currentDirectory,'LR_model.pkl')\nif os.path.exists(LR_model_dir) == False:\n    lr = LogisticRegression()\n    parameters = {\n            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n            }\n    cv=GridSearchCV(lr, parameters, cv=5)\n    cv.fit(tr_features,tr_labels.values.ravel())      \n    print_results(cv,'Logistic Regression (LR)','LR_GridSearchCV_results.txt')\n    cv.best_estimator_\n    LR_model_dir=os.path.join(currentDirectory,'LR_model.pkl')\n    joblib.dump(cv.best_estimator_,LR_model_dir)\nelse:\n    print('Already have LR')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.2 Machine Learning Models:  Support Vector Machine"},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine:  \n### Hyperparameter used in GridSearchCV\n#### HP1,  kernelstring, optional (default=’rbf’)\nSpecifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n###### Details\nA linear kernel type is good when the data is Linearly seperable, which means it can be separated by a single Line.\nA radial basis function (rbf) kernel type is an expontential function of the squared Euclidean distance between two vectors and a constant.  Since the value of RBF kernel decreases with distance and ranges between zero and one, it has a ready interpretation as a similiarity measure.  \n###### Values chosen\n'kernel': ['linear','rbf']\n\n#### HP2,  C:  float, optional (default=1.0)\nRegularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n###### Details\nRegularization is when a penality is applied with increasing value to prevent overfitting.  The inverse of regularization strength means as the value of C goes up, the value of the regularization strength goes down and vice versa.  \n###### Values chosen\n'C': [0.1, 1, 10]"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(SVC())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SVM_model_dir=os.path.join(currentDirectory,'SVM_model.pkl')\nif os.path.exists(SVM_model_dir) == False:\n    svc = SVC()\n    parameters = {\n            'kernel': ['linear','rbf'],\n            'C': [0.1, 1, 10]\n            }\n    cv=GridSearchCV(svc,parameters, cv=5)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Support Vector Machine (SVM)','SVM_GridSearchCV_results.txt')\n    cv.best_estimator_\n    SVM_model_dir=os.path.join(currentDirectory,'SVM_model.pkl')\n    joblib.dump(cv.best_estimator_,SVM_model_dir)\nelse:\n    print('Already have SVM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.3 Machine Learning Models:  Neural Network"},{"metadata":{},"cell_type":"markdown","source":"## Neural Network:  (sklearn)\n### Hyperparameter used in GridSearchCV\n#### HP1, hidden_layer_sizes:  tuple, length = n_layers - 2, default (100,)\nThe ith element represents the number of neurons in the ith hidden layer.\n###### Details\nA rule of thumb is (2/3)*(# of input features) = neurons per hidden layer. \n###### Values chosen\n'hidden_layer_sizes': [(10,),(50,),(100,)]\n\n#### HP2, activation:  {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\nActivation function for the hidden layer.\n###### Details\n* ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n* ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n* ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n* ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)   \n###### Values chosen\n'hidden_layer_sizes': [(10,),(50,),(100,)]\n\n#### HP3, learning_rate:  {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’\nLearning rate schedule for weight updates.\n###### Details\n* ‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n* ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)\n* ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5.\n\nOnly used when solver='sgd'.\n  \n###### Values chosen\n'learning_rate': ['constant','invscaling','adaptive']"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(MLPClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MLP_model_dir=os.path.join(currentDirectory,'MLP_model.pkl')\nif os.path.exists(MLP_model_dir) == False:\n    mlp = MLPClassifier()\n    parameters = {\n            'hidden_layer_sizes': [(10,),(50,),(100,)],\n            'activation': ['relu','tanh','logistic'],\n            'learning_rate': ['constant','invscaling','adaptive']\n            }\n    cv=GridSearchCV(mlp, parameters, cv=5)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Neural Network (MLP)','MLP_GridSearchCV_results.txt')\n    cv.best_estimator_\n    MLP_model_dir=os.path.join(currentDirectory,'MLP_model.pkl')\n    joblib.dump(cv.best_estimator_,MLP_model_dir)\nelse:\n    print('Already have MLP')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.4 Machine Learning Models:  Random Forest"},{"metadata":{},"cell_type":"markdown","source":"## Random Forest:  \n### Hyperparameter used in GridSearchCV\n#### HP1, n_estimators:  integer, optional (default=100)\nThe number of trees in the forest.\n\nChanged in version 0.22: The default value of n_estimators changed from 10 to 100 in 0.22.\n###### Details\nUsually 500 does the trick and the accuracy and out of bag error doesn't change much after. \n###### Values chosen\n'n_estimators': [500],\n\n#### HP2, max_depth:  integer or None, optional (default=None)\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n###### Details\nNone usually does the trick, but a few shallow trees are tested. \n###### Values chosen\n'max_depth': [5,7,9, None]"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(RandomForestClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF_model_dir=os.path.join(currentDirectory,'RF_model.pkl')\nif os.path.exists(RF_model_dir) == False:\n    rf = RandomForestClassifier(oob_score=False)\n    parameters = {\n            'n_estimators': [500],\n            'max_depth': [5,7,9, None]\n            }\n    cv = GridSearchCV(rf, parameters, cv=5)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Random Forest (RF)','RF_GridSearchCV_results.txt')\n    cv.best_estimator_\n    RF_model_dir=os.path.join(currentDirectory,'RF_model.pkl')\n    joblib.dump(cv.best_estimator_,RF_model_dir)\nelse:\n    print('Already have RF')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.5 Machine Learning Models:  Gradient Boosting"},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting:  \n### Hyperparameter used in GridSearchCV\n#### HP1, n_estimators:  int (default=100)\nThe number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n###### Details\nUsually 500 does the trick and the accuracy and out of bag error doesn't change much after. \n###### Values chosen\n'n_estimators': [5, 50, 250, 500],\n\n#### HP2, max_depth:  integer, optional (default=3)\nmaximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n###### Details\nA variety of shallow trees are tested. \n###### Values chosen\n'max_depth': [1, 3, 5, 7, 9],\n\n#### HP3, learning_rate:  float, optional (default=0.1)\nlearning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n###### Details\nA variety was chosen because of the trade-off.\n###### Values chosen\n'learning_rate': [0.01, 0.1, 1]"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(GradientBoostingClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GB_model_dir=os.path.join(currentDirectory,'GB_model.pkl')\nif os.path.exists(GB_model_dir) == False:\n    gb = GradientBoostingClassifier()\n    parameters = {\n            'n_estimators': [5, 50, 250, 500],\n            'max_depth': [1, 3, 5, 7, 9],\n            'learning_rate': [0.01, 0.1, 1]\n            }\n    cv=GridSearchCV(gb, parameters, cv=5)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'Gradient Boost (GB)','GR_GridSearchCV_results.txt')\n    cv.best_estimator_\n    GB_model_dir=os.path.join(currentDirectory,'GB_model.pkl')\n    joblib.dump(cv.best_estimator_,GB_model_dir)\nelse:\n    print('Already have GB') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.6 Machine Learning Models:  eXtreme Gradient Boosting"},{"metadata":{},"cell_type":"markdown","source":"## eXtreme Gradient Boosting:  \n### Hyperparameter used in GridSearchCV\n#### HP1, n_estimators:  (int) – Number of trees to fit.\n###### Details\nUsually 500 does the trick and the accuracy and out of bag error doesn't change much after. \n###### Values chosen\n'n_estimators': [5, 50, 250, 500],\n\n#### HP2, max_depth:  (int) – \nMaximum tree depth for base learners.\n###### Details\nA variety of shallow trees are tested. \n###### Values chosen\n'max_depth': [1, 3, 5, 7, 9],\n\n#### HP3, learning_rate: (float) – \nBoosting learning rate (xgb’s “eta”)\n###### Details\nA variety was chosen because of the trade-off.\n###### Values chosen\n'learning_rate': [0.01, 0.1, 1]"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB_model_dir=os.path.join(currentDirectory,'XGB_model.pkl')\nif os.path.exists(XGB_model_dir) == False:\n    xgb = XGBClassifier()\n    parameters = {\n            'n_estimators': [5, 50, 250, 500],\n            'max_depth': [1, 3, 5, 7, 9],\n            'learning_rate': [0.01, 0.1, 1]\n            }\n    cv=GridSearchCV(xgb, parameters, cv=5)\n    cv.fit(tr_features, tr_labels.values.ravel())\n    print_results(cv,'eXtreme Gradient Boost (XGB)','XGB_GridSearchCV_results.txt')\n    cv.best_estimator_\n    XGB_model_dir=os.path.join(currentDirectory,'XGB_model.pkl')\n    joblib.dump(cv.best_estimator_,XGB_model_dir)\nelse:\n    print('Already have XGB')  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Evaluate Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"## all models\nmodels = {}\n\n#for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB','XGB']:\nfor mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB','XGB']:\n    model_path=os.path.join(currentDirectory,'{}_model.pkl')\n    models[mdl] = joblib.load(model_path.format(mdl))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Function: evaluate_model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(name, model, features, labels, y_test_ev, fc):\n        start = time()\n        pred = model.predict(features)\n        end = time()\n        y_truth=y_test_ev\n        accuracy = round(accuracy_score(labels, pred), 3)\n        precision = round(precision_score(labels, pred), 3)\n        recall = round(recall_score(labels, pred), 3)\n        print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,\n                                                                                       accuracy,\n                                                                                       precision,\n                                                                                       recall,\n                                                                                       round((end - start)*1000, 1)))\n        \n        \n        pred=pd.DataFrame(pred)\n        pred.columns=['diagnosis']\n        # Convert Diagnosis for Cancer from Binary to Categorical\n        diagnosis_name={0:'Benign',1:'Malginant'}\n        y_truth['diagnosis']=y_truth['diagnosis'].map(diagnosis_name)\n        pred['diagnosis']=pred['diagnosis'].map(diagnosis_name)\n        class_names = ['Benign','Malginant']        \n        cm = confusion_matrix(y_test_ev, pred, class_names)\n        \n        FP_L='False Positive'\n        FP = cm[0][1]\n        FN_L='False Negative'\n        FN = cm[1][0]\n        TP_L='True Positive'\n        TP = cm[1][1]\n        TN_L='True Negative'\n        TN = cm[0][0]\n\n        #TPR_L= 'Sensitivity, hit rate, recall, or true positive rate'\n        TPR_L= 'Sensitivity'\n        TPR = round(TP/(TP+FN),3)\n        #TNR_L= 'Specificity or true negative rate'\n        TNR_L= 'Specificity'\n        TNR = round(TN/(TN+FP),3) \n        #PPV_L= 'Precision or positive predictive value'\n        PPV_L= 'Precision'\n        PPV = round(TP/(TP+FP),3)\n        #NPV_L= 'Negative predictive value'\n        NPV_L= 'NPV'\n        NPV = round(TN/(TN+FN),3)\n        #FPR_L= 'Fall out or false positive rate'\n        FPR_L= 'FPR'\n        FPR = round(FP/(FP+TN),3)\n        #FNR_L= 'False negative rate'\n        FNR_L= 'FNR'\n        FNR = round(FN/(TP+FN),3)\n        #FDR_L= 'False discovery rate'\n        FDR_L= 'FDR'\n        FDR = round(FP/(TP+FP),3)\n\n        ACC_L= 'Accuracy'\n        ACC = round((TP+TN)/(TP+FP+FN+TN),3)\n        \n        stats_data = {'Name':name,\n                     ACC_L:ACC,\n                     FP_L:FP,\n                     FN_L:FN,\n                     TP_L:TP,\n                     TN_L:TN,\n                     TPR_L:TPR,\n                     TNR_L:TNR,\n                     PPV_L:PPV,\n                     NPV_L:NPV,\n                     FPR_L:FPR,\n                     FNR_L:FDR}\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        cax = ax.matshow(cm,cmap=plt.cm.gray_r)\n        plt.title('Figure {}.A: {} Confusion Matrix on Unseen Test Data'.format(fc,name),y=1.08)\n        fig.colorbar(cax)\n        ax.set_xticklabels([''] + class_names)\n        ax.set_yticklabels([''] + class_names)\n        # Loop over data dimensions and create text annotations.\n        for i in range(len(class_names)):\n            for j in range(len(class_names)):\n                text = ax.text(j, i, cm[i, j],\n                               ha=\"center\", va=\"center\", color=\"r\")\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.savefig('Figure{}.A_{}_Confusion_Matrix.png'.format(fc,name),dpi=400,bbox_inches='tight')\n        #plt.show()\n        \n        if  name == 'RF' or name == 'GB' or name == 'XGB': \n            # Get numerical feature importances\n            importances = list(model.feature_importances_)\n            importances=100*(importances/max(importances))               \n            feature_list = list(features.columns)\n            sorted_ID=np.argsort(importances)   \n            plt.figure(figsize=[10,10])\n            plt.barh(sort_list(feature_list,importances),importances[sorted_ID],align='center')\n            plt.title('Figure {}.B: {} Variable Importance Plot'.format(fc,name))\n            plt.xlabel('Relative Importance')\n            plt.ylabel('Feature') \n            plt.savefig('Figure{}.B_{}_Variable_Importance_Plot.png'.format(fc,name),dpi=300,bbox_inches='tight')\n            #plt.show()\n        \n        return accuracy,name, model, stats_data\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Function:  sort_list"},{"metadata":{"trusted":true},"cell_type":"code","source":"    def sort_list(list1, list2): \n        zipped_pairs = zip(list2, list1)   \n        z = [x for _, x in sorted(zipped_pairs)]       \n        return z ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Search for best model using test features"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"ev_accuracy=[None]*len(models)\nev_name=[None]*len(models)\nev_model=[None]*len(models)\nev_stats=[None]*len(models)\ncount=1\nfor name, mdl in models.items():\n        y_test_ev=y_test\n        ev_accuracy[count-1],ev_name[count-1],ev_model[count-1], ev_stats[count-1] = evaluate_model(name,mdl,val_features, val_labels, y_test_ev,count+1)\n        diagnosis_name={'Benign':0,'Malginant':1}\n        y_test['diagnosis']=y_test['diagnosis'].map(diagnosis_name)\n        count=count+1\n\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_name=ev_name[ev_accuracy.index(max(ev_accuracy))]    #picks the maximum accuracy\nprint('Best Model:',best_name,'with Accuracy of ',max(ev_accuracy))   \nbest_model=ev_model[ev_accuracy.index(max(ev_accuracy))]    #picks the maximum accuracy\n\nif best_name == 'RF' or best_name == 'GB' or best_name == 'XGB': \n    # Get numerical feature importances\n    importances = list(best_model.feature_importances_)\n    importances=100*(importances/max(importances))               \n    feature_list = list(X.columns)\n    sorted_ID=np.argsort(importances)   \n    plt.figure(figsize=[10,10])\n    plt.barh(sort_list(feature_list,importances),importances[sorted_ID],align='center')\n    plt.title('Figure 8:  Variable Importance Plot -- {}'.format(best_name))\n    plt.xlabel('Relative Importance')\n    plt.ylabel('Feature') \n    plt.savefig('Figure8.png',dpi=300,bbox_inches='tight')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # 8. Conclusions \n  When it comes to diagnosing breast cancer, we want to make sure we don't have too many false-positives (you don't have cancer, but told you do and go on treatment) or false-negatives (you have cancer, but told you don't and don't get treatment). Therefore, the highest overall accuracy model is chosen.  \n\n  All of the models performed well after fine tunning their hyperparameters, but the best model is the one the highest overall accuracy.  Out of the 20% of data witheld in this test (114 random individuals), only a handful were misdiagnosed.  No model is perfect, but I am happy about how accurate my model is here.  If on average less than a handful of people out of 114 are misdiagnosed, that is a good start for making a model.  Furthermore, the Feature Importance plots show that the \"concave points worst\" and \"concave points mean\" were the significant features.  Therefore, I recommend the concave point features should be extracted from each future biopsy as a strong predictor for diagnosing breast cancer.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"ev_stats=pd.DataFrame(ev_stats)\nprint(ev_stats.head(10))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}