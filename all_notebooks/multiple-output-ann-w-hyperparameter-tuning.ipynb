{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==========================================================\n# import libraries\n# ==========================================================\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\nimport time\nimport os\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nLOG_DIR = f\"{int(time.time())}\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-15T19:41:11.773317Z","iopub.execute_input":"2021-06-15T19:41:11.773769Z","iopub.status.idle":"2021-06-15T19:41:11.785023Z","shell.execute_reply.started":"2021-06-15T19:41:11.773726Z","shell.execute_reply":"2021-06-15T19:41:11.784001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==========================================================\n# define functions\n# ==========================================================\ndef build_model(hp):\n    input_layer = Input(shape=(data_x_train_scaled.shape[1]), name='Input_Layer')\n    common_path = Dense(units=hp.Int(f\"dense_1_cp\", 16, 128, 16), activation='relu', name='first_dense')(input_layer)\n    common_path = Dropout(hp.Choice('drouput_rate_1', values=[0., 0.1, 0.2, 0.3]))(common_path)\n    common_path = Dense(units=hp.Int(f\"dense_2_cp\", 16, 128, 16), activation='relu', name='second_dense')(common_path)\n    common_path = Dropout(hp.Choice('drouput_rate_2', values=[0., 0.1, 0.2, 0.3]))(common_path)\n\n    # path 1\n    first_output = Dense(units='1', name='first_output_last_layer')(common_path)\n    \n    # for the number of hidden layers --> needs to be debugged cause sometimes I got nan as loss\n    # first_output_path = Dense(units=hp.Int(f\"dense_0_p1\", 16, 128, 16), activation='relu',\n    #                          name='first_output_dense0')(common_path)\n    # for i in range(hp.Int('n_layers_p1', 1, 4)):\n    #    first_output_path = Dense(units=hp.Int(f\"dense_{i+1}_p1_\", 16, 128, 16), activation='relu')(first_output_path)\n    # first_output = Dense(units='1', name='first_output_last_layer')(first_output_path)\n\n    # path 2\n    second_output_path = Dense(units=hp.Int(f\"dense_1_p2\", 16, 128, 16), activation='relu',\n                               name='second_output_dense0')(common_path)\n    second_output_path = Dropout(hp.Choice('drouput_rate_3', values=[0., 0.1, 0.2, 0.3]))(second_output_path)\n    second_output = Dense(units='1', name='second_output_last_layer')(second_output_path)\n    #  for the number of hidden layers --> needs to be debugged cause sometimes I got nan as loss\n    # second_output_path = Dense(units=hp.Int(f\"dense_0_p2\", 16, 128, 16), activation='relu',\n    #                           name='second_output_dense0')(common_path)\n    # for i in range(hp.Int('n_layers_p2', 1, 4)):\n    #     second_output_path = Dense(units=hp.Int(f\"dense_{i+1}_p2_\", 16, 128, 16), activation='relu')(second_output_path)\n    # second_output = Dense(units='1', name='second_output_last_layer')(second_output_path)\n\n    #\n    model = Model(inputs=input_layer, outputs=[first_output, second_output])\n    \n    model.compile(optimizer=tf.keras.optimizers.SGD(hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n                  loss={'first_output_last_layer': 'mse', 'second_output_last_layer': 'mse'},\n                  metrics={'first_output_last_layer': tf.keras.metrics.RootMeanSquaredError(),\n                           'second_output_last_layer': tf.keras.metrics.RootMeanSquaredError()})\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:49:39.918537Z","iopub.execute_input":"2021-06-15T19:49:39.918849Z","iopub.status.idle":"2021-06-15T19:49:39.930355Z","shell.execute_reply.started":"2021-06-15T19:49:39.91882Z","shell.execute_reply":"2021-06-15T19:49:39.929343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"y1 Heating Load\n\ny2 Cooling Load","metadata":{}},{"cell_type":"code","source":"# ==========================================================\n# data preparation\n# ==========================================================\ndata = pd.read_csv('../input/eergy-efficiency-dataset/ENB2012_data.csv')\ndata_x = data.iloc[:, :-2]\ndata_y = data.iloc[:, -2:]\n\ndel data\n\n#\ndata_x_train_scaled, data_x_test_scaled, data_y_train, data_y_test = \\\n    train_test_split(data_x, data_y, test_size=0.2, random_state=42)\n\n#\nsc = StandardScaler()\ndata_x_train_scaled = sc.fit_transform(data_x_train_scaled)\ndata_x_test_scaled = sc.transform(data_x_test_scaled)\n\n#\ndata_x_train_scaled, data_x_test_scaled, data_y_train, data_y_test = \\\n    np.array(data_x_train_scaled), np.array(data_x_test_scaled), np.array(data_y_train), \\\n    np.array(data_y_test)\n\n#\ndata_y_train = (data_y_train[:, 0], data_y_train[:, 1])\ndata_y_test = (data_y_test[:, 0], data_y_test[:, 1])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:41:16.385816Z","iopub.execute_input":"2021-06-15T19:41:16.386189Z","iopub.status.idle":"2021-06-15T19:41:16.407803Z","shell.execute_reply.started":"2021-06-15T19:41:16.386155Z","shell.execute_reply":"2021-06-15T19:41:16.407072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0.0001 and 200 epochs --> good results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==========================================================\n# data prediction\n# ==========================================================\ninput_layer = Input(shape=(data_x_train_scaled.shape[1]), name='Input_Layer')\ncommon_path = Dense(units='128', activation='relu', name='First_Dense')(input_layer)\ncommon_path = Dropout(0.3)(common_path)\ncommon_path = Dense(units='128', activation='relu', name='Second_Dense')(common_path)\ncommon_path = Dropout(0.3)(common_path)\n\nfirst_output = Dense(units='1', name='First_Output__Last_Layer')(common_path)\n\nsecond_output_path = Dense(units='64', activation='relu', name='Second_Output__First_Dense')(common_path)\nsecond_output_path = Dropout(0.3)(second_output_path)\nsecond_output = Dense(units='1', name='Second_Output__Last_Layer')(second_output_path)\n\n#\nmodel = Model(inputs=input_layer, outputs=[first_output, second_output])\nprint(model.summary())\n\n#\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.00001)\n\n#\nmodel.compile(optimizer=optimizer,\n              loss={'First_Output__Last_Layer': 'mse', 'Second_Output__Last_Layer': 'mse'},\n              metrics={'First_Output__Last_Layer': tf.keras.metrics.RootMeanSquaredError(),\n                       'Second_Output__Last_Layer': tf.keras.metrics.RootMeanSquaredError()})","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:49:07.96724Z","iopub.execute_input":"2021-06-15T19:49:07.96756Z","iopub.status.idle":"2021-06-15T19:49:08.040185Z","shell.execute_reply.started":"2021-06-15T19:49:07.967531Z","shell.execute_reply":"2021-06-15T19:49:08.03931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:41:22.56158Z","iopub.execute_input":"2021-06-15T19:41:22.562095Z","iopub.status.idle":"2021-06-15T19:41:22.697543Z","shell.execute_reply.started":"2021-06-15T19:41:22.562053Z","shell.execute_reply":"2021-06-15T19:41:22.696448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nearlyStopping_callback = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=10,\n                              verbose=1) \n\nhistory = model.fit(x=data_x_train_scaled, y=data_y_train, verbose=0, epochs=500, batch_size=10,\n                    validation_split=0.3, callbacks=earlyStopping_callback)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:49:10.799054Z","iopub.execute_input":"2021-06-15T19:49:10.799397Z","iopub.status.idle":"2021-06-15T19:49:32.911596Z","shell.execute_reply.started":"2021-06-15T19:49:10.799369Z","shell.execute_reply":"2021-06-15T19:49:32.910863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display training history\nprint(history.history.keys())","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:42:23.51349Z","iopub.execute_input":"2021-06-15T19:42:23.513806Z","iopub.status.idle":"2021-06-15T19:42:23.518731Z","shell.execute_reply.started":"2021-06-15T19:42:23.513777Z","shell.execute_reply":"2021-06-15T19:42:23.517732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for the first output rmse\nplt.plot(history.history['First_Output__Last_Layer_root_mean_squared_error'])\nplt.plot(history.history['val_First_Output__Last_Layer_root_mean_squared_error'])\nplt.title('model\\'s rmse for the first output')\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# summarize history for the second output accuracy\nplt.plot(history.history['Second_Output__Last_Layer_root_mean_squared_error'])\nplt.plot(history.history['val_Second_Output__Last_Layer_root_mean_squared_error'])\nplt.title('model\\'s rmse for the second output')\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:42:24.944455Z","iopub.execute_input":"2021-06-15T19:42:24.944763Z","iopub.status.idle":"2021-06-15T19:42:25.196608Z","shell.execute_reply.started":"2021-06-15T19:42:24.944736Z","shell.execute_reply":"2021-06-15T19:42:25.195742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for total loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Total Loss')\nplt.ylabel('total loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# summarize history for the first output loss\nplt.plot(history.history['First_Output__Last_Layer_loss'])\nplt.plot(history.history['val_First_Output__Last_Layer_loss'])\nplt.title('the first output Loss')\nplt.ylabel('y1 loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# summarize history for the second output loss\nplt.plot(history.history['Second_Output__Last_Layer_loss'])\nplt.plot(history.history['val_Second_Output__Last_Layer_loss'])\nplt.title('the second output Loss')\nplt.ylabel('y2 loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:42:27.367768Z","iopub.execute_input":"2021-06-15T19:42:27.368109Z","iopub.status.idle":"2021-06-15T19:42:27.764115Z","shell.execute_reply.started":"2021-06-15T19:42:27.36808Z","shell.execute_reply":"2021-06-15T19:42:27.763184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model\ntotal_loss, first_output_loss, second_output_loss, first_output_rmse, second_output_rmse = np.round(model.evaluate(\n    x=data_x_test_scaled, y=data_y_test, verbose=0), 3)\nprint(\n    \"Loss = {}, Y1_loss = {}, Y1_rmse = {}, Y2_loss = {}, Y2_rmse = {}\".format(total_loss, first_output_loss,\n                                                                             first_output_rmse, second_output_loss,\n                                                                             second_output_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:49:55.778662Z","iopub.execute_input":"2021-06-15T19:49:55.779269Z","iopub.status.idle":"2021-06-15T19:49:55.832677Z","shell.execute_reply.started":"2021-06-15T19:49:55.779219Z","shell.execute_reply":"2021-06-15T19:49:55.831663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOG_DIR = f\"{int(time.time())}\"","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:49:58.310119Z","iopub.execute_input":"2021-06-15T19:49:58.310432Z","iopub.status.idle":"2021-06-15T19:49:58.314256Z","shell.execute_reply.started":"2021-06-15T19:49:58.310404Z","shell.execute_reply":"2021-06-15T19:49:58.313357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==========================================================\n# Hyperparameter tuning using Keras Tuner\n# ==========================================================\ntuner = RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=12,\n    executions_per_trial=1,\n    directory=LOG_DIR\n)\n\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntuner.search(x=data_x_train_scaled,\n             y=data_y_train,\n             verbose=1,\n             epochs=500,\n             batch_size=10,\n             validation_split=0.3,\n             callbacks=[stop_early])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:58:13.74559Z","iopub.execute_input":"2021-06-15T19:58:13.745922Z","iopub.status.idle":"2021-06-15T20:00:06.034327Z","shell.execute_reply.started":"2021-06-15T19:58:13.745884Z","shell.execute_reply":"2021-06-15T20:00:06.033652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.results_summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:01:35.771377Z","iopub.execute_input":"2021-06-15T20:01:35.771718Z","iopub.status.idle":"2021-06-15T20:01:35.78656Z","shell.execute_reply.started":"2021-06-15T20:01:35.771687Z","shell.execute_reply":"2021-06-15T20:01:35.785744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The best result is as follow:\n    dense_1_cp: {best_hps.get('dense_1_cp')},\n    drouput_rate_1: {best_hps.get('drouput_rate_1')},\n    dense_2_cp: {best_hps.get('dense_2_cp')},\n    drouput_rate_2: {best_hps.get('drouput_rate_2')},\n    dense_1_p2: {best_hps.get('dense_1_p2')},\n    drouput_rate_3: {best_hps.get('drouput_rate_3')},\n    learning_rate: {best_hps.get('learning_rate')}\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:01:39.024408Z","iopub.execute_input":"2021-06-15T20:01:39.024721Z","iopub.status.idle":"2021-06-15T20:01:39.030323Z","shell.execute_reply.started":"2021-06-15T20:01:39.024692Z","shell.execute_reply":"2021-06-15T20:01:39.029409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\nmodel_tuned = tuner.hypermodel.build(best_hps)\nhistory = model_tuned.fit(x=data_x_train_scaled, y=data_y_train, verbose=0, epochs=500, batch_size=10,\n                    validation_split=0.3, callbacks=earlyStopping_callback)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:01:42.75713Z","iopub.execute_input":"2021-06-15T20:01:42.757469Z","iopub.status.idle":"2021-06-15T20:01:48.295069Z","shell.execute_reply.started":"2021-06-15T20:01:42.757442Z","shell.execute_reply":"2021-06-15T20:01:48.294078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model\ntotal_loss_tunned, first_output_loss_tunned, second_output_loss_tunned, first_output_rmse_tunned, second_output_rmse_tunned = np.round(model_tuned.evaluate(\n    x=data_x_test_scaled, y=data_y_test, verbose=0), 3)\nprint(\n    \"Loss = {}, Y1_loss = {}, Y1_rmse = {}, Y2_loss = {}, Y2_rmse = {}\".format(total_loss_tunned, first_output_loss_tunned,\n                                                                             first_output_rmse_tunned, second_output_loss_tunned,\n                                                                             second_output_rmse_tunned))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:02:07.129233Z","iopub.execute_input":"2021-06-15T20:02:07.129545Z","iopub.status.idle":"2021-06-15T20:02:07.183084Z","shell.execute_reply.started":"2021-06-15T20:02:07.129516Z","shell.execute_reply":"2021-06-15T20:02:07.181433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"y1 Heating Load\n\ny2 Cooling Load","metadata":{}},{"cell_type":"code","source":"print(f'Results Before Tunning:\\n Test Set RMSE: Heating Load:{np.round(first_output_rmse, 3)}, Cooling Load:{np.round(second_output_rmse, 3)}\\n')\nprint(f'Results After Tunning:\\n Test Set RMSE: Heating Load:{np.round(first_output_rmse_tunned, 3)}, Cooling Load:{np.round(second_output_rmse_tunned, 3)}\\n')\nprint(f'{np.round((first_output_rmse - first_output_rmse_tunned)*100/first_output_rmse)}% Improvement in predicting Heating Load')\nprint(f'{np.round((second_output_rmse - second_output_rmse_tunned)*100/second_output_rmse)}% Improvement in predicting Cooling Load')","metadata":{"execution":{"iopub.status.busy":"2021-06-15T20:02:09.232442Z","iopub.execute_input":"2021-06-15T20:02:09.232759Z","iopub.status.idle":"2021-06-15T20:02:09.239498Z","shell.execute_reply.started":"2021-06-15T20:02:09.23273Z","shell.execute_reply":"2021-06-15T20:02:09.238335Z"},"trusted":true},"execution_count":null,"outputs":[]}]}