{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"###########################################################################\n# Solution NoteBook for the Problem Keep babies save                      #\n###########################################################################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##################### Making Essential Imports ############################\nimport sklearn\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nimport cv2\nimport pytesseract\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nconf = r'-- oem 2'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####################################\n# Defining a skeleton for our       #\n# DataFrame                         #\n#####################################\n\nDataFrame = {\n    'photo_name' : [],\n    'flattenPhoto' : [],\n    'text' : [],\n\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################################################################################\n#      The Approach is to apply transfer learning hence using Resnet50 as my          #\n#      pretrained model                                                               #\n#######################################################################################\n\nMyModel = tf.keras.models.Sequential()\nMyModel.add(tf.keras.applications.ResNet101(\n    include_top = False, weights='imagenet',    pooling='avg',\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# freezing weights for 1st layer\nMyModel.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Now defining dataloading Function\ndef LoadDataAndDoEssentials(path, h, w):\n    img = cv2.imread(path)\n    DataFrame['text'].append(pytesseract.image_to_string(img, config = conf))\n    img = cv2.resize(img, (h, w))\n    ## Expanding image dims so this represents 1 sample\n    img = img = np.expand_dims(img, 0)\n    \n    img = tf.keras.applications.resnet50.preprocess_input(img)\n    extractedFeatures = MyModel.predict(img)\n    extractedFeatures = np.array(extractedFeatures)\n    DataFrame['flattenPhoto'].append(extractedFeatures.flatten())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### with this all done lets write the iterrrative loop\ndef ReadAndStoreMyImages(path):\n    list_ = os.listdir(path)\n\n    for mem in list_:\n        DataFrame['photo_name'].append(mem)\n        imagePath = path + '/' + mem\n        LoadDataAndDoEssentials(imagePath, 224, 224)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### lets give the address of our Parent directory and start\npath = '/kaggle/input/keep-babies-safe/dataset/images'\nReadAndStoreMyImages(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################\n#        lets now do clustering                      #\n######################################################\n\nTraining_Feature_vector = np.array(DataFrame['flattenPhoto'], dtype = 'float64')\nfrom sklearn.cluster import AgglomerativeClustering\nkmeans = AgglomerativeClustering(n_clusters = 2)\nkmeans.fit(Training_Feature_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = kmeans.labels_\nNamePred = []\nfor mem in predictions:\n    if mem == 0:\n        NamePred.append('toys')\n    else:\n        NamePred.append('consumer_products')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textAns = np.array(DataFrame['text'])\nrealText = []\nimport re\n\nfor mem in textAns:\n    newMem = re.sub(\"\\s\\s+\", \" \", mem)\n    if len(newMem) == 0 or newMem == \" \":\n        realText.append('Unnamed')\n        continue\n    else:\n        realText.append(str(newMem))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = DataFrame['photo_name']\ndf = {\n    'Image' : names,\n    'Class_of_image' : NamePred,\n    'Brand_name' : realText\n}\ndf = pd.DataFrame(df)\ndf.to_csv('predictions2.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nstdsc = StandardScaler()\npca = PCA( n_components= 2)\n\nTraining_Feature_vector = stdsc.fit_transform(Training_Feature_vector)\npca.fit(Training_Feature_vector)\nTraining_Feature_vector = pca.transform(Training_Feature_vector)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets make this a dataFrame\nimport seaborn as sb\nimport matplotlib.pyplot as plt \n\ndimReducedDataFrame = pd.DataFrame(Training_Feature_vector)\ndimReducedDataFrame = dimReducedDataFrame.rename(columns = { 0: 'V1', 1 : 'V2'})\ndimReducedDataFrame['Category'] = list (df['Class_of_image'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plotting this\nplt.figure(figsize = (10, 5))\nsb.scatterplot(data = dimReducedDataFrame, x = 'V1', y = 'V2',hue = 'Category')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}