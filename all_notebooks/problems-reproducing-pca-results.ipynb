{"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"source":"Hi, I would like to share with the community the simple PCA analysis I performed on this dataset. The results I have are different form what the author obtained (shown in his [blog post](http://rpubs.com/burakh/robobohr)) and I am wondering why. Can anyone help?","metadata":{"_uuid":"ce5628eafb9dc3c0a421d101274c7e7a48976de6","_cell_guid":"c1acba63-db9c-4852-b9f8-73dabc4a2e8f"},"cell_type":"markdown"},{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom scipy import stats\nfrom statsmodels.formula.api import ols\nimport seaborn\nimport sklearn\nfrom sklearn.decomposition import RandomizedPCA, PCA, SparsePCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import linalg as LA\n","outputs":[],"execution_count":null,"metadata":{"_uuid":"832d4738eb07fba01ee20e0e0e29239089414856","_cell_guid":"47d663b3-2b74-4862-8c64-de38de14e617"},"cell_type":"code"},{"source":"First we import the data, we drop \"pubchem_id\" and \"Eat\" columns from the dataframe. Then we normalize the data with StandardScaler.","metadata":{"_uuid":"e62d74241e31da61ae7324b9ca356377e1dfa09e","_cell_guid":"27695ff7-b20d-4b98-ac1d-c6e86cfe2af2"},"cell_type":"markdown"},{"source":"m = pd.read_csv('../input/roboBohr.csv')\nm.drop('pubchem_id', axis = 1)\nX = m.drop(['Eat', 'pubchem_id'],axis=1).values\nX = StandardScaler().fit_transform(X)\nY = m['Eat']","outputs":[],"execution_count":null,"metadata":{"_uuid":"ee59c6b5c25e00bace2ce32237eb8639a5b2b2a0","collapsed":true,"_cell_guid":"8e037d1e-613e-4be6-a243-e66bd87c3ad4"},"cell_type":"code"},{"source":"Now it is time to perform PCA with the sklearn library","metadata":{"_uuid":"7745ecd1f4298cbe477b3bc4ce4aeb7704c0480d","_cell_guid":"af63f829-d45c-4b3b-81b5-a6e975025eaa"},"cell_type":"markdown"},{"source":"pca = PCA()\nXt = pca.fit_transform(X)\npca_score = pca.explained_variance_ratio_\nV = pca.components_","outputs":[],"execution_count":null,"metadata":{"_uuid":"9fb8f2aba87e2f0f19e5e34e88bab45ccbe8976e","collapsed":true,"_cell_guid":"0cc85729-6193-49a0-a52d-b862385ecd16"},"cell_type":"code"},{"source":"We then plot the variance ratio, which looks ok. There are two principal components who explains nearly 80% of the variance. This is great! We can look at the PCA analysis on a simple 2D scatter plot!","metadata":{"_uuid":"fb849d9f9177be142292998d4a45c07363602372","_cell_guid":"ca7dfb24-c422-45aa-80af-0059c340b398"},"cell_type":"markdown"},{"source":"fig = plt.figure(figsize=(16, 6))\nax1 = fig.add_subplot(111)\nax1.set_ylim([0,1])\nlin1 = ax1.scatter(range(0, int(pca_score.shape[0])), pca_score, c = 'b', label = 'no random noise')\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_uuid":"8c857f619593e0036ea7615d1aae93db47c99eb8","_cell_guid":"85fd054a-d930-4f9d-a676-7adcc74fdcbf"},"cell_type":"code"},{"source":"So let's plot the two principal components againts one another. We also color the dots proportionnaly to their energy value.","metadata":{"_uuid":"886033db88d4187cf0667afcef0006c2a9e72334","_cell_guid":"ea9525b4-8c50-463a-a20b-9a74a30a67ce"},"cell_type":"markdown"},{"source":"fig = plt.figure(figsize=(16, 6))\nax2 = fig.add_subplot(111)\nax2.scatter(Xt[:,0], Xt[:, 1], c=Y)\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_uuid":"832a15d84d5046203624914e8e725705eef1357a","scrolled":true,"_cell_guid":"620fd0a1-2a4b-4d47-b760-876ea8d8aae6"},"cell_type":"code"},{"source":"The plot looks great but it looks nothing like the beautiful conchoid shape plot the author of this dataset presents in his [blog post](http://rpubs.com/burakh/robobohr). This is some what disappointing... Maybe something is wrong with the PCA package of sklearn? Let's try implementing PCA our selves with linear algebra tools.","metadata":{"_uuid":"42b1b85b4ef9ce5374cc4b41bb954ad77719c992","collapsed":true,"_cell_guid":"92c44e04-13f2-4684-97d7-7f9376f453ca"},"cell_type":"markdown"},{"source":"#Singular value decomposition of the covariance matrix\ncov = np.cov(X, rowvar = False)\nevals , evecs = LA.eigh(cov)\n\n#Sort the eigenvectors based on the eigenvalues\nidx = np.argsort(evals)[::-1]\nevecs = evecs[:,idx]\nevals = evals[idx]\n\n#Transform the data\nXt2 = np.dot(X, evecs)","outputs":[],"execution_count":null,"metadata":{"_uuid":"eb8470e49fc3971886f304ee684c6db1d80655e9","collapsed":true,"_cell_guid":"6f75d9b6-232e-4b0a-9c84-4cba5bcf6b99"},"cell_type":"code"},{"source":"fig = plt.figure(figsize=(16, 6))\nax2 = fig.add_subplot(111)\nax2.scatter(Xt2[:,0], Xt2[:, 1], c=Y)\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_uuid":"c476ed17a7f09e3c2201a323a295ae308612d533","_cell_guid":"51cbe5ac-b703-437f-8060-8642be442544"},"cell_type":"code"},{"source":"The plot looks similar but with an inversion of sign for one of the components. That's not the end of the world. \n\nWhat is more problematic is not being able to reproduce the results shown on the [blog post](http://rpubs.com/burakh/robobohr). \n\nAnyone has any idea what is going wrong? I am totally mystified here ... I do not feel confortable going further with the analysis before being sure every thing is allright with the dataset and all...","metadata":{"_uuid":"b1305e3c9b990ce29053a4835f78866047d1ec19","_cell_guid":"226dceaf-3589-404b-a735-9ecace47c310"},"cell_type":"markdown"},{"source":"# I tried dropping the first column of, but it didn't help much\nX = X[:,1:]\npca = PCA()\nXt = pca.fit_transform(X)\nfig = plt.figure(figsize=(16, 6))\nax2 = fig.add_subplot(111)\nax2.scatter(Xt[:,0], Xt[:, 1], c=Y)\nplt.show()","outputs":[],"execution_count":null,"metadata":{"_uuid":"2cc13a9bb7765f9e74c3d90735063b43fe19c01d","_cell_guid":"4098b6d6-2814-4837-903d-e1f582b48e5e"},"cell_type":"code"}],"nbformat":4}