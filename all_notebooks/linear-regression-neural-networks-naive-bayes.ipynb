{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n# !pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n# import torch\n# print (torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\nfrom keras.datasets import mnist #load our MNIST data\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n# from sklearn.metrics import confusion_matrix\n# plotting tool\nimport matplotlib.pyplot as plt\n\ncolumn_names = [\n    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', \n    'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', \n    'LSTAT', 'MEDV'\n]\n\nboston_data = pd.read_csv('../input/boston-house-prices/housing.csv', \n                          header=None, \n                          delimiter=r\"\\s+\", \n                          names=column_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Model"},{"metadata":{},"cell_type":"markdown","source":"First, let's build a linear model. Due to the large number of parameters relative to number of training examples, I will be using Ridge regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\n# Translating our data into arrays for processing.\nx = np.array(boston_data.drop(['MEDV'], axis=1))\ny = boston_data['MEDV'].values\n\n# Train/test split for validation.\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)\n\n# Our Model\nlr = Ridge(alpha=0.5)\nlr.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\nr2_score(lr.predict(x_test), y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Our Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"Now, let's build our neural networks. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\n\nbatch_size = 50\nnum_epochs = 250\nlearning_rate = 0.001\nhidden_size = 64\nbatch_no = len(x_train) // batch_size\ninput_dim = x.shape[1]\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Let's try no hidden layers at all first\nmodel_linear = nn.Sequential(\n    nn.Linear(input_dim, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, 1)\n)\n\n# Use a single hidden layer NN\nmodel = nn.Sequential(\n    nn.Linear(input_dim, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, 1)\n)\n\n# Use mean squared error loss.\nloss = nn.MSELoss(reduce='mean')\n\n# Use Adam to optimize our NN.\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_linear = model_linear.to(device)\nmodel = model.to(device)\n\nprint (\"Linear model first\")\nrunning_loss_lin = 0\n\nfor epoch in range(num_epochs):\n    for i in range(402):\n        start = i\n        end = start + 1\n        \n        x_batch = autograd.Variable(torch.FloatTensor(x_train[start:end]).to(device))\n        y_batch = autograd.Variable(torch.FloatTensor(y_train[start:end]).to(device))\n        \n        y_pred = model_linear(x_batch).to(device)\n     \n        loss_step = loss(y_pred, torch.unsqueeze(y_batch, dim=1))\n        \n        optimizer.zero_grad()\n        loss_step.backward()\n        optimizer.step()\n        running_loss_lin += loss_step.item()\n        \n   \n    print(\"Epoch {}, Loss: {}. Validation R2: {}\".format(\n        epoch + 1, running_loss_lin, \n        r2_score(model_linear(torch.Tensor(x_test).to(device)).detach().cpu().numpy(), y_test)))\n    running_loss_lin = 0.0\n\nprint (\"+++++++++++++++++++++++++++++++++++++\")    \nprint (\"Now for a single hidden layer network\")    \n    \nrunning_loss = 0\n\nfor epoch in range(num_epochs):\n    for i in range(402):\n        start = i\n        end = start + 1\n        \n        x_batch = autograd.Variable(torch.FloatTensor(x_train[start:end])).to(device)\n        y_batch = autograd.Variable(torch.FloatTensor(y_train[start:end])).to(device)\n            \n        y_pred = model(x_batch).to(device)\n        \n        loss_step = loss(y_pred, torch.unsqueeze(y_batch, dim=1))\n        optimizer.zero_grad()\n        loss_step.backward()\n        optimizer.step()\n        running_loss += loss_step.item()\n    \n    print(\"Epoch {}, Loss: {}. Validation R2: {}\".format(\n        epoch + 1, running_loss, \n        r2_score(model(torch.Tensor(x_test).to(device)).detach().cpu().numpy(), y_test)))\n    running_loss = 0.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n\n# only use 1s and 2s\ntrain_filter = np.where((y_train == 1 ) | (y_train == 2))\ntest_filter = np.where((y_test == 1) | (y_test == 2))\nX_train, y_train = X_train[train_filter], y_train[train_filter]\nX_test, y_test = X_test[test_filter], y_test[test_filter]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show 20 random images from the data set\nn_images = X_train.shape[0]\nn_rows=4\nn_cols=5\n\nfor i in range(1,n_rows*n_cols+1):\n    im_idx = random.randint(0,n_images-1)\n    pixels=X_train[im_idx]\n    plt.subplot(n_rows, n_cols, i)\n    plt.imshow(pixels, cmap='gray')\n    plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# flatten 28*28 images to a 784 vector for each image\nnum_pixels = X_train.shape[1] * X_train.shape[2]\n\nX_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# investigate the size of the feature matrices\nprint(X_train.shape)\nprint (y_train.shape)\nprint(X_test.shape)\n# inspect one example\nprint(X_train[1])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize the model with standard parameters\nclf_nb = MultinomialNB()\n# train the model\nclf_nb.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the Naive Bayes classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions with the NB classifier\ny_test_pred_nb = clf_nb.predict(X_test);\na_nb = accuracy_score(y_test, y_test_pred_nb);\nprint(a_nb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes Classifier in PyTorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport math\nimport torch\nimport torchvision\nfrom IPython import display\n\n\ndisplay.set_matplotlib_formats('svg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n    \"\"\"Plot a list of images.\"\"\"\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten()\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        if torch.is_tensor(img):\n            # Tensor Image\n            if img.device == \"cpu\":\n                ax.imshow(img.numpy())\n            else:\n                ax.imshow(img.cpu().numpy())\n        else:\n            # PIL Image\n            ax.imshow(img)\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transform = torchvision.transforms.Compose(\n    [torchvision.transforms.ToTensor()])\n\nmnist_train = torchvision.datasets.MNIST(\n    root='./temp', train=True, transform=data_transform, download=True)\nmnist_test = torchvision.datasets.MNIST(\n    root='./temp', train=False, transform=data_transform, download=True)\n\n\n# Selecting 1s and 2s only\nfor part in [mnist_train, mnist_test]:\n    idx = (part.targets==1) | (part.targets==2)\n    part.targets = part.targets[idx]\n    part.data = part.data[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image, label = mnist_train[2]\nprint(image.shape, label)\nprint(image.shape, image.dtype)\nprint(label, type(label))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = torch.stack([mnist_train[i][0] for i in range(10,38)], \n                     dim=1).squeeze(0)\nlabels = torch.tensor([mnist_train[i][1] for i in range(10,38)])\nimages.shape, labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_images(images, 2, 9);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = torch.stack([mnist_train[i][0] for i in range(len(mnist_train))], \n                dim=1).squeeze(0).to(device)\nY = torch.tensor([mnist_train[i][1] for i in range(len(mnist_train))]).to(device)\n\n# n_y = torch.zeros(10)\nn_y = torch.zeros(2)\n# for y in range(10):\nfor y in [1,2]:\n#     n_y[y] = (Y == y).sum()\n    n_y[y-1] = (Y == y).sum()\nP_y = n_y / n_y.sum()\nP_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print (Y.numpy())\n# print (X.numpy())\n# n_x = torch.zeros((10, 28, 28))\nn_x = torch.zeros((2, 28, 28))\n# for y in range(10):\nfor y in [1,2]:\n    n_x[y-1] = torch.tensor(X.cpu().numpy()[Y.cpu().numpy() == y].sum(axis=0))\n# print ((n_x + 1))\n# print ((n_y + 1).reshape(10, 1, 1))    \n# P_xy = (n_x + 1) / (n_y + 1).reshape(10, 1, 1)\nP_xy = (n_x + 1) / (n_y + 1).reshape(2, 1, 1)\n\n# show_images(P_xy, 2, 5);\nshow_images(P_xy, 1, 2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_P_xy = torch.log(P_xy).to(device)\nlog_P_xy_neg = torch.log(1 - P_xy).to(device)\nlog_P_y = torch.log(P_y).to(device)\n\n\ndef bayes_pred_stable(x):\n    x = x.unsqueeze(0)  # (28, 28) -> (1, 28, 28)\n    x = x.to(device)\n    p_xy = log_P_xy * x + log_P_xy_neg * (1 - x)\n#     p_xy = p_xy.reshape(10, -1).sum(axis=1)  # p(x|y)\n    p_xy = p_xy.reshape(2, -1).sum(axis=1)  # p(x|y)\n    return (p_xy + log_P_y).to(device)\n\npy = bayes_pred_stable(image)\npy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# py.argmax(dim=0) == label\npy.argmax(dim=0) == label-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def predict(X):\n#     return [bayes_pred_stable(x).argmax(dim=0).type(torch.int32).item() \n#             for x in X]\ndef predict(X):\n    return [bayes_pred_stable(x).argmax(dim=0).type(torch.int32).item()+1 \n            for x in X]\n\nX = torch.stack([mnist_train[i][0] for i in range(10,38)], dim=1).squeeze(0).to(device)\ny = torch.tensor([mnist_train[i][1] for i in range(10,38)]).to(device)\npreds = predict(X)\nshow_images(X, 2, 9, titles=[str(d) for d in preds]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = torch.stack([mnist_train[i][0] for i in range(len(mnist_test))], \n                dim=1).squeeze(0).to(device)\ny = torch.tensor([mnist_train[i][1] for i in range(len(mnist_test))]).to(device)\npreds = torch.tensor(predict(X), dtype=torch.int32).to(device)\nfloat((preds == y).sum()) / len(y)  # Validation accuracy","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}