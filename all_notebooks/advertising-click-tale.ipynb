{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/advertising.csv') # load the data","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"64780722fccc23ac5c31d1010c5ae80fe46fb18a","_cell_guid":"6e898114-533a-4a48-9377-d8bf785e874c","trusted":true},"cell_type":"code","source":"df.head()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"215c061ed5e9b1efbb514cd358f270fd83230dbb","collapsed":true,"_cell_guid":"f4b2cc54-90ef-48a0-b3e6-8bc2997b1145","trusted":true},"cell_type":"code","source":"# Featuretools is a framework to perform automated feature engineering. \n# It excels at transforming transactional and relational datasets into feature matrices for machine learning.\nimport featuretools as ft ","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"ed34e0bcc53e557aec8ff1b854c3e48f3be00943","_cell_guid":"221aff6b-6fe8-4e56-a2e7-c146acd46280","trusted":true},"cell_type":"code","source":"# Check if there is any missing values\ndf.isnull().sum()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"dcbcf81a4103e6a0af43f52e6f9a0cf61f789575","_cell_guid":"5bd2240e-f2c1-452f-8af0-11abeeab1a49","trusted":true},"cell_type":"code","source":"# Explore a bit about the count, mean, standard deviation, minimum and maximum values and the quantiles of the data\ndf.describe()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"ca90ca1fc89017dab7964b1ad6f4f6b661abd7f7","collapsed":true,"_cell_guid":"b2fd9035-b887-4798-8be2-8fe44c3816f7","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('classic')\n%matplotlib inline","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"9f84da51ba61eea1de85a4155643fb052dd81ecc","_cell_guid":"4d49b879-c56c-4e2e-a804-091200870285","trusted":true},"cell_type":"code","source":"df.info()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"1c21e7acfad50318799a9007ceca6ec34b3d2314","_cell_guid":"f2c1649e-104a-4148-87ec-dc49e0420f8d","trusted":true},"cell_type":"code","source":"len(df['City'].unique())","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"5c3c3554fae8e5a6c8ef3cde9dd0e884b2ed7a42","_cell_guid":"fb1a45cd-855a-4588-87f1-ee4069d3dba4","trusted":true},"cell_type":"code","source":"len(df['Country'].unique())","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"73f6271d95e364d6e2c1979536541a5e1c2b526f","_cell_guid":"94f1d472-fb74-4dd4-9268-b5208bf62f89"},"cell_type":"markdown","source":"There are 969 different cities in 1000 instances in the data, we need to consider the usefulness of this variable and if there is a need to use it. For country, there are 237 unique. "},{"metadata":{"_uuid":"a5bb5f43108fa372285c50d30d23b704480635b2","_cell_guid":"3a853f9b-5d0a-48b2-8ec9-d26df0a78e28","trusted":true},"cell_type":"code","source":"df['Clicked on Ad'].value_counts()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"e730a786c9220e0fd20e5bbe0e75be62e39e29b4","_cell_guid":"d36ab6e9-a7e6-4c4f-8de4-8c86be80e598","trusted":true},"cell_type":"code","source":"g0 = sns.countplot(x='Clicked on Ad', data = df, palette='husl')","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"2c744b2abaa2f3cf34a274983f9c864845e6e06e","_cell_guid":"09e6b92f-5953-4aae-b752-e6024dcf040e"},"cell_type":"markdown","source":"Looks like 50/50 click or no click, a perfect sythetic data set. "},{"metadata":{"_uuid":"75a60e727e1bee2ad9b846be130bbc1b5c9511ae","_cell_guid":"71c931e1-44d8-4a6e-b91a-13a3f0c9a811","trusted":true},"cell_type":"code","source":"df.groupby('Clicked on Ad').mean()","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"2f4d5e6f205de805d899db84581f71741310366d","_cell_guid":"d141d5c2-4ac5-4e7d-bb62-25e0f949d42c"},"cell_type":"markdown","source":"Observations: \n* The one who clicked spend a shorter amount of time on Site, is older in age (40), has lower incoem and shorter Daily Internet Usage.\n\nLet us explore more categorical means for other variables such as Country, Male or female"},{"metadata":{"_uuid":"98083452a66e60094826d90811b9982802debf0d","_cell_guid":"240e5174-9083-49af-8bd7-67890b7b8a68","trusted":true},"cell_type":"code","source":"df.groupby('Country').mean()","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"57a6c6c723167b5aa3b96fed0c2ae25bbd172129","_cell_guid":"0cb0c76d-7863-489e-9b80-ed2c20b64c0f","trusted":true},"cell_type":"code","source":"df.groupby('Country')['Clicked on Ad'].mean().nlargest(30)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"4737ad19b8075efaccc782430a38ae3259b37084","_cell_guid":"8b6934cf-78e2-4eea-ae89-eeab329a1350","trusted":true},"cell_type":"code","source":"df_1 = df.groupby('Country').mean()\ng1 = sns.pairplot(df_1, palette=\"husl\", kind=\"reg\")","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"4642a1344d7021718a30b2495a0cc817a32ce668","_cell_guid":"e694dbce-16de-4d42-8ee8-79db62d2deea"},"cell_type":"markdown","source":"We focus on if any of these numeric variables having linear relationship with the percentage of click rate in each country (the last row).\n* Higher Daily Time Spent on Site, lower % click. \n* Higher Age, higher % click\n* Higher the income, lower % click\n* Higher Daily Internet Usage,, lower % click. "},{"metadata":{"_uuid":"057403430251e7176bdbbb9e247457d9d85978b0","collapsed":true,"_cell_guid":"3f90ded5-004b-4368-96e4-75dc773f2f9e","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b72753dae389dd7f51c7966403a4c0b09200f22d","_cell_guid":"d4d0d9a5-58c8-45ab-bda5-3b3aa32b1fdb","trusted":true},"cell_type":"code","source":"g = sns.pairplot(df, hue=\"Clicked on Ad\", palette=\"husl\")","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"83181bc42bdc51d4218a60d9ce14caaea352c05c","_cell_guid":"6c55e8c1-e37f-4547-8efd-22fd8c67ff8f","trusted":true},"cell_type":"code","source":"# Convert Timestamp column as a datetime object\n# http://hamelg.blogspot.com/2015/11/python-for-data-analysis-part-17.html\ndf2 = df.copy()\ndf2['Timestamp'] = pd.to_datetime(df2[\"Timestamp\"] )\ndf2.info()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"64cfce6cadac5620517badee62eb2a9e1b130f82","_cell_guid":"f14303f7-becb-4ed6-8047-a3877b0d29c9","trusted":true},"cell_type":"code","source":"df2[\"month\"] = df2['Timestamp'].dt.month\ndf2[\"day\"] = df2['Timestamp'].dt.day\ndf2[\"dayofweek\"] = df2['Timestamp'].dt.dayofweek\ndf2[\"hour\"] = df2['Timestamp'].dt.hour\ndf2.head()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"b795356ec37e04e18ea0c8ea25a0d290f71d5645","_cell_guid":"da7577e7-433d-40ad-a089-cd6ffdd18971","trusted":true},"cell_type":"code","source":"g2 = sns.pairplot(df2[['Clicked on Ad', 'month', 'day', 'dayofweek', 'hour']], hue=\"Clicked on Ad\", palette=\"husl\")\n","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"2df813ea8415eb1edc50263118f79f2febe2b938","_cell_guid":"90c5ae99-251c-4201-a14a-58c5e15601d5"},"cell_type":"markdown","source":"Observations: By looking at the diagonal bar plots, we can see that there are no clear indication of month, day, day of week, hour showing effects on the click outcome. Overall, we can see no seasonality effetcs and this data set is probably created to be easier to analyze. We can bucketize some of these variables, such as hours."},{"metadata":{"_uuid":"e41779e041a7ca0bee41b2cd9409134a5ad4b1de","_cell_guid":"3924a3fb-b7c7-4dbf-b55c-215bb8ee5bdc"},"cell_type":"markdown","source":"## Based on the above observation, we can use logistic regression and random forest for this classification problem. \nLet us try create a base model using data with only numeric conlumns containing [\"Daily Time Spent on Site\", \"Age\", \"Area Income\", \"Daily Internet Usage\", \"Male\"]\n\n(1) logistic regression: \nLogistic Regression Assumptions\n* Binary logistic regression requires the dependent variable to be binary.\n* For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.\n* Only the meaningful variables should be included.\n* The independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\n* The independent variables are linearly related to the log odds.\n* Logistic regression requires quite large sample sizes.\n\n"},{"metadata":{"_uuid":"4c652f7a2ce8211a9f06c457b3e90d0cfdf6e42d","_cell_guid":"a33e45bc-45a7-478e-94a6-76dedfb06d45","trusted":true},"cell_type":"code","source":"df3 = df2.copy()\ndf3 = pd.concat([df3, pd.get_dummies(df3['Country'], prefix='Country')],axis=1)\ndf3 = pd.concat([df3, pd.get_dummies(df3['month'], prefix='Month')],axis=1)\ndf3 = pd.concat([df3, pd.get_dummies(df3['dayofweek'], prefix='Dayofweek')],axis=1)\n# create a bucket for hours into [0-5, 6-11, 12-17, 18-23] hour\ndf3['Hour_bin'] = pd.cut(df3['hour'], [0, 5, 11, 17, 23], labels=['hour_0-5', 'hour_6-11', 'hour_12-17', 'hour_18-23'], include_lowest=True)\ndf3 = pd.concat([df3, pd.get_dummies(df3['Hour_bin'], prefix='Hour')],axis=1)\ndf3.drop(['Country', 'Ad Topic Line', 'City', 'Timestamp', 'day', 'month', 'dayofweek', 'hour', 'Hour_bin'],axis=1, inplace=True)\ndf3.head(10)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"63922cc8e8d1344414be4d47d763113542ac567e","_cell_guid":"391fcb47-98c3-4bd3-aa6c-03109c1fa92c","trusted":true},"cell_type":"code","source":"# Check our final column variable names\ndf3.columns.values","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"51839ff3af8f4cb21d0d10e77706c001b5f9e319","_cell_guid":"cc354c67-7f5b-48a4-a7e3-bba0c3736666"},"cell_type":"markdown","source":"# Use all feature and implementing the LogisticRegression model"},{"metadata":{"_uuid":"5ba820403083b0ef160c9b29eb45fbe895dc837f","_cell_guid":"704f5b17-b2d4-4d8b-82c2-6bd0c348bffb","scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n# Select dependent variable and prediction outcome from the data\ndf3_final_vars = df3.columns.values.tolist()\ny = df3['Clicked on Ad']\nX_features = [i for i in df3_final_vars if i not in y]\nX = df3[X_features]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"86a0bfe83676b868726252df9e4ce4adbae05cbb","_cell_guid":"3add27ab-6463-4db4-9c80-2318cc65cf2e"},"cell_type":"markdown","source":"# Do a 10 fold cross-validation"},{"metadata":{"_uuid":"5ee88c653bf0590271647734532f608ed6465269","_cell_guid":"75d7a516-7102-4334-af14-4d22a7e105e6","trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nkfold = model_selection.KFold(n_splits=10, random_state=10)\nmodelCV = LogisticRegression()\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"958498f8da04ac25fa6b752f61e0c244353bad94","_cell_guid":"90e4c510-357c-428a-9d89-1f12483eb89d"},"cell_type":"markdown","source":"The average accuracy remains very close to the Logistic Regression model accuracy; we can conclude that our model generalizes well in the test set."},{"metadata":{"_uuid":"891a8d594d5c9e04647c7b66a0de2420e857bade","_cell_guid":"8f762143-6149-47ae-a92e-3badbf3d5f47"},"cell_type":"markdown","source":"# Look at different performance evaluation metrics in testing data set：confusion matrix, ROC, precision, recall, and f1-score"},{"metadata":{"_uuid":"b93423302e230bf1f6d3461940a23a6460521145","collapsed":true,"_cell_guid":"dd9824d5-b5d4-4ade-ba8f-0a0c45b645a9","trusted":true},"cell_type":"code","source":"# Looking at different performance evaluation metrics in testing data set：confusion matrix, ROC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score, f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom time import time\n\ndef evaluation(estimator, X_test, y_test):\n\n    start = time()\n    y_pred = estimator.predict(X_test)\n    print(\"Querying with the best model took %f seconds.\" % (time() - start))\n    print(len(y_pred))\n    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n    print(confmat)\n\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i,\n                    s=confmat[i, j],\n                    va='center', ha='center')\n\n    plt.xlabel('predicted label')\n    plt.ylabel('true label')\n    plt.show()\n\n    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n    print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n    print('ROC AUC: %.3f' % roc_auc_score(y_true=y_test, y_score=y_pred))\n    print('Accuracy: %.3f' % accuracy_score(y_true=y_test, y_pred=y_pred))\n    print('-----------------------------------------')\n    print(metrics.classification_report(y_test, y_pred))","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"f9477878bf02fa113aaf0d76a9f371e7e2198061","_cell_guid":"073631d5-a37b-4330-b676-926a3e8b774f","trusted":true},"cell_type":"code","source":"evaluation(logreg, X_test, y_test)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"9aec5a803e82adc970a57604e1a6b848a86642a4","_cell_guid":"776c14ee-1f3a-49f2-850c-bbb25371ff14"},"cell_type":"markdown","source":"According to Jean-Sebastien Provost's proposal, we assume that you have a marketing campaign for which you spend 1000 per potential customer. For each customer that you target with your ad campaign and that clicks on the ad, let's assume that you'll get an overall profit of 100 (earn back 1100 per correct target). However, if you target a customer that ends up not clicking on your ad, then you get a net loss of 1050. Let's calculate the performance of this model: profit = (all_positive x 1100) - (true_positive + false_positive)  x 1000 -  false_positive x 1050 (not making sense of this assumtion made ????? what if someone who is not tagerted end up with clicking the ad?)"},{"metadata":{"_uuid":"2d9f3907331dda70324f0e9bc5e373d2b278620f","_cell_guid":"be1ee32f-6dec-4714-a424-5f821799aefd","trusted":true},"cell_type":"code","source":"profit_of_campaign = 126 * 100 - 3 * 50\nprint('How much money is made: ${}'.format(profit_of_campaign))\nprint('How much money is made per customer: ${}'.format(profit_of_campaign/129))","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"2bc0f3f94a6844d42b914876cf061fec6154ac0e","_cell_guid":"5abc5b17-2894-4a04-96ef-10aef196ceb6"},"cell_type":"markdown","source":"# Using Sklearn's Pipeline function to combining features transformers and estimators to see if it can further improve performance \nIt consisted of two intermediate steps, a StandardScaler and a PCA transformer, and a LogisticRegression classifier as a final estimator."},{"metadata":{"_uuid":"172ccbc07deb5d5dd839089a49257cbfb60761a5","_cell_guid":"fe1fe8ea-739b-4da4-9e65-19a9e1b695aa","trusted":true},"cell_type":"code","source":"# Using Sklearn's Pipeline function to combining features transformers and estimators; \n# It consisted of two intermediate steps, a StandardScaler and a PCA transformer, and a LogisticRegression classifier as a final estimator.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# Check performance of LogisticRegression algorithm with feature thransformation of StandardScaler and a PCA transformer\npipe_1 = Pipeline([('scl', StandardScaler()), ('pca', PCA(n_components=2)), ('lr', LogisticRegression(random_state=2))])\npipe_1.fit(X_train, y_train)\nprint('LogisticRegression (with scaler/PCA) Test Accuracy: %.3f' % pipe_1.score(X_test, y_test))","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"a9de0f7d612abd8d9db7f293c1f9715017c3a6a4","_cell_guid":"a0fb24da-53e1-43c3-8fe9-00402d06cfc6"},"cell_type":"markdown","source":"We can see an improvment of test accuracy from 0.957 to 0.977. "},{"metadata":{"_uuid":"29212f52c7e96fa5b28cf4c649c96d7019bfb5d4","_cell_guid":"b72d182b-a40d-4058-b27e-1c5315230453","trusted":true},"cell_type":"code","source":"results_2 = model_selection.cross_val_score(pipe_1, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"10-fold cross validation average accuracy of pipe_1: %.3f\" % (results_2.mean()))","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"68d355b3bf9cdcd14e4d33118247bea57c763ea8","_cell_guid":"de4401f5-0c9a-44ce-813a-81fee0ac98d2"},"cell_type":"markdown","source":"10-fold cross validation average accuracy of pipe_1: 0.980, it performs better. "},{"metadata":{"_uuid":"d7064dcbf36c9719e797e2e2726a00ef8ea54061","_cell_guid":"10afd3a8-d9ef-4934-b889-972050b73b57","trusted":true},"cell_type":"code","source":"# Let's do an evaluation to check all performance metrics. \nevaluation(pipe_1, X_test, y_test)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"55c6d13454bf4442b579f893a21fadb3a55a8a2e","_cell_guid":"d522d88b-dc20-499b-9f55-91f3c4f4e162"},"cell_type":"markdown","source":"We can see the second model peform slightly better than the first one in metrics, see below for the increased profit for each customer."},{"metadata":{"trusted":true,"_uuid":"902d11899cf3694540afe73619366dbe179060d0"},"cell_type":"code","source":"profit_of_campaign_2 = 133 * 100 - 3 * 50\nprint('How much money is made: ${}'.format(profit_of_campaign_2))\nprint('How much money is made per customer: ${}'.format(profit_of_campaign_2/136))","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"a7603aa53163931083d7497caee001a9160e20c0","_cell_guid":"6f0e4746-36d3-4552-b89e-2cde4cc1a792"},"cell_type":"markdown","source":"# Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."},{"metadata":{"_uuid":"4366fb58d0716250c253974a3e78e1180543627d","_cell_guid":"0c852905-d5e1-454d-a3ee-092257d36e44","scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# Select dependent variable and prediction outcome from the data\ndf3_final_vars = df3.columns.values.tolist()\ny = ['Clicked on Ad']\nX = [i for i in df3_final_vars if i not in y]\n\nfeature_names = df3.columns.values\n# print(feature_names)\nlogreg = LogisticRegression()\nrfe = RFE(logreg)\nrfe = rfe.fit(df3[X], df3[y])\nprint(rfe.support_)\nprint(\"Features sorted by their rank:\")\nprint(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), feature_names)))","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"9f53e75bde71bc5c3bea28148abb1c7bd4138c8b","_cell_guid":"fa8ad876-f3dd-4cca-be88-42a790c9cbcf"},"cell_type":"markdown","source":"# Selecting the best ranked features"},{"metadata":{"_uuid":"fdb4938992057af7105a3a7e528b13e237400b0a","_cell_guid":"54a6be77-1c06-4493-bfa6-5c1a134622c4","trusted":true},"cell_type":"code","source":"mask = rfe.support_ #list of booleans\nnew_features = [] # The list of your K best features\nfor bool, feature in zip(mask, feature_names):\n    if bool:\n        new_features.append(feature)\nprint(new_features)\ndf3_final = df3[new_features]\n#print(df3_final)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"ee43d4b641dd0727fe303a368d01a796978da158","_cell_guid":"5aecea42-cef1-46b4-8913-f8b2a5080ff6","trusted":true},"cell_type":"code","source":"df3_final.head(10)","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"66a7f27c8570e11f53eced02724a3f45b4185530","_cell_guid":"c7bda011-cae3-4c17-aa6e-680b03de8e13"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8dc3e1d4e55b4c76d2791943635feda161077815","_cell_guid":"74107905-d8e2-48de-93b3-5d248badf656"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8e7eeecf1a37e452bbbf3deb7edbd7b87072a922","_cell_guid":"1aaca872-398a-4379-b715-23a1ceda8c78"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"cae9ae44fc7fc470ef1c5ebaea19e385fca92e4d","_cell_guid":"0c154c5f-cedf-41ca-a152-1482f0d069cb"},"cell_type":"markdown","source":""}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}