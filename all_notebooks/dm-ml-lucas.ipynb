{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom mlxtend.plotting import plot_pca_correlation_graph\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import lasso_path\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import mode\nfrom sklearn import metrics\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Charger les données \nopt = pd.read_csv('/kaggle/input/optdigit/optdigits.csv')\ndata_red = pd.read_csv('/kaggle/input/wine-quality-selection/winequality-red.csv',sep=',')\ndata_white = pd.read_csv('/kaggle/input/wine-quality-selection/winequality-white.csv',sep=',')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Afficher les histogrammes des grandeurs mesurées\n\nf, axes = plt.subplots(4,3,figsize = (15,15))\nplt.suptitle('Histogramme des variables mesurées')\n\nsns.distplot(data_red['fixed acidity'],color='skyblue' ,ax=axes[0,0])\nsns.distplot(data_red['volatile acidity'],color='olive',ax=axes[0,1])\nsns.distplot(data_red['citric acid'],color='gold',ax=axes[0,2])\nsns.distplot(data_red['residual sugar'],color='teal',ax=axes[1,0])\nsns.distplot(data_red['chlorides'],color='red',ax=axes[1,1])\nsns.distplot(data_red['free sulfur dioxide'],color='olive',ax=axes[1,2])\nsns.distplot(data_red['total sulfur dioxide'],color='skyblue',ax=axes[2,0])\nsns.distplot(data_red['density'],color='teal',ax=axes[2,1])\nsns.distplot(data_red['sulphates'],color='red',ax=axes[2,2])\nsns.distplot(data_red['pH'],color='skyblue',ax=axes[3,0])\nsns.distplot(data_red['alcohol'],ax=axes[3,1])\n\nplt.savefig('Histogramme.png')\n                      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Enlever les valeurs aberrantes \n#Calcul de la moyenne et de l'écart type des grandeurs mesurées\n\nmfa = data_red['fixed acidity'].mean()\nstd_fa = data_red['fixed acidity'].std()\n\nmva = data_red['volatile acidity'].mean()\nstd_va = data_red['volatile acidity'].std()\n\nmca = data_red['citric acid'].mean()\nstd_ca = data_red['citric acid'].std()\n\nmc = data_red['chlorides'].mean()\nstd_c = data_red['chlorides'].std()\n\nmfsd = data_red['free sulfur dioxide'].mean()\nstd_fsd = data_red['free sulfur dioxide'].std()\n\nmtsd = data_red['total sulfur dioxide'].mean()\nstd_tsd = data_red['total sulfur dioxide'].std()\n\nmd = data_red['density'].mean()\nstd_d = data_red['density'].std()\n\nms = data_red['sulphates'].mean()\nstd_s = data_red['sulphates'].std()\n\nmp = data_red['pH'].mean()\nstd_p = data_red['pH'].std()\n\nma = data_red['alcohol'].mean()\nstd_a = data_red['alcohol'].std()\n\nmrs = data_red['residual sugar'].mean()\nstd_rs = data_red['residual sugar'].std()\n\n# On ne garde que les vins non aberrants \noutliers_free = data_red.loc[(abs(data_red['fixed acidity']-mfa)<3*std_fa) & (abs(data_red['volatile acidity']-mva)<3*std_va) & (abs(data_red['citric acid']-mca)<3*std_ca) & (abs(data_red['chlorides']-mc)<3*std_c) & (abs(data_red['free sulfur dioxide']-mfsd)<3*std_fsd) & (abs(data_red['total sulfur dioxide']-mtsd)<3*std_tsd) & (abs(data_red['density']-md)<3*std_d) & (abs(data_red['sulphates']-ms)<3*std_s) & (abs(data_red['pH']-mp)<3*std_p) & (abs(data_red['alcohol']-ma)<3*std_a) & (abs(data_red['residual sugar']-mrs)<3*std_rs)]\nprint(outliers_free)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse en composant principal\n\n# On centre et réduit les données \ndata_scaled = preprocessing.scale(outliers_free, axis = 0)\n\n#PCA avec les 2 premiers axes\npca = PCA(n_components = 12)\n\n#ACP \npca.fit(data_scaled)\n\n\n#Pourcentage d'inertie portée par chaque axe\nplt.figure(figsize=(13,7))\nplt.subplot(1,2,1)\nvariance = pca.explained_variance_ratio_*100\nplt.bar(range(1,13),variance)\nplt.xlabel(\"rang de l'axe d'inertie\")\nplt.ylabel(\"pourcentage d'inertie\")\nplt.title(\"Pourcentage d'inertie des composantes\")\n\n#Projection sur les 2 axes des vins \n\nprojection = pca.transform(data_scaled)[:,0:2]\npairplot = pd.DataFrame(data = projection, columns=[\"F1\",\"F2\"])\npairplot['quality']= outliers_free['quality']\nsns.pairplot(x_vars='F1', y_vars='F2',data = pairplot,hue = 'quality', height = 8)\n\n#Deuxieme façon de faire pour la projection, avec la couleur bar \nplt.scatter(projection[:,0],projection[:,1],c=outliers_free['quality'])\nplt.colorbar()\nplt.savefig('ACP1.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cercle des corrélations \n\n#Nom des variables\nfeatures = data_red.columns.tolist()\n\n#Matrice des composants et leur coordonnées sur les variables \npcs = pca.components_\n\nfig, ax = plt.subplots(figsize=(10,10))\n\n#Affichage des flèches ( on veut afficher les variables sur les 2 premières composantes - les lignes sont les vecteurs propres et colonnes les variables initiales)\nplt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),pcs[0,:], pcs[1,:],width=0.0025,angles='xy', scale_units='xy', scale=1, color=\"black\")\n\n#Affichage du cercle \ncircle = plt.Circle((0,0), 1, facecolor='none', edgecolor='olive')\nfig = plt.gcf()\nax = fig.gca()\nax.add_artist(circle)\n\n#Les axes principaux sont affichés\nplt.plot([-1, 1], [0, 0], color='black',ls='--')\nplt.plot([0, 0], [-1, 1], color='black', ls='--')  \n\nfor i,(x, y) in enumerate(pcs[[0,1]].T):\n       plt.text(x,y, data_red.columns[i], fontsize='11', ha='center', color=\"black\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cercle des corrélations avec fonction de mlxtend\nfeatures_projection = pcs[:2,:]\nfeatures_projection\nplot_pca_correlation_graph(data_scaled,features)\n\nplt.savefig('cercle.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Regression multilineaire avec X réduit et centré\nX = data_scaled[:,0:11]\nY = data_scaled[:,11]\nlr = LinearRegression().fit(X,Y)\n\nprint(\"lr.coef_normalisé:{}\".format(lr.coef_))\nprint(\"lr.intercept_normalisé:{}\".format(lr.intercept_))\nprint(\"R2:{}\".format(round(lr.score(X,Y),2)))\n\n\n#Regression multilineaire avec X réduit\nmoyenne = np.ones((1458,1))\ndata_centre = np.array(outliers_free) - np.dot(moyenne,np.array([np.mean(outliers_free)]))\nXreduit = data_centre[:,0:11]\nYreduit = data_centre[:,11]\n\nlr = LinearRegression().fit(Xreduit,Yreduit)\n\nprint(\"lr.coef_centre:{}\".format(lr.coef_))\nprint(\"lr.intercept_centre:{}\".format(lr.intercept_))\nprint(\"R2:{}\".format(round(lr.score(Xreduit,Yreduit),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lasso regression \n#Séparation du jeu de données \n\ndatatrain,datatest = train_test_split(data_scaled,train_size=500)\n\nregLasso = Lasso(alpha = 0.02)\n\nXtrain = datatrain[:,0:11]\nYtrain = datatrain[:,11]\nXtest = datatest[:,0:11]\nYtest = datatest[:,11]\n\n#Apprentissage\nlasso = regLasso.fit(Xtrain,Ytrain)\nprint(\"lasso.coef_:{}\".format(lasso.coef_))\n\n\n#Alpha trop grand on a des coefficients tous nuls ! \n#Cross validation pour trouver le bon alpha \na=np.linspace(0.00001,1,1000)\nlassocv = LassoCV(alphas=a,max_iter = 5000, random_state = 10000, cv=10)\nlassocv.fit(Xtrain,Ytrain)\n\n       \n#Moyenne des MSE sur les CV \nmean_mse = np.mean(lassocv.mse_path_,axis=1)\nprint(pd.DataFrame({'alpha':lassocv.alphas_,'MSE':mean_mse}))\n\n#Visualisation des MSE en fonction de alpha \n#Peut etre fait plus rapidement avec la fonction validation_curve \nplt.plot(lassocv.alphas_,mean_mse)\nplt.show\nplt.xlabel('alpha')\nplt.ylabel('MSE')\n\nprint(\"R2:{}\" .format(round(lasso.score(Xtest,Ytest),2)))\n\nplt.savefig('MSE alpha.png')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Affichage des beta_j en fonction des valeurs prises par alpha\nbeta = np.zeros((1000,11))\nfor i in range(1000) :\n    for j in range(11) :\n        lasso_alpha = Lasso(a[i])\n        lasso_alpha_train = lasso_alpha.fit(Xtrain,Ytrain)\n        beta[i,j] = lasso_alpha_train.coef_[j]\n\nplt.plot(a,beta)\nplt.title('Valeurs des coefficients en fonction de alpha')\nplt.xlabel('alpha')\nplt.ylabel('Beta')\n\nplt.savefig('betaj.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Déterminer le meilleur alpha avec GridSearchCV\nparam_grid = {'alpha' : a}\ngrid = GridSearchCV(lasso,param_grid, cv = 10 )\ngrid.fit(Xtrain,Ytrain)\n\nprint(\"alpha_star :{}\" .format(grid.best_params_))\n\n\n#Prédiction avec le modèle de Ytest_modèle, à comparer avec Ytest, on prend alpha tel que MSE minimum\nYtest_modèle = regLasso.predict(Xtest)\n\nprint(\"MSE_prediction :{}\" .format(round(sklearn.metrics.mean_squared_error(Ytest_modèle,Ytest),2)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification \n#Ajout d'une colonne donnant la couleur du vin \n\ndata_red['Color']='red wine'\ndata_white['Color']='white wine'\n\n#Répartition des vins selon les grandeurs\ndata_concat=pd.concat([data_red,data_white],axis=0)\ndata_set=data_concat.drop(['quality'],axis=1)\nsns.pairplot(data_set, hue='Color')\n\nplt.savefig('pairplot.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification \n\n#Séparation des données en test et train \nTrain, Test = train_test_split(data_set,train_size=0.8)\ntarget_train = Train['Color']\ndata_train = Train.drop(['Color'],axis=1)\n\ntarget_test = Test['Color']\ndata_test = Test.drop(['Color'],axis=1)\n\nNBayes = GaussianNB()\nNBayes.fit(data_train,target_train)\nprediction_bayes = NBayes.predict(data_test)\nscore = sklearn.metrics.accuracy_score(prediction_bayes,target_test)\nprint(\"score_Nbayes :{}%\".format(round(score*100,1)))\n\n#Analyse du modèle sur données de test avec Naive Bayes \n             \nplt.figure(figsize=(12,12))\nplt.subplot(2,2,1)\nplt.title('Naive Bayes')\nMbayes = confusion_matrix(prediction_bayes,target_test)\nsns.heatmap(Mbayes, annot = True, xticklabels = ['red wine', 'white wine'] ,yticklabels = ['red wine', 'white wine'])\n\n\n#Avec les voisins \n\nknn = KNeighborsClassifier(20)\nknn.fit(data_train, target_train)\n\nprediction_knn = knn.predict(data_test)\nMknn = confusion_matrix(prediction_knn,target_test)\nplt.subplot(2,2,2)\nplt.title('K voisins')\nsns.heatmap(Mknn, annot = True, xticklabels = ['red wine', 'white wine'] ,yticklabels = ['red wine', 'white wine'])\nscore1 = sklearn.metrics.accuracy_score(prediction_knn,target_test)\nprint(\"score_Mknn :{} %\".format(round(score1*100,1)))\n\n#Avec la Random Forest\n\nrfs = RandomForestClassifier(10)\nrfs.fit(data_train, target_train)\n\nprediction_rfs = rfs.predict(data_test)\nMrfs = confusion_matrix(prediction_rfs,target_test)\nplt.subplot(2,2,3)\nplt.title('Random Forest ')\nsns.heatmap(Mrfs, annot = True, xticklabels = ['red wine', 'white wine'] ,yticklabels = ['red wine', 'white wine'])\nscore2 = sklearn.metrics.accuracy_score(prediction_rfs,target_test)\nprint(\"score_Mrfs :{} %\".format(round(score2*100,1)))\n\n\nplt.savefig('confusion matrix.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clustering\n#Centrer et reduire les données \nopt_scaled = preprocessing.scale(opt, axis = 1)\nopt_scaled = opt_scaled[:,0:64]\n#Kmeans process\nKm = KMeans(n_clusters = 10, n_init = 20)\nKm.fit(opt_scaled)\n\n#Km coordonnées\nlabel = Km.labels_\ncentroids = Km.cluster_centers_\nprint(centroids.shape)\nprint(\"score :{}\" .format(Km.score(opt_scaled)))\n\ninertia = []\nk = range(1,20) \nfor i in k :\n    model = KMeans(n_clusters = i)\n    model.fit(opt_scaled)\n    inertia.append(-model.score(opt_scaled))\n\nplt.plot(k,inertia)\nplt.xlabel('Nombres de clusters')\nplt.ylabel('Inertie')\n\nplt.savefig('clusters.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Chaque centroid est un nombre en 64 pixel\n\ncenters = centroids.reshape(10,8,8)\nplt.figure(figsize=(8,4))\nfor k in range(1,11) : \n    plt.subplot(2,5,k)\n    plt.imshow(centers[k-1,:,:])\n\nplt.savefig('chiffres.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Efficacité clustering \n\nclusters = Km.fit_predict(opt_scaled)\nlabels = opt['0.29']\npred_labels = np.zeros_like(clusters)\npred_labels10 = pred_labels\n\nfor i in range(10):\n    mask = ( clusters == i)\n    pred_labels[mask] = mode(labels[mask])[0]\n\nprint(\"Accuracy: %.2f\" % metrics.accuracy_score(labels, pred_labels))\n\n\nplt.figure(figsize=(10,7))\nM = confusion_matrix(labels,pred_labels)\nsns.heatmap(M, annot =True)\n\nplt.savefig('heatmapscaledigit.png')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ACP pour diminuer le la quantité d'information\n\nacp = PCA(n_components = 64)\nacp.fit(opt_scaled)\nplt.plot(np.cumsum(acp.explained_variance_ratio_))\nplt.title(\"Inertie cumulée en fonction du rang de l'axe principal\")\nplt.ylabel('Inertie cumulée')\nplt.xlabel(\"Rang de l'axe principal\")\n\ni = 0\nrang = 0\nwhile rang < 0.95:\n    rang = np.cumsum(acp.explained_variance_ratio_)[i]\n    i = i+1\n\nprint(\"Rang de l'axe principal tel que 95% de l'information est retenu : {}\" .format(i))\n\nplt.savefig('acpdigit.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}