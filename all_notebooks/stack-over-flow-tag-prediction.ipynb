{"cells":[{"metadata":{},"cell_type":"markdown","source":"### IMPORTING LIBRARIES"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### READING THE DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"question= pd.read_csv('../input/stacksample/Questions.csv', encoding='latin')\nanswer= pd.read_csv('../input/stacksample/Answers.csv', encoding='latin')\ntags= pd.read_csv('../input/stacksample/Tags.csv', encoding='latin')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(question.shape, answer.shape, tags.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(question.Id.nunique(), answer.ParentId.nunique(), tags.Id.nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Number of rows are different in the dataframes. \nThe ids have been repeated in the tags & answer dataframe, because of the discrepancy between the number of rows and the unique IDs."},{"metadata":{},"cell_type":"markdown","source":"### MERGING THE DATA FRAMES"},{"metadata":{"trusted":true},"cell_type":"code","source":"answer.drop(columns=['Id','OwnerUserId', 'CreationDate'],inplace=True)\nanswer.columns=['Id', 'A_Score', 'A_Body']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_answer = answer.groupby(\"Id\")['A_Body'].apply(lambda answer: ' '.join(answer))\ngrouped_answer= grouped_answer.to_frame()\ngrouped_answer= grouped_answer.sort_values(by='Id')\ngrouped_answer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags['Tag']= tags['Tag'].astype(str)\ngrouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n#grouped_tags = tags.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n\ngrouped_tags= grouped_tags.to_frame()\ngrouped_tags= grouped_tags.sort_values(by='Id')\ngrouped_tags.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grouped_answer.shape, grouped_tags.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Merging Question and grouped_answer dataframes to get df\n2. Merging df and grouped_answer dataframes to get df1"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_answer['Ids']= grouped_answer.index\ngrouped_tags['Ids']= grouped_tags.index\nquestion.columns= ['Ids', 'OwnerUserId', 'CreationDate', 'ClosedDate', 'Score', 'Title',\n       'Body']\nquestion= question.sort_values(by='Ids')\ndf= pd.merge(question,grouped_answer,how='left')\ndf1= pd.merge(df,grouped_tags,how='left',on='Ids')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### REMOVING UNNECESSARY VARIABLES"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.drop(columns=['Ids', 'OwnerUserId', 'CreationDate', 'ClosedDate'],inplace=True)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CHECKING FOR DUPLICATE ROWS"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df1.drop_duplicates()\ndf1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no duplicate rows because the number of rows after applying drop_duplicates function, remains the same. "},{"metadata":{},"cell_type":"markdown","source":"### FILTERING DATA BASED ON SCORE AND MOST FREQUENTLY USED TAGS"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df1.Score.min(), df1.Score.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z= df1['Tag'].value_counts().sort_values(ascending=False)\nz.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2= df1.groupby(by='Tag')['Tag'].count().sort_values(ascending=False).to_frame()\ndf2.columns= ['Tag_count']\ndf2['Tags']=df2.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.columns= ['Score', 'Title', 'Body', 'A_Body', 'Tags']\ndf1= pd.merge(df1,df2,how='left',on='Tags')\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1= df1[df1['Tag_count']>=1000]\ndf1= df1[df1['Score']>3]\ndf1.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For better prediction we will be using only those tags which have been repeated for atleast 1000 times and the score is more than 3. Low scores mean that the question is either erroneous or does not have sufficient information."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.Tags.value_counts().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CHECKING FOR MISSING VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df1.isnull().sum())\n\nprint('Shape of df1:',df1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A_Body is neither categorical nor continuous in nature. So it is not possible to impute its missing values. So we will remove it from our analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.drop(columns=['A_Body'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CLEANING THE TEXT FOR TITLE AND BODY"},{"metadata":{},"cell_type":"markdown","source":"1. Removing punctuation\n2. Removing HTML tags (if required)\n3. Changing text into lowercase\n4. Splitting the text into words\n5. Removing stopwords"},{"metadata":{},"cell_type":"markdown","source":"#### PUNCTUATION & HTML TAGS REMOVAL, LOWERCASE, WORD TOKENIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(text):\n    for punctuation in string.punctuation:\n        text= text.replace(punctuation,'')\n    return text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Title']= df1['Title'].astype(str)\n\ndf1['Title1']= df1['Title'].apply(remove_punctuation)\ndf1['Title1']=df1['Title1'].str.lower()\ndf1['Title1']= df1['Title1'].str.split()\ndf1['Title1'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Body']= df1['Body'].astype(str)\nimport re\n\ndf1['Body1']= df1['Body'].apply(lambda x: re.sub('<[^<]+?>','',x))\ndf1['Body1'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Body1']= df1['Body1'].apply(remove_punctuation)\ndf1['Body1']=df1['Body1'].str.lower()\ndf1['Body1']= df1['Body1'].str.split()\ndf1['Body1'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LEMMATIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlematizer= WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    lem_text=[lematizer.lemmatize(i) for i in text]\n    return lem_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Title1']= df1['Title1'].apply(lambda x: word_lemmatizer(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Body1']= df1['Body1'].apply(lambda x: word_lemmatizer(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### STOPWORD REMOVAL USING SPACY"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nsp= spacy.load('en_core_web_sm')\nall_stopwords= sp.Defaults.stop_words\ndf1['Title1']= df1['Title1'].apply(lambda x:[word for word in x if not word in all_stopwords])\ndf1['Title1'].head()                                   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nsp= spacy.load('en_core_web_sm')\nall_stopwords= sp.Defaults.stop_words\ndf1['Body1']= df1['Body1'].apply(lambda x:[word for word in x if not word in all_stopwords])\ndf1['Body1'].head()                                   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FINAL DATAFRAME AFTER TEXT CLEANING"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.drop(columns=['Title', 'Body', 'Tag_count','Score'], inplace=True)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF VECTORIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndf1['Title1']= df1['Title1'].astype(str)\nvectorizer = TfidfVectorizer()\nX1 = vectorizer.fit_transform(df1['Title1'].str.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['Body1']= df1['Body1'].astype(str)\nvectorizer = TfidfVectorizer()\nX2 = vectorizer.fit_transform(df1['Body1'].str.lower())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CHANGING CATEGORICAL VARIABLES INTO NUMERIC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle= LabelEncoder() \ndf1['Tags']= le.fit_transform(df1['Tags'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SPLITTING THE DATASET INTO TRAIN AND TEST SET"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df1['Tags'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X2, y, test_size=0.30, random_state=42)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### APPLYING DIFFERENT ALGORITHMS"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(C=10)\n\n# Creating the model on Training Data\nLOG=clf.fit(x_train,y_train)\nprediction=LOG.predict(x_test)\n\n# Printing the Overall Accuracy of the model\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=XGBClassifier(max_depth=2, learning_rate=0.2, n_estimators=400, objective='binary:logistic', booster='gbtree')\n\n# Creating the model on Training Data\nXGB=clf.fit(x_train,y_train)\nprediction=XGB.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MultinomialNB().fit(x_train,y_train)\nprediction= model.predict(x_test)\n\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=4)\n\n# Creating the model on Training Data\nKNN=clf.fit(x_train,y_train)\nprediction=KNN.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=4, n_estimators=600,criterion='entropy')\n\n# Creating the model on Training Data\nRF=clf.fit(x_train,y_train)\nprediction=RF.predict(x_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.classification_report(y_test, prediction).split()[-2]\nprint('Accuracy of the model:', F1_Score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Applied the ML algorithms on the TF-IDF vectorization of body because it is giving a higher accuracy as compared to the TF-IDF vectorization of the title.__\n\n__XGBoost is giving the highest accuracy out of all the algorithms that have been applied.__\n\n__F1 Score: 55%__"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}