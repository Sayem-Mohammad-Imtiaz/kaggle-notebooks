{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### XGBoost\nXGBoost improves the gradient boosting method even further.\n> **XGBoost** (*extreme gradient boosting*) regularises data better than normal gradient boosted Trees.\n\nIt was developed by Tianqi Chen in C++ but now has interfaces for Python, R, Julia.\n\nXGBoost's objective function is the sum of loss function evaluated over all the predictions and a regularisation function for all predictors ($j$ trees). In the formula $f_j$ means a prediction coming from the $j^th$ tree.\n\n$$\nobj(\\theta) = \\sum_{i}^{n} l(y_i - \\hat{y_i}) +  \\sum_{j=1}^{j} \\Omega (f_j)\n$$\n\nLoss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n\n$$\n\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n$$\n\nFirst part ($\\gamma T$) is responsible for controlling the overall number of created leaves, and the second term ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) watches over the scores.","metadata":{}},{"cell_type":"markdown","source":"**Mathematics Involved**\nUnlike the other tree-building algorithms, XGBoost doesnâ€™t use entropy or Gini indices. Instead, it utilises gradient (the error term) and hessian for creating the trees. Hessian for a Regression problem is the *number of residuals* and for a classification problem. Mathematically, Hessian is a second order derivative of the loss at the current estimate given as:\n\n<img src=\"hessian.PNG\" width=\"300\">\n\nwhere **L** is the loss function. \n\n- Initialise the tree with only one leaf.\n- compute the similarity using the formula\n$$\nSimilarity= \\frac {Gradient^2}{ hessian +\\lambda}\n$$\nWhere $\\lambda $ is the regularisation term.\n- Now for splitting data into a tree form, calculate\n$$\nGain=  left similarity+right similarity-similarity for root\n$$ \n- For tree pruning, the parameter $ \\gamma$ is used. The algorithm starts from the lowest level of the tree and then starts pruning based on the value of $\\gamma$.\n\n\n If $Gain- \\gamma < 0$, remove that branch. Else, keep the branch \n \n- Learning is done using the equation\n$$\nNew Value= old Value+ \\eta * prediction\n$$\n\nwhere $\\eta$ is the learning rate\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"## Import libraries\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport pickle\nfrom sklearn import datasets\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Problem Statement**:\nThe Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\nIt is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. Missing values are believed to be encoded with zero values. The variable names are as follows:\n1.\tNumber of times pregnant.\n2.\tPlasma glucose concentration 2 hours in an oral glucose tolerance test.\n3.\tDiastolic blood pressure (mm Hg).\n4.\tTriceps skinfold thickness (mm).\n5.\t2-Hour serum insulin (mu U/ml).\n6.\tBody mass index (weight in kg/(height in m)^2).\n7.\tDiabetes pedigree function.\n8.\tAge (years).\n9.\tIs Diabetic (0 or 1).","metadata":{}},{"cell_type":"code","source":"##Import data \ndata= pd.read_csv('../input/pima-dataset/pima-indians-diabetes.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:20:43.90035Z","iopub.execute_input":"2021-06-20T14:20:43.900736Z","iopub.status.idle":"2021-06-20T14:20:43.922842Z","shell.execute_reply.started":"2021-06-20T14:20:43.900705Z","shell.execute_reply":"2021-06-20T14:20:43.921718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the top 5 values from the table\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:20:46.910255Z","iopub.execute_input":"2021-06-20T14:20:46.910645Z","iopub.status.idle":"2021-06-20T14:20:46.937678Z","shell.execute_reply.started":"2021-06-20T14:20:46.910615Z","shell.execute_reply":"2021-06-20T14:20:46.936953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:20:52.400467Z","iopub.execute_input":"2021-06-20T14:20:52.401223Z","iopub.status.idle":"2021-06-20T14:20:52.40707Z","shell.execute_reply.started":"2021-06-20T14:20:52.401175Z","shell.execute_reply":"2021-06-20T14:20:52.406073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['Plasma glucose concentration',\n       'Diastolic blood pressure (mm Hg)', 'Triceps skinfold thickness (mm)',\n       '2-Hour serum insulin (mu U/ml)',\n       'Body mass index (weight in kg/(height in m)^2)',\n       'Diabetes pedigree function', 'Age']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:21:52.44414Z","iopub.execute_input":"2021-06-20T14:21:52.444827Z","iopub.status.idle":"2021-06-20T14:21:52.449581Z","shell.execute_reply.started":"2021-06-20T14:21:52.444783Z","shell.execute_reply":"2021-06-20T14:21:52.448513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The missing values have been replaced by zeroes. So, we are replacing zeroes with nan\nfor col in cols:\n    data[col]=data[col].replace(0, np.nan)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:22:09.700849Z","iopub.execute_input":"2021-06-20T14:22:09.701237Z","iopub.status.idle":"2021-06-20T14:22:09.715145Z","shell.execute_reply.started":"2021-06-20T14:22:09.701202Z","shell.execute_reply":"2021-06-20T14:22:09.714024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for missing values\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:22:26.126399Z","iopub.execute_input":"2021-06-20T14:22:26.126792Z","iopub.status.idle":"2021-06-20T14:22:26.136947Z","shell.execute_reply.started":"2021-06-20T14:22:26.126758Z","shell.execute_reply":"2021-06-20T14:22:26.13578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing the missing values\ndata['Plasma glucose concentration']=data['Plasma glucose concentration'].fillna(data['Plasma glucose concentration'].mode()[0])\ndata['Diastolic blood pressure (mm Hg)']=data['Diastolic blood pressure (mm Hg)'].fillna(data['Diastolic blood pressure (mm Hg)'].mode()[0])\ndata['Triceps skinfold thickness (mm)']=data['Triceps skinfold thickness (mm)'].fillna(data['Triceps skinfold thickness (mm)'].mean())\ndata['2-Hour serum insulin (mu U/ml)']=data['2-Hour serum insulin (mu U/ml)'].fillna(data['2-Hour serum insulin (mu U/ml)'].mean())\ndata['Body mass index (weight in kg/(height in m)^2)']=data['Body mass index (weight in kg/(height in m)^2)'].fillna(data['Body mass index (weight in kg/(height in m)^2)'].mean())\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:22:48.839292Z","iopub.execute_input":"2021-06-20T14:22:48.839736Z","iopub.status.idle":"2021-06-20T14:22:48.85264Z","shell.execute_reply.started":"2021-06-20T14:22:48.8397Z","shell.execute_reply":"2021-06-20T14:22:48.851261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for missing values after imputation\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:23:13.133294Z","iopub.execute_input":"2021-06-20T14:23:13.133668Z","iopub.status.idle":"2021-06-20T14:23:13.142935Z","shell.execute_reply.started":"2021-06-20T14:23:13.133638Z","shell.execute_reply":"2021-06-20T14:23:13.141673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separating the feature and the Label columns \nx=data.drop(labels='Is Diabetic', axis=1)\ny= data['Is Diabetic']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:23:36.034274Z","iopub.execute_input":"2021-06-20T14:23:36.034656Z","iopub.status.idle":"2021-06-20T14:23:36.041193Z","shell.execute_reply.started":"2021-06-20T14:23:36.034626Z","shell.execute_reply":"2021-06-20T14:23:36.040254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:23:51.791483Z","iopub.execute_input":"2021-06-20T14:23:51.792067Z","iopub.status.idle":"2021-06-20T14:23:51.810586Z","shell.execute_reply.started":"2021-06-20T14:23:51.792012Z","shell.execute_reply":"2021-06-20T14:23:51.80975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# as the datapoints differ a lot in magnitude, we'll scale them\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaled_data=scaler.fit_transform(x)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:24:26.104648Z","iopub.execute_input":"2021-06-20T14:24:26.105162Z","iopub.status.idle":"2021-06-20T14:24:26.115028Z","shell.execute_reply.started":"2021-06-20T14:24:26.105107Z","shell.execute_reply":"2021-06-20T14:24:26.114093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y=train_test_split(scaled_data,y,test_size=0.3,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:24:52.568821Z","iopub.execute_input":"2021-06-20T14:24:52.569236Z","iopub.status.idle":"2021-06-20T14:24:52.577584Z","shell.execute_reply.started":"2021-06-20T14:24:52.569202Z","shell.execute_reply":"2021-06-20T14:24:52.576385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit model on training data\nmodel = XGBClassifier(objective='binary:logistic')\nmodel.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:25:17.311357Z","iopub.execute_input":"2021-06-20T14:25:17.311725Z","iopub.status.idle":"2021-06-20T14:25:17.452314Z","shell.execute_reply.started":"2021-06-20T14:25:17.31169Z","shell.execute_reply":"2021-06-20T14:25:17.451595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cheking training accuracy\ny_pred = model.predict(train_x)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(train_y,predictions)\naccuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:26:39.360666Z","iopub.execute_input":"2021-06-20T14:26:39.361195Z","iopub.status.idle":"2021-06-20T14:26:39.374772Z","shell.execute_reply.started":"2021-06-20T14:26:39.361136Z","shell.execute_reply":"2021-06-20T14:26:39.373881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cheking initial test accuracy\ny_pred = model.predict(test_x)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(test_y,predictions)\naccuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:26:59.845477Z","iopub.execute_input":"2021-06-20T14:26:59.846076Z","iopub.status.idle":"2021-06-20T14:26:59.857679Z","shell.execute_reply.started":"2021-06-20T14:26:59.846021Z","shell.execute_reply":"2021-06-20T14:26:59.856781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now to increae the accuracy of the model, we'll do hyperparameter tuning using **grid search CV**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:28:27.7856Z","iopub.execute_input":"2021-06-20T14:28:27.786143Z","iopub.status.idle":"2021-06-20T14:28:27.790776Z","shell.execute_reply.started":"2021-06-20T14:28:27.786109Z","shell.execute_reply":"2021-06-20T14:28:27.789678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid={\n   \n    'learning_rate':[1,0.5,0.1,0.01,0.001],\n    'max_depth': [3,5,10,20],\n    'n_estimators':[10,50,100,200],\n    'reg_lambda': [0,1,2]\n    \n}","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:36:17.480314Z","iopub.execute_input":"2021-06-20T14:36:17.48069Z","iopub.status.idle":"2021-06-20T14:36:17.485817Z","shell.execute_reply.started":"2021-06-20T14:36:17.480661Z","shell.execute_reply":"2021-06-20T14:36:17.484828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid= GridSearchCV(XGBClassifier(objective='binary:logistic'),param_grid, verbose=3)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:37:13.535264Z","iopub.execute_input":"2021-06-20T14:37:13.535847Z","iopub.status.idle":"2021-06-20T14:37:13.540972Z","shell.execute_reply.started":"2021-06-20T14:37:13.535806Z","shell.execute_reply":"2021-06-20T14:37:13.539852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid.fit(train_x,train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:37:26.543635Z","iopub.execute_input":"2021-06-20T14:37:26.544107Z","iopub.status.idle":"2021-06-20T14:38:48.929563Z","shell.execute_reply.started":"2021-06-20T14:37:26.544076Z","shell.execute_reply":"2021-06-20T14:38:48.928454Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To  find the parameters givingmaximum accuracy\ngrid.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:42:57.473299Z","iopub.execute_input":"2021-06-20T14:42:57.473727Z","iopub.status.idle":"2021-06-20T14:42:57.480822Z","shell.execute_reply.started":"2021-06-20T14:42:57.473692Z","shell.execute_reply":"2021-06-20T14:42:57.479593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new model using the same parameters\nnew_model=XGBClassifier(learning_rate= 0.01, max_depth= 5, n_estimators= 200, reg_lambda =2)\nnew_model.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:44:27.8622Z","iopub.execute_input":"2021-06-20T14:44:27.862578Z","iopub.status.idle":"2021-06-20T14:44:28.025048Z","shell.execute_reply.started":"2021-06-20T14:44:27.862545Z","shell.execute_reply":"2021-06-20T14:44:28.024087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_new = new_model.predict(test_x)\npredictions_new = [round(value) for value in y_pred_new]\naccuracy_new = accuracy_score(test_y,predictions_new)\naccuracy_new","metadata":{"execution":{"iopub.status.busy":"2021-06-20T14:44:33.030193Z","iopub.execute_input":"2021-06-20T14:44:33.030551Z","iopub.status.idle":"2021-06-20T14:44:33.048053Z","shell.execute_reply.started":"2021-06-20T14:44:33.030522Z","shell.execute_reply":"2021-06-20T14:44:33.046885Z"},"trusted":true},"execution_count":null,"outputs":[]}]}