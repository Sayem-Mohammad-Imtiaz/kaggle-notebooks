{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom tensorflow.contrib.learn import preprocessing\nfrom keras.callbacks import ModelCheckpoint\nimport re\nfrom pickle import dump\nimport tensorflow as tf\nimport os\nfrom sklearn.utils import shuffle\n\n\nprint(os.listdir(\"../input\"))\n\nprint(tf.test.gpu_device_name())\n# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# PREPROCESSING\n\ntrain_df = pd.read_csv('../input/Medium_AggregatedData.csv', skipinitialspace=True, usecols = ['text', 'totalClapCount', 'language']).rename(columns=lambda x: x.strip())\ntrain_df = train_df.drop_duplicates()\ntrain_df = shuffle(train_df)\n\ncolumns = [\"text\", \"totalClapCount\", \"language\"]\ntrain_df = train_df.reindex(columns=columns)\n\ntrain_df = train_df[train_df['totalClapCount'] > 1000][train_df['language'] == 'en'][\"text\"].apply(lambda x: x.lower().replace('\\n', ' ').replace(',', '').replace('.', ''))\n\nprint(\"PREPROCESSING complete with quantity {}\".format(len(train_df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[:5]\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TOKENIZATION\n\nmax_words = 1250 #1250 # 1000 # 5000 # Max size of the dictionary\ntok = keras.preprocessing.text.Tokenizer(num_words=max_words)\ntok.fit_on_texts(train_df.values)\nsequences = tok.texts_to_sequences(train_df.values)\n#sequences = [ np.int16(x) for x in sequences ]\nprint(sequences[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Flatten sequence list.\ntext = [item for sublist in sequences for item in sublist]\nlen(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_len = 14 #7 #20\npred_len = 1\ntrain_len = sentence_len - pred_len\nseq = []\n# Sliding window to generate test and train data\nfor i in range(len(text)-sentence_len):\n    seq.append(text[i:i+sentence_len])\n# Reverse dictionary so as to decode tokenized sequences back to words and sentences\nreverse_word_map = dict(map(reversed, tok.word_index.items()))\ndump(tok, open('tokenizer.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX = []\ntrainy = []\nfor i in seq:\n    trainX.append(i[:train_len])\n    trainy.append(i[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential()\nmodel.add(keras.layers.Embedding(max_words,100,input_length=train_len)) #100\nmodel.add(keras.layers.LSTM(200, dropout=0.6, recurrent_dropout=0.2)) #128 #256\nmodel.add(keras.layers.Dense(300,activation=\"relu\")) #128 #1024\nmodel.add(keras.layers.Dense(max_words-1,activation=\"softmax\")) #4999\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#filepath = \"./weight_tr5.hdf5\"\ncheckpoint_path = \"training/cp-{epoch:04d}.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\nif os.path.isdir(checkpoint_dir):\n  print(\"Reloading checkpointed model from {}\".format(checkpoint_dir))\n  latest = tf.train.latest_checkpoint(checkpoint_dir)\n  print(latest)\n  model.load_weights(latest)\n  #model.load_weights(filepath)\n  #print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(checkpoint_dir))\nprint(os.listdir(\"training/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\ncheckpoint = ModelCheckpoint(checkpoint_path, #filepath, \n                             monitor='loss', \n                             verbose=1,  \n                             save_weights_only=True,\n                             #save_best_only=True, \n                             period=1, # Save weights, every 5-epochs.\n                             mode='min') \ncallbacks_list = [checkpoint]\n    \n# Fit model using the gpu.\nwith tf.device('/gpu:0'):\n  history = model.fit(np.asarray(trainX).astype(np.int16), #(np.int8),\n           np.asarray(pd.get_dummies(np.asarray(trainy),sparse=True)).astype(np.int16), #(np.int8),\n           epochs = 500,\n           batch_size = 2000, #200, #10240,\n           callbacks = callbacks_list,\n           verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen(seq,max_len = 7): #19\n    sent = tok.texts_to_sequences([seq])\n    #print(sent)\n    while len(sent[0]) < max_len:\n        sent2 = keras.preprocessing.sequence.pad_sequences(sent[-(sentence_len-1):],maxlen=(sentence_len-1))\n        op = model.predict(np.asarray(sent2).reshape(1,-1))\n        sent[0].append(op.argmax()+1)\n    return \" \".join(map(lambda x : reverse_word_map[x],sent[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = [(\"this generation is remarkably \",20),(\"seriously. every single time \",32),\n         (\"he couldn't stand such \",24),(\"and in that moment, he felt \",20),\n        (\"the last day they could ever enjoy \",50),(\"\",600)]\n\nfor i in range(len(start)):\n    print(\"<<-- Sentence %d -->>\\n\"%(i), gen(start[i][0],start[i][1]))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}