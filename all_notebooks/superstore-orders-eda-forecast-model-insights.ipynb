{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Given Datasets:\n#### 1. Sample - Superstore_Orders.csv - Superstore orders between Jan 2017 to Dec 2020.\n#### 2. Sales Target (US)_Full Data.csv - Sales Target Data by Product Category, Segment between Jan 2017 to Dec 2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport math\nfrom IPython.display import Image\n\npd.set_option('max_columns', None)\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders = pd.read_csv('/kaggle/input/superstore-orders-sales-target-data-2017-to-2020/Sample - Superstore_Orders.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"orders.info() \norders.shape ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for duplicates."},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observing the dataset Order ID and Product ID are the primary keys\n\norders[['Order ID','Product ID']].drop_duplicates().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders[['Order ID','Product ID']].duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"orders.loc[orders[['Order ID','Product ID']].duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.loc[(orders['Order ID'] == 'CA-2019-129714') & (orders['Product ID'] == 'OFF-PA-10001970')]\n\n# This is due to error in data collection or reporting - we could delete, manipulate or proceed as it is.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data screening & cleaning steps for orders data:\n\n#### 1. Postal Code is having missing values and should be converted to string.\n#### 2. Remove special characters from Sales, Discount, Profit, Sales Forecast columns and convert data type to float64.\n#### 3. Ship Date & Order Date to be converted to datetime and rearrange the dataset by Order Date.\n#### 4. Keep the latest entry in the duplicates, remove other duplicates (Assumption : Correction is allowed after customer has ordered).\n#### 5. Rearrange columns in a meaningful order and drop Row ID column."},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.loc[(orders['Postal Code'].isnull()),'Postal Code'] = '05401'\norders['Postal Code'] = orders['Postal Code'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"orders['Sales'] = orders['Sales'].apply(lambda x: x.replace('$','').replace(',','')).astype('float64')\norders['Sales Forecast'] = orders['Sales Forecast'].apply(lambda x: x.replace('$','').replace(',','')).astype('float64')\norders['Discount'] = orders['Discount'].apply(lambda x: x.strip('%')).astype('float64')\norders['Profit'] = orders['Profit'].apply(lambda x: x.replace('(','-').replace(')', '').replace('$','').replace(',','')).astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders['Order Date'] = pd.to_datetime(orders['Order Date'])\norders['Ship Date'] = pd.to_datetime(orders['Ship Date'])\norders = orders.sort_values(by = 'Order Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.drop_duplicates(subset = ['Order ID','Product ID'], keep = 'last', inplace = True)\n# asssumption made - keep last duplicated rows as correction is allowed after the customer has ordered. \n# pandas series version doesn't take 'ignore_index' parameter in 'drop_duplicates', and so we have to reset index\norders = orders.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rearranged_col = ['Order ID','Order Date','Ship Date','Ship Mode','Days to Ship Actual','Days to Ship Scheduled','Ship Status','Customer ID','Customer Name','Segment','Country/Region','City','State','Postal Code','Region','Product ID','Category','Sub-Category','Product Name','Sales','Quantity','Discount','Profit','Sales Forecast']\n# column 'Row ID' is dropped.\norders_data = orders.reindex(columns = rearranged_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Importing Sales Target Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target = pd.read_csv('/kaggle/input/superstore-orders-sales-target-data-2017-to-2020/Sales Target (US)_Full Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.info()\nsales_target.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# observing the dataset Category, Order Date and Segment are the primary keys\n\nsales_target[['Category','Order Date','Segment']].drop_duplicates().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target[['Category','Order Date','Segment']].duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.loc[sales_target[['Category','Order Date','Segment']].duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.loc[(sales_target['Category'] == 'Furniture') & (sales_target['Order Date'] == '3/1/2018') & (sales_target['Segment'] == 'Corporate')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data screening & cleaning steps for sales target data:\n\n#### 1. Order Date to be converted to datetime and rearrange dataset by Order Date.\n#### 2. Keep the latest entry in the duplicates, remove other duplicates (Assumption : Correction is allowed in Sales Target Data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target['Order Date'] = pd.to_datetime(sales_target['Order Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.drop_duplicates(subset = ['Category','Order Date','Segment'], keep = 'last', inplace = True)\n# asssumption made - keep last duplicated rows as correction is allowed in Sales Target Data. \n# pandas series version doesn't take 'ignore_index' parameter in 'drop_duplicates', and so we have to reset index\nsales_target = sales_target.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orders_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing the final data."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data = pd.merge(orders_data,sales_target,how='left',on=['Category','Order Date','Segment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data.info()\nfinal_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Additional information we could pull out from the dataset:\n\n#### 1. Price of Product\n#### 2. Profit Ratio\n#### 3. Sales Target status\n#### 4. Forecast bias\n#### 5. Sales Forecast status"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data['Price'] = round(final_data['Sales']/final_data['Quantity'],2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data['Profit Ratio'] = round(final_data['Profit']*100/final_data['Sales'],2)\nfinal_data.loc[(final_data['Sales'] == 0),'Profit Ratio'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data.loc[((final_data['Sales Target'] - final_data['Sales'])<=0),'Sales Target Status'] = 'Target Achieved'\nfinal_data.loc[((final_data['Sales Target'] - final_data['Sales'])>0),'Sales Target Status'] = 'Target Not Achieved'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data['forecast_bias'] = final_data['Sales Forecast'] - final_data['Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data.loc[(final_data['forecast_bias']>0),'Sales Forecast Status'] = 'Over Forecast'\nfinal_data.loc[(final_data['forecast_bias']== 0),'Sales Forecast Status'] = 'Accurate Forecast'\nfinal_data.loc[(final_data['forecast_bias']<0),'Sales Forecast Status'] = 'Under Forecast'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#final_data.to_csv('final_data.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Descriptive statistics of the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Insights for categorical variables:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_var = ['Ship Mode','Ship Status','Customer ID','Customer Name','Segment','Country/Region','City','State','Postal Code','Region','Product ID','Category','Sub-Category','Product Name','Sales Target Status','Sales Forecast Status']\n\nuniq_cat = pd.DataFrame()\nuniq_cat['variables'] = cat_var\n\nfor x in cat_var:\n    uniq_cat.loc[(uniq_cat['variables'] == x),'no. of unique values'] = final_data[x].nunique()\n\nuniq_cat['no. of unique values'] = uniq_cat['no. of unique values'].astype(int)\n    \nnew_cols = uniq_cat.loc[(uniq_cat['no. of unique values'] < 10),'variables'].to_list()\n\nuniq_cat.loc[(uniq_cat['no. of unique values'] > 10),'unique values'] = '-'\n\nfor x in new_cols:\n    uniq_cat.loc[(uniq_cat['variables'] == x),'unique values'] = str(final_data[x].unique())\n\nuniq_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 6))\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(final_data.corr(), dtype=np.bool))\nheatmap = sns.heatmap(final_data.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize':18}, pad=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Insights from correlation matrix:\n\n - Looking at the table, we see that Discount and Profit Ratio have the highest negative correlation, and we can deduce that high discounts mean less profit.\n - The correlation coefficient for Profit and Selling Price is high. As Selling Price increases, Profit also is higher.\n - Sales, Sales Forecast, Price, forecast_bias are obviously positively correlated and are inter related to each other.\n - Sales Target & Profit are positively correlated with Sales, implies we can obtain high profit with high sales amount and obviously our sales target would also increase."},{"metadata":{},"cell_type":"markdown","source":"### Insights:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/sales_univariate_quarter.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- At quarterly aggregate level - the forecast model always over forecasts and the sales target is always greater than the actual sales.\n\n- Sales are always high in Q4 of every year and it decreases significantly in Q1. One of the reason being Q4 as a holiday season and so are the sales high.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/yearly_sales.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- In every year, sales are having a dip in October and the reason should be investigated.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Product_Category_Stats.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Sales are highest for Chairs, Storage and Phones in their respective Product Categories.\n- Profit is highest for Chairs, Paper and Copiers in their respective Product Categories.\n- Tables & Supplies are having lowest profit and could be due to high shipping costs."},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Regional_Sales.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- California has the highest sales and there is an opportunity to open another store. North Dakota has least number of sales.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Regional_Profit.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- California & New Jersey has the highest profit amount.Texas & Illinois has the least profit amount. There is an opportunity to open another store in New Jersey as well.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Regional_Profit_Ratio.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- District of Columbia & Iowa have the highest profit ratio's across all the states. Illinois & Texas has the lowest profit ratio's.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Regional_Discount.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Highest Discounts are recorded in Illinois & Texas. This could be the reason for having least profits from these two states. If we could reduce discounts in these two states, we could increase the profits.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/shipment_status.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- In all of the years, the orders which are shipped early has the highest number of sales and profit.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Sales_Target_Stats.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Sales belonging to Office Supplies, Furniture, Technology, Office Supplies haven't achieved the target in 2017, 2018, 2019 & 2020 respectively.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Ship_Mode_Stats.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- In the case of Office Supplies and Tables, profit is really low when these items are shipped in standard class. We could achieve profit if we shift the demand for these two product categories from standard class to the remaining ship modes. Organization could remove the standard class ship mode option for these two product categories.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(filename=\"../input/analysisfiles/Executive_Overview.PNG\", width= \"800\", height=\"400\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Executive Dashboard\n\n- When considered the evaluation period between 2017 to 2020, organization should focus on improving the sales & profit from states of Illinois and Texas. Organization should also consider opening a new store in California to increase their profits.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Forecast model KPI's and insights."},{"metadata":{},"cell_type":"markdown","source":"#### Additional information required for model accuracy calculation:\n\n#### 1. dollarized weight\n#### 2. MAPE\n#### 3. WMAPE"},{"metadata":{},"cell_type":"markdown","source":"#### Inorder to get forecast model accuracy at global level, initially we need to aggregate the data to sku * Order Date level"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating dollarized weight\n\ndte = list(pd.date_range(final_data['Order Date'].min(),final_data['Order Date'].max(),freq='D'))\n\nfor x in range(0,len(dte)):\n    final_data.loc[(final_data['Order Date'] == dte[x]), 'dollarized_wgt'] = round(final_data.loc[(final_data['Order Date'] == dte[x]), 'Sales']/final_data.loc[(final_data['Order Date'] == dte[x]), 'Sales'].sum(),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data['MAPE'] = 0\nfinal_data.loc[(final_data['Sales']==0)&(final_data['Sales Forecast']>0), 'MAPE'] = 100\nfinal_data.loc[final_data['Sales']>0, 'MAPE'] = abs(final_data.loc[final_data['Sales']>0, 'Sales'] - final_data.loc[final_data['Sales']>0, 'Sales Forecast']) / final_data.loc[final_data['Sales']>0, 'Sales'] * 100\nfinal_data['MAPE'] = round(final_data['MAPE'],2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_data['WMAPE'] = final_data['dollarized_wgt'] * final_data['MAPE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_data = final_data.groupby(['Product ID','Order Date'])['WMAPE'].sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = 100 - accuracy_data['WMAPE'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forecast model insights:\n\n - forecast model never underforecasts\n - forecast model has 93.83 % global forecast accuracy for the evaluation period between Jan 2017 to Dec 2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}