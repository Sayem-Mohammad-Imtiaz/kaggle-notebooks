{"cells":[{"metadata":{"_cell_guid":"6a94806a-f025-47d3-b69f-c197c55a4086","_uuid":"2525b076559e77a9acc47f25fc03d22da079a616"},"cell_type":"markdown","source":"    Import the training and test data for the BigMart Sale"},{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n#Read files:\ntrain = pd.read_csv(\"../input/Train.csv\")\ntest = pd.read_csv(\"../input/Test.csv\")","execution_count":100,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"    As we have different files containing the train the test data, we need to merge them. Print the shape/number of columns and rows of the merged dataset"},{"metadata":{"_cell_guid":"2ed34a30-afd0-4a02-9f14-9ff0aaf04c2f","_uuid":"9ebc1296d49486ebf6e6b8583bdd4a666b9b0f5c","trusted":true},"cell_type":"code","source":"train['source']='train'\ntest['source']='test'\ndata = pd.concat([train, test],ignore_index=True)\nprint (train.shape, test.shape, data.shape)\nprint (data.head())","execution_count":102,"outputs":[]},{"metadata":{"_cell_guid":"dd6653b4-cc3c-426d-a746-570876c3ef8b","_uuid":"f2be1ef61933c7109864473f9f2ce3c97a8e4e42"},"cell_type":"markdown","source":"Check whether the dataset has any missing columns"},{"metadata":{"_cell_guid":"11a5797e-5bdd-42dd-b4f0-11a5cc7818b9","_uuid":"c62e9bedfb341cd0ca9fe7b33e23f72ad975ac29","trusted":true},"cell_type":"code","source":"data.apply(lambda x: sum(x.isnull()))","execution_count":103,"outputs":[]},{"metadata":{"scrolled":true,"_cell_guid":"84a52871-745b-4a0e-a5c9-667bc95e51cd","_uuid":"00707b98eb0228b56283bf97f1a14785933cc066","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":104,"outputs":[]},{"metadata":{"_cell_guid":"d4b96970-e4bc-449d-92bc-99b71220e139","_uuid":"e44e5b6f15b6410c8cbe992067f58bb5c3a25140"},"cell_type":"markdown","source":"* Missing values are present in Item_Outlet_Sales and Item_Weight columns\n* Item_Outlet_Sales has 5681 missing entries and Item_Weight has 2439 missing entries.\n* Also the column present in the test data named \"Outlet_Size\" has missing values\n* The minimum value of \"Item_Visibility\" column is zero \"0\"."},{"metadata":{"scrolled":false,"_cell_guid":"b32a8de7-783f-4af7-93d9-c7b85e5b0ba3","_uuid":"4155af8e315d82acbd8fc00c6646ffd9636ce698","trusted":true},"cell_type":"code","source":"data.apply(lambda x: len(x.unique()))","execution_count":105,"outputs":[]},{"metadata":{"_cell_guid":"df2eeaa2-772d-479a-8abc-18050865fb72","_uuid":"8b377da199df5720126c94d4201610f273b7123a"},"cell_type":"markdown","source":"The above output shows the following points:\n1.   There are 1559 of different products\n1.   There are 16 types of item types\n1.   There are 10 different outlets"},{"metadata":{"scrolled":true,"_cell_guid":"e8d27307-9944-4137-8426-e82c8881adcc","_uuid":"5c1f73599ad45e6584f0bd977f29dcb518cc9394","trusted":true},"cell_type":"code","source":"#Filter categorical variables\ncategorical_columns = [x for x in data.dtypes.index if data.dtypes[x]=='object']\n#Exclude ID cols and source:\ncategorical_columns = [x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier','source']]\n#Print frequency of categories\nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for varible %s'%col)\n    print (data[col].value_counts())","execution_count":106,"outputs":[]},{"metadata":{"_cell_guid":"6c9d7683-8908-44be-b647-318728ea8e26","_uuid":"ac4e7a2a410d090f31cfced2f4b11742a1486385"},"cell_type":"markdown","source":"Following are the observations from the above results:\n1.  For column \"Item_Fat_Content\" , there are 2 more values for  \"Low Fat\" .i.e \"LF\" and \"low fat\". These can be merged together. Similarly two values \"Regular\" and \"reg\" which is kind of redundant data\n1. Too many categories in \"Item_Type\" column.  We can try to merge them into common category."},{"metadata":{"_cell_guid":"cc476832-e616-4cbd-9e96-74e0f4d4ef27","_uuid":"17ffa1b19f1be54de21f04e5ca7afbed51c1aeb6"},"cell_type":"markdown","source":"Lets fill the missing for \"Item_Weight\" column by the mean weight of each item."},{"metadata":{"_cell_guid":"185af24a-f207-4084-9880-23a492bd8d2a","_uuid":"45fbd49760b4188847979f92e50e23f5a6b8079c","trusted":true},"cell_type":"code","source":"itemAvgWt = data.pivot_table(values='Item_Weight', index='Item_Identifier')\ngetBooleanData = data['Item_Weight'].isnull() \nprint ('Orignal #missing: %d'% sum(getBooleanData))\ndata.loc[getBooleanData,'Item_Weight'] = data.loc[getBooleanData,'Item_Identifier'].apply(lambda x: itemAvgWt.loc[x] )\nprint ('Final #missing: %d'% sum(data['Item_Weight'].isnull()))","execution_count":107,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0a810dbf-6273-4e8b-b155-a06da7162e29","_uuid":"ba8c0a05e776bd32c56a4a35c9b2fa8df79d7eb2","trusted":false},"cell_type":"markdown","source":"Lets fill the missing for \"Outlet_Size\" column by the mean weight of each item."},{"metadata":{"_cell_guid":"fe3b8801-a748-4d8f-be28-2ba3396cb971","_uuid":"1627a40ada0d8dd2290a9bf960ebfe95ac00976f","trusted":true},"cell_type":"code","source":"#Import mode function:\nfrom scipy.stats import mode\n\ngetBooleanData = data['Outlet_Size'].isnull() \ndata['Outlet_Size'].fillna('Small',inplace=True)\noutletSizeMode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x).mode[0]) )\nprint ('Mode for each Outlet_Type:')\nprint (outletSizeMode)\n\n#Impute data and check #missing values before and after imputation to confirm\nprint ('\\nOrignal #missing: %d'% sum(miss_bool))\ndata.loc[getBooleanData,'Outlet_Size'] = data.loc[getBooleanData,'Outlet_Type'].apply(lambda x: outletSizeMode.loc[x])\nprint (sum(data['Outlet_Size'].isnull()))","execution_count":109,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"926c700c6166e53c497c25f61d0ecc3be4f45495"},"cell_type":"code","source":"data.pivot_table(values='Item_Outlet_Sales',index='Outlet_Type')","execution_count":110,"outputs":[]},{"metadata":{"_uuid":"d6c9b116567805aa84658c815e0f35cb80e39805"},"cell_type":"markdown","source":"Remove the values having \"0\" in column Item_Visibility"},{"metadata":{"trusted":true,"_uuid":"0f366df42432d2326535c240ecc9c71c3340ce09"},"cell_type":"code","source":"#Determine average visibility of a product\nvisibility_avg = data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n\n#Impute 0 values with mean visibility of that product:\nmiss_bool = (data['Item_Visibility'] == 0)\n\nprint ('Number of 0 values initially: %d'%sum(miss_bool))\ndata.loc[miss_bool,'Item_Visibility'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x: visibility_avg.loc[x])\nprint ('Number of 0 values after modification: %d'%sum(data['Item_Visibility'] == 0))","execution_count":111,"outputs":[]},{"metadata":{"_uuid":"9dcfdf01a512c9a7e3a079edc43a56a82f920320"},"cell_type":"markdown","source":"    The columns \"Item_Type\" has 16 different categories. Lets club them together fewer amount of categories"},{"metadata":{"trusted":true,"_uuid":"c64dbd01ed499dbc60a72d5c962dc1e16deb4268"},"cell_type":"code","source":"data['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\nprint(data['Item_Type_Combined'].value_counts())\n#Rename them to more intuitive categories:\ndata['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food',\n                                                             'NC':'Non-Consumable',\n                                                             'DR':'Drinks'})\ndata['Item_Type_Combined'].value_counts()","execution_count":112,"outputs":[]},{"metadata":{"_uuid":"9261284c8c90c8ea964f0640f701003985e58ed2"},"cell_type":"markdown","source":"    Lets determine the operation years of the store"},{"metadata":{"trusted":true,"_uuid":"1c83fe91d3cf52cb978c03fa5485f18531e79a46"},"cell_type":"code","source":"#Years:\ndata['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\ndata['Outlet_Years'].describe()","execution_count":114,"outputs":[]},{"metadata":{"_uuid":"63fd5a5dcb9a52cc07db941553f5c6acfc61cf7d"},"cell_type":"markdown","source":"Lets modify the redundant entries of \"Item_Fat_Content\" into similar ones"},{"metadata":{"trusted":true,"_uuid":"425a7f75ed8ede1efcb6f9dfcd26dcbaef01cab0"},"cell_type":"code","source":"#Change categories of low fat:\nprint ('Original Categories:')\nprint (data['Item_Fat_Content'].value_counts())\n\nprint ('\\nModified Categories:')\ndata['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat',\n                                                             'reg':'Regular',\n                                                             'low fat':'Low Fat'})\nprint (data['Item_Fat_Content'].value_counts())","execution_count":115,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da085597a12783f6135ce2aac6cdfd32edeb58b3"},"cell_type":"code","source":"#Mark non-consumables as separate category in low_fat:\ndata.loc[data['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\ndata['Item_Fat_Content'].value_counts()","execution_count":116,"outputs":[]},{"metadata":{"_uuid":"95f19469623f7b5c744d5b5e21e8631357195e5b"},"cell_type":"markdown","source":"        Lets start encoding the non-numeric columns "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60be7dbcab2b07380e7254f28f8b714ed1c6f8b3"},"cell_type":"code","source":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#New variable for outlet\ndata['Outlet'] = le.fit_transform(data['Outlet_Identifier'])\nvar_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']\nle = LabelEncoder()\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])","execution_count":117,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e1b25f2da82919fb758aa24d9f7b1a64009727d"},"cell_type":"code","source":"#One Hot Coding:\ndata = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type','Item_Type_Combined','Outlet'])\ndata.dtypes","execution_count":118,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68e80c11e6c49a3dbe35a46d8d8d2292eb8f8957"},"cell_type":"code","source":"data[['Item_Fat_Content_0','Item_Fat_Content_1','Item_Fat_Content_2']].head(10)","execution_count":119,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1533982ea6d26c10e40b547341f01873f28f517b"},"cell_type":"code","source":"#Drop the columns which have been converted to different types:\ndata.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)\n\n#Divide into test and train:\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)\n\n#Export files as modified versions:\ntrain.to_csv(\"train_modified.csv\",index=False)\ntest.to_csv(\"test_modified.csv\",index=False)","execution_count":120,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df2474589f3c55a14baa41379d9df06b841cb833"},"cell_type":"code","source":"#Mean based:\nmean_sales = train['Item_Outlet_Sales'].mean()\n\n#Define a dataframe with IDs for submission:\nbase1 = test[['Item_Identifier','Outlet_Identifier']]\nbase1['Item_Outlet_Sales'] = mean_sales\n\n#Export submission file\nbase1.to_csv(\"alg0.csv\",index=False)","execution_count":121,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d39538478b4ab61f742a08000e037392ce542782"},"cell_type":"code","source":"#Define target and ID columns:\ntarget = 'Item_Outlet_Sales'\nIDcol = ['Item_Identifier','Outlet_Identifier']\nfrom sklearn import cross_validation, metrics\n\ndef modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[target])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n\n    #Perform cross-validation:\n    cv_score = cross_validation.cross_val_score(alg, dtrain[predictors], dtrain[target], cv=20, scoring='mean_squared_error')\n    cv_score = np.sqrt(np.abs(cv_score))\n    \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n    print (\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    \n    #Predict on testing data:\n    dtest[target] = alg.predict(dtest[predictors])\n    \n    #Export submission file:\n    IDcol.append(target)\n    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n    submission.to_csv(filename, index=False)","execution_count":123,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"015738669b54fd3e906693bbb4488f444089c031"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\npredictors = [x for x in train.columns if x not in [target]+IDcol]\n# print predictors\nalg1 = LinearRegression(normalize=True)\nmodelfit(alg1, train, test, predictors, target, IDcol, 'alg1.csv')\ncoef1 = pd.Series(alg1.coef_, predictors).sort_values()\ncoef1.plot(kind='bar', title='Model Coefficients')","execution_count":124,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f1c830f02684ab07bd585f7577ae2d1af93e8a5b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}