{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text Generation or What's the Next Word?"},{"metadata":{},"cell_type":"markdown","source":"### Hey kagglers, Wassup? One more concept in my dictonary. \n> This notebook is all about text generation or you can get the next word of your word list using this model. Interesting, Isn't it?"},{"metadata":{},"cell_type":"markdown","source":"Let's Start. This is a probabilistic model which will predict the next word of your text. Now you would have already guessed it that we can use it for text generation. Amazing!\n\nThe main concept is, if you already have start words then will predict the probability of the next word from our text corpus. Example - if your start word is 'Welcome to' then what will be your next word? So will choose the next word from our text corpus having highest probability given that it's previous two words are 'Welcome' and 'to'. Getting concept of probability?\n\n### -------------------------- **Index** ----------------------------------\n1. Data Loading.\n2. Data Cleaning.\n3. Model Development.\n4. Testing.\n5. What Next?"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk import ngrams\nimport re\nfrom collections import defaultdict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Data Loading\nI have randomly choosen the dataset so if you want to work with your choice of dataset the please go ahead and test it. Remember to fork my kernel. Hahaha.\n\nThis data is all about the chat between different users. For particular user id, there will be one message per row. There are so many columns in the dataset but for my purpose there are only two columns i.e, user id and text message. So Let's load the data, what are you waiting for? love me **** *** **."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/all-posts-public-main-chatroom/freecodecamp_casual_chatroom.csv',\n                 usecols=['fromUser.id', 'text'])\nprint(df.head())\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming columns for my better understanding\ndf.rename(columns={'fromUser.id': 'id'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.astype(str)\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check top 10 users who have texted the most. Means, top 10 active users.\n\nI will use the chat of the top active user."},{"metadata":{"trusted":true},"cell_type":"code","source":"id_count = df.id.value_counts().reset_index(drop=False).head(10)\n#id_count.head()\nplt.figure(figsize=(12, 8))\ng = sns.barplot(x='index', y='id', data=id_count, color='green')\ng.set_xticklabels(labels=id_count['index'], rotation=45)\nplt.title('Top Active users')\nplt.xlabel(\"User ID\")\nplt.ylabel(\"Count of Message\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df.id=='55b977f00fc9f982beab7883']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Data Cleaning.\nData Cleaning is very crucial part of machine learning and in case of NLP it's must. So let's dig into the data.\n\nBelow are the data cleaning steps and this is not the hard and fast rule of data cleaning, you can clean the data according to your requirement. I am using **re** package of python. You can try **spacy** also. There are so many interesting nlp packages are available just explore and have fun.\n* Lowercase the text\n* Remove URL's\n* Remove @ tags\n* Remove # tags\n* Remove everything except alphabets.\n* Remove single letters\n* strip multiple spaces between the text\n* Strip left and right most spaces.\n\nThant's it. Keep in mind these steps work for me sequentially."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', ' ', text)\n    text = re.sub(r'@\\S+', ' ', text)\n    text = re.sub(r'#\\S+', ' ', text)\n    text = re.sub(r'[^a-z]', ' ', text)\n    text = ' '.join(['' if len(word)<2 else word for word in text.split()])\n    text = re.sub(r' +', ' ', text)\n    text = text.strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.text = df.text.map(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_count'] = df.text.apply(lambda text: len(text.split(' ')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating text corpus\ntext = df.text.str.cat(sep=' ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's read starting 300 letters of text. Looks clean and clear text? Amazing!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(text))\nprint(text[:300])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Model Development.\nBasically we are generating trigrams and first two words of trigram will be keys input and 3rd word will be value for these keys.\n\nIf you don't know the concept of ngram or bigram and trigram then this paragraph is for you others can skip this. If your text is \"I am a python developer and working in some abc company\", then bigram will be [(\"I\", \"am\"), (\"am\", \"a\"), (\"a\", \"python\").....], similary you can get trigram or ngram.\n\nI am using nltk package for trigram. You can develop your own algorithm also."},{"metadata":{"trusted":true},"cell_type":"code","source":"trigram = ngrams(text.split(), n=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are using defaultdictonay of collection package. Speciality of default dictionary is, it will not throw any error if you accessed the unknown key, it returns blank."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = defaultdict(lambda: defaultdict(lambda: 0))\n\nfor w1, w2, w3 in trigram:\n    model[(w1, w2)][w3] += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's calculate the probability of 3rd word of trigram where we already know it's previous two words."},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in model:\n    total_count_sum = sum(model[key].values())\n    total_count_sum = float(total_count_sum)\n    for index in model[key]:\n        model[key][index] = model[key][index]/total_count_sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here start is the staring two words of the text and then predicting next word. For the purpose of text generation we are randomly choosing next word from key value pair of trigram. If you only want to choose only one next word, then you can choose the word having highest probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"start = ['sends', 'brownie']\nstop = False\nfirst = True\ngenerated_text = start\n\nwhile not stop:\n    if first:\n        # Covers only initial scenario.\n        try:\n            words = list(model[start[0], start[1]].keys())\n            random = np.random.randint(len(words))\n            word = words[random]\n            # print(word)\n        except Exception as e:\n            print(str(e))\n            break\n        first = False\n        generated_text.append(word)\n    start = generated_text[-2:]\n    if len(start)==0:\n        stop=True\n    try:\n        words = list(model[start[0], start[1]].keys())\n        random = np.random.randint(len(words))\n        word = words[random]\n    except Exception as e:\n        print(str(e))\n        break\n\n    generated_text.append(word)\n\n    if len(generated_text) > 100:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Testing\nLet's check the output and interpret. Model has genmerated preety cool output. Isn't it? you can use more input data for this model, so that you can get better result maybe."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_text = ' '.join(generated_text)\n\nprint(final_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tip - Most probable next word."},{"metadata":{"trusted":true},"cell_type":"code","source":"start = ['welcome', 'to']\n\nmax(model[start[0], start[1]], key=model[start[0], start[1]].get)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. What's Next?\nThis is very basic langauage model, you can use modern models like RNN, LSTM, GRU etc to generate texts. These modern models considers grammer, contexts etc.\n\nDisadvantage of this model is it is not very flexible model and works with medium quantity of data."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}