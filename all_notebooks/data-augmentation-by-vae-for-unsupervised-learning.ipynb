{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.14.0 \n!pip install gast==0.2.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport collections\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn import preprocessing, metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport warnings\nimport itertools\nfrom PIL import Image\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.__version__)\ntf.reset_default_graph()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(r\"../input/hr2u-vae/pulsar_stars.csv\")\n# data = pd.read_csv(\"../input/hr2u-vae/HTRU_2.csv\")\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for n, i in enumerate(data):\n#     if i == 1:\n#         data[n] = -1\n# print(data)\n\ndata['target_class'] = data['target_class'].replace([1],-1)\ndata['target_class'] = data['target_class'].replace([0],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['target_class'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = data.columns.str.strip()\ndata.columns = ['IP Mean', 'IP Sd', 'IP Kurtosis', 'IP Skewness', \n              'DM-SNR Mean', 'DM-SNR Sd', 'DM-SNR Kurtosis', 'DM-SNR Skewness', 'target_class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the percentage distribution of target_class column\ndata['target_class'].value_counts()/np.float(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['target_class'], axis=1)\n\ny = data['target_class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 10)\nX_test_label = y_test\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_test_label.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.to_numpy()\ny_train = y_train.to_numpy()\nX_test =X_test.to_numpy()\nX_test_label =X_test_label.to_numpy()\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_test_label.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train)\nprint(\"*\"*30)\nprint(X_test_label)\nprint(\"*\"*30)\nprint(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)\n\n# scaler = MinMaxScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nMLP Variational AutoEncoder for Anomaly Detection\nreference: https://pdfs.semanticscholar.org/0611/46b1d7938d7a8dae70e3531a00fceb3c78e8.pdf\n\"\"\"\n\n\n\ndef lrelu(x, leak=0.2, name='lrelu'):\n\treturn tf.maximum(x, leak*x)\n\n\ndef build_dense(input_vector,unit_no,activation):    \n    return tf.layers.dense(input_vector,unit_no,activation=activation,\n            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n            bias_initializer=tf.zeros_initializer())\n\nclass MLP_VAE:\n    def __init__(self,input_dim,lat_dim, outliers_fraction):\n       # input_paras:\n           # input_dim: input dimension for X\n           # lat_dim: latent dimension for Z\n           # outliers_fraction: pre-estimated fraction of outliers in trainning dataset\n        \n        self.outliers_fraction = outliers_fraction # for computing the threshold of anomaly score       \n        self.input_dim = input_dim\n        self.lat_dim = lat_dim # the lat_dim can exceed input_dim    \n        \n        self.input_X = tf.placeholder(tf.float32,shape=[None,self.input_dim],name='source_x')\n        \n        self.learning_rate = 0.0005\n        self.batch_size =  32\n        # batch_size should be smaller than normal setting for getting\n        # a relatively lower anomaly-score-threshold\n        self.train_iter = 8000\n        self.hidden_units = 128\n        self._build_VAE()\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        self.pointer = 0\n        \n    def _encoder(self):\n        with tf.variable_scope('encoder',reuse=tf.AUTO_REUSE):\n            l1 = build_dense(self.input_X,self.hidden_units,activation=lrelu)\n#            l1 = tf.nn.dropout(l1,0.8)\n            l2 = build_dense(l1,self.hidden_units,activation=lrelu)\n#            l2 = tf.nn.dropout(l2,0.8)          \n            mu = tf.layers.dense(l2,self.lat_dim)\n            sigma = tf.layers.dense(l2,self.lat_dim,activation=tf.nn.softplus)\n            sole_z = mu + sigma *  tf.random_normal(tf.shape(mu),0,1,dtype=tf.float32)\n        return mu,sigma,sole_z\n        \n    def _decoder(self,z):\n        with tf.variable_scope('decoder',reuse=tf.AUTO_REUSE):\n            l1 = build_dense(z,self.hidden_units,activation=lrelu)\n#            l1 = tf.nn.dropout(l1,0.8)\n            l2 = build_dense(l1,self.hidden_units,activation=lrelu)\n#            l2 = tf.nn.dropout(l2,0.8)\n            recons_X = tf.layers.dense(l2,self.input_dim)\n        return recons_X\n\n\n    def _build_VAE(self):\n        self.mu_z,self.sigma_z,sole_z = self._encoder()\n        self.recons_X = self._decoder(sole_z)\n        \n        with tf.variable_scope('loss'):\n            KL_divergence = 0.5 * tf.reduce_sum(tf.square(self.mu_z) + tf.square(self.sigma_z) - tf.log(1e-8 + tf.square(self.sigma_z)) - 1, 1)\n            mse_loss = tf.reduce_sum(tf.square(self.input_X-self.recons_X), 1)          \n            self.all_loss =  mse_loss\n            self.loss = tf.reduce_mean(mse_loss + KL_divergence)\n\n            \n        with tf.variable_scope('train'):            \n            self.train_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n            \n\n    def _fecth_data(self,input_data):        \n        if (self.pointer+1) * self.batch_size  >= input_data.shape[0]:\n            return_data = input_data[self.pointer*self.batch_size:,:]\n            self.pointer = 0\n        else:\n            return_data =  input_data[ self.pointer*self.batch_size:(self.pointer+1)*self.batch_size,:]\n            self.pointer = self.pointer + 1\n        return return_data\n    \n     \n\n    def train(self,train_X):\n        for index in range(self.train_iter):\n            this_X = self._fecth_data(train_X)\n            self.sess.run([self.train_op],feed_dict={\n                        self.input_X: this_X\n                        })\n#             print(f'iter:{index}, loss:{[self.train_op]}')\n        self.arrage_recons_loss(train_X)\n        \n    def arrage_recons_loss(self,input_data):\n        all_losses =  self.sess.run(self.all_loss,feed_dict={\n                self.input_X: input_data                  \n                })\n        self.judge_loss = np.percentile(all_losses,(1-self.outliers_fraction)*100)\n                \n\n    def judge(self,input_data):\n        return_label = []\n        for index in range(input_data.shape[0]):\n            single_X = input_data[index].reshape(1,-1)\n            this_loss = self.sess.run(self.loss,feed_dict={\n                    self.input_X: single_X                  \n                    })\n            if this_loss < self.judge_loss:\n                return_label.append(1)\n            else:\n                return_label.append(-1)\n        return return_label\n       \ndef plot_confusion_matrix(y_true, y_pred, labels,title):\n    cmap = plt.cm.binary\n    cm = confusion_matrix(y_true, y_pred)\n    tick_marks = np.array(range(len(labels))) + 0.5\n    np.set_printoptions(precision=2)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(4, 2), dpi=120)\n    ind_array = np.arange(len(labels))\n    x, y = np.meshgrid(ind_array, ind_array)\n    intFlag = 0 \n    for x_val, y_val in zip(x.flatten(), y.flatten()):\n        #\n\n        if (intFlag):\n            c = cm[y_val][x_val]\n            plt.text(x_val, y_val, \"%d\" % (c,), color='red', fontsize=8, va='center', ha='center')\n\n        else:\n            c = cm_normalized[y_val][x_val]\n            if (c > 0.01):\n                plt.text(x_val, y_val, \"%0.2f\" % (c,), color='red', fontsize=7, va='center', ha='center')\n            else:\n                plt.text(x_val, y_val, \"%d\" % (0,), color='red', fontsize=7, va='center', ha='center')\n    if(intFlag):\n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    else:\n        plt.imshow(cm_normalized, interpolation='nearest', cmap=cmap)\n    plt.gca().set_xticks(tick_marks, minor=True)\n    plt.gca().set_yticks(tick_marks, minor=True)\n    plt.gca().xaxis.set_ticks_position('none')\n    plt.gca().yaxis.set_ticks_position('none')\n    plt.grid(True, which='minor', linestyle='-')\n    plt.gcf().subplots_adjust(bottom=0.15)\n    plt.title(title)\n    plt.colorbar()\n    xlocations = np.array(range(len(labels)))\n    plt.xticks(xlocations, labels)\n    plt.yticks(xlocations, labels)\n    plt.ylabel('Index of True Classes')\n    plt.xlabel('Index of Predict Classes')\n    plt.show()\n \ndef mlp_vae_predict(train,test,test_label):\n    mlp_vae = MLP_VAE(8,20,0.07)\n    mlp_vae.train(train)\n    mlp_vae_predict_label = mlp_vae.judge(test)\n    print(collections.Counter(test_label))\n    print(collections.Counter(mlp_vae_predict_label))\n#     print(confusion_matrix(test_label, mlp_vae_predict_label))\n    print(metrics.roc_auc_score(test_label, mlp_vae_predict_label ))\n    plot_confusion_matrix(test_label, mlp_vae_predict_label, ['anomaly','normal'],'MLP_VAE Confusion-Matrix')\n    confusion_matrix(test_label, mlp_vae_predict_label)\n\ndef iforest_predict(train,test,test_label):\n    from sklearn.ensemble import IsolationForest\n    iforest = IsolationForest(max_samples = 'auto',\n                                 behaviour=\"new\",contamination=0.01)\n\n    iforest.fit(train)\n    iforest_predict_label = iforest.predict(test)\n    print(collections.Counter(test_label))\n    print(collections.Counter(iforest_predict_label))\n    print(metrics.roc_auc_score(test_label, iforest_predict_label ))\n    plot_confusion_matrix(test_label, iforest_predict_label, ['anomaly','normal'],'iforest Confusion-Matrix')\n    \ndef random_predict(train,y_train,test,test_label):\n    from sklearn.ensemble import RandomForestClassifier\n    rforest = RandomForestClassifier()\n\n    rforest.fit(train,y_train)\n    rforest_predict_label = rforest.predict(test)\n    print(collections.Counter(test_label))\n    print(collections.Counter(rforest_predict_label))\n    print(metrics.roc_auc_score(test_label, rforest_predict_label ))\n    plot_confusion_matrix(test_label, rforest_predict_label, ['anomaly','normal'],'iforest Confusion-Matrix')\n\ndef lof_predict(train,test,test_label):\n    from sklearn.neighbors import LocalOutlierFactor\n    lof = LocalOutlierFactor(novelty=True,contamination=0.01)\n    lof.fit(train)\n    lof_predict_label = lof.predict(test)\n    print(collections.Counter(test_label))\n    print(collections.Counter(lof_predict_label))\n    print(metrics.roc_auc_score(test_label, lof_predict_label ))\n    plot_confusion_matrix(test_label, lof_predict_label, ['anomaly','normal'],'LOF Confusion-Matrix')\n\nif __name__ == '__main__':\n    mlp_vae_predict(X_train,X_test,X_test_label)\n    iforest_predict(X_train,X_test,X_test_label)\n    lof_predict(X_train,X_test,X_test_label)\n    \n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}