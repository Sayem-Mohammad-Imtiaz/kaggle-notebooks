{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.manifold import Isomap\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster import KMeans\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AR,AutoReg\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport spacy as sp\nnlps = sp.load('en')\nimport random\nplt.rc('figure',figsize=(17,13))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-18T06:31:03.043259Z","iopub.execute_input":"2021-06-18T06:31:03.043715Z","iopub.status.idle":"2021-06-18T06:31:08.277466Z","shell.execute_reply.started":"2021-06-18T06:31:03.043554Z","shell.execute_reply":"2021-06-18T06:31:08.276518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_data = pd.read_csv('/kaggle/input/reddit-vaccine-myths/reddit_vm.csv')\nr_data = r_data[pd.to_datetime(r_data.timestamp).dt.year>=2021]\nr_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:31:34.601105Z","iopub.execute_input":"2021-06-18T06:31:34.60139Z","iopub.status.idle":"2021-06-18T06:31:34.685643Z","shell.execute_reply.started":"2021-06-18T06:31:34.601367Z","shell.execute_reply":"2021-06-18T06:31:34.68495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_data = r_data[['title','timestamp']].copy()\nbody_data = r_data[['body','timestamp']].copy()\nbody_data = body_data.dropna()\ntitle_data = title_data.dropna()\n\n\ntitle_data.title =title_data.title.str.lower()\nbody_data.body =body_data.body.str.lower()\n\n#Remove handlers\ntitle_data.title = title_data.title.apply(lambda x:re.sub('@[^\\s]+','',x))\nbody_data.body   = body_data.body.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n# Remove URLS\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\nbody_data.body   = body_data.body.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n# Remove all the special characters\ntitle_data.title = title_data.title.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\nbody_data.body   = body_data.body.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n#remove all single characters\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\nbody_data.body   = body_data.body.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\ntitle_data.title = title_data.title.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\nbody_data.body   = body_data.body.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n\n\n\n#Remove Time From Timestamp\ntitle_data.timestamp = pd.to_datetime(title_data.timestamp).dt.date\nbody_data.timestamp = pd.to_datetime(body_data.timestamp).dt.date","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:32:06.242142Z","iopub.execute_input":"2021-06-18T06:32:06.242645Z","iopub.status.idle":"2021-06-18T06:32:06.285998Z","shell.execute_reply.started":"2021-06-18T06:32:06.242614Z","shell.execute_reply":"2021-06-18T06:32:06.284994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sid = SIA()\nbody_data['sentiments']           = body_data['body'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nbody_data['Positive Sentiment']   = body_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nbody_data['Neutral Sentiment']    = body_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nbody_data['Negative Sentiment']   = body_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nbody_data.drop(columns=['sentiments'],inplace=True)\n\n\ntitle_data['sentiments']           = title_data['title'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\ntitle_data['Positive Sentiment']   = title_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \ntitle_data['Neutral Sentiment']    = title_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\ntitle_data['Negative Sentiment']   = title_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\ntitle_data.drop(columns=['sentiments'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:32:24.752823Z","iopub.execute_input":"2021-06-18T06:32:24.753204Z","iopub.status.idle":"2021-06-18T06:32:24.894587Z","shell.execute_reply.started":"2021-06-18T06:32:24.753172Z","shell.execute_reply":"2021-06-18T06:32:24.893413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"body_data['# Of Words']          = body_data['body'].apply(lambda x: len(x.split(' ')))\nbody_data['# Of StopWords']      = body_data['body'].apply(lambda x: len([word for word in x.split(' ') if word in list(STOPWORDS)]))\nbody_data['Average Word Length'] = body_data['body'].apply(lambda x: np.mean(np.array([len(va) for va in x.split(' ') if va not in list(STOPWORDS)])))\n\ntitle_data['# Of Words']          = title_data['title'].apply(lambda x: len(x.split(' ')))\ntitle_data['# Of StopWords']      = title_data['title'].apply(lambda x: len([word for word in x.split(' ') if word in list(STOPWORDS)]))\ntitle_data['Average Word Length'] = title_data['title'].apply(lambda x: np.mean(np.array([len(va) for va in x.split(' ') if va not in list(STOPWORDS)])))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:32:40.94829Z","iopub.execute_input":"2021-06-18T06:32:40.948656Z","iopub.status.idle":"2021-06-18T06:32:41.022274Z","shell.execute_reply.started":"2021-06-18T06:32:40.948625Z","shell.execute_reply":"2021-06-18T06:32:41.0216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_data['# Of Times Currency Was Mentioned']          = title_data['title'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'MONEY' ]))\ntitle_data['# Of Organizations Mentioned']           = title_data['title'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'ORG' ]))\n\nprint('Processed Title DataFrame')\nbody_data['# Of Times Currency Was Mentioned']          = body_data['body'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'MONEY' ]))\nbody_data['# Of Organizations Mentioned']           = body_data['body'].apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'ORG' ]))\nprint('Processed Body DataFrame')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:33:02.712317Z","iopub.execute_input":"2021-06-18T06:33:02.712835Z","iopub.status.idle":"2021-06-18T06:33:09.9943Z","shell.execute_reply.started":"2021-06-18T06:33:02.712805Z","shell.execute_reply":"2021-06-18T06:33:09.993393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(title_data['Negative Sentiment'],bw_method=0.1)\nsns.kdeplot(title_data['Positive Sentiment'],bw_method=0.1)\nsns.kdeplot(title_data['Neutral Sentiment'],bw_method=0.1)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(title_data['Negative Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(title_data['Positive Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(title_data['Neutral Sentiment'],bw_method=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:33:22.085427Z","iopub.execute_input":"2021-06-18T06:33:22.085846Z","iopub.status.idle":"2021-06-18T06:33:22.632753Z","shell.execute_reply.started":"2021-06-18T06:33:22.08581Z","shell.execute_reply":"2021-06-18T06:33:22.631805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sorting And Feature Engineering\nf_data = title_data.sort_values(by='timestamp')\nft_data=f_data.copy()\nft_data = ft_data.rename(columns={'timestamp':'date'})\nft_data['year']         = pd.DatetimeIndex(ft_data['date']).year\nft_data['month']        = pd.DatetimeIndex(ft_data['date']).month\nft_data['day']          = pd.DatetimeIndex(ft_data['date']).day\nft_data['day_of_year']  = pd.DatetimeIndex(ft_data['date']).dayofyear\nft_data['quarter']      = pd.DatetimeIndex(ft_data['date']).quarter\nft_data['season']       = ft_data.month%12 // 3 + 1","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:33:50.437106Z","iopub.execute_input":"2021-06-18T06:33:50.437449Z","iopub.status.idle":"2021-06-18T06:33:50.452291Z","shell.execute_reply.started":"2021-06-18T06:33:50.437423Z","shell.execute_reply":"2021-06-18T06:33:50.451388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_data=f_data.reset_index().drop(columns=['index'])\nf_data = f_data.rename(columns={'timestamp':'date'})\n\npartitions = []\npartitions.append(f_data.loc[44:np.round(len(f_data)/3,0)-1,:])\npartitions.append(f_data.loc[np.round(len(f_data)/3,0):2*int(len(f_data)/3)-1,:])\npartitions.append(f_data.loc[2*np.round(len(f_data)/3,0):3*int(len(f_data)/3)-1,:])\n\n\n\nneg_part_means =[]\nneg_part_std   =[]\npos_part_means =[]\npos_part_std   =[]\nfor part in partitions:\n    neg_part_means.append(part['Negative Sentiment'].mean())\n    neg_part_std.append(part['Negative Sentiment'].std())\n    pos_part_means.append(part['Positive Sentiment'].mean())\n    pos_part_std.append(part['Positive Sentiment'].std())\n    \nres_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n                     index = [f'Partition_{i}' for i in range(1,4)])\n\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n\n    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n\n    return df1\n\n\n\n#res_df.style.apply(highlight_greater,axis=None)\nres_df = res_df.T\nres_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\nres_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\nres_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\nres_df.style.apply(highlight_greater,axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:34:14.00871Z","iopub.execute_input":"2021-06-18T06:34:14.009084Z","iopub.status.idle":"2021-06-18T06:34:14.105738Z","shell.execute_reply.started":"2021-06-18T06:34:14.009055Z","shell.execute_reply":"2021-06-18T06:34:14.104954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nb_date_mean = ft_data.groupby(by='date').mean().reset_index()\n\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    res = seasonal_decompose(b_date_mean[column], period=5, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.observed)), y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.trend)), y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.seasonal)), y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.resid)), y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:35:02.073341Z","iopub.execute_input":"2021-06-18T06:35:02.073779Z","iopub.status.idle":"2021-06-18T06:35:02.264413Z","shell.execute_reply.started":"2021-06-18T06:35:02.073739Z","shell.execute_reply":"2021-06-18T06:35:02.26313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\nax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:35:31.088461Z","iopub.execute_input":"2021-06-18T06:35:31.089051Z","iopub.status.idle":"2021-06-18T06:35:31.618434Z","shell.execute_reply.started":"2021-06-18T06:35:31.089016Z","shell.execute_reply":"2021-06-18T06:35:31.617034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =f_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =f_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:36:51.470751Z","iopub.execute_input":"2021-06-18T06:36:51.471187Z","iopub.status.idle":"2021-06-18T06:36:51.554779Z","shell.execute_reply.started":"2021-06-18T06:36:51.471154Z","shell.execute_reply":"2021-06-18T06:36:51.553816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_date_count = ft_data.groupby(by='date').count().reset_index()\nb_date_count = b_date_count.rename(columns={'title':'Posts Per Day'})\nfig = ex.line(b_date_count,x='date',y='Posts Per Day')\n\n\nfig.add_shape(type=\"line\",\n    x0=b_date_count['date'].values[0], y0=b_date_count['Negative Sentiment'].mean(), x1=b_date_count['date'].values[-1], y1=b_date_count['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n)\n\nfig.update_traces(mode=\"markers+lines\")\nfig.update_layout(hovermode=\"x unified\")\n\n\nfig.update_layout(title='<b>Daily Post Count<b>')\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:37:19.314737Z","iopub.execute_input":"2021-06-18T06:37:19.315402Z","iopub.status.idle":"2021-06-18T06:37:20.210383Z","shell.execute_reply.started":"2021-06-18T06:37:19.315368Z","shell.execute_reply":"2021-06-18T06:37:20.209404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUMBER_OF_COMPONENTS = 100\n\nCVZ = CountVectorizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = f_data.title.copy()\ntext_data = text_data.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS and len(word) > 1]).strip())\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\n\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Post Text Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:38:34.491603Z","iopub.execute_input":"2021-06-18T06:38:34.49199Z","iopub.status.idle":"2021-06-18T06:38:34.638794Z","shell.execute_reply.started":"2021-06-18T06:38:34.491958Z","shell.execute_reply":"2021-06-18T06:38:34.637338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(body_data['Negative Sentiment'],bw_method=0.1)\nsns.kdeplot(body_data['Positive Sentiment'],bw_method=0.1)\nsns.kdeplot(body_data['Neutral Sentiment'],bw_method=0.1)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Posts',fontsize=19,fontweight='bold')\nsns.kdeplot(body_data['Negative Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(body_data['Positive Sentiment'],bw_method=0.1,cumulative=True)\nsns.kdeplot(body_data['Neutral Sentiment'],bw_method=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:52:20.971947Z","iopub.execute_input":"2021-06-18T06:52:20.972245Z","iopub.status.idle":"2021-06-18T06:52:21.448782Z","shell.execute_reply.started":"2021-06-18T06:52:20.972221Z","shell.execute_reply":"2021-06-18T06:52:21.447777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sorting And Feature Engineering\nf_data = body_data.sort_values(by='timestamp')\nft_data=f_data.copy()\nft_data = ft_data.rename(columns={'timestamp':'date'})\nft_data['year']         = pd.DatetimeIndex(ft_data['date']).year\nft_data['month']        = pd.DatetimeIndex(ft_data['date']).month\nft_data['day']          = pd.DatetimeIndex(ft_data['date']).day\nft_data['day_of_year']  = pd.DatetimeIndex(ft_data['date']).dayofyear\nft_data['quarter']      = pd.DatetimeIndex(ft_data['date']).quarter\nft_data['season']       = ft_data.month%12 // 3 + 1","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:53:06.7093Z","iopub.execute_input":"2021-06-18T06:53:06.709703Z","iopub.status.idle":"2021-06-18T06:53:06.722575Z","shell.execute_reply.started":"2021-06-18T06:53:06.709677Z","shell.execute_reply":"2021-06-18T06:53:06.72182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_data=f_data.reset_index().drop(columns=['index'])\nf_data = f_data.rename(columns={'timestamp':'date'})\n\npartitions = []\npartitions.append(f_data.loc[44:np.round(len(f_data)/3,0)-1,:])\npartitions.append(f_data.loc[np.round(len(f_data)/3,0):2*int(len(f_data)/3)-1,:])\npartitions.append(f_data.loc[2*np.round(len(f_data)/3,0):3*int(len(f_data)/3)-1,:])\nneg_part_means =[]\nneg_part_std   =[]\npos_part_means =[]\npos_part_std   =[]\nfor part in partitions:\n    neg_part_means.append(part['Negative Sentiment'].mean())\n    neg_part_std.append(part['Negative Sentiment'].std())\n    pos_part_means.append(part['Positive Sentiment'].mean())\n    pos_part_std.append(part['Positive Sentiment'].std())\n    \nres_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n                     index = [f'Partition_{i}' for i in range(1,4)])\n\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n\n    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n\n    return df1\n\n\n\n#res_df.style.apply(highlight_greater,axis=None)\nres_df = res_df.T\nres_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\nres_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\nres_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\nres_df.style.apply(highlight_greater,axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T06:53:37.73818Z","iopub.execute_input":"2021-06-18T06:53:37.738508Z","iopub.status.idle":"2021-06-18T06:53:37.79048Z","shell.execute_reply.started":"2021-06-18T06:53:37.738481Z","shell.execute_reply":"2021-06-18T06:53:37.78939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=3, cols=2)\n\nfor idx,prt in enumerate(partitions):\n    by_date = prt.groupby(by='date').mean().reset_index()\n    fig.add_trace(\n    go.Scatter(x=by_date['date'], y=by_date['Positive Sentiment'],name=f'Positive Part {idx+1}'),\n    row=idx+1, col=1)\n    fig.add_trace(\n    go.Scatter(x=by_date['date'], y=by_date['Negative Sentiment'],name=f'Negative Part {idx+1}'),\n    row=idx+1, col=2)\n\nfig.update_layout(height=600, width=900, title_text=\"Distibution Of Daily Mean Sentiments Over Our Time Line For Each Partition\")\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:03:07.766626Z","iopub.execute_input":"2021-06-18T07:03:07.76694Z","iopub.status.idle":"2021-06-18T07:03:07.867762Z","shell.execute_reply.started":"2021-06-18T07:03:07.766915Z","shell.execute_reply":"2021-06-18T07:03:07.86688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nb_date_mean = ft_data.groupby(by='date').mean().reset_index()\n\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    res = seasonal_decompose(b_date_mean[column], period=5, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.observed)), y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.trend)), y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.seasonal)), y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.resid)), y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:03:32.02977Z","iopub.execute_input":"2021-06-18T07:03:32.030121Z","iopub.status.idle":"2021-06-18T07:03:32.15766Z","shell.execute_reply.started":"2021-06-18T07:03:32.030092Z","shell.execute_reply":"2021-06-18T07:03:32.156767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\nax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:03:50.979186Z","iopub.execute_input":"2021-06-18T07:03:50.979529Z","iopub.status.idle":"2021-06-18T07:03:51.437893Z","shell.execute_reply.started":"2021-06-18T07:03:50.979502Z","shell.execute_reply":"2021-06-18T07:03:51.436715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\nax[0].set_ylim(-1.1,1.1)\nax[1].set_ylim(-1.1,1.1)\n\n\nplot_pacf(b_date_mean['Negative Sentiment'],lags=5, ax=ax[0],title='Partial Autocorrelation Negative')\nplot_pacf(b_date_mean['Positive Sentiment'],lags=5, ax=ax[1],color='tab:green',title='Partial Autocorrelation Positive')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:04:08.655943Z","iopub.execute_input":"2021-06-18T07:04:08.656297Z","iopub.status.idle":"2021-06-18T07:04:09.102138Z","shell.execute_reply.started":"2021-06-18T07:04:08.65627Z","shell.execute_reply":"2021-06-18T07:04:09.101169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ar_1 = AutoReg(endog=b_date_mean['Positive Sentiment'],lags=1,trend='n',old_names=True).fit()\nfig = plt.figure(figsize=(16,9))\nfig = ar_1.plot_diagnostics(fig=fig, lags=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:04:33.526274Z","iopub.execute_input":"2021-06-18T07:04:33.526741Z","iopub.status.idle":"2021-06-18T07:04:34.631106Z","shell.execute_reply.started":"2021-06-18T07:04:33.526709Z","shell.execute_reply":"2021-06-18T07:04:34.630192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_AR_1 = ar_1.predict()\n\noutput = pd.DataFrame({'Prediction':predicted_AR_1,'Actual':b_date_mean['Positive Sentiment']})\n\nfig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean.date,\n        y=output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=b_date_mean.date,\n        y=output[\"Prediction\"],\n        mode=\"lines+markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"lines+markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:04:54.212733Z","iopub.execute_input":"2021-06-18T07:04:54.213109Z","iopub.status.idle":"2021-06-18T07:04:54.303113Z","shell.execute_reply.started":"2021-06-18T07:04:54.213079Z","shell.execute_reply":"2021-06-18T07:04:54.302146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ar_1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:05:16.464467Z","iopub.execute_input":"2021-06-18T07:05:16.464809Z","iopub.status.idle":"2021-06-18T07:05:16.477593Z","shell.execute_reply.started":"2021-06-18T07:05:16.464784Z","shell.execute_reply":"2021-06-18T07:05:16.476589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_date_mean = ft_data.groupby(by='date').mean().reset_index()\nb_date_std = ft_data.groupby(by='date').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Positive Sentiment',  'Daily Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:05:32.049646Z","iopub.execute_input":"2021-06-18T07:05:32.050041Z","iopub.status.idle":"2021-06-18T07:05:32.135634Z","shell.execute_reply.started":"2021-06-18T07:05:32.050009Z","shell.execute_reply":"2021-06-18T07:05:32.134298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =f_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =f_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:06:47.286126Z","iopub.execute_input":"2021-06-18T07:06:47.286467Z","iopub.status.idle":"2021-06-18T07:06:47.342657Z","shell.execute_reply.started":"2021-06-18T07:06:47.286439Z","shell.execute_reply":"2021-06-18T07:06:47.341625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"had_org = body_data[body_data['# Of Organizations Mentioned']>0].copy()\norg_names = []\nfor body in had_org.body:\n    org_names+=[str(tok) for tok in nlps(body).ents if tok.label_ == 'ORG' ]\norg_freq=dict(nltk.FreqDist(org_names))\norg_freq = {k: v for k, v in sorted(org_freq.items(), key=lambda item: item[1],reverse=True)}\ntop_10_org = list(org_freq.keys())[:10]\nmask = []\nindx = []\nfor idx,b in enumerate(had_org.body):\n    for m in top_10_org:\n        if b.find(m) !=-1:\n            mask.append(m)\n            indx.append(idx)\n            break\n\ntop_10_org_df = had_org.iloc[indx,:].copy()\ntop_10_org_df['Organization'] = mask\n\nby_org = top_10_org_df.groupby('Organization').mean().reset_index()\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['Positive Sentiment'],name='Positive Sentiment',marker_color='lightgreen'))\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['Negative Sentiment'],name='Negative Sentiment',marker_color='salmon'))\nfig.update_layout(barmode='group',title='Average Sentiment Intensity In The Top 10 Discussed Organization')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:11:19.824572Z","iopub.execute_input":"2021-06-18T07:11:19.82492Z","iopub.status.idle":"2021-06-18T07:11:20.358939Z","shell.execute_reply.started":"2021-06-18T07:11:19.824893Z","shell.execute_reply":"2021-06-18T07:11:20.357931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['# Of Words'],name='Number Of Words',marker_color='skyblue'))\nfig.add_trace(go.Bar(x=by_org['Organization'],y=by_org['# Of StopWords'],name='Number Of Stopwords',marker_color='salmon'))\nfig.update_layout(barmode='group',title='Average Naive Text Attributes The Top 10 Discussed Organization')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:11:42.733636Z","iopub.execute_input":"2021-06-18T07:11:42.733954Z","iopub.status.idle":"2021-06-18T07:11:42.75424Z","shell.execute_reply.started":"2021-06-18T07:11:42.733929Z","shell.execute_reply":"2021-06-18T07:11:42.753538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_10_text_bulk = [' '.join(top_10_org_df[top_10_org_df['Organization'] == i].body) for i in top_10_org]\ntop_org_freqs = []\nstopwords = nltk.corpus.stopwords.words('english')\nfor i in top_10_text_bulk:\n    freq = dict(nltk.FreqDist(i.split(' ')))\n    freq = {k: v for k, v in sorted(freq.items(), key=lambda item: item[1],reverse=True)}\n    freq = {k: v for k, v in freq.items() if k not in stopwords and len(k)>1}\n    top_org_freqs.append(freq)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:12:00.064285Z","iopub.execute_input":"2021-06-18T07:12:00.064736Z","iopub.status.idle":"2021-06-18T07:12:00.088969Z","shell.execute_reply.started":"2021-06-18T07:12:00.064695Z","shell.execute_reply":"2021-06-18T07:12:00.088149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\n\nfor freq,org in zip(top_org_freqs,top_10_org):\n    fig.add_trace(\n        go.Bar(\n            x = list(freq.keys())[1:11],\n            y = list(freq.values())[1:11],\n            name = org\n        )\n    )\n    \n\nbtns = []\nfor x,col in enumerate(top_10_org):\n    bol = [False]*12\n    bol[x]=True\n    d = dict(label = col,\n                  method = 'update',\n                  args = [{'visible':bol},\n                          {'title': 'Distribution Of Top 10 Words in [' +col+'] Related Posts',\n                           'showlegend':True}])\n    btns.append(d)\n    \n    \nfig.update_layout(title='Distribution Of 10 Most Common Words In Different Organization Related Posts',\n    updatemenus=[go.layout.Updatemenu(\n        active=0,\n        showactive=True,\n        buttons=btns\n        )\n    ])\n\nfig.update_xaxes(title_text='Word')\nfig.update_yaxes(title_text='Appearances')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:12:19.120206Z","iopub.execute_input":"2021-06-18T07:12:19.120559Z","iopub.status.idle":"2021-06-18T07:12:19.169643Z","shell.execute_reply.started":"2021-06-18T07:12:19.12053Z","shell.execute_reply":"2021-06-18T07:12:19.168883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUMBER_OF_COMPONENTS = 2\n\n#Preprocessing Top10 Organization Post Bodies\ntext_data = top_10_org_df.body.copy()\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\n\n#Vectorization\n\nCVZ = CountVectorizer()\nReduction_Pipeline = Pipeline(steps=[('scale',StandardScaler(with_mean=False)),('iso',Isomap(n_components= NUMBER_OF_COMPONENTS))])\n\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = Reduction_Pipeline.fit_transform(C_vector)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:12:44.673722Z","iopub.execute_input":"2021-06-18T07:12:44.674072Z","iopub.status.idle":"2021-06-18T07:12:44.836531Z","shell.execute_reply.started":"2021-06-18T07:12:44.674044Z","shell.execute_reply":"2021-06-18T07:12:44.835426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_top_10_df= top_10_org_df.reset_index().copy()\ntemp = pd.DataFrame(pc_matrix,columns=['Dim1','Dim2'])['Dim1']\ndec_top_10_df['Dim1'] = pc_matrix[:,0]\ndec_top_10_df['Dim2'] = pc_matrix[:,1]\nfig = ex.scatter(dec_top_10_df,x='Dim1',y='Dim2',color='Organization')\nfig.update_layout(title='Vectorized Post Body Text Projected On to 2D Space Using Isomap')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:12:56.886243Z","iopub.execute_input":"2021-06-18T07:12:56.886571Z","iopub.status.idle":"2021-06-18T07:12:56.979123Z","shell.execute_reply.started":"2021-06-18T07:12:56.886543Z","shell.execute_reply":"2021-06-18T07:12:56.978444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DBS = DBSCAN(eps=0.8,min_samples=15)\nDBS.fit(dec_top_10_df[['Dim1','Dim2']])\ndec_top_10_df['Cluster']  = DBS.labels_\nfig = ex.scatter(dec_top_10_df,x='Dim1',y='Dim2',color='Cluster')\nfig.update_layout(title='Using DBSCAN To Cluster All Anomalies Which Deviate From The Main Span')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:13:12.688734Z","iopub.execute_input":"2021-06-18T07:13:12.689239Z","iopub.status.idle":"2021-06-18T07:13:12.779956Z","shell.execute_reply.started":"2021-06-18T07:13:12.68921Z","shell.execute_reply":"2021-06-18T07:13:12.779196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anam = dec_top_10_df.query('Cluster == -1')\nfig = ex.pie(anam,names='Organization')\nfig.update_layout(title='Proportion Each Organization Appears As An Anomaly')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:13:51.257808Z","iopub.execute_input":"2021-06-18T07:13:51.258144Z","iopub.status.idle":"2021-06-18T07:13:51.328964Z","shell.execute_reply.started":"2021-06-18T07:13:51.258117Z","shell.execute_reply":"2021-06-18T07:13:51.328026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anam_text = ' '.join(anam.body)\nawc = WordCloud(width=600,height=500,background_color='white',stopwords=stopwords).generate(anam_text)\n\nplt.title('Most Frequent Words In Anomaly​ Labeld Posts',fontsize=17,fontweight='bold')\nplt.imshow(awc)\nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:14:02.415038Z","iopub.execute_input":"2021-06-18T07:14:02.415356Z","iopub.status.idle":"2021-06-18T07:14:03.479244Z","shell.execute_reply.started":"2021-06-18T07:14:02.415329Z","shell.execute_reply":"2021-06-18T07:14:03.478097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Distriubtion Of Sentiments Across Observed Anomalies',fontsize=19,fontweight='bold')\nsns.kdeplot(anam['Negative Sentiment'],label='Negative Sentiment')\nsns.kdeplot(anam['Positive Sentiment'],label='Positive Sentiment')\nsns.kdeplot(anam['Neutral Sentiment'],label='Neutral Sentiment')\nplt.xlabel('Sentiment Strength',fontsize=13,fontweight='bold')\nplt.ylabel('Density',fontsize=13,fontweight='bold')\nplt.xticks(np.arange(0,1.05,0.05),rotation=-45)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:14:15.526016Z","iopub.execute_input":"2021-06-18T07:14:15.526377Z","iopub.status.idle":"2021-06-18T07:14:15.951394Z","shell.execute_reply.started":"2021-06-18T07:14:15.52635Z","shell.execute_reply":"2021-06-18T07:14:15.95036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"firt_mode = anam[anam['Negative Sentiment'].between(0,0.03)]\nsec_mode = anam[anam['Negative Sentiment'].between(0.03,0.25)]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:14:28.739522Z","iopub.execute_input":"2021-06-18T07:14:28.739852Z","iopub.status.idle":"2021-06-18T07:14:28.747033Z","shell.execute_reply.started":"2021-06-18T07:14:28.739824Z","shell.execute_reply":"2021-06-18T07:14:28.746208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anam_count = anam.groupby('timestamp').count().reset_index()\nfig = ex.line(anam_count,x='timestamp',y='body')\nfig.update_layout(title='Number Of Anomalies Observed Per Time-Stamp')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:14:38.848492Z","iopub.execute_input":"2021-06-18T07:14:38.848819Z","iopub.status.idle":"2021-06-18T07:14:38.917269Z","shell.execute_reply.started":"2021-06-18T07:14:38.848788Z","shell.execute_reply":"2021-06-18T07:14:38.916623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pymc3 as pm\n\nanam_count_observed = anam_count['index']\nN = 15000\nwith pm.Model():\n    \n    LAMB = pm.Uniform('lambda',0,10)\n    \n    observed = pm.Poisson('obs',LAMB,observed=anam_count_observed)\n    \n    step = pm.Metropolis()\n    trace = pm.sample(N,step=step)\n    burned_trace = trace[int(0.8 * N):]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:14:51.921949Z","iopub.execute_input":"2021-06-18T07:14:51.922456Z","iopub.status.idle":"2021-06-18T07:15:42.01509Z","shell.execute_reply.started":"2021-06-18T07:14:51.922426Z","shell.execute_reply":"2021-06-18T07:15:42.0139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Posterior Distribution Of $\\lambda$',fontsize=18,fontweight='bold')\nplt.hist(burned_trace['lambda'],label='Values of $\\lambda$ ')\nplt.legend(prop=dict(size=18))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:15:42.016942Z","iopub.execute_input":"2021-06-18T07:15:42.017269Z","iopub.status.idle":"2021-06-18T07:15:42.461918Z","shell.execute_reply.started":"2021-06-18T07:15:42.017239Z","shell.execute_reply":"2021-06-18T07:15:42.46105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.stats as stats\nplt.title('Overlayed Distribution With Infered $\\lambda$',fontsize=18,fontweight='bold')\nRANGE = np.arange(0,100,1)\nmean_l = np.mean(burned_trace['lambda'])\nplt.hist(anam_count_observed,color='grey',label='Daily Anomaly Count Distribution')\nplt.plot(RANGE,stats.poisson.pmf(RANGE,mu=np.mean(burned_trace['lambda']))*100,'r-',lw=4,label=f'Poission Distribution with $\\lambda = {np.round(mean_l,2)}$')\nplt.legend(prop=dict(size=20))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:15:42.463133Z","iopub.execute_input":"2021-06-18T07:15:42.463388Z","iopub.status.idle":"2021-06-18T07:15:42.834313Z","shell.execute_reply.started":"2021-06-18T07:15:42.463364Z","shell.execute_reply":"2021-06-18T07:15:42.832643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 8))\nax.set_title('Daily Anomaly Count Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(anam_count['body'],ax=ax,lw=3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:15:43.935247Z","iopub.execute_input":"2021-06-18T07:15:43.935732Z","iopub.status.idle":"2021-06-18T07:15:44.204144Z","shell.execute_reply.started":"2021-06-18T07:15:43.935701Z","shell.execute_reply":"2021-06-18T07:15:44.203452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anam_freq = dict(nltk.FreqDist([i for i in anam_text.split(' ') if i not in stopwords and len(i)>1 ]))\nanam_freq = {k: v for k, v in sorted(anam_freq.items(), key=lambda item: item[1],reverse=True)}\ntop_10_pos = list(anam_freq.keys())[:10]\n\ntoken=nltk.word_tokenize(' '.join(anam.body))\npos_bigram=ngrams(token,2)\npos_bigram_dict = dict()\npos_trigram =ngrams(token,3)\npos_trigram = [k for k in pos_trigram if k[0] in top_10_pos]\n\n\nfor i in pos_bigram:\n    pos_bigram_dict[i] = pos_bigram_dict.get(i,0)+1\n        \npos_trigram_dict = dict()\n\nfor i in pos_trigram:\n    pos_trigram_dict[i] = pos_trigram_dict.get(i,0)+1\n\n\npos_trigram_df = pd.DataFrame(random.sample(list(pos_trigram_dict.keys()),k=15),columns=['One Of Top 10 Words','Second Word','Third Word'])\n\ndef get_prob(sir):\n    key = (sir['One Of Top 10 Words'],sir['Second Word'],sir['Third Word'])\n    w3 = pos_trigram_dict[key]\n    w2 = pos_bigram_dict[(sir['One Of Top 10 Words'],sir['Second Word'])]\n    return w3/w2\n\npos_trigram_df['Probabilty Of Sentence'] = pos_trigram_df.apply(get_prob,axis=1)\n\npos_trigram_df.style.background_gradient(subset='Probabilty Of Sentence',cmap='vlag')","metadata":{"execution":{"iopub.status.busy":"2021-06-18T07:16:47.074769Z","iopub.execute_input":"2021-06-18T07:16:47.075203Z","iopub.status.idle":"2021-06-18T07:16:47.145326Z","shell.execute_reply.started":"2021-06-18T07:16:47.075171Z","shell.execute_reply":"2021-06-18T07:16:47.144552Z"},"trusted":true},"execution_count":null,"outputs":[]}]}