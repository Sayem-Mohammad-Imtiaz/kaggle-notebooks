{"cells":[{"metadata":{"_uuid":"4092ec02cad1c1251d6a82cee9a2c7e7b689558c"},"cell_type":"markdown","source":"**What is the differences between RNN & LSTM?**\n\nBefore I tell you what is LSTM let me tell you about the biggest problem with RNNs. So far everything looks good about RNNs until we train it via back-propagation. As the gradient of our training samples gets propagated backward through our network, it gets weaker and weaker, by the time it gets to those neurons that represent older data points in our time-series it has no juice to adjust them properly. This problem is called Vanishing Gradient. A LSTM cell is a type of RNN which stores important information about the past and forgets the unimportant pieces. In this way, when gradient back-propagates, it wonâ€™t be consumed by unnecessary information."},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://ibb.co/rv1GT5D\"><img src=\"https://i.ibb.co/fQPYBrj/rnn-lstm.png\" alt=\"rnn-lstm\" border=\"0\"></a>"},{"metadata":{"_uuid":"c855a92a8980bdb670c8342db36884b2effb11a1"},"cell_type":"markdown","source":"*We will use our 50 data to predict 51th data. Also I will take first 200000 data*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"bit_data=pd.read_csv(\"../input/bitstampUSD_1-min_data_2012-01-01_to_2019-03-13.csv\")\nbit_data[\"date\"]=pd.to_datetime(bit_data[\"Timestamp\"],unit=\"s\").dt.date\ngroup=bit_data.groupby(\"date\")\ndata=group[\"Close\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcb79be1ca66fff1bb9186622afafa3ff1976d76"},"cell_type":"markdown","source":"What I did here? I added a colum which is \"date\" and I converted \"Timestamp\" columns to date form."},{"metadata":{"trusted":true,"_uuid":"1a5deff671724737f5cf6563da1f9bcc383fea29"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef6c5d1afd2e2ad8f8db6502d6ab19dd5536f4df"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a33c39a4aa1e344c8fa50d2ca56961c37b0ebf65"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05cc03451f03406b8c1138bee571ada74a1de08b"},"cell_type":"markdown","source":"The goal is making a prediction of daily Close data. So we will predict \"close\" values of bitcoin data"},{"metadata":{"_uuid":"34da6f362adbd1c072fa82e4b10818fa79f8f2cb"},"cell_type":"markdown","source":"**First I will use RNN to predict our data**"},{"metadata":{"_uuid":"a41a90996c44e924ec017ff19af7fe892a51ede0"},"cell_type":"markdown","source":"I am separating last 50 rows as the test data."},{"metadata":{"trusted":true,"_uuid":"757a234889f2a67e648db4273c4a4c4db74d803a"},"cell_type":"code","source":"close_train=data.iloc[:len(data)-50]\nclose_test=data.iloc[len(close_train):]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73b47c588fa00f144c1a87c335d0babe45918a10"},"cell_type":"markdown","source":"Here I will set our values between 0-1 in order to avoid domination of high values."},{"metadata":{"trusted":true,"_uuid":"d30f46e2cdcaa483eb9bce041ba08a898cdd1533"},"cell_type":"code","source":"#feature scalling (set values between 0-1)\nclose_train=np.array(close_train)\nclose_train=close_train.reshape(close_train.shape[0],1)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler(feature_range=(0,1))\nclose_scaled=scaler.fit_transform(close_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac63ba26bca53cf1f5df75c31d3dc6268f27eaeb"},"cell_type":"markdown","source":"Let's choose each 50 data as x-train and 51th as y-train"},{"metadata":{"trusted":true,"_uuid":"7874ac0b39a80e5d013d4c9cdc19f59c93d0440a","scrolled":true},"cell_type":"code","source":"timestep=50\nx_train=[]\ny_train=[]\n\nfor i in range(timestep,close_scaled.shape[0]):\n    x_train.append(close_scaled[i-timestep:i,0])\n    y_train.append(close_scaled[i,0])\n\nx_train,y_train=np.array(x_train),np.array(y_train)\nx_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1) #reshaped for RNN\nprint(\"x_train shape= \",x_train.shape)\nprint(\"y_train shape= \",y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"246a5606d4768110cd8c56ece1b93ff0d3c1a9fa"},"cell_type":"markdown","source":"Time to prepare and run our RNN method"},{"metadata":{"trusted":true,"_uuid":"95e9302fe14e8af6e55d3c32775ba8e266b839cd","scrolled":false,"_kg_hide-output":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, Dropout,Flatten\n\nregressor=Sequential()\n#first RNN layer\nregressor.add(SimpleRNN(128,activation=\"relu\",return_sequences=True,input_shape=(x_train.shape[1],1)))\nregressor.add(Dropout(0.25))\n#second RNN layer\nregressor.add(SimpleRNN(256,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.25))\n#third RNN layer\nregressor.add(SimpleRNN(512,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.35))\n#fourth RNN layer\nregressor.add(SimpleRNN(256,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.25))\n#fifth RNN layer\nregressor.add(SimpleRNN(128,activation=\"relu\",return_sequences=True))\nregressor.add(Dropout(0.25))\n#convert the matrix to 1-line\nregressor.add(Flatten())\n#output layer\nregressor.add(Dense(1))\n\nregressor.compile(optimizer=\"adam\",loss=\"mean_squared_error\")\nregressor.fit(x_train,y_train,epochs=100,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d1786f25bf66d0e666d23252377b5bff9196a70"},"cell_type":"markdown","source":"Now we are preparing our test data for prediction"},{"metadata":{"trusted":true,"_uuid":"789694f60ef54c6fe588978c7917c05f140f501e"},"cell_type":"code","source":"inputs=data[len(data)-len(close_test)-timestep:]\ninputs=inputs.values.reshape(-1,1)\ninputs=scaler.transform(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b64f25de4f2c3d3e97ff67430e63b730808fb29b"},"cell_type":"code","source":"x_test=[]\nfor i in range(timestep,inputs.shape[0]):\n    x_test.append(inputs[i-timestep:i,0])\nx_test=np.array(x_test)\nx_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab9b3f5f19e8cdb8b6fa1be54e710fba4b4281dd"},"cell_type":"code","source":"predicted_data=regressor.predict(x_test)\npredicted_data=scaler.inverse_transform(predicted_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84126a12f90fb2bea6a543dfea7c9225216bca32"},"cell_type":"markdown","source":"**What did we do???? We learnt our model with train data and then we tried to predict next 50 data (train data=50). Then we compared predictions with our test (real) data.**"},{"metadata":{"trusted":true,"_uuid":"e37a1a9195ba81668ee00aa47b98bda8a2e549c2"},"cell_type":"code","source":"data_test=np.array(close_test)\ndata_test=data_test.reshape(len(data_test),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d14bd8d6dc5dea3a2cd0b807652f41d9a34a28d1","scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(data_test,color=\"r\",label=\"true result\")\nplt.plot(predicted_data,color=\"b\",label=\"predicted result\")\nplt.legend()\nplt.xlabel(\"Time(50 days)\")\nplt.ylabel(\"Close Values\")\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6a9163de2304f9bceb4e4d3337ae7347a1c1144"},"cell_type":"markdown","source":"Here we have the results of our prediction. As we can see on the chart that with RNN method we don't have a good result. So let's check LSTM method, then we can compare both results."},{"metadata":{"trusted":true,"_uuid":"a86a7a09336182de5b84b8fd747c47216090f712"},"cell_type":"markdown","source":"**Keep going with LSTM method**"},{"metadata":{"trusted":true,"_uuid":"6fbf6c5708d1ffba850644a767a33192fdd2cdd7"},"cell_type":"markdown","source":"I will use the same train and test data"},{"metadata":{"trusted":true,"_uuid":"6b20a92dc41d1e62aaea366a4d0c96afe0dab1e3","scrolled":false,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout,Flatten\n\nmodel=Sequential()\n\nmodel.add(LSTM(10,input_shape=(None,1),activation=\"relu\"))\n\nmodel.add(Dense(1))\n\nmodel.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n\nmodel.fit(x_train,y_train,epochs=100,batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"add16d8c9ba53ae8abccfb8e46f33c6586e0ca65"},"cell_type":"markdown","source":"Keep going with test data"},{"metadata":{"trusted":true,"_uuid":"3565fca8454345628ef64e3c04478463fca1ff74"},"cell_type":"code","source":"inputs=data[len(data)-len(close_test)-timestep:]\ninputs=inputs.values.reshape(-1,1)\ninputs=scaler.transform(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46cde05f13fb1ca04c2d0c980d1644d975dd3870"},"cell_type":"code","source":"x_test=[]\nfor i in range(timestep,inputs.shape[0]):\n    x_test.append(inputs[i-timestep:i,0])\nx_test=np.array(x_test)\nx_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62794ffaa6a6b6bc64232424c03436edbb7b6825"},"cell_type":"markdown","source":"it's time to predict"},{"metadata":{"trusted":true,"_uuid":"a837c6f9642fba4332ec63e89f99d2a876f6ea3f"},"cell_type":"code","source":"predicted_data=model.predict(x_test)\npredicted_data=scaler.inverse_transform(predicted_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7111f36c85adace9f4d965d6dbe9fc4cf344139b"},"cell_type":"code","source":"data_test=np.array(close_test)\ndata_test=data_test.reshape(len(data_test),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa7d96f41874e8d3207f30ddb50e0b9c658e759a"},"cell_type":"code","source":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(data_test,color=\"r\",label=\"true result\")\nplt.plot(predicted_data,color=\"b\",label=\"predicted result\")\nplt.legend()\nplt.xlabel(\"Time(50 days)\")\nplt.ylabel(\"Close Values\")\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76aa7f58212d6a4f3453f33eea23c4fbec6f4c42"},"cell_type":"markdown","source":"* ** Conclusion**\n\nWhen we compare the both results (RNN and LSTM) we can see that we have better prediction with LSTM. \n\nUp to here I was trying to show the differences between RNN - LSTM and how to use these methods on time series. \n\nIf you have any question or I have any error please write me dirctly.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}