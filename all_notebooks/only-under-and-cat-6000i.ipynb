{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport scikitplot as skplt\n\nfrom keras import callbacks\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 20)\npd.set_option('display.max_rows', 10)\n\nplt.rc('figure', figsize=(10, 7))\n\nnum_epoch = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Structure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')\ndata.drop(columns=\"Unnamed: 0\", axis=1, inplace=True)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"department_list = data['Department Name'].dropna().unique()\ndepartment_list = [x.lower() for x in department_list]\ndepartment_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_list = data['Class Name'].dropna().unique()\nclass_list = [x.lower() for x in class_list]\nclass_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"department_and_class = np.concatenate((department_list, class_list, ['dress', 'petite', 'petit', 'skirt', 'shirt', 'jacket', 'intimate', 'blouse', 'coat', 'sweater']), axis=0)\ndepartment_and_class","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_data = data[['Review Text','Recommended IND']]\nreview_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_data.isnull().sum().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_data.dropna(axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import for test train split and vect\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef tfidf(data):\n    tfidf_vectorizer =TfidfVectorizer(min_df=3,  max_features=None, \n             analyzer='word', use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import  TruncatedSVD\nimport matplotlib\nimport matplotlib.patches as mpatches\n\n\ndef plot_LSA(test_data, test_labels):\n        #reduce into 2 dimensions using svd \n        lsa = TruncatedSVD(n_components=2)\n        #fits to the train data\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue','blue']\n        if plt:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            red_patch = mpatches.Patch(color='orange', label='Recommended IND = 0')\n            blue_patch = mpatches.Patch(color='blue', label='Recommended IND = 1')\n            plt.legend(handles=[red_patch, blue_patch], prop={'size': 12})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = review_data[\"Review Text\"]\ny = review_data[\"Recommended IND\"]\n\n# Create sequence\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(review_data['Review Text'])\nvocabulary_size = len(tokenizer.word_index) + 1\nprint(vocabulary_size)\n\n# 限制最长长度为70，过长截断，过短就在后方（post）补齐\nmax_length = 70\n\nsequences = tokenizer.texts_to_sequences(X)\nfeatures = pad_sequences(sequences, maxlen=max_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Review Text Feature Transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\n\n# if you don't have stopwords and have some error, please use the download code bollow!\n# nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n### Text Normalizing function. Part of the following function was taken from this link. \ndef clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    text = [w for w in text if not w in stop_words]\n    \n    text = \" \".join(text)\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n#     text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    ## Stemming\n    text = text.split()\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_data['Review Text'] = review_data['Review Text'].map(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类。\n\nword_index: 字典，将单词（字符串）映射为它们的排名或者索引。仅在调用fit_on_texts之后设置。\n\ntexts_to_sequences(texts)\n\ntexts：待转为序列的文本列表\n\n返回值：序列的列表，列表中每个序列对应于一段输入文本\n\npad_sequences 将多个序列截断或补齐为相同长度。\n\n该函数将一个 num_samples 的序列（整数列表）转化为一个 2D Numpy 矩阵，其尺寸为 (num_samples, num_timesteps)。 num_timesteps 要么是给定的 maxlen 参数，要么是最长序列的长度。\n\n比 num_timesteps 短的序列将在末端以 value 值补齐。\n\n比 num_timesteps 长的序列将会被截断以满足所需要的长度。补齐或截断发生的位置分别由参数 pading 和 truncating 决定。\n\n向前补齐为默认操作。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\n\nX = review_data[\"Review Text\"]\ny = review_data[\"Recommended IND\"]\n\n# Create sequence\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(review_data['Review Text'])\nvocabulary_size = len(tokenizer.word_index) + 1\nprint(vocabulary_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(review_data['Review Text'])\nnp.max([len(x) for x in sequences])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 限制最长长度为70，过长截断，过短就在后方（post）补齐\nmax_length = 70\npadded_features = pad_sequences(sequences, maxlen=max_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_LSA(padded_features, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import interp\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\nfrom sklearn.metrics import roc_curve, auc\n\ndef plot_roc(n_classes, y_test, y_score, title, class_name_list):\n    # Plot linewidth.\n    lw = 2\n\n    y_test = sentiment_test[1]\n    y_score = test_score\n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Compute macro-average ROC curve and ROC area\n\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr /= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure(1)\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 label='ROC curve of class {0} (area = {1:0.2f})'\n                 ''.format(class_name_list[i], roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"lower right\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recommended IND Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(review_data['Review Text'], review_data['Recommended IND'], test_size=0.2, random_state=666)\nX_test, X_val, y_test, y_val = model_selection.train_test_split(X_val, y_val, test_size=0.5, random_state=888)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_train))\nprint(len(X_val))\nprint(len(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"弱智算法1：0.4undersampling，0.4oversamplling，0.2是取随机取句子的前半句或者后半句生成新的数据。这样的理由是，有部分用户会在评论一开始或者最后面强烈表达自己的情感。所以使用这样的方式来做数据增强有利于分类。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler()\nros = RandomOverSampler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_under, X_cat, y_under, y_cat = model_selection.train_test_split(X_train, y_train, test_size=0.4, random_state=888)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_under.shape,y_under.shape, X_cat.shape, y_cat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rus_sequences = tokenizer.texts_to_sequences(X_under)\nrus_features = pad_sequences(rus_sequences, maxlen=max_length, padding='post')\ntrain_X_rus, train_y_rus = rus.fit_sample(rus_features, y_under)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X_rus.shape, train_y_rus.shape)\ntrain_y_rus.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_cat.shape, y_cat.shape)\ny_cat.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cat.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_0_idx = y_cat[y_cat == 0]\ncat_0_idx = list(cat_0_idx.keys())\ncat_1_idx = y_cat[y_cat == 1]\ncat_1_idx = list(cat_1_idx.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat_0 = X_cat[cat_0_idx]\nX_cat_1 = X_cat[cat_1_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_0 = len(cat_0_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nnew_X_0 = []\nfor idx in cat_0_idx:\n    cur = X_cat_0[idx]\n    p = random.randint(0, 1)\n    cur_idx = len(cur) // 2\n    cur = cur[:cur_idx] if p == 0 else cur[cur_idx:]\n    new_X_0.append(cur)\nnew_X_0.extend(list(X_cat_0.values))\n\nnew_X_1 = []\nfor idx in cat_1_idx:\n    cur = X_cat_1[idx]\n    p = random.randint(0, 1)\n    cur_idx = len(cur) // 2\n    cur = cur[:cur_idx] if p == 0 else cur[cur_idx:]\n    new_X_1.append(cur)\nnew_X_1 = random.sample(new_X_1, count_0)\nnew_X_1.extend(random.sample(list(X_cat_1.values), count_0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(new_X_0))\nprint(len(new_X_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(new_X_0 + new_X_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cat = pd.Series(new_X_0 + new_X_1)\ny_cat = pd.Series([0] * count_0 * 2 + [1] * count_0 * 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_sequences = tokenizer.texts_to_sequences(X_cat)\ncat_features = pad_sequences(cat_sequences, maxlen=max_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X_rus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_all = np.concatenate((cat_features, train_X_rus))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_all = y_cat.append(train_y_rus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_all = to_categorical(y_all)\nlabels_all[0]\nprint(labels_all.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_LSA(features_all, y_all)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_sequences = tokenizer.texts_to_sequences(X_val)\nval_features = pad_sequences(val_sequences, maxlen=max_length, padding='post')\nval_labels = to_categorical(y_val)\nprint(val_features.shape, val_labels.shape)\n\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_features = pad_sequences(test_sequences, maxlen=max_length, padding='post')\ntest_labels = to_categorical(y_test)\nprint(test_features.shape, test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(features_all, labels_all, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels), shuffle=True)\n\nscore = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - hyper') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nclass_weight = {0: 5, 1: 1}\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(features_all, labels_all, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels), shuffle=True, class_weight=class_weight)\n\nscore = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - hyper') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"虽然好评的recall和差评的prec都下降了，好评的prec和差评的recall都很高，但是我觉得是合理的：如果要做评论精选，比如把好评放在前面，那么好评prec高是合理的，说明给用户看的评论基本都是好评；如果店家想看舆情分析，就是想看自己店铺的差评，那么差评recall高是合理的，说明此时给店家看的基本都是差评。\n后期不知道多训练多几次，还有调整三种resample方式的比重，不知道效果如何。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nclass_weight = {0: 10, 1: 1}\ne = Embedding(vocabulary_size, 100, input_length=max_length, trainable=True)\nmodel.add(e)\nmodel.add(Bidirectional(LSTM(128, dropout=0.5, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256, dropout=0.5)))\nmodel.add(Dense(2, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(features_all, labels_all, epochs=num_epoch, batch_size=256, verbose=1,\n          validation_data=(val_features, val_labels), shuffle=True, class_weight=class_weight)\n\nscore = model.evaluate(test_features, test_labels, verbose=1)\n\nprint('loss : {}, acc : {}'.format(score[0], score[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_score = model.predict(test_features)\ntest_predictions = np.argmax(test_score, axis=1)\n\nclass_names = ['(0) Not recommended class', '(1) Recommended class']\nreport = classification_report(np.argmax(test_labels, axis=1), test_predictions, target_names=class_names)\nmatrix = pd.DataFrame(confusion_matrix(y_true=np.argmax(test_labels, axis=1), y_pred=test_predictions), \n                                        index=class_names, columns=class_names)\nprint(matrix)\nprint(report)\nf1_score(np.argmax(test_labels, axis=1), test_predictions, average='micro')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skplt.metrics.plot_roc(np.argmax(test_labels, axis=1), model.predict_proba(test_features),\n                      title='ROC Curves - hyper') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference\n1. https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n2. https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n3. https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}