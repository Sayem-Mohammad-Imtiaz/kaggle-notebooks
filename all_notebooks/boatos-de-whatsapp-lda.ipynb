{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introdução\n\nReferencia: Topic Modeling with Gensim (Python)\nhttps://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n\nAPI do Gensim: https://radimrehurek.com/gensim/models/ldamodel.html\n\nUtilização do método LDA para clusterizar as mensagens de WhatsApp classificadas como boatos pelo site \"boato.org\" dataset de 2018 - Kaggle","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Análise Exploratória\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Importando as bibliotecas necessárias:\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nimport re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import RSLPStemmer\nimport time\nfrom nltk import FreqDist\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport os # accessing directory structure\nfrom wordcloud import WordCloud, STOPWORDS\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2 csv files in the current version of the dataset:\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora você está pronto para ler os dados e usar as funções de plotagem para visualizar os dados.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let's check 1st file: ../input/boatos.csv","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# boatos.csv tem 1170 linhas\ndf = pd.read_csv('../input/boatos.csv', delimiter=',')\ndf.dataframeName = 'boatos.csv'\nnRow, nCol = df.shape\nprint(f'Há {nRow} linhas e {nCol} colunas')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initial_clean(text):\n    \"\"\"\n    Função para limpeza de textos de websites, emails e pontuação\n    Também converte para minúsculas\n    \"\"\"\n    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n    text = text.lower() # lower case the text\n    text = nltk.word_tokenize(text)\n    return text\n\nstop_words = stopwords.words('portuguese') #stop words in Português\ndef remove_stop_words(text):\n    \"\"\"\n    Função apra remover \"stop-words\"\n    \"\"\"\n    return [word for word in text if word not in stop_words]\n\nstemmer = nltk.stem.RSLPStemmer() # Stemmer in Portuguese\ndef stem_words(text):\n    \"\"\"\n    Função para iguralar singular e plural\n    \"\"\"\n    try:\n        text = [stemmer.stem(word) for word in text]\n        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n    except IndexError: # the word \"oed\" broke this, so needed try except\n        pass\n    return text\n\ndef apply_all(text):\n    \"\"\"\n    Aglutina todas as funções anteriores\n    \"\"\"\n    return stem_words(remove_stop_words(initial_clean(text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean text and title and create new column \"tokenized\"\nt1 = time.time()\ndf['tokenized'] = df['hoax'].apply(apply_all)\nt2 = time.time()\nprint(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first get a list of all words\nall_words = [word for item in list(df['tokenized']) for word in item]\n# use nltk fdist to get a frequency distribution of all words\nfdist = FreqDist(all_words)\nlen(fdist) # number of unique words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top= fdist.most_common()\nprint(top)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nk = 50000\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose k and visually inspect the bottom 10 words of the top k\n# Escolhe o valor de k e veririca visualmente as 10 palavras menos usadas das k mais frequentes\nk = 1500\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k = 50,000 é um valor elevado já que as palavras menos frequentes não são palavras corretas e ocorrem muit raramente (apenas uma vez na base).\n\nk = 1500 é um valor mais razoavel. As palvras são utilizadas pelo menos 11 vezes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# definição de uma função para encontrar as palavras mais frequentes\n# fdist.most_common(k) monta um vetor de tuplas (palavra_1,quantidade_1), (palavra_2,quantidade_2), ...\n# *fdist.most_common(k) separa os itens (tuplas) do vetor em argumentos para a função zip\n# a função zip agrega todas as palavras de cada tupla em uma nova tupla, e todos os numeros (quantidades) em outra tupla\n# Ex: (palavra_1, plavra_2, ...), (4,7,...)\n# top_k_words pega primeira tupla, a tupla das palavras\ntop_k_words,_ = zip(*fdist.most_common(k))\n# a função set torna a tupla em um conjunto de valores que não tem indices, nem pode ter itens repetidos.\n# a ordem é aleatoria e não importa\ntop_k_words = set(top_k_words)\ndef keep_top_k_words(text):\n    return [word for word in text if word in top_k_words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# aplica a função à coluna 'tokenized' para manter só as mais frequentes (palavras incomuns são removidas)\ndf['tokenized'] = df['tokenized'].apply(keep_top_k_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# O tamanho (coluna 'doc_len') é calculado\ndf['doc_len'] = df['tokenized'].apply(lambda x: len(x))\n# coloca os tamnahos em um vetor\ndoc_lengths = list(df['doc_len'])\n# remove a coluna 'doc_len' do dataframe\ndf.drop(labels='doc_len', axis=1, inplace=True)\n\nprint(\"length of list:\",len(doc_lengths),\n      \"\\naverage document length\", np.average(doc_lengths),\n      \"\\nminimum document length\", min(doc_lengths),\n      \"\\nmaximum document length\", max(doc_lengths))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot a histogram of document length\nnum_bins = 1000\nfig, ax = plt.subplots(figsize=(20,6));\n# the histogram of the data\nn, bins, patches = ax.hist(doc_lengths, num_bins)\nax.set_xlabel('Document Length (tokens)', fontsize=15)\nax.set_ylabel('Normed Frequency', fontsize=15)\nax.grid()\nax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))\nplt.xlim(0,2000)\nax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-',\n        label='average doc length')\nax.legend()\nax.grid()\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop short articles\nLDA does not work very well on short documents, which we will explain later, so we will drop some of the shorter articles here before training the model.\n\nFrom the histogram above, droping all articles less than 40 tokens seems appropriate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# MANTEM APENAS OS ARTIGOS COM MAIS DE 40 TOKENS\ndf = df[df['tokenized'].map(len) >= 40]\n\n# make sure all tokenized items are lists\ndf = df[df['tokenized'].map(type) == list]\ndf.reset_index(drop=True,inplace=True)\nprint(\"Após a limpeza e exclusão de artigos curtos, o dataframe tem agora:\", len(df), \"artigos\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Será efetuada agora a divisão do dataframe em duas partes: treinamento e teste.\n\nO conjunto de dados de treinamento será utilizado para treinar o modelo LDA, enquanto que o de teste será usado para encontrar artigos similares em nosso algoritmo de recomendação.\n\nO conjunto de dados já está embaralhado desde o começo, então não é necessário fazê-lo novamente.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria uma lista aleatoria de itens TRUE or FALSE, onde 98% é TRUE, para ser utilizado na divisão do dataframe\nmsk = np.random.rand(len(df)) < 0.98","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Divisão do dataframe usando a lista de valores TRUE e FALSE\n# para treinamento os valores TRUE\ntrain_df = df[msk]\ntrain_df.reset_index(drop=True,inplace=True)\n# para teste são usados os valores FALSE\ntest_df = df[~msk]\ntest_df.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_csv('train_df.csv')\ntest_df.to_csv('test_df.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tamanho dos conjutnos de ddos de treino e teste\nprint(len(df),len(train_df),len(test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_lda(data):\n    \"\"\"\n    Esta função treina o modelo LDA\n    Configuramos os parametros como o número de tópicos, o 'chunksize' para usar o método de Hoffman\n    Fazemos duas passagens de dados já que o dataset é pequeno, queremos que o as distribuições se estabilizem\n    \"\"\"\n    num_topics = 100\n    chunksize = 300\n    dictionary = corpora.Dictionary(data['tokenized'])\n    corpus = [dictionary.doc2bow(doc) for doc in data['tokenized']]\n    t1 = time.time()\n    # low alpha means each document is only represented by a small number of topics, and vice versa\n    # low eta means each topic is only represented by a small number of words, and vice versa\n    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n                   alpha=1e-2, eta=0.5e-2, chunksize=chunksize, minimum_probability=0.0, passes=2)\n    t2 = time.time()\n    print(\"Tempo para treinar o modelo LDA com \", len(df), \"artigos: \", (t2-t1)/60, \"min\")\n    return dictionary,corpus,lda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary,corpus,lda = train_lda(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# O método show_topics mostra as palavras mais frequentes (quantidade definida por 'num_words') na quantidade 'num_topics' de tópicos aleatórios.\nlda.show_topics(num_topics=5, num_words=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mostra um tópico especifico. argumentos: id do topico e quantidade de palavras (mais significativas)\nlda.show_topic(topicid=98, topn=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seleciona um artigo aleatoriamente de train_df\nrandom_article_index = np.random.randint(len(train_df)) # pega um numero aleatorio menor que o tamanho de train_df\nbow = dictionary.doc2bow(train_df.iloc[random_article_index,4]) # Lista de tuplas do artigo com (token_id, token_count)\nprint(random_article_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.iloc[random_article_index,4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lista os topicos do artigo escolhido aleatoriamente (primeiro item das tuplas)\ndoc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)]) # a função get_document_topics retorna a distribuição dos topicos","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gráfico da distribuição de topicos do artigo\nfig, ax = plt.subplots(figsize=(12,6));\n# Histograma dos dados\npatches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)\nax.set_xlabel('ID do tópico', fontsize=15)\nax.set_ylabel('Contribuição do tópico', fontsize=15)\nax.set_title(\"Distribuição de tópicos do artigo \" + str(random_article_index), fontsize=20)\n#ax.set_xticks(np.linspace(10,100,10))\n#fig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Os 5 tópicos que mais contribuem e suas palavras\nfor i in doc_distribution.argsort()[-5:][::-1]:\n    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* Consultas de similaridade e dados não vistos\n* Agora, voltaremos nossa atenção ao conjunto de dados de teste que o modelo ainda não viu. Embora os artigos em test_df não tenham sido vistos pelo modelo, o gensim tem uma maneira de inferir suas distribuições de tópicos, dado o modelo treinado. Obviamente, a abordagem correta para obter resultados precisos seria treinar o modelo novamente com esses novos artigos, mas isso pode ser oportuno e inviável em um cenário de caso real em que os resultados são necessários rapidamente.\n\n* Primeiro, vamos mostrar como podemos inferir tópicos do documento para um novo artigo não visto.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Escolha de um artigo aleatório dos dados de teste (test_df)\nrandom_article_index = np.random.randint(len(test_df))\nprint(random_article_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aqui está a parte importante. Ao obter a representação do BOW para este artigo não visto, o gensim apenas considera as palavras no dicionário existente que usamos para treinar o modelo. Portanto, se houver novas palavras neste artigo, elas não serão consideradas ao inferir a distribuição do tópico. Isso é bom, pois nenhum erro ocorre para palavras não vistas, mas ruim, pois algumas palavras podem ser cortadas e, portanto, podemos perder uma distribuição precisa de tópicos para este artigo.\n\nNo entanto, atenuamos esse risco porque o conjunto de treinamento é muito representativo de todo o corpus; 98% das observações estão no conjunto de treinamento, com apenas 0,02% das observações no conjunto de testes. Portanto, a maioria das palavras, se não todas, do conjunto de testes deve estar no dicionário do conjunto de treinamento.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Repetimos agora as operações efetuadas sobre um artigo escolhido de forma aleatória","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.iloc[random_article_index,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gráfico da distribuição de topicos do artigo\nfig, ax = plt.subplots(figsize=(12,6));\n# Histograma dos dados\npatches = ax.bar(np.arange(len(new_doc_distribution)), new_doc_distribution)\nax.set_xlabel('ID do tópico', fontsize=15)\nax.set_ylabel('Contribuição do tópico', fontsize=15)\nax.set_title(\"Distribuição dos tópicos para um artigo não visto\", fontsize=20)\nax.set_xticks(np.linspace(10,100,10))\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Os 5 tópicos que mais contribuem e suas palavras\nfor i in new_doc_distribution.argsort()[-5:][::-1]:\n    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}