{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src='https://img.rankedboost.com/wp-content/uploads/2016/07/Pokemon-Go-Pok%C3%A9dex.png' style='height:400px'>","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.express as px\ninit_notebook_mode(connected=True)\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary:</h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. Data <span class=\"badge badge-primary badge-pill\">2</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\">3. Model<span class=\"badge badge-primary badge-pill\">3</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"messages\">4. Data Augmentation<span class=\"badge badge-primary badge-pill\">4</span></a>\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>1 - Introduction</b></font><br><a id=\"1\"></a>\n<br> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* the purpose of this kernel is to be fun and to test the use of a convolutional neural network in the identification of pokemon we know that pokemon usually has characteristics of animals that belong to their types and very striking colors such as, for example, fire pokemons are usually red already the grass ones are green the dark ones are black and so on, so it usually has striking features to represent its types, but this is not always confirmed as in the following examples:\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://images.uncyc.org/pt/thumb/d/df/Octilery.png/230px-Octilery.png' style='height:100px'>\n\n* Octilery: it is a water type pokemon but has striking red colors its water features are striking because the pokemon is an octopus.\n\n\n<img src='https://assets.pokemon.com/assets/cms2/img/pokedex/full/324.png' style='height:150px'>\n\n* Torkoal: it is a turtle pokemon but its fire type which draws attention to its flame shape is the fact that it is a turtle with a volcano on its back.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* But how we see that pokemon of diferents types can be similiar, we can have we also have pokemon with big differences between them:\n\n<img src='https://cdn.bulbagarden.net/upload/thumb/f/fe/055Golduck.png/250px-055Golduck.png' style='height:150px'>\n\n\n<img src='https://assets.pokemon.com/assets/cms2/img/pokedex/full/077.png' style='height:150px'>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* So let's evaluate the performance of a CNN in classifying pokemon types based only on its images and ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>2 - Data</b></font><br><a id=\"2\"></a>\n<br> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* prepare our data to train CNN, lets merge each pokemons type with your image and construct the data set","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"pokemons = pd.read_csv('/kaggle/input/pokemon-images-and-types/pokemon.csv')\nnumbers = []\nfor i in range(1,pokemons.shape[0]+1):\n    numbers.append(i)\npokemons['pkn'] = numbers\nIMG_DIR = '/kaggle/input/pokemon-images-dataset/pokemon/pokemon'\nfrom os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir(IMG_DIR) if isfile(join(IMG_DIR, f))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import re\ndataframe_img = pd.DataFrame([])\nimages = []\npokemon_number = []\nfor img in onlyfiles:\n    if not re.search('-', img):\n        pkn = img.split('.')\n        n = re.sub(\"[^0-9]\", \"\", pkn[0])\n        path = IMG_DIR +'/' +str(img)\n        images.append(path)\n        pokemon_number.append(n)\ndataframe_img['images'] = images\ndataframe_img['pkn'] = pokemon_number\ndataframe_img['pkn'] = dataframe_img['pkn'].astype(int)\ndataframe_img['pkn'] = dataframe_img['pkn'].astype(int)\nresult = pokemons.merge(dataframe_img, left_on='pkn', right_on='pkn')\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now i have the data with pokemons type and your images, my objective is classify Water and Fire type know i will select them","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"select = ['Water', 'Fire']\nresult = result[result['Type1'].isin(select)]\nfig = go.Figure()\n\nfig.add_trace(go.Bar(x=[result['Type1'].value_counts().index[0]],\n                     y=[result['Type1'].value_counts()[0]],\n                     marker_color='blue',\n                     name='water'\n                     ))\n\nfig.add_trace(go.Bar(x=[result['Type1'].value_counts().index[1]],\n                     y=[result['Type1'].value_counts()[1]],\n                     marker_color='red',\n                     name='fire'\n                    ))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Pokemon Distribution',\n        'width': 500, \n        'height': 400\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have in our selection we have twice as many water type pokemon. Now i will split data in train, test and val to evaluate the CNN. So first I need construct the folder relation that keras need to split data and train my model.\n\n\n\n<br/>\n<br/>\n\n<br/>\n<br/>\n\n<img src='https://miro.medium.com/max/1682/1*HpvpA9pBJXKxaPCl5tKnLg.jpeg' style='height:200px'>\n\n\n<br/>\n<br/>\n\n<br/>\n<br/>\n\n<br/>\n<br/>\n\n* As you can see above a path is required for each split of the data and within that path a sub path with the classification classes and their sampling.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Split data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom shutil import copyfile\nos.mkdir('train/')\nos.mkdir('test/')\nos.mkdir('val/')\nfor class_ in result['Type1'].unique():\n    os.mkdir('train/'+str(class_)+'/')\n    os.mkdir('test/'+str(class_)+'/')\n    os.mkdir('val/'+str(class_)+'/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    result, result['Type1'],test_size=0.33, stratify=result['Type1'])\n\nX_test, X_val, y_test, y_val = train_test_split(\n    X_test, y_test, test_size=0.33,stratify=y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from shutil import copyfile, copy2\n\nfor image,type_  in zip(X_train['images'], y_train):\n    copy2(image, 'train/'+type_)\n\nfor image,type_ in zip(X_test['images'], y_test):\n    copy2(image, 'test/'+type_)\n    \nfor image,type_ in zip(X_val['images'], y_val):\n    copy2(image, 'val/'+type_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator()\n\ntrain = datagen.flow_from_directory('train/')\ntest = datagen.flow_from_directory('test/')\nval = datagen.flow_from_directory('val/')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - model</b></font><br><a id=\"3\"></a>\n<br> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src='https://miro.medium.com/max/2510/1*vkQ0hXDaQv57sALXAJquxA.jpeg' style='height:200px'>\n\n<br/>\n<br/>\n<br/>\n<br/>\n\n* Now we have train, test and val we can start modelling de CNN. The architecture used will be simple with 3 convolution layer, maxpooling and Dropout\n\n* Convolution Layer, maxpooling, dropout:\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization, Lambda\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndef build():\n    model = Sequential()\n    IMAGE_WIDTH = 256\n    IMAGE_HEIGHT = 256\n    IMAGE_CHANNELS = 3\n    model.add(Lambda(lambda x: x, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n    model.add(Conv2D(32, (2, 2), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(3, 3)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (2, 2), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(3, 3)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, (2, 2), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(3, 3)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(2, activation='softmax')) \n\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n\n    model.summary()\n    return model\nmodel = build()\nhistory = model.fit_generator(train, epochs=30, validation_data=val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict = model.predict_generator(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfig = go.Figure()\nepochs = []\nfor i in range(len(history.history['acc'])):\n    epochs.append(i)\nfig.add_trace(go.Scatter(x=epochs,y=history.history['acc'], mode='lines',name='train'))\nfig.add_trace(go.Scatter(x=epochs,y=history.history['val_acc'], mode='lines',name='val'))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'width': 500, \n        'height': 400\n\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* the model converged at 0.76 in val data, lets see whats happen in test data","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\npredict_frame = pd.DataFrame([])\npredict_frame['category'] = np.argmax(predict, axis=-1)\nlabels = dict((v,k) for k,v in val.class_indices.items())\npredict_frame['category'] = predict_frame['category'].replace(labels)\nprint(classification_report(y_test, predict_frame['category']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the model clearly had difficulties in classifying the fire type pokemons, and while the water ones managed to perform better","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* lets see wrongs model classfication","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def show_wrong_classification(y_test, predict, result):\n    tmp = result[result.index.isin(y_test.index)]\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\n    i=0\n    for imag, true, pred in zip(tmp['images'], tmp['Type1'], predict):\n        if true!=pred:\n            if i <3:\n                img = Image.open(imag)\n                fig = plt.figure()\n                ax[i].imshow(img)\n                ax[i].set_title(str(pred))\n                i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from PIL import Image\nshow_wrong_classification(y_test, predict_frame['category'], result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we can see we have pokemons of different types badly classified but visually ignoring their animal characteristics they are similar in relation to their colors, so let's see how the pixel rgb channels are distributed among the fire and water pokemons\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2,3,figsize=(15,10))\nk =0\nimport cv2\nlist_b =[]\nlist_r = []\nlist_g = []\nfrom tqdm import tqdm\nfor type_ in tqdm(result['Type1'].unique()):\n    tmp = result[result['Type1']==type_]\n    for img in tmp['images']:\n        img = cv2.imread(img)\n        b, g, r = cv2.split(img)\n        color = 'blue'\n        for i in b:\n            for j in i:\n                if j != 0:\n                    list_b.append(j)\n        color = 'green'\n        for i in g:\n            for j in i:\n                if j != 0:\n                    list_g.append(j)\n        color = 'red'\n        for i in r:\n            for j in i:\n                if j != 0:\n                    list_r.append(j)\n    sns.distplot(list_g, ax=axes[k, 0], color='g')\n    sns.distplot(list_b, ax=axes[k, 1], color='b')\n    sns.distplot(list_r, ax=axes[k, 2], color='r')\n    axes[k, 0].set_title('Pokemon type color channel ' + type_)\n    if type_ =='Fire':\n        list_g_f = list_g\n        list_b_f = list_b\n        list_r_f = list_r\n    else:\n        list_g_w = list_g\n        list_b_w = list_b\n        list_r_w = list_r\n    list_b =[]\n    list_r = []\n    list_g = []\n    \n    k += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The distribution of pixels in the channels is similar, mainly between the green and red channels, the blue channel is the one with the most difference in its distribution where for fire type pokemon there is a low occurrence between 200 and 255 pixel value.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ng_fire = np.std(np.array(list_g_f), axis=0)\nr_fire = np.std(np.array(list_r_f), axis=0)\nb_fire = np.std(np.array(list_b_f), axis=0)\n\n\ng_water = np.std(np.array(list_g_w), axis=0)\nr_water = np.std(np.array(list_r_w), axis=0)\nb_water = np.std(np.array(list_b_w), axis=0)\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Fire std channel\",\"Water std channel\"))\nfig.add_trace(go.Scatter(\n    y=[b_fire, g_fire, g_fire],\n    x=['blue', 'green', 'red'],\n    mode='markers',\n    marker=dict(size=[b_fire, g_fire, r_fire],\n                color=['blue', 'green', 'red'],\n                showscale=True)\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    y=[b_water, g_water, r_water],\n    x=['blue', 'green', 'red'],\n    mode='markers',\n    marker=dict(size=[b_water, g_water, r_water],\n                color=['blue', 'green', 'red'],\n                showscale=True)\n), row=1, col=2)\nfig.update_layout(showlegend=False)\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'width': 850, \n        'height': 400\n\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Water pokemon has high std in red channel and fire has low std in blue channel. Thus the sensation that water type pokemons have a red presence which ends up mischaracterizing pokemons. So lets know identify pokemons that has red presence.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"water = result[result['Type1']=='Water']\nstds = []\nvalues = []\nfor image in water['images']:\n    img = cv2.imread(image)\n    b, g, r = cv2.split(img)\n    for i in r:\n        for j in i:\n            if j != 0:\n                stds.append(j)       \n    std = np.mean(np.array(stds), axis=0)\n    values.append(std)\n    stds = []\nwater['stds'] = values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"water = water.sort_values(by='stds', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets see pokemons with higher averages in the red channel","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nj = 0\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))\nfor imag in water['images']:\n    if i == 1 and j ==3:\n        break\n    if j > 2:\n        i =1\n        j = 0\n    img = Image.open(imag)\n    fig = plt.figure()\n    ax[i][j].imshow(img)\n    j+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fire = result[result['Type1']=='Fire']\nstds = []\nvalues = []\nfor image in fire['images']:\n    img = cv2.imread(image)\n    b, g, r = cv2.split(img)\n    for i in b:\n        for j in i:\n            if j != 0:\n                stds.append(j)       \n    std = np.mean(np.array(stds), axis=0)\n    values.append(std)\n    stds = []\nfire['stds'] = values\nfire = fire.sort_values(by='stds', ascending=False)\ni = 0\nj = 0\nfig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))\nfor imag in fire['images']:\n    if i == 1 and j ==3:\n        break\n    if j > 2:\n        i =1\n        j = 0\n    img = Image.open(imag)\n    fig = plt.figure()\n    ax[i][j].imshow(img)\n    j+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>4 - Data Augmentation</b></font><br><a id=\"4\"></a>\n<br> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* To try improve our result, i will apply some image augmentation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## brightness","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of horizontal shift image augmentation\nfrom numpy import expand_dims\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom matplotlib import pyplot\nimg = load_img(water['images'][170])\ndata = img_to_array(img)\nsamples = expand_dims(data, 0)\ndatagen = ImageDataGenerator(brightness_range=[0.2,1.5])\nit = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\nfor i in range(3):\n    fig = plt.figure()\n    batch = it.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## zoom","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img = load_img(fire['images'][156])\ndata = img_to_array(img)\nsamples = expand_dims(data, 0)\ndatagen = ImageDataGenerator(zoom_range=[0.5, 1.0])\nit = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\nfor i in range(3):\n    fig = plt.figure()\n    batch = it.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## rotation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img = load_img(water['images'][118])\ndata = img_to_array(img)\nsamples = expand_dims(data, 0)\ndatagen = ImageDataGenerator(rotation_range=35)\nit = datagen.flow(samples, batch_size=1)\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\nfor i in range(3):\n    fig = plt.figure()\n    batch = it.next()\n    image = batch[0].astype('uint8')\n    ax[i].imshow(image)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagentrain = ImageDataGenerator(rotation_range=35, \n                                 zoom_range=[0.5, 1.0], \n                                 brightness_range=[0.2,1.5])\n\ndatagen = ImageDataGenerator()\n\ntrain = datagentrain.flow_from_directory('train/')\ntest = datagen.flow_from_directory('test/')\nval = datagen.flow_from_directory('val/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build()\nhistory = model.fit_generator(train, epochs=30, validation_data=val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nepochs = []\nfor i in range(len(history.history['acc'])):\n    epochs.append(i)\nfig.add_trace(go.Scatter(x=epochs,y=history.history['acc'], mode='lines',name='train'))\nfig.add_trace(go.Scatter(x=epochs,y=history.history['val_acc'], mode='lines',name='val'))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'width': 500, \n        'height': 400\n\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_frame = pd.DataFrame([])\npredict = model.predict_generator(test)\npredict_frame['category'] = np.argmax(predict, axis=-1)\nlabels = dict((v,k) for k,v in val.class_indices.items())\npredict_frame['category'] = predict_frame['category'].replace(labels)\nprint(classification_report(y_test, predict_frame['category']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wrong_classification(y_test, predict_frame['category'], result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So a classification of the fire type pokemon is impaired due to the red channel of the water type pokemon being very difficult to perform the classification correctly even with augmentation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}