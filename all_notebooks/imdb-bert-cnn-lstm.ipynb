{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"!pip install bert-for-tf2","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:07.134925Z","iopub.execute_input":"2021-08-04T13:25:07.135378Z","iopub.status.idle":"2021-08-04T13:25:20.775165Z","shell.execute_reply.started":"2021-08-04T13:25:07.135333Z","shell.execute_reply":"2021-08-04T13:25:20.774249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport re\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:20.777743Z","iopub.execute_input":"2021-08-04T13:25:20.778011Z","iopub.status.idle":"2021-08-04T13:25:21.565827Z","shell.execute_reply.started":"2021-08-04T13:25:20.777983Z","shell.execute_reply":"2021-08-04T13:25:21.564912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport transformers\nimport bert \nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer \nfrom tensorflow import keras\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Input,  Dropout, Conv1D, Flatten\nfrom tensorflow.keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.models import Model,Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:21.567899Z","iopub.execute_input":"2021-08-04T13:25:21.56833Z","iopub.status.idle":"2021-08-04T13:25:27.088997Z","shell.execute_reply.started":"2021-08-04T13:25:21.568288Z","shell.execute_reply":"2021-08-04T13:25:27.088117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Dataset\nTrain / Validation / Test = 7 / 1 / 2","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/imdb-dataset/train.csv', usecols = ['review','sentiment'])\ndf_val = pd.read_csv('../input/imdb-dataset/val.csv', usecols = ['review','sentiment'])\ndf_test = pd.read_csv('../input/imdb-dataset/test.csv', usecols = ['review','sentiment'])","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:27.09048Z","iopub.execute_input":"2021-08-04T13:25:27.090854Z","iopub.status.idle":"2021-08-04T13:25:29.014105Z","shell.execute_reply.started":"2021-08-04T13:25:27.090816Z","shell.execute_reply":"2021-08-04T13:25:29.013256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.info())\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:29.015292Z","iopub.execute_input":"2021-08-04T13:25:29.015649Z","iopub.status.idle":"2021-08-04T13:25:29.062397Z","shell.execute_reply.started":"2021-08-04T13:25:29.015612Z","shell.execute_reply":"2021-08-04T13:25:29.061453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_val.info())\ndf_val","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:29.063829Z","iopub.execute_input":"2021-08-04T13:25:29.064208Z","iopub.status.idle":"2021-08-04T13:25:29.086615Z","shell.execute_reply.started":"2021-08-04T13:25:29.064157Z","shell.execute_reply":"2021-08-04T13:25:29.085609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.info())\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:29.087906Z","iopub.execute_input":"2021-08-04T13:25:29.088258Z","iopub.status.idle":"2021-08-04T13:25:29.112359Z","shell.execute_reply.started":"2021-08-04T13:25:29.088223Z","shell.execute_reply":"2021-08-04T13:25:29.11147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def get_metrics(y_test, y_pred_proba):\n    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred_proba >= 0.5), 4))\n    print('F1_SCORE: ', round(f1_score(y_test, y_pred_proba >= 0.5, average = \"macro\"), 4))\n    print('ROC_AUC_SCORE: ', round(roc_auc_score(y_test, y_pred_proba), 4))\n    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred_proba >= 0.5),'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:29.115174Z","iopub.execute_input":"2021-08-04T13:25:29.115504Z","iopub.status.idle":"2021-08-04T13:25:29.120865Z","shell.execute_reply.started":"2021-08-04T13:25:29.115477Z","shell.execute_reply":"2021-08-04T13:25:29.119654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning\nRef: https://www.kaggle.com/colearninglounge/nlp-data-preprocessing-and-cleaning","metadata":{}},{"cell_type":"code","source":"#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndf_train['review'] = df_train['review'].apply(lambda z: remove_punctuations(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_html(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_url(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_emoji(z))\n\ndf_val['review'] = df_val['review'].apply(lambda z: remove_punctuations(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_html(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_url(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_emoji(z))\n\ndf_test['review'] = df_test['review'].apply(lambda z: remove_punctuations(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_html(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_url(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_emoji(z))","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:29.123049Z","iopub.execute_input":"2021-08-04T13:25:29.123441Z","iopub.status.idle":"2021-08-04T13:25:38.279484Z","shell.execute_reply.started":"2021-08-04T13:25:29.123406Z","shell.execute_reply":"2021-08-04T13:25:38.278612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_abb(data):\n    data = re.sub(r\"he's\", \"he is\", data)\n    data = re.sub(r\"there's\", \"there is\", data)\n    data = re.sub(r\"We're\", \"We are\", data)\n    data = re.sub(r\"That's\", \"That is\", data)\n    data = re.sub(r\"won't\", \"will not\", data)\n    data = re.sub(r\"they're\", \"they are\", data)\n    data = re.sub(r\"Can't\", \"Cannot\", data)\n    data = re.sub(r\"wasn't\", \"was not\", data)\n    data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n    data= re.sub(r\"aren't\", \"are not\", data)\n    data = re.sub(r\"isn't\", \"is not\", data)\n    data = re.sub(r\"What's\", \"What is\", data)\n    data = re.sub(r\"haven't\", \"have not\", data)\n    data = re.sub(r\"hasn't\", \"has not\", data)\n    data = re.sub(r\"There's\", \"There is\", data)\n    data = re.sub(r\"He's\", \"He is\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"You're\", \"You are\", data)\n    data = re.sub(r\"I'M\", \"I am\", data)\n    data = re.sub(r\"shouldn't\", \"should not\", data)\n    data = re.sub(r\"wouldn't\", \"would not\", data)\n    data = re.sub(r\"i'm\", \"I am\", data)\n    data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n    data = re.sub(r\"I'm\", \"I am\", data)\n    data = re.sub(r\"Isn't\", \"is not\", data)\n    data = re.sub(r\"Here's\", \"Here is\", data)\n    data = re.sub(r\"you've\", \"you have\", data)\n    data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n    data = re.sub(r\"we're\", \"we are\", data)\n    data = re.sub(r\"what's\", \"what is\", data)\n    data = re.sub(r\"couldn't\", \"could not\", data)\n    data = re.sub(r\"we've\", \"we have\", data)\n    data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n    data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n    data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n    data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n    data = re.sub(r\"who's\", \"who is\", data)\n    data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n    data = re.sub(r\"y'all\", \"you all\", data)\n    data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n    data = re.sub(r\"would've\", \"would have\", data)\n    data = re.sub(r\"it'll\", \"it will\", data)\n    data = re.sub(r\"we'll\", \"we will\", data)\n    data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n    data = re.sub(r\"We've\", \"We have\", data)\n    data = re.sub(r\"he'll\", \"he will\", data)\n    data = re.sub(r\"Y'all\", \"You all\", data)\n    data = re.sub(r\"Weren't\", \"Were not\", data)\n    data = re.sub(r\"Didn't\", \"Did not\", data)\n    data = re.sub(r\"they'll\", \"they will\", data)\n    data = re.sub(r\"they'd\", \"they would\", data)\n    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n    data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n    data = re.sub(r\"they've\", \"they have\", data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"should've\", \"should have\", data)\n    data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n    data = re.sub(r\"where's\", \"where is\", data)\n    data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n    data = re.sub(r\"we'd\", \"we would\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"weren't\", \"were not\", data)\n    data = re.sub(r\"They're\", \"They are\", data)\n    data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n    data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n    data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n    data = re.sub(r\"let's\", \"let us\", data)\n    data = re.sub(r\"it's\", \"it is\", data)\n    data = re.sub(r\"can't\", \"cannot\", data)\n    data = re.sub(r\"don't\", \"do not\", data)\n    data = re.sub(r\"you're\", \"you are\", data)\n    data = re.sub(r\"i've\", \"I have\", data)\n    data = re.sub(r\"that's\", \"that is\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"doesn't\", \"does not\",data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"didn't\", \"did not\", data)\n    data = re.sub(r\"ain't\", \"am not\", data)\n    data = re.sub(r\"you'll\", \"you will\", data)\n    data = re.sub(r\"I've\", \"I have\", data)\n    data = re.sub(r\"Don't\", \"do not\", data)\n    data = re.sub(r\"I'll\", \"I will\", data)\n    data = re.sub(r\"I'd\", \"I would\", data)\n    data = re.sub(r\"Let's\", \"Let us\", data)\n    data = re.sub(r\"you'd\", \"You would\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"Ain't\", \"am not\", data)\n    data = re.sub(r\"Haven't\", \"Have not\", data)\n    data = re.sub(r\"Could've\", \"Could have\", data)\n    data = re.sub(r\"youve\", \"you have\", data)  \n    data = re.sub(r\"donå«t\", \"do not\", data)  \n    return data\n    \ndf_train['review'] = df_train['review'].apply(lambda z: remove_abb(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_abb(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_abb(z))","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:38.280993Z","iopub.execute_input":"2021-08-04T13:25:38.281331Z","iopub.status.idle":"2021-08-04T13:25:46.625385Z","shell.execute_reply.started":"2021-08-04T13:25:38.281295Z","shell.execute_reply":"2021-08-04T13:25:46.624227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_train.head(5))\nprint(df_val.shape)\nprint(df_val.head(5))\nprint(df_test.shape)\nprint(df_test.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.630346Z","iopub.execute_input":"2021-08-04T13:25:46.632766Z","iopub.status.idle":"2021-08-04T13:25:46.651864Z","shell.execute_reply.started":"2021-08-04T13:25:46.632647Z","shell.execute_reply":"2021-08-04T13:25:46.648585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding ","metadata":{}},{"cell_type":"code","source":"class IntentDetectionData:\n    DATA_COLUMN,  LABEL_COLUMN  = \"review\",\"sentiment\"\n\n    def __init__(self, train, val, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n\n        ((self.train_x, self.train_y), (self.val_x, self.val_y), (self.test_x, self.test_y)) = map(self._prepare, [train, val, test])\n\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.train_x, self.val_x, self.test_x = map(self._pad, [self.train_x, self.val_x, self.test_x])\n\n    def _prepare(self, df):\n        x, y = [], []\n    \n        for non, row in tqdm(df.iterrows()):\n            text, label =\\\n                row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n\n            tokens = self.tokenizer.tokenize(text)\n            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"] ## Tokens beigning and ending specified by separation of tokens.\n\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens) ## Convert Tokens to IDs\n\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n\n        return np.array(x), np.array(y)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)] ## -2 as ignoring tokens provided by bert\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids)) ## padding by zeros\n            x.append(np.array(input_ids))\n        \n        return np.array(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.653236Z","iopub.execute_input":"2021-08-04T13:25:46.653583Z","iopub.status.idle":"2021-08-04T13:25:46.679882Z","shell.execute_reply.started":"2021-08-04T13:25:46.653535Z","shell.execute_reply":"2021-08-04T13:25:46.677208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### LSTM","metadata":{}},{"cell_type":"code","source":"def LSTM_V0(bert_output):\n    #...\n    return net","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.681388Z","iopub.execute_input":"2021-08-04T13:25:46.682015Z","iopub.status.idle":"2021-08-04T13:25:46.689832Z","shell.execute_reply.started":"2021-08-04T13:25:46.681977Z","shell.execute_reply":"2021-08-04T13:25:46.688999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BiLSTM","metadata":{}},{"cell_type":"code","source":"# https://github.com/hmohebbi/SentimentAnalysis\ndef BiLSTM_V0(bert_output):\n    net = Bidirectional(LSTM(units=32, return_sequences=True,))(bert_output)\n    net = GlobalAveragePooling1D()(net)\n    net = Dense(20, activation='relu')(net)\n    net = Dropout(rate=0.5)(net)\n    net = Dense(1, activation='sigmoid', name='classifier')(net) \n    return net","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.691254Z","iopub.execute_input":"2021-08-04T13:25:46.691776Z","iopub.status.idle":"2021-08-04T13:25:46.698898Z","shell.execute_reply.started":"2021-08-04T13:25:46.691743Z","shell.execute_reply":"2021-08-04T13:25:46.697797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN + LSTM","metadata":{}},{"cell_type":"code","source":"def CNN_LSTM_V0(bert_output):\n    net = Dropout(0.3)(bert_output)\n    net = Conv1D(200, 5, activation='relu')(net)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = LSTM(100)(net)\n    net = Dropout(0.3)(net)\n    net = Dense(16,activation='relu')(net)\n    net = Dense(1, activation='sigmoid', name='classifier')(net)\n    return net","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.700141Z","iopub.execute_input":"2021-08-04T13:25:46.700789Z","iopub.status.idle":"2021-08-04T13:25:46.708358Z","shell.execute_reply.started":"2021-08-04T13:25:46.700668Z","shell.execute_reply":"2021-08-04T13:25:46.707516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM + CNN","metadata":{}},{"cell_type":"code","source":"def LSTM_CNN_V0(bert_output):\n    net = LSTM(256,return_sequences=True)(bert_output)\n    net = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(net)\n    net = GlobalMaxPooling1D()(net)\n    net = Dropout(0.2)(net)\n    net = Dense(64,activation='relu')(net)\n    net = Dense(1, activation='sigmoid', name='classifier')(net)\n    return net\n\ndef LSTM_CNN_V1(bert_output):\n    net = LSTM(512, return_sequences=True,dropout=0.25, recurrent_dropout=0.1)(bert_output)\n    net = Conv1D(filters=64, kernel_size=7, padding='same', activation='relu', strides=1)(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(filters=128, kernel_size=5, padding='same', activation='relu', strides=1)(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(filters=256, kernel_size=3, padding='same', activation='relu', strides=1)(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(256,activation='relu')(net)\n    net = Dropout(0.2)(net)\n    net = Dense(1, activation='sigmoid', name='classifier')(net)\n    return net","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.711875Z","iopub.execute_input":"2021-08-04T13:25:46.712332Z","iopub.status.idle":"2021-08-04T13:25:46.737088Z","shell.execute_reply.started":"2021-08-04T13:25:46.7123Z","shell.execute_reply":"2021-08-04T13:25:46.735965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choose model","metadata":{}},{"cell_type":"code","source":"def create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file):\n\n    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n        bc = StockBertConfig.from_json_string(reader.read()) ## Reading bert config\n        bert_params = map_stock_config_to_params(bc) ## Mapping parameters \n        bert_params.adapter_size = None # Adapter size helps tune Bert model faster\n        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n        \n    ## Creat dictionary\n    choose_model = {'LSTM':{},\n                    'BiLSTM':{0: BiLSTM_V0},\n                    'CNN+LSTM':{0: CNN_LSTM_V0},\n                    'LSTM+CNN':{0: LSTM_CNN_V0, 1: LSTM_CNN_V1},}\n    ## Specifying input\n    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n    bert_output = bert(input_ids)\n        \n    net = choose_model[model_name][model_ver](bert_output)\n\n    model = keras.Model(input_ids, net)\n    model.build(input_shape=(None, max_seq_len))\n    load_stock_weights(bert, bert_checkpnt_file) ##Loading the weights from bert chckpoint file\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.740318Z","iopub.execute_input":"2021-08-04T13:25:46.741992Z","iopub.status.idle":"2021-08-04T13:25:46.755747Z","shell.execute_reply.started":"2021-08-04T13:25:46.741956Z","shell.execute_reply":"2021-08-04T13:25:46.754539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BERT model","metadata":{}},{"cell_type":"code","source":"# Load BERT model\n# https://github.com/google-research/bert/blob/master/README.md\n\nbert_model_name = \"uncased_L-12_H-768_A-12\"\n# uncased_L-4_H-512_A-8\n# uncased_L-12_H-768_A-12\n!wget  https://storage.googleapis.com/bert_models/2020_02_20/{bert_model_name}.zip\n!unzip {bert_model_name}.zip","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:46.760443Z","iopub.execute_input":"2021-08-04T13:25:46.763151Z","iopub.status.idle":"2021-08-04T13:25:54.390855Z","shell.execute_reply.started":"2021-08-04T13:25:46.763112Z","shell.execute_reply":"2021-08-04T13:25:54.389896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model_path=\"./\"\nbert_checkpnt_file = os.path.join(bert_model_path, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_model_path, \"bert_config.json\")\nbert_vocab_file = os.path.join(bert_model_path, \"vocab.txt\")\nprint(bert_checkpnt_file)\nprint(bert_config_file)\nprint(bert_vocab_file)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:54.393425Z","iopub.execute_input":"2021-08-04T13:25:54.393821Z","iopub.status.idle":"2021-08-04T13:25:54.400345Z","shell.execute_reply.started":"2021-08-04T13:25:54.39378Z","shell.execute_reply":"2021-08-04T13:25:54.399513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pretrain","metadata":{}},{"cell_type":"code","source":"# Tokenize\ntokenizer = FullTokenizer(vocab_file=bert_vocab_file)\ntokens = tokenizer.tokenize(\"People say nothing is impossible, but I do nothing everyday\")\nprint(tokens)\ntokenizer.convert_tokens_to_ids(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:54.401442Z","iopub.execute_input":"2021-08-04T13:25:54.401777Z","iopub.status.idle":"2021-08-04T13:25:54.527328Z","shell.execute_reply.started":"2021-08-04T13:25:54.401745Z","shell.execute_reply":"2021-08-04T13:25:54.526596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = [0, 1]\nmax_seq_len = 256\ndata = IntentDetectionData(df_train, df_val, df_test, tokenizer, classes, max_seq_len)\nprint(data.max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:25:54.528629Z","iopub.execute_input":"2021-08-04T13:25:54.528973Z","iopub.status.idle":"2021-08-04T13:30:32.245719Z","shell.execute_reply.started":"2021-08-04T13:25:54.528939Z","shell.execute_reply":"2021-08-04T13:30:32.24484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creat model","metadata":{}},{"cell_type":"code","source":"model_name = \"BiLSTM\"\nmodel_ver = 0\nLR = 2e-5\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate=LR)\nmetrics = tf.metrics.BinaryAccuracy()\n\nmodel = create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:31:36.866167Z","iopub.execute_input":"2021-08-04T13:31:36.866486Z","iopub.status.idle":"2021-08-04T13:31:48.594574Z","shell.execute_reply.started":"2021-08-04T13:31:36.866456Z","shell.execute_reply":"2021-08-04T13:31:48.59374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot architecture model\ntf.keras.utils.plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T15:46:51.982695Z","iopub.execute_input":"2021-08-04T15:46:51.98302Z","iopub.status.idle":"2021-08-04T15:46:52.460258Z","shell.execute_reply.started":"2021-08-04T15:46:51.98299Z","shell.execute_reply":"2021-08-04T15:46:52.459399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"# Save model\nmodel_ckpt_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.hdf5\"\ncheckpoint = ModelCheckpoint(model_ckpt_path, monitor='val_loss', verbose=1, save_best_only=True, mode='max',save_weights_only=True)\ncallbacks_list = [checkpoint]\n\n# Training\nprint(f\"Training model with {bert_model_name}_{model_name}_V{model_ver}_{max_seq_len}\\n\")\ntrain_history = model.fit(data.train_x, data.train_y, validation_data=(data.val_x,data.val_y), epochs=5, batch_size=32, verbose=1, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:32:09.426815Z","iopub.execute_input":"2021-08-04T13:32:09.427136Z","iopub.status.idle":"2021-08-04T15:37:43.146439Z","shell.execute_reply.started":"2021-08-04T13:32:09.427107Z","shell.execute_reply":"2021-08-04T15:37:43.145619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot accuracy and loss\nhistory_dict = train_history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","metadata":{"execution":{"iopub.status.busy":"2021-08-04T15:37:43.1484Z","iopub.execute_input":"2021-08-04T15:37:43.148762Z","iopub.status.idle":"2021-08-04T15:37:43.517179Z","shell.execute_reply.started":"2021-08-04T15:37:43.148724Z","shell.execute_reply":"2021-08-04T15:37:43.516274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save architecture model\nconfig = model.to_json()\nmodel_config_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.json\"\nwith open(model_config_path, \"w\") as outfile:\n    json.dump(config, outfile)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T03:01:46.676493Z","iopub.status.idle":"2021-08-04T03:01:46.677345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"model.load_weights(model_ckpt_path)\ny_pred_proba = model.predict(data.test_x)\nget_metrics(data.test_y, y_pred_proba)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T15:37:43.518933Z","iopub.execute_input":"2021-08-04T15:37:43.519279Z","iopub.status.idle":"2021-08-04T15:39:35.791399Z","shell.execute_reply.started":"2021-08-04T15:37:43.519243Z","shell.execute_reply":"2021-08-04T15:39:35.790537Z"},"trusted":true},"execution_count":null,"outputs":[]}]}