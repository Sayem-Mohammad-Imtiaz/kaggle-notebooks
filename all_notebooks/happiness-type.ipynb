{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option('display.max_rows', 104)\npd.set_option('display.max_columns',104)\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style('whitegrid')\n%matplotlib inline\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer \nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report, precision_recall_fscore_support, precision_score, recall_score, f1_score\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Embedding,Dropout, Layer ,LSTM, SpatialDropout1D,Bidirectional,GlobalMaxPool1D, Input\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping \nimport keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nimport keras\nimport multiprocessing\nnthreads = multiprocessing.cpu_count()\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nimport string\npunctuations = string.punctuation\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/bonding/hm_train.csv')\ntest = pd.read_csv('/kaggle/input/bonding/hm_test.csv')\ndf.tail(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words(\"english\"))\ndef cleanup_text(doc):\n    #print(1)\n    texts = []\n    #doc = re.sub(r\"\\$\\s[0-9]+\", \"moneyvalue\", doc)        \n    doc = nlp(doc, disable=['parser', 'ner'])\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n\n    tokens = [tok for tok in tokens if tok not in stop_words and tok not in punctuations and not tok.isdigit()]\n    tokens = ' '.join(tokens)\n\n    texts.append(tokens)\n    return pd.Series(texts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwith multiprocessing.Manager().Pool(nthreads) as pool:\n    res = pool.map_async(cleanup_text, df['cleaned_hm'].values)\n    pool.close()\n    pool.join()\n    \ndf['c'] = pd.concat(res.get(), axis=0, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_binarizer = LabelBinarizer()\ny = label_binarizer.fit_transform(df.predicted_category)\nlabels = label_binarizer.classes_\nprint(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 150000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['c'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tokenizer.texts_to_sequences(df['c'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.10, random_state = 42)\nx_train.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BiLSTM Model","metadata":{}},{"cell_type":"code","source":"# model = Sequential()\n# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n# model.add(Bidirectional(LSTM(64)))\n# model.add(Dense(100, activation='relu'))\n# model.add(Dropout(rate=0.4))\n# model.add(Dense(100, activation='relu'))\n# model.add(Dense(len(labels), activation=\"softmax\"))\n# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs = 5\n# batch_size = 128\n\n# history = model.fit(x_train, y_train, epochs=epochs,\n#                     batch_size=batch_size,validation_split=0.2,\n#                    verbose =1\n#                    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def my_func(a):\n#     n = a.argmax()\n#     for i in range(len(a)):\n#         if i==n:\n#             a[i] = 1\n#         else:\n#             a[i] = 0\n#     return a\n\n# y_pred = model.predict(x_test)\n# y_pr = np.apply_along_axis(my_func, 1, y_pred)\n# print('accuracy score -->',accuracy_score(y_pr, y_test))      \n# print('F1 score -->',f1_score(y_pr,y_test, average = 'weighted'))\n# print('prescision score -->',precision_score(y_test, y_pr, average='macro'))\n# print('recall score -->',recall_score(y_test, y_pr, average='macro'))\n#classification_report(y_test, y_pr).split('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# with multiprocessing.Manager().Pool(nthreads) as pool:\n#     res = pool.map_async(cleanup_text, test['cleaned_hm'].values)\n#     pool.close()\n#     pool.join()\n    \n# test['c'] = pd.concat(res.get(), axis=0, ignore_index=True)\n# X = tokenizer.texts_to_sequences(test['c'].values)\n# X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n# print('Shape of data tensor:', X.shape)\n# y_pred = model.predict(X)\n# y_pred = np.apply_along_axis(my_func, 1, y_pred)\n# test['predicted_category'] = label_binarizer.inverse_transform(y_pred, threshold=None)\n# test[['hmid','predicted_category']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Model","metadata":{}},{"cell_type":"code","source":"len_mat=[]\nfor i in range(len(X)):\n    len_mat.append(len(X[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class attention(Layer):\n    def __init__(self,**kwargs):\n        super(attention,self).__init__(**kwargs)\n\n    def build(self,input_shape):\n        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n        super(attention, self).build(input_shape)\n\n    def call(self,x):\n        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n        at=K.softmax(et)\n        at=K.expand_dims(at,axis=-1)\n        output=x*at\n        return K.sum(output,axis=1)\n\n    def compute_output_shape(self,input_shape):\n        return (input_shape[0],input_shape[-1])\n\n    def get_config(self):\n        return super(attention,self).get_config()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs=Input((X.shape[1],))\nx=Embedding(input_dim=MAX_NB_WORDS+1,output_dim=EMBEDDING_DIM,input_length=X.shape[1],\\\n            embeddings_regularizer=keras.regularizers.l2(.001))(inputs)\natt_in=Bidirectional(LSTM(64, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)\natt_in = Dense(100, activation=\"relu\")(att_in)\natt_in = Dropout(0.4)(att_in)\natt_in = Dense(100, activation=\"relu\")(att_in)\natt_out=attention()(att_in)\noutputs=Dense(7,activation='softmax',trainable=True)(att_out)\nmodel=Model(inputs,outputs)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\nepochs = 16\nbatch_size = 128\n\nhistory = model.fit(x_train, y_train, epochs=epochs,\n                    batch_size=batch_size,validation_split=0.2,\n                   verbose =1\n                   )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_func(a):\n    n = a.argmax()\n    for i in range(len(a)):\n        if i==n:\n            a[i] = 1\n        else:\n            a[i] = 0\n    return a\n\ny_pred = model.predict(x_test)\ny_pr = np.apply_along_axis(my_func, 1, y_pred)\nprint('accuracy score -->',accuracy_score(y_pr, y_test))      \nprint('F1 score -->',f1_score(y_pr,y_test, average = 'weighted'))\nprint('prescision score -->',precision_score(y_test, y_pr, average='macro'))\nprint('recall score -->',recall_score(y_test, y_pr, average='macro'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwith multiprocessing.Manager().Pool(nthreads) as pool:\n    res = pool.map_async(cleanup_text, test['cleaned_hm'].values)\n    pool.close()\n    pool.join()\n    \ntest['c'] = pd.concat(res.get(), axis=0, ignore_index=True)\nX = tokenizer.texts_to_sequences(test['c'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\ny_pred = model.predict(X)\ny_pred = np.apply_along_axis(my_func, 1, y_pred)\ntest['predicted_category'] = label_binarizer.inverse_transform(y_pred, threshold=None)\ntest[['hmid','predicted_category']].to_csv('submission_attn_8808.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}