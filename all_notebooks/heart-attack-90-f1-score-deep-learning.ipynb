{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T17:52:39.242518Z","iopub.execute_input":"2021-06-05T17:52:39.24288Z","iopub.status.idle":"2021-06-05T17:52:39.254881Z","shell.execute_reply.started":"2021-06-05T17:52:39.24285Z","shell.execute_reply":"2021-06-05T17:52:39.253591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Heart Attack Analysis:**\n#     Task: Predict if a person is prone to a heart attack or not\n     \n     Strategy: Using deep learning\n     \n     Steps: 1. Data Visualisation and analysis\n            2. Dealing with missing data (if any)\n            3. Dealing with categorical data (if any)\n            4. Data Preprocessing\n            5. Create the model\n            6. Train the model\n            7. Evaluate Performance","metadata":{}},{"cell_type":"code","source":"# reading the date, make sure it's the right data\ndf = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:39.25724Z","iopub.execute_input":"2021-06-05T17:52:39.257909Z","iopub.status.idle":"2021-06-05T17:52:39.284061Z","shell.execute_reply.started":"2021-06-05T17:52:39.257859Z","shell.execute_reply":"2021-06-05T17:52:39.28262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 1: Data Visualisation and analysis**","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:39.285901Z","iopub.execute_input":"2021-06-05T17:52:39.286329Z","iopub.status.idle":"2021-06-05T17:52:39.291336Z","shell.execute_reply.started":"2021-06-05T17:52:39.286285Z","shell.execute_reply":"2021-06-05T17:52:39.290351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 8))\nsns.countplot(x = 'output', data=df)\n# We check if the data is uniform with respect to the output result\n# and it seems to be (that's good)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:39.292693Z","iopub.execute_input":"2021-06-05T17:52:39.293223Z","iopub.status.idle":"2021-06-05T17:52:39.465882Z","shell.execute_reply.started":"2021-06-05T17:52:39.293185Z","shell.execute_reply":"2021-06-05T17:52:39.465066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(df.corr(), annot=True)\n# We create the heatmap of the corrolations in order to see\n# if there are 2 or more features verry similar (in that case we could have dropped some features)\n# While there are some features highly corrolated with eachother,\n# they are not similar enought for us to want to drop them","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:39.469318Z","iopub.execute_input":"2021-06-05T17:52:39.469816Z","iopub.status.idle":"2021-06-05T17:52:40.749707Z","shell.execute_reply.started":"2021-06-05T17:52:39.469782Z","shell.execute_reply":"2021-06-05T17:52:40.74859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()['output'].sort_values()\n# Here we see the corrolations of each feature with the output","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:40.752948Z","iopub.execute_input":"2021-06-05T17:52:40.753293Z","iopub.status.idle":"2021-06-05T17:52:40.762877Z","shell.execute_reply.started":"2021-06-05T17:52:40.753261Z","shell.execute_reply":"2021-06-05T17:52:40.761684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x='sex', data=df, hue='output')\n# here we see that there is a huge difference between the sexes in the risk of\n# having a heart attack","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:40.764548Z","iopub.execute_input":"2021-06-05T17:52:40.764897Z","iopub.status.idle":"2021-06-05T17:52:40.939808Z","shell.execute_reply.started":"2021-06-05T17:52:40.764863Z","shell.execute_reply":"2021-06-05T17:52:40.938764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.countplot(x='exng', data=df, hue='output')\n# We also see how exercise induced anigma helps reducing drastically the risk of heart failure","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:40.941191Z","iopub.execute_input":"2021-06-05T17:52:40.941523Z","iopub.status.idle":"2021-06-05T17:52:41.13182Z","shell.execute_reply.started":"2021-06-05T17:52:40.941491Z","shell.execute_reply":"2021-06-05T17:52:41.130554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While data visualisation is important, in this case we understood pretty much which are the important features and how they might affect the output.\n(Take into consideration that because we are using a deep learning based approach, there is no need to **completely** understant the feature correlations)","metadata":{}},{"cell_type":"markdown","source":"# **Step 2: Dealing with missing data**","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()\n# we check whether there is missing data","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.132928Z","iopub.execute_input":"2021-06-05T17:52:41.133219Z","iopub.status.idle":"2021-06-05T17:52:41.141581Z","shell.execute_reply.started":"2021-06-05T17:52:41.133184Z","shell.execute_reply":"2021-06-05T17:52:41.140479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we got lucky, we have no missing data so we don't need to do anything.","metadata":{}},{"cell_type":"markdown","source":"# **Step 3: Dealing with categorical data**","metadata":{}},{"cell_type":"code","source":"df.nunique()\n# Now we are looking for those features that have a relative small amount of unique values\n# we have to see whether there should be or not a correlations between those unique values","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.14293Z","iopub.execute_input":"2021-06-05T17:52:41.143378Z","iopub.status.idle":"2021-06-05T17:52:41.163974Z","shell.execute_reply.started":"2021-06-05T17:52:41.143346Z","shell.execute_reply":"2021-06-05T17:52:41.162732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The values that we are going to check are cp, restecg, slp, caa and thall \n# (notice that having 2 unique values is already like having them in a categorical way, so we can neglect them )\nsns.countplot(x='cp', data=df, hue='output')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.1654Z","iopub.execute_input":"2021-06-05T17:52:41.165812Z","iopub.status.idle":"2021-06-05T17:52:41.365093Z","shell.execute_reply.started":"2021-06-05T17:52:41.165775Z","shell.execute_reply":"2021-06-05T17:52:41.363955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# while cp types (1,2,3) seem to behave similar, cp type 0 seem to be totally different\n# one way to deal with this is chaning the elements (1,2,3) into 1\n# if we look into the documentation, we see that 0 will mean typical, and the rest of them are atypical\ndf['cp'] = df['cp'].apply(lambda x: min(1, x))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.36647Z","iopub.execute_input":"2021-06-05T17:52:41.366794Z","iopub.status.idle":"2021-06-05T17:52:41.374342Z","shell.execute_reply.started":"2021-06-05T17:52:41.366722Z","shell.execute_reply":"2021-06-05T17:52:41.37324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['restecg'].value_counts()\n# There is a small amount of type 2 so we can just reasign them to the value that behaves the same","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.377607Z","iopub.execute_input":"2021-06-05T17:52:41.378162Z","iopub.status.idle":"2021-06-05T17:52:41.391647Z","shell.execute_reply.started":"2021-06-05T17:52:41.378116Z","shell.execute_reply":"2021-06-05T17:52:41.390337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='restecg', data=df, hue='output')\n# We see that the type 2 behaves more like type 0, so we can reasign 2 to the value 0","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.393591Z","iopub.execute_input":"2021-06-05T17:52:41.39409Z","iopub.status.idle":"2021-06-05T17:52:41.588389Z","shell.execute_reply.started":"2021-06-05T17:52:41.394039Z","shell.execute_reply":"2021-06-05T17:52:41.587288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['restecg'] = df['restecg'].apply(lambda x: 1 if x==1 else 0)\ndf['restecg'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.58967Z","iopub.execute_input":"2021-06-05T17:52:41.590135Z","iopub.status.idle":"2021-06-05T17:52:41.59822Z","shell.execute_reply.started":"2021-06-05T17:52:41.5901Z","shell.execute_reply":"2021-06-05T17:52:41.596574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The slope can be taken as a continous value\ndf['slp'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.599737Z","iopub.execute_input":"2021-06-05T17:52:41.600073Z","iopub.status.idle":"2021-06-05T17:52:41.617997Z","shell.execute_reply.started":"2021-06-05T17:52:41.600041Z","shell.execute_reply":"2021-06-05T17:52:41.616619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The number of major vessels can be taken as a continous value, not categorical\ndf['caa'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.619652Z","iopub.execute_input":"2021-06-05T17:52:41.620302Z","iopub.status.idle":"2021-06-05T17:52:41.630712Z","shell.execute_reply.started":"2021-06-05T17:52:41.620253Z","shell.execute_reply":"2021-06-05T17:52:41.629839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='thall', data=df, hue='output')\n# There are various behaviour so we have to use dummy variables, we take it as categorical data\n# There is nothing about the Thall rate in the documentation, so we don't know if we have to preserve\n# some sort of relation between values","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.631976Z","iopub.execute_input":"2021-06-05T17:52:41.632516Z","iopub.status.idle":"2021-06-05T17:52:41.865421Z","shell.execute_reply.started":"2021-06-05T17:52:41.632475Z","shell.execute_reply":"2021-06-05T17:52:41.86437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df.drop('thall', axis=1), pd.get_dummies(df['thall'], drop_first=True)], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.86733Z","iopub.execute_input":"2021-06-05T17:52:41.867746Z","iopub.status.idle":"2021-06-05T17:52:41.877055Z","shell.execute_reply.started":"2021-06-05T17:52:41.867709Z","shell.execute_reply":"2021-06-05T17:52:41.875837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.878888Z","iopub.execute_input":"2021-06-05T17:52:41.879237Z","iopub.status.idle":"2021-06-05T17:52:41.901638Z","shell.execute_reply.started":"2021-06-05T17:52:41.879207Z","shell.execute_reply":"2021-06-05T17:52:41.900662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There Were a few columns that we have to verify.\nWhen there aren't a lot of unique values, we can use one-hot encoding if there is no value based relation between them.\nLuckily, in this set there was only 1 feature that really needed that.\n(When there are not a lot of values, we can just reasign them, like how we did with 'restecg')","metadata":{}},{"cell_type":"markdown","source":"# **Step 4: Data Processing**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.903092Z","iopub.execute_input":"2021-06-05T17:52:41.903607Z","iopub.status.idle":"2021-06-05T17:52:41.913994Z","shell.execute_reply.started":"2021-06-05T17:52:41.903558Z","shell.execute_reply":"2021-06-05T17:52:41.913087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first we get the data into features and labels, and split it intro train and test\nX = df.drop('output', axis=1)\ny = df['output']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.915542Z","iopub.execute_input":"2021-06-05T17:52:41.916211Z","iopub.status.idle":"2021-06-05T17:52:41.933861Z","shell.execute_reply.started":"2021-06-05T17:52:41.916159Z","shell.execute_reply":"2021-06-05T17:52:41.932713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We use a MinMaxScaler from sklearn in order to normalize the data \n# (check the documentation to see how it works)\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.935386Z","iopub.execute_input":"2021-06-05T17:52:41.935858Z","iopub.status.idle":"2021-06-05T17:52:41.952599Z","shell.execute_reply.started":"2021-06-05T17:52:41.935808Z","shell.execute_reply":"2021-06-05T17:52:41.951484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 5/6: Create the Model + Train**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.954281Z","iopub.execute_input":"2021-06-05T17:52:41.954761Z","iopub.status.idle":"2021-06-05T17:52:41.965872Z","shell.execute_reply.started":"2021-06-05T17:52:41.954712Z","shell.execute_reply":"2021-06-05T17:52:41.964777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.967847Z","iopub.execute_input":"2021-06-05T17:52:41.968321Z","iopub.status.idle":"2021-06-05T17:52:41.980082Z","shell.execute_reply.started":"2021-06-05T17:52:41.968278Z","shell.execute_reply":"2021-06-05T17:52:41.978607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\n# Usually use the number of columns as the first layer\nmodel.add(Dense(16, activation = 'relu'))\n# Using a droput layer to reduce overfitting\nmodel.add(Dropout(0.125))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dense(8, activation = 'relu'))\nmodel.add(Dense(8, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:41.981747Z","iopub.execute_input":"2021-06-05T17:52:41.982732Z","iopub.status.idle":"2021-06-05T17:52:42.011532Z","shell.execute_reply.started":"2021-06-05T17:52:41.982683Z","shell.execute_reply":"2021-06-05T17:52:42.010336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience = 25)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:42.013303Z","iopub.execute_input":"2021-06-05T17:52:42.013756Z","iopub.status.idle":"2021-06-05T17:52:42.020152Z","shell.execute_reply.started":"2021-06-05T17:52:42.01371Z","shell.execute_reply":"2021-06-05T17:52:42.018739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=1000, verbose=1, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:42.022188Z","iopub.execute_input":"2021-06-05T17:52:42.022683Z","iopub.status.idle":"2021-06-05T17:52:47.638989Z","shell.execute_reply.started":"2021-06-05T17:52:42.022634Z","shell.execute_reply":"2021-06-05T17:52:47.637952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model.history.history).plot()\n# ploting the history to see how the model behaved","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:47.643658Z","iopub.execute_input":"2021-06-05T17:52:47.644118Z","iopub.status.idle":"2021-06-05T17:52:47.857476Z","shell.execute_reply.started":"2021-06-05T17:52:47.644084Z","shell.execute_reply":"2021-06-05T17:52:47.85653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 7: Evaluation**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:47.85998Z","iopub.execute_input":"2021-06-05T17:52:47.860447Z","iopub.status.idle":"2021-06-05T17:52:47.866069Z","shell.execute_reply.started":"2021-06-05T17:52:47.860383Z","shell.execute_reply":"2021-06-05T17:52:47.864637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict_classes(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:47.867602Z","iopub.execute_input":"2021-06-05T17:52:47.867896Z","iopub.status.idle":"2021-06-05T17:52:47.969802Z","shell.execute_reply.started":"2021-06-05T17:52:47.867869Z","shell.execute_reply":"2021-06-05T17:52:47.968448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We care more about the f1-score\n# Because there was pretty even data, accuracy is also a good way to evaluate the model \nprint(classification_report(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:47.971553Z","iopub.execute_input":"2021-06-05T17:52:47.971981Z","iopub.status.idle":"2021-06-05T17:52:47.98697Z","shell.execute_reply.started":"2021-06-05T17:52:47.971936Z","shell.execute_reply":"2021-06-05T17:52:47.985576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In medicine especially what we want are less false negative\nprint(confusion_matrix(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:52:47.988455Z","iopub.execute_input":"2021-06-05T17:52:47.988863Z","iopub.status.idle":"2021-06-05T17:52:47.998161Z","shell.execute_reply.started":"2021-06-05T17:52:47.988827Z","shell.execute_reply":"2021-06-05T17:52:47.99687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now for the evaluation:\n    Accuracy of 90%\n    f1-score 0.90\nIt's not really great but it's definitely not bad either**","metadata":{}}]}