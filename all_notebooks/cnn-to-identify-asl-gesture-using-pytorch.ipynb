{"cells":[{"metadata":{},"cell_type":"markdown","source":"Credits : \n* [Python programming Tutorials](https://pythonprogramming.net/)\n          \n* [Deep Learning and Neural networks with Python and Pytorch by Sentdex on YouTube](https://www.youtube.com/playlist?list=PLQVvvaa0QuDdeMyHEYc0gxFpYwHY2Qfdh)"},{"metadata":{},"cell_type":"markdown","source":"Processing input data from csv file\n* Data from csv converted to list first (train_list)\n* Then converted to a list (train_data) containing lists of image tensors (x) and label tensor (y)\n* x is a tensor of 1 dimension containing 784 (28 * 28) grayscale values and y is a tensor containing only 1 value ie label (0-25)\n* Testing data stored as test_data in a similar way"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom random import shuffle\n\ntrain_path = \"/kaggle/input/sign-language-mnist/sign_mnist_train.csv\"\ntrain_csv = pd.read_csv(train_path)               #csv file read and stored in DataFrame object\ntrain_list = train_csv.values.tolist()            #contents of DF object converted to list with each row as a list\ntrain_data = []\nfor i in tqdm(train_list):                        #tqdm used for progress bar\n    x = torch.Tensor(i[1:])\n    x = x/255                                     #rescaling values of image tensor b/w 0-1 through division by one\n    y = torch.Tensor(i[0:1])\n    z = [x,y]\n    train_data.append(z)\n    \ntest_path = \"/kaggle/input/sign-language-mnist/sign_mnist_test.csv\"\ntest_csv = pd.read_csv(test_path)\ntest_list = test_csv.values.tolist()\ntest_data = []\nfor i in tqdm(test_list):\n    x = torch.Tensor(i[1:])\n    x = x/255\n    y = torch.Tensor(i[0:1])\n    z = [x,y]\n    test_data.append(z)\n\nshuffle(train_data)\nshuffle(test_data)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CUDA : API for doing matrix calculations on gpu to decrease training time"},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    print(\"gpu\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definig class for model\n\nmodel contains : \n* one 2d input layer\n* two hidden 2d layer\n* one hidden linear layer\n* one linear output layer\n\nfunctions in model :\n* constructor for creating layers\n* function convs for finding input dimensions of first linear layer and passing data through 2d layers\n* function forward for calling convs , passing data through linear layers and returning result \n\nActivation function used : relu (rectified linear unit)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):                                           # class Net inherits from predefined Module class in torch.nn\n    def __init__(self):                                         # calling constructor of  parent class\n        super().__init__()                                     \n        \n        \n        self.conv1 = nn.Conv2d(1,32,3)              # 2d convolution layer : (input : 1 image , output : 32 channels , kernel size : 3*3)\n        self.conv2 = nn.Conv2d(32,64,3)\n        self.conv3 = nn.Conv2d(64,128,3)\n        \n        self.linear_in = None                      # used to calculate input of first linear layer by passing fake data through 2d layers\n        x = torch.rand(28,28).view(-1,1,28,28)     # using convs function\n        self.convs(x)\n    \n        self.fc1 = nn.Linear(self.linear_in,512)\n        self.fc2 = nn.Linear(512,26)\n        \n    def convs(self,x):\n        x = F.max_pool2d(F.relu(self.conv1(x)) , (2,2) )      # relu used for activation function \n        x = F.max_pool2d(F.relu(self.conv2(x)) , (2,2) )      # max_pool2d for max pooling results of each kernel with window size 2*2\n        x = F.max_pool2d(F.relu(self.conv3(x)) , (2,2) )\n        \n        if self.linear_in == None:\n            self.linear_in = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]  # input of first linear layer is multiplication of dimensions of ouput \n        return x                                                        # tensor of the 2d layers\n    \n    def forward(self,x):                                    # forward pass function uses the convs function to pass through 2d layers\n        x = self.convs(x)\n        x = x.view(-1,self.linear_in)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = F.log_softmax(x ,dim = -1)                     # log_softmax for finding output neuron with highest value\n        return x\n\nnet  = Net()    \nnet.to(device)                                            # for moving model over to gpu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining loss function and optimizer\n* loss function used is CrossEntropyLoss\n* optimizer used is Adam\n* learning rate  = 0.001\n* batch size = 100"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(),lr = 0.001)\n\nfor epoch in tqdm(range(10)):\n    for i in (range(0,27400,100)):\n        batch = train_data[i:i+100]\n        \n        batch_x = torch.FloatTensor(100,784)\n        batch_y = torch.LongTensor(100,1)\n        \n        for j in range(100):\n            batch_x[j] = batch[j][0]                         \n            batch_y[j] = batch[j][1]\n        \n        batch_x = batch_x.view(100,1,28,28)\n        batch_y = batch_y.view(100)\n        \n        batch_x =  batch_x.to(device)                   # for moving each batch to gpu\n        batch_y =  batch_y.to(device)\n      \n        net.zero_grad()                                 # to make the gradients zero before calculating loss \n        outputs  = net(batch_x)\n        loss = F.nll_loss(outputs , batch_y)\n        loss.backward()                                 # backpropagation \n        optimizer.step()                                # adjusting parameters of model\n    print(f\"Epoch : {epoch} , Loss : {loss}\")\n \n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing on out of sample data"},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = 0\ntotal = 0\nwith torch.no_grad():                           # not calculating gradients for testing data\n    for data in (test_data):\n        x = data[0]\n        x = x.view(-1,1,28,28)\n        y = data[1]\n        \n        x = x.to(device)\n        y = y.to(device)\n        \n        output = net(x)\n        output = torch.argmax(output)\n        if output == y:\n            correct += 1\n        total += 1\nprint(\"correct : \" , correct)\nprint(\"total : \" , total)\nprint(\"accuracy : \" , round(correct/total , 3))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}