{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n       \n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))        \n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import sys\nimport os\nimport subprocess\nfrom six import string_types\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport scipy\nfrom skimage import io\nfrom scipy import ndimage\nfrom IPython.display import display\n%matplotlib inline\n\n!pip install --upgrade pip chart_studio\nimport chart_studio.plotly as py\nimport plotly.tools as tls\nfrom plotly.offline import iplot\nimport cv2\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom operator import itemgetter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport gc\npal = sns.color_palette()\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nprint('# File sizes')\nfor f in os.listdir('../input/planets-dataset/planet/planet'):\n    if not os.path.isdir('../input/planets-dataset/planet/planet/' + f):\n        print(f.ljust(30) + str(round(os.path.getsize('../input/planets-dataset/planet/planet/' + f) / 1000000, 2)) + 'MB')\n    else:\n        sizes = [os.path.getsize('../input/planets-dataset/planet/planet/'+f+'/'+x)/1000000 for x in os.listdir('../input/planets-dataset/planet/planet/' + f)]\n        print(f.ljust(30) + str(round(sum(sizes), 2)) + 'MB' + ' ({} files)'.format(len(sizes)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lha ../input/planets-dataset/planet/planet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input/planets-dataset/planet/planet\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PLANET_KAGGLE_ROOT = os.path.abspath(\"../input/\")\nPLANET_KAGGLE_JPEG_DIR = os.path.join(PLANET_KAGGLE_ROOT, '../input/planets-dataset/planet/planet/train-jpg/')\nPLANET_KAGGLE_LABEL_CSV = os.path.join(PLANET_KAGGLE_ROOT, '../input/planets-dataset/planet/planet/train_classes.csv')\nassert os.path.exists(PLANET_KAGGLE_ROOT)\nassert os.path.exists(PLANET_KAGGLE_JPEG_DIR)\nassert os.path.exists(PLANET_KAGGLE_LABEL_CSV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -lha ../input/planets-dataset/planet/planet/train_classes.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_df = pd.read_csv(PLANET_KAGGLE_LABEL_CSV)\nlabels_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels_df['tags'].unique()) # number of categories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_df.shape # number of individuals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build list with unique labels\nlabel_list = []\nfor tag_str in labels_df.tags.values:\n    labels = tag_str.split(' ')\n    for label in labels:\n        if label not in label_list:\n            label_list.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add onehot features for every label\nfor label in label_list:\n    labels_df[label] = labels_df['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n# Display head\nlabels_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#labels = np.array(labels_df['tags'])\n\n#vect = CountVectorizer()\n#vect.fit(labels)\n#vect.get_feature_names()\n\n#labels_dtm = vect.transform(labels)\n#df_labels = pd.DataFrame(labels_dtm.toarray(), columns = vect.get_feature_names())\n#print(df_labels.head())\n#print(labels_dtm[:10])\n\n#print(len(df_labels))\n#df_labels['image_name'] = labels_df['image_name']\n#print(df_labels.head())\n#print(df_labels.columns.values)\n\n #create a dict to collect total values of each class of label\n#amazon_condition = {}\n#for col in df_labels.columns.values:\n    #print(col)\n    \n    #z = df_labels.groupby([col])[col].count().astype(int)\n    #print(type(z))\n    #print(col, [np.array(k.astype(int)) for l, k in enumerate(z)])\n    #print([z])\n    #amazon_condition[col] = 0\n    #for i, j in enumerate(z):\n        #if i != 0:\n            #print ('j =', j) \n            #amazon_condition[col] += j\n#print(amazon_condition)\n\n#amazon_condition_labels = [x for x in amazon_condition.keys()]\n#amazon_condition_values = [x for x in amazon_condition.values()]\n\n#print(amazon_condition_labels)\n#print(amazon_condition_values)\n\n\n#fig, ax = plt.subplots()\n\n#N = len(amazon_condition_labels)\n#ind = np.arange(N)    # the x locations for the groups\n#width = 0.5       # the width of the bars\n\n#p1 = ax.bar(ind, amazon_condition_values, width, color='r')\n#ax.set_ylabel('Count', size=20)\n#ax.set_xlabel('Amazon Condition', size=20)\n#ax.set_title('Count by label type', size = 25)\n#ax.set_xticks(ind + width/2.)\n#ax.set_xticklabels(amazon_condition_labels)\n\n#plotly_fig = tls.mpl_to_plotly( fig )\n\n# For Legend\n#plotly_fig[\"layout\"][\"showlegend\"] = True\n#plotly_fig[\"data\"][0][\"name\"] = \"Count\"\n#plotly_fig[\"data\"][1][\"name\"] = \"Test\"\n\n#for tick in ax.get_xmajorticklabels():\n    #tick.set_rotation(45)\n    #tick.set_horizontalalignment(\"right\")\n    #tick.set_fontsize(15)\n\n    \n#for tick in ax.get_ymajorticklabels():\n    #tick.set_rotation(45)\n    #tick.set_horizontalalignment(\"right\")\n    #tick.set_fontsize(15)\n    \n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = labels_df['tags'].apply(lambda x: x.split(' '))\nfrom collections import Counter, defaultdict\ncounts = defaultdict(int)\nfor l in labels:\n    for l2 in l:\n        counts[l2] += 1\n\ndata=[go.Bar(x=list(counts.keys()), y=list(counts.values()))]\nlayout=dict(height=800, width=800, title='Distribution of training labels')\nfig=dict(data=data, layout=layout)\npy.iplot(data, filename='train-label-dist')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Co-occurence Matrix\ncom = np.zeros([len(counts)]*2)\nfor i, l in enumerate(list(counts.keys())):\n    for i2, l2 in enumerate(list(counts.keys())):\n        c = 0\n        cy = 0\n        for row in labels.values:\n            if l in row:\n                c += 1\n                if l2 in row: cy += 1\n        com[i, i2] = cy / c\n\ndata=[go.Heatmap(z=com, x=list(counts.keys()), y=list(counts.keys()))]\nlayout=go.Layout(height=800, width=800, title='Co-occurence matrix of training labels')\nfig=dict(data=data, layout=layout)\npy.iplot(data, filename='train-com')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_cooccurence_matrix(labels):\n    numeric_df = labels_df[labels]; \n    c_matrix = numeric_df.T.dot(numeric_df)\n    sns.heatmap(c_matrix)\n    return c_matrix\n    \n# Compute the co-ocurrence matrix\nmake_cooccurence_matrix(label_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the first 9 images in the planet dataset\nfrom matplotlib.image import imread\n\n# define location of dataset\nfolder = PLANET_KAGGLE_JPEG_DIR\n\n# plot first few images\nfor i in range(9):\n\t# define subplot\n\tplt.subplot(330 + 1 + i)\n\t# define filename\n\tPLANET_KAGGLE_LABEL_CSV = folder + 'train_' + str(i) + '.jpg'\n\t# load image pixels\n\timage = imread(PLANET_KAGGLE_LABEL_CSV)\n\t# plot raw pixel data\n\tplt.imshow(image)\n    \n# show the figure\nplt.show()\n\n                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom os import listdir\nfrom numpy import zeros\nfrom numpy import asarray\nfrom numpy import savez_compressed\n\n# create a mapping of tags to integers given the loaded mapping file\ndef create_tag_mapping(labels_df):\n\t# create a set of all known tags\n\tlabels = set()\n\tfor i in range(len(labels_df)):\n\t\t# convert spaced separated tags into an array of tags\n\t\ttags = labels_df['tags'][i].split(' ')\n\t\t# add tags to the set of known labels\n\t\tlabels.update(tags)\n\t# convert set of labels to a list to list\n\tlabels = list(labels)\n\t# order set alphabetically\n\tlabels.sort()\n\t# dict that maps labels to integers, and the reverse\n\tlabels_map = {labels[i]:i for i in range(len(labels))}\n\tinv_labels_map = {i:labels[i] for i in range(len(labels))}\n\treturn labels_map, inv_labels_map\n\n# create a mapping of filename to a list of tags\ndef create_file_mapping(labels_df):\n\tmapping = dict()\n\tfor i in range(len(labels_df)):\n\t\tname, tags = labels_df['image_name'][i], labels_df['tags'][i]\n\t\tmapping[name] = tags.split(' ')\n\treturn mapping\n \n# create a one hot encoding for one list of tags\ndef one_hot_encode(tags, mapping):\n\t# create empty vector\n\tencoding = zeros(len(mapping), dtype='uint8')\n\t# mark 1 for each tag in the vector\n\tfor tag in tags:\n\t\tencoding[mapping[tag]] = 1\n\treturn encoding\n \n# load all images into memory\ndef load_dataset(path, file_mapping, tag_mapping):\n\tphotos, targets = list(), list()\n\t# enumerate files in the directory\n\tfor filename in listdir(folder):\n\t\t# load image\n\t\tphoto = load_img(path + filename, target_size=(32,32))\n\t\t# convert to numpy array\n\t\tphoto = img_to_array(photo, dtype='uint8')\n\t\t# get tags\n\t\ttags = file_mapping[filename[:-4]]\n\t\t# one hot encode tags\n\t\ttarget = one_hot_encode(tags, tag_mapping)\n\t\t# store\n\t\tphotos.append(photo)\n\t\ttargets.append(target)\n\tX = asarray(photos, dtype='uint8')\n\ty = asarray(targets, dtype='uint8')\n\treturn X, y\n \n# load the mapping file\n#os.chdir(\"/input/planets-dataset/planet/planet/train_classes.csv\")\nfilename = '../input/planets-dataset/planet/planet/train_classes.csv'\nlabels_df = pd.read_csv(filename, encoding='latin1')\n\n# create a mapping of tags to integers\nmapping, inv_mapping = create_tag_mapping(labels_df)\nprint(len(mapping))\nprint(mapping)\n\n# create a mapping of tags to integers\ntag_mapping, _ = create_tag_mapping(labels_df)\n\n# create a mapping of filenames to tag lists\nfile_mapping = create_file_mapping(labels_df)\n\n# load the jpeg images\nfolder = '../input/planets-dataset/planet/planet/train-jpg/'\nX, y = load_dataset(folder, file_mapping, tag_mapping)\nprint(X.shape, y.shape)\n\n# save both arrays to one file in compressed format\nsavez_compressed('planet_data.npz', X, y)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom numpy import load\nfrom numpy import ones\nfrom numpy import asarray\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import fbeta_score\nimport sys\nfrom keras import backend\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD\nfrom keras import backend\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dropout\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load train and test dataset\ndef load_dataset():\n\t# load dataset\n\tdata = load('planet_data.npz')\n\tX, y = data['arr_0'], data['arr_1']\n    #print('Loaded: ', X.shape, y.shape)\n    \n\t# separate into train and test datasets\n\tX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\tprint(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n\treturn X_train, Y_train, X_test, Y_test\n\n# load dataset\nX_train, Y_train, X_test, Y_test = load_dataset()\n\n# make all one predictions\nY_train_pred = asarray([ones(Y_train.shape[1]) for _ in range(Y_train.shape[0])])\nY_test_pred = asarray([ones(Y_test.shape[1]) for _ in range(Y_test.shape[0])])\n\n# evaluate predictions\ntrain_score = fbeta_score(Y_train, Y_train_pred, 2, average='samples')\ntest_score = fbeta_score(Y_test, Y_test_pred, 2, average='samples')\nprint('All Ones: train=%.3f, test=%.3f' % (train_score, test_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vgg16 with fine-tuning and data augmentation \n# load train and test dataset\ndef load_dataset():\n\t# load dataset\n\tdata = load('planet_data.npz')\n\tX, y = data['arr_0'], data['arr_1']\n    #print('Loaded: ', X.shape, y.shape)\n    \n\t# separate into train and test datasets\n\tX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\tprint(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n\treturn X_train, Y_train, X_test, Y_test\n\n# calculate fbeta score for multi-class/label classification\ndef fbeta(y_true, y_pred, beta=2):\n\t# clip predictions\n\ty_pred = backend.clip(y_pred, 0, 1)\n\t# calculate elements\n\ttp = backend.sum(backend.round(backend.clip(y_true * y_pred, 0, 1)), axis=1)\n\tfp = backend.sum(backend.round(backend.clip(y_pred - y_true, 0, 1)), axis=1)\n\tfn = backend.sum(backend.round(backend.clip(y_true - y_pred, 0, 1)), axis=1)\n\t# calculate precision\n\tp = tp / (tp + fp + backend.epsilon())\n\t# calculate recall\n\tr = tp / (tp + fn + backend.epsilon())\n\t# calculate fbeta, averaged across each class\n\tbb = beta ** 2\n\tfbeta_score = backend.mean((1 + bb) * (p * r) / (bb * p + r + backend.epsilon()))\n\treturn fbeta_score \n\n# define cnn model\ndef define_model(in_shape=(32, 32, 3), out_shape=17):\n\t# load model\n\tmodel = VGG16(include_top=False, input_shape=in_shape)\n\t# mark loaded layers as not trainable\n\tfor layer in model.layers:\n\t\tlayer.trainable = False\n\t# allow last vgg block to be trainable\n\tmodel.get_layer('block5_conv1').trainable = True\n\tmodel.get_layer('block5_conv2').trainable = True\n\tmodel.get_layer('block5_conv3').trainable = True\n\tmodel.get_layer('block5_pool').trainable = True\n\t# add new classifier layers\n\tflat1 = Flatten()(model.layers[-1].output)\n\tclass1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n\toutput = Dense(out_shape, activation='sigmoid')(class1)\n\t# define new model\n\tmodel = Model(inputs=model.inputs, outputs=output)\n\t# compile model\n\topt = SGD(lr=0.01, momentum=0.9)\n\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n\treturn model\n \n# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n\t# plot loss\n\tplt.subplot(211)\n\tplt.title('Cross Entropy Loss')\n\tplt.plot(history.history['loss'], color='blue', label='train')\n\tplt.plot(history.history['val_loss'], color='orange', label='test')\n\t# plot accuracy\n\tplt.subplot(212)\n\tplt.title('Fbeta')\n\tplt.plot(history.history['fbeta'], color='blue', label='train')\n\tplt.plot(history.history['val_fbeta'], color='orange', label='test')\n\t# save plot to file\n\tfilename = sys.argv[0].split('/')[-1]\n\tplt.savefig(filename + '_plot.png')\n\tplt.close()\n    \n # run the test harness for evaluating a model\ndef run_test_harness():\n\t# load dataset\n\tX_train, Y_train, X_test, Y_test = load_dataset()\n\t# create data generator\n\ttrain_datagen = ImageDataGenerator(featurewise_center=True, horizontal_flip=True, vertical_flip=True, rotation_range=90)\n\ttest_datagen = ImageDataGenerator(featurewise_center=True)\n\t# specify imagenet mean values for centering\n\ttrain_datagen.mean = [123.68, 116.779, 103.939]\n\ttest_datagen.mean = [123.68, 116.779, 103.939]\n\t# prepare iterators\n\ttrain_it = train_datagen.flow(X_train, Y_train, batch_size=128)\n\ttest_it = test_datagen.flow(X_test, Y_test, batch_size=128)\n\t# define model\n\tmodel = define_model()\n\t# fit model\n\thistory = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n\t\tvalidation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)\n\t# evaluate model\n\tloss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)\n\tprint('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n\t# learning curves\n\tsummarize_diagnostics(history)\n\n# entry point, run the test harness\nrun_test_harness()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction for a new image\n\n# save model\nmodel.save('final_model.h5')\n\ncp train-jpg/train_1.jpg ./sample_image.jpg\n\n# create a mapping of tags to integers given the loaded mapping file\n#def create_tag_mapping(labels_df):\n\t# create a set of all known tags\n\t#labels = set()\n\t#for i in range(len(mapping_csv)):\n\t\t# convert spaced separated tags into an array of tags\n\t\t#tags = mapping_csv['tags'][i].split(' ')\n\t\t# add tags to the set of known labels\n\t\t#labels.update(tags)\n\t# convert set of labels to a list to list\n\t#labels = list(labels)\n\t# order set alphabetically\n\t#labels.sort()\n\t# dict that maps labels to integers, and the reverse\n\t#labels_map = {labels[i]:i for i in range(len(labels))}\n\t#inv_labels_map = {i:labels[i] for i in range(len(labels))}\n\t#return labels_map, inv_labels_map\n \n# convert a prediction to tags\ndef prediction_to_tags(inv_mapping, prediction):\n\t# round probabilities to {0, 1}\n\tvalues = prediction.round()\n\t# collect all predicted tags\n\ttags = [inv_mapping[i] for i in range(len(values)) if values[i] == 1.0]\n\treturn tags\n \n# load and prepare the image\ndef load_image(filename):\n\t# load the image\n\timg = load_img(filename, target_size=(32, 32))\n\t# convert to array\n\timg = img_to_array(img)\n\t# reshape into a single sample with 3 channels\n\timg = img.reshape(1, 32, 32, 3)\n\t# center pixel data\n\timg = img.astype('float32')\n\timg = img - [123.68, 116.779, 103.939]\n\treturn img\n \n# load an image and predict the class\ndef run_example(inv_mapping):\n\t# load the image\n\timg = load_image('sample_image.jpg')\n\t# load model\n\tmodel = load_model('final_model.h5')\n\t# predict the class\n\tresult = model.predict(img)\n\tprint(result[0])\n\t# map prediction to tags\n\ttags = prediction_to_tags(inv_mapping, result[0])\n\tprint(tags)\n \n# load the mapping file\n#filename = 'train_v2.csv'\nmapping_csv = read_csv(filename)\n# create a mapping of tags to integers\n_, inv_mapping = create_tag_mapping(labels_df)\n# entry point, run the example\nrun_example(inv_mapping)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}