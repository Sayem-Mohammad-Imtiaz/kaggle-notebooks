{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Support Vector Machine (SVM)\nIt's a supervised machine learning algorithm which can be used for both classification or regression problems. But it's usually used for classification. Given 2 or more labeled classes of data, it acts as a discriminative classifier, formally defined by an optimal hyperplane that seperates all the classes. New examples that are then mapped into that same space can then be categorized based on on which side of the gap they fall.\n\n![](https://camo.githubusercontent.com/ae3d247a4c7cf5bc9f4134a1a90c0df69b39e988/68747470733a2f2f7777772e64747265672e636f6d2f75706c6f616465642f70616765696d672f53766d4d617267696e322e6a7067)"},{"metadata":{},"cell_type":"markdown","source":"## Simple SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple SVM\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn import svm\n\nxBlue = np.array([0.3,0.5,1,1.4,1.7,2])\nyBlue = np.array([1,4.5,2.3,1.9,8.9,4.1])\n\nxRed = np.array([3.3,3.5,4,4.4,5.7,6])\nyRed = np.array([7,1.5,6.3,1.9,2.9,7.1])\n\nX = np.array([[0.3,1],[0.5,4.5],[1,2.3],[1.4,1.9],[1.7,8.9],[2,4.1],[3.3,7],[3.5,1.5],[4,6.3],[4.4,1.9],[5.7,2.9],[6,7.1]])\ny = np.array([0,0,0,0,0,0,1,1,1,1,1,1]) # 0: blue class, 1: red class\n\nplt.plot(xBlue, yBlue, 'ro', color='blue')\nplt.plot(xRed, yRed, 'ro', color='red')\nplt.plot(2.5,4.5,'ro',color='green')\n\n#\n#\tImportant parameters for SVC: gamma and C\n#\t\tgamma -> defines how far the influence of a single training example reaches\n#\t\t\t\t\tLow value: influence reaches far      High value: influence reaches close\n#\n#\t\tC -> trades off hyperplane surface simplicity + training examples missclassifications\n#\t\t\t\t\tLow value: simple/smooth hyperplane surface \n#\t\t\t\t\tHigh value: all training examples classified correctly but complex surface \nclassifier = svm.SVC()\nclassifier.fit(X,y)\n\nprint( classifier.predict([[2.5,4.5]]))\n\nplt.axis([-0.5,10,-0.5,10])\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deal with missig data and hyperparameter optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Titanic Dataset\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nfile_path = \"../input/titanicdataset-traincsv/\"\ndata = pd.read_csv(os.path.join(file_path,'train.csv'))\n\nprint(data.head())\nprint(data.describe())\nprint(data.corr())\nprint(data.isnull().sum())\ndata.head()\ndata.describe()\n\n# replace missing data (Age)\n# convert to array\nage = data['Age'].values\nage = np.reshape(age,(-1,1))\n#print(age)\n\nimp = SimpleImputer(missing_values = np.nan , strategy='most_frequent')\nimp.fit(age)\ndata['Age'] = imp.transform(age)\nprint(data.isnull().sum())\n\n#convert label to int\ndata.Sex=data.Sex.astype('category').cat.codes\nprint(data.head())\n\n# input and output data\nfeatures = data[[\"Pclass\", \"Fare\", \"Age\"]]\ntarget = data.Survived\n\n#features scaling\nscale = StandardScaler()\nfeatures = scale.fit_transform(features)\nprint(features[0,:])\n\n# split data for training and testing\nfeature_train, feature_test, target_train, target_test = train_test_split(features,target, test_size=0.3, random_state=42)\nprint(feature_train.shape)\nprint(feature_test.shape)\n\n#hyperparameter tuning (C)\nparameters = { 'C':np.arange(1,11,0.5)}\nsvc = svm.SVC(gamma='auto')\nSVM=GridSearchCV(svc, parameters)\nSVM.fit(feature_train,target_train)\nprint(SVM.best_estimator_)\n\n# prediction\npredictions = SVM.predict(feature_test)\nprint(confusion_matrix(target_test, predictions))\nprint(accuracy_score(target_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 1\nFrom code above, add more features to increase accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Titanic Dataset\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nfile_path = \"../input/titanicdataset-traincsv/\"\ndata = pd.read_csv(os.path.join(file_path,'train.csv'))\n\nprint(data.head())\nprint(data.describe())\nprint(data.corr())\nprint(data.isnull().sum())\ndata.head()\ndata.describe()\n\n# replace missing data (Age)\n# convert to array\nage = data['Age'].values\nage = np.reshape(age,(-1,1))\n#print(age)\n\nimp = SimpleImputer(missing_values = np.nan , strategy='most_frequent')\nimp.fit(age)\ndata['Age'] = imp.transform(age)\nprint(data.isnull().sum())\n\n#convert label to int\ndata.Sex=data.Sex.astype('category').cat.codes\nprint(data.head())\n\n# input and output data\nfeatures = data[[\"Pclass\", \"Fare\", \"Age\", \"Parch\", \"Sex\"]]\ntarget = data.Survived\n\n#features scaling\nscale = StandardScaler()\nfeatures = scale.fit_transform(features)\nprint(features[0,:])\n\n# split data for training and testing\nfeature_train, feature_test, target_train, target_test = train_test_split(features,target, test_size=0.3, random_state=42)\nprint(feature_train.shape)\nprint(feature_test.shape)\n\n#hyperparameter tuning (C)\nparameters = { 'C':np.arange(1,11,0.5)}\nsvc = svm.SVC(gamma='auto')\nSVM=GridSearchCV(svc, parameters)\nSVM.fit(feature_train,target_train)\nprint(SVM.best_estimator_)\n\n# prediction\npredictions = SVM.predict(feature_test)\nprint(confusion_matrix(target_test, predictions))\nprint(accuracy_score(target_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.stack.imgur.com/GbW5S.png)\n![](https://miro.medium.com/max/1713/1*6HVomcqW7BWuZ2vvGOEptw.png)\n![](https://qph.fs.quoracdn.net/main-qimg-531aaee161be90bf810f8eaff9d364d9)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iris dataset\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import datasets\n\n#\n#\tImportant parameters for SVC: gamma (for Gaussian RBF kernel) and C\n#\t\tgamma -> defines how far the influence of a single training example reaches\n#\t\t\t\t\tLow value: influence reaches far      High value: influence reaches close\n#\n#\t\tC -> trades off hyperplane surface simplicity + training examples missclassifications\n#\t\t\t\t\tLow value: simple/smooth hyperplane surface \n#\t\t\t\t\tHigh value: all training examples classified correctly but complex surface \n\ndataset = datasets.load_iris()\n\n#print(dataset)\n\nfeatures = dataset.data\ntargetVariables = dataset.target\n\nfeatureTrain, featureTest, targetTrain, targetTest = train_test_split(features, targetVariables, test_size=0.3, random_state=42)\n\nmodel = svm.SVC(gamma=0.9, C=100000)\n#model = svm.SVC()\nfittedModel = model.fit(featureTrain, targetTrain)\npredictions = fittedModel.predict(featureTest)\n\nprint(confusion_matrix(targetTest, predictions))\nprint(accuracy_score(targetTest, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 2\nFrom code above, adjust the parameters gamma and C to increase accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iris dataset\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import datasets\n\n#\n#\tImportant parameters for SVC: gamma (for Gaussian RBF kernel) and C\n#\t\tgamma -> defines how far the influence of a single training example reaches\n#\t\t\t\t\tLow value: influence reaches far      High value: influence reaches close\n#\n#\t\tC -> trades off hyperplane surface simplicity + training examples missclassifications\n#\t\t\t\t\tLow value: simple/smooth hyperplane surface \n#\t\t\t\t\tHigh value: all training examples classified correctly but complex surface \n\ndataset = datasets.load_iris()\n\n#print(dataset)\n\nfeatures = dataset.data\ntargetVariables = dataset.target\n\nfeatureTrain, featureTest, targetTrain, targetTest = train_test_split(features, targetVariables, test_size=0.3, random_state=42)\n\nprint(featureTrain.shape)\nprint(targetTrain.shape)\n\nmodel = svm.SVC(gamma=0.1, C=100)\n#model = svm.SVC()\nfittedModel = model.fit(featureTrain, targetTrain)\npredictions = fittedModel.predict(featureTest)\n\nprint(confusion_matrix(targetTest, predictions))\nprint(accuracy_score(targetTest, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 3\nImplement SVM classifier and predict the result using Digits dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# The digits dataset\ndigits = datasets.load_digits()\n\n\nprint(digits.images.shape)\n\n# To apply a classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\nprint(data.shape)\nprint(digits.target)\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# Split data into train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(\n    data, digits.target, test_size=0.3, shuffle=False)\n\n# We learn the digits on the first half of the digits\nclassifier.fit(X_train, y_train)\n\n# Now predict the value of the digit on the second half:\npredicted = classifier.predict(X_test)\n\n\n# results\nprint(confusion_matrix(y_test, predicted))\nprint(accuracy_score(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 4\nImplement SVM classifier and predict the result using Wine dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# The wine dataset\nwine = datasets.load_wine()\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# Split data into train and test subsets\nx_train, x_test, y_train, y_test = train_test_split(\n    wine.data, wine.target, test_size=0.3, random_state=25)\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001,C=100)\n\n\n# training\nclassifier.fit(x_train, y_train)\n\n# prediction\npredicted = classifier.predict(x_test)\n\n\n# results\nprint(confusion_matrix(y_test, predicted))\nprint(accuracy_score(y_test, predicted))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}