{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom numpy import savetxt\nfrom scipy import stats\nfrom keras import Sequential\nfrom keras.layers.core import Dense, Dropout\nimport tensorflow as tf\nfrom keras.wrappers.scikit_learn import KerasClassifier","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:34.965045Z","iopub.execute_input":"2021-06-25T10:59:34.96542Z","iopub.status.idle":"2021-06-25T10:59:34.97189Z","shell.execute_reply.started":"2021-06-25T10:59:34.965385Z","shell.execute_reply":"2021-06-25T10:59:34.970884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **1. Data Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"### **1.1. Preprocessing of training data**","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/les-variables-des-images-de-publicit/data_train.csv\", sep='\\t') #dataframe object","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:34.975505Z","iopub.execute_input":"2021-06-25T10:59:34.975783Z","iopub.status.idle":"2021-06-25T10:59:35.335588Z","shell.execute_reply.started":"2021-06-25T10:59:34.975753Z","shell.execute_reply":"2021-06-25T10:59:35.334523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:35.337096Z","iopub.execute_input":"2021-06-25T10:59:35.337376Z","iopub.status.idle":"2021-06-25T10:59:35.341602Z","shell.execute_reply.started":"2021-06-25T10:59:35.337347Z","shell.execute_reply":"2021-06-25T10:59:35.340747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1.1.1. Removing the duplicate values**","metadata":{}},{"cell_type":"code","source":"# Select duplicate rows except first occurrence based on all columns:\ndf_duplicateRows = df_train[df_train.duplicated()]\nprint(\"Duplicate rows:\", df_duplicateRows)\nprint(df_duplicateRows.shape[0]) # 562 rows / 2459 rows\n\n# Remove duplicate rows:\ndf_train = df_train.drop_duplicates() # 2459 rows - 562 rows = 1897 rows\n# print(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:35.343524Z","iopub.execute_input":"2021-06-25T10:59:35.343818Z","iopub.status.idle":"2021-06-25T10:59:35.765496Z","shell.execute_reply.started":"2021-06-25T10:59:35.343789Z","shell.execute_reply":"2021-06-25T10:59:35.764713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_tr= df_train.transpose()\ndf_tr_duplicateCols = df_train_tr[df_train_tr.duplicated()]\n# print(\"Duplicate columns:\", df_duplicateCols)\nprint(df_tr_duplicateCols.shape[0]) # 820 cols / 1559 cols","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:35.767005Z","iopub.execute_input":"2021-06-25T10:59:35.767561Z","iopub.status.idle":"2021-06-25T10:59:36.379454Z","shell.execute_reply.started":"2021-06-25T10:59:35.767517Z","shell.execute_reply":"2021-06-25T10:59:36.378137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove duplicate columns:\ndf_train_tr = df_train_tr.drop_duplicates() # 1559 cols - 820 rows = 739 cols\ndf_train = df_train_tr.transpose()\nprint(df_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:36.381066Z","iopub.execute_input":"2021-06-25T10:59:36.381462Z","iopub.status.idle":"2021-06-25T10:59:36.823337Z","shell.execute_reply.started":"2021-06-25T10:59:36.381418Z","shell.execute_reply":"2021-06-25T10:59:36.822183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1.1.2. Filling the missing values**","metadata":{}},{"cell_type":"markdown","source":"#### Ratio of the missing values:","metadata":{}},{"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = df_train.isna().sum()\nprint(\"missing values count:\\n\", missing_values_count)\n\n# how many total missing values do we have?\ntotal_cells = np.product(df_train.shape)\ntotal_missing = missing_values_count.sum()\n\nprint(\"total_cells:\", total_cells)\nprint(\"total_missing:\",total_missing)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:36.824439Z","iopub.execute_input":"2021-06-25T10:59:36.824689Z","iopub.status.idle":"2021-06-25T10:59:36.886107Z","shell.execute_reply.started":"2021-06-25T10:59:36.824663Z","shell.execute_reply":"2021-06-25T10:59:36.885212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fill the missing values in first three columns and with the median of these columns:","metadata":{}},{"cell_type":"code","source":"median_x1 = df_train['X1'].median()\nmedian_x2 = df_train['X2'].median()\nmedian_x3 = df_train['X3'].median()\nmedian_x4 = df_train['X4'].median()\n\n# print(\"median_x1:\", median_x1) # 60.0\n# print(\"median_x2:\", median_x2) # 114.0\n# print(\"median_x3:\", median_x3) # 2.2708\n# print(\"median_x4:\", median_x4) # 1.0\n\ndf_train['X1'].fillna(median_x1, inplace=True)\ndf_train['X2'].fillna(median_x2, inplace=True)\ndf_train['X3'].fillna(median_x3, inplace=True)\ndf_train['X4'].fillna(median_x4, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:36.88793Z","iopub.execute_input":"2021-06-25T10:59:36.889352Z","iopub.status.idle":"2021-06-25T10:59:36.934123Z","shell.execute_reply.started":"2021-06-25T10:59:36.889309Z","shell.execute_reply":"2021-06-25T10:59:36.933155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1.2. Preprocessing of test data**","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/les-variables-des-images-de-publicit/data_test.csv\", sep='\\t') ","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:36.936959Z","iopub.execute_input":"2021-06-25T10:59:36.937565Z","iopub.status.idle":"2021-06-25T10:59:37.10618Z","shell.execute_reply.started":"2021-06-25T10:59:36.937518Z","shell.execute_reply":"2021-06-25T10:59:37.105188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1.2.1. Filling the missing values**","metadata":{}},{"cell_type":"code","source":"df_nans = df_test[df_test.isnull().any(axis=1)] # 394 rows have one or more NaN values\nprint(df_nans)\n\nmedian_x1_t = df_test['X1'].median()\nmedian_x2_t = df_test['X2'].median()\nmedian_x3_t = df_test['X3'].median()\nmedian_x4_t = df_test['X4'].median()\n\n# print(\"median_x1:\", median_x1_t) \n# print(\"median_x2:\", median_x2_t) \n# print(\"median_x3:\", median_x3_t) \n# print(\"median_x4:\", median_x4_t) \n\ndf_test['X1'].fillna(median_x1_t, inplace=True)\ndf_test['X2'].fillna(median_x2_t, inplace=True)\ndf_test['X3'].fillna(median_x3_t, inplace=True)\ndf_test['X4'].fillna(median_x4_t, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.108167Z","iopub.execute_input":"2021-06-25T10:59:37.108444Z","iopub.status.idle":"2021-06-25T10:59:37.136396Z","shell.execute_reply.started":"2021-06-25T10:59:37.108417Z","shell.execute_reply":"2021-06-25T10:59:37.135235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head() ","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.138039Z","iopub.execute_input":"2021-06-25T10:59:37.138448Z","iopub.status.idle":"2021-06-25T10:59:37.159003Z","shell.execute_reply.started":"2021-06-25T10:59:37.138397Z","shell.execute_reply":"2021-06-25T10:59:37.158281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1. 3. Normalization of numeric dataframes of training and test data**","metadata":{}},{"cell_type":"markdown","source":"#### **1.3.1. Separating the numeric and boolean dataframes**","metadata":{}},{"cell_type":"markdown","source":"#### Creating two dataframes, as the first part is continuous variables (floating) and the second part is boolean variables :","metadata":{}},{"cell_type":"code","source":"df_train_fl = df_train.iloc[: , :4]\n# print(\"df_pubData_Train_fl:\\n\", df_train_fl.head())\n\ndf_train_bool = df_train.iloc[: , 4:]\n# print(\"boolean dataframe:\\n\", df_train_bool.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.160025Z","iopub.execute_input":"2021-06-25T10:59:37.160412Z","iopub.status.idle":"2021-06-25T10:59:37.173614Z","shell.execute_reply.started":"2021-06-25T10:59:37.160372Z","shell.execute_reply":"2021-06-25T10:59:37.172785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Same division of dataframes for the test data:","metadata":{}},{"cell_type":"code","source":"df_test_fl = df_test.iloc[: , :4]\n# print(\"df_pubData_Train_fl:\\n\", df_test_fl.head())\n\ndf_test_bool = df_test.iloc[: , 4:]\n# print(\"boolean dataframe:\\n\",df_test_bool.head())","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.174625Z","iopub.execute_input":"2021-06-25T10:59:37.175085Z","iopub.status.idle":"2021-06-25T10:59:37.182624Z","shell.execute_reply.started":"2021-06-25T10:59:37.175042Z","shell.execute_reply":"2021-06-25T10:59:37.181694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1. 3. 2. Removing outliers of numeric training dataframe**","metadata":{}},{"cell_type":"markdown","source":"#### Using the box plot below, we can easiliy visualize the outliers of the numerical columns as individual points.","metadata":{}},{"cell_type":"code","source":"# X1_column = df_test_fl[\"X1\"]\n# X2_column = df_test_fl[\"X2\"]\n# X3_column = df_test_fl[\"X3\"]\n\n# train_fl_columns = [X1_column, X2_column, X3_column]\n\n# fig, ax = plt.subplots()\n# ax.boxplot(train_fl_columns)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.183589Z","iopub.execute_input":"2021-06-25T10:59:37.183992Z","iopub.status.idle":"2021-06-25T10:59:37.198905Z","shell.execute_reply.started":"2021-06-25T10:59:37.183955Z","shell.execute_reply":"2021-06-25T10:59:37.197796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The script below computes the Z-score of each value in the column, relative to the column mean and standard deviation.\n#### While calculating the Z-score, we re-scale and center the data and look for data points which are too far from zero. If the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers. ","metadata":{}},{"cell_type":"code","source":"''' we better NOT remove the outliers of the test dataset '''\n# print(df_test_fl.shape)\n# df_test_fl_filtered = df_test_fl[(np.abs(stats.zscore(df_test_fl)) < 3).all(axis=1)] # \"all(axis=1)\" ensures that all column satisfy the constraint.\n# print(df_test_fl_filtered.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.200383Z","iopub.execute_input":"2021-06-25T10:59:37.200676Z","iopub.status.idle":"2021-06-25T10:59:37.219553Z","shell.execute_reply.started":"2021-06-25T10:59:37.200645Z","shell.execute_reply":"2021-06-25T10:59:37.218362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train_fl.shape)\ndf_train_fl_filtered = df_train_fl[(np.abs(stats.zscore(df_train_fl)) < 3).all(axis=1)]\nprint(df_train_fl_filtered.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.220981Z","iopub.execute_input":"2021-06-25T10:59:37.221364Z","iopub.status.idle":"2021-06-25T10:59:37.233785Z","shell.execute_reply.started":"2021-06-25T10:59:37.221321Z","shell.execute_reply":"2021-06-25T10:59:37.233141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame\n    \n# X1_filtered = X1_column[X1_column.between(X1_column.quantile(.05), X1_column.quantile(.95))]\n# print(X1_filtered.shape)\n# X1_filtered.plot.box()\n\n# X2_filtered = X2_column[X2_column.between(X2_column.quantile(.05), X2_column.quantile(.95))]\n# print(X2_filtered.shape)\n# X2_filtered.plot.box()\n\n# X3_filtered = X3_column[X3_column.between(X3_column.quantile(.05), X3_column.quantile(.95))]\n# print(X3_filtered.shape)\n# X3_filtered.plot.box()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.234954Z","iopub.execute_input":"2021-06-25T10:59:37.235206Z","iopub.status.idle":"2021-06-25T10:59:37.246011Z","shell.execute_reply.started":"2021-06-25T10:59:37.235181Z","shell.execute_reply":"2021-06-25T10:59:37.244983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1. 3. 3. Scaling of numeric dataframes of training and test data**","metadata":{}},{"cell_type":"code","source":"# define min max scaler\nscaler = MinMaxScaler(feature_range=(0,1))\n\ncols = [\"X1\", \"X2\", \"X3\", \"X4\"]\n# transform train data\ndf_train_fl = pd.DataFrame(scaler.fit_transform(df_train_fl), columns = cols)\n# transform test data\ndf_test_fl = pd.DataFrame(scaler.fit_transform(df_test_fl), columns = cols)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.247282Z","iopub.execute_input":"2021-06-25T10:59:37.247658Z","iopub.status.idle":"2021-06-25T10:59:37.267588Z","shell.execute_reply.started":"2021-06-25T10:59:37.247615Z","shell.execute_reply":"2021-06-25T10:59:37.266473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1. 4. Dimensionality reduction of training and test data**","metadata":{}},{"cell_type":"markdown","source":"#### While applying dimensionality reduction, first we fit the model using training data, and then we can use to transform the training and validation data.\n#### (We can't apply oversampling to validation data.)","metadata":{}},{"cell_type":"markdown","source":"#### **1. 4. 1. SVD for the dimensionality reduction of sparse boolean features** \n\n#### \"Singular Value Decomposition, or SVD, is one of the most popular techniques for dimensionality reduction for sparse data (data with many zero values).\"\n#### https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/\n#### https://machinelearningmastery.com/singular-value-decomposition-for-dimensionality-reduction-in-python/","metadata":{}},{"cell_type":"markdown","source":"#### Separating label column from training boolean dataframe:","metadata":{}},{"cell_type":"code","source":"label_train = df_train_bool.iloc[: , -1:] \n# label_train.shape # (1895, 1)\nprint(label_train.value_counts()) # 0: 1585, 1: 312\n\n# drop the \"outcome\" column (binary label column):\ndf_train_bool = df_train_bool.iloc[: , :-1] \ndf_train_bool.head()\n# df_pubData_bool.shape # (1895, 1556)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.269077Z","iopub.execute_input":"2021-06-25T10:59:37.269472Z","iopub.status.idle":"2021-06-25T10:59:37.301869Z","shell.execute_reply.started":"2021-06-25T10:59:37.269432Z","shell.execute_reply":"2021-06-25T10:59:37.300716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1. 4. 2. Optimizing \"n_components\" parameter in TruncatedSVD**","metadata":{}},{"cell_type":"markdown","source":"#### **For training boolean data:**","metadata":{}},{"cell_type":"markdown","source":"#### The script below belongs to Chris Albon: https://chrisalbon.com/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/","metadata":{}},{"cell_type":"code","source":"# Create and run an TSVD with one less than number of features\ntsvd = TruncatedSVD(n_components=df_train_bool.shape[1]-1)\nX_tsvd = tsvd.fit(df_train_bool)\n\n# List of explained variances\ntsvd_var_ratios = tsvd.explained_variance_ratio_\n\n# Calculating number of components required to pass threshold\ndef select_n_components(var_ratio, goal_var: float) -> int:\n\n    total_variance = 0.0\n    n_components = 0\n    \n    for explained_variance in var_ratio:\n        # Add the explained variance to the total\n        total_variance += explained_variance\n        # Add one to the number of components\n        n_components += 1\n        \n        if total_variance >= goal_var:\n            break\n            \n    return n_components\n\nselect_n_components(tsvd_var_ratios, 0.95) # 290","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:37.303196Z","iopub.execute_input":"2021-06-25T10:59:37.303461Z","iopub.status.idle":"2021-06-25T10:59:39.668232Z","shell.execute_reply.started":"2021-06-25T10:59:37.303433Z","shell.execute_reply":"2021-06-25T10:59:39.667331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **For test boolean data:** ","metadata":{}},{"cell_type":"code","source":"# Create and run an TSVD with one less than number of features\ntsvd = TruncatedSVD(n_components=df_test_bool.shape[1]-1)\nX_tsvd = tsvd.fit(df_test_bool)\n\n# List of explained variances\ntsvd_var_ratios = tsvd.explained_variance_ratio_\n\n# Calculating number of components required to pass threshold\ndef select_n_components(var_ratio, goal_var: float) -> int:\n\n    total_variance = 0.0\n    n_components = 0\n    \n    for explained_variance in var_ratio:\n        # Add the explained variance to the total\n        total_variance += explained_variance\n        # Add one to the number of components\n        n_components += 1\n        \n        if total_variance >= goal_var:\n            break\n            \n    return n_components\n\nselect_n_components(tsvd_var_ratios, 0.95) # 206","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:39.669363Z","iopub.execute_input":"2021-06-25T10:59:39.66986Z","iopub.status.idle":"2021-06-25T10:59:42.217176Z","shell.execute_reply.started":"2021-06-25T10:59:39.669819Z","shell.execute_reply":"2021-06-25T10:59:42.216082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1. 4. 3. Applying SVD encoding to boolean dataframes:**","metadata":{}},{"cell_type":"code","source":"SVD_model = TruncatedSVD(n_components=290).fit(df_train_bool)\nprint(df_train_bool.shape)\n\ndf_train_bool_reduced = pd.DataFrame(SVD_model.transform(df_train_bool)) # pd.DataFrame is used to retain as data frame object\nprint(df_train_bool_reduced.shape)\n\n# apply same transformation to df_test to boolean:\n\nSVD_model = TruncatedSVD(n_components=206).fit(df_test_bool)\nprint(df_test_bool.shape)\n\ndf_test_bool_reduced = pd.DataFrame(SVD_model.transform(df_test_bool))\nprint(df_test_bool_reduced.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:42.221799Z","iopub.execute_input":"2021-06-25T10:59:42.222256Z","iopub.status.idle":"2021-06-25T10:59:44.228128Z","shell.execute_reply.started":"2021-06-25T10:59:42.222211Z","shell.execute_reply":"2021-06-25T10:59:44.226982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test_bool_reduced.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.230026Z","iopub.execute_input":"2021-06-25T10:59:44.230832Z","iopub.status.idle":"2021-06-25T10:59:44.23514Z","shell.execute_reply.started":"2021-06-25T10:59:44.230776Z","shell.execute_reply":"2021-06-25T10:59:44.233849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Putting dataframes of test data together:","metadata":{}},{"cell_type":"code","source":"# indexes are not matching after preprocessing so we need to drop:\ndf_test_bool_reduced.reset_index(drop=True, inplace=True)\ndf_test_fl.reset_index(drop=True, inplace=True)\n\n# concatenation of boolean with numeric test dataframes:\nx_test = pd.concat( [df_test_fl, df_test_bool_reduced], axis=1 )\nprint((x_test.shape)) # (820, 27)\nx_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.241305Z","iopub.execute_input":"2021-06-25T10:59:44.245248Z","iopub.status.idle":"2021-06-25T10:59:44.295426Z","shell.execute_reply.started":"2021-06-25T10:59:44.245176Z","shell.execute_reply":"2021-06-25T10:59:44.294368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **1. 5. Balancing the highly imbalanced dataset**","metadata":{}},{"cell_type":"markdown","source":"#### **1. 5. 1. Visualizing the ratio of classes**","metadata":{}},{"cell_type":"code","source":"target_count = df_train.outcome.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)')","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.300745Z","iopub.execute_input":"2021-06-25T10:59:44.303374Z","iopub.status.idle":"2021-06-25T10:59:44.445485Z","shell.execute_reply.started":"2021-06-25T10:59:44.303306Z","shell.execute_reply":"2021-06-25T10:59:44.444378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **1. 5. 2. Applying several oversampling methods and comparing with cross validation**","metadata":{}},{"cell_type":"code","source":"# oversample = RandomOverSampler(sampling_strategy='minority')\n# x_over, y_over = oversample.fit_resample(df_train_bool, label_train)\n\n# sm = SMOTE(random_state = 42)\n# x_sm, y_sm = sm.fit_resample(df_train_bool, label_train)\n\n# sm = ADASYN()\n# x_syn, y_syn = sm.fit_resample(df_train_bool, label_train)\n\n# # print(\"sizes before:\", df_train_bool.shape, label_train.shape)\n# # print(\"sizes after RandomOverSampler resampling:\",x_over.shape, y_over.shape)\n# # print(\"sizes after SMOTE resampling:\",x_sm.shape, y_sm.shape)\n# # print(\"sizes after ADASYN resampling:\",x_syn.shape, y_syn.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.44663Z","iopub.execute_input":"2021-06-25T10:59:44.446871Z","iopub.status.idle":"2021-06-25T10:59:44.450568Z","shell.execute_reply.started":"2021-06-25T10:59:44.446846Z","shell.execute_reply":"2021-06-25T10:59:44.449516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # define the pipeline\n# steps = [('svd', TruncatedSVD(n_components=272)), ('m', LogisticRegression())] \n# model = Pipeline(steps=steps)\n\n# # evaluate model\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n\n# y_pred_ROS = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) # 0.97 recall for nonad\n# print(\"classification report after RandomOverSampler:\\n\", classification_report(y_over, y_pred_ROS))\n\n# y_pred_sm = cross_val_predict(model, x_sm, y_sm, cv=cv, n_jobs=-1)\n# print(\"classification report after SMOTE:\\n\", classification_report(y_sm, y_pred_sm))\n\n# y_pred_syn = cross_val_predict(model, x_syn, y_syn, cv=cv, n_jobs=-1)\n# print(\"classification report after ADASYN:\\n\", classification_report(y_syn, y_pred_syn))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.451673Z","iopub.execute_input":"2021-06-25T10:59:44.451954Z","iopub.status.idle":"2021-06-25T10:59:44.461867Z","shell.execute_reply.started":"2021-06-25T10:59:44.451901Z","shell.execute_reply":"2021-06-25T10:59:44.461096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RandomOverSampler gives the highest recall and f1-score on boolean datasets, so it will be the sampling method to apply:","metadata":{}},{"cell_type":"code","source":"oversample = RandomOverSampler(sampling_strategy='minority')\n# oversampling of boolean dataframe:\nx_over_bool, y_over = oversample.fit_resample(df_train_bool_reduced, label_train) # df_train_bool_reduced before oversampling: (1897, 273), label_train: (1897, 1)\nprint((x_over_bool.shape), (y_over.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.462899Z","iopub.execute_input":"2021-06-25T10:59:44.463358Z","iopub.status.idle":"2021-06-25T10:59:44.524123Z","shell.execute_reply.started":"2021-06-25T10:59:44.463317Z","shell.execute_reply":"2021-06-25T10:59:44.523158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dataframe sizes before oversampling: (1897, 290), (1897, 1) ; after oversampling: (3170, 290), (3170, 1)","metadata":{}},{"cell_type":"code","source":"# oversampling of floating dataframe:\nx_over_fl, y_over = oversample.fit_resample(df_train_fl, label_train)\nprint((x_over_fl.shape), (y_over.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.525321Z","iopub.execute_input":"2021-06-25T10:59:44.525603Z","iopub.status.idle":"2021-06-25T10:59:44.539841Z","shell.execute_reply.started":"2021-06-25T10:59:44.525575Z","shell.execute_reply":"2021-06-25T10:59:44.538694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Putting dataframes of train data together:","metadata":{}},{"cell_type":"code","source":"# concatenation of boolean with numeric training dataframes:\nx_over = pd.concat( [x_over_fl, x_over_bool], axis=1 )\n# print((x_over.shape))\nx_over.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-25T10:59:44.54114Z","iopub.execute_input":"2021-06-25T10:59:44.541413Z","iopub.status.idle":"2021-06-25T10:59:44.57694Z","shell.execute_reply.started":"2021-06-25T10:59:44.541381Z","shell.execute_reply":"2021-06-25T10:59:44.575706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **2. Evaluating classification models with 10-fold cross validation**","metadata":{}},{"cell_type":"markdown","source":"#### À chaque itération, cross_val_predict générera un score métrique individuel pour ce lot. En fin de compte, il renverra k score pour chaque itération. ","metadata":{}},{"cell_type":"code","source":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n\nmodel = LogisticRegression()\ny_pred_logr = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) \nprint(\"classification report for logistic regression classifier:\\n\", classification_report(y_over, y_pred_logr))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T11:00:49.899703Z","iopub.execute_input":"2021-06-25T11:00:49.900092Z","iopub.status.idle":"2021-06-25T11:00:50.577884Z","shell.execute_reply.started":"2021-06-25T11:00:49.900057Z","shell.execute_reply":"2021-06-25T11:00:50.576821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DecisionTreeClassifier()\ny_pred_dt = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) \nprint(\"classification report for decision tree classifier:\\n\", classification_report(y_over, y_pred_dt))","metadata":{"execution":{"iopub.status.busy":"2021-06-25T11:00:53.806895Z","iopub.execute_input":"2021-06-25T11:00:53.807319Z","iopub.status.idle":"2021-06-25T11:00:57.930504Z","shell.execute_reply.started":"2021-06-25T11:00:53.807286Z","shell.execute_reply":"2021-06-25T11:00:57.929712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### In the above confusion matrices, both methods are resulting with same *average accuracy* score. Since our data has a bias for minority class \"nonad\", we need to compare *recall* and *precision* values to be sure that class is correctly classified. For the non-ad class, *recall* value returns 0.93 with the decision tree classifier, and 0.98 with the logistic regression classifier. Therefore, the logistic regression classifier outperforms the decision tree classifier for this dataset.","metadata":{}},{"cell_type":"markdown","source":"## **3. Predictions based on several models**","metadata":{}},{"cell_type":"markdown","source":"### **3. 1. Fitting a logistic regression model**","metadata":{}},{"cell_type":"code","source":"print(x_over.shape)\nprint(y_over.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.558551Z","iopub.status.idle":"2021-06-23T09:51:22.560418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_model = LogisticRegression()\nlogreg_model.fit(x_over, y_over)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.562238Z","iopub.status.idle":"2021-06-23T09:51:22.564046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predicting the test set results:","metadata":{}},{"cell_type":"code","source":"y_pred = logreg_model.predict(x_test) # output: numpy.ndarray","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.565855Z","iopub.status.idle":"2021-06-23T09:51:22.567714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # save to csv file\n# savetxt('y_pred.csv', y_pred, delimiter=',', fmt=('%s'))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.569507Z","iopub.status.idle":"2021-06-23T09:51:22.571341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3. 2. Fitting a deep neural network model**","metadata":{}},{"cell_type":"code","source":"# training dataset: x_over, y_over\n# testing dataset: x_test\n\n# transform label dataset from string to integer https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html \nfrom sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\ny_over = lb.fit_transform(y_over) # 'numpy.ndarray' object ","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.573207Z","iopub.status.idle":"2021-06-23T09:51:22.575053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data to get validation dataset:\nx_train, x_valid, y_train, y_valid = train_test_split(x_over, y_over, test_size=0.33, shuffle= True)\n# print(x_train.shape)\n# print(y_train.shape)\n# print(x_valid.shape)\n# print(y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.576877Z","iopub.status.idle":"2021-06-23T09:51:22.578775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **To try in future:** We can model 2 different networks by using 2 different input sets we have (boolean and numeric) and then concatenate these at the end: \nhttps://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/ ","metadata":{}},{"cell_type":"code","source":"# how to choose between sigmoid or relu activation function?\n#\"Relu is less susceptible to vanishing gradients that prevent deep models from being trained, although it can suffer from other problems like saturated or “dead” units.\"\n# https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\n\n# Dropout regularization to prevent overfitting https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n# \"You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\"\n\n# create a multilayer perceptron model with number of layers 7:\ndef create_model():\n    model = Sequential()\n    model.add(Dense(128, input_dim = x_train.shape[1], activation = 'sigmoid'))\n    model.add(Dropout(.2))\n    model.add(Dense(64, activation = 'relu'))\n    model.add(Dropout(.2))\n    model.add(Dense(32, activation = 'relu'))\n    model.add(Dropout(.2))\n    model.add(Dense(16, activation = 'relu'))\n    model.add(Dropout(.1))\n    model.add(Dense(8, activation = 'relu'))\n    model.add(Dense(2, activation = 'relu'))\n    model.add(Dense(1, activation = 'sigmoid'))\n\n# compile model:\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), tf.keras.metrics.Accuracy()])\n    # Binary cross-entropy loss is used for binary (0 or 1) classification applications. \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.581211Z","iopub.status.idle":"2021-06-23T09:51:22.582739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model with standardized dataset\nkeras_model = KerasClassifier(build_fn = create_model, verbose=1) # \"verbose=2\" to see the training progress for each epoch.\n\n# fit the keras model on the dataset\nhistory = keras_model.fit(x_train, y_train, \n                    epochs=300, \n                    batch_size=50, \n                    validation_data=(x_valid, y_valid), \n                    shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.583947Z","iopub.status.idle":"2021-06-23T09:51:22.584667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_dict = history.history\n# print(history_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.58582Z","iopub.status.idle":"2021-06-23T09:51:22.586533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.587703Z","iopub.status.idle":"2021-06-23T09:51:22.58841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = keras_model.predict(x_test) # output: numpy.ndarray\n# print(y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.589517Z","iopub.status.idle":"2021-06-23T09:51:22.590225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reverse transform the prediction to string value:\ny_pred = lb.inverse_transform(y_pred, threshold=None)\n# print(y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T09:51:22.591335Z","iopub.status.idle":"2021-06-23T09:51:22.59202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}