{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport shutil\n\nimport random\nimport pickle\n\nfrom ast import literal_eval\nfrom tqdm import tqdm as print_progress\nfrom glob import glob\n\nimport dask.dataframe as dd\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['TF_KERAS'] = '1'\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.metrics import TopKCategoricalAccuracy\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, Callback\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import (\n    Layer, \n    Input, InputLayer, Embedding, \n    Dropout, Dense, \n    Dot, Concatenate, Average, Add,\n    Bidirectional, LSTM,\n    Lambda, Reshape\n)\nfrom tensorflow.keras.activations import softmax, sigmoid\nfrom tensorflow.keras.initializers import Identity, GlorotNormal\nfrom tensorflow.keras.utils import plot_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install stellargraph","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install gradient-centralization-tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"label_encoder = pickle.load(open('../input/hotel-comment/label_encoder.pkl', 'rb'))\nlabels = list(label_encoder.classes_)\nlen(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Generator**","metadata":{}},{"cell_type":"code","source":"labels_matrix = np.load('../input/hotel-comment-valtest-distiluse/labels_embeddings.npy')\nlabels_matrix.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom ast import literal_eval\nfrom tensorflow.keras.utils import Sequence, to_categorical\n\n\nclass DataGenerator(Sequence):\n\n    def __init__(self,\n                 data_root,\n                 labels_fixed: np.array,\n                 max_seq_len: int=512,\n                 batch_size: int = 256, \n                 shuffle: bool = True):\n        \n        self.data_root = data_root\n        self.num_labels = labels_fixed.shape[-2]\n        self.max_seq_len = max_seq_len\n        self.labels_fixed = labels_fixed\n        self.embedding_dim = labels_fixed.shape[-1]\n        \n        # list of files containing both word-embeddings and multi-labels\n        if isinstance(self.data_root, str):\n            self.files = glob(os.path.join(self.data_root, 'sample_*.npz'))\n        elif isinstance(self.data_root, (list, tuple)):\n            self.files = []\n            for data_dir in self.data_root:\n                self.files += glob(os.path.join(data_dir, 'sample_*.npz'))\n                \n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.indices = np.array(list(range(len(self.files))))\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\"\n        Denotes the number of batches per epoch\n        \"\"\"\n        n_samples = len(self.files)\n        return n_samples//self.batch_size + (0 if n_samples%self.batch_size==0 else 1)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generate one batch of data\n        \"\"\"\n        # Generate indexes of the batch\n        start_index = self.batch_size * index\n        end_index = self.batch_size * (index+1)\n        indices = self.indices[start_index:end_index]\n\n        # Generate data\n        wb_batch = []\n        mtl_batch = []\n        for idx in indices:\n            \n            sample_file = self.files[idx]\n            \n            # Load word embeddings\n            wb_pad = np.zeros((self.max_seq_len, self.embedding_dim))\n            wb = np.load(sample_file)['emb']\n            wb_pad[:wb.shape[0],:] = wb\n            wb_batch += [wb_pad]\n            \n            # Load multi-labels\n            mtl = np.load(sample_file)['mtl']\n            mtl_batch += [self.smooth_labels(mtl)]\n        return [np.array(wb_batch), self.labels_fixed], np.array(mtl_batch)\n\n    def smooth_labels(self, labels, factor=0.1):\n        # smooth the labels\n        labels *= (1 - factor)\n        labels += (factor / labels.shape[-1])\n        return labels\n\n    def on_epoch_end(self):\n        \"\"\"\n        Update indices after each epoch\n        \"\"\"\n        if self.shuffle:\n            self.indices = sklearn.utils.shuffle(self.indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dset_paths = [\n    ['../input/hotel-comment-valtest-distiluse/valuatingdata'],\n    ['../input/hotel-comment-testset-distiluse/testingdata']\n]\ndata_generator = dict()\nfor dataset, dset_path in zip(['valuating', 'testing'], dset_paths):\n    data_generator[dataset] = DataGenerator(data_root=dset_path, \n                                            labels_fixed=labels_matrix, \n                                            batch_size=128, \n                                            shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_generator['testing']), len(data_generator['valuating'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data_generator['testing'][0]\nprint(X[0][0].shape)\nprint(X[0][1].shape)\nprint(X[1].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Model**","metadata":{}},{"cell_type":"code","source":"class Adjacency(Layer):\n\n    def __init__(self, nodes=1, weights=None, init_method='identity'):\n        super(Adjacency, self).__init__()\n\n        self.shape = (1, nodes, nodes)\n\n        if weights is not None:\n            assert weights.shape==(nodes, nodes), \\\n                f'Adjacency Matrix must have shape ({nodes}, {nodes})' + \\\n                f' while its shape is {weights.shape}'\n            w_init = tf.convert_to_tensor(weights)\n        else:\n            init_method = init_method.lower()\n            if init_method == 'identity':\n                initializer = tf.initializers.Identity()\n            elif init_method in ['xavier', 'glorot']:\n                initializer = tf.initializers.GlorotNormal()\n            w_init = initializer(shape=(nodes, nodes))\n\n        self.w = tf.Variable(\n            initial_value=tf.expand_dims(w_init, axis=0), \n            dtype=\"float32\", trainable=True\n        )\n\n    def call(self, inputs):\n        return tf.convert_to_tensor(self.w)\n\n    def compute_output_shape(self):\n        return self.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gctf\nfrom stellargraph.layer import GraphAttention\nfrom stellargraph.utils import plot_history\n\n\ndef buil_MAGNET(n_labels,\n                embedding_dim: int,\n                sequence_length: int=512, \n                lstm_units: int=64,\n                dropout_rates=[0.3, 0.2],\n                attention_heads=[4, 2],\n                adjacency_matrix=None,\n                adjacency_generation='xavier', # 'identity' or 'xavier' or 'glorot'\n                feed_text_embeddings=True, # if False, add additional Embedding layer\n                text_embeddings_matrix=None, # initialized weights for text Embedding layer\n                feed_label_embeddings=True, # if False, add additional Embedding layer\n                label_embeddings_matrix=None, # initialized weights for label Embedding layer\n                ) -> Model:\n\n    if isinstance(attention_heads, int):\n        attention_heads = [attention_heads, attention_heads]\n    if not isinstance(attention_heads, (list, tuple)):\n        raise ValueError('`attention_heads` must be INT, LIST or TUPLE')\n\n    # 1. Sentence Representation\n    if feed_text_embeddings:\n        sentence_model = Sequential(name='sentence_model')\n        sentence_model.add(Dropout(dropout_rates[0], input_shape=(sequence_length, embedding_dim), name='word_embeddings'))\n        word_inputs, word_embeddings = sentence_model.inputs, sentence_model.outputs\n    else:\n        word_inputs = Input(shape=(sequence_length, ), name='word_inputs')\n        embedding_args = {\n            'input_dim': sequence_length,\n            'output_dim': embedding_dim,\n            'name': 'word_embeddings'\n        }\n        if text_embeddings_matrix is not None \\\n            and text_embeddings_matrix.shape==(sequence_length, embedding_dim):\n            embedding_args['weights'] = [text_embeddings_matrix]\n        word_embeddings = Embedding(**embedding_args)(word_inputs)\n        word_embeddings = Dropout(dropout_rates[0], name='WE_dropout')(word_embeddings)\n    \n    forward_rnn = LSTM(units=lstm_units, return_sequences=True, name='forward_rnn')\n    backward_rnn = LSTM(units=lstm_units, return_sequences=True, name='backward_rnn', go_backwards=True)\n    bidir_rnn = Bidirectional(layer=forward_rnn, backward_layer=backward_rnn, merge_mode=\"concat\", name='bidir_rnn')\n    \n    sentence_repr = bidir_rnn(word_embeddings)\n    sentence_repr = K.mean(sentence_repr, axis=1)\n    # print(f\"sentence_repr: {K.int_shape(sentence_repr)}\")\n\n    # 2. Labels Representation\n    if feed_label_embeddings:\n        label_inputs = Input(batch_shape=(1, n_labels, embedding_dim), name='label_embeddings')\n        label_embeddings = label_inputs\n    else:\n        label_inputs = Input(batch_shape=(1, n_labels), name='label_inputs')\n        embedding_args = {'input_dim': n_labels,\n                          'output_dim': embedding_dim,\n                          'name': 'label_embeddings'}\n        if label_embeddings_matrix is not None \\\n            and label_embeddings_matrix.shape==(n_labels, embedding_dim):\n            embedding_args['weights'] = [label_embeddings_matrix]\n        label_embeddings = Embedding(**embedding_args)(label_inputs)\n        label_embeddings = Dropout(rate=dropout_rates[0], name='LE_dropout')(label_embeddings)\n    label_embeddings = Dense(units=embedding_dim//2, name='label_embeddings_reduced')(label_embeddings)\n    # print(f\"label_inputs: {K.int_shape(label_inputs)}\")\n\n    label_correlation = Adjacency(nodes=n_labels, \n                                  weights=adjacency_matrix,\n                                  init_method=adjacency_generation)(label_embeddings)\n    # print(f\"label_correlation: {K.int_shape(label_correlation)}\")\n\n    label_attention = GraphAttention(units=embedding_dim//2//attention_heads[0],\n                                     activation='tanh',\n                                     attn_heads=attention_heads[0],\n                                     in_dropout_rate=dropout_rates[1],\n                                     attn_dropout_rate=dropout_rates[1], )([label_embeddings, label_correlation])\n    # print(f\"label_attention: {K.int_shape(label_attention)}\")\n\n    label_residual = Add(name='label_residual')([label_attention, label_embeddings])\n    # print(f\"label_residual: {K.int_shape(label_residual)}\")\n\n    label_repr = GraphAttention(units=2*lstm_units,\n                                activation='tanh',\n                                attn_heads_reduction='average',\n                                attn_heads=attention_heads[1],\n                                in_dropout_rate=dropout_rates[1],\n                                attn_dropout_rate=dropout_rates[1], )([label_residual, label_correlation])\n\n    label_repr = K.sum(label_repr, axis=0, keepdims=False)\n    # print(f\"label_repr: {K.int_shape(label_repr)}\")\n\n    # 3. Prediction\n    prediction = tf.einsum('Bk,Nk->BN', sentence_repr, label_repr)\n    prediction = sigmoid(prediction)\n    # print(f\"prediction: {K.int_shape(prediction)}\")\n\n    return Model(inputs=[word_inputs, label_inputs], outputs=prediction, name='MAGNET')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_cross_entropy(y_true, y_pred, pos_weight=1.69):\n    losses = y_true * -K.log(y_pred) * pos_weight + (1-y_true) * -K.log(1-y_pred)\n    losses = K.clip(losses, 0.0, 11.27)\n    return K.mean(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gctf\nfrom stellargraph.utils import plot_history\n\n\nclass MAGNET:\n\n    def __init__(self, n_labels: int, embedding_dim: int, model_ckpt=None):\n\n        self.embedding_dim = embedding_dim\n\n        # Build model(s)\n        print(f\"\\n\\n\\nBuilding MAGNET ...\\n\\n\\n\")\n        self.model = buil_MAGNET(n_labels, embedding_dim=embedding_dim, sequence_length=512, lstm_units=32)\n        self.model.summary()\n        \n        # Load weights\n        if model_ckpt:\n            try:\n                print(f\"Try to load checkpoint @{model_ckpt} ...\")\n                self.model.load_weights(model_ckpt)\n            except:\n                print(f\"\\t==> Loading Fail !!!\")\n            \n    def compile(self, model_saved: str, logs_path: str, schedule_step: int, verbose: int=1):\n                    \n        # Compile optimizer, loss & metric functions\n        print(f\"Compiling MAGNET using \\n\\tgrad-centralized ADAM, \\n\\ttop-k Accuracy, \\n\\tweighted Cross-Entropy \\n...\")\n        self.model.compile(optimizer=gctf.optimizers.adam(learning_rate=0.00169), \n                           # optimizer=Adam(learning_rate=0.001), \n                           metrics=[\"accuracy\", TopKCategoricalAccuracy(k=3)],\n                           loss=weighted_cross_entropy)\n\n        # Define Callbacks\n        return [\n            ModelCheckpoint(filepath=model_saved, monitor='accuracy', save_weights_only=True, save_best_only=False, save_freq='epoch'),\n        ]\n\n    def train(self, train_generator, val_generator, \n                    model_saved: str, logs_path: str,\n                    max_epochs: int=50, verbose: int=1):\n        # Compile\n        schedule_step = len(train_generator) // 2\n        custom_callbacks = self.compile(model_saved, logs_path, schedule_step, verbose)\n\n        # Training\n        train_history = self.model.fit_generator(generator=train_generator,\n                                                 steps_per_epoch=len(train_generator),\n                                                 validation_data=val_generator,\n                                                 validation_steps=len(val_generator),\n                                                 callbacks=custom_callbacks, \n                                                 epochs=max_epochs,\n                                                 initial_epoch=0)\n        return train_history\n\n    def load_weights(self, weight_path: str):\n        self.model.load_weights(weight_path)\n\n    def predict(self, sent_embeddings: np.array, label_embeddings: np.array):\n        sent_embeddings = np.reshape(sent_embeddings, (-1, 512, self.embedding_dim))\n        preds = self.model([sent_embeddings, label_embeddings], training=False)\n        return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_LABELS = labels_matrix.shape[-2]\nembedding_dim = labels_matrix.shape[-1]\n\nmodel = MAGNET(n_labels=N_LABELS, \n               embedding_dim=embedding_dim, \n               model_ckpt='../input/magnet-distiluse-checkpoints/weightedCE/ep027_acc0.388_val_acc0.370_topk0.753_val_topk0.793.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predict**","metadata":{}},{"cell_type":"code","source":"eval_dir = '/kaggle/working/evaluations'\nif not os.path.isdir(eval_dir):\n    os.makedirs(eval_dir)\n    \npred_dir = '/kaggle/working/predictions'\nif not os.path.isdir(pred_dir):\n    os.makedirs(pred_dir)\n    \ncfs_mtrx_dir = os.path.join(eval_dir, 'confusion_matrix')\nif not os.path.isdir(cfs_mtrx_dir):\n    os.makedirs(cfs_mtrx_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator = data_generator['testing']\npreds, ys_true = [], []\nfor i in print_progress(range(len(test_generator))):\n    Xs, labels = test_generator[i]\n    labels = np.where(labels>0.5, 1., 0.)\n    Y = model.predict(*Xs).numpy()\n    preds.extend(Y)\n    ys_true.extend(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluate**","metadata":{}},{"cell_type":"code","source":"import string\nfrom sklearn.metrics import f1_score, multilabel_confusion_matrix\n\n\ndef Precision_Recall_at_K(y_true: np.array, y_pred: np.array, K: int=-1, threshold: float=0.5):\n\n    # Clip by threshold\n    preds = np.where(y_pred>threshold, y_pred, 0.0)\n\n    # Rank predictions\n    preds = np.argsort(y_pred)[::-1]\n\n    # Convert multi-hot into categories\n    labels = np.where(y_true==1)[0]\n\n    # Calculate precision@k and recall@k\n    precisions, recalls, relevances = np.zeros(len(preds)), np.zeros(len(preds)), np.zeros(len(preds))\n    n_corrects = 0\n    for k in range(len(preds)):\n        if preds[k] in labels:\n            n_corrects += 1\n            relevances[k] = 1\n        precisions[k] = n_corrects / (k+1)\n        recalls[k] = n_corrects / len(labels)\n    return precisions[:K], recalls[:K], relevances[:K]\n\n\ndef AP_at_K(y_true: np.array, y_pred: np.array, K: int=3, post_norm: bool=False):\n\n    # Average Precision at top-k predictions\n    precision, _, relevance = Precision_Recall_at_K(y_true, y_pred)\n    mean_precision = np.dot(precision, relevance)\n    if post_norm:\n        return mean_precision\n\n    L = len(np.where(y_true==1)[0])\n    return mean_precision / min(K,L)\n\n\ndef MAP_at_K(ys_true: list, ys_pred: list, K=3):\n    L = np.max([len(np.where(y_true==1)[0]) for y_true in ys_true])\n    B = len(ys_true)\n\n    # Mean Average Precision at batch's top-k predictions\n    APs = [AP_at_K(y_true, y_pred, K, post_norm=True) \\\n               for y_true, y_pred in zip(ys_true, ys_pred)]\n    return np.sum(APs) / (B*min(L,K))\n\n\ndef TP_multilabel(ys_true: np.array, ys_pred: np.array, threshold=0.5):\n    n_classes = ys_true.shape[1]\n    if isinstance(threshold, float):\n        threshold = [threshold] * n_classes\n    if len(threshold) != n_classes:\n        raise ValueError('Number of thresholds doesnt match number of classes')\n\n    TP_by_classes = []\n    for clss in range(n_classes):\n        TP_by_classes += [\n            np.sum(np.logical_and(ys_pred[:, clss]>=threshold[clss], ys_true[:, clss]==1))\n        ]\n    return TP_by_classes\n\n\ndef TN_multilabel(ys_true: np.array, ys_pred: np.array, threshold=0.5):\n    n_classes = ys_true.shape[1]\n    if isinstance(threshold, float):\n        threshold = [threshold] * n_classes\n    if len(threshold) != n_classes:\n        raise ValueError('Number of thresholds doesnt match number of classes')\n\n    TN_by_classes = []\n    for clss in range(n_classes):\n        TN_by_classes += [\n            np.sum(np.logical_and(ys_pred[:, clss]<threshold[clss], ys_true[:, clss]==0))\n        ]\n    return TN_by_classes\n\n\ndef FP_multilabel(ys_true: np.array, ys_pred: np.array, threshold=0.5):\n    n_classes = ys_true.shape[1]\n    if isinstance(threshold, float):\n        threshold = [threshold] * n_classes\n    if len(threshold) != n_classes:\n        raise ValueError('Number of thresholds doesnt match number of classes')\n\n    FP_by_classes = []\n    for clss in range(n_classes):\n        FP_by_classes += [\n            np.sum(np.logical_and(ys_pred[:, clss]>=threshold[clss], ys_true[:, clss]==0))\n        ]\n    return FP_by_classes\n\n\ndef FN_multilabel(ys_true: np.array, ys_pred: np.array, threshold=0.5):\n    n_classes = ys_true.shape[1]\n    if isinstance(threshold, float):\n        threshold = [threshold] * n_classes\n    if len(threshold) != n_classes:\n        raise ValueError('Number of thresholds doesnt match number of classes')\n\n    FN_by_classes = []\n    for clss in range(n_classes):\n        FN_by_classes += [\n            np.sum(np.logical_and(ys_pred[:, clss]<threshold[clss], ys_true[:, clss]==1))\n        ]\n    return FN_by_classes\n\n\ndef hamming_loss(y_true, y_pred, mode='multilabel'):\n    mode = mode.lower()\n    if mode not in ['multiclass', 'multilabel']:\n        raise TypeError('`mode` must be: [None, multilabel])')\n\n    if mode == 'multiclass':\n        non_zero_values = tf.cast(tf.math.count_nonzero(y_true*y_pred, axis=-1), tf.float32)\n        return 1.0 - non_zero_values\n    else:\n        non_zero_values = tf.cast(tf.math.count_nonzero(y_true-y_pred, axis=-1), tf.float32)\n        return non_zero_values / y_true.shape[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fn = '../input/hotel-comment/testing_data*.csv'\ndataset = dd.read_csv(fn).compute().loc[:len(preds)-1, :]\ndataset = dataset[['Comment', 'Sections_Name', 'label_encoder']]\n\nys_true, ys_pred = np.array(ys_true), np.array(preds)\nprint(ys_true.shape, ys_pred.shape)\nfor K in range(3):\n    print(f\"MAP@{K+1} = {MAP_at_K(ys_true, ys_pred, K+1)}\")\n    \nprint('Calculating Confusion Matrix ...')\nTP_by_classes = TP_multilabel(ys_true, ys_pred)\nTN_by_classes = TN_multilabel(ys_true, ys_pred)\nFP_by_classes = FP_multilabel(ys_true, ys_pred)\nFN_by_classes = FN_multilabel(ys_true, ys_pred)\n\nprint('\\n\\n\\nConfusion Matrix\\n')\nfor clss, (TP, TN, FP, FN) in enumerate(zip(TP_by_classes, TN_by_classes, FP_by_classes, FN_by_classes)):\n    clss_name = label_encoder.inverse_transform([clss])[0]\n    clss_name = clss_name.translate(str.maketrans('', '', string.punctuation))\n    print(f'\\tClass {clss_name}\\n')\n    print(f'\\t\\tTP = {TP:05d}\\tFP = {FP:05d}\\n\\t\\tFN = {FP:05d}\\tTN = {TN:05d}\\n')\n\n    df_cm = pd.DataFrame([[TP, FP], [FN, TN]], ['True', 'False'], ['True', 'False'])\n    sns.set(font_scale=1.4) # for label size\n    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n    plt.savefig(os.path.join(cfs_mtrx_dir, f'{clss_name}.png'))\n    plt.clf()\n\n# Calculate micro-metrics\nprint('Calculating other metrics ...')\nprint('\\n\\n\\nMicro Metrics\\n')\n\nPrecision_micro = np.sum(TP_by_classes) / (np.sum(TP_by_classes) + np.sum(FP_by_classes))\nprint(f'\\tPrecision_micro = {Precision_micro}\\n')\n\nRecall_micro = np.sum(TP_by_classes) / (np.sum(TP_by_classes) + np.sum(FN_by_classes))\nprint(f'\\tRecall_micro = {Recall_micro}\\n')\n\nF1_micro = 2*np.sum(TP_by_classes) / (2*np.sum(TP_by_classes) + np.sum(FP_by_classes) + np.sum(FN_by_classes))\nprint(f'\\tF1_micro = {F1_micro}\\n')\n\n# Calculate Hamming loss\nys_pred = np.where(ys_pred<0.5, 0, 1)\nloss = hamming_loss(ys_true, ys_pred)\nloss_ = np.sum(loss) / len(loss)\nprint(f'\\n\\n\\nhamming_loss = {loss_}\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.chdir(r'/kaggle/working')\n# dir_path = '/kaggle/working/'\n# shutil.make_archive(dir_path+\"data\", 'zip', dir_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}