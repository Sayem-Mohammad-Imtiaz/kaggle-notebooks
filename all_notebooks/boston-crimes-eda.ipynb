{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h3>Import Libraries & Data</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.style.use('ggplot')\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the first five items in our dataset."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/boston-ds/crime.csv\", index_col = None, encoding='windows-1252', parse_dates = ['OCCURRED_ON_DATE', 'YEAR', 'DAY_OF_WEEK'], engine='python')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's find out how many entries there are in our dataset and what type the variables are."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'The dataset contains %s rows and %s columns' % (df.shape[0],df.shape[1]), '\\n')\nprint('The columns and the its values types:\\n')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Task1 - Top Offense Code</h3>\n<br>Here I am investigating the most \"popular\" report registered. This will help us to better understand the structure and dataset in general. Additionally, this will be useful for our further discoveries."},{"metadata":{},"cell_type":"markdown","source":"Let's load the offense code dataset which will help us with code decoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"codes = pd.read_csv(\"../input/boston-ds/offense_codes.csv\", index_col = None, encoding='windows-1252', engine='python')\ncodes.CODE.value_counts() #This line is for checking of whether codes are unique or not\ncodes.drop_duplicates(subset=['CODE'], keep='first', inplace=True) #Since there are duplicates, let's drop them","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look on this dataset briefly"},{"metadata":{"trusted":true},"cell_type":"code","source":"codes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'The dataset contains %s rows and %s columns' % (codes.shape[0],codes.shape[1]), '\\n')\nprint('The columns and the its values types:\\n')\ncodes.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, lets' sumup the codes from the main dataset and sort them descending"},{"metadata":{"trusted":true},"cell_type":"code","source":"top = df.OFFENSE_CODE.value_counts().to_frame().reset_index(level=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top.columns.values[0] = 'CODE'\ntop.columns.values[1] = 'TOTAL_AMOUNT'\ntop.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we are merging <i>top</i> dataset with <i>codes</i> on <i>'CODE'</i> column as a key"},{"metadata":{"trusted":true},"cell_type":"code","source":"code_top = top.merge(codes, on='CODE', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Brief observation of that everything went smoothly:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"code_top.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Visualization of Task1</b>. <br>\nHere, for the further exploratory purpuse, I added OFFENSE_CODE_GROUP column to our decoded top list."},{"metadata":{"trusted":true},"cell_type":"code","source":"gr = df.loc[:,['OFFENSE_CODE','OFFENSE_CODE_GROUP']]\ngr.info()\ncode_tg = pd.merge(code_top, gr, left_on='CODE', right_on='OFFENSE_CODE', how='inner')\ncode_tg.drop_duplicates(subset=['CODE'], keep='first', inplace=True)\ncode_tg.reset_index(drop=True,inplace=True)\ncode_tg.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most interesting part of Task1 - visualization! Look how the plot is built in code.<br>\nHere only first 20 codes are pictured."},{"metadata":{"trusted":true},"cell_type":"code","source":"code_tg.head(20).plot(kind = 'barh', x = 'NAME', y = 'TOTAL_AMOUNT', figsize=(12, 12))\n\nplt.gca().invert_yaxis() \n\nplt.xlabel('Number of Reports')\nplt.ylabel('Offense Type')\ndf.sort_values(['OCCURRED_ON_DATE'], ascending=True, inplace=True)\nplt.title('Boston Offense Rating: ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[-1]))\n\n# This loop automatically add the value of each position to the each bar:\nfor index, value in enumerate(code_tg.head(20)['TOTAL_AMOUNT']):\n    label = format(int(value), ',')\n    plt.annotate(label, xy=(value - 100, index + 0.10), ha='right', color='white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, I'd also like to know the top reports rating, grouped by OFFENSE_CODE_GROUP.<br>\nFor this reason we need to make some additional manipulations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_gr = code_tg.groupby(['OFFENSE_CODE_GROUP'], as_index=False).sum(axis=1)\ntop_gr = top_gr[['OFFENSE_CODE_GROUP','TOTAL_AMOUNT']].sort_values(['TOTAL_AMOUNT'], ascending=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_gr.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's plot it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_gr.head(10).plot(kind = 'barh', x = 'OFFENSE_CODE_GROUP', y = 'TOTAL_AMOUNT', figsize=(12, 5))\n\nplt.gca().invert_yaxis() \n\nplt.xlabel('Number of Reports')\nplt.ylabel('Offense Group')\nplt.title('Boston Offense Groups Rating: ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[-1]))\n\n\n# This loop automatically add the value of each position to the each bar:\nfor index, value in enumerate(top_gr.head(10)['TOTAL_AMOUNT']):\n    label = format(int(value), ',')\n    plt.annotate(label, xy=(value - 300, index + 0.13), ha='right', color='white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see a slightly different picture rather than comparing offense names individually. The most-reported here are incidents related to Motor Vehicle Incidents."},{"metadata":{},"cell_type":"markdown","source":"<h3>Task2 - Most Shooting Areas</h3>\n<br>This part is about the understanding of which boroughs are most dangerous in the meaning of shooting level. The library I used here is <b>folium</b> - it works with maps and coordinates, can combine the close locations into groups and many other things"},{"metadata":{},"cell_type":"markdown","source":"Firstly, let's extract the rows we are interested in. We need those with a 'Y' shooting mark and the filled district value."},{"metadata":{"trusted":true},"cell_type":"code","source":"shtng = df[(df.SHOOTING == 'Y') & (df.DISTRICT.notnull())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After that, we can plot the incidents using folium library:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import folium\nimport folium.plugins as plugins\n\nlatitude = list(shtng.Lat)[1] # This is to initiate the latitude start point for the map\nlongitude = list(shtng.Long)[1] # This is to initiate the longitude start point for the map\n\nlatitudes = list(shtng.Lat) #create the list of all reported latitudes\nlongitudes = list(shtng.Long) #create the list of all reported longitudes\n\nshooting_map = folium.Map(location = [latitude, longitude], zoom_start = 12) # instantiate a folium.map object\n\nshooting = plugins.MarkerCluster().add_to(shooting_map) # instantiate a mark cluster object for the incidents in the dataframe\n\n# loop through the dataframe and add each data point to the mark cluster\nfor lat, lng, label, in zip(shtng.Lat, shtng.Long, shtng.DISTRICT):\n    if (not np.isnan(lat)) & (not np.isnan(lng)): # also, we check a non-nullness of the coordinates\n        folium.Marker(\n            location=[lat, lng],\n#             icon=None,\n            popup=label,\n            icon=folium.Icon(icon='exclamation-sign')\n        ).add_to(shooting)\n\n# display the map\nshooting_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After a short time, we can see the map with all reported incidents with the shooting. The most frequently mentioned areas are colored in dark orange. <br>\nAccording to the plot, the most  \"dangerous\" boroughs are situated in the south part of the city, mainly along the railroad."},{"metadata":{},"cell_type":"markdown","source":"Now I want to range the districts to highlight those where I wouldn't recommend settling."},{"metadata":{"trusted":true},"cell_type":"code","source":"# re-assemble the dataset for the more convenient plotting process\ntop_sh = shtng.DISTRICT.value_counts().to_frame().reset_index(level=0)\ntop_sh.columns.values[0] = 'DISTRICT'\ntop_sh.columns.values[1] = 'NUMBER'\ntop_sh.plot(kind = 'barh', x = 'DISTRICT', y = 'NUMBER', figsize=(12, 7))\n\n# invert y-axis\nplt.gca().invert_yaxis()\n\n# Name axis and title\nplt.xlabel('Number of Shooting Reports')\nplt.ylabel('Districts')\n\nplt.title('Boston Top Shooting Districts: ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(df.OCCURRED_ON_DATE.dt.date.iloc[-1]))\n\n# Lop for values plotting\nfor index, value in enumerate(top_sh['NUMBER']):\n    label = format(int(value), ',')\n    plt.annotate(label, xy=(value - 1, index + 0.11),\n                 ha='right', \n                 color='white'\n                )\n    \n# Loop for arrows plotting. Notice that the arrowhead will always point on the bottom-right bar's corner.\n# Also, here I separately defined a starting arrows' point to maximize the procedural plotting \nxy_label = (250,5)\nfor index, value in enumerate(top_sh['NUMBER']):\n    plt.annotate('',\n             xy=(value, index + 0.3),\n             xytext=xy_label,\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2',\n                             connectionstyle='arc3', \n                             color='xkcd:blue', \n                             lw=2\n                            )\n            )\n    if index == 2: # We want to plot only top 3 the most shooting districts, so we need to interrupt the loop here.\n        break\n\n# This dictionary I built using googling method.\ndict0 = {'C11' : 'DORCHESTER', 'B3' : 'MATTAPAN', 'B2' : 'ROXBURY'} \n\n# Plot the district name decoding it using our dictionary.\nfor index, value in enumerate(top_sh['NUMBER']):\n    v = top_sh.loc[top_sh['NUMBER']==value]['DISTRICT'].astype('str')\n    plt.annotate('[ ' + dict0[v[index]] + ' ]',\n             xy=(value - 15, index + 0.13),\n             rotation=0,\n             va='bottom',\n             ha='right',\n             color = 'white'\n            )\n    if index == 2:\n        break\n        \n# Plot the annotation text. Here I used xy_label defined earlier for automation.\nplt.annotate('The Most Shooting Districts', # text to display\n             xy=(xy_label[0],xy_label[1] + 0.5),\n             rotation=0,\n             va='bottom',\n             ha='center',\n            )\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The figure above clearly shows that Roxbury, Mattapan and Dorchester are the most restless districts."},{"metadata":{},"cell_type":"markdown","source":"To finalize with folium library, I'd like to show how else we can depict the incidents using a heat map.<br>\nIn this case, all shooting incidents are colored depending on the intensity at any particular area. For the time-saving purpose, I limited the number of points to be pictured."},{"metadata":{"trusted":true},"cell_type":"code","source":"shooting_hmap = folium.Map(location=[df.Lat[100],df.Long[100]], \n                       tiles = \"Stamen Toner\",\n                      zoom_start = 12)\n\nfrom folium.plugins import HeatMap   \n\nhm = df.loc[:,['Lat','Long','SHOOTING']]\nhm.dropna(axis=0, inplace=True)\nhlimit = hm.shape[0]\nhm = hm.sample(hlimit)\nhdata = []\nfor ln, lt in zip(hm.Lat, hm.Long):\n    hdata.append((ln,lt))\nHeatMap(hdata, \n        gradient = {0.01: 'blue', 0.15: 'lime', 0.25: 'red'},\n        blur = 15,\n        radius=5).add_to(shooting_hmap)\n\nshooting_hmap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Task3 - Most Dangerous Time in Boston</h3>\n<br>To continue the previous chapter, let's consider the question of determining the time of the day when It is better to stay at home. In particular, I want to know, when the chance to catch a bullet is peaked."},{"metadata":{},"cell_type":"markdown","source":"Let's prepare the data first: we're interested only in the rows where 'Y' shooting mark is and the 'HOUR' values are filled."},{"metadata":{"trusted":true},"cell_type":"code","source":"shtngh = df[(df.SHOOTING == 'Y') & (df.HOUR.notnull())]\nshtngh.sort_values(['OCCURRED_ON_DATE'], ascending=True, inplace=True)\nshtngh.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset we need seems to be relatively small, so we will plot all the observations.<br>\nThis time I use histogram plot, which shows how frequently each particular hour, when the incident with the shooting was reported, appears in the dataset.<br>\nI am going to use different kinds of arrows to point the main features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the histogram\nplt.figure(figsize=(9, 5))\nplt.hist(shtngh.HOUR, bins=range(24))\nplt.title('Shooting Time Distribution in Boston: ' + str(shtngh.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(shtngh.OCCURRED_ON_DATE.dt.date.iloc[-1]))\nplt.xticks(range(24))\n\n# Decrease Arrow\nplt.annotate('',\n             xy=(10, 25), # Arrow head\n             xytext=(1, 120), # Starting point\n             xycoords='data', # Use the coordinate system of the object being annotated \n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='angle3, angleA=110,angleB=0', color='xkcd:blue', lw=2)\n            ) # Arrow props\n\n# After Midday Arrow\nplt.annotate('',\n             xy=(16, 85),\n             xytext=(15, 100),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:blue', lw=2)\n            )\n\n# Latenight Madness Arrow\nplt.annotate('',\n             xy=(21.5, 190),\n             xytext=(20, 65),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:blue', lw=2)\n            )\n\n# Annotate Text\nplt.annotate('Gradual decrease ',\n             xy=(2.5, 38),\n             rotation=-40,\n             va='bottom',\n             ha='left',\n            )\n\n# Annotate Text\nplt.annotate('After midday peak', # text to display\n             xy=(15, 100),\n             rotation=0,\n             va='bottom',\n             ha='right',\n            )\n\n# Annotate Text\nplt.annotate('Latenight madness',\n             xy=(19.5, 90),\n             rotation=79,\n             va='bottom',\n             ha='left',    \n            )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we can conclude from the histogram is:\n1. Nobody shoots at 7 am (too sleepy, probably)\n2. There is a midday shooting peak (too hot?)\n3. And the late-night madness at midnight (everybody has fun)\n4. Then the fun gradually goes down and fades in the early morning."},{"metadata":{},"cell_type":"markdown","source":"Well, usually the shooting follows by the verbal disputes. I want to check, is the correlation between these two events or not.<br>\nFirst of all, I want to check it graphically, using the old good histogram."},{"metadata":{},"cell_type":"markdown","source":"Let's bultd a dataset we want to examine:"},{"metadata":{"trusted":true},"cell_type":"code","source":"vd = df[(df.OFFENSE_CODE_GROUP == 'Verbal Disputes') & (df.HOUR.notnull())]\nvd.sort_values(['OCCURRED_ON_DATE'], ascending=True, inplace=True)\nvd.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It contains over 13k observations so that our data look statistically significant."},{"metadata":{},"cell_type":"markdown","source":"This is how I plot it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9, 5))\nplt.hist(vd.HOUR, bins=range(24))\nplt.title('Verbal Disputes Rate in Boston:'  + str(vd.OCCURRED_ON_DATE.dt.date.iloc[0]) + ' : ' + str(vd.OCCURRED_ON_DATE.dt.date.iloc[-1]))\nplt.xticks(range(24))\nplt.annotate('',\n             xy=(21.5, 1400),\n             xytext=(5, 200),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='->', connectionstyle='arc, angleA=90, angleB=-95, armA=40, armB=60, rad=45.0', color='xkcd:blue', lw=2)\n            )\nplt.annotate('Verbal disputes gradually increases', # text to display\n             xy=(3, 1200),\n             rotation=0,\n             va='bottom',\n             ha='left',\n            )\nplt.annotate('all day long and reaches its peak', # text to display\n             xy=(3, 1100),\n             rotation=0,\n             va='bottom',\n             ha='left'\n            )\nplt.annotate('at midnight', # text to display\n             xy=(3, 1000),\n             rotation=0,\n             va='bottom',\n             ha='left',\n            )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My theory of that incidents with shooting are preceded by verbal disputes is confirmed graphically.<br>But I need more evidence.<br>\nSuddenly, I've decided to check whether we could build a prediction model that could predict the probability of the shooting using only OFFENSE_CODE_GROUP features.<br>Then we will see which OFFENSE_CODE_GROUP codes are the most related to incidents with shooting."},{"metadata":{},"cell_type":"markdown","source":"Data preparation step:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = df.loc[:,['SHOOTING', 'OFFENSE_CODE_GROUP']]\npred.replace(np.nan, 0, inplace=True)\npred.replace('Y', 1, inplace=True)\ngroups_dummy = pd.get_dummies(pred['OFFENSE_CODE_GROUP'])\npred = pd.concat([pred,groups_dummy], axis=1)\npred.drop(['OFFENSE_CODE_GROUP'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Development"},{"metadata":{},"cell_type":"markdown","source":"I chose the Logistic Regression model since we need to predict the probability of 0 or 1 and it usually suits perfectly for this purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pred[['SHOOTING']]\nX = pred.iloc[:,1:]\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(x_train,y_train)\nLR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, the model is trained. It's time to evaluate it."},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = LR.predict(x_test)\nyhat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation using Jaccard Index\nfrom sklearn.metrics import jaccard_similarity_score\njaccard_similarity_score(y_test, yhat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outstanding result! Seems like our model fits test data perfectly."},{"metadata":{},"cell_type":"markdown","source":"Now let's see the probalility of the classes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat_prob = LR.predict_proba(x_test)\nyhat_prob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, <b>predict_proba</b> returns evaluations for all classes, ordered by the label of the classes. <br>\nSo, the first column is the probability of class 1, P(Y=1|X), and second column is probability of class 0, P(Y=0|X):"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, yhat, labels=[1,0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how confusion matrix looks like. All wee need to do here is to measure Log Loss for this model. The lower value, the better model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss\nlog_loss(y_test, yhat_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great result, let's find the importance for the each feature."},{"metadata":{},"cell_type":"markdown","source":"Hare are some data manipulations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance=pd.concat([pd.DataFrame(X.columns), pd.DataFrame(LR.coef_.T)], axis = 1)\nfeature_importance.columns = ['features', 'importance']\nfeature_importance.sort_values(['importance'], ascending=False, inplace=True)\nfeature_importance = feature_importance.reset_index(drop=True)\nfeature_importance.head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What I see here is that there is no place for Verbal Disputes! However, even these outcomes do not exclude the relationship between shooting and verbal disputes reported. They just could follow one after another since they are both in different observations."},{"metadata":{},"cell_type":"markdown","source":"<h3>Task4 - The tendency in the incedents with shooting</h3>\n<br>Ok, now let's switch to a slightly different question - I want to know how is everything going with the number of shooting incidents - is it increasing or decreasing over the recent years?"},{"metadata":{},"cell_type":"markdown","source":"For this purpose, we need to transform our data, parse the date column and leave only year-month value because I don't want to plot everyday-dots."},{"metadata":{"trusted":true},"cell_type":"code","source":"shtng_gr = shtng.loc[:,['OCCURRED_ON_DATE']]\nshtng_gr['Amount'] = 1\nshtng_gr['Date'] = pd.DatetimeIndex(shtng_gr.OCCURRED_ON_DATE).normalize()\nshtng_gr.drop(['OCCURRED_ON_DATE'], axis=1, inplace=True)\nshtng_gr['YM'] = pd.to_datetime(shtng_gr[\"Date\"], format='%Y00%m').apply(lambda x: x.strftime('%Y-%m'))\nshtng_gr['YM'] = pd.to_datetime(shtng_gr[\"YM\"])\nshtng_gr.drop(['Date'], axis=1, inplace=True)\nshtng_gr = shtng_gr.groupby(['YM'], as_index=False).sum()\nshtng_gr.reset_index(drop=False, inplace=True)\nshtng_gr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I want to plot just a regression line to see the trend briefly"},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.polynomial.polynomial import polyfit\npx = np.asarray(shtng_gr.index)\nb, m = polyfit(shtng_gr.index, shtng_gr.Amount, 1)\nshtng_gr.plot(kind='scatter',x='index', y='Amount', rot='90', figsize=(10, 6), alpha = 1, c='xkcd:salmon')\nplt.plot(px, b + m * px, '-', c='xkcd:blue')\nplt.xticks(shtng_gr.index, shtng_gr.YM.dt.date, rotation=90)\nplt.ylabel('Amount of Shooting Reports / Month')\nplt.xlabel('Months')\nplt.title('Total Shooting Reports in Boston: ' + str(shtng_gr.YM.dt.date.iloc[0]) + ' : ' + str(shtng_gr.YM.dt.date.iloc[-1]))\n\nplt.annotate('Regression Line : Insignificant growth',                      # s: str. Will leave it blank for no text\n             xy=(20, 27),             # place head of the arrow at point (year 2012 , pop 70)\n             xytext=(12, 46),         # place base of the arrow at point (year 2008 , pop 20)\n             xycoords='data',         # will use the coordinate system of the object being annotated \n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:blue', lw=2)\n            )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Even though the number of shooting reports experiences slow growth, it still seems like a lateral trend."},{"metadata":{},"cell_type":"markdown","source":"In comparison to the trend above, I would like to show realy disturbing tendency - the tendency in Medical Assistance reports."},{"metadata":{"trusted":true},"cell_type":"code","source":"ma = df[(df.OFFENSE_CODE_GROUP == 'Medical Assistance') & (df.OCCURRED_ON_DATE.notnull())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ma_gr = ma.loc[:,['OCCURRED_ON_DATE']]\nma_gr['Amount'] = 1\nma_gr['Date'] = pd.DatetimeIndex(ma_gr.OCCURRED_ON_DATE).normalize()\nma_gr.drop(['OCCURRED_ON_DATE'], axis=1, inplace=True)\nma_gr['YM'] = pd.to_datetime(ma_gr[\"Date\"], format='%Y00%m').apply(lambda x: x.strftime('%Y-%m'))\nma_gr['YM'] = pd.to_datetime(ma_gr[\"YM\"])\nma_gr.drop(['Date'], axis=1, inplace=True)\nma_gr = ma_gr.groupby(['YM'], as_index=False).sum()\nma_gr.reset_index(drop=False, inplace=True)\nma_gr.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To diverse the project, I use a seaborn library here to plot a regression line. From my perspective, it makes it more spectacular and informative."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(15, 10))\nax = sns.regplot(x='index', y='Amount', data=ma_gr, color='green', marker='+', scatter_kws={'s': 50,'color':'xkcd:salmon', 'alpha' : 1})\n\nax.set(xlabel='Months', ylabel='Amount of Medical Assistance Reports / Month')\nax.set_title('Total Medical Assistance Reports in Boston: ' + str(ma_gr.YM.dt.date.iloc[0]) + ' : ' + str(ma_gr.YM.dt.date.iloc[-1]))\nax.set_ylim(200)\nplt.xticks(ma_gr.index, ma_gr.YM.dt.date, rotation=90)\n\nplt.annotate('Steady Growth',\n             xy=(25, 600),\n             xytext=(12, 350),\n             xycoords='data',\n             arrowprops=dict(arrowstyle='fancy ,head_length=0.4,head_width=0.4,tail_width=0.2', connectionstyle='arc3', color='xkcd:salmon', lw=2)\n            )\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What I see here is a clear uptrend. There is more than a 50% increase in 'the Medical Assistance reports' in just 3 years.\n<br>However, to investigate the reasons for this we need other datasets."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}