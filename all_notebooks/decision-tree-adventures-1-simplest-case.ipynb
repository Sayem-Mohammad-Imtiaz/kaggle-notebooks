{"cells":[{"metadata":{"_uuid":"0942344302f7b570101958eccd983f21b4663e83"},"cell_type":"markdown","source":"## Decision Tree Adventures 1 - Simplest Case"},{"metadata":{"_uuid":"ce525da52f7cbf042affa739ecf8547d51b0a7c9"},"cell_type":"markdown","source":"### Abstract"},{"metadata":{"_uuid":"bb93b0035857bc7c11eb8133256f89ce7cb9e830"},"cell_type":"markdown","source":"It is the first notebook of a series about decision trees. The followings are included in this first one:\n\n- Basic visualization of data\n\n- One Hot Encoding\n\n- Modelling a decision tree with default parameters (without any tuning)\n\n- Creating a visual tree to present the decision tree\n\n- Model results\n\n- Small hints (e.g. why drop_first should be applied after one hot encoding?, why one hot encoding is required for a decision tree?)\n\nReaching a higher accuracy by tuning the model is not in the scope of this notebook. These studies will be handled in the next notebooks and following topics will be explained in detail in these notebooks:\n\n- Mechanism of decision tree models\n\n- Parameter explanation and tuning of a decision tree\n\n- Pruning details for a decision tree\n\n- Applying Random Forest for the same data\n\n- Applying XGBoost for the same data\n\n- Applying Light GBM for the same data\n\n- Comparison of decision tree models applied\n"},{"metadata":{"_uuid":"920427812ac734e3bdc34e1cd100fafd33f8c39d"},"cell_type":"markdown","source":"### Explanation of the Study"},{"metadata":{"_uuid":"ba54465d30145d122a1b352337371db8bf94bb2f"},"cell_type":"markdown","source":"A classification decision tree is modelled to predict the success in math of a student depending on the features (gender, race/ethnicity, parental level of education, lunch, test preparation course). \n\nModel is established by using just default parameters that is why the tree is very big. Accuracy rate is 56% for the test set and %80 percent for the training set. This big difference between accuracies of train and test sets imply an overfitting case."},{"metadata":{"_uuid":"373daab99cea4e12a22683f91acc5be04817c11c"},"cell_type":"markdown","source":"### Importing Libraries"},{"metadata":{"trusted":true,"_uuid":"753e846c40c116d4a9a90de13dee0954a3407ff3"},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn import tree\nimport graphviz \nimport numpy as np\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dbd35a146bb91f335df376d41eb079c2fd4caf2"},"cell_type":"markdown","source":"### Loading and Displaying the Dataset"},{"metadata":{"trusted":false,"_uuid":"e9c4cbe36271d13504d5dc6b66fb87f4847a6592"},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/StudentsPerformance.csv\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69fe2376fc9d4d316d183dd3037cb7f03362e76e"},"cell_type":"markdown","source":"### Visualizations"},{"metadata":{"trusted":false,"_uuid":"f42f3c9f6d34fca9cbb15b8a67194e076fd1c048"},"cell_type":"code","source":"ax = sns.countplot(x=\"gender\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d76a884cf640d45d5b1bc6226cb4227f9faa9193"},"cell_type":"code","source":"ax = sns.countplot(x=\"race/ethnicity\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"35b5f6211ee0552680e35c47c5183009e02f3914"},"cell_type":"code","source":"ax = sns.countplot(y=\"parental level of education\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e5a44fa8732108e9c02522c4d394d6c888af196c"},"cell_type":"code","source":"ax = sns.countplot(x=\"lunch\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"15d66a9a2591f9cb7d84e732d5fa3d2120c95f5a"},"cell_type":"code","source":"ax = sns.countplot(x=\"test preparation course\", data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a77a980f3e9bd9d38e969ea3d79032cb10f249e9"},"cell_type":"code","source":"ax = sns.distplot(dataset[\"math score\"]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f98fc9f87e3220e5d19b38425fab94badf64adcd"},"cell_type":"markdown","source":"### Data Processing"},{"metadata":{"_uuid":"32b3398db7ae46084c9d4535b0e238116efdc724"},"cell_type":"markdown","source":"Lets create a new feature, named as math grade. Math grade is Pass if math score is above 60, math grade Fail if math score is below 60. It is just an assumption. After creating the new column, I have removed the unneccasry columns and check the latest form of dataset again.\n\n#### *Hint: it is a good example for changing some rows of a column according to some conditional expressions on values of other columns. "},{"metadata":{"trusted":false,"_uuid":"3b669560a9f66eed386a7537ab37c73af3812aca"},"cell_type":"code","source":"dataset[\"math grade\"] = \"\"\ndataset.loc[(dataset[\"math score\"] >= 60), \"math grade\"] = \"Pass\"\ndataset.loc[(dataset[\"math score\"] < 60), \"math grade\"] = \"Fail\"\ndataset.drop(columns=['math score', 'reading score', 'writing score'], inplace=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfec731cec0183192bffaa09a83ceae2f3b0e618"},"cell_type":"markdown","source":"### One Hot Encoding"},{"metadata":{"_uuid":"ea700316ce41b30e6c825119b73877f1995f555c"},"cell_type":"markdown","source":"#### *Hint: One hot encoding is necessary if you use scikit-learn library for modelling. \n\nScikit-learn uses only numerical features and these numerical features are considered as continuos numeric values. In our case race feature includes group A, group B, group C, group D, group E and it will be encoded as 1,2,3,4,5 under one column if you use just label encoding not one hot encoding. Till now everything is fine but problem starts here. Since model assumes that all numerical values are continuous, you will see such internal nodes (splitting point): \"race < 4 which means group A race < group B race\" which is very weird. Weird because there is no continouity among race values. It would be the same if our feature was color. It may be acceptable for parental level of education if you set the order correctly because there is a continuity. Of course master's degree is higher than bachelor's degree and it is higher than some college etc. That is why we have to apply one hot encoding if we use scikit-learn. \n\n#### *Hint: Dropping one of the one hot encoded columns are a good approach otherwise weigths and effects of features over the model may be higher or lower. \n\nAssume that in our dataset we have only age and gender features. Since age is continous numeric variable that is good for scikit-learn but we have to apply one hot encoding for gender. After applying it we have 3 features, age, gender_male, gender_female. Lets give an example, lets say our person is male so as values gender_male is 1 and gender_female is 0 and lets say age is 20. That sounds ok but not actually. Lets say you are talking with the model and you check age feature and tell \"this person is 20 years old\". And then you check gender_male feature and tell \"this person is male\". And then you check gender_female column and tell \"this person is not a female (which means person is a male)\". Just to tell the model once about the person's gender, you have to drop one of the one hot encoded columns and drop_first=True makes it for us.\n"},{"metadata":{"trusted":false,"_uuid":"208d581b6f05f70db7f63170f377525aba7066de"},"cell_type":"code","source":"one_hot = pd.get_dummies(dataset['gender'], prefix='gender', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['race/ethnicity'], prefix='race/ethnicity', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['parental level of education'], prefix='parental level of education', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['lunch'], prefix='lunch', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['test preparation course'], prefix='test preparation course', drop_first=True)\ndataset = dataset.join(one_hot)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"229e8aa62ae3cc0c52d8fd3fe041a800f4506ca9"},"cell_type":"markdown","source":"### Train - Test Split"},{"metadata":{"_uuid":"989c48276fc97229697f143945211f9bd4b6607a"},"cell_type":"markdown","source":"Splitting the dataset into two datasets as train, test and blind hold-out datasets. The ratio is 70/20/10. \n\n#### *Hint: Seperating and keeping blind hold out dataset is an important concept. Since we try to tune the model by using train and test datasets, it is possible that at a certain point model can overfit to test dataset even. That is why keeping a blind hold out dataset and use it very very often, only when we think that our model is tuned and works very well.\n\n#### *Hint: random_state parameter is used if you want to have exactly the same train and test datasets and if you don't want them to be different in every splitting when you run your notebook from the beginning. It doesn't matter which number, you can set 5, 77, 100 whatever. The only important point is to set the same number. For me it is 21 in this case."},{"metadata":{"trusted":false,"_uuid":"d80a3ef8af5b98df3bf28a4c3e7dc209a38a8b8d"},"cell_type":"code","source":"data_train, data_test_hold = train_test_split(dataset, test_size=0.30, random_state=21)\ndata_test, data_hold = train_test_split(data_test_hold, test_size=0.33, random_state=21)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c55738bd8c38a59e9425bae0c731fe65db07bea2"},"cell_type":"markdown","source":"### Create Numerical Arrays for the Model"},{"metadata":{"trusted":false,"_uuid":"417c31fd1758823cecf76167b12c311a165fea8e"},"cell_type":"code","source":"columns_move = [\"gender\", \"race/ethnicity\", \"parental level of education\", \"lunch\", \"test preparation course\", \"gender_male\", \"race/ethnicity_group B\", \"race/ethnicity_group C\", \"race/ethnicity_group D\", \"race/ethnicity_group E\", \"parental level of education_bachelor's degree\", \"parental level of education_high school\", \"parental level of education_master's degree\", \"parental level of education_some college\", \"parental level of education_some high school\", \"lunch_standard\", \"test preparation course_none\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8fb708125afeda0cd56fdb267e08f01bc4de6cd3"},"cell_type":"code","source":"y_train = data_train[\"math grade\"].values\nX_train = data_train[columns_move].values\ny_test = data_test[\"math grade\"].values\nX_test = data_test[columns_move].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20d6a45bb5a9d16b19ab17bb15a9900ed3109499"},"cell_type":"markdown","source":"### Create the Model"},{"metadata":{"_uuid":"d8351f4f4e2a98ca6930aaae2aa29a52e1d3191e"},"cell_type":"markdown","source":"There is no tuning for the model. It is created with the default values of the DecisionTreeClassifier. That is why it will be very very big overfitting decision tree :)."},{"metadata":{"trusted":false,"_uuid":"ee3f82523e54d07f8d4978dcd5eb6e3858a0b1bc"},"cell_type":"code","source":"model = DecisionTreeClassifier(criterion='gini', splitter='best', \n                               max_depth=None, min_samples_split=2, \n                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                               max_features=None, random_state=None, \n                               max_leaf_nodes=None, min_impurity_decrease=0.0, \n                               min_impurity_split=None, class_weight=None, \n                               presort=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b78375473e14e7c621be026eeeaddefe18ddcb67"},"cell_type":"markdown","source":"### Training the Model"},{"metadata":{"trusted":false,"_uuid":"964a6c2d5d983c9fafb68f23991c54620fbfe213"},"cell_type":"code","source":"model.fit(X_train[:,5:], y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"652818485b2553a667fa28dc6c8aaa01e55dc5c3"},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":false,"_uuid":"d6c0dca0bd2f48ab761837766dcfe90546342b64"},"cell_type":"code","source":"y_pred = model.predict(X_test[:,5:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2880e252b55316e2770378e35652707c66b2d78b"},"cell_type":"code","source":"print(\"Model Accuracy: %.2f\" % (accuracy_score(y_test,y_pred)*100), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"16607a7ca100a53a97835c6048cd9fa3cfda83a6"},"cell_type":"code","source":"a = pd.DataFrame(confusion_matrix(y_test,y_pred), columns=['prediction/fail', 'prediction/pass'], index=['actual/fail', 'actual/pass'])\nprint(\"Confusion Matrix:\")\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"382ce7c2e9a1801763b3fcdf1ace77b7166bbeb3"},"cell_type":"code","source":"print(\"Classification Report:\")\nprint(\"\")\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d0a3895ec747606c8804276256631878d9b15cc"},"cell_type":"markdown","source":"Model accuracy is 56%. As all we know, it is never enough to check only accuracy so lets see how we can get a confusion matrix, precision, recall and f1 scores by using scikit-learn library. \n\nThere are so many resources explain these concepts. I can advice the article \"Accuracy, Precision, Recall or F1?\" by Koo Ping Shung. Here is the link: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n\nLets make predictions by using the train dataset and compare it with the results from test set. The purpose is to check whether there is an overfitting or not. As I said at the beginning, 56% and 80% are quite different from each other, 56% is almost a flip-coin, on the other hand 80% is a promising accuracy rate. "},{"metadata":{"trusted":false,"_uuid":"a4c8a4547b032e2c4119c48ec30e2618af429c1c"},"cell_type":"code","source":"y_pred_train = model.predict(X_train[:,5:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"41c226d8bc52273e6779ee496f575d6aaa4201a5"},"cell_type":"code","source":"print(\"Model Accuracy: %.2f\" % (accuracy_score(y_train,y_pred_train)*100), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c2cd49cb989fc90ac32661a4dd406aea82061fe"},"cell_type":"markdown","source":"### Graph of Decision Tree"},{"metadata":{"trusted":false,"_uuid":"3428f0422b7e4ab26d3c388c6a7423fb6208f893"},"cell_type":"code","source":"dot_data = StringIO()\nexport_graphviz(model, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0fdf28dbcb6969092380a627503b3fc448eaadb"},"cell_type":"markdown","source":"### Combined Result Data Frame with Predictions"},{"metadata":{"trusted":false,"_uuid":"cb6e7779f08e2f93ffeb6b364007518313169638"},"cell_type":"code","source":"columns_move.append(\"math grade test\")\ncolumns_move.append(\"math grade pred\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"063727af22640e7745f530647a7d804614d35cde"},"cell_type":"code","source":"y_pred = y_pred.reshape(len(y_pred),1)\ny_test = y_test.reshape(len(y_test),1)\nresultarray = np.append(X_test, y_test, axis=1)\nresultarray = np.append(resultarray, y_pred, axis=1)\nresultdf = pd.DataFrame(resultarray, columns=columns_move)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7d4f95cdfa7fec8079b7a97374bbfe52ec0db73b"},"cell_type":"code","source":"resultdf.drop(columns=[\"gender_male\", \"race/ethnicity_group B\", \"race/ethnicity_group C\", \"race/ethnicity_group D\", \"race/ethnicity_group E\", \"parental level of education_bachelor's degree\", \"parental level of education_high school\", \"parental level of education_master's degree\", \"parental level of education_some college\", \"parental level of education_some high school\", \"lunch_standard\", \"test preparation course_none\"], inplace=True)\nresultdf.head(200)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}