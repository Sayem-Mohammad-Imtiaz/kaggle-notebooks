{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression - Statistical and Machine Learning Model.\n\nObjective of buidling the model is to estimate the prediction of affected by the CHD- Heart Disease.\n\n## Steps of this notebook\n\nThis notebook has the following useful features\n\n* Checking outliers,distributions\n* Data Cleaning\n* Feature Selection- Backward Elimination\n* Statistical Model with classification report,ROC analysis\n* Machine Learning LogisticRegression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the neccasry Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Loading\ndf=pd.read_csv(\"/kaggle/input/heart-disease-prediction-using-logistic-regression/framingham.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of the target Varaible"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.TenYearCHD.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.TenYearCHD.value_counts(normalize=True).plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking for Null Values and imputing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forward and Backward fill is used to fill the null values so the distribution is not affected\ndf.fillna(method='ffill',inplace=True)\ndf.fillna(method='bfill',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for outliers in the data set\n\ncols=[\"age\",\"cigsPerDay\",\"totChol\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\"]\n\nfor col in cols:\n    sns.boxplot(df[col])\n    #df[col].plot(kind='box')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(df[\"BMI\"],df[\"glucose\"],hue=df[\"TenYearCHD\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performing Statistical Logit Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\ny=df.TenYearCHD\nX=df.drop('TenYearCHD',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xc=sm.add_constant(X)\nmodel=sm.Logit(y,Xc)\nresult=model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(Xc.values, i) for i in range(Xc.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## VIF tabel shows there is few serious multicollonearity\n## backward elimination to drop varables one by one\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# VIF tabel shows tht there is no serious multicollonearity\n# backward elimination to drop varables one by one\n\ncols=list(Xc.columns)\np=[]\nwhile len(cols)>2:\n    Xc=Xc[cols]\n    model=sm.Logit(y,Xc).fit().pvalues\n    p=pd.Series(model.values[1:],index=Xc.columns[1:])\n    pmax= max(p)\n    pid=p.idxmax()\n    \n    if pmax>0.05:\n        cols.remove(pid)\n        print('column removed :',pid,pmax)\n    else:\n        break\n        \ncols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=sm.Logit(y,Xc[cols])\nresult=model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the coeffecients of the features\nexp_cof=np.exp(result.params)\nexp_cof","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Age\n## 1.Positive sign of age co efficient indicate that, probability of CHD increases with age\n## 2.When age increase by 1 yr, log(odds) of CHD increase by 0.0646\n## 3.When age increase by 1 yr, odds of CHD increase by 6%(So 1.066-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Male\n# 1.Positive sign of male co efficient indicate that, probability of CHD in male is high.\n# 2.log(odds) of CHD for male is higher by 0.49 compared to female\n# 3.odds(male)/odds(female)=1.63, odds(male) is 63% higher compared to odds(female)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Assiging the threshold to determine the prediction from probability","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob=result.predict(Xc[cols])\nprob.name='prob'\ndf_pred=pd.DataFrame([prob,y]).T\ndf_pred['pred']=df_pred['prob'].apply(lambda x:0 if x<0.5 else 1)\ndf_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(df_pred.TenYearCHD,df_pred.pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(df_pred['TenYearCHD'],df_pred['pred'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(df_pred['TenYearCHD'],df_pred['pred']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## roc analisys"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score , roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('AUC for model:',roc_auc_score(df_pred['TenYearCHD'],df_pred['prob']))\nprint('ROC for model:',roc_curve(df_pred['TenYearCHD'],df_pred['prob']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr,tpr,threshold=roc_curve(df_pred['TenYearCHD'],df_pred['prob'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.xlabel('fpr')\nplt.ylabel('tpr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold[0]=threshold[0]-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.plot(fpr,threshold,'g-')\nplt.xlabel('fpr')\nplt.ylabel('tpr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model is not performing its best in estimation, there is a large trade of bias and variance in the  model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare x,y and split using X_train and y_train\nX= df.drop(['TenYearCHD'],axis='columns')\ny= df.TenYearCHD\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=1)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the logisticRegression to validate x_test and y_test\n# Logistic Regression is maximum likehood model-- iteration till the maximum\n# Using solver we can stimulate and converge at a faster rate\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(fit_intercept=True,solver='liblinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train, y_train)  \n# In stats we provide y,x while machine learning we provide X,y\n# Check for any warning if its there again you need to do that","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determine the prediction and Probability\ny_train_prob = lr.predict_proba(X_train)[:,1]\ny_train_pred = lr.predict(X_train)\ny_train_prob  # output- Probability for 0 and 1 i.e ( P,(1-P)) # thats the reason we slice and take the value for 1 alone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating on the training data\n# Determine the confusion matrix ,accuracy_score,roc_curve,roc_auc_score,classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,roc_auc_score,classification_report\nprint(\"confusion_matrix \\n\",confusion_matrix(y_train,y_train_pred))         # Evaluating the data on the trained data set-->Train and Predicted\nprint(\"accuracy_score\",accuracy_score(y_train,y_train_pred))           # Train and Predicted\nprint(\"roc_accuracy acore\",roc_auc_score(y_train,y_train_prob))                 # Train and Probability\nprint(\"classification_report \\n \",classification_report(y_train,y_train_pred))   # Train and Predicted\nfpr, tpr, thresholds =roc_curve(y_train,y_train_prob)  # Train and Probability-- Plotting\n\nplt.plot(fpr, tpr)\nplt.plot(fpr, fpr, \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validating the data on the test data set\ny_test_prob = lr.predict_proba(X_test)[:,1]\ny_test_pred = lr.predict(X_test)\nprint(\"confusion_matrix \\n\",confusion_matrix(y_test,y_test_pred))         # Validating the data on the trained data set-->Train and Predicted\nprint(\"accuracy_score\",accuracy_score(y_test,y_test_pred))           # Test and Predicted\nprint(\"roc_accuracy acore\",roc_auc_score(y_test,y_test_prob))                 # Test and Probability\nprint(\"classification_report \\n \",classification_report(y_train,y_train_pred))   # Test and Predicted\nfpr,tpr,thresholds = roc_curve(y_test,y_test_prob)  # Test and Probability-- Plotting\nplt.plot(fpr, tpr)\nplt.plot(fpr, fpr, \"r-\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model is Performing good in both Test and Training data Set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IF we want to change the thresold to estimate the predicts( threshold= 0.25)\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_features=20, n_samples=1000, random_state=10\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf = LogisticRegression(class_weight=\"balanced\")\nclf.fit(X_train, y_train)\nTHRESHOLD = 0.25\npreds = np.where(clf.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)\n\npd.DataFrame(data=[accuracy_score(y_test, preds), recall_score(y_test, preds),\n                   precision_score(y_test, preds), roc_auc_score(y_test, preds)], \n             index=[\"accuracy\", \"recall\", \"precision\", \"roc_auc_score\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Statistical Model, Machine Learning Model both performs with a normal accuracy of 85% \n## If the threshold value is been reduced to 25% rather than 50% the accuarcy value increases with the sacrifice of precision","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}