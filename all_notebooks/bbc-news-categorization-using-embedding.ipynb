{"cells":[{"metadata":{},"cell_type":"markdown","source":"## BBC News Categorization using Keras\n\nIn this notebook, I am going to train a model to categorize BBC news using embedding technique.\n\nThe dataset contains 5 news categories namely: business, entertainment, politics, sport, and tech. \n\nThere are 2225 rows and 4 columns. However, only 2 columns (category and content) are used.\n \n![](https://images.spot.im/v1/production/jyzxethffjsr6xwwz0ky)\n\n*Image Source: Daily Express*","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import csv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the csv file\n\ndf = pd.read_csv(\"../input/bbcnewsarchive/bbc-news-data.csv\", sep=\"\\t\")\n\nprint(df)\n\ndf[\"category\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffle the dataframe to evenly distribute the labels\n\ndf = df.sample(frac=1).reset_index(drop=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"content = []\nlabels = []\n\nfor label in df.category:\n    labels.append(label)\n    \nfor con in df.content:\n    for word in stopwords:\n        token = \" \" + word + \" \"\n        con = con.replace(token, \" \")\n        con = con.replace(\" \", \" \")\n    content.append(con)\n\nprint(len(content))\nprint(len(labels))\nprint(\"\\nContent:\", content[0])\nprint(\"\\nLabel:\", labels[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the dataset into training set and test set\n\ntrain_content, test_content = content[:1900], content[1900:]\ntrain_labels, test_labels = labels[:1900], labels[1900:]\n\ntrain_content = np.array(train_content)\ntest_content = np.array(test_content)\n\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\n\nprint(len(train_content))\nprint(len(train_labels))\nprint(len(test_content))\nprint(len(test_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the distribution of labels in the training set and test set\n\nunique_train_content, number_train_content = np.unique(train_labels, return_counts=True)\n\nprint(\"Training set labels:\")\nprint(unique_train_content)\nprint(number_train_content)\n\nunique_test_content, number_test_content = np.unique(test_labels, return_counts=True)\n\nprint(\"\\nTest set labels:\")\nprint(unique_test_content)\nprint(number_test_content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenize the content\n\nvocab_size = 10000\nembedding_dim = 32\nmax_len = 200\ntrunc_type = \"post\"\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_content)\n\nword_index = tokenizer.word_index\n\nsequences = tokenizer.texts_to_sequences(train_content)\npadded = pad_sequences(sequences, maxlen=max_len, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_content)\ntest_padded = pad_sequences(test_sequences, maxlen=max_len, truncating=trunc_type)\n\nprint(test_padded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tokenize the labels\n\nlabel_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)\n\nlabel_index = label_tokenizer.word_index\n\nlabel_sequences = np.array(label_tokenizer.texts_to_sequences(train_labels))\n\ntest_label_sequences = np.array(label_tokenizer.texts_to_sequences(test_labels))\n\nprint(label_sequences.shape)\nprint(test_label_sequences.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define an NN model\n\nmodel = keras.Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n                        \n                         layers.GlobalAveragePooling1D(), #simpler and faster than Flatten()\n                         layers.Dense(128, activation=\"relu\"),\n                         layers.Dense(6, activation=\"softmax\")])\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\nmodel.summary()\nkeras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model\n\nnum_epochs = 10\n\nhistory = model.fit(padded,\n                   label_sequences,\n                   epochs=num_epochs,\n                   validation_data = (test_padded, test_label_sequences))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot accuracy and loss\n\nacc = history.history[\"accuracy\"]\nval_acc = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\n# accuracy\n\nplt.plot(epochs, acc, \"b\", label=\"Training accuracy\")\nplt.plot(epochs, val_acc, \"b--\", label=\"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.show()\n\n# loss\n\nplt.plot(epochs, loss, \"r\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"r--\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Result \n\n> **After training for 10 epochs, the model was able to reach 100% accuracy on training set and near 100% on the validation set.**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}