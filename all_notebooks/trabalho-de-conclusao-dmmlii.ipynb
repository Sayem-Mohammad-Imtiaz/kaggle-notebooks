{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <h2>IESB - Instituto de Educação Superior de Brasília</h2> \n## <h3>Pós Graduação em Ciência de Dados</h3>\n## <h3>Data Mining e Machine Learning II</h3>\n## <h3>Professor: Marcos Vinícios Guimarães</h3>\n## <h3>Aluno: Cláudio Costa Cardoso – 1931133029 </h3>\n## <h3>Abril de 2020</h3>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>ANÁLISE DOS PRINCIPAIS COMPONENTES NA TOMADA DE DECISÕES PARA APROVAÇÃO DAS LINHAS DE CRÉDITO: UMA ABORDAGEM COM MODELOS RANDOM FOREST, RANDOM FOREST COM GRIDSEARCHCV, GBM E XGBOOST</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h2>SUMARIO:</h2>\n\n[I - INTRODUÇÃO](#I - INTRODUÇÃO)\n\n[II - FUNDAMENTAÇÃO TEÓRICA](#II - FUNDAMENTAÇÃO TEÓRICA)\n\n[III - METODOLOGIA](#III - METODOLOGIA)\n   \n   A - Dicionário de Dados\n   \n   B - Carregamento dos Dados\n   \n   C - Análise Exploratória\n   \n   D - Tratamento de Dados\n   \n[IV - ANALISE E DISCUSSÃO DOS RESULTADOS](#IV - ANALISE E DISCUSSÃO DOS RESULTADOS)\n  \n   A - Separar o Universo de Dados em Treino e Teste\n   \n   B - Instanciando o Modelo Random Forest e usando Crosss Validation\n   \n   C - Instanciando o Modelo Random Forest e usando GridSearchCV\n   \n   D - Instanciando o Modelo GBM e usando Crosss Validation\n   \n   E - Instanciando o Modelo XGBoost e usando Crosss Validation\n   \n   F - Analise dos Resultados dos Modelos Random Forest,GBM e XGBoost\n   \n   [V - CONCLUSÃO](#V - CONCLUSÃO)\n   "},{"metadata":{},"cell_type":"markdown","source":"# <h2>I - INTRODUÇÃO</h2> <a name=\"I - INTRODUÇÃO\"></a>\n\nO trabalho busca analisar aa principais componentes da linha de credito da base de dados HMEQ_Date (Home Equity), os dados são coletados de solicitantes que foram concedidos crédito através do processo atual de subscrição de empréstimos, a base é composta de 5960 registros e 13 variáveis. \n\nO objetivo do notebook é verificar dentre um conjunto de variáveis aquelas que mais possam predizer o processo de tomada de decisão para aprovação das linhas de crédito baseando na capacidade do cliente  honrar com o compromisso até o final ou não. A hipótese central é que as variáveis independentes utilizadas possam orientar na tomada de decissão. Para tal fim, utilizamos análise exploratória e os modelos de Random Forest, Random Forest com GridSearchCV, GBM e XGBoost. "},{"metadata":{},"cell_type":"markdown","source":"# <h2>II - FUNDAMENTAÇÃO TEÓRICA</h2> <a name=\"II - FUNDAMENTAÇÃO TEÓRICA\"></a>\n\n* <h3>O Texto que Documenta o Notebook Vem de Diferentes Fontes: </h3>\n\n  1.  Pandas – Manipulação e análise de dados\n  2. Matplotlib – Visualização gráfica\n  3. Seaborn - Visualização gráfica\n  4. Scikit-learn - Aprendizado de máquina\n    \n\n* <h3>MÉTODOS: Random Forest x Random Forest com GridSearchCV x GBM x XGBoost </h3>\n\nOs três métodos usam árvore de decisão como base e são chamados de métodos ensemble, que se caracterizam como métodos que procuram usar modelos mais simples/fracos de forma conjunta para melhorar o desempenho.\n\n 1. Random Forest:   O Random Forest é um excelente algoritmo para ser utilizado nos primeiros estágios de criação do processo de desenvolvimento de um modelo, para se ter uma ideia de performance. Devido à sua simplicidade, é difícil construir um Random Forest “ruim”. Este algoritmo é também uma boa opção se você precisa desenvolver um modelo em curto espaço de tempo. Além disso, ele provê um bom indicador de importância para as características.\n  \n  Para obter melhor acurárica, o algoritimo de RF vai criar diversas árvores de decisão (parâmetro n_estimators) e chegar ao resultado final com base no resultado de cada árvore criada. A idéia básica é separar o conjunto de dados diversas vezes e para cada sub-conjunto treinar um novo regressor/classificador. Os diferentes regressores/classificadores irão produzir resultados diferentes, e o resultado final será determinado com base nessas regressões/classificações.\n\n 2. Gradient Boosting: GBM é um método de boosting, construído em cima de regressores/classificadores fracos. A idéia é adicionar um regressor/classificador de cada vez, então o próximo regressor/classificador é treinado para melhorar o resultado atingido até o momento ('soma de resultados'). Ao contrário do RF, que treina cada regressor/classificador de forma independente, no GBM eles são treinados em conjunto, um ligado ao outro.\n\n 3. XGBoost: XGB é uma implementação específica do GBM, dita melhor e mais rápida que a implementação padrão do scikit-learn. Tanto o GBM quanto o XGB precisam de maior trabalho de interpretação dos dados e tunning do modelo.\n \n 4. Modelo Random Forest com GridSearchCV: O GridSearchCV é um módulo do Scikit Learn que é amplamente usado para automatizar grande parte do processo de tuning. O objetivo primário do GridSearchCV é a criação de combinações de parâmetros para posteriormente avaliá-las.\n\n\n\n* <h3> Hiperparâmetros Importantes</h3>\n\nOs parâmetros no Floresta Aleatória são utilizados ou para aumentar o poder preditivo do modelo ou para tornar o modelo mais rápido. Abaixo os hiperparâmetros da função random forest do pacote sklearn.\n\n  1. **Aumentar o poder preditivo**\n\nPrimeiramente, há o hiperparâmetro n_estimators, que indica o número de árvores construídas pelo algoritmo antes de tomar uma votação ou fazer uma média de predições. Em geral, uma quantidade elevada de árvores aumenta a performance e torna as predições mais estáveis, mas também torna a computação mais lenta.\n\nOutor hiperparâmetro importante é max_features, que indica o número máximo de características a serem utilizadas pelo Floresta Aleatória na construção de uma dada árvore. Sklearn oferece várias opções, descritas na sua documentação.\n\nUm ultimo hiperparâmetro que discutiremos é o min_sample_leaf. Este parâmetro indica o número mínimo de folhas que devem existir em uma dada árvore.\n\n  2. **Aumentar a velocidade do modelo**\n\nO hiperparâmetro n_jobs informa quantos processadores o algoritmo pode utilizar. Se ele tiver valor 1, pode utilizar apenas um processador. O valor -1 significa que não há limite na quantidade de processadores a ser utilizada.\n\nO parâmetro random_state torna o resultado do modelo replicável. O modelo será produzido do mesmo modo se ele tiver um valor definido de random_state e se forem utilizados os mesmos parâmetros com o mesmos dados de treinamento.\n\ncriterion: É a métrica utilizada para construção da árvore de decisão. Pode ser gini ou entropy.\n\nmax_depth: É a profundida máxima da árvore, profundida demais pode gerar um sistema super especializado nos dados de treinamento, também conhecido como overfitting. Profundida de menos vai diminuir a capacidade de generalização do modelo.\n\nPor último, há o oob_score(também chamado de oob sampling), que é um método de validação cruzada para floresta aleatória. Neste tipo de amostragem (sampling), cerca de um terço dos dados não é utilizado no treinamento e pode ser utilizado para avaliar a performance. Estas amostras são chamadas out of the bag samples. É uma técnica similar ao método de validação cruzada leave one out, mas sem nenhum custo computacional extra.\n\n\n* <h3>Métricas de Avaliação de Modelos de Classificação/Predição</h3>\n\n\n1. **ACURÁCIA**\n\nA proporção de predições corretas, sem levar em consideração o que é positivo e o que é negativo. Esta medida é altamente suscetivel a desbalanceamentos do conjunto de dados e pode facilmente induzir a uma conclusão errada sobre o desempenho do sistema.\n\nACURACIA = TOTAL DE ACERTOS / TOTAL DE DADOS NO CONJUNTO\n\nACURACIA = (VP + VN) / (P + N)\n\n2. **SENSIBILIDADE**\n\nA proporção de verdadeiros positivos: a capacidade do sistema em predizer corretamente a condição para casos que realmente a têm.\n\nSENSIBILIDADE = ACERTOS POSITIVOS / TOTAL DE POSITIVOS\n\nSENSIBILIDADE = VP / (VP + FN)\n\n3. **ESPECIFICIDADE**\n\nA proporção de verdadeiros negativos: a capacidade do sistema em predizer corretamente a ausência da condição para casos que realmente não a têm.\n\nESPECIFICIDADE = ACERTOS NEGATIVOS / TOTAL DE NEGATIVOS\n\nESPECIFICIDADE = VN / (VN + FP)\n\n4. **EFICIÊNCIA**\n\nA média aritmética da Sensibilidade e Especificidade. Na prática, a sensibilidade e a especificidade variam em direções opostas. Isto é, geralmente, quando um método é muito sensível a positivos, tende a gerar muitos falso-positivos, e vice-versa. Assim, um método de decisão perfeito (100 % de sensibilidade e 100% especificidade) raramente é alcançado, e um balanço entre ambos deve ser atingido.\n\nEFICIENCIA = (SENS + ESPEC) / 2"},{"metadata":{},"cell_type":"markdown","source":"\n# <h2>III - METODOLOGIA</h2> <a name=\"III - METODOLOGIA\"></a>\n\n<h2>A - Dicionário de Dados</h2>\n\nPara analisar as causas do rísco de credito foi realizado uma análie exploratória e simulações com modelos Random Forest, GBM e XGBoost. Os dados contém informações de linha de base e de desempenho de empréstimos para 5.960 empréstimos recentes sobre o patrimônio da casa. Um empréstimo de capital próprio é um empréstimo em que o devedor usa o patrimônio de sua casa como garantia subjacente. Todas as análises efetuadas estão condicionadas a não violação de nenhum dos pressupostos dos modelos Random Forest, GBM e XGBoost.\n\nO alvo (BAD) é uma variável binária que indica se um requerente acabou por falhar ou se foi gravemente delinqüente. Esse desfecho adverso ocorreu em 1.189 casos (20%).\n\n<h4>Nota:</h2>\nHipoteca Existente (MORTDUE): No empréstimo de compra e venda de imóveis nos EUA é garantido por uma hipoteca (ou um ato de confiança). Geralmente, há um empréstimo pendente na casa com a compra ou refinanciamento, que é considerado \"sênior\" na prioridade de pagamento. O patrimônio líquido é a diferença entre o atual valor estimado de mercado da casa e o saldo devedor da hipoteca sênior. Por exemplo, uma propriedade avaliada em US $ 400.000 com um saldo de US $ 300.000 tem um patrimônio líquido de US $ 100.000. (Confie em mim, eu era um advogado de títulos lastreados em hipotecas.)\n\n\n**<h4>Variáveis Utilizadas no modelo:</h4>**\n\nMAU (BAD) 1 = cliente inadimplente no empréstimo 0 = empréstimo reembolsado\n\nEMPRÉSTIMO(LOAN) - Montante do pedido de empréstimo\n\nMORTDUE - Valor devido da hipoteca existente\n\nVALOR (VALUE) - Valor da propriedade atual\n\nRAZÃO (REASON) - DebtCon = consolidação da dívida HomeImp = melhoria da casa\n\nTRABALHO (JOB) - Seis categorias ocupacionais\n\nYOJ - Anos no emprego atual\n\nDEROG - Número de principais relatórios depreciativos\n\nDELINQ - Número de linhas de crédito inadimplentes\n\nCLAGE - Idade da linha comercial mais antiga em meses\n\nNINQ - Número de linhas de crédito recentes\n\nCLNO - Número de linhas de crédito\n\nDEBTINC - Rácio dívida / rendimento\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<h2>B - Carregamento dos Dados</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hmeq-data/hmeq.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os tipos e os valores nulos\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>C - Análise Exploratória</h2>\n\nPara ter uma avaliação mais abrangente do rísco de credito, é importante analisar as principais caracteristica do conjuto de dados conforme tabelas e gráfico abaixo, busca-se mais especificamente variáveis, recursos e correlação que contenham poder de classificação."},{"metadata":{},"cell_type":"markdown","source":"* <h3> Estatísticas Descritivas para as Principais Componentes do Risco de Credito</h3>\n\nAtravés das estatísticas descritivas do quadro abaixo, observamos diferenças significativas entre os valores da média (mean) e da mediana (50%) para todas as variaveis numericas, isso indica que as distribuiçõs dos dados tem comportamento assimetrico e os mesmos não tem tedência normalidade."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Estatísticas descritivas para as principais componentes do risco de credito\ndf_d= df[['LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']]\ndf_d.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Por meio dos histogramas abaixo, observamos também que as viariáveis explicativas tem distribuição assiemetria a direita e a esquerda e os mesmos não tendênica a normalidade.\n\nOs modelos de arvore de decisão são modelos simples que não levam em consideração todos os presupostos de não violação dos modelos de regressão multimpla por OLS como a normalidade, que neste dos metodos OLS muitas vezes se faz necessãiros transformações nas varivéis target e explicativas."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando a biblioteca gráfica matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogramas das variávis explicativa numericas \nnumeric_feats = [c for c in df.columns if df[c].dtype != 'object'and c not in ['BAD']]\ndf_numeric_feats = df[numeric_feats]\n\ndf_numeric_feats.hist(figsize=(20,8), bins=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No gráficos de caixa abaixo, podemos verficar a distribuição dos dados, a assimetria, os percentis, a mediana, os valores maximos e minimos e a presença de autlier das variávis explicativas que compoem o modelo de concessão de emprestimo por condição do cliente (inadimplente/ não inadimplente).\n\nCom excessão das componentes LOAN, MORTDUE, VALUE e CLAGE por condiçaõ do cliente (BAD) que tem menor assimetria, as distribuições das demais apresentam forte assimetria a direita e a esquerda portano não estão bem distribuidos."},{"metadata":{"trusted":true},"cell_type":"code","source":"# BoxPlot das variavéis explicativas por condição do emprestimo\nplt.figure(figsize=(16,16))\nc = 1\nfor i in df_numeric_feats.columns:\n    if c < len(df_numeric_feats.columns):\n        plt.subplot(3,3,c)\n        sns.boxplot(x='BAD' , y= i, data=df)\n        c+=1\n    else:\n        sns.boxplot(x='BAD' , y= i, data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No gráfico de caixa abaixo, podemos verifica o comportameto da distribuição dos dados por tipo de trabalho e em função das variávies explicativas e condição do cliente (BAD), a maioria das distribuições apresentam forte assimetria a direita e a esquerda portano os seus dados não estão bem distribuidos.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# BoxPlot da dsitribuição do tipo de trabalho em função das variáveis explicativas e condição do cliente(BAD)\n\nplt.figure(figsize=(16,16))\nc = 1\nfor i in df_numeric_feats.columns:\n    if c < len(df_numeric_feats.columns):\n        plt.subplot(3,3,c)\n        sns.boxplot(x='JOB' , y= i, data=df)\n        c+=1\n    else:\n        sns.boxplot(x='JOB' , y= i, data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <h3>Os Cinco Clientes que Tiveram o Maior Volume de Emprestimo</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Os cinco maiores volumse de emprestimo\n\ndf.nlargest(5,'LOAN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No quadro abaixo, a relação dos empréstimos pagos e não pagos mostra alguma dependência com relação a ocupação. Trabalhadores de escritório e executivos profissionais têm a maior probabilidade de pagar seus empréstimos, enquanto vendas e trabalhadores independentes têm a maior probabilidade de inadimplência. A ocupação mostra um bom poder de discriminação e provavelmente será uma característica importante do nosso modelo de classificação."},{"metadata":{"trusted":true},"cell_type":"code","source":"# O Gráfico de barras abaixo, avalia o total de registros por emprestimo (BAD)\ndf[\"BAD\"].value_counts().plot.bar(title='BAD')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando os empresitimos com relacao a profisão do pagador\n\nJOB=pd.crosstab(df['JOB'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='Tipos de Ocupação e Condição do Empréstimo', figsize=(8,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando os empresitimos com relacao a consolidação da dívida (DebtCon) e melhoria da casa (HomeImp)\nJOB=pd.crosstab(df['REASON'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='Tipos de Ocupação e Condição do Empréstimo', figsize=(8,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <h3>Correlação entre as Dez Variáveis Determinantes para o Risco de Credito</h3>\n\nA tabela abaixo apresenta a correlação entre todas as variáveis que foram propostas para o modelo de risco de credito, é possivel observar que MARTUDUE é a que tem maior correlão com VALUE."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlação entre as 10 variáveis determinantes para o risco de credito.\n\ndf_c= df[['LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']]\ndf_c.corr()\n\n# Plotando a correlação\n# Aumentando a area do gráfico\nf, ax =plt.subplots(figsize=(9,5))\nsns.heatmap(df_c.corr(), annot=True, fmt='.2f', linecolor='black', lw=.7, ax=ax, cmap=plt.cm.Blues)\nplt.title('Correlação entre as 10 variáveis determinantes para o risco de credito')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com o intuito de justificar o uso das variáveis e, também, verificar a existência de multicolinearidade entre as variáveis, foi gerada uma matriz de correlação, como pode ser visto na tabela de modo geral não houve altos coeficientes de correlação entre pares de variáveis com execeção de MARTUDUE e VALUE. A significância dos coeficientes foi avaliada considerando o nível de 5% como limite de Pvalor.\n\nAs variaveis MARTUDUE e VALUE foram mantidas no modelo apesar da correlação forte devido a importancia individual de suas informações para o modelo."},{"metadata":{},"cell_type":"markdown","source":"<h2>D - Tratamento de Dados</h2>\n\n* <h3>Tratamento de Dados: Inputação, Transformação e Criação de Variáveis </h3>\n\nNo quandro abaixo, as variaveis DEROG e DEBTINC tem respectivamente 11.87% e 21,25 de missing, o ideal seria que tivesse no maximo 10%, isso que dizer que estas informações contidas na base de dados não são totalmente consitêntes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o percental de valores missing por variável\ndef missing_values(data_frame):\n    total = data_frame.isnull().count()\n    missing = data_frame.isnull().sum()\n    missing_percent = missing/total * 100\n    display(missing_percent)\n    \nmissing_values(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O procedimento abaixo busca eliminar os registros que tem mais da metada das colunas (variavéis) com registros nulos, buscar eliminar os registros que possuem poucos componentes de predição, a decisção foi tomada baseada no não enviesamento do modelo caso fosse feito um processo de inputação."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna(thresh=df.shape[1]/2, axis=0)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando o percental de valores missing após a eliminação dos registros que tem mais da metada das colunas nulas\ndef missing_values(data_frame):\n    total = data_frame.isnull().count()\n    missing = data_frame.isnull().sum()\n    missing_percent = missing/total * 100\n    display(missing_percent)\n    \nmissing_values(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tratameto das Variáveis Categóricas:\n\nConforme quandro acima, como as variáveis categoricas REASON e JOB tem menos de 10% de missing em sua composição, vamos utilizar o procedimento abaixo para correção através da categoria que apresenta a maior frequência (moda)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tratando colunas categóricas\nfor col in df.select_dtypes(include='object').columns:\n    if df[col].isna().sum() > 0:\n         df[col].fillna(df[col].mode()[0], inplace=True)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tratameto das Variáveis Numéricas:\n\nPara corrigir as variavéis numéricas vamos preencher os valores nulos (missing) com o valor da médiana, visto que esta medida se apresenta melhor do que a média e moda dado que as distribuições não tende a normalidade como já foi mostrados nos gráficos anteriores.\n\nComo a variável numérica DEBTINC tem mais de 10% de missing em sua composição poderiamos adotar o seguinte critérios:\n\n* A eliminação total desta variável, mais neste caso ela não tem uma correlação forte com as demais e é importante para o modelo. \n* A eliminação de todos os registros que tem essa variável como missing poderia levar a uma perda significativa de informações para o modelo, tendo em vista que a base de dados não é tão ampla."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tratando colunas numéricas\nfor col in df.select_dtypes(exclude='object').columns:\n    if df[col].isna().sum() > 0:\n        df[col].fillna(df[col].median(), inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Balanceamento de Classes\n\nO balanceamento de classes é importante para a precisão do modelo, o método escolhido foi o de \"upsampling\" da classe de menor frequência por meio de uma função do pacote Scikit-Learn. A funçaõ mostra a distribuição das classes em todas as variáveis categóricas, mas iremos balancear somente a variável \"BAD\".\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def showBalance(df, col):\n    for c in col:\n        print('Distribuição da Coluna: ', c,'\\n',df[c].value_counts(normalize=True),'\\n')\n    else:\n       pass\n        \nshowBalance(df, col=['REASON','JOB','BAD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando a biblioteca resample\nfrom sklearn.utils import resample\n\n# Função para realizar balanceamento\ndef balance(df, col):\n    df_maior = df[df[col] == df[col].mode()[0]]\n    df_menor = df[df[col] != df[col].mode()[0]]\n \n    # Upsample da menor classe\n    df_menor_upsampled = resample(df_menor, \n                                  replace=True,     \n                                  n_samples=df_maior.shape[0],\n                                  random_state=42) \n \n    # Combinar as classe predominante com a menor classe aumentada\n    df_upsample = pd.concat([df_maior, df_menor_upsampled])\n\n    # Display new class counts\n    print('Contagem de registros')\n    print(df_upsample['BAD'].value_counts())\n    print('\\nDistribuição dos registros')\n    print(df_upsample['BAD'].value_counts(normalize=True))\n\n    return df_upsample\n    \ndf_upsample = balance(df, 'BAD')\nprint('\\n')\nshowBalance(df_upsample, col=['REASON','JOB','BAD'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como muitos dos modelos de Machine Learning usam apenas os valores numéricos como entrada, precisamos converter colunas categóricas em variáveis ​​numéricas Dummy, transforma variáveis ​​categóricas em uma série de 0 e 1, facilitando muito a comparação"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando dummys para todas as colunas\ndf_upsample = pd.get_dummies(df_upsample, columns=['REASON', 'JOB'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os tipos dos dados e os tamanhos\ndf_upsample.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Olhando os dados aleatoriamente\ndf_upsample.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <h2>IV - ANALISE E DISCUSSÃO DOS RESULTADOS</h2> <a name=\"IV - ANALISE E DISCUSSÃO DOS RESULTADOS\"></a>\n\n<h2>A - Separar o Universo de Dados em Treino e Teste</h2>\n\nO universo de dados foi separado em 70% para treino e 30% para teste."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando o dataframe\n\nfrom sklearn.model_selection import train_test_split\n\n# Separando treino e teste\n# Em 70% para treino e 30% para teste\ntrain, test = train_test_split(df_upsample, test_size=0.20, random_state=42)\n\n# Não vamos mais usar o dataset de validação\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# definindo colunas de entrada\nfeats = [c for c in df_upsample.columns if c not in ['BAD']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>B - Instanciando o Modelo Random Forest e usando Crosss Validation</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Trabalhando com Random Forest e oob_score\n# Importando o modelo\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instanciar o modelo\nrf = RandomForestClassifier(n_jobs=-1, oob_score=True, n_estimators=200, random_state=42, criterion='gini')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usar o cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>C - Instanciando o Modelo Random Forest e usando GridSearchCV</h2>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importação bibliotecas\n# Importação GridSearchCV.\nfrom sklearn.model_selection import GridSearchCV\n\n#instanciando o modelo\nrf2=RandomForestClassifier(n_jobs=-1)\n\n#setando parametros para o gridSearchCV\nparam_dict = {'n_estimators':[100,400,800,1000],  'criterion': ['gini','entropy'] }\n\ngrid2 = GridSearchCV(rf2, param_dict, cv=10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usar o cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf2, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>D - Instanciando o Modelo GBM e usando Crosss Validation</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trabalhando com GBM\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usar o cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(gbm, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>E - Instanciando o Modelo XGBoost e usando Crosss Validation</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trabalhando com XGBoost\n\n# Importar o modelo\nfrom xgboost import XGBClassifier\n\n# Instanciar o modelo\nxgb = XGBClassifier(n_jobs=-1, oob_score=True, n_estimators=200, random_state=42, learning_rate=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usando o cross validation\nscores = cross_val_score(xgb, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>F- Analise dos Resultados dos Modelos Random Forest, Random Forest com GridSearchCV, GBM e XGBoost</h2>\n\nFoi utilizado o cross validation em 5 agrupamentos de dados aleatórios a fim de verificar o desempenho de cada modelo ao lidar com observaçoes diferentes, o desempenho é avaliado pela média dos scores, presetando diferenaçs apenas na terceira casa decimal.\n\nOs modelos Random Forest e Random Forest com GridSearchCV tiveram o melhor desempenho entre os modelos, ambos tiveram um resultado muito proximo pelo teste de Score, apenas houve diferêncas nas casa decimais. \n\n<h4>Accuracy</h4>\n\nNo contexto abaixo, a \"accuracy\" diz quanto o meu modelo acertou das previsões possíveis, é a razão entre o somatório das previsões corretas (verdadeiros positivos com verdadeiros negativos) sobre o somatório total das previsões.\n\nAcúracia=(Verdadeiros Positivos(VP) +  Verdadeiros Negativos(VN)) / Total\n\n<h4>Matriz de Confusão das Previsões dos Dados de Validação</h4>\n\nÉ uma tabela que mostra as frequências de classificação para cada classe do modelo, ela busca saber se meu modelo previu bem.\n\n- Verdadeiro positivo (true positive — TP): ocorre quando no conjunto real, a classe que estamos buscando foi prevista corretamente.\n    \n- Falso positivo (false positive — FP): ocorre quando no conjunto real, a classe que estamos buscando prever foi prevista incorretamente\n    \n- Falso verdadeiro (true negative — TN): ocorre quando no conjunto real, a classe que não estamos buscando prever foi prevista corretamente.\n    \n- Falso negativo (false negative — FN): ocorre quando no conjunto real, a classe que não estamos buscando prever foi prevista incorretamente."},{"metadata":{},"cell_type":"markdown","source":"* <h2>Random Forest: Accuracy e Matriz de Confusão</h2>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Treinar o modelo,usando o Random Forest para treinamento e predição\nrf.fit(train[feats], train['BAD'])\n\n# Fazendo predições\npreds = rf.predict(test[feats])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Accuracy </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando o desempenho do modelo (Accuracy)\n\n# Importando a metrica\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(test['BAD'], preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Matriz de Confusão - Random Forest </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# matriz de cofussão\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(test['BAD'], rf.predict(test[feats]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(test['BAD'], rf.predict(test[feats]))\n\nfig, ax = plt.subplots(figsize=(7,6))\nsns.heatmap(cm, annot=True, ax=ax, fmt='.0f',cmap=plt.cm.Blues); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('Real labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['1', '0'])\nax.yaxis.set_ticklabels(['1', '0']);\nplt.show()\n\nsens = cm[0,0] / (cm[0,0] + cm[1,0])\nesp = cm[1,1] / (cm[0,1] + cm[1,1])\nefi = (sens+esp)/2\n\nprint('Sensibilidade: ', round(sens,4))\nprint('Especificidade: ', round(esp,4))\nprint('Esficiência: ', round(efi,4))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Matriz de confusão: \n\nPortatno, a Matriz de Confusão nos mostra que o modelo de Random Forest teve altos índices de acerto de acordo com a Accuracy, Sensibilidade, Especificidade e Eficiência.\n\nNota:\nNa prática, a sensibilidade e a especificidade variam em direções opostas. Isto é, geralmente, quando um método é muito sensível a positivos, tende a gerar muitos falso-positivos, e vice-versa. Assim, um método de decisão perfeito (100 % de sensibilidade e 100% especificidade) raramente é alcançado, e um balanço entre ambos deve ser atingido.\n"},{"metadata":{},"cell_type":"markdown","source":"<h3>Avaliando a Importância de Cada Variável no Modelo Random Forest </h3>\n\nNo gráfico abaixo, verificamos que a variável DBTINC é a que tem maior efeito sobre a classificação do modelo de risco de emprestimo.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando a importancia de cada coluna de entrada\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <h2>Random Forest com GridSearchCV: Accuracy e Matriz de Confusão</h2>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Accuracy </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Treinar o modelo,usando o Random Forest para treinamento e predição\nrf2.fit(train[feats], train['BAD'])\n\n# Fazendo predições\npreds = rf2.predict(test[feats])\n\naccuracy_score(test['BAD'], preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><h3>Matriz de Confusão - Random Forest com GridSearchCV </h3> </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# matriz de cofussão\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(test['BAD'], rf2.predict(test[feats]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(test['BAD'], rf2.predict(test[feats]))\n\nfig, ax = plt.subplots(figsize=(7,6))\nsns.heatmap(cm, annot=True, ax=ax, fmt='.0f',cmap=plt.cm.Blues); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('Real labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['1', '0'])\nax.yaxis.set_ticklabels(['1', '0']);\nplt.show()\n\nsens = cm[0,0] / (cm[0,0] + cm[1,0])\nesp = cm[1,1] / (cm[0,1] + cm[1,1])\nefi = (sens+esp)/2\n\nprint('Sensibilidade: ', round(sens,4))\nprint('Especificidade: ', round(esp,4))\nprint('Esficiência: ', round(efi,4))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Matriz de confusão: \n\nPortatno, a Matriz de Confusão nos mostra que o modelo de Random Forest com GridSearchCV também teve altos índices de acerto de acordo com a Accuracy, a Sensibilidade, Especificidade e Eficiência."},{"metadata":{},"cell_type":"markdown","source":"<h3>Avaliando a Importância de Cada Variável no Modelo com Rondom Forest com GridSearchCV </h3>\n\nNo gráfico abaixo, verificamos que a variável DBTINC é a que tem maior efeito sobre a classificação do modelo de risco de emprestimo."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando a importancia de cada coluna de entrada\npd.Series(rf2.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <h2>V - CONCLUSÃO</h2> <a name=\"V - CONCLUSÃO\"></a>\n\nEste notebook buscou explorar diferentes algoritmos de aprendizado supervisionado para problemas de classificação, como Random Forest, Random Forest com GridSearchCV, GBM e XGBoost.\n\n   A análise geral mostra que o modelo de Random Forest e Random Forest com GridSearchCV funcionou muito bem em comparação com outros modelos.\n \n   A diferênça entre os dois ficou apenas na terceira casa decimal no teste de Score, accuracy, sensibilidade, especificidade e eficiência, portanto os dois modelos em especial o Random Forest(com oob_score) poderiam ser usados para estimar as probalidades de de classificação para as linhas de créditos."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}