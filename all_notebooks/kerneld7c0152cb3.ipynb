{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nimport random as rn\n\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout\nfrom tensorflow import set_random_seed\nimport torch as tc\n\npath = './../input/aclimdb/'\nnp.random.seed(1)\nset_random_seed(1)\ntc.manual_seed(1)\nrn.seed(1)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def shuffle(X, y):\n    perm = np.random.permutation(len(X))\n    X = X[perm]\n    y = y[perm]\n    return X, y","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_imdb_dataset(path):\n    imdb_path = os.path.join(path, 'aclImdb')\n\n    \n    train_texts = []\n    train_labels = []\n    test_texts = []\n    test_labels = []\n    for dset in ['train', 'test']:\n        for cat in ['pos', 'neg']:\n            dset_path = os.path.join(imdb_path, dset, cat)\n            for fname in sorted(os.listdir(dset_path)):\n                if fname.endswith('.txt'):\n                    with open(os.path.join(dset_path, fname)) as f:\n                        if dset == 'train': train_texts.append(f.read())\n                        else: test_texts.append(f.read())\n                    label = 0 if cat == 'neg' else 1\n                    if dset == 'train': train_labels.append(label)\n                    else: test_labels.append(label)\n\n    train_texts = np.array(train_texts)\n    train_labels = np.array(train_labels)\n    test_texts = np.array(test_texts)\n    test_labels = np.array(test_labels)\n\n    train_texts, train_labels = shuffle(train_texts, train_labels)\n    test_texts, test_labels = shuffle(test_texts, test_labels)\n\n    return train_texts, train_labels, test_texts, test_labels","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trX, trY, ttX, ttY = load_imdb_dataset(path)\n\nprint ('Train samples shape :', trX.shape)\nprint ('Train labels shape  :', trY.shape)\nprint ('Test samples shape  :', ttX.shape)\nprint ('Test labels shape   :', ttY.shape)","execution_count":43,"outputs":[{"output_type":"stream","text":"Train samples shape : (25000,)\nTrain labels shape  : (25000,)\nTest samples shape  : (25000,)\nTest labels shape   : (25000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"NGRAM_RANGE = (1, 2)\nTOP_K = 25000\nTOKEN_MODE = 'word'\nMIN_DOC_FREQ = 2\n\ndef ngram_vectorize(train_texts, train_labels, val_texts):\n    kwargs = {\n        'ngram_range' : NGRAM_RANGE,\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : TOKEN_MODE,\n        'min_df' : MIN_DOC_FREQ,\n    }\n    \n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    x_train = tfidf_vectorizer.fit_transform(train_texts)\n    x_val = tfidf_vectorizer.transform(val_texts)\n    \n    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n    selector.fit(x_train, train_labels)\n    x_train = selector.transform(x_train).astype('float32')\n    x_val = selector.transform(x_val).astype('float32')\n    return x_train, x_val","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_last_layer_units_and_activation(num_classes):\n    activation = 'softmax'\n    units = num_classes\n    return units, activation","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n    op_units, op_activation = get_last_layer_units_and_activation(num_classes)\n    model = models.Sequential()\n    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n    \n    for _ in range(layers-1):\n        model.add(Dense(units=units, activation='relu'))\n        model.add(Dropout(rate=dropout_rate))\n        \n    model.add(Dense(units=op_units, activation=op_activation))\n    return model","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_ngram_model(data, learning_rate=1e-3, epochs=1000, batch_size=128, layers=2, units=64, \n                      dropout_rate=0.2):\n    \n    num_classes = 25000\n    \n    \n    trX, trY, ttX, ttY = data\n    \n    x_train, x_val = ngram_vectorize(trX, trY, ttX)\n    \n    \n    model = mlp_model(layers, units=units, dropout_rate=dropout_rate,\n                      input_shape=x_train.shape[1:], num_classes=num_classes)\n    \n    \n   \n    loss = 'sparse_categorical_crossentropy'\n    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n    \n    \n    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n    \n\n    history = model.fit(x_train, trY, epochs=epochs, validation_data=(x_val, ttY),\n                        verbose=2, batch_size=batch_size, callbacks=callbacks)\n    \n\n    history = history.history\n    val_acc = history['val_acc'][-1]\n    val_loss = history['val_loss'][-1]\n    print ('Validation accuracy: {acc}, loss: {loss}'.format(\n            acc=val_acc, loss=val_loss))\n    \n    model.save('IMDB_mlp_model_' + str(val_acc) + '_' + str(loss) + '.h5')\n    return val_acc, val_loss","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/device:GPU:2'):\n    results = train_ngram_model((trX, trY, ttX, ttY))\n\nprint ('With lr=1e-3 | val_acc={results[0]} | val_loss={results[1]}'.format(results=results))\nprint ('===========================================================================================')","execution_count":48,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1577: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n  UserWarning)\n","name":"stderr"},{"output_type":"stream","text":"Train on 25000 samples, validate on 25000 samples\nEpoch 1/1000\n - 5s - loss: 4.3918 - acc: 0.5284 - val_loss: 0.7342 - val_acc: 0.5000\nEpoch 2/1000\n - 4s - loss: 0.6814 - acc: 0.6892 - val_loss: 0.5799 - val_acc: 0.8617\nEpoch 3/1000\n - 4s - loss: 0.4780 - acc: 0.8328 - val_loss: 0.3927 - val_acc: 0.8796\nEpoch 4/1000\n - 4s - loss: 0.3471 - acc: 0.8760 - val_loss: 0.3179 - val_acc: 0.8912\nEpoch 5/1000\n - 4s - loss: 0.2813 - acc: 0.8970 - val_loss: 0.2814 - val_acc: 0.8968\nEpoch 6/1000\n - 4s - loss: 0.2419 - acc: 0.9140 - val_loss: 0.2605 - val_acc: 0.9014\nEpoch 7/1000\n - 4s - loss: 0.2119 - acc: 0.9269 - val_loss: 0.2464 - val_acc: 0.9038\nEpoch 8/1000\n - 4s - loss: 0.1876 - acc: 0.9330 - val_loss: 0.2368 - val_acc: 0.9061\nEpoch 9/1000\n - 4s - loss: 0.1590 - acc: 0.9459 - val_loss: 0.2328 - val_acc: 0.9055\nEpoch 10/1000\n - 4s - loss: 0.1370 - acc: 0.9552 - val_loss: 0.2300 - val_acc: 0.9080\nEpoch 11/1000\n - 4s - loss: 0.1208 - acc: 0.9616 - val_loss: 0.2307 - val_acc: 0.9070\nEpoch 12/1000\n - 4s - loss: 0.1063 - acc: 0.9650 - val_loss: 0.2342 - val_acc: 0.9064\nValidation accuracy: 0.906440019607544, loss: 0.23417968753814697\nWith lr=1e-3 | val_acc=0.906440019607544 | val_loss=0.23417968753814697\n===========================================================================================\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}