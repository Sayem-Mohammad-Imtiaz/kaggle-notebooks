{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# importing useful libraries\nimport numpy as np\nimport tensorflow as tf\nimport random as python_random\n\n#setting random seed\nnp.random.seed(1)\npython_random.seed(12)\ntf.random.set_seed(123)\n\nimport pandas as pd\n\nfrom sklearn.metrics import fbeta_score\n\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Conv2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.metrics import Metric\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_classes_df = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\nn_train = !ls ../input/planets-dataset/planet/planet/train-jpg | wc -l\nprint('There are {} images in the training set'.format(n_train[0]))\ntrain_classes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_number =10\nimg = io.imread('../input/planets-dataset/planet/planet/train-jpg/train_{}.jpg'.format(image_number))\nprint('images are of shape {}'.format(img.shape))\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding the unique labels in our dataset\nunique_labels = set()\n\n# defining a function to add labels to a set\ndef append_labels(tags):\n    for tag in tags.split():\n        unique_labels.add(tag)\n\ntrain_classes = train_classes_df.copy() # creates a copy to avoid mutating traiin_classes_df\ntrain_classes['tags'].apply(append_labels)\nunique_labels = list(unique_labels) # converting unique_labels to a list so they can be ordered\nprint('Unique labels are {} in number and they are {}'.format(len(unique_labels), unique_labels))\n\n# let's do one hot encoding on the labels in 'train_classes'\nfor tag in unique_labels:\n    train_classes[tag] = train_classes['tags'].apply(lambda x: 1 if tag in x.split() else 0)\n    \n# adding '.jpg' extension to 'image_name'\ntrain_classes['image_name'] = train_classes['image_name'].apply(lambda x: '{}.jpg'.format(x)) \ntrain_classes.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col = list(train_classes.columns[2:]) # storing the tags column names as a variable\n\n# initializing an image generator and rescaling the images\nimage_gen = ImageDataGenerator(rescale=1/255)\n\n# loading images from dataframe\nX = image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='/kaggle/input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=1, batch_size=128)\n\n# X is an iterable, It contains 317 batches, each batch contains 128 images and labels because \n#40479 / 128 is 316 remainder 31 each image is of shape (128, 128, 3), each label is of shape (17, )\n\n# let's abitrarily view an image say the 109th image\nx109 = X[0][0][109] # first batch, images, 109th image\ny109 = X[0][1][109] # first batch, labels, 109th label\nprint(\"Each image's shape is {}\".format(x109.shape))\nprint(\"Each label's shape is {}\".format(y109.shape))\nprint('We have {} batches'.format(len(X)))\nprint('Each batch has {} images/labels'.format(X[0][0].shape[0]))\nprint('40479/128 is {:.2F}, so the last batch will have {} images/labels'.format(\\\n                                                                                40479/128, X[316][0].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"beta = 2 # arbitrarily setting beta to 2. You can set it to any value you choose to\nthreshold = 0.2 # arbitrarily setting beta to 0.2. You can set it to any value you choose to\n\ndef multi_label_fbeta(ytrue , ypred, beta=beta, average='samples', threshold=threshold, epsilon=1e-7, \\\n                      sample_weight=None):\n    # epsilon is set to avoid division by zero error\n    beta_squared = beta**2\n\n    # casting ytrue and ypred as floats\n    ytrue = tf.cast(ytrue, tf.float32)\n    \n    # making ypred one hot encoded \n    ypred = tf.cast(tf.greater_equal(tf.cast(ypred, tf.float32), tf.constant(threshold)), tf.float32)\n    \n    if average == 'samples':\n        tp = tf.reduce_sum(ytrue * ypred, axis=-1) # calculating true positives\n        predicted_positive = tf.reduce_sum(ypred, axis=-1) # calculating predicted positives\n        actual_positive = tf.reduce_sum(ytrue, axis=-1) # calculating actual positives\n    \n    else: # either any of 'macro', 'weighted' and 'raw'\n        tp = tf.reduce_sum(ytrue * ypred, axis=0) # calculating true positives\n        predicted_positive = tf.reduce_sum(ypred, axis=0) # calculating predicted positives\n        actual_positive = tf.reduce_sum(ytrue, axis=0) # calculating actual positives\n    \n    # calculating precision and recall\n    precision = tp/(predicted_positive+epsilon)\n    recall = tp/(actual_positive+epsilon)\n\n    # finding fbeta\n    fb = (1+beta_squared)*precision*recall / (beta_squared*precision + recall + epsilon)\n\n    if average == 'weighted':\n        supports = tf.reduce_sum(ytrue, axis=0)\n        return tf.reduce_sum(fb*supports / tf.reduce_sum(supports))\n\n    elif average == 'raw':\n        return fb\n    \n    elif average == 'samples' and sample_weight is not None:\n        return tf.reduce_sum(fb*sample_weight)\n    \n    return tf.reduce_mean(fb) # then it is either 'macro' or 'samples' (without sample weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(metrics=multi_label_fbeta):\n    model = Sequential() # initializes a sequential layer\n    \n    model.add(Conv2D(filters=128, kernel_size=3, input_shape=(128, 128, 3))) # adds a convolutional layer\n    \n    model.add(Flatten()) # flattens the layer\n    \n    model.add(Dense(17, activation='sigmoid')) # output layer. Notice the actiation function\n    \n    opt = Adam(lr=1e-2) # initializes an optimizer\n    \n    # compiling the model\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[metrics])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_gen = ImageDataGenerator(rescale=1/255,validation_split=0.2)\n\n# generating the 80% training image data\ntrain_gen = train_image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='../input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=0, batch_size=128, subset='training')\n\n# generating the 20% validation image data\nval_gen = train_image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='../input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=0, batch_size=128, subset='validation')\n\n# setting step size for training and validation image data\nstep_train_size = int(np.ceil(train_gen.samples / train_gen.batch_size))\nstep_val_size = int(np.ceil(val_gen.samples / train_gen.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model() # building model for training\n\n# fitting the model\nmodel.fit(x=train_gen, steps_per_epoch=step_train_size, validation_data=val_gen, validation_steps=step_val_size,\n         epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_ytrue = np.random.choice([0, 1], (20, 17))\nrandom_ypred = np.random.choice([0, 1], (20, 17))\n\nprint('f1_score of prediction using multi_label_fbeta is {}'.format(multi_label_fbeta(random_ytrue, random_ypred)))\nprint('f1_score of prediction using scikit-learn fbeta is {}'.format(fbeta_score(\\\n                                                    random_ytrue, random_ypred, beta=2, average='samples')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_class = 17\nclass StatefullMultiLabelFBeta(Metric):\n    def __init__(self, name='state_full_binary_fbeta', beta=beta, average='samples', \\\n                 n_class=n_class, threshold=threshold, epsilon=1e-7, **kwargs):\n        \n        # initializing an object of the super class\n        super(StatefullMultiLabelFBeta, self).__init__(name=name, **kwargs)\n            \n        # initializing atrributes\n        self.tp = self.add_weight(name='tp', shape=(n_class,), initializer='zeros') # initializing true positives\n        self.actual_positives = self.add_weight(name='ap', shape=(n_class,), initializer='zeros') \n        self.predicted_positives = self.add_weight(name='pp', shape=(n_class,), initializer='zeros')\n\n        self.n_samples = self.add_weight(name='n_samples', initializer='zeros')\n        self.sum_fb = self.add_weight(name='sum_fb', initializer='zeros')\n\n        # initializing other atrributes that wouldn't be changed for every object of this class\n        self.beta_squared = beta**2\n        self.average = average\n        self.n_class = n_class\n        self.threshold = threshold\n        self.epsilon = epsilon\n\n    def update_state(self, ytrue, ypred, sample_weight=None):\n        # casting ytrue float dtype\n        ytrue = tf.cast(ytrue, tf.float32)\n        \n        # making ypred one hot encoded \n        ypred = tf.cast(tf.greater_equal(tf.cast(ypred, tf.float32), tf.constant(threshold)), tf.float32)\n        \n        if self.average == 'samples': # we are to keep track of only fbeta\n            # calculate true positives, predicted positives and actual positives atrribute along the last axis\n            tp = tf.reduce_sum(ytrue*ypred, axis=-1) \n            predicted_positives = tf.reduce_sum(ypred, axis=-1)\n            actual_positives = tf.reduce_sum(ytrue, axis=-1)\n            \n            precision = tp/(predicted_positives+self.epsilon) # calculate the precision\n            recall = tp/(actual_positives+self.epsilon) # calculate the recall\n            \n            # calculate the fbeta score\n            fb = (1+self.beta_squared)*precision*recall / (self.beta_squared*precision + \\\n                                                                      recall + self.epsilon)\n            \n            if sample_weight is not None: # if sample weight is available for stand alone usage\n                self.fb = tf.reduce_sum(fb*sample_weight)\n            else:\n                n_rows = tf.reduce_sum(tf.shape(ytrue)*tf.constant([1, 0])) # getting the number of rows in ytrue\n                self.n_samples.assign_add(tf.cast(n_rows, tf.float32)) # updating n_samples\n                self.sum_fb.assign_add(tf.reduce_sum(fb)) # getting the running sum of fb\n                self.fb = self.sum_fb / self.n_samples # getting the running mean of fb\n\n        else:\n            # keep track of true, predicted and actual positives because they are calculated along axis 0\n            self.tp.assign_add(tf.reduce_sum(ytrue*ypred, axis=0)) \n            self.assign_add(predicted_positives = tf.reduce_sum(ypred, axis=0))\n            self.actual_positives.assign_add(tf.reduce_sum(ytrue, axis=0)) \n            \n    def result(self):\n        if self.average != 'samples':\n            precision = self.tp/(self.predicted_positives+self.epsilon) # calculate the precision\n            recall = self.tp/(self.actual_positives+self.epsilon) # calculate the recall\n\n            # calculate the fbeta score\n            fb = (1+self.beta_squared)*precision*recall / (self.beta_squared*precision + \\\n                                                                      recall + self.epsilon)\n            if self.average == 'weighted':\n                return tf.reduce_sum(fb*self.actual_positives / tf.reduce_sum(self.actual_positives))\n\n            elif self.average == 'raw':\n                return fb\n            \n            return tf.reduce_mean(fb) # then it is 'macro' averaging \n    \n        return self.fb # then it is either 'samples' with or without sample weight\n\n    def reset_states(self):\n        self.tp.assign(tf.zeros(self.n_class)) # resets true positives to zero\n        self.predicted_positives.assign(tf.zeros(self.n_class)) # resets predicted positives to zero\n        self.actual_positives.assign(tf.zeros(self.n_class)) # resets actual positives to zero\n        self.n_samples.assign(0)\n        self.sum_fb.assign(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"statefull_multi_label_fbeta = StatefullMultiLabelFBeta()\n\nstatefull_model = build_model(metrics=statefull_multi_label_fbeta) # building model for training\n\ntrain_gen.reset() # reseting the training set generator \nval_gen.reset() # reseting the validation set generator \n\n# fitting the model\nstatefull_model.fit(x=train_gen, steps_per_epoch=step_train_size, validation_data=val_gen, \\\n                    validation_steps=step_val_size, epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_sample = 20\nn_class = 17\nm = StatefullMultiLabelFBeta(n_class=n_class) # initializes a stateful multi class fbeta object\n\nrandom_ytrue = np.random.choice([0, 1], (n_sample, n_class))\nrandom_ypred = np.random.choice([0, 1], (n_sample, n_class))\n\nm.update_state(random_ytrue, random_ypred)\nprint('Intermediate result for stateful multi class fbeta is: {}'.format(float(m.result())))\nprint('Intermediate result for scikit-learn fbeta is: {}'.format(fbeta_score(\\\n                                                        random_ytrue, random_ypred, beta=2, average='samples')))\nprint()\n\nincrement_size = 20\na_true = np.random.choice([0, 1], (increment_size, n_class))\na_pred = np.random.choice([0, 1], (increment_size, n_class))\n\nm.update_state(a_true, a_pred)\nprint('Final result for stateful multi class fbeta is: {}'.format(float(m.result())))\n\narr_true = np.append(random_ytrue, a_true, axis=0)\narr_pred = np.append(random_ypred, a_pred, axis=0)\n\nprint('Final result for scikit-learn multi class fbeta is: {}'.format(\\\n                                                    fbeta_score(arr_true, arr_pred, beta=2, average='samples')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}