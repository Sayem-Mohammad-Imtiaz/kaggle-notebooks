{"cells":[{"metadata":{"id":"N4PCaG4zmB6u"},"cell_type":"markdown","source":"# Milan Price Prediction\nN.b to see the output of the commands, go to my colab file: https://colab.research.google.com/drive/1SNDJqy8hIXT6z2BjiBQqc3YifkhVX1_9?usp=sharing\n\nThis summer, I am going to work in Milan. Therefore, I though it would be intersting to predict the price at which an airbnb is gonna sell so that I can estimate whether the house I am renting is overpriced or underpriced. \n\n\n\n","execution_count":null},{"metadata":{"id":"MA4JGr8tn687"},"cell_type":"markdown","source":"## Context \nThe dataset I am using is [this](https://www.kaggle.com/antoniokaggle/milan-airbnb-open-data-only-entire-apartments) one, which contains all the entire apartments located in Milan. The dataset was originally taken from the airbnb site","execution_count":null},{"metadata":{"id":"6N3LwjY3n7RD"},"cell_type":"markdown","source":"## Problem statement\nWe would like to predict, given a series of variables relative to an apartment, the price at which it should be rented. ","execution_count":null},{"metadata":{"id":"C_-Pc06Fn7fz"},"cell_type":"markdown","source":"## Setup\nImport the necessary libraries","execution_count":null},{"metadata":{"id":"M5iXtaeyn0Ol","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport sklearn\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"L2DQITnWoGJj"},"cell_type":"markdown","source":"Load the data","execution_count":null},{"metadata":{"id":"e2eWQ2FRoFE8","trusted":true},"cell_type":"code","source":"apartments = pd.read_csv(\"../input/milan-airbnb-open-data-only-entire-apartments/Airbnb_Milan.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"7-pl48I0orWD"},"cell_type":"markdown","source":"Let's start by taking a look at the data","execution_count":null},{"metadata":{"id":"E1R7l1xfokTz","outputId":"83f9de16-a66f-4291-d1c3-23587d9e862f","trusted":true},"cell_type":"code","source":"apartments.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"1y9R5Zoknh4R"},"cell_type":"markdown","source":"We can see that we have many fields, some of which may not be that helpful. We also note that there are no null values","execution_count":null},{"metadata":{"id":"-51BQmIPowAc","outputId":"0c2891d9-0e46-4371-aafc-a869f0c113d4","trusted":true},"cell_type":"code","source":"apartments.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"WPgDOmplrk9p"},"cell_type":"markdown","source":"## Data cleaning\n\nLet's take a look more deeply into the data to clean it","execution_count":null},{"metadata":{"id":"YvRia6D1r1tH","outputId":"a671a99d-4b8d-40b3-8c25-8354f367980e","trusted":true},"cell_type":"code","source":"apartments.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"X-ZJLKI_r3ev"},"cell_type":"markdown","source":"As we can see, we can for sure remove the Unnamed column (which is just a record counter), the id of the apartment, the host id and the zip code.","execution_count":null},{"metadata":{"id":"XGEA3UFFsSJP","trusted":true},"cell_type":"code","source":"apartments.drop(apartments.columns[[0, 1, 2]],axis=1,inplace=True)\napartments.drop(columns=[\"zipcode\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"jFwnA1k3vTR1"},"cell_type":"markdown","source":"We are going to assume a stay of 7 days, since the guest has to pay both for the cleaning fee and the daily fee, it's convenient to put them togheter in a new column. The cleaning fee has to be paid only one time","execution_count":null},{"metadata":{"id":"0OM2l03ovrXQ","trusted":true},"cell_type":"code","source":"weekly_price = apartments.cleaning_fee + apartments.daily_price * 7\napartments[\"weekly_price\"] = weekly_price.values\napartments.drop(columns=[\"cleaning_fee\", \"daily_price\"], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"XJiG2qPnjAbt"},"cell_type":"markdown","source":"We are also going to remove some data which we don't find important in our analysis. For example, since room type is always the same","execution_count":null},{"metadata":{"id":"FzFgXwbMjO8W","outputId":"1269014e-d8a4-46df-8b87-2937070ec625","trusted":true},"cell_type":"code","source":"apartments.room_type.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"VKkWH8WYjSxF"},"cell_type":"markdown","source":"We can remove it","execution_count":null},{"metadata":{"id":"XLKPThKMjUNF","trusted":true},"cell_type":"code","source":"apartments.drop(columns=[\"room_type\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"YK6AQGusuyQ2"},"cell_type":"markdown","source":"The data from the dataset is overall pretty clean and the other information may be useful for our predictions, so for now we are going to leave them.","execution_count":null},{"metadata":{"id":"F3RAHs5PvFTe"},"cell_type":"markdown","source":"## Explorative analysis\n\n","execution_count":null},{"metadata":{"id":"VjCzIbZymgA2"},"cell_type":"markdown","source":"\n### Visualizing the data\nOne interesting thing we can do is visualize the apartments on a map. For the map to be relevant, we should plot the points given their latitude, longitude, weekly price, and the \"municipio\" in which they are. ","execution_count":null},{"metadata":{"id":"aBbIOtd0y5Qb","outputId":"a5de42ef-891f-40dc-ccac-b5952befc26d","trusted":true},"cell_type":"code","source":"import urllib\n\ncmap = cm.jet\nm = cm.ScalarMappable(cmap=cmap)\nquartiere_colors = m.to_rgba(apartments.neighbourhood_cleansed)\nquant_minimum = apartments.weekly_price.quantile(0.1)\nquant_maximum = apartments.weekly_price.quantile(0.9)\nprice = ((apartments.weekly_price - quant_minimum) / (quant_maximum - quant_minimum)) * 30\n\n#initializing the figure size\nplt.figure(figsize=(20,20))\n#loading the png milan image found on open street map and saving to my local folder along with the project\ni=urllib.request.urlopen('https://i.ibb.co/s1Jf5k7/map-2.png')\nmil_img=plt.imread(i)\n#scaling the image based on the latitude and longitude max and mins for proper output\nplt.imshow(mil_img, zorder=0, extent=[\n                                      apartments.longitude.min(), \n                                      apartments.longitude.max(), \n                                      apartments.latitude.min(), \n                                      apartments.latitude.max(),\n                                    ]\n           )\n\nax=plt.gca()\n# plot the points\napartments.plot(kind='scatter', x='longitude', y='latitude', label='price', c=quartiere_colors, s=price, ax=ax, zorder=5, edgecolors='black')\n\npatch = []\nfor a in range(1, 9):\n  patch.append(mpatches.Patch(color=m.to_rgba(a), label='Neighborhood ' + str(a)))\n\nplt.legend(handles=patch)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"sfOvkLvsVZf7"},"cell_type":"markdown","source":"We can notice how the majority of the Airbnb apartments are in the center (neighbourhood = 1, the one in dark blue) and the further we are from the center the less the price of the housing is","execution_count":null},{"metadata":{"id":"6bA4efK2gGqu","outputId":"27dd7f1d-4109-4d50-fd7f-0b4edb06b521","trusted":true},"cell_type":"code","source":"apartments.neighbourhood_cleansed.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"P2qfW0dDdRLZ"},"cell_type":"markdown","source":"### Visualizing specific type of apartments\n\nLet's now take a look at parts of our data on the map. Let's first create a function to draw the graph\n","execution_count":null},{"metadata":{"id":"4k4KxWUcdgdc","trusted":true},"cell_type":"code","source":"def draw_plot(apartments):\n    cmap = cm.jet\n    m = cm.ScalarMappable(cmap=cmap)\n    quartiere_colors = m.to_rgba(apartments.neighbourhood_cleansed)\n    quant_minimum = apartments.weekly_price.quantile(0.1)\n    quant_maximum = apartments.weekly_price.quantile(0.9)\n    price = ((apartments.weekly_price - quant_minimum) / (quant_maximum - quant_minimum)) * 30\n\n    #initializing the figure size\n    plt.figure(figsize=(20,20))\n    #loading the png milan image found on open street map and saving to my local folder along with the project\n    i=urllib.request.urlopen('https://i.ibb.co/s1Jf5k7/map-2.png')\n    mil_img=plt.imread(i)\n    #scaling the image based on the latitude and longitude max and mins for proper output\n    plt.imshow(mil_img, zorder=0, extent=[\n                                          apartments.longitude.min(), \n                                          apartments.longitude.max(), \n                                          apartments.latitude.min(), \n                                          apartments.latitude.max(),\n                                        ]\n              )\n\n    ax=plt.gca()\n    # plot the points\n    apartments.plot(kind='scatter', x='longitude', y='latitude', label='price', c=quartiere_colors, s=price, ax=ax, zorder=5, edgecolors='black')\n    patch = []\n    for a in range(1, 9):\n      patch.append(mpatches.Patch(color=m.to_rgba(a), label='Neighborhood ' + str(a)))\n\n    plt.legend(handles=patch)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qzqJY4C3dpql"},"cell_type":"markdown","source":"#### Distribution by price\nNow, let's visualize the apartments which costs more than 800â‚¬ per week\n","execution_count":null},{"metadata":{"id":"tLpCsxSTdzdj","outputId":"841bfdd4-8b14-4d15-89c8-8cf6a36a501f","trusted":true},"cell_type":"code","source":"draw_plot(apartments[apartments.weekly_price > 800])","execution_count":null,"outputs":[]},{"metadata":{"id":"PjGVdXH3d3q3"},"cell_type":"markdown","source":"As I was saying before, we can see clearly how the very expensive apartments (> 75 quartile) are all concentred in the center. In contrast, looking at inexpensive apartments","execution_count":null},{"metadata":{"id":"nlvMQttoeHKF","outputId":"72cad057-a801-45e9-d2e5-79f18f3dbf7f","trusted":true},"cell_type":"code","source":"draw_plot(apartments[apartments.weekly_price < 350])","execution_count":null,"outputs":[]},{"metadata":{"id":"ohv55a4keK0k"},"cell_type":"markdown","source":"We can see how none of them are in the center.","execution_count":null},{"metadata":{"id":"IZ_EjQR3ehiu"},"cell_type":"markdown","source":"#### Distribution by number of reviews\nIt may also be interesting to see which apartments are visited the most. Let's take a look using the number of reviews","execution_count":null},{"metadata":{"id":"Yr6BVHXOfDFd","outputId":"8d51e593-0590-4166-e2e0-ffaa0406d28e","trusted":true},"cell_type":"code","source":"apartments.number_of_reviews.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"QKtaH-NyfFJg","outputId":"59e76723-6cf7-4988-cb65-01b6e89c20f4","trusted":true},"cell_type":"code","source":"draw_plot(apartments[apartments.number_of_reviews > 44])","execution_count":null,"outputs":[]},{"metadata":{"id":"KQ0kvsmtfLCI","outputId":"a78c9dd2-c38d-446c-91c4-05339c4d4b35","trusted":true},"cell_type":"code","source":"draw_plot(apartments[apartments.number_of_reviews <= 4])","execution_count":null,"outputs":[]},{"metadata":{"id":"v32S-xuofTTe"},"cell_type":"markdown","source":"I expected that the places at the center were visited more often, but that's not the case as it seems by our graphs. One thing to notice is that very expensive apartments have very few reviews.","execution_count":null},{"metadata":{"id":"1IbMZrdbj7Dv"},"cell_type":"markdown","source":"#### Distribution by number of bedrooms\nIt may also be intersting to take a look where bigger houses are located. Let's consider them","execution_count":null},{"metadata":{"id":"-9YKbcFfi3S7","outputId":"5cbeb676-ccf9-4911-c53c-8facb140a988","trusted":true},"cell_type":"code","source":"draw_plot(apartments[apartments.bedrooms > 2])","execution_count":null,"outputs":[]},{"metadata":{"id":"a34cFpKzi4sL","outputId":"1d4347a9-9ab8-4fdd-c5b2-35c937bfa24d","trusted":true},"cell_type":"code","source":"draw_plot(apartments[apartments.bedrooms == 1])","execution_count":null,"outputs":[]},{"metadata":{"id":"pAM1dT_tl-U3"},"cell_type":"markdown","source":"We can notice how most of the apartments have only one bedroom. Contrary to what I believed, big houses can be found in all the neighborhoods, and they are not that expensive.  ","execution_count":null},{"metadata":{"id":"K_Evn3sNm2VD"},"cell_type":"markdown","source":"### Visualizing correlations","execution_count":null},{"metadata":{"id":"APZT2tXRxUWI"},"cell_type":"markdown","source":"Let's now explore some possible data correlation to see what parameters may be interesting to predict price. Let's see the price correlation based on neighbourhood","execution_count":null},{"metadata":{"id":"81esqzD6yJiz","outputId":"523091c3-a6e8-4a8e-db7b-83686ec381c5","trusted":true},"cell_type":"code","source":"apartments.boxplot(by=\"neighbourhood_cleansed\", column=\"weekly_price\", figsize=(10,10), showmeans=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"lyuGbLr6zrJd"},"cell_type":"markdown","source":"We can see how the price distribution is very diversified. What we can notice is that on average the price in the center of Milan are slightly higher, which was expected. The thing that strikes the most about these prices are the number of outliers present. We will have to fix that before start predicting","execution_count":null},{"metadata":{"id":"mHTndLwX2kpI"},"cell_type":"markdown","source":"#### Matrix of correlations\nLet's now look at the Pearson correlation between two variables.","execution_count":null},{"metadata":{"id":"UWcGNAP1yzJt","outputId":"e34a06cf-95d6-4b9a-b254-c7bbf16b1ada","trusted":true},"cell_type":"code","source":"# taken from https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas\nf = plt.figure(figsize=(25, 20))\nplt.matshow(apartments.corr(method='pearson'), fignum=f.number)\nplt.xticks(range(apartments.shape[1]), apartments.columns, fontsize=14, rotation=90)\nplt.yticks(range(apartments.shape[1]), apartments.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","execution_count":null,"outputs":[]},{"metadata":{"id":"ybMtIQiS3cXv"},"cell_type":"markdown","source":"This matrix is super interesting. I want to highlight some things that are worth be mentioned.\n\nRegarding the weekly price:\n*   It sees that **the weekly price is strongly correlated with the number of  bed, bedrooms and how many people it can accomodate**, which I didn't notice in the map over Milan. It is however, expected\n*   It also seems that **there is a correlation between the number of guest and extra people that are inside the house**. Since this is a positive correlation, that wasn't quite expected, as usually having more guests in the house should lower the price of it\n*   There also seem to be a **negative correlation with the number of reviews**, meaning that the less they are the higher is the price. That does make sense since people don't like to spend money so they will prefer a cheaper place over a more expensive one.\n*   Finally, it seems that **the more listing a host has, the higher his price is.**\n\nRegarding other intersting correlation:\n*   The host response rate and the host response time are correlated. Makes sense\n*   If there are guests included, the number of bedrooms and bed is higher. This is intersting because the latter refers only to the beds available to the client, not the total\n*   All data regarding the reviews is strongly correlated between each other. It is indeed more likely that with a positive review in something, you are also likely to have a positive review for the rest\n*   What's more, if the host is superhost he will receive a higher number of reviews with a more positive rating. This indeed means that once you are a superhost it is more likely you'll get more positive reviews. \n*   In contrast to that, a higher number of listing means that an host will receive a lower number of reviews with a more negative value. This may be because with a lot of listing it's harder to pay enough attention to all apartments.\n","execution_count":null},{"metadata":{"id":"beaacrFPWABp"},"cell_type":"markdown","source":"## Feature Engineering","execution_count":null},{"metadata":{"id":"d7ev7RhcWB6I"},"cell_type":"markdown","source":"We want to express the latitude and longitude in a more meaningful way. Let's add a new column, `dist_from_center`, which represents the distance of every place from the center of milan","execution_count":null},{"metadata":{"id":"scHLPH2LWefS","trusted":true},"cell_type":"code","source":"from geopy.distance import great_circle\n\ndef distance_to_mid(lat, lon):\n    milan_centre = (45.464664, 9.188540)\n    accommodation = (lat, lon)\n    return great_circle(milan_centre, accommodation).km\n\napartments[\"dist_from_center\"] = apartments.apply(lambda x: distance_to_mid(x.latitude, x.longitude), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ke0vzr1pZUO9"},"cell_type":"markdown","source":"## Feature Selection\n\nBefore starting to predict the data, we want to remove the values that have very low variability and therefore they do not impact the prediction, only making it slow","execution_count":null},{"metadata":{"id":"uBlErBPxZcLn","outputId":"5476a6d1-378b-48f0-a669-0f34f8bd01a4","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\ndef remove_almost_constant_columns(threshold=0):\n    qconstant_filter = VarianceThreshold(threshold=threshold)\n    qconstant_filter.fit(apartments)\n    constant_columns = [column for column in apartments.columns\n                        if column not in apartments.columns[qconstant_filter.get_support()]]\n    print(constant_columns)\n    apartments.drop(labels=constant_columns, axis=1, inplace=True)\n\nremove_almost_constant_columns(threshold=0.03) # we remove data that is 97% of the time the same","execution_count":null,"outputs":[]},{"metadata":{"id":"K4VrtTdsAzOy"},"cell_type":"markdown","source":"## Price prediction","execution_count":null},{"metadata":{"id":"oCUPOqJrGmF0"},"cell_type":"markdown","source":"### Note before starting\nBefore starting to predict the data, we have seen how the weekly price presents a lot of outliers. For a regression model to work correctly, we ideally would like to have a [gaussian curve instead of a skewed model](https://towardsdatascience.com/skewed-data-a-problem-to-your-statistical-model-9a6b5bb74e37). Therefore, we must first transform our weekly price in something more resembling that.\n\nAs suggested by [a notebook about the new york data](https://www.kaggle.com/duygut/airbnb-nyc-price-prediction), we are going to try a log.\n\nFirst, let's take a look at the weekly price distribution now","execution_count":null},{"metadata":{"id":"G8QMvUqLHrfj","outputId":"3c8b6ae6-73bc-4b06-de8e-68c45be4e41a","trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom scipy.stats import norm\n\ndef plot_price_distribution(prices):\n  plt.figure(figsize=(10,10))\n  sns.distplot(prices, fit=norm)\n  plt.title(\"Price Distribution Plot\", size=15, weight='bold')\n\nplot_price_distribution(apartments.weekly_price)","execution_count":null,"outputs":[]},{"metadata":{"id":"uKJxO_8hIZKc"},"cell_type":"markdown","source":"As stated before, the distribution is highly skewed. Let's try to use a log","execution_count":null},{"metadata":{"id":"EmqNnb4AHYBu","outputId":"b475298e-cbfb-4164-d91b-202775324f58","trusted":true},"cell_type":"code","source":"log_weekly_price = np.log2(apartments.weekly_price)\nplot_price_distribution(log_weekly_price)","execution_count":null,"outputs":[]},{"metadata":{"id":"KzCM3DtnJGzG"},"cell_type":"markdown","source":"Much better, let's add it to the dataframe","execution_count":null},{"metadata":{"id":"dM16Yg01JLCZ","trusted":true},"cell_type":"code","source":"apartments[\"log_weekly_price\"] = log_weekly_price.values","execution_count":null,"outputs":[]},{"metadata":{"id":"w9EZ5p2hA2jJ"},"cell_type":"markdown","source":"Let's get started and try to make some predictions. Let's first divide the Dataset into X, y and divide between the train set and the test set.","execution_count":null},{"metadata":{"id":"RwTuA5EyA5fS","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = apartments.drop(columns=[\"log_weekly_price\", \"weekly_price\"])\ny = apartments.log_weekly_price\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"id":"KF7P_bo7BW4Y"},"cell_type":"markdown","source":"We will also define a 5-fold cross validation that we are going to use whenever possible","execution_count":null},{"metadata":{"id":"_bgmnzeEBM-H","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nkf = KFold(5, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"13ubVf7MBrKp"},"cell_type":"markdown","source":"Let's now create a function that will help us in training and asserting the score of our models","execution_count":null},{"metadata":{"id":"8h0pk2oqB0FR","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\n\ndef train_and_validate(model, X, y, cv):\n    cv_result = cross_validate(model, X, y, cv=kf, return_train_score=True)\n    return pd.DataFrame(cv_result)","execution_count":null,"outputs":[]},{"metadata":{"id":"fvj4EoKIx1-U"},"cell_type":"markdown","source":"Finally, let's also define a function for the confidence interval of our predictions","execution_count":null},{"metadata":{"id":"AyRbKai3zDil","trusted":true},"cell_type":"code","source":"from statsmodels.stats.proportion import proportion_confint\n\ndef confidence_interval(n_elements, R2_score, confidence):    \n    return proportion_confint(n_elements * R2_score, n_elements, 1-confidence/100, method='wilson')\n\ndef print_confidence_interval(n_elements, R2_score):\n    lower, upper = confidence_interval(n_elements, R2_score, 95)\n    print(f\"Interval of confidence: {lower:.3f}, {upper:.3f}\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vXHSrBINFPBf"},"cell_type":"markdown","source":"### A simple LinearRegression","execution_count":null},{"metadata":{"id":"lc_rr6PACIxI","outputId":"12f47829-b530-4af1-c190-56c494462f3f","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nresult = train_and_validate(LinearRegression(), X, y, kf)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"id":"0N_beNIpFKOt"},"cell_type":"markdown","source":"The results are promising, especially if we compare them with the one without using the log\n\n","execution_count":null},{"metadata":{"id":"IqbIDHobLlD5","outputId":"ae7246e2-a952-4413-d999-73896707cc51","trusted":true},"cell_type":"code","source":"print(train_and_validate(LinearRegression(), X, apartments.weekly_price, kf))","execution_count":null,"outputs":[]},{"metadata":{"id":"srbg-rWpzlG8"},"cell_type":"markdown","source":"### Normalization of data\n\nLet's now try to use a StandardScaler to see if we can perform better\n\n","execution_count":null},{"metadata":{"id":"s-NibQGG0B6b","outputId":"080fe2f9-f9f8-4324-b0e8-b237c9b36366","trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nmodel = Pipeline([\n    (\"scale\",  StandardScaler()),   # <- aggiunto\n    (\"linreg\", LinearRegression())\n])\nprint(train_and_validate(model, X, y, kf))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IW61enhQ0juz"},"cell_type":"markdown","source":"No difference at all","execution_count":null},{"metadata":{"id":"nEjxEqDi0nST"},"cell_type":"markdown","source":"### Regolarization\n\nLet's now try to swap the LinearRegression with an ElasticNet to see if the regularization L1 and L2 can improve our scores","execution_count":null},{"metadata":{"id":"az4xtZ3s1P09","outputId":"ac9da44c-c5c0-43b4-85d7-287e7b2b7366","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = Pipeline([\n    (\"scale\",  StandardScaler()),   # <- aggiunto\n    (\"regr\", ElasticNet())\n])\n\ngrid = {\n    \"regr__l1_ratio\": np.linspace(0, 1, 10),      # <- grado polinomio\n    \"regr__alpha\":  [0.1, 1, 10] # <- regolarizzazione\n}\ngs = GridSearchCV(model, grid, cv=kf)\ngs.fit(X_train, y_train);\n\ndisplay(pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_score\", ascending=False))\nprint_confidence_interval(len(X_test), gs.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"0Qm5Ser_2fv0"},"cell_type":"markdown","source":"but we don't get a strong improvement, even for `alpha = 0.1` and `l1_ratio = 0`. What we should do in this is try to use a non-linear regression","execution_count":null},{"metadata":{"id":"J9sy_DZI23GD"},"cell_type":"markdown","source":"### Non Linear Regression\n\n\n","execution_count":null},{"metadata":{"id":"KgDt9h4f_Lwp"},"cell_type":"markdown","source":"\nLet's now try to use a non linear regression to see if we can improve our score.","execution_count":null},{"metadata":{"id":"z8gtiIiq_gmI","outputId":"d8ff681a-2e0d-4f08-fb0b-d97cd5cf7fae","trusted":true},"cell_type":"code","source":"from sklearn.kernel_ridge import KernelRidge\n# best param alpha = 50, coef0=4, degree = 3\nmodel = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"regr\",  KernelRidge(alpha=20, kernel=\"poly\", degree=3, coef0=2))\n])\ngrid = {\n    \"regr__alpha\":  np.linspace(50, 200, 3), # <- regolarizzazione\n    \"regr__coef0\": [4,5,6,7,3],\n}\ngs = GridSearchCV(model, grid, cv=kf)\ngs.fit(X_train, y_train);\ndisplay(pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_score\", ascending=False))\nprint_confidence_interval(len(X_test), gs.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"WTyJQqIfeOxd"},"cell_type":"markdown","source":"With the KernelRidge we obtain better result getting around 49%. However, if we want to further improve accurancy, we need a new model","execution_count":null},{"metadata":{"id":"nDiCiz2TtBiT"},"cell_type":"markdown","source":"## Why our regressions are performing badly?\n\nThe main problem that makes our regression perform badly seems to be in the outliers. With the logarithm, we greatly improved accuracy, but maybe that's not enough. Let's create a new box plot, this time showing the log of the price","execution_count":null},{"metadata":{"id":"8dDEIERNtc_m","outputId":"65b33f8b-5a58-484d-e461-14795e8787dc","trusted":true},"cell_type":"code","source":"red_square = dict(markerfacecolor='r', markeredgecolor='r', marker='.')\napartments.log_weekly_price.plot(kind='box', xlim=(6, 15), vert=False, flierprops=red_square, figsize=(20,2));","execution_count":null,"outputs":[]},{"metadata":{"id":"cwUABWOsuCPk"},"cell_type":"markdown","source":"As we can see from this graph, a lot of the values are outliers, making our regression struggle. For this reason, let's try to excluse this outliers and see how our model performs","execution_count":null},{"metadata":{"id":"4R2o-ietb2Ho","outputId":"9c27dd72-a34f-482f-a047-4b9a47edbdfa","trusted":true},"cell_type":"code","source":"apartments.drop(apartments[(apartments.log_weekly_price > 13) | (apartments.log_weekly_price < 7) ].index, axis=0, inplace=True)\napartments.log_weekly_price.plot(kind='box', xlim=(7, 13), vert=False, flierprops=red_square, figsize=(20,2));","execution_count":null,"outputs":[]},{"metadata":{"id":"R0heLQlJcbj6","trusted":true},"cell_type":"code","source":"X = apartments.drop(columns=[\"log_weekly_price\", \"weekly_price\"])\ny = apartments.log_weekly_price\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"id":"IjkYVbEMcNkP"},"cell_type":"markdown","source":"Then, we can try use models that performs relatively good even with outliers","execution_count":null},{"metadata":{"id":"nphxEhL2yK8L"},"cell_type":"markdown","source":"### Random Forest Regressor","execution_count":null},{"metadata":{"id":"eQ9sfCab47JP"},"cell_type":"markdown","source":"A notorious model that performs well with outliers is the Random Forest regressor","execution_count":null},{"metadata":{"id":"lnQqWNiFvJMO","outputId":"0bd70f07-97c3-41b5-ef8f-ccbfb5177076","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = RandomForestRegressor()\n\ngrid = { \n            \"n_estimators\"      : [10,20,30,50,100],\n            \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n            \"min_samples_split\" : [2,4,8],\n            \"bootstrap\": [True, False],\n            \"max_depth\": [1,20,100, None],\n            \"min_samples_leaf\": [1, 5, 10]\n}\ngs = GridSearchCV(model, grid, cv=kf)\ngs.fit(X_train, y_train);\ndisplay(pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_score\", ascending=False))\nprint_confidence_interval(len(X_test), gs.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"VV2NQmlQ6IQ9"},"cell_type":"markdown","source":"As we can see, with a RandomForestRegressor we get to a 52% accuracy. Another really good model that performs well with outliers is XGBoost","execution_count":null},{"metadata":{"id":"-zFbMOvkFD3W"},"cell_type":"markdown","source":"### XGBoost","execution_count":null},{"metadata":{"id":"t52psAehDt5X"},"cell_type":"markdown","source":"To further improve our accuracy, we decided to use XGBoost which is the state of art regression algorithm for most of the situations. Firstly, we want to search for the best hyperparameter to XGBoost","execution_count":null},{"metadata":{"id":"-8Vq5Px9dvlR","outputId":"bb1c8c82-8187-4274-c6c2-7eb17c2ca570","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\n\nbooster = xgb.XGBRegressor()\n# create Grid\nparam_grid = {'n_estimators': [100, 150, 200],\n              'learning_rate': [0.01, 0.05, 0.1], \n              'max_depth': [3, 4, 5, 6, 7],\n              'colsample_bytree': [0.6, 0.7, 1],\n              'gamma': [0.0, 0.1, 0.2],\n              'alpha': [0.0, 0.5, 1, 2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"id":"05W_7UIJEUFp"},"cell_type":"markdown","source":"Then, we want to compare it to a standard LinearRegression","execution_count":null},{"metadata":{"id":"KFuK8eEXKz22","outputId":"11008940-7005-43cf-82f7-0d734b40ebfe","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)\n\ndata_dmatrix = xgb.DMatrix(data=X,label=y)\nbooster = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.6, learning_rate = 0.05,\n                max_depth = 6, gamma = 0, alpha=1, n_estimators = 300)\n\nbooster.fit(X_train,y_train)\n\npreds = booster.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\nprint(f\"R2 score: {r2_score(y_test, preds)}\")\nprint_confidence_interval(len(X_test), r2_score(y_test, preds))\n\nlin = LinearRegression()\nlin.fit(X_train, y_train)\npreds = lin.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))\nprint(f\"R2 score: {r2_score(y_test, preds)}\")\nprint_confidence_interval(len(X_test), r2_score(y_test, preds))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8c9tEO18Egwu"},"cell_type":"markdown","source":"We can see how we get a score almost 14% better than a normal linear regression. Let's now use cross-fold validation to see how our xgboost algorithm performs","execution_count":null},{"metadata":{"id":"f7BXSkrshFAt","trusted":true},"cell_type":"code","source":"xg_train = xgb.DMatrix(data=X_train, label=y_train)\nparams = {'colsample_bytree': 0.6, 'gamma': 0, 'alpha': 1,  'learning_rate': 0.05, 'max_depth': 6}\n\ncv_results = xgb.cv(dtrain=xg_train, params=params, nfold=4,\n                    num_boost_round=400, early_stopping_rounds=10, \n                    metrics=\"rmse\", as_pandas=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"UEgTpx32kl2x","outputId":"3124a3c7-fbb9-47e7-db5a-d32fc62e955f","trusted":true},"cell_type":"code","source":"cv_results.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"7ni4r0q4FcBt","outputId":"89d4cfb2-f503-4189-833f-f0814e500928","trusted":true},"cell_type":"code","source":"train_and_validate(booster, X, y, kf)","execution_count":null,"outputs":[]},{"metadata":{"id":"MskmYnSmEumV"},"cell_type":"markdown","source":"We can see how it reaches 60% accurancy for the test group and almost 80% for the training set. The rmse it obtains is at about 0.49. Thus, this means that on any predition we must consider an error of +-0.49. \n\nFor example, for an estimated log price of 9, which is 2**9 = 512\\$ per week, the price range of confidence is 364\\$ - 719\\$.","execution_count":null},{"metadata":{"id":"S28FmQPOH9v0"},"cell_type":"markdown","source":"\n### Performance with all outliers\nLet's now see how xgboost performs with all the outliers \n","execution_count":null},{"metadata":{"id":"WqS42LsGmroi","outputId":"61cc5ebc-3a8f-4f7b-f1d0-90295547a11b","trusted":true},"cell_type":"code","source":"# we put back all the outliers by re-executing code above\ntrain_and_validate(booster, X, y, kf)","execution_count":null,"outputs":[]},{"metadata":{"id":"an_kIiARLBTC"},"cell_type":"markdown","source":"We lose on average 2.5% accuracy. \n\n### Performance only with non-airbnb data\nLet's say, as a new airbnb host, we want to predict the price at which we should sell our house. Therefore, we can't use the current prediction for the given house. Let's then remove all this data and see how our algorithm performs","execution_count":null},{"metadata":{"id":"N98EfiPzFRZW","outputId":"3daa42fc-7e37-4409-c698-a6feb3d9e2a5","trusted":true},"cell_type":"code","source":"X_no_airbnb_data = X.drop(columns=[\n                                   \"number_of_reviews\", \n                                   \"review_scores_rating\", \n                                   \"review_scores_accuracy\", \n                                   \"review_scores_cleanliness\", \n                                   \"review_scores_checkin\", \n                                   \"review_scores_communication\", \n                                   \"review_scores_location\", \n                                   \"review_scores_value\",\n                                  ])\ntrain_and_validate(booster, X_no_airbnb_data, y, kf)","execution_count":null,"outputs":[]},{"metadata":{"id":"RZI1q1fANSBr"},"cell_type":"markdown","source":"In this case, on average we lose a 3% in accuracy. \n\n\n","execution_count":null},{"metadata":{"id":"MyUfjTrfOw_C"},"cell_type":"markdown","source":"## Comparing our best model against a random \n\nLet's see how our XGBoost model performs against a random model generated from the log_price_distribution","execution_count":null},{"metadata":{"id":"H78FeaWdMyjc","outputId":"0530d6f6-c471-4656-ad16-f03afdf33990","trusted":true},"cell_type":"code","source":"y_train.plot.hist(bins=40, figsize=(12, 4));","execution_count":null,"outputs":[]},{"metadata":{"id":"_6hBgKoyO_RM","outputId":"280558eb-9c49-4d47-dac0-d63c316d3acc","trusted":true},"cell_type":"code","source":"np.random.seed(42)\nrandom_preds = np.random.normal(\n    y_train.mean(),   # centro (media)\n    y_train.std(),    # scala (dev. standard)\n    len(y)        # numero di campioni\n)\nplt.figure(figsize=(12, 4))\nplt.hist(random_preds, bins=40);","execution_count":null,"outputs":[]},{"metadata":{"id":"MAFCbXAKPFIj","outputId":"d6604d0a-ed9a-4207-d7ab-252d60294901","trusted":true},"cell_type":"code","source":"scores = []\nfor i in range(1, 1000):\n  np.random.seed(i)\n  random_preds = np.random.normal(\n      y_train.mean(),   # centro (media)\n      y_train.std(),    # scala (dev. standard)\n      len(y)        # numero di campioni\n  )\n  scores.append(r2_score(y, random_preds))\n\nnp.mean(scores)","execution_count":null,"outputs":[]},{"metadata":{"id":"3m3NbqtDPt_1"},"cell_type":"markdown","source":"So, a random model is definetely worst than our XGBoost algorithm ","execution_count":null},{"metadata":{"id":"lmcARpXpE8t_"},"cell_type":"markdown","source":"## Conclusions\n\n\n### Visualizing the most important features\nLet's visualize the features that are the most important for our XGBoost model using [Shap](https://github.com/slundberg/shap)\n\n","execution_count":null},{"metadata":{"id":"rZPxtp5XE-FJ","outputId":"93a33b3c-2ada-4056-bb3f-ea09c955f700","trusted":true},"cell_type":"code","source":"import xgboost\nimport shap\n# download shap with pip3 install https://github.com/slundberg/shap/archive/master.zip\n# load JS visualization code to notebook\nshap.initjs()\n\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\nexplainer = shap.TreeExplainer(booster)\nshap_values = explainer.shap_values(X)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{"id":"x_QP1yoYgzyi"},"cell_type":"markdown","source":"The above explanation is about the first prediction and shows features each contributing to push the model output from the base value, `9.299`, to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue","execution_count":null},{"metadata":{"id":"JgNlkP1SlNHA"},"cell_type":"markdown","source":"Let's now take a look more in general about the most important features.\n\nThe plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). \n\nFor example, an high value on `dist_from_center` reduce the prediction by 0.50 from the base value, whereas and high value on the number of extra people an apartment can accomade boost the value by more than 1.","execution_count":null},{"metadata":{"id":"bkavlHgygg1_","outputId":"d0dbe61f-180c-4346-bc3e-d68e0031c892","trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{"id":"52-vQAi0PUlF"},"cell_type":"markdown","source":"\n\nTaking a look at all the work done, we can make quite a few observations:\n\n\n*   Our accuracy is not enough to provide a good estimation for a weekly price, but it's good enough to give a range at which you should rent your house \n*   To improve our accurancy, we need more data. When a person makes a judgment about an Airbnb apartment, he also takes into account the pictures the host has published, the text of the review, the description that the hosts gives and also the size. I believe that if we had all this extra data with some text mining we could get to an 80-90% accuracy.\n*   The features that are the most important in estimating the price are the one which we expected. The only thing out of place is that only the `number_of_reviews` is inversly correlated with the price. While I expected the contrary (the more I have reviews on Airbnb the more I can set an higher price), probably what happens is that houses which have a price that is set too high are not booked, thus having a lower number of reviews. \n*   All the features such as Kitchen, Heating, Washer and Wi-Fi, which we expected to be important, turned out not to be that intersting because almost all of the hosts have them. \n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}