{"cells":[{"metadata":{"id":"iv9TAzYbuIQo"},"cell_type":"markdown","source":"Till now we have looked at 6 feature encoding techniques.\n\n* Label Encoding\n* One Hot Encoding\n* Binary Encoding\n* Mapping\n* pd.factorize\n\nIn this notebook we will look at 2 new encoding techniques.\n\n* Frequency Encoding\n* Mean Encoding","execution_count":null},{"metadata":{"id":"UdF1BWhDuBee","trusted":true},"cell_type":"code","source":"import pandas as pd #import pandas\nimport numpy as np #import numpy\nfrom sklearn.preprocessing import LabelEncoder  #importing LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"id":"fgMhUVLbu614","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/bigmart-sales-data/Train.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"eKaLCeNOvGSt","outputId":"1cf3162c-f6a6-4107-cf82-464d0970efbc","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Av9KbBsLvZs2","outputId":"76a4cd7d-5f78-41bd-9abe-8a3288bd7909","trusted":true},"cell_type":"code","source":"#check the size of the dataset\nprint('Data has {} Number of rows'.format(train.shape[0]))\nprint('Data has {} Number of columns'.format(train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"vVfFG5kXviWf","trusted":true},"cell_type":"code","source":"#let's keep our categorical variables in one table\ncat_data = train[['Item_Identifier','Item_Fat_Content','Item_Type','Outlet_Identifier','Outlet_Size','Outlet_Location_Type','Outlet_Type','Item_Outlet_Sales']]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"g9WaLjCRvlnO","outputId":"cf3eeb45-b5bb-4222-d618-eb06612d55be","trusted":true},"cell_type":"code","source":"cat_data.head()   #check the head of categorical data","execution_count":null,"outputs":[]},{"metadata":{"id":"R5XK4RIKvtdE","outputId":"4515758a-83c3-4429-a889-ff82059df84d","trusted":true},"cell_type":"code","source":"#Let's start where we had left \nprint(cat_data['Item_Type'].nunique())\nprint(cat_data['Item_Type'].unique())","execution_count":null,"outputs":[]},{"metadata":{"id":"6QOPc-htv77x"},"cell_type":"markdown","source":"**Frequency Encoding**\n\nIt is a way to utilize the frequency of labels.","execution_count":null},{"metadata":{"id":"y27lvBAfwEMa","outputId":"d97c62a0-ad96-4252-86fa-51a6170924bd","trusted":true},"cell_type":"code","source":"fe = cat_data['Item_Type'].value_counts(ascending=True)/len(cat_data)  #count the frequency of labels\nprint(fe)","execution_count":null,"outputs":[]},{"metadata":{"id":"neIzK-T8wH7U","outputId":"7d9c19d2-3f44-4d27-8461-98bc3439621e","trusted":true},"cell_type":"code","source":"cat_data['Item_Type'].map(fe).head(10)  #map frequency to item type","execution_count":null,"outputs":[]},{"metadata":{"id":"fc7A9uyavw1z"},"cell_type":"markdown","source":"\nThis technique is useful when the frequency is somewhat related with the target variable.","execution_count":null},{"metadata":{"id":"weXSTFA-wT4a"},"cell_type":"markdown","source":"**Mean Encoding**\n\nIt is the most followed approach by the kagglers. We will not go into it's technality here. We will just look at it use and it's drwaback.\n\nWe go through following steps for mean encoding\n\n1. Group by categorical variable and obtain aggregated sum over target\n\n2. Group by categorical variable and obtain aggregated count over target\n\n3. divide step 2 / step 1\n","execution_count":null},{"metadata":{"id":"OgoXYaK3xEIG","outputId":"a55e96d1-6235-47bf-fadb-b3490c768fb0","trusted":true},"cell_type":"code","source":"#get the mean of target variable label wise\nme = cat_data.groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean()\nprint(me)","execution_count":null,"outputs":[]},{"metadata":{"id":"CmYx2g1rxHly","outputId":"2fc2b014-433b-47d5-e6d0-b9f64c244a33","trusted":true},"cell_type":"code","source":"#get the mean of target variable label wise\ncat_data['Outlet_Identifier'].map(me).head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"4OoFA-sWxM7J"},"cell_type":"markdown","source":"\nHere we have mapped different labels with the mean of the target variable.\n\nWhen we have large number of features mean encoding is a way to go about encoding. As it doesnot creates any new feature. It also correlates with the target feature.\n\nThe disadvantage of mean encoding is that it is prone to overfitting.","execution_count":null},{"metadata":{"id":"5dHnQr-BxPb1","outputId":"b7e34402-05d9-492c-d3ea-774c88e74389","trusted":true},"cell_type":"code","source":"#check value counts in Outlet_Size\ncat_data['Outlet_Size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"wgn0VkNqxTRb"},"cell_type":"markdown","source":"It is a ordinal variable we will make a dictionary as assign\n\n* Small-----> 0\n* Medium -----> 1\n* High -----> 2","execution_count":null},{"metadata":{"id":"GX3tAA56xVvj","outputId":"58a87efd-5d9e-4068-c6ae-7e6cb9ae7881","trusted":true},"cell_type":"code","source":"#Check the null values\ncat_data['Outlet_Size'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"AKyzjm6GwSAY","outputId":"7e8350d7-af5a-4f6f-c227-b605da2ce43e","trusted":true},"cell_type":"code","source":"#fill the null values with other category for now\ncat_data['Outlet_Size'].fillna('Others',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"1Y8zlD-mxijL","outputId":"e8db130b-458d-4c34-d40d-0f9f7ceba8ee","trusted":true},"cell_type":"code","source":"#prepare a dictionary to map\nsize_fe = {\"Small\" : 0, \"Medium\" : 1, \"High\" : 2, \"Others\" : 3}\ncat_data['Outlet_Size'].map(size_fe).head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"FvdI-FXpxonP","outputId":"264962b9-9f9f-4b9c-8040-4d36c4bd1898","trusted":true},"cell_type":"code","source":"cat_data['Outlet_Location_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"3xnC8S7UyAOQ"},"cell_type":"markdown","source":"Here Tier 1, Teir 2 and Teir 3 are ordinal variables. We can use Label Encoding or map the values.\n\n* Tier 3-----> 0\n* Tier 2 -----> 1\n* Tier 1-----> 2","execution_count":null},{"metadata":{"id":"Pvlg9tTeyFOh","outputId":"eb6ca802-f235-4f66-d2b8-ed7da6af724d","trusted":true},"cell_type":"code","source":"location_fe = {\"Tier 3\" : 1, \"Tier 2\" : 2, \"Tier 1\" : 3}\ncat_data['Outlet_Location_Type'].map(location_fe).head(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"ttVQpk1XyLfC","outputId":"e4387ade-c2b6-4f13-dfb6-5606711de631","trusted":true},"cell_type":"code","source":"#Check last variable and do the encoding\ncat_data['Outlet_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"TVkWxkfwygM_"},"cell_type":"markdown","source":"\nThe labels here are nominal. It will be better to use nominal encoding. We have only 4 labels we can try one hot encoding or binary encoding as well.","execution_count":null},{"metadata":{"id":"pzFQe3n2yVNS","outputId":"bd3595bb-4bef-48e0-b175-1de49ea2f2e0","trusted":true},"cell_type":"code","source":"pd.get_dummies(cat_data['Outlet_Type'],drop_first=True).head()","execution_count":null,"outputs":[]},{"metadata":{"id":"PZBwivHQyYvF"},"cell_type":"markdown","source":"\nNext we will use all the encoding techniques we have learnt till now on different datasets. So that you will have some practice and will have better understanding when to use which encoding.\n\nLink to final kernel : https://www.kaggle.com/krishnaheroor/final-kernel","execution_count":null},{"metadata":{"id":"146xZtE3yZqw","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}