{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center> Breast Cancer Prediction using XGBoost </center>\n## <center> Authored by: Pratham Tripathi </center>\n\n# Contents:\n\n## <u>1.Aim:</u>\nTo predict whether the patient has malignant(1) or Benign(0) cells.\n## <u>2.Approach:</u> \nThe Approach was to build a classifier that could efficient predict the same.\n## <u>3.Model:</u> \nThe model used here is XGBoost Classifier.\n## <u>4.About the model:</u> \nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\nThus, with proper tuning, the model could easily identify the significant columns on its own and build effective and generalized model easily.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Major Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading The CSV file","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\ndf = df.drop(['Unnamed: 32'], axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"diagnosis\"].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding \"Diagnosis\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nle.fit([\"M\",\"B\"])\ndf[\"diagnosis\"] = le.transform(df[\"diagnosis\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature and Target Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[df.columns[df.columns!=\"diagnosis\"]]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"diagnosis\"]\ny.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating DMatrix for XGBClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dmatrix = xgb.DMatrix(data = X,label = y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stratified Spliting of Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state = 123, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Classifier Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_css = xgb.XGBClassifier(n_estimators = 100,objective = \"reg:logistic\",colsample_bytree = 0.3,learning_rate = 0.1, max_depth = 5,alpha =10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_css.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predciting Outcomes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = xgb_css.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport itertools\ndef plot_confusion_matrix(cm,classes,\n                         normalize = False,\n                         title='Confusion Matrix',\n                         cmap = plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float')/cm.sum(axis = 1)[:,np.newaxis]\n        print(\"After Normalization\")\n    else:\n        print(\"Without Normalization\")\n    print(cm)\n    plt.imshow(cm,interpolation='nearest',cmap = 'Wistia')\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks,classes,rotation = True,color='white')\n    plt.yticks(tick_marks,classes,rotation =True,color='white')\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max()/2\n    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,format(cm[i,j],fmt),\n                horizontalalignment = \"center\",\n                color = 'white' if cm[i,j]>thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.xlabel(\"Predicted\",color='white',size=20)\n    plt.ylabel(\"True\",color='white',size=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix=confusion_matrix(y_test,pred,labels=[0,1])\nnp.set_printoptions(precision = 2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix,classes=['benign(0)','malignant(1)'],normalize=False,title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Report","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# F1 Score of the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1_score(y_test,pred,average='weighted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-fold Cross Validation using xgb.cv()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"objective\":\"reg:logistic\",\"colsample_bytree\":\"0.3\",\"learning_rate\": \"0.1\",\"max_depth\":\"5\",\"alpha\":\"10\"}\ncv_results = xgb.cv(dtrain = dmatrix, params = params, nfold = 3,early_stopping_rounds =10,metrics=\"error\", as_pandas = True, seed = 123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Last Validation Score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print((cv_results[\"test-error-mean\"]).tail(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About Test-error-Mean and Accuracy\nThe model has very less loss (O.06) and a high F1 Score (0.96)\nHence, the model is highly efficient in this case.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}