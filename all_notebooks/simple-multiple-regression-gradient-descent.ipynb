{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nRegression is one of the most important machine learning techniques. It is mainly used for identifying the relationship between two or more variables and for predicting a continuous variable. \n\nA particular dataset contains a response variable and a set of explanatory variables, where the explanatory variables are said to be the features of the response variable. Regression modeling is used to estimate the relationship between the variables and predict the value of response variable by finding a line of best fit that minimizes the Residual Sum of Squares (RSS) i.e. the difference between the actual value and predicted value. \n\nIn this analysis, we will first build a simple and a multiple regression model using python library scikit learn and then use the Gradient Descent Algorithm, an optimization technique, to find the minimum of the function RSS. \n\nWe must always remember that our results are based on the quantity and quality of the available dataset. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importing libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.linear_model import LinearRegression\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load Dataset\ndf_house = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the dataset\ndf_house.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Rows and Columns\ndf_house.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Datatypes\ndf_house.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping Variable ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are some varibales such as ID and date which are irrelevant in predicting the value of the house. Therefore,before proceeding further we will drop these variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_house = df_house.drop(['id','date'],axis =1)\ndf_house.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before moving further with analysis it is important to check if there are any missing values in the dataset. As we can see from the code below, there are no missing values in the dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_house.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To get an overview of the distribution of the response or dependent variable \"price\", we will plot a normal distribution curve which will tell us the skewness, and spread of the data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"min_ = min(df_house['price'])\nmax_ = max(df_house['price'])\nx = np.linspace(min_,max_,100)\nmean = np.mean(df_house['price'])\nstd = np.std(df_house['price'])\n\n# For Histogram\nplt.hist(df_house['price'], bins=20, density=True, alpha=0.3, color='b')\ny = norm.pdf(x,mean,std)\n\n# For normal curve\nplt.plot(x,y, color='red')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the above graph, that the variable 'price' is skewed towards the right. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_house['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplot","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The boxplot provides a visual summary of the following items in a data:\n1. Inter Quartile Rande (IQR)\n2. Median\n3. Whiskers\n4. Outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(df_house['price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The boxplot shows that there are some suspected outliers in the dataset. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The correlation coefficient shows the relationship or association between the dependent variable and independent variable. The value of the coefficient is between -1 and 1. \n\n**Positive Correlation** - Positive Correlation lies between 0 and 1, where a value close to 1 represents a strong positive correlation and a value close to 0 represents a weak positive correlation. \n\n**Negative Correlation** - Negative Correlation lies between -1 and 0, where a value close to -1 represents a strong negative correlation and a value close to 0 represents a weak negative correlation. \n\nHowever, we must always remember that correlation does not imply causation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix = df_house.corr()\nprint(correlation_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(correlation_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the value of correlation coefficients from above, we can see identify variables that have a strong (+ve or -ve) relationship with the variable 'price'. These are 'sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms', 'sqft_basement'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Multivariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Price & Sqft Living\ndf_house.plot(x='sqft_living',y='price',style = 'o')\nplt.title('Sqft_Living Vs Price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_house.boxplot(column = ['price'],by='bedrooms')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_house.plot(x='lat',y='price',style = 'o')\nplt.title('lat Vs Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Regression Model\nIn a simple regression model, only a single explanatory variable is used to predict the value of response variable. In this case, the response variable is 'price' while explanatory variable would be 'sqft_living' as this variable had the highest positive correlation with price.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Training and Test Dataset\nThe dataset will be split into a training and test dataset. The training dataset will be used to fit the model i.e. it will find the estimated parameters of the model while the test dataset will be used for predicting the value of price. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_house, test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attributes and Labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_simple = train['sqft_living'].values.reshape(-1,1)\nX_test_simple = test['sqft_living'].values.reshape(-1,1)\n\ny_train_simple = train['price'].values.reshape(-1,1)\ny_test_simple = test['price'].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting A Simple Regression Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_s = LinearRegression()\nmodel_s.fit(X_train_simple,y_train_simple)\n\nprint('Intercept: ', model_s.intercept_)\n\nprint('Sqft_living Coefficient: ', model_s.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions - Simple Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making Predictions\npred_simple = model_s.predict(X_test_simple)\npred_simple","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual Sum of Squares","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# RSS\nRSS_simple = np.sum((y_test_simple - pred_simple)**2)\nprint(\"RSS_simple: \", RSS_simple)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting line of Best Fit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(test['sqft_living'],test['price'],'.',\n        test['sqft_living'], pred_simple,'-')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.residplot('sqft_living','price', data = test, color = 'red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Covariance\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cov = pd.DataFrame.cov(df_house[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms', \n                                 'sqft_basement','waterfront','floors']])\nprint(cov)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cov,fmt='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above covariance matrix, we can see that the variables ('sqft_living','sqft_above', 'sqft_living15') have a strong relationship with one another.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Multiple Regression Model\n\nWe will build 3 multiple regression models by adding and removing different variables to see their affect on RSS. This will be done by using the built-in library in python i.e. scikit learn.\n\nThe training and testing datasets defined above will be used in this model.\n\n### Model 1\nIn this model, we will take all the variables ('sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms') which were highly corelated to the variable 'price' in our model. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating Attributes and Labels\nX_train = train[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms']].values\nX_test = test[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'bedrooms']].values\ny_train = train['price'].values.reshape(-1,1)\ny_test = test['price'].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Regression Model\nmodel1 = LinearRegression()\nmodel1.fit(X_train,y_train)\n\nprint('Intercept: ', model1.intercept_)\n\nprint('Coefficients: ', model1.coef_)\n\ndf1 = pd.DataFrame(model1.coef_, columns = ['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view',\n                                            'bedrooms'])\nprint(df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value of the coefficients, for e.g. sqft_living, represents the predicted change in the value of price per unit change in the value of square feet.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the predictions od model 1\npred1 = model1.predict(X_test)\npred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RSS of model 1\nRSS_1 = np.sum((y_test - pred1)**2)\nprint(\"RSS_1: \", RSS_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Model 2\nIn model 2 we will add the variables  ('sqft_basement', 'lat') as they are correlated with \"price\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating Attributes and Labels\nX_train = train[['sqft_living', 'grade', 'bathrooms', 'view', 'bedrooms', 'sqft_above', 'sqft_living15','sqft_basement'\n                 ,'lat']].values\nX_test = test[['sqft_living', 'grade', 'bathrooms', 'view', 'bedrooms','sqft_above', 'sqft_living15','sqft_basement'\n              ,'lat']].values\n\ny_train = train['price'].values.reshape(-1,1)\ny_test = test['price'].values.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a Regression Model 2\nmodel2 = LinearRegression()\nmodel2.fit(X_train,y_train)\n\nprint('Intercept: ', model2.intercept_)\n\nprint('Coefficients: ', model2.coef_)\n\ndf2 = pd.DataFrame(model2.coef_, columns = ['sqft_living', 'grade', 'bathrooms', 'view', 'bedrooms', \n                                           'sqft_above', 'sqft_living15','sqft_basement','lat'])\nprint(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making Predictions of Model 2\npred2 = model2.predict(X_test)\npred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RSS of model 2\nRSS_2 = np.sum((y_test - pred2)**2)\nprint(\"RSS_2: \", np.sum((y_test - pred2)**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model 3\nIn this model, we will remove the variables ('sqft_above', 'sqft_living15','sqft_basement') as they are strongly related to the variable sqft_living.\n\n\n\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating Attributes and Labels\nX_train = train[['sqft_living', 'grade', 'bathrooms', 'bedrooms','view','lat']].values\nX_test = test[['sqft_living', 'grade', 'bathrooms', 'bedrooms','view','lat']].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fiting a Regression Model\nmodel3 = LinearRegression()\nmodel3.fit(X_train,y_train)\n\nprint('Intercept: ', model3.intercept_)\n\nprint('Coefficients: ', model3.coef_)\n\ndf3 = pd.DataFrame(model3.coef_, columns = ['sqft_living', 'grade', 'bathrooms', 'bedrooms', 'view','lat'])\nprint(df3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the value\npred3 = model3.predict(X_test)\npred3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RSS\nRSS_3 = np.sum((y_test - pred3)**2)\nprint(\"RSS_3: \", np.sum((y_test - pred3)**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, adding or removing variables may result in the Residual Sum of Squares to rise.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RSS = pd.DataFrame(np.array([[RSS_1,RSS_2,RSS_3]]),columns = ['RSS_1','RSS_2','RSS_3'])\nprint(RSS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Gradient Descent\n\nIn a simple regression model, we try to estimate the value of the response variable (house price) from a single explanatory varibale which in this case is \"sqft_living\" by fitting a line that best fits our model. \n\nBut how do we measure the quality or performance of our model? \n\nThis is done by defining a cost function such as Residual Sum of Squares (RSS) in terms of our estimated parameters (w0, w1) where w0 is the intercept and w1 is the coefficeint of 'sqft_living'. Therefore, our main goal in fitting a model is to minimize cost function, RSS(w0,w1),  over all possible values of (w0, w1) i.e. search over the space of all possible lines and find the line that minimize the RSS.\n\nGradient Descent is an optimization technique in Machine Learning that allow us to find specific values of w0 and w1 which minimizes our RSS. \n\nTo understand this in more detail, we can try to find the minimum and maximum of the cost function analyticaly. For example, in order to find the minimum of a convex function, we will compute the derivative of that function and equate it to 0. However, this could become computationaly intensive if there are more than 1 variables. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A better approach would then be to use hill climbing or descent technique for finding the maximum or minimum respectively. Instead of equating the derivative to 0, we move along the curve from one point to another by updating the value or vector of the estimated parameter W (or weights of the given input features). This is done through an iterative process by defining a stepsize and convergence criteria. \n\nFirst, we assume an initial value of our estimated parameters (w,regression coefficients) and compute the derivative of the cost function (RSS) at that point. Then at each iteration the previous value of w is either increased or decreased by the amount based on the derivative as determined by the stepsize (Instead of the derivative, the value is increased or decresed by the amount of stepsize). \n\nIn case of min, if the value of the derivative is -ve, increase the value of w. If the value of the derivative is +ve, decrease the value of w.\n\nLaslty, how do we assess the convergence? The algorithm will not converge until the magnitude of the derivative is less than the tolerance level i.e. threshold 'e', which will be a very small number. The threshold will be set by us and the algorithm will not terminate until the condition is satisfied.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Simple Regression - Gradient Descent\nTo understand Gradient Descent, let's start with simple regression by taking a single input feature 'sqft_living' from the housing dataset.\n\nThe following function takes data, input features, output and returns a feature_matrix which will consist a first column of ones and then the value of input features in the order defined. It also returns an output 'price' of the dataset. \n\n\n### Input and Output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(data,features, output):\n    data['constant'] = 1\n    features = ['constant'] + features\n\n    features_new = data[features]\n    feature_matrix = np.asarray(features_new)\n    \n    output_data = data[output]\n    output_array = output_data.to_numpy()\n    return(feature_matrix,output_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions\n\nThen, we get the 1D array predictions by multiplying 2D rfeature_matrix with 1D regression weights which is a dot product between the two vectors. We will define the initial weights to be [-47000, 1]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(feature_matrix, weights):\n    predictions = np.dot(feature_matrix,weights)\n    return(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Derivative\n\nNow we take the derivative of the cost function. A derivative function is then defined which takes feature and error array and returns a number. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_derivative(errors,feature):\n    derivative = 2*(np.dot(errors,feature))\n    return(derivative)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Descent Algorithm\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n    converged = False\n    weights = np.array(initial_weights)\n    while not converged:\n        predictions = prediction(feature_matrix,weights)\n        \n        errors = predictions - output\n        \n        gradient_sum_squares = 0\n        for i in range(len(weights)):\n            derivative = feature_derivative(errors,feature_matrix[:,i])\n            \n            gradient_sum_squares = derivative**2 + gradient_sum_squares\n            \n            # subtract the step size times the derivative from the current weight\n            weights = weights - step_size*(derivative)\n            \n            gradient_magnitude = sqrt(gradient_sum_squares)\n            if gradient_magnitude < tolerance:\n                converged = True\n    return(weights)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Parameters\n\n1. features\n2. output\n3. errors\n4. tolerance\n5. stepsize","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(feature_matrix,output_array) = get_data(train, ['sqft_living'],'price')\ninitial_weights = np.array([-47000., 1.])\npredictions = prediction(feature_matrix, initial_weights)\nerrors = output_array - predictions\nstep_size = 7e-12\ntolerance = 2.5e7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = regression_gradient_descent(feature_matrix, output_array, initial_weights, step_size, tolerance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual Sum of Squares","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using test data to compute RSS\n(feature_matrix_test,output_array_test) = get_data(test, ['sqft_living'],'price')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_test = prediction(feature_matrix_test, weights)\nerrors = output_array_test - prediction_test\nRSS_simple_GD = np.sum((errors)**2)\nprint(\"RSS_simple_GD: \", np.sum((errors)**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"RSS using Simple Regression: \", RSS_simple)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can see that the RSS of Simple Regression using the optimization technique, Gradient Descent, is much lower than RSS of simple regression using a pyhton library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}