{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 3. Import libraries and modules\n!pip install git+https://github.com/qubvel/classification_models.git\nfrom classification_models.keras import Classifiers\nimport numpy as np\nnp.random.seed(123)  # for reproducibility\nimport pandas as pd\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten,Embedding\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\n!pip install adabelief-tf==0.1.0\nfrom adabelief_tf import AdaBeliefOptimizer\n!pip install keras-rectified-adam\nfrom keras_radam.training import RAdamOptimizer\n!pip install keras-adabound\nfrom keras_adabound import AdaBound\n!pip install git+https://github.com/tensorflow/addons\nimport tensorflow_addons\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import IPython.display as ipd\nimport librosa\nimport os\nimport librosa.display\n\ndf = pd.read_csv(\"../input/urbansound8k/UrbanSound8K.csv\")\ndf['class'].value_counts()\ndef features_extractor(file):\n    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n    \n    return mfccs_scaled_features\n\nextracted_features=[]\nfor i in range(8732):\n    file_name = '../input/urbansound8k/fold' + str(df[\"fold\"][i]) + '/' + df[\"slice_file_name\"][i]\n    final_class_labels=df[\"class\"][i]\n    data=features_extractor(file_name)\n    extracted_features.append([data,final_class_labels])\n    \nextracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\nX=np.array(extracted_features_df['feature'].tolist())\ny=np.array(extracted_features_df['class'].tolist())\n\ny=np.array(pd.get_dummies(y))\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=0,stratify=y)\nnum_labels=y.shape[1]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape(7422,1, 40)\nprint(X_train.shape)\nprint(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.reshape(7422,1, 10)\nprint(y_train.shape)\nprint(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.reshape(1310,1, 40)\nprint(X_test.shape)\nprint(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = y_test.reshape(1310,1,10)\nprint(y_test.shape)\nprint(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import LSTM,Input\n\nmodel = Sequential()\nmodel.add(LSTM(40,input_shape=(1,40), return_sequences=True))\nmodel.add(LSTM(40,input_shape=(1,40),return_sequences=True))\nmodel.add(Dense(512,activation ='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10,activation ='softmax'))\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = []\nok = 0\nyes = 0\nzet = 0\nyep = 0\n\n\nopt =  [keras.optimizers.Adam(learning_rate=0.001),\n                  keras.optimizers.SGD(learning_rate=0.01),\n                  keras.optimizers.Nadam(learning_rate=0.001),\n                  keras.optimizers.RMSprop(learning_rate=0.001),\n                  AdaBeliefOptimizer(learning_rate=0.001),\n                  RAdamOptimizer(learning_rate=0.001),\n                  AdaBound(learning_rate=0.001, final_lr=0.1),\n                  tensorflow_addons.optimizers.yogi.Yogi(learning_rate=0.001)]\n        \nfor t in opt:\n    del model\n    #del base_model\n    #del x\n    #del output\n    if (t == opt[0]):\n        print(\"-------------------------------------optimizer = Adam-------------------------------------\")\n    elif (t == opt[1]):\n            print(\"-------------------------------------optimizer = SGD-------------------------------------\")\n    elif (t == opt[2]):\n            print(\"-------------------------------------optimizer = Nadam-------------------------------------\")\n    elif (t == opt[3]):\n            print(\"-------------------------------------optimizer = RMSprop-------------------------------------\")\n    elif (t == opt[4]):\n            print(\"-------------------------------------optimizer = AdaBelief-------------------------------------\")\n    elif (t == opt[5]):\n            print(\"-------------------------------------optimizer = RAdam-------------------------------------\")\n\n    elif (t == opt[6]):\n            print(\"-------------------------------------optimizer = AdaBound-------------------------------------\")\n\n    elif (t == opt[7]):\n            print(\"-------------------------------------optimizer = Yogi-------------------------------------\")\n\n    model = Sequential()\n    model.add(LSTM(40,input_shape=(1,40), return_sequences=True))\n    model.add(LSTM(40,input_shape=(1,40),return_sequences=True))\n    model.add(Dense(512,activation ='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10,activation ='softmax'))\n    # 8. Compile model\n    model.compile(loss='categorical_crossentropy',\n              optimizer = t,\n              metrics=['accuracy'])\n\n\n       # 9. Fit model on training data\n    k.append(model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test),verbose=1))\n\n\n\nwhile (ok<len(k)):  \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[ok+0].history['accuracy'],color='b', label=\"opt = Adam\")\n    plt.plot(k[ok+1].history['accuracy'],color='r', label=\"opt = SGD\")\n    plt.plot(k[ok+2].history['accuracy'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[ok+3].history['accuracy'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[ok+4].history['accuracy'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[ok+5].history['accuracy'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[ok+6].history['accuracy'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[ok+7].history['accuracy'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('accuracy')\n    plt.legend()\n    plt.show() \n\n    ok +=8\n\n\n\n\n\n\n\nwhile (yes<len(k)):   \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[yes+0].history['loss'],color='b', label=\"opt = Adam\")\n    plt.plot(k[yes+1].history['loss'],color='r', label=\"opt = SGD\")\n    plt.plot(k[yes+2].history['loss'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[yes+3].history['loss'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[yes+4].history['loss'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[yes+5].history['loss'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[yes+6].history['loss'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[yes+7].history['loss'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show() \n\n    yes+=8\n\n\n\n\nwhile (zet<len(k)):   \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[zet+0].history['val_accuracy'],color='b', label=\"opt = Adam\")\n    plt.plot(k[zet+1].history['val_accuracy'],color='r', label=\"opt = SGD\")\n    plt.plot(k[zet+2].history['val_accuracy'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[zet+3].history['val_accuracy'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[zet+4].history['val_accuracy'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[zet+5].history['val_accuracy'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[zet+6].history['val_accuracy'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[zet+7].history['val_accuracy'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('val_accuracy')\n    plt.legend()\n    plt.show() \n\n    zet+=8\n\n\n\n\n\nwhile (yep<len(k)):   \n\n\n\n    plt.figure(figsize=(10,10))\n    plt.plot(k[yep+0].history['val_loss'],color='b', label=\"opt = Adam\")\n    plt.plot(k[yep+1].history['val_loss'],color='r', label=\"opt = SGD\")\n    plt.plot(k[yep+2].history['val_loss'],color='g', label=\"opt = Nadam\")\n    plt.plot(k[yep+3].history['val_loss'],color='c', label=\"opt = RMSprop\")\n    plt.plot(k[yep+4].history['val_loss'],color='m', label=\"opt = AdaBelief\")\n    plt.plot(k[yep+5].history['val_loss'],color='k', label=\"opt = RAdam\")\n    plt.plot(k[yep+6].history['val_loss'],color='y', label=\"opt = AdaBound\")\n    plt.plot(k[yep+7].history['val_loss'],color='#A18820', label=\"opt = Yogi\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('val_loss')\n    plt.legend()\n    plt.show() \n\n    yep+=8","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}