{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import ensemble\nfrom sklearn import svm\nimport time\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nimport math\nimport matplotlib.ticker as plticker\nimport matplotlib.patches as mpatches\nimport matplotlib.lines as mlines\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics \nfrom sklearn.ensemble import AdaBoostClassifier\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.decomposition import FactorAnalysis\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_outcome_var = \"Survived\"\nneg_outcome_var = \"Died\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_columns(df, columns_to_drop):\n    for col in columns_to_drop:\n        del df[col]  \n# Read in data\nbig_df = pd.read_csv(\"../input/titanic-cleaned-data/train_clean.csv\")\nbig_df.info()\ncolumns_to_drop = [\"Cabin\", \"Name\", \"Ticket\", \"Parch\", \"Embarked\", \"Title\", \"PassengerId\"]  # TODO include reasoning for dropping these\ndrop_columns(big_df, columns_to_drop)\nis_male = {\"male\": 1, \"female\": 0}\nbig_df[\"Sex\"].replace(is_male, inplace=True)\nall_y = big_df[pos_outcome_var]\nall_x  = big_df.drop(pos_outcome_var, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_k_means(x_data, start=1, end=50):\n    ssds = []\n    binary_labels = []\n    n_iter = []\n    binary_k_means = None\n    print(x_data)\n    for k in range(start, end + 1):\n        start = time.time()\n        k_means = KMeans(n_clusters=k, random_state=0).fit(x_data)\n        end = time.time()\n        n_iter.append(end - start)\n        ssds.append(k_means.inertia_)\n        #n_iter.append(k_means.n_iter_)\n        if k == 2:\n            binary_labels = k_means.labels_\n            binary_k_means = k_means\n    return (ssds, binary_labels, binary_k_means, n_iter)\n\ndef plot_cluster_accuracy(data, title=\"Titanic SSD over K for KMC\", print_default=True,\\\n                          x_label=\"Number of Clusters (k)\", y_label=\"Sum of Squared Distances\"):\n    plt.rcParams[\"figure.figsize\"] = (6,4)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.plot(range(1, len(data) + 1), data, '-o')\n    if print_default:\n        default_y = [data[0] / float(i) for i in range(1,len(data) + 1)]\n        plt.plot(range(1, len(data) + 1), default_y, color=\"orange\")\n    plt.show()\n\ndef k_means_helper(x_reduced, big_df, outcome_var=\"Survived\"):\n    (ssds, binary_labels, binary_k_means, n_iter) = \\\n    compute_k_means(x_reduced)\n    reduced_with_labels = pd.DataFrame(data=x_reduced, columns=[\"component1\", \"component2\"])\n    reduced_with_labels.insert(2,outcome_var, big_df[outcome_var], True)\n    cluster_counts, cluster_col, cluster_samples = match_survived_to_clusters(reduced_with_labels, binary_k_means)\n    reduced_with_labels.insert(2,\"ClusterId\", cluster_col, True)\n    return {\n        \"reduced_with_labels\": reduced_with_labels,\n        \"cluster_counts\": cluster_counts,\n        \"log_likelihoods\": log_likelihoods, \n        \"binary_weights\": binary_weights,\n        \"ssds\": ssds,\n        \"n_iter\": n_iter,\n        \"binary_k_means\": binary_k_means,\n        \"cluster_samples\": cluster_samples\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_EM(x_data, start=1, end=50):\n    binary_weights = []\n    log_likelihoods = []\n    n_iter = []\n    binary_EM = None\n    for k in range(start, end + 1):\n        start = time.time()\n        EM = GaussianMixture(n_components=k, random_state=22).fit(x_data)\n        end = time.time()\n        n_iter.append(end - start)\n        log_likelihoods.append(EM.lower_bound_)\n        #TODO iterations\n        \n        if k == 2:\n            binary_EM = EM\n            binary_weights = EM.weights_\n            \n    return (log_likelihoods, binary_weights, n_iter, binary_EM)\n\ndef get_cluster_samples(cluster_rows):\n    cluster_dfs = [\n        big_df[big_df.index.isin(cluster_rows[0])], \n        big_df[big_df.index.isin(cluster_rows[1])]\n    ]\n    print(\"cluster_0\")\n    cluster_dfs[0].sample(n=30, random_state=1).describe()\n    print(\"cluster_0\")\n    cluster_dfs[1].sample(n=30, random_state=1).describe()\n    return cluster_dfs\n    \ndef match_survived_to_clusters(all_data, EM):\n    cluster_counts = [{pos_outcome_var: 0, neg_outcome_var: 0},\n    {pos_outcome_var: 0, neg_outcome_var: 0}]\n    #cluster_id = pd.DataFrame(columns=['Cluster_ID'])\n    cluster_col = np.zeros(len(all_data))\n    cluster_rows = [set(), set()]\n    for i,row in all_data.iterrows():\n        cluster_idx = EM.predict(row.drop(pos_outcome_var).values.reshape(1, -1))[0]\n        key_to_increment = pos_outcome_var if all_data.iloc[i,:][pos_outcome_var] else neg_outcome_var\n        cluster_col[i] = cluster_idx\n        cluster_counts[cluster_idx][key_to_increment] += 1\n        cluster_rows[cluster_idx].add(i)\n    print(\"len cluster rows 0: \", len(cluster_rows[0]))\n    print(\"len cluster rows 1: \", len(cluster_rows[1]))\n\n    cluster_analysis = [\n        big_df[big_df.index.isin(cluster_rows[0])], \n        big_df[big_df.index.isin(cluster_rows[1])]\n    ]\n        \n    return (cluster_counts, cluster_col, cluster_analysis)\n\ndef EM_helper(x_reduced, big_df, outcome_var=\"Survived\"):\n    (log_likelihoods, binary_weights, n_iter, binary_EM) = \\\n    compute_EM(x_reduced)\n    reduced_with_labels = pd.DataFrame(data=x_reduced, columns=[\"component1\", \"component2\"])\n    reduced_with_labels.insert(2,outcome_var, big_df[outcome_var], True)\n    cluster_counts, cluster_col, cluster_samples = match_survived_to_clusters(reduced_with_labels, binary_EM)\n    reduced_with_labels.insert(2,\"ClusterId\", cluster_col, True)\n    return {\n        \"reduced_with_labels\": reduced_with_labels,\n        \"cluster_counts\": cluster_counts,\n        \"log_likelihoods\": log_likelihoods, \n        \"binary_weights\": binary_weights, \n        \"n_iter\": n_iter,\n        \"binary_EM\": binary_EM,\n        \"cluster_samples\": cluster_samples\n    }\n    \n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(ssds, binary_labels, binary_k_means, n_iter) = compute_k_means(all_x)\n(cluster_counts_k_means, cluster_col_k_means, cluster_analysis_k_means) = match_survived_to_clusters(big_df, binary_k_means)\n#print(cluster_counts)\nplot_cluster_accuracy(n_iter, title=\"Titanic KMC Time Training\", print_default=False, y_label= \"Seconds\")\n\nplot_cluster_accuracy(ssds, title=\"Titanic KMC Sum of Squared Distances\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_analysis_k_means[0].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_analysis_k_means[1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(log_likelihoods, binary_weights, n_iter, binary_EM) = compute_EM(all_x)\nprint(\"log_likelihoods EM: \", log_likelihoods)\nprint(\"binary_weights EM: \", binary_weights)\nplot_cluster_accuracy(log_likelihoods, title=\"Titanic EM Log Likelihoods\", print_default=False, y_label= \"log_likelihoods\")\nplot_cluster_accuracy(n_iter, title=\"Titanic EM Training Time\", print_default=False, y_label= \"Seconds\")\n(cluster_counts_EM, cluster_col, cluster_analysis_EM) = match_survived_to_clusters(big_df, binary_EM)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_analysis_EM[0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_analysis_EM[1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_first = PCA(n_components=6)\npca_first.fit(all_x)\nplt.xlabel(\"component number\")\nplt.ylabel(\"captured variance ratio\")\nplt.title(\"Titanic PCA component explanatory power\")\nplt.bar(np.arange(1, 7), pca_first.explained_variance_ratio_)\nprint(pca_first.explained_variance_ratio_)\npca = PCA(n_components=2)\npca.fit(all_x)\nprint(pca.explained_variance_ratio_)\n#For PCA, what is the distribution of eigenvalues? \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_reduced_pca = pca.fit_transform(all_x)\npca_k_means_info = k_means_helper(x_reduced_pca, big_df)\nplot_cluster_accuracy(pca_k_means_info[\"ssds\"], \"Titanic PCA k means\")\nplot_cluster_accuracy(pca_k_means_info[\"n_iter\"], title=\"Titanic PCA KMC Time Training\", print_default=False, y_label= \"Seconds\")\n\n\npca_EM_info = EM_helper(x_reduced_pca, big_df)\nprint(pca_EM_info[\"cluster_counts\"])\n#pca_all_data.insert(3,\"cluster_id\", cluster_col_EM_pca, True)\nplot_cluster_accuracy(pca_EM_info[\"n_iter\"], title=\"PCA Titanic EM Time Training\", print_default=False, y_label= \"Seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_k_means_info[\"cluster_samples\"][0].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_k_means_info[\"cluster_samples\"][1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_EM_info[\"cluster_samples\"][0].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_EM_info[\"cluster_samples\"][1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#how kurtotic are the distributions? \n#Do the projection axes for ICA seem to capture anything \"meaningful\"?\n#did you get the same clusters as before? Different clusters? Why? Why not?\nica = FastICA(n_components=2)\nx_reduced_ica = ica.fit_transform(all_x)\n\nica_k_means_info = k_means_helper(x_reduced_ica, big_df)\nplot_cluster_accuracy(ica_k_means_info[\"ssds\"], \"Titanic ICA k means\")\nplot_cluster_accuracy(ica_k_means_info[\"n_iter\"], title=\"Titanic ICA KMC Time Training\", print_default=False, y_label= \"Seconds\")\n\n\nica_EM_info = EM_helper(x_reduced_ica, big_df)\nprint(ica_EM_info[\"cluster_counts\"])\n\nplot_cluster_accuracy(ica_EM_info[\"n_iter\"], title=\"ICA Titanic EM Time Training\", print_default=False, y_label= \"Seconds\")\n\nprint(ica_EM_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ica_k_means_info[\"cluster_samples\"][0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ica_k_means_info[\"cluster_samples\"][1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ica_EM_info[\"cluster_samples\"][0].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ica_EM_info[\"cluster_samples\"][1].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assuming you only generate k projections (i.e., you do dimensionality reduction), how well is the data reconstructed by the randomized projections? \n#Do the clusters change every time?\nr_p = GaussianRandomProjection(n_components=2)\nx_reduced_r_p = r_p.fit_transform(all_x)\n\nr_p_k_means_info = k_means_helper(x_reduced_r_p, big_df)\nplot_cluster_accuracy(r_p_k_means_info[\"ssds\"], \"Titanic RP k means\")\nplot_cluster_accuracy(r_p_k_means_info[\"n_iter\"], title=\"Titanic RP KMC Time Training\", print_default=False, y_label= \"Seconds\")\n\n\nr_p_EM_info = EM_helper(x_reduced_r_p, big_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_p_k_means_info[\"cluster_samples\"][0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_p_k_means_info[\"cluster_samples\"][1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_p_EM_info[\"cluster_samples\"][0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r_p_EM_info[\"cluster_samples\"][1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_a = FactorAnalysis(n_components=2,random_state=3)\nx_reduced_f_a = f_a .fit_transform(all_x)\n\nf_a_k_means_info = k_means_helper(x_reduced_f_a, big_df)\nplot_cluster_accuracy(f_a_k_means_info[\"ssds\"], \"Titanic Factor Analysis k means\")\nplot_cluster_accuracy(f_a_k_means_info[\"n_iter\"], title=\"Titanic FA KMC Time Training\", print_default=False, y_label= \"Seconds\")\n\n\n\nf_a_EM_info = EM_helper(x_reduced_f_a, big_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_a_k_means_info[\"cluster_samples\"][0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_a_k_means_info[\"cluster_samples\"][1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_a_EM_info[\"cluster_samples\"][0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_a_EM_info[\"cluster_samples\"][1].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# util functions\n\ndef eval_for_conclusion(model_id, clf, test_x, test_y):\n    y_pred = clf.predict(test_x)\n    print(classification_report(test_y, y_pred))\n    print(confusion_matrix(test_y, y_pred))\n    accuracy = metrics.accuracy_score(test_y, y_pred)\n    precision = metrics.precision_score(test_y, y_pred)\n    recall = metrics.recall_score(test_y, y_pred)\n    print(\"Final {0} model accuracy:\".format(model_id), accuracy)\n    print(\"Final {0} model precision:\".format(model_id), precision) \n    print(\"Final {0} model recall:\".format(model_id), recall) \n    return {\"model\":model_id, \"recall\":recall, \"accuracy\":accuracy, \"precision\":precision}\n\n \ndef split_test_train(train_size, all_data):\n    msk = np.random.rand(len(all_data)) < train_size\n    train_df = all_data[msk]\n    test_df = all_data[~msk]\n    train_y = train_df[pos_outcome_var]\n    train_x = train_df.drop(pos_outcome_var, axis=1)\n    test_y = test_df[pos_outcome_var]\n    test_x  = test_df.drop(pos_outcome_var, axis=1)\n    return (train_x, train_y, test_x, test_y)\n\ndef cross_validate(all_data, model):\n    depth = []\n    all_y = all_data[pos_outcome_var]\n    all_x  = all_data.drop(pos_outcome_var, axis=1)\n    # Perform k-fold cross validation \n    scores = cross_val_score(estimator=model, X=all_x, y=all_y, cv=5, n_jobs=4)\n    depth.append((i,scores.mean()))\n    return depth\n    \ndef train_and_test(all_data, model):\n    test_scores = []\n    train_scores = []\n    times = []\n    for i in range(1,10):\n        (train_x, train_y, test_x, test_y) = split_test_train(0.1 * i, big_df)\n        #print(\"len test: \", len(test_x), \", len train: \", len(train_x))\n        start = time.time()\n        #TODO iterations\n        model.fit(train_x, train_y)\n        end = time.time()\n        times.append(end - start)\n        pred_test_y = model.predict(test_x) # TODO add wallclock time\n        test_score = round(model.score(test_x, test_y) * 100, 2)\n        pred_train_y = model.predict(train_x)\n        train_score = round(model.score(train_x, train_y) * 100, 2)\n        test_scores.append(test_score)\n        train_scores.append(train_score)\n    return (test_scores, train_scores, times)\n\ndef plot_data(x_vars, x_label, all_y_vars, y_var_labels, y_label, title, y_bounds=None):\n    plt.rcParams[\"figure.figsize\"] = (4,3)\n    colors = ['red','orange','black','green','blue','violet']\n    i = 0\n    for y_var in all_y_vars:\n#         if i == 2: # don't plot when i = 1 for cv\n#             x_vars = x_vars[1:]\n        plt.plot(x_vars, y_var, 'o-', color=colors[i % 6], label=y_var_labels[i])\n        i += 1\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    if y_bounds != None:\n        plt.ylim(y_bounds)\n    leg = plt.legend();\n    plt.show()\n\ndef evaluate_model(all_data, model, model_id):\n    (test_scores, train_scores, times) = train_and_test(all_data, model)\n    print(\"{0} train timings (seconds): {1}\".format(model_id, times))\n    print(\"{0} test set scores: {1} \".format(model_id, test_scores))\n    print(\"{0} train set scores: {1}\".format(model_id, train_scores))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [test_scores, train_scores],\\\n              [\"test_scores\", \"train_scores\"], \"Accuracy\", \"{0} Accuracy Over Train/Test Split\".format(model_id), (50,105))\n    plot_data([x * 10 for x in range(1,10)], \"Percentage of data in training set\", [times],\n             [\"times\"], \"Train time in Seconds\", \"{0} Time Spent Training Over Train/Test Split\".format(model_id))\n    return (test_scores, train_scores, times)\n\ndef plot_grid_search(grid_results, plotting_func, title, x_label, y_label, grid_size, model_handles):\n    means = grid_results.cv_results_['mean_test_score']\n    stds = grid_results.cv_results_['std_test_score']\n    params = grid_results.cv_results_['params']\n    plt.rcParams[\"figure.figsize\"] = grid_size\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n    plt.subplots\n    ax = plt.subplot()\n    \n    for mean, std, params in zip(means, stds, params):\n        #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n        plotting_func(mean, params, plt, ax)\n    if model_handles: plt.legend(handles=model_handles)\n    plt.show()\n\n\n#def grid_search(model, params, x_train, y_train, x_test, y_test):\n    \n\n#TODO come up with graphing function that takes in two arrays of test and train and plots them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(x_reduced_pca, neural_net_classifier, \"Titanic NN PCA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(x_reduced_ica, neural_net_classifier, \"Titanic NN ICA\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(x_reduced_r_p, neural_net_classifier, \"Titanic NN Randomized Projections\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(x_reduced_f_a, neural_net_classifier, \"Titanic NN Factor Analysis\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(pca_EM_info[\"reduced_with_labels\"], neural_net_classifier, \"Titanic NN PCA with labels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(ica_EM_info[\"reduced_with_labels\"], neural_net_classifier, \"Titanic NN ICA with labels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(r_p_EM_info[\"reduced_with_labels\"], neural_net_classifier, \\\n               \"Titanic NN Randomized Projections with labels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neural_net_classifier = MLPClassifier(max_iter=10000) #, alpha=0.01, hidden_layer_sizes=(6, 3), random_state=1)\n# tried with6,3 and works great. Other dimensions are horrible\nevaluate_model(f_a_EM_info[\"reduced_with_labels\"], neural_net_classifier, \\\n               \"Titanic NN Factor Analysis with labels\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}