{"cells":[{"metadata":{"_uuid":"def75dfd19c7d1798b3c2da637e5797d980781e0"},"cell_type":"markdown","source":"# ANALIZING FACE LANDMARKS\n\nThis is a very very simple kernel whose purpose is just to check what is in the dataset regarding the given coordinates for landmarks and bounding boxes.\n\nFor that, we're going to load the data, create the bounding boxes onto the images and plot them to see where they are."},{"metadata":{"_uuid":"c5104844ebd7128258730dc8813755b8bce82c89"},"cell_type":"markdown","source":"## Imports and helper functions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom PIL import Image\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n#displays for a batch of images as a numpy array basic statistics\ndef inspect(x, name):\n    print(name + \": \", \n          'shape:', x.shape, \n          'min:', x.min(), \n          'max:',x.max(),\n          'mean:',x.mean(), \n          'std:', x.std())\n    \n#plots the passed images side by side\ndef plotImages(*images, minVal =0, maxVal = 1):\n    fig, ax = plt.subplots(1,len(images), figsize=(13,4))\n    \n    for i,image in enumerate(images):\n        ax[i].imshow(image,vmin = minVal, vmax=maxVal)\n    \n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"729547cecf79baea4aaa858b88f70fc4caeafbae"},"cell_type":"markdown","source":"## Loading and checking raw data\n\nNotice the \"assert\" command, to make sure all arrays that were loaded are matching rows. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3eff865b827ea320f6608502fcc2af3167a2f2d6"},"cell_type":"code","source":"#Folder and image files\ngeneralFolder = '../input/'\nfolder = generalFolder + 'img_align_celeba/img_align_celeba'\nallFiles = [folder + \"/\" + f for f in sorted(os.listdir(folder))]\n\n#CSV files \nallAttributes = pd.read_csv(generalFolder + 'list_attr_celeba.csv')\nfaceBoxes = pd.read_csv(generalFolder + 'list_bbox_celeba.csv')\nfeatureLocations = pd.read_csv(generalFolder + 'list_landmarks_align_celeba.csv')\n\n\n#Make sure our filenames and all CSV files are matching\nfor f, a, face, loc in zip(allFiles, \n                           allAttributes['image_id'], \n                           faceBoxes['image_id'], \n                           featureLocations['image_id']):\n    assert(f.split('/')[-1] == a == face == loc)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c61f54f4b07c554e3e9ddf3171194d70bc182761"},"cell_type":"markdown","source":"## What is in the CSV files and how are we preprocessing them\n\nHere, we understand the data and process it so it can be easily used in some models such as a Keras model. \n\n### File: list_attr_celeba.csv (`allAtributes`) \n\nThis is not in the scope of this kernel, so, just a quick description: it's a file that for every image, has 40 columns identifying one feature, such as shadow, certain types of eyebrows, etc. The values are -1 and 1.\n\nIn the preprocessing, they're being normalized from 0 to 1, so they can be used with sigmoid activations and loss functions specialized in classification. \n\n### File: list_bbox_celeba.csv (`faceBoxes`)\n\nFor each image, contains `x`, `y`, `width` and `height`, defining a complete bounding box that should cover the entire face.\nWe just create an numpy array of zeros, and in the range defined by these cordinates and sizes, we define the value as 1. \n\n### File: list_landmarks_align_celeba.csv (`featureLocations`)  \n\nFor each image, contains \"only the location\" of these:\n\n- right eye   \n- left eye    \n- nose    \n- right side of the mouth    \n- left side of the mouth    \n\nFor each location, we're creating a 20x20 bounding box, centered on the given coordinate pair. We use the same convention as with the face bounding boxes (0 and 1) and stack everything, these and the face bounding boxes in a 6 channel array, for using in future models.\n"},{"metadata":{"_uuid":"1abe791593578301ca11fd44f46089636c16a1e4"},"cell_type":"markdown","source":"## Creating a batch generator\n\nThis is a working batch generator that can be used for training models. (Not strictly what we need for this test, but useful).\nThis generator, if derived from the `keras.utils.Sequence` class, can be used directly with `Model.fit_generator(generator, len(generator), ....)`    "},{"metadata":{"trusted":true,"_uuid":"35c86381e1349144afc455f34338afbebed0d844","collapsed":true},"cell_type":"code","source":"class Reader():\n    #If you make this a 'class Reader(keras.utils.Sequence)', \n    #you can use it with `Model.fit_generator`   \n    \n    #creates the generator taking the raw data and a batch size\n    def __init__(self, files, attributes, faces, locations, batchSize):\n\n        assert len(files)==len(attributes)==len(faces)==len(locations)\n        \n        self.files = files\n        \n        #from the CSV files, make them arrays without the \"image_id\" column \n        #we checked before that the array rows are matching the filenames    \n        self.attributes = (np.array(attributes)[:,1:] + 1)/2. #normalized from 0 to 1\n        self.faces = np.array(faces)[:,1:]           #x, y, width, height\n        self.locations = np.array(locations)[:,1:]   #5 coordinate pairs\n        \n        self.batchSize = batchSize\n        \n        #defining the number of batches (length) and the last batch size\n        l,r = divmod(len(files), batchSize)\n        self.length = l + (1 if r > 0 else 0)\n        self.lastSize = r if r > 0 else batchSize\n        \n        \n    #gets the number of batches that can be generated    \n    def __len__(self):\n        return self.length\n    \n    \n    #gets one batch respective to the passed index    \n    def __getitem__(self, i):\n        batch = self.batchSize if i + 1 < len(self) else self.lastSize\n        start = i * self.batchSize\n        end = start + batch\n        \n        imgs = list()\n        for im in self.files[start:end]:\n            imgs.append(np.array(Image.open(im), dtype=np.float64)/255.)\n        imgs = np.stack(imgs, axis=0)\n        masks = np.zeros(imgs.shape[:3]+(6,))\n        \n        self.addFaceMasks(self.faces[start:end], masks)\n        self.addLocations(self.locations[start:end], masks)\n        \n        return imgs, masks, self.attributes[start:end]\n        \n    #processes the face bounding boxes into mask images   \n    def addFaceMasks(self,faces,addTo):\n        \n        #for each face in the batch\n        for i, face in enumerate(faces):\n            x, y, w, h = face #gets coordinates and sizes\n            \n            addTo[i,y:y+h, x:x+w,0] = 1. #updates the masks    \n    \n    #processes the other coordinates into mask images   \n    def addLocations(self, locations, addTo):\n        \n        #for each face in the batch\n        for i, locs in enumerate(locations):\n            locs = locs.reshape((-1,2))    #reshapes into pairs of coords\n            \n            #for each pair of coords\n            for ch, (posX, posY) in enumerate(locs):\n                #20x20 bounding boxes for this coord pair \n                x = posX - 10\n                y = posY - 10\n                x2 = x + 20\n                y2 = y + 20\n                if x < 0: x = 0\n                if y < 0: y = 0\n                    \n                addTo[i,y:y2, x:x2,ch+1] = 1.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91ae2e59d263e9b375dbab01d790751fbb084afa"},"cell_type":"markdown","source":"## Getting batches and visualizing the data\n\nHere, we're taking only the first image of each batch to plot (otherwise there would be too many images)\n\n**Warning:** The face bounding boxes are wrong in the dataset (see details after the plots)"},{"metadata":{"trusted":true,"_uuid":"09af79713e91a19e68ef7cd71720c9cef4f31995","scrolled":false},"cell_type":"code","source":"generator = Reader(allFiles, allAttributes, faceBoxes, featureLocations, 32)\n\n#test 20 batches\nfor i in range(20):\n    imgs, masks, atts = generator[i]\n    \n    #print the batch shape:\n    if i == 0:\n        print(\"image batch shape:\", imgs.shape)\n        print(\"masks batch shape:\", masks.shape)\n    \n    #use these inspections to verify that the values are within the expected ranges   \n#     inspect(imgs, 'imgs')\n#     inspect(masks, 'masks')\n#     inspect(atts, 'atts')\n    \n    #takes four faded images from the batch (we fade them to add with the masks)\n    n = 2 #plotting n images per batch\n    imgs = imgs[:n]/2.\n    #plotImages(*imgs)\n    \n    #gets the face bounding boxes (first channel in the masks array)\n    faceM = np.array(masks[:n,:,:,:3]) #we keep 3 channels for compatibility   \n    faceM[:,:,:,1:] = 0      #we make the non related channels 0\n    \n    #gets the other bounding boxes, summing two groups of 3 channels\n    #each channel is one feature (remove the face boxes that are above)\n    otherM = masks[:n,:,:,:3] + masks[:n,:,:,3:] - faceM\n    \n    #pairs of images:\n        #faces + face bounding box\n        #faces + other features bounding boxes\n    faceBxs = np.clip(imgs + faceM , 0,1)\n    otherBxs = np.clip(imgs + otherM, 0, 1)\n        \n    #putting pairs side by side\n    imgs = np.stack([faceBxs,otherBxs], axis=1)\n    shp = imgs.shape\n    imgs = imgs.reshape((-1,) + shp[2:])\n    \n    #plotting\n    plotImages(*imgs )\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50d16bfb47e753b7cb7cd2d61f65c86a5d4347bc"},"cell_type":"markdown","source":"## Wrong bounding boxes?\n\nWell, we may think there was a bug in the code, that's not rare after all. \n\nBut then we look closer at the dataset and quickly see something is clearly wrong. \nWhile our images are shaped as 218 x 178, values in the CSV file contain numbers such as 500 and greater.\n\n![Image from the dataset showing values greater than the image shape](https://pix.toile-libre.org/upload/original/1533859338.png)\n\n# Conclusions\n\nRegarding the landmarks and bounding boxes we can say that this dataset:\n\n- Is great when dealing with eyes, mouth and nose locations    \n- Has wrong bounding boxes for the faces    \n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}