{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-08T11:31:26.528563Z","iopub.execute_input":"2021-07-08T11:31:26.529182Z","iopub.status.idle":"2021-07-08T11:31:26.550586Z","shell.execute_reply.started":"2021-07-08T11:31:26.529044Z","shell.execute_reply":"2021-07-08T11:31:26.549491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Installing pyspark\n!pip install pyspark\n!pip install findspark","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:31:26.552483Z","iopub.execute_input":"2021-07-08T11:31:26.553015Z","iopub.status.idle":"2021-07-08T11:32:15.548122Z","shell.execute_reply.started":"2021-07-08T11:31:26.552979Z","shell.execute_reply":"2021-07-08T11:32:15.546834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:15.55043Z","iopub.execute_input":"2021-07-08T11:32:15.550759Z","iopub.status.idle":"2021-07-08T11:32:15.738307Z","shell.execute_reply.started":"2021-07-08T11:32:15.550724Z","shell.execute_reply":"2021-07-08T11:32:15.737125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.appName('Spark Application').getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:15.740032Z","iopub.execute_input":"2021-07-08T11:32:15.740354Z","iopub.status.idle":"2021-07-08T11:32:21.708577Z","shell.execute_reply.started":"2021-07-08T11:32:15.740323Z","shell.execute_reply":"2021-07-08T11:32:21.706846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = spark.read.csv(\"/kaggle/input/us-accidents/US_Accidents_Dec20_Updated.csv\",inferSchema=True, header=True)\ndf = spark.read.csv(\"/kaggle/input/us-accidents/US_Accidents_Dec20_Updated.csv\",inferSchema=True, header=True).limit(1500000)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:21.711014Z","iopub.execute_input":"2021-07-08T11:32:21.711489Z","iopub.status.idle":"2021-07-08T11:32:42.238909Z","shell.execute_reply.started":"2021-07-08T11:32:21.71144Z","shell.execute_reply":"2021-07-08T11:32:42.237674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:42.240574Z","iopub.execute_input":"2021-07-08T11:32:42.245082Z","iopub.status.idle":"2021-07-08T11:32:42.754126Z","shell.execute_reply.started":"2021-07-08T11:32:42.24502Z","shell.execute_reply":"2021-07-08T11:32:42.753101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:42.757165Z","iopub.execute_input":"2021-07-08T11:32:42.757773Z","iopub.status.idle":"2021-07-08T11:32:42.777896Z","shell.execute_reply.started":"2021-07-08T11:32:42.757721Z","shell.execute_reply":"2021-07-08T11:32:42.776846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing and Data Explorations","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import *","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:42.779447Z","iopub.execute_input":"2021-07-08T11:32:42.780063Z","iopub.status.idle":"2021-07-08T11:32:42.791361Z","shell.execute_reply.started":"2021-07-08T11:32:42.779997Z","shell.execute_reply":"2021-07-08T11:32:42.790144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols_to_drop = ['ID','Start_Time','End_Time','Description','Street',\n                'Weather_Timestamp','Zipcode','County','City','Airport_Code','Precipitation(in)'] \n# Weather_Condition to be processed again after cleaning","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:42.795596Z","iopub.execute_input":"2021-07-08T11:32:42.79609Z","iopub.status.idle":"2021-07-08T11:32:42.811312Z","shell.execute_reply.started":"2021-07-08T11:32:42.796041Z","shell.execute_reply":"2021-07-08T11:32:42.809112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(*cols_to_drop)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:42.815446Z","iopub.execute_input":"2021-07-08T11:32:42.816052Z","iopub.status.idle":"2021-07-08T11:32:42.877281Z","shell.execute_reply.started":"2021-07-08T11:32:42.816Z","shell.execute_reply":"2021-07-08T11:32:42.875692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:42.879452Z","iopub.execute_input":"2021-07-08T11:32:42.880181Z","iopub.status.idle":"2021-07-08T11:32:42.892945Z","shell.execute_reply.started":"2021-07-08T11:32:42.880134Z","shell.execute_reply":"2021-07-08T11:32:42.891494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:42.894614Z","iopub.execute_input":"2021-07-08T11:32:42.89521Z","iopub.status.idle":"2021-07-08T11:32:43.179706Z","shell.execute_reply.started":"2021-07-08T11:32:42.895166Z","shell.execute_reply":"2021-07-08T11:32:43.178632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dict_Null = {col:df.filter(isnan(df[col[0]])).count() for col in df.dtypes if col[1] != 'boolean'}\n# Dict_Null","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:43.181237Z","iopub.execute_input":"2021-07-08T11:32:43.181603Z","iopub.status.idle":"2021-07-08T11:32:43.185916Z","shell.execute_reply.started":"2021-07-08T11:32:43.181565Z","shell.execute_reply":"2021-07-08T11:32:43.184575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace nan with Nulls\ncolumns = df.dtypes\nfor cols, typ in columns:\n    if typ != 'boolean':\n        df = df.withColumn(cols,when(isnan(col(cols)),None).otherwise(col(cols)))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:43.187556Z","iopub.execute_input":"2021-07-08T11:32:43.188256Z","iopub.status.idle":"2021-07-08T11:32:44.333293Z","shell.execute_reply.started":"2021-07-08T11:32:43.188208Z","shell.execute_reply":"2021-07-08T11:32:44.331902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dict_Null = {col:df.filter(isnan(df[col[0]])).count() for col in df.dtypes if col[1] != 'boolean'}\n# Dict_Null","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:44.335174Z","iopub.execute_input":"2021-07-08T11:32:44.335994Z","iopub.status.idle":"2021-07-08T11:32:44.341091Z","shell.execute_reply.started":"2021-07-08T11:32:44.335948Z","shell.execute_reply":"2021-07-08T11:32:44.339663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dict_Null2 = {col:df.filter(isnull(df[col[0]])).count() for col in df.dtypes }\n# Dict_Null2","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:44.343411Z","iopub.execute_input":"2021-07-08T11:32:44.344604Z","iopub.status.idle":"2021-07-08T11:32:44.356505Z","shell.execute_reply.started":"2021-07-08T11:32:44.344332Z","shell.execute_reply":"2021-07-08T11:32:44.355303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = 'Severity'\nstring_cols =  [cols[0] for cols in df.dtypes if cols[1] == \"string\" ]\nnum_cols = [cols[0] for cols in df.dtypes if cols[1] == \"int\" or cols[1] == \"double\" ]\nnum_cols.remove(label)\nbool_cols = [cols[0] for cols in df.dtypes if cols[1] == \"boolean\"]\n\n# string_cols,\n# num_cols\n# bool_cols","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:44.358555Z","iopub.execute_input":"2021-07-08T11:32:44.359232Z","iopub.status.idle":"2021-07-08T11:32:44.386267Z","shell.execute_reply.started":"2021-07-08T11:32:44.359182Z","shell.execute_reply":"2021-07-08T11:32:44.385318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.fillna(\"unknown\",string_cols)\ndf = df.fillna(0,num_cols)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:44.388077Z","iopub.execute_input":"2021-07-08T11:32:44.388718Z","iopub.status.idle":"2021-07-08T11:32:44.585713Z","shell.execute_reply.started":"2021-07-08T11:32:44.388669Z","shell.execute_reply":"2021-07-08T11:32:44.58449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nDict_Null3 = {col:df.filter(isnull(df[col[0]])).count() for col in df.dtypes }\nDict_Null3","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:32:44.587159Z","iopub.execute_input":"2021-07-08T11:32:44.587574Z","iopub.status.idle":"2021-07-08T11:33:56.998896Z","shell.execute_reply.started":"2021-07-08T11:32:44.587509Z","shell.execute_reply":"2021-07-08T11:33:56.997594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in bool_cols:\n    df = df.withColumn(c,col(c).cast(\"integer\"))    ","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:33:57.000801Z","iopub.execute_input":"2021-07-08T11:33:57.001246Z","iopub.status.idle":"2021-07-08T11:33:57.420638Z","shell.execute_reply.started":"2021-07-08T11:33:57.0012Z","shell.execute_reply":"2021-07-08T11:33:57.419279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train, test) = df.randomSplit([0.7, 0.3])\ntrain.count(), test.count()\n# df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:33:57.426516Z","iopub.execute_input":"2021-07-08T11:33:57.427131Z","iopub.status.idle":"2021-07-08T11:36:50.090316Z","shell.execute_reply.started":"2021-07-08T11:33:57.427077Z","shell.execute_reply":"2021-07-08T11:36:50.089164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:36:50.092493Z","iopub.execute_input":"2021-07-08T11:36:50.093303Z","iopub.status.idle":"2021-07-08T11:36:50.259657Z","shell.execute_reply.started":"2021-07-08T11:36:50.093247Z","shell.execute_reply":"2021-07-08T11:36:50.258544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sIndexer = [StringIndexer(inputCol=cols, outputCol=cols+\"Index\") for cols in  string_cols]","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:36:50.261072Z","iopub.execute_input":"2021-07-08T11:36:50.261345Z","iopub.status.idle":"2021-07-08T11:36:50.359122Z","shell.execute_reply.started":"2021-07-08T11:36:50.261317Z","shell.execute_reply":"2021-07-08T11:36:50.357901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assembler = VectorAssembler(inputCols=[s.getOutputCol() for s in sIndexer]+bool_cols+num_cols, outputCol='features')","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:36:50.360797Z","iopub.execute_input":"2021-07-08T11:36:50.361197Z","iopub.status.idle":"2021-07-08T11:36:50.378066Z","shell.execute_reply.started":"2021-07-08T11:36:50.361154Z","shell.execute_reply":"2021-07-08T11:36:50.376938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reg = 0,\nlr = LogisticRegression(featuresCol=\"features\", labelCol=label)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:36:50.379614Z","iopub.execute_input":"2021-07-08T11:36:50.379931Z","iopub.status.idle":"2021-07-08T11:36:50.436181Z","shell.execute_reply.started":"2021-07-08T11:36:50.379901Z","shell.execute_reply":"2021-07-08T11:36:50.435062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline(stages=sIndexer+[assembler, lr])\nmodel = pipeline.fit(train)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-08T11:36:50.43758Z","iopub.execute_input":"2021-07-08T11:36:50.437914Z","iopub.status.idle":"2021-07-08T12:08:32.879853Z","shell.execute_reply.started":"2021-07-08T11:36:50.437885Z","shell.execute_reply":"2021-07-08T12:08:32.86274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.transform(train)\nprint(\"Prediction\")\npreds.select(\"Severity\",\"prediction\").show(20)\n\n# evaluate the accuracy of the model using the test set\nevaluator = MulticlassClassificationEvaluator(metricName='accuracy', labelCol=\"Severity\")\naccuracy = evaluator.evaluate(preds)\n\nprint()\nprint('#####################################')\nprint(f\"Accuracy is {accuracy}\")\nprint('#####################################')\nprint()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.882965Z","iopub.status.idle":"2021-07-08T12:08:32.891393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install onnx\n!pip install onnxmltools\n!pip install mlflow\n!pip install onnxruntime","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.892854Z","iopub.status.idle":"2021-07-08T12:08:32.8954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from onnxmltools import convert_sparkml\nfrom onnxmltools.convert.sparkml.utils import buildInitialTypesSimple\nfrom onnxmltools.convert.common.data_types import StringTensorType, FloatTensorType\nimport mlflow.onnx","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.898325Z","iopub.status.idle":"2021-07-08T12:08:32.899149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.900466Z","iopub.status.idle":"2021-07-08T12:08:32.901227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.902442Z","iopub.status.idle":"2021-07-08T12:08:32.903208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nif not os.path.exists('model_onnx/'):\n    os.mkdir('model_onnx/')\n    \nif not os.path.exists('model_spark/'):\n    os.mkdir('model_spark/')\n    \nif not os.path.exists('model_mlflow'):\n    os.mkdir('model_mlflow')","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.904413Z","iopub.status.idle":"2021-07-08T12:08:32.9052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Native ONNX framework\ninitial_types = buildInitialTypesSimple(df.drop('Severity'))\n# initial_types\nonnx_model = convert_sparkml(model, 'Pyspark model', initial_types, spark_session = spark)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwith open(os.path.join(\"model_onnx/\", \"model.onnx\"), \"wb\") as f:\n    f.write(onnx_model.SerializeToString())","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.906502Z","iopub.status.idle":"2021-07-08T12:08:32.907375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n# Pyspark native\nmodel.save('model_spark/spark_model')","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.908697Z","iopub.status.idle":"2021-07-08T12:08:32.909549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# ONNX on MLFlow\nmlflow.onnx.save_model(onnx_model,\"model_mlflow/mlflow_model\")","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.910796Z","iopub.status.idle":"2021-07-08T12:08:32.911722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -ltrh","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.913106Z","iopub.status.idle":"2021-07-08T12:08:32.914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install py-cpuinfo","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.915348Z","iopub.status.idle":"2021-07-08T12:08:32.916214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import platform\nplatform.processor()\n# import cpuinfo\n\n# 'Intel64 Family 6 Model 23 Stepping 6, GenuineIntel'","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.917567Z","iopub.status.idle":"2021-07-08T12:08:32.918386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cpuinfo\ncpuinfo.get_cpu_info()['brand_raw']","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.919664Z","iopub.status.idle":"2021-07-08T12:08:32.920475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pyspark.sql.functions import array_contains, col, explode\n# def BinaryEncoder(df, column_name):\n#     col_vals = [\n#         x[0] for x in \n#         df.select(column_name).distinct().orderBy(column_name).collect()]\n#     print(col_vals)\n    \n    \n    \n# %time BinaryEncoder(df_1, \"Side\")","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:08:32.921735Z","iopub.status.idle":"2021-07-08T12:08:32.922542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}