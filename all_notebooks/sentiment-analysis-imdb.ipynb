{"cells":[{"metadata":{"collapsed":true,"_uuid":"d93a1a1ad1248f6ad1b1c79890a9cdbd0cee7363"},"cell_type":"markdown","source":"#  IMDB movie review "},{"metadata":{"_uuid":"ec00ff67e1f737000b776882a467519a7276f7fd"},"cell_type":"markdown","source":"## Table of Contents"},{"metadata":{"collapsed":true,"_uuid":"ecd93123b10c7dd26c15e3671bfa931a0d9540d2"},"cell_type":"markdown","source":"1. [Introduction](#intro)<br><br>\n2. [Reading the Data](#reading)<br><br>\n3. [Data Cleaning and Text Preprocessing](#preprocess)<br> - [3.1. Removing HTML Markup by using BeautifulSoup Package](#beauti)<br> - [3.2. Removing Non-Letter Characters & Converting Reviews to Lower Case](#non-char)<br> - [3.3. Tokenization](#token)<br> -  [3.4. Removing Stop words](#stop)<br> - [3.5. Stemming / Lemmatization](#stlm)<br> - [3.6. Putting It All Together](#together)<br><br>\n\n4. [Visualization](#visu)<br>- [4.1. WordCloud](#wc) <br>- [4.2. Distribution](#dist)<br><br>\n5. [Bag of Words](#bag)<br><br>\n6. [Modeling](#modeling)<br>- [6.1. Support Vector Machine](#svm)<br>- [6.2. Bernoulli Naive Bayes Classifier](#bnb)<br>- [6.4. Logistic Regression](#logi)<br><br>\n\n\n\n"},{"metadata":{"_uuid":"aafee2a93ff26ebdccc41e26e17ac69ff33feb5c"},"cell_type":"markdown","source":"## 1. Introduction <a id='intro'></a>"},{"metadata":{"collapsed":true,"_uuid":"8f5113036da3afe3957201869f6e40bb782aa339"},"cell_type":"markdown","source":"The goal of our project is to classifiy correctly whether 25,000 movie reviews from IMDB are positive or negative. This is the first part of sentiment analysis which will be used a Bag of Words for creating features. Once we obtain the result of the prediction, we will compare it with the seoncd part of our sentiment analysis."},{"metadata":{"_uuid":"159351a66f66fdb4934461018a17fed003f05ce3"},"cell_type":"markdown","source":"## 2. Reading the Data <a id='reading'></a>"},{"metadata":{"trusted":true,"_uuid":"51d8be665307c523bcaf26bcabc8a36360045641"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7f68e6a1cd864e4f77a40a3eb9e86f808c1e119"},"cell_type":"code","source":"# Read the labeled training and test data\n# Header = 0 indicates that the first line of the file contains column names, \n# delimiter = \\t indicates that the fields are seperated by tabs, and \n# quoting = 3 tells python to ignore doubled quotes\n\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)\ntest = pd.read_csv(\"../input/testData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3be00abd99b7f5cf567d1092ca545411efea7314"},"cell_type":"code","source":"# Display check the dimensions and the first 2 rows of the file.\n\nprint('train dim:', train.shape, 'test dim:', test.shape)\ntrain.iloc[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b07bcfcab7055ed964526b42601df73a27bfd5f4"},"cell_type":"code","source":"# Let's check the first review.\n\ntrain.iloc[0][\"review\"][:len(train.iloc[0][\"review\"])//2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1baee6b32d1639457e41b5b31adf10ece9a90e8"},"cell_type":"markdown","source":"As you can see the above review, the html tags are disturbing and also in order to make the data machine-learning friendly, we need to clean the data."},{"metadata":{"_uuid":"89856bfe37e21acaac707c2fcee37b823b61d037"},"cell_type":"markdown","source":"## 3. Data Cleaning and Text Preprocessing <a id='preprocess'></a>"},{"metadata":{"_uuid":"995dc06a427642681107cf2137a34bb91747a8f3"},"cell_type":"markdown","source":"### 3.1. Removing HTML Markup by using BeautifulSoup Package <a id='beauti'></a>"},{"metadata":{"trusted":true,"_uuid":"fc61772b105d6f12ce3285ceb8c84a398324af8d"},"cell_type":"code","source":"from bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"539ac00a037a8ccd3ecc031f414c748948fbafc8"},"cell_type":"code","source":"example1 = BeautifulSoup(train[\"review\"][0], \"html.parser\")\n\n# Without the second argument \"html.parser\", it will pop out the warning message.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0ce377435325358f18639fd2e5769585ed1afb1"},"cell_type":"code","source":"print(example1.get_text())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5fe3d1f875637991281a8e9bab95b1f1659db99"},"cell_type":"markdown","source":"You can clearly see the effect of removing HTML markup. "},{"metadata":{"_uuid":"2c05e139db493f42f131aeb80f6d658a688ec10d"},"cell_type":"markdown","source":"### 3.2. Removing Non-Letter Characters & Converting Reviews to Lower Case <a id='non-char'></a>"},{"metadata":{"_uuid":"ed09c2e62c513f1fc7938b5513f0ef70e40138d8"},"cell_type":"markdown","source":"It may be important to include some punctuations and numbers such as :-). However for this project, for simplicity, we remove both of them."},{"metadata":{"trusted":true,"_uuid":"6eafb6e96c5f6ba67aa7b3e4a1ba78ba863029bf"},"cell_type":"code","source":"import re\n\nletters = re.sub(\"[^a-zA-Z]\", \" \", example1.get_text())\nletters = letters.lower()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fb4a3285caced7a3925d6986970a327f24dfacc"},"cell_type":"markdown","source":"The meaning of the above regular expression is that except for (^) the letters from a to z and from A to Z ([a-zA-Z]) substitute all the characters to spaces. lower() means conversion any capital letters to lower case."},{"metadata":{"trusted":true,"_uuid":"20ccd669941d16830b8aca117ee548bc9d4a2954"},"cell_type":"code","source":"print(letters)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0822500bf872e13c8c6b4041c40ac811e24258c1"},"cell_type":"markdown","source":"### 3.3. Tokenization <a id='token'></a>\n\nTokenization is the process splitting a sentence or paragraph into the most basic units."},{"metadata":{"trusted":true,"_uuid":"074c4e79498f85c57ab5d520d3600911d40f63a9"},"cell_type":"code","source":"# Import Natural Language Toolkit\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"110192070022039e186e3ac7e3310b91571828ad"},"cell_type":"code","source":"# Instead of using just split() method, used word_tokenize in nltk library.\nword = nltk.word_tokenize(letters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0359dad768bd55598fc9b8a080df910f00d76d0c"},"cell_type":"code","source":"word","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0e42dbf47527b8091bd4430c6ec1e3e2c6b5a8e"},"cell_type":"markdown","source":"### 3.4. Removing Stop words <a id='stop'></a>"},{"metadata":{"_uuid":"bf61692c4ea4f9e3429100a2ce2a33fed9ac0f64"},"cell_type":"markdown","source":"\"Stop words\" is the frequently occurring words that do not carry much meaning such as \"a\", \"and\" , \"is\", \"the\". In order to use the data as input for machine learning algorithms, we need to get rid of them. Fortunately, there is a function called stopwords which is already built in NLTK library."},{"metadata":{"trusted":true,"_uuid":"bbff03bf5c8ffefa948898de6659eee423f81dac"},"cell_type":"code","source":"from nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7633a17ec024609cf1e30668ef3ad9ce03e2935"},"cell_type":"markdown","source":"Below is the list of stopwords."},{"metadata":{"trusted":true,"_uuid":"71ba70278cc4c43ede172e88bf4f2423e4c3d1c7"},"cell_type":"code","source":"print(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"825f743c185376a96d4a8d4283263fe099a3a4ae"},"cell_type":"code","source":"# Exclude the stop words from the original tokens.\n\nword = [w for w in word if not w in set(stopwords.words(\"english\"))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cccc150dc37f99976dbd8e0956bd327f7582cb0d"},"cell_type":"code","source":"word","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"081c346b7f7799aa3e8a05b9f94cbaf54679fbeb"},"cell_type":"markdown","source":"### 3.5. Stemming / Lemmatization <a id='stlm'></a>\n\nIt is important to know the difference between these two.\n\n- __Stemming:__ Stemming algorithms work by cutting off the end of the word, and in some cases also the beginning while looking for the root. This indiscriminate cutting can be successful in some occasions, but not always, that is why we affirm that this an approach that offers some limitations. ex) studying -> study, studied -> studi <br>\n<br>\n- __Lemmatization:__ Lemmatization is the process of converting the words of a sentence to its dictionary form. For example, given the words amusement, amusing, and amused, the lemma for each and all would be amuse. ex) studying -> study, studied -> study. Lemmatization also discerns the meaning of the word by understanding the context of a passage. For example, if a \"meet\" is used as a noun then it will print out a \"meeting\"; however, if it is used as a verb then it will print out \"meet\".  \n<br>\n\nUsually, either one of them is chosen for text-analysis not both. As a side note, Lancaster is the most aggressive stemmer among three major stemming algorithms (Porter, Snowball, Lancaster) and Porter is the least aggressive. The \"aggressive algorithms\" means how much a working set of words are reduced. The more aggressive the algorithms, the faster it is; however, in some certain circumstances, it will hugely trim down your working set. Therefore, in this project I decide to use snowball since it is slightly faster than Porter and does not trim down too much information as Lancaster does."},{"metadata":{"trusted":true,"_uuid":"727021d5755b8beb84e8dcd749534735824e16af"},"cell_type":"code","source":"snow = nltk.stem.SnowballStemmer('english')\nstems = [snow.stem(w) for w in word]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc4c663753ef6bf44066c086d2e5e4a78dc069d6"},"cell_type":"code","source":"stems","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0336ea372b9b052a9840dfd05a292298d7e7edb3"},"cell_type":"markdown","source":"As you can see the word \"started\", it is converted to \"start\" and \"listening\" and \"watching\" are converted to \"listen\" and \"watch\"."},{"metadata":{"_uuid":"78ec290deffccebd00f5798b391fdbdc58fa702b"},"cell_type":"markdown","source":"### 3.6. Putting It All Together <a id='together'></a>"},{"metadata":{"_uuid":"0ae0c1e36085c600b9be1d3f76ccdee09e4c8bf1"},"cell_type":"markdown","source":"So far, we have cleaned only one datapoint. Now it's time to apply all the cleaning process to all the data.<br>\nTo make the code reusable, we need to create a function that can be called many times."},{"metadata":{"trusted":true,"_uuid":"eb0010c98bed9811c5bbdedcc07c64eba2b02175"},"cell_type":"code","source":"def cleaning(raw_review):\n    import nltk\n    \n    # 1. Remove HTML.\n    html_text = BeautifulSoup(raw_review,\"html.parser\").get_text()\n    \n    # 2. Remove non-letters.\n    letters = re.sub(\"[^a-zA-Z]\", \" \", html_text)\n    \n    # 3. Convert to lower case.\n    letters = letters.lower()\n    \n    # 4. Tokenize.\n    tokens = nltk.word_tokenize(letters)\n    \n    # 5. Convert the stopwords list to \"set\" data type.\n    stops = set(nltk.corpus.stopwords.words(\"english\"))\n    \n    # 6. Remove stop words. \n    words = [w for w in tokens if not w in stops]\n    \n    # 7. Stemming\n    words = [nltk.stem.SnowballStemmer('english').stem(w) for w in words]\n    \n    # 8. Join the words back into one string separated by space, and return the result.\n    return \" \".join(words)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ff0cb1f3162a0246af833fafcee18f4e2cb5ee"},"cell_type":"code","source":"# Add the processed data to the original data. Perhaps using apply function would be more elegant and concise than using for loop\ntrain['clean'] = train['review'].apply(cleaning)\ntest['clean'] = test['review'].apply(cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91508370b1ccdc87410e26d35c7ef7e45f6a351c"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49dc83a2b77dfc8fcf8d9275e2a131ff37fe5ba1"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c638d3d5e73a32b36802eb328a6c03260ae4d73e"},"cell_type":"markdown","source":"## 4. Visualization <a id='visu'></a>\n"},{"metadata":{"_uuid":"3bb6cdde2711b702bb11bfaf09f8c6e68032e95e"},"cell_type":"markdown","source":"### 4.1 WordCloud <a id='wc'></a>\n\nAs a tool for visualization by using the frequency of words appeared in text, we use WordCloud. Note that it can give more information and insight of texts by analyzing correlations and similarities between words rather than analyzing texts only by the frequency of words appeared; however, it can give you some general shape of what this text is about quickly and intuitively. "},{"metadata":{"trusted":true,"_uuid":"63e8aad8ab04efac2074042e9366e9547f27da9a"},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b23f86d170aa5a345f21dcae74c8a74759021862"},"cell_type":"code","source":"def cloud(data,backgroundcolor = 'white', width = 800, height = 600):\n    wordcloud = WordCloud(stopwords = STOPWORDS, background_color = backgroundcolor,\n                         width = width, height = height).generate(data)\n    plt.figure(figsize = (15, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d46f18dcc93313219a05725d59a3dc23e109c672"},"cell_type":"code","source":"cloud(' '.join(train['clean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bcbc407d65d05cf2557ef19ee547dc91d3554a2"},"cell_type":"code","source":"cloud(' '.join(test['clean']))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"aba4a922ba9dda652333380551de28cb027e8baf"},"cell_type":"markdown","source":"It is not surprising that the most of large words are just the words frequently appeared in the text."},{"metadata":{"_uuid":"3fe698198fd4cec55bb3115a1cc0ae16fcd1fc80"},"cell_type":"markdown","source":"### 4.2 Distribution <a id='dist'></a>"},{"metadata":{"trusted":true,"_uuid":"a22796ac45fce4949043787dcbe4b08d1cefcc4a"},"cell_type":"code","source":"# We need to split each words in cleaned review and then count the number of each rows of data frame.\n\ntrain['freq_word'] = train['clean'].apply(lambda x: len(str(x).split()))\ntrain['unique_freq_word'] = train['clean'].apply(lambda x: len(set(str(x).split())))\n                                                 \ntest['freq_word'] = test['clean'].apply(lambda x: len(str(x).split()))\ntest['unique_freq_word'] = test['clean'].apply(lambda x: len(set(str(x).split())))                                                 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e55944f1df99ac1b02ec09722639e940d44e29a"},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2)\nfig.set_size_inches(10,5)\n\nsns.distplot(train['freq_word'], bins = 90, ax=axes[0], fit = stats.norm)\n(mu0, sigma0) = stats.norm.fit(train['freq_word'])\naxes[0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu0, sigma0)],loc='best')\naxes[0].set_title(\"Distribution Word Frequency\")\naxes[0].axvline(train['freq_word'].median(), linestyle='dashed')\nprint(\"median of word frequency: \", train['freq_word'].median())\n\n\nsns.distplot(train['unique_freq_word'], bins = 90, ax=axes[1], color = 'r', fit = stats.norm)\n(mu1, sigma1) = stats.norm.fit(train['unique_freq_word'])\naxes[1].set_title(\"Distribution Unique Word Frequency\")\naxes[1].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu1, sigma1)],loc='best')\naxes[1].axvline(train['unique_freq_word'].median(), linestyle='dashed')\nprint(\"median of uniuqe word frequency: \", train['unique_freq_word'].median())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c72e9cee60e265a748eb8fb5b01f48cfa5d58ae7"},"cell_type":"markdown","source":"The black contour of the distribution graphs represent the normal distribution if the data would have been distributed as normal. Compared to the black contour, the actual distribution is pretty skwed; therefore, median would be better to use as a measure of representative of data since mean is very sensitive to outliers and noise especially the distribution is highly skewed. As shown in the legend, the mean of the word frequency is 119.50 and the mean of the unique word is 94.04. It means 119.50 words and 94.04 unique words are used for each review. Also the dashed lines represent the median of the distribution. Another thing to notice is that the median values are very closely located to the normal distribution's mean points."},{"metadata":{"_uuid":"7bc5bdf9f4e2e933a81e484d3db376a501112b02"},"cell_type":"markdown","source":"## 5. Bag of Words <a id='bag'></a>\n\nEven though we cleaned the data with many steps, we still have one more step to create machine learning-friendly input. One common approach is called a Bag of Words. It is simply the matrix that counts how many each word appears in documents (disregard grammar and word order). In order to do that, we use \"CountVectorizer\" method in sklearn library. As you know already, the number of vocabulary is very large so it is important to limit the size of the feature vectors. In this project, we use the 18000 most frequent words. Also, the other things to notice is that we set min_df = 2 and ngram_range = (1,3). min_df = 2 means in order to include the vocabulary in the matrix, one word must appear in at least two documents. ngram_range means we cut one sentence by number of ngram. Let's say we have one sentence, I am a boy. If we cut the sentence by digram (ngram=2) then the sentence would be cut like this [\"I am\",\"am a\", \"a boy\"]. The result of accuracy can be highly dependent on parameters so feel free to alter them and see if you can improve the score."},{"metadata":{"trusted":true,"_uuid":"196b65d01100762bf8ecb67b3b875454d7ccbaa0"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb1d1910ae5ad432ed34a183590c1fef33d5df4e"},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer = \"word\", \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 18000,\n                             min_df = 2,\n                             ngram_range = (1,3)\n                            )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acb9993ede83d5754b64f1243726af370c3b3b6c"},"cell_type":"markdown","source":"As mentioned many times, the matrix is going to be huge so it would be a good idea to use Pipeline for encapsulating and avoiding a data leakage."},{"metadata":{"trusted":true,"_uuid":"32b2701f2b27af1bbc99524b6d7c3e80b2587a83"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"017f78ff8880d101bf02f44b886efeea842b82a3"},"cell_type":"code","source":"pipe = Pipeline( [('vect', vectorizer)] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd1d708343a5b067251e366bde406025b26d439e"},"cell_type":"code","source":"# Complete form of bag of word for machine learning input. We will be using this for machine learning algorithms.\n\ntrain_bw = pipe.fit_transform(train['clean'])\n\n# We only call transform not fit_transform due to the risk of overfitting.\n\ntest_bw = pipe.transform(test['clean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fbe5da97585f922792541e8fcd7796aa6fbe516"},"cell_type":"code","source":"print('train dim:', train_bw.shape, 'test dim:', test_bw.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9e91791fec1f0a0cc66a9dc5ff4a439beda74c1"},"cell_type":"code","source":"# Get the name fo the features\n\nlexi = vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de91795a54317de7ad6e7ed2eedf135816eea3dc"},"cell_type":"code","source":"lexi[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8b0994e7830154e2d69ea8709ecb0e9ffb8be61"},"cell_type":"code","source":"# Instead of 1 and 0 representation, create the dataframe to see how many times each word appears (just sum of 1 of each row)\n\ntrain_sum = pd.DataFrame(np.sum(train_bw, axis=0), columns = lexi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90cc8c655715d6ebf0391ad3f75ed2dc3f30869d"},"cell_type":"code","source":"train_sum.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db1c6a958c4ec84ea3762edd2f1d73b536fec895"},"cell_type":"markdown","source":"## 6. Modeling <a id='modeling'></a>\n\nAs text data usually is very sparse and has a high dimensionality, using linear, and simple models such as Linear Support Vector Machine, Bernoulli Naive Bayes, Logistic Regression or MultiLayer Perceptron would be better choice rather than using Random Forest. "},{"metadata":{"trusted":true,"_uuid":"c74d7a07b0137c5a70dde40dd55f3e15f3d9fd87"},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c470211635ea4e90facb42b75128cfff565f6c8"},"cell_type":"code","source":"kfold = StratifiedKFold( n_splits = 5, random_state = 2018 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae18467307f7289a20042e4d8d75db947e320047"},"cell_type":"markdown","source":"### 6.1 Support Vector Machine <a id='svm'></a>"},{"metadata":{"trusted":true,"_uuid":"ed858417d6c13cb4200fdc08c4e048d42a284821"},"cell_type":"code","source":"# LinearSVC\n\nsv = LinearSVC(random_state=2018)\n\nparam_grid2 = {\n    'loss':['hinge'],\n    'class_weight':[{1:1}],\n    'C': [0.01]\n}\n\ngs_sv = GridSearchCV(sv, param_grid = [param_grid2], verbose = 1, cv = kfold, n_jobs = -1, scoring = 'roc_auc' )\ngs_sv.fit(train_bw, train['sentiment'])\ngs_sv_best = gs_sv.best_estimator_\nprint(gs_sv.best_params_)\n\n# {'C': 0.01, 'class_weight': {1: 1}, 'loss': 'hinge'} - 0.88104","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fe864ec7216f348d81b96b50eb3c026680d22a9"},"cell_type":"code","source":"submission1 = gs_sv.predict(test_bw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90640ed24a2397140fe9ca9d1cd943a618145d7f"},"cell_type":"code","source":"print(gs_sv.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da669d745521b94826b9fbad210b41f40a7d6bd3"},"cell_type":"markdown","source":"### 6.2 Bernoulli Naive Bayes Classifier <a id='bnb'></a>"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8750ab629dcbf8817996db4bdc405e46386b712c"},"cell_type":"code","source":"bnb = BernoulliNB()\ngs_bnb = GridSearchCV(bnb, param_grid = {'alpha': [0.03],\n                                         'binarize': [0.001]}, verbose = 1, cv = kfold, n_jobs = -1, scoring = 'roc_auc')\ngs_bnb.fit(train_bw, train['sentiment'])\ngs_bnb_best = gs_bnb.best_estimator_\nprint(gs_bnb.best_params_)\n\n# {'alpha': 0.1, 'binarize': 0.001} - 0.85240\n# {'alpha': 0.03, 'binarize': 0.001} - 0.85240","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e17e398751c3f64b680351e151bdfe715178054a"},"cell_type":"code","source":"submission2 = gs_bnb.predict(test_bw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"020f2cc5f226a053bfa6e4163d62e006bbfb7de6"},"cell_type":"code","source":"print(gs_bnb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ae7dc5baef4ded740b8d68a0daacde1cd20429a"},"cell_type":"markdown","source":"### 6.4 Logistic Regression <a id='logi'></a>"},{"metadata":{"trusted":false,"_uuid":"fa1e43bbd184fa5ccd911269646d3fad8ec78d6c","collapsed":true},"cell_type":"code","source":"lr = LogisticRegression(random_state = 2018)\n\n\nlr2_param = {\n    'penalty':['l2'],\n    'dual':[False],\n    'C':[0.05],\n    'class_weight':['balanced']\n    }\n\nlr_CV = GridSearchCV(lr, param_grid = [lr2_param], cv = kfold, scoring = 'roc_auc', n_jobs = -1, verbose = 1)\nlr_CV.fit(train_bw, train['sentiment'])\nprint(lr_CV.best_params_)\nlogi_best = lr_CV.best_estimator_\n\n\n# {'C': 0.1, 'class_weight': 'balanced', 'dual': False, 'penalty': 'l2'} - 0.87868\n# {'C': 0.05, 'class_weight': 'balanced', 'dual': False, 'penalty': 'l2'} - 0.88028","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"4a40d84a9d501e237a1aa4101e83b3e561fd5347"},"cell_type":"code","source":"submission4 = lr_CV.predict(test_bw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"30b240eb0f78709a646078b227d4d5acd3ee4c90","collapsed":true},"cell_type":"code","source":"print(lr_CV.best_score_)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}