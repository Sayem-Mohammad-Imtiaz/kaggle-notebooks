{"cells":[{"metadata":{"_uuid":"850d468227645d451509f07cb1e4d70f86612d39"},"cell_type":"markdown","source":"\n**Wstęp**\n\nAnalizujemy dane dotyczące użytkowników Twittera.\n\n*Co udało się zrobić?*\n\n\n1.     Popracować z różnymi klasyfikatorami i przygotować sobie metodę do sprawnej analizy: \n         \n             train_model (classifier, X_train, y_train,X_test, y_test):\n\nGdzie:\n\n     class classifier:\n\n          def fit\n              ...\n\n          def predict\n              ...\n\n          def  predict_proba\n               ...\n\n\nPrzyklady:\n\n* MLP Multi-layer Perceptron classifier\n* KNeighborsClassifier\n* Random Forests\n\n\n2.  Przeprowadzić analizę tekstu\n\n * cleaning\n * Stemming\n * stopwords\n * wektoryzacja (unigramy, n-gramy)\n * MultinomialNB()\n * TF-IDF  Term frequency - Inverse document frequency\n\n3.  Modularne Sieci Neuronowe - Modular neural network\n\n\n*      voting strategy\n*   myEnseblePredict(estimators, data, voting=\"Fuzzy voting\")\n\n\n4. Ocena modelu\n\n\n\n*   Null accuracy\n\n\n\n\n\n\n\n"},{"metadata":{"_uuid":"032b2a22185edc34b419f8613e6eb7ea7613a45d"},"cell_type":"markdown","source":"**Ładowanie bibliotek**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.get_backend()\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn import decomposition, ensemble\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import  precision_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # for bag of words\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import normalize\nimport operator\nfrom functools import reduce\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e47ed3b86548cc7c237f2675b853a6b0f06c192b"},"cell_type":"markdown","source":"**Ładowanie danych**\n\nWszystkie dane z podsumowaniem."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"metadata = pd.read_csv('../input/gender-classifier-DFE-791531.csv', encoding='latin1')\n\nprint(metadata.info())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c04d2123b640cee168c8e90551892e4d18ad0eba"},"cell_type":"markdown","source":"**Wybieramy cechy do analizy**\n\nWybieramy do analizy kolumny:\n\n|Kolumna | Opis|\n|--|--|\n|  _golden   |                       czy klient ma konto gold|\n| _trusted_judgments  |           number of trusted judgments|\n|  gender:\t        |                        one of male, female, or brand (for non-human profiles)|\n|  gender_confidence\t  |      a float representing confidence in the provided gender|\n|  profile_yn:confidence |      confidence in the existence/non-existence of the profile|\n|retweet: \t      |                number of times the user has retweeted (or possibly, been retweeted)| \n|  text:    \t    |                    text of a random one of the user's tweets|\n|  tweet_count:       |      \t number of tweets that the user has posted|\n|  description: \t  |           the user's profile description|\n|  fav number:       |       \t number of tweets the user has favorited |\n| link_color| |\n| sidebar_color| | \n"},{"metadata":{"trusted":true,"_uuid":"7ba0fd63f19247626c4485014c4ae518dde9232a"},"cell_type":"code","source":"data_row = pd.read_csv('../input/gender-classifier-DFE-791531.csv',usecols= [1,3,5,6,8,10,11,13,17,18,19,21],encoding='latin1')\ndisplay(data_row.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4abac6da9f7ecbda481c0a471baa2973c66eef0e"},"cell_type":"markdown","source":"**Czyszczenie danych**\n\nUsuwamy elementy o nieznanym gender i gender:confidence wiekszym niz 90%.\n"},{"metadata":{"trusted":true,"_uuid":"7ca9e5bfacd9335b29f93bdfb859b9b2bb3735d8"},"cell_type":"code","source":"\ndata = data_row.where((data_row['gender:confidence'] > 0.9) & (data_row['gender'] != 'unknown'))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5211c3c341270d8bface5d6352ddcbc48579789"},"cell_type":"markdown","source":"Zmieniamy etykiety na liczby:\n\n'male'    $ \\Rightarrow$ 0, \n\n'female'    $ \\Rightarrow$ 1, \n\nbrand'    $ \\Rightarrow$ 2"},{"metadata":{"trusted":true,"_uuid":"6a5f39de113166c472de56570c75cfa0ea91b4b3","scrolled":true},"cell_type":"code","source":"\nprint(\"Zmieniam gender: {'male':0, 'female':1, 'brand':2}\")\nprint(data.gender.head(5))\ndata['gender_label'] = data.gender.map({'male':0, 'female':1, 'brand':2})\nprint(data.gender_label.head(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2f3f32005a8272d9edf2acda767e9460c2e7894"},"cell_type":"markdown","source":"**Kolory**\n\n\n\nBędziemy dokonywać predykcji gender na podstawie dwu kolorów:\n\n    koloru panelu bocznego\n\n    koloru linku\n\nRozpoczynamy od przekształcenia koloru w wersji #RRGGBB  na  (r,g,b)"},{"metadata":{"trusted":true,"_uuid":"7b19cf106db5b92c3169ff51cd8779d5fbb2165b"},"cell_type":"code","source":"#------------------------Kolory-------------------------\n\ndef hexToRGB(color):\n    if color == '0':\n        return 255,255,255\n    if len(color)<5:\n        return None, None, None\n    try:\n        r=int(color[0:2],16)\n        g=int(color[2:4],16)\n        b=int(color[4:6],16)\n    except (RuntimeError, TypeError, NameError, ValueError):\n        return None, None, None\n    else:\n        return r,g,b\n    \n    \n\nprint(\"Zmeniam kolory z postaci #RRGGBB w wersji hex na (rr,gg,bb) w wersji dec\")\nprint(data[\"link_color\"].head(5))\ndata[\"rl\"] = data[\"link_color\"].apply(lambda x: hexToRGB(str(x))[0])\ndata[\"gl\"] = data[\"link_color\"].apply(lambda x: hexToRGB(str(x))[1])\ndata[\"bl\"] = data[\"link_color\"].apply(lambda x: hexToRGB(str(x))[2])\ndata[\"rs\"] = data[\"sidebar_color\"].apply(lambda x: hexToRGB(str(x))[0])\ndata[\"gs\"] = data[\"sidebar_color\"].apply(lambda x: hexToRGB(str(x))[1])\ndata[\"bs\"] = data[\"sidebar_color\"].apply(lambda x: hexToRGB(str(x))[2])\nprint(data[['rl', 'gl','bl']].head(5))\ndata.dropna(inplace=True,axis=0)\n\n\n\nmale_top_sidebar_color = data[data['gender'] == 'male']['sidebar_color'].value_counts().head(10)\nmale_top_sidebar_color_idx = male_top_sidebar_color.index\nmale_top_color = male_top_sidebar_color_idx.values\n\nmale_top_color[1] = '000000'\nprint (male_top_color)\nl = lambda x: '#'+x\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#F5ABB5\"})\nplot3 = sns.barplot (x = male_top_sidebar_color, y = male_top_color, palette=list(map(l, male_top_color)))\nfig3 = plot3.get_figure()\nfig3.savefig(\"male_colours.jpg\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15933d20f620b419f454593e6b0a08747a992cae"},"cell_type":"code","source":"female_top_sidebar_color = data[data['gender'] == 'female']['sidebar_color'].value_counts().head(10)\nfemale_top_sidebar_color_idx = female_top_sidebar_color.index\nfemale_top_color = female_top_sidebar_color_idx.values\n\nfemale_top_color[2] = '000000'\nprint (female_top_color)\nl = lambda x: '#'+x\n\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#F5ABB5\"})\nplot4 =sns.barplot (x = female_top_sidebar_color, y = female_top_color, palette=list(map(l, female_top_color)))\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \"#FFFFFF\"})\nfig4 = plot4.get_figure()\nfig4.savefig(\"female_colours.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92dd070bfe4a03c2f9bade9fb971990f10d1ca27"},"cell_type":"markdown","source":"Definuję funkcję train_model, która będzie za mnie robić uczenie."},{"metadata":{"trusted":true,"_uuid":"dd4e67d00409e3c35254ca7d1ee1f266415792b4"},"cell_type":"code","source":"\ndef train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y, is_neural_net=False):\n    \"\"\"Model training\n    Keyword arguments:\n    classifier -- Klasyfikator posiadajacy metody fit,predict, predict_proba\n    feature_vector_train -- X_train\n    label -- Y_train\n    feature_vector_valid -- x_val\n    valid_y -- y_val\n    Returns:\n    accuracy -- dokładność dopasowania,\n    predykcja_prawdopodobienstwa -- prawdopodobieństwo dopasowania dla każdej klasy,\n    predykcja -- numerklasy najbardziej prawdopodobnej,\n    przetrenowany klasyfikator.\"\"\"\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    #only for neural networks\n    if is_neural_net:\n        classifier.fit(feature_vector_train, label,epochs = 2)\n        predictions = predictions.argmax(axis=-1)\n\n    return metrics.accuracy_score(predictions, valid_y),  classifier.predict_proba(feature_vector_valid), precision_score(y_test, classifier.predict(feature_vector_valid),average=None), classifier\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23fb8ac9b65de820c06cc9b726eefa45550bbe33"},"cell_type":"markdown","source":"Podział na dane testowe i uczące.\n\n15% danych do testow\n85% do uczenia"},{"metadata":{"trusted":true,"_uuid":"6cdf23bc15eef5c361b22755017277be06b5fb39"},"cell_type":"code","source":"test_percentage = 0.15\nX_train, X_test, y_train, y_test = train_test_split(data[['rl','gl','bl','rs','gs','bs']], data['gender_label'], test_size=test_percentage, random_state=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58bf2d6ef87c68bc5612d848585b86f14bb2842b"},"cell_type":"markdown","source":"Model Perceptronowy Multi-layer Perceptron classifier.\n\nJedna warstwa HIDDEN.\n\nPróbujemy różnej liczby perceptronów za wzorem:\n\n    (no of inputs + no of outputs)^0.5 + (1 to 20)"},{"metadata":{"trusted":true,"_uuid":"efe33771c1f57c52d01ab658cb466e125aa425ae","scrolled":true},"cell_type":"code","source":"\nprint(\"Szukam najlepszej liczby perceptronow miedzy 10 a 19 za wzorem:\")\nhidden_min = 4\nhidden_layer_size = 20\nprint(\"(no of inputs + no of outputs)^0.5 + (1 to 10)\")\nmlp_accuracy = [0]*(hidden_min+hidden_layer_size)\nfor i in range(hidden_min,hidden_min+hidden_layer_size):\n    accuracy, Color_predictions, color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(i), early_stopping=True, random_state = np.random.RandomState(47)),\n                                                               X_train, y_train, X_test, y_test)\n    mlp_accuracy[i] = accuracy\n    print(\"N iter= \", color_classifier.n_iter_ )\n\nfig5, ax = plt.subplots()\nax.plot(range(hidden_min,hidden_min+hidden_layer_size), mlp_accuracy[hidden_min:hidden_layer_size+hidden_layer_size], 'o')\nplt.ylabel('accuracy')\nplt.xlabel('n_perceptrons')\nplt.title('MLPClassifier accuracy')\nplt.show()  \n\nfig5.savefig(\"mlpclassifier_accuracy.jpg\")\n\naccuracy, Color_predictions,color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(np.argmax(mlp_accuracy)), early_stopping=True, random_state = np.random.RandomState(47)), X_train, y_train, X_test,y_test)\nprint(\"MLPClassifier(hidden_layer_sizes={0}) Color accuracy: {1}  \".format(np.argmax(mlp_accuracy),accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f582184f37fb61aca41a45ae6a5628dea103e6f"},"cell_type":"code","source":"hidden_layer_size = 20\nprint(\"Szukam najlepszej liczby perceptronow w dwu warstwach miedzy 10 a \", hidden_layer_size, \" za wzorem:\")\n\nprint(\"(no of inputs + no of outputs)^0.5 + (1 to \", hidden_layer_size,\")\")\nmlp_accuracy = [0]*(hidden_layer_size+hidden_layer_size)\nfor i in range(hidden_layer_size,hidden_layer_size+hidden_layer_size):\n    accuracy, Color_predictions, color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(i,i), early_stopping=True, learning_rate='invscaling', random_state = np.random.RandomState(47)),\n                                                               X_train, y_train, X_test, y_test)\n    mlp_accuracy[i] = accuracy\n    print(\"N iter= \", color_classifier.n_iter_ )\n\nfig5, ax = plt.subplots()\nax.plot(range(hidden_layer_size,hidden_layer_size+hidden_layer_size), mlp_accuracy[hidden_layer_size:hidden_layer_size+hidden_layer_size], 'o')\nplt.ylabel('accuracy')\nplt.xlabel('n_perceptrons')\nplt.title('MLPClassifier accuracy')\nplt.show()  \n\nfig5.savefig(\"mlpclassifier_accuracy_2_layers.jpg\")\n\naccuracy, Color_predictions,color_precision, color_classifier = train_model(MLPClassifier(hidden_layer_sizes=(np.argmax(mlp_accuracy),np.argmax(mlp_accuracy)), early_stopping=True,learning_rate='invscaling', random_state = np.random.RandomState(47)), X_train, y_train, X_test,y_test)\nprint(\"MLPClassifier(hidden_layer_sizes={0}) Color accuracy: {1}  \".format(np.argmax(mlp_accuracy),accuracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a20fbae64d04a6901ec53df02c1959b80a08c75e"},"cell_type":"markdown","source":"Powtarzam zabawę dla metody Nearest Neighbours,\n\nposzukuję najlepszej liczby sąsiadów między 1 a 20."},{"metadata":{"trusted":true,"_uuid":"40bedef195fb963bec3019fd579e632c006b25fe"},"cell_type":"code","source":"max_neighbours = 40\nprint(\"Szukam najlepszej liczby sąsiadów miedzy 1 a \", max_neighbours)\n\nneighbours_accuracy = [0]*max_neighbours\nfor i in range(1,max_neighbours):\n    accuracy, Color_predictions, color_precision, color_classifier = train_model(KNeighborsClassifier(i), X_train, y_train, X_test,y_test)\n    neighbours_accuracy[i] = accuracy\n    \nfig6, ax = plt.subplots()\nax.plot(range(1,max_neighbours), neighbours_accuracy[1:max_neighbours], 'o')\nplt.ylabel('accuracy')\nplt.xlabel('n_neighbours')\nplt.title('KNeighborsClassifier accuracy')\nplt.show() \naccuracy, Color_predictions, color_precision, color_classifier = train_model(KNeighborsClassifier(np.argmax(neighbours_accuracy)), X_train, y_train, X_test,y_test)\nprint(\"KNeighborsClassifier({0}) Color accuracy:  {1} \".format(np.argmax(neighbours_accuracy), accuracy))\nfig5.savefig(\"KNeighborsClassifier_accuracy.jpg\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b83bada0693c53f62447dc776d9c1d969ffd699"},"cell_type":"markdown","source":"**Analiza pozostałych features**"},{"metadata":{"trusted":true,"_uuid":"5016d071fb549aad2168496e762de6bfbcbabf1f"},"cell_type":"code","source":"#----------------------------- Other Features --------------------------------\n\n# other_features = data [[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]]\nX_train, X_test, y_train, y_test = train_test_split(normalize(data[[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]], axis=0), data['gender_label'], test_size=test_percentage, random_state=20)\n\n\nprint(\"Szukam najlpeszej liczby perceptronow miedzy 10 a 19 za wzorem:\")\nhidden_layer_size = 10\nprint(\"(no of inputs + no of outputs)^0.5 + (1 to 10)\")\nmlp_accuracy = [0]*(hidden_layer_size+9)\nfor i in range(hidden_layer_size,hidden_layer_size+9):\n    accuracy, Other_Features, other_precision, other_classifier = train_model(\n        MLPClassifier(hidden_layer_sizes=(i), max_iter=200, early_stopping = True, random_state = np.random.RandomState(47)), X_train, y_train, X_test, y_test)\n    mlp_accuracy[i] = accuracy\n\naccuracy, Other_Features, other_precision, other_classifier = train_model(MLPClassifier(hidden_layer_sizes=(np.argmax(mlp_accuracy)),max_iter=200, early_stopping = True, random_state = np.random.RandomState(47)), X_train, y_train, X_test,y_test)\nprint(\"MLPClassifier(hidden_layer_sizes={0}) Other_Features accuracy: {1}  \".format(np.argmax(mlp_accuracy),accuracy))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b4e22c6f192b730d40ec790d68a705e16d8e07b"},"cell_type":"markdown","source":"**Analiza tekstu**"},{"metadata":{"trusted":true,"_uuid":"9ab7ea96c7e8f69a7d17921565b2cda9f90bff92"},"cell_type":"code","source":"def cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = s.replace(\"'s\",' is')\n    s = s.replace(\"'re\",' are')\n    s = s.replace(\"'ve\",' have')\n    s = re.sub('\\s\\W',' ',s) #whitespace characters\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"ù\",\"\")\n    s = s.replace(\"ù\", \"\")\n    s = s.replace(\"û\", \"\")\n    s = s.replace(\"âù\", \"\")\n    s = s.replace(\"ü\", \"\")\n    s = s.replace(\"å\", \"\")\n    s = s.replace(\"â\", \"\")\n    s = s.replace(\"ä\", \"\")\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s\n\n\n\nprint(\"Przykladowy wpis: \")\nprint(data.text.get(64))\nprint(\"\\n\")\ndata['Tweets'] = [cleaning(s) for s in data['text']]\ndata['Description'] = [cleaning(s) for s in data['description']]\n\nprint(\"Przykladowy wpis poczyszczeniu ze znakow specjalnych: \")\nprint(data.Tweets.get(64))\n\nprint(\"\\n\")\n\ndata.dropna(inplace=True,axis=0) # pozbywam sie wierszy bez danych NaN","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33b22858db11d6ef9b3ed7ec63e44e861083604f"},"cell_type":"markdown","source":"**Stemming**\n\n\nPrzeksztacamy odmienione słowa na formy podstawowe:\n\n\nam, are, is $\\Rightarrow$ be\n\n\ncar, cars, car's, cars' $\\Rightarrow$ car "},{"metadata":{"trusted":true,"_uuid":"d388073eab2d1874d2292411de2c317edfcb0078"},"cell_type":"code","source":"def stop_words(data, column_name,splitted=False):\n    #funcion deleting stop words \n    \n    stop = set(stopwords.words('english'))\n    notInStopSet = lambda word: word not in stop\n    if splitted == False:\n        data[column_name] = data[column_name].str.lower().str.split()\n\n    data[column_name] = data[column_name].apply(lambda x: [item for item in x if notInStopSet(item)])\n\n    if splitted == False:\n        data[column_name] = data[column_name].apply(lambda x: ' '.join(x))\n        \ndef stemming(data, column_name,splitted=False):\n    #funcion stemming \n    sno = nltk.stem.SnowballStemmer('english')\n    stop = set(stopwords.words('english'))\n    notInStopSet = lambda word: word not in stop\n    if splitted == False:\n        data[column_name] = data[column_name].str.lower().str.split()\n\n    data[column_name] = data[column_name].apply(lambda x: [sno.stem(item) for item in x if notInStopSet(item)])\n\n    if splitted == False:\n        data[column_name] = data[column_name].apply(lambda x: ' '.join(x))\n\n        \nstop_words(data, 'Tweets')\nstop_words(data, 'Description')\n\nprint(\"Przykladowy wpis  usunieciu stop words: \")\nprint(data.Tweets.get(64))\nprint(\"\\n\")        \n        \nstemming(data, 'Tweets')\nstemming(data, 'Description')\n\nprint(\"Przykladowy wpis po Stemmingu \")\nprint(data.Tweets.get(64))\nprint(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"615743cfeaa9db21f93574d3e9f87a6c07ae25eb"},"cell_type":"markdown","source":"Oglądamy dane:"},{"metadata":{"trusted":true,"_uuid":"d47ead9d878b214155a1557aba91d40a417a182e","scrolled":true},"cell_type":"code","source":"Male = data[data['gender_label'] == 0]\nFemale = data[data['gender_label'] == 1]\nBrand = data[data['gender_label'] == 2]\nMale_Words = pd.Series(' '.join(Male['Tweets'].astype(str)).lower().split(\" \")).value_counts()[:20]\nFemale_Words = pd.Series(' '.join(Female['Tweets'].astype(str)).lower().split(\" \")).value_counts()[:20]\nBrand_words = pd.Series(' '.join(Brand['Tweets'].astype(str)).lower().split(\" \")).value_counts()[:20]\nplot0 = Female_Words.plot(kind='bar',stacked=True, colormap='OrRd', title='Most used words by female users')\nfig0 = plot0.get_figure()\nfig0.savefig(\"female_words.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c007343e0d8acb2db0462fd3d75056f42f42e491"},"cell_type":"code","source":"plot1 = Male_Words.plot(kind='bar',stacked=True, colormap='plasma', title='Most used words by male users' )\nfig1 = plot1.get_figure()\nfig1.savefig(\"male_words.jpg\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"071ea57d637730f048a1c90a4c072e129282e85f"},"cell_type":"code","source":"plot2 = Brand_words.plot(kind='bar',stacked=True, colormap='Paired', title='Most used words by brands')\nfig2 = plot2.get_figure()\nfig2.savefig(\"brand_words.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"801cd360f6de170248dcfd0db9cbe1596482fa10"},"cell_type":"markdown","source":"\n\nBędziemy głównie korzystać z *naiwnej metody Bayesa.*\nW skrócie:\n\nZliczamy ile jest różnych słów:\n\nPrzykład:\n\n    “A great game”            \n\n    “A clean but forgettable game” \n\nPo wektoryzacji unigramowej:\n\n\n                                       A   great  game  clean  but  forgettable  \n    “A great game”                    [1    1     1      0     0       0     ]\n\n    “A clean but forgettable game”    [1    0     1      1     1       1     ]\n\n\n"},{"metadata":{"trusted":true,"_uuid":"84873a6ad7e466c0665a12a740eaecbb360aaffc"},"cell_type":"code","source":"#------------------------TEXT ANALYSIS------------------------------------\n\n\ny_all = data.gender_label\nX_Tweets = data.Tweets\nX_Description = data.Description\n\n\nX_Tweets_train, X_Tweets_test, y_train, y_test = train_test_split(X_Tweets, y_all, test_size=test_percentage, random_state=20)\nX_Description_train, X_Description_test, y_train, y_test = train_test_split(X_Description, y_all, test_size=test_percentage, random_state=20)\n# print(y_test)Usredniona 64%\n\n#Unigramy\n\n\ndef vektoriser(X_train, X_test,ngram = False):\n    if ngram == False:\n        vect= CountVectorizer(analyzer='word', token_pattern=r'[0-9a-zA-Z]{2,}',min_df=2)\n    else:\n        vect = CountVectorizer(analyzer='word', token_pattern=r'[0-9a-zA-Z]{2,}', ngram_range=(1, 2),min_df=2)\n    vect.fit(X_train)\n    return vect.transform(X_train), vect.transform(X_test), vect\n\n# print(\"Typ danych = \" , type(X_Tweets_test))\nX_Tweets_train_dtm, X_Tweets_test_dtm, tweets_vect = vektoriser(X_Tweets_train, X_Tweets_test)\nprint(len(tweets_vect.vocabulary_))\n\nX_Description_train_dtm, X_Description_test_dtm, description_vect = vektoriser(X_Description_train,X_Description_test)\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63d54ca8398841bdb2720132d2340a3a180cd4d7"},"cell_type":"markdown","source":"*Naiwna metoda Bayesa*\n\n   | Text         |        \t      Tag|\n       |--|--|\n    |“A great game”            \t|Male|\n    |“The election was over”       | Female|\n    |“Very clean match”           \t|Male|\n    |“A clean but forgettable game” \t|Male|\n    |“It was a close election” \t  | Female|\n    \nChemy policzyć prawdopodobieństwo, że słowa: \"A very close game\" wypowiedział mężczyzna\n$$P(Male |\\; A\\;very\\; close\\; game) = \\frac{P( A\\;very\\; close\\; game |\\; Male) \\cdot P(Male)}{P(A\\;very\\; close\\; game )}$$\n\nZależy nam jednak jedynie aby porównać prawdopodobieństwo, że powiedział to mężczyzna z prawdopodobieństwoem, żę zrobiła to kobieta lub wpis nie pochodził od człowieka. Pomijamy zatem mianownik.\n\n\nLiczymy jedynie: $$P( A\\;very\\; close\\; game |\\; Male) \\cdot P(Male)$$\n\n\nTeraz wyjaśni się, dlaczego metoda jest \"naiwna\", otóż przyjmuje się, że:\n$$P( A\\;very\\; close\\; game |\\; Male) = P(A |\\; Male) \\cdot P(very |\\; Male) \\cdot P(close |\\; Male) \\cdot P(game |\\; Male)$$\n\n    \n    \n    \n    \n    "},{"metadata":{"trusted":true,"_uuid":"fb38c4899efbc0bcf5958e2f546cec82a7ff7620","scrolled":false},"cell_type":"code","source":"\n\naccuracy, Tweets_predictions, tweet_precision, tweet_classifier = train_model(MultinomialNB(), X_Tweets_train_dtm, y_train, X_Tweets_test_dtm, y_test)\nprint(\"NB, Tweets WordLevel:  \", accuracy)\n\n\n\naccuracy, Description_predictions, description_precision,description_classifier = train_model(MultinomialNB(), X_Description_train_dtm, y_train, X_Description_test_dtm,y_test)\nprint(\"NB, Description WordLevel:  \", accuracy)\n\nprobs = np.transpose(description_classifier.feature_log_prob_)\nwords = list(description_vect.vocabulary_.keys())\n# print(words)\nprobs = description_classifier.predict_proba(description_vect.transform(words))\nnot_log_probabs = [[i for i in x] for x in probs]\n# print(not_log_probabs)\n\nall_gender_words = {x[1]: np.argmax(x[0])  for x in zip(probs,words) if max(x[0]) > 0.7}\nMost_masculine_words = {key:value for key, value in all_gender_words.items() if value == 0}\nprint(\"Meskie slowa\", Most_masculine_words.keys())\nMost_masculine_words = {key:value for key, value in all_gender_words.items() if value == 1}\n\nprint('Kobiece slowa',  Most_masculine_words.keys())\nMost_masculine_words = {key:value for key, value in all_gender_words.items() if value == 2}\n\nprint('Brandowe slowa', Most_masculine_words.keys())\n\nimport sys\nfrom termcolor import colored, cprint\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"706223898e7312fc2e2a7af163239228bec6d466"},"cell_type":"code","source":"def print_colorful_tweet(Tweet):\n     \n    if type(Tweet) == str:\n#         print('size == 1')\n        input_frase = Tweet.split()\n    else:\n#         print('size != 1')\n        input_frase = Tweet\n        \n    colors = {0:'\\033[34m', 1:'\\033[95m', 2:'\\033[32m'} # blue, pink, green\n    frase = str()\n    if not input_frase:\n        return \"\"\n    for word in input_frase:\n        if word in all_gender_words:\n            gender = all_gender_words.get(word)\n#             print(gender)\n            color = colors.get(gender)\n#             print(color)\n            frase += colors.get(gender) + word + '\\x1b[0m'+ \" \" \n        else:\n            frase += word + \" \"\n            pass\n    print(frase)\n\nprint_colorful_tweet(data.Description.get(64))\nprint_colorful_tweet(data.Description.get(75))\nprint_colorful_tweet(data.Description.get(94))\nprint_colorful_tweet(data.Description.get(67))\nprint_colorful_tweet(data.Description.get(86))\nprint_colorful_tweet(data.Description.get(84))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"192b99738149763fcd9cd6a3139b5b07596ba6ee"},"cell_type":"markdown","source":"Pewne ulepszenie do modelu można wprowadzić stosują n-gramy:\n\n* unigram (1-gram)\n\n|A| clean |but| forgettable| game  |\n|--|--|--|--|--|\n\n* bigram (2-gram)\n\n|A clean | clean but| but  forgettable| forgettable game  |\n|--|--|--|--|--|\n\n* trigram (3-gram)\n\n|A clean but | clean but  forgettable| but forgettable game  |\n|--|--|--|\n\n.\n\n"},{"metadata":{"trusted":true,"_uuid":"bbc7d98f9c3457c8428a33e7cd60112261a07079"},"cell_type":"code","source":"#Bigramy----------------------------------------------------------------------------------------------------------------------\n\nX_Tweets_train_dtm, X_Tweets_test_dtm, tweets_vect = vektoriser(X_Tweets_train, X_Tweets_test,ngram=True)\nX_Description_train_dtm, X_Description_test_dtm, description_vect = vektoriser(X_Description_train,X_Description_test,ngram=True)\n\n\naccuracy, Tweets_predictions, tweet_precision, tweet_classifier = train_model(MultinomialNB(), X_Tweets_train_dtm, y_train, X_Tweets_test_dtm,y_test)\nprint(\"NB, Tweets N-Gram Vectors:\", accuracy)\n# print(tweet_classifier.predict_proba((tweets_vect.transform(X_Tweets))[1]))\n\n\n\naccuracy, Description_predictions,description_precision,description_classifier  = train_model(MultinomialNB(), X_Description_train_dtm, y_train, X_Description_test_dtm,y_test)\nprint(\"NB, Description N-Gram Vectors:\", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c957d1f53497e8324a51100bc46b7964f90e477c"},"cell_type":"markdown","source":"*TF-IDF*  *Term frequency - Inverse document frequency\n*\n\n\n\n\n   | Text         |        \t      Tag|\n       |--|--|\n    |“A great game”            \t|Male|\n    |“The election was over”       | Female|\n    |“Very **clean** match”           \t|Male|\n    |“A **clean** but forgettable game” \t|Male|\n    |“It was a close election” \t  | Female|\n    \n    \n\nObliczanie Term frequency\n$$TF(clean, \\;A\\; clean\\; but\\; forgettable\\; game) =  20\\%$$\nObliczanie Inverse document frequency\n$$ IDF(clean) = \\ln{\\frac{wszystkich\\; wpisów}{wpisów\\; zawierających\\; słowo\\; \"clean\"}} =\\ln(\\frac{5}{2}) $$"},{"metadata":{"trusted":true,"_uuid":"deff3625339a2d0742e79ebb6569536c1628836b"},"cell_type":"code","source":"X_Tweets_train, X_Tweets_test, y_train, y_test = train_test_split(X_Tweets, y_all, test_size=test_percentage, random_state=20)\n\nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(y_train)\nvalid_y = encoder.fit_transform(y_test)\n\nxtrain_count =  X_Tweets_train_dtm\nxvalid_count =  X_Tweets_test_dtm\n\n\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n\n\n\ntfidf_vect.fit(X_Tweets_train)\n\nxtrain_tfidf =  tfidf_vect.transform(X_Tweets_train)\nxvalid_tfidf =  tfidf_vect.transform(X_Tweets_test)\n\n\n# print(tfidf_vect.vocabulary_)\n\n# ngram level tf-idf\ntfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram.fit(X_Tweets_train)\nxtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_Tweets_train)\nxvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_Tweets_test)\n\n# characters level tf-idf\ntfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\ntfidf_vect_ngram_chars.fit(X_Tweets_train)\nxtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_Tweets_train)\nxvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_Tweets_test)\n\n\n\n# Naive Bayes on Word Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)[0]\nprint(\"NB, Tweets WordLevel TF-IDF: \", accuracy)\n\n# Naive Bayes on Ngram Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)[0]\nprint(\"NB, Tweets N-Gram Vectors: \", accuracy)\n\n# Naive Bayes on Character Level TF IDF Vectors\naccuracy = train_model(MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)[0]\nprint(\"NB, Tweets CharLevel Vectors: \", accuracy)\n\n# Linear Classifier on Count Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count, valid_y)[0]\nprint(\"LR, Tweets Count Vectors: \", accuracy)\n\n# Linear Classifier on Word Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)[0]\nprint(\"LR, Tweets WordLevel TF-IDF: \", accuracy)\n\n# Linear Classifier on Ngram Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)[0]\nprint(\"LR, Tweets N-Gram Vectors: \", accuracy)\n\n# Linear Classifier on Character Level TF IDF Vectors\naccuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)[0]\nprint(\"LR, Tweets CharLevel Vectors: \", accuracy)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1fcee77458e560ebc2368df356e1117a32f898d"},"cell_type":"markdown","source":"**Random Forests**\n\n![obraz.png](https://blog.citizennet.com/hs-fs/hubfs/Imported_Blog_Media/RF.jpg?t=1536614411555&width=2356&name=RF.jpg)"},{"metadata":{"trusted":true,"_uuid":"b2a3af393c45a8aadb01c37bc23776c24292a97d"},"cell_type":"code","source":"# RF on Count Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count, valid_y)\nprint(\"RF, Count Vectors: \", accuracy)\n\n# RF on Word Level TF IDF Vectors\naccuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\nprint(\"RF, WordLevel TF-IDF: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec182a0b51d7fccfdbdfc7bf360fee6efd65ceb4"},"cell_type":"markdown","source":"**Modularne Sieci Neuronowe - Modular neural network**\n\n*“Voting”, *\n\n\nto najpopularniejsza strategia dla multi-module cooperation. \nOutput każdego modułu, wskazujący na konkretną klasę traktowany jest jak jeden głos. Decyzja podejmowana jest zgodnie z przyjętą strategią głosowania -  “voting strategy.” \n\n\nŻródło: https://pdfs.semanticscholar.org/721c/f173c00fff7af5edca9bd8976407591f647c.pdf\n\n*1.  Średnia ważona:*\n\n\nPrzykład:\n\nSieć A o dokładności 50%   [0.2, 0.6, 0.1]\nSieć B o dokładności 90%   [0.1, 0.5, 0.4]\n\nObliczamy: \n        50% * [0.2, 0.6, 0.1] + 90%  *  [0.1, 0.5, 0.4] =  [0.1, 0.3, 0.05] +  [0.09, 0.45, 0.36]\n \n        Wynik = [0.19,0.75,0.41]"},{"metadata":{"trusted":true,"_uuid":"b760816887d3704c348ad5310a836f383a6889b6"},"cell_type":"code","source":"def getPrediction(Probabs):\n    return [ np.argmax(x) for x in Probabs]\n\n\n\ntotal_Predictions = (tweet_precision*Tweets_predictions + description_precision*Description_predictions + color_precision*Color_predictions + other_precision*Other_Features)\ny_pred_class = getPrediction(total_Predictions)\n\n\nprint(\"Weighted mean bigrams: \" , metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f5b58b68f0d0df8a27f6b78d36e2f6b6ef01dbb"},"cell_type":"markdown","source":"Próbujemy pominąć najmniej efektywną sieć - Other Features:"},{"metadata":{"trusted":true,"_uuid":"964b054c72596d8ea6eb0220da5ea23a698520a2"},"cell_type":"code","source":"total_Predictions = (tweet_precision*Tweets_predictions + description_precision*Description_predictions + color_precision*Color_predictions)\ny_pred_class = getPrediction(total_Predictions)\nprint(\"Weighted mean bigrams: \" , metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5d4f0df484f3316af78dd250d922452b9a8ddaa"},"cell_type":"markdown","source":"2.  *Plurality \nvoting. *\n\n\nIt \nis \nthe \nmost  common \nvoting \nscheme. \nEach  voter  votes \nfor \none \nalternative, \nand the \nalternative  with \nthe \nlargest \nnumber \nof \nvotes  wins \n[9]. \nThe \nadvantage \nof \nthis \nscheme, \nfrom \nthe \nNN \nperspective,  is \nthat \nit \nonly \nuses \nthe \nhighest \noutput \nvalue, \nwhich \nis \nthe \nmost probable \noutput \nto \nbe true. \nHowever, \nit \ndoes \nnot \nconsider \nthe outputs’ \npreferences. \n\n\n\nPrzykład:\n\nSieć A                    [0.2, **0.6**, 0.1]\n\nSieć B                    [0.1, **0.5**, 0.4]\n\nSieć C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n         [0, 1, 0] +  [0, 1, 0] + [1, 0, 0] = [1, **2**, 0] \n \n        Wynik = [0 , 1 , 0]\n        \n 3. *Borda \ncount \nvoting. *\n\n\nFor \nm \nalternatives, \nassign \n \n\n1 \npoints \nfor \nthe \nalternative  ranked \nfirst, \n \n2 \npoints \nfor \nthe \nsecond, \n \nand \nso \non. \n\n\n\nThe \nalternative \nranked \nlast \nreceives \nzero \npoints. \nThe \npoints \ngiven \nto \nthe \ndifferent \nalternatives \n(modules) \nare summed \nup \nand \nthe \nhighest \nis \nthe \nwinner.\n\nPrzykład:\n\nSieć A                    [0.2, **0.6**, 0.1]\n\nSieć B                    [0.1, **0.5**, 0.4]\n\nSieć C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n         [1, 2 0] +  [0, 2, 1] + [2, 1, 0] = [3, **5**, 1] \n \n        Wynik = [0 , 1 , 0]\n        \n4. *Fuzzy voting.* \n \nEach  voter  assigns \na \nnumber \nbetween zero \nand \none \nfor each \ncandidate. \nCompare \nthe summation \nof \nthe \nvotes’ values for \nall \ncandidates. \nThe \nhigher \nis \nthe \nwinner.   And \nsince \nthe \nmodules’ \noutputs \nare \nnormally \nbetween \nthe \nzero \nand \none \nvalues, \nthe \nvalue \nwill \nrepresent \nthe true \nvoting bid.         \n\nPrzykład:\n\nSieć A                    [0.2, **0.6**, 0.1]\n\nSieć B                    [0.1, **0.5**, 0.4]\n\nSieć C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n          [0.2, **0.6**, 0.1] + [0.1, **0.5**, 0.4]+  [**0.6**, 0.3, 0.1]= [0.9, **1.4**, 0.6] \n \n        Wynik = [0 , 1 , 0]\n        \n5. *Nash \nvoting.* \nIt \nis similar \nto \nthe \nFuzzy \nvoting \nbut \ncompares \nthe \nproduct \nof \nthe \nvotes’ values \nfor \nall candidates. \nThe \nhigher \nis  the \nwinner. \n\n\nPrzykład:\n\nSieć A                    [0.2, **0.6**, 0.1]\n\nSieć B                    [0.1, **0.5**, 0.4]\n\nSieć C                    [**0.6**, 0.3, 0.1]\n\n\nObliczamy: \n          [0.2, **0.6**, 0.1] .* [0.1, **0.5**, 0.4].*  [**0.6**, 0.3, 0.1]= [0.012, **0.09**, 0.004] \n \n        Wynik = [0 , 1 , 0]\n        "},{"metadata":{"trusted":true,"_uuid":"6a7bdb0370fdca5e447a7a58fc72185e4ecb6c53"},"cell_type":"code","source":"\ndef myEnseblePredict(estimators, data, voting=\"Fuzzy voting\"):\n    \"\"\"Modular Neural Network Voting\n          zrodlo: Voting  Schemes For Cooperative  Neural  Network  Classifiers\n          https://pdfs.semanticscholar.org/721c/f173c00fff7af5edca9bd8976407591f647c.pdf\n          \n    Keyword arguments:\n    estimators -- lista par (Klasyfikator, kolumny) \n    data -- dane do predykcji/klasyfikacji\n    voting -- rodzaj głowsowania\n    \n    Returns:\n    \n    result -- klasyfikacja\n    \"\"\"\n    \n    def prod(iterable):\n        \"\"\"iloczyn elementów listy\"\"\"\n        return reduce(operator.mul, iterable, 1)\n    \n    def plurality(row):\n        \"\"\"wyostrzanie listy [0.2, 0.6, 0.2] -> [0, 1, 0]\"\"\"\n        maxIdx = np.argmax(row)\n        result = [0 for x in row]\n        result[maxIdx] = 1\n        return result\n\n    def borda(row):\n        \"\"\"zliczanie głosów typu border [0.1, 0.6, 0.3] -> [0, 2, 1]\"\"\"\n        count = len(row)\n        row_copy = row\n        maxIdx = [0]*count\n        result = [0 for x in row]\n        for  i in range(count-1):\n            maxIdx[i] = np.argmax(row_copy)\n            row_copy[maxIdx[i]] = 0\n        for i in range(count-1):\n            result[maxIdx[i]] = count - 1 - i\n        return result\n\n    votes = [[0,0,0] for x in data.index]\n    for el in estimators:\n        estimator = el[0]\n        columns = el[1]\n        dane = data.loc[:, data.columns.isin(columns)]\n        if len(el) == 3:\n            # print(\"Typ do wektoryzacji:\", type(data))\n            dane = el[2].transform(dane.T.squeeze())\n            # print(\"Dane po wektoryzacji:\" , dane)\n\n        temp = estimator.predict_proba(dane)\n        # print(temp)\n        if voting == \"plurality\":\n            temp = [plurality(x) for x in temp]\n            # print(temp)\n        if voting == \"Borda _count_voting\":\n            temp = [borda(x) for x in temp]\n        if voting == \"Fuzzy voting\":\n            pass\n\n        if voting == \"Nash voting\":\n            votes = [list(map(prod, zip(*t))) for t in zip(votes, temp)]\n        else:\n            votes = [list(map(sum, zip(*t))) for t in zip(votes, temp)]\n\n    # print(\"Final votes:\" , votes)\n    result = [np.argmax(vote) for vote in votes]\n    return result\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab97cef66b533250f0d55194b91a0c5e30008aa3"},"cell_type":"markdown","source":"Dzielimy dane na testowe i uczące."},{"metadata":{"trusted":true,"_uuid":"c88d1446e64169d3ce25a0d21bdab45e706a4765"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data, data['gender_label'], test_size=test_percentage, random_state=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"399b80840242ed0333559f07e6cb31394ff4e1f0"},"cell_type":"code","source":"\n\n\n\n\nplurality_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='plurality')\nprint(\"Podejscie pluralistyczne: \" , metrics.accuracy_score(y_test, plurality_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Borda _count_voting')\nprint(\"Podejscie Borda count voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Fuzzy voting')\nprint(\"Podejscie Fuzzy voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( other_classifier,[\"_golden\" , \"_trusted_judgments\" , \"fav_number\", \"retweet_count\" , \"tweet_count\" , \"profile_yn:confidence\"]), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Nash voting')\nprint(\"Podejscie Nash voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\n\nprint(\"Rezygnujemy z Other_Features:\")\n\nplurality_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']),  ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='plurality')\nprint(\"Podejscie pluralistyczne: \" , metrics.accuracy_score(y_test, plurality_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Borda _count_voting')\nprint(\"Podejscie Borda count voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\n\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Nash voting')\nprint(\"Podejscie Nash voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\nborda_votes = myEnseblePredict(estimators=[( color_classifier, ['rl','gl','bl','rs','gs','bs']), ( tweet_classifier, [\"Tweets\"], tweets_vect), ( description_classifier, [\"Description\"], description_vect)],data = X_test, voting='Fuzzy voting')\nprint(\"Podejscie Fuzzy voting voting: \" , metrics.accuracy_score(y_test, borda_votes))\n\n\n\nresults = confusion_matrix(y_test, y_pred_class)\nprint('Confusion Matrix :')\nprint(results)\n# print ('Accuracy Score :',accuracy_score(y_test, y_pred_class))\nprint('Report : ')\nprint(classification_report(y_test, y_pred_class))\n\n\n\nnull_accuracy = y_test.value_counts().head(1) / len(y_test)\nprint('Null accuracy:', null_accuracy)\n\n# Manual calculation of null accuracy by always predicting the majority class\nprint('Manual null accuracy:',(1172 / (1172 + 1084 + 849)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"870b508bf28a6a6cf94ef849b4a5b0117839839d"},"cell_type":"markdown","source":"Co można jeszcze zrobić (where to go next)\n\n*Lepsza miara dokładności modelu - accuracy measure\n\nhttp://regulatorygenomics.upf.edu/courses/Master_AGB/2_ClassificationAlgorithms/Lecture_Accuracy.pdf\n\nstrony: 28-32\n\n* Rozdzielic ceche gender na human, sex\nMoznaby zobaczyć, jak zachodzi odróżnanie człowieka od firmy, kobiety od mężczyzny jeśli wiadomo, że jest to człowiek"},{"metadata":{"trusted":true,"_uuid":"df06e1edd29d469a184266e143ebd452d6b5ffef"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}