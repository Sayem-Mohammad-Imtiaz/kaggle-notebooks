{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### importing standard libraries \n\nimport operator\nimport gc\nimport os\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport sklearn \nfrom sklearn.model_selection import train_test_split\nimport re ## Regular expresssions\nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom gensim.models import KeyedVectors\n\n### importing the tf libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D,GRU,Bidirectional, GlobalMaxPool1D \nfrom tensorflow.keras.models import Model,load_model, Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorboard.plugins import projector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.read_csv('/kaggle/input/clickbait-dataset/clickbait_data.csv')\nall_data.rename(columns={\"headline\": \"text\", \"clickbait\": \"target\"}, inplace=True) #renaming columns for compatibility with my offline version\nprint(all_data.tail())\nprint(all_data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### defining parameters\n#\nMAX_FEATURES = 3000\nMAX_LENGTH = 40 #of a sentence\n\nEMBEDDING_SIZE = 128\nHIDDEN_LAYER_SIZE = 64\nBATCH_SIZE = 512 #using\nNUM_EPOCHS = 5  #using\n\nFEATURE_VEC=300\n\nVERBOSE = False #if detailed checking True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Divide the data into training set, test set, and holdout. We reserve holdout for checking the final model\n#\ntrain, ds_set = train_test_split(all_data, test_size=0.3, random_state=42) #70% train\ntest, validation = train_test_split(ds_set, test_size=0.3, random_state=42)#21% test, 9% holdout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Word contractions -- all one line to save space.\ncontractions = {\"I'm\": 'I am', \"I'm'a\": 'I am about to', \"I'm'o\": 'I am going to', \"I've\": 'I have', \"I'll\": 'I will', \"I'll've\": 'I will have', \"I'd\": 'I would', \"I'd've\": 'I would have', 'Whatcha': 'What are you', \"amn't\": 'am not', \"ain't\": 'are not', \"aren't\": 'are not', \"'cause\": 'because', \"can't\": 'can not', \"can't've\": 'can not have', \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"daren't\": 'dare not', \"daresn't\": 'dare not', \"dasn't\": 'dare not', \"didn't\": 'did not', 'didn’t': 'did not', \"don't\": 'do not', 'don’t': 'do not', \"doesn't\": 'does not', \"e'er\": 'ever', \"everyone's\": 'everyone is', 'finna': 'fixing to', 'gimme': 'give me', \"gon't\": 'go not', 'gonna': 'going to', 'gotta': 'got to', \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"he've\": 'he have', \"he's\": 'he is', \"he'll\": 'he will', \"he'll've\": 'he will have', \"he'd\": 'he would', \"he'd've\": 'he would have', \"here's\": 'here is', \"how're\": 'how are', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how's\": 'how is', \"how'll\": 'how will', \"isn't\": 'is not', \"it's\": 'it is', \"'tis\": 'it is', \"'twas\": 'it was', \"it'll\": 'it will', \"it'll've\": 'it will have', \"it'd\": 'it would', \"it'd've\": 'it would have', 'kinda': 'kind of', \"let's\": 'let us', 'luv': 'love', \"ma'am\": 'madam', \"may've\": 'may have', \"mayn't\": 'may not', \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": 'might not have', \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": 'must not have', \"needn't\": 'need not', \"needn't've\": 'need not have', \"ne'er\": 'never', \"o'\": 'of', \"o'clock\": 'of the clock', \"ol'\": 'old', \"oughtn't\": 'ought not', \"oughtn't've\": 'ought not have', \"o'er\": 'over', \"shan't\": 'shall not', \"sha'n't\": 'shall not', \"shalln't\": 'shall not', \"shan't've\": 'shall not have', \"she's\": 'she is', \"she'll\": 'she will', \"she'd\": 'she would', \"she'd've\": 'she would have', \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": 'should not have', \"so've\": 'so have', \"so's\": 'so is', \"somebody's\": 'somebody is', \"someone's\": 'someone is', \"something's\": 'something is', 'sux': 'sucks', \"that're\": 'that are', \"that's\": 'that is', \"that'll\": 'that will', \"that'd\": 'that would', \"that'd've\": 'that would have', 'em': 'them', \"there're\": 'there are', \"there's\": 'there is', \"there'll\": 'there will', \"there'd\": 'there would', \"there'd've\": 'there would have', \"these're\": 'these are', \"they're\": 'they are', \"they've\": 'they have', \"they'll\": 'they will', \"they'll've\": 'they will have', \"they'd\": 'they would', \"they'd've\": 'they would have', \"this's\": 'this is', \"those're\": 'those are', \"to've\": 'to have', 'wanna': 'want to', \"wasn't\": 'was not', \"we're\": 'we are', \"we've\": 'we have', \"we'll\": 'we will', \"we'll've\": 'we will have', \"we'd\": 'we would', \"we'd've\": 'we would have', \"weren't\": 'were not', \"what're\": 'what are', \"what'd\": 'what did', \"what've\": 'what have', \"what's\": 'what is', \"what'll\": 'what will', \"what'll've\": 'what will have', \"when've\": 'when have', \"when's\": 'when is', \"where're\": 'where are', \"where'd\": 'where did', \"where've\": 'where have', \"where's\": 'where is', \"which's\": 'which is', \"who're\": 'who are', \"who've\": 'who have', \"who's\": 'who is', \"who'll\": 'who will', \"who'll've\": 'who will have', \"who'd\": 'who would', \"who'd've\": 'who would have', \"why're\": 'why are', \"why'd\": 'why did', \"why've\": 'why have', \"why's\": 'why is', \"will've\": 'will have', \"won't\": 'will not', \"won't've\": 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have', \"y'all\": 'you all', \"y'all're\": 'you all are', \"y'all've\": 'you all have', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have', \"you're\": 'you are', \"you've\": 'you have', \"you'll've\": 'you shall have', \"you'll\": 'you will', \"you'd\": 'you would', \"you'd've\": 'you would have', 'jan.': 'january', 'feb.': 'february', 'mar.': 'march', 'apr.': 'april', 'jun.': 'june', 'jul.': 'july', 'aug.': 'august', 'sep.': 'september', 'oct.': 'october', 'nov.': 'november', 'dec.': 'december', 'I’m': 'I am', 'I’m’a': 'I am about to', 'I’m’o': 'I am going to', 'I’ve': 'I have', 'I’ll': 'I will', 'I’ll’ve': 'I will have', 'I’d': 'I would', 'I’d’ve': 'I would have', 'amn’t': 'am not', 'ain’t': 'are not', 'aren’t': 'are not', '’cause': 'because', 'can’t': 'can not', 'can’t’ve': 'can not have', 'could’ve': 'could have', 'couldn’t': 'could not', 'couldn’t’ve': 'could not have', 'daren’t': 'dare not', 'daresn’t': 'dare not', 'dasn’t': 'dare not', 'doesn’t': 'does not', 'e’er': 'ever', 'everyone’s': 'everyone is', 'gon’t': 'go not', 'hadn’t': 'had not', 'hadn’t’ve': 'had not have', 'hasn’t': 'has not', 'haven’t': 'have not', 'he’ve': 'he have', 'he’s': 'he is', 'he’ll': 'he will', 'he’ll’ve': 'he will have', 'he’d': 'he would', 'he’d’ve': 'he would have', 'here’s': 'here is', 'how’re': 'how are', 'how’d': 'how did', 'how’d’y': 'how do you', 'how’s': 'how is', 'how’ll': 'how will', 'isn’t': 'is not', 'it’s': 'it is', '’tis': 'it is', '’twas': 'it was', 'it’ll': 'it will', 'it’ll’ve': 'it will have', 'it’d': 'it would', 'it’d’ve': 'it would have', 'let’s': 'let us', 'ma’am': 'madam', 'may’ve': 'may have', 'mayn’t': 'may not', 'might’ve': 'might have', 'mightn’t': 'might not', 'mightn’t’ve': 'might not have', 'must’ve': 'must have', 'mustn’t': 'must not', 'mustn’t’ve': 'must not have', 'needn’t': 'need not', 'needn’t’ve': 'need not have', 'ne’er': 'never', 'o’': 'of', 'o’clock': 'of the clock', 'ol’': 'old', 'oughtn’t': 'ought not', 'oughtn’t’ve': 'ought not have', 'o’er': 'over', 'shan’t': 'shall not', 'sha’n’t': 'shall not', 'shalln’t': 'shall not', 'shan’t’ve': 'shall not have', 'she’s': 'she is', 'she’ll': 'she will', 'she’d': 'she would', 'she’d’ve': 'she would have', 'should’ve': 'should have', 'shouldn’t': 'should not', 'shouldn’t’ve': 'should not have', 'so’ve': 'so have', 'so’s': 'so is', 'somebody’s': 'somebody is', 'someone’s': 'someone is', 'something’s': 'something is', 'that’re': 'that are', 'that’s': 'that is', 'that’ll': 'that will', 'that’d': 'that would', 'that’d’ve': 'that would have', 'there’re': 'there are', 'there’s': 'there is', 'there’ll': 'there will', 'there’d': 'there would', 'there’d’ve': 'there would have', 'these’re': 'these are', 'they’re': 'they are', 'they’ve': 'they have', 'they’ll': 'they will', 'they’ll’ve': 'they will have', 'they’d': 'they would', 'they’d’ve': 'they would have', 'this’s': 'this is', 'those’re': 'those are', 'to’ve': 'to have', 'wasn’t': 'was not', 'we’re': 'we are', 'we’ve': 'we have', 'we’ll': 'we will', 'we’ll’ve': 'we will have', 'we’d': 'we would', 'we’d’ve': 'we would have', 'weren’t': 'were not', 'what’re': 'what are', 'what’d': 'what did', 'what’ve': 'what have', 'what’s': 'what is', 'what’ll': 'what will', 'what’ll’ve': 'what will have', 'when’ve': 'when have', 'when’s': 'when is', 'where’re': 'where are', 'where’d': 'where did', 'where’ve': 'where have', 'where’s': 'where is', 'which’s': 'which is', 'who’re': 'who are', 'who’ve': 'who have', 'who’s': 'who is', 'who’ll': 'who will', 'who’ll’ve': 'who will have', 'who’d': 'who would', 'who’d’ve': 'who would have', 'why’re': 'why are', 'why’d': 'why did', 'why’ve': 'why have', 'why’s': 'why is', 'will’ve': 'will have', 'won’t': 'will not', 'won’t’ve': 'will not have', 'would’ve': 'would have', 'wouldn’t': 'would not', 'wouldn’t’ve': 'would not have', 'y’all': 'you all', 'y’all’re': 'you all are', 'y’all’ve': 'you all have', 'y’all’d': 'you all would', 'y’all’d’ve': 'you all would have', 'you’re': 'you are', 'you’ve': 'you have', 'you’ll’ve': 'you shall have', 'you’ll': 'you will', 'you’d': 'you would', 'you’d’ve': 'you would have'}\nprint(f'There are {len(contractions.keys())} contractions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Various utility functions\n\ndef Preprocess(doc):\n    corpus=[]\n    for text in tqdm(doc):\n        text=\" \".join([contraction_fix(w) for w in text.split()])\n        text=re.sub(r'[^a-z0-9A-Z]',\" \",text)\n        text=re.sub(r'[0-9]{1}',\"#\",text)\n        text=re.sub(r'[0-9]{2}','##',text)\n        text=re.sub(r'[0-9]{3}','###',text)\n        text=re.sub(r'[0-9]{4}','####',text)\n        text=re.sub(r'[0-9]{5,}','#####',text)\n        corpus.append(text)\n    return corpus\n\ndef contraction_fix(word):\n    try:\n        a=contractions[word]\n    except KeyError:\n        a=word\n    return a\n\n\n## defining the vocabulary function\ndef get_vocab(corpus):\n    vocab={}\n    for text in tqdm(corpus):\n        for word in text.split():\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n    vocab=dict(sorted(vocab.items(),reverse=True ,key=lambda item: item[1]))\n    return vocab\n\n### gettting the index for the each word in vocabulary\ndef get_word_index(vocab):\n    word_index=dict((w,i+1) for i,w in enumerate(vocab.keys()))\n    return word_index\n\ndef fit_one_hot(word_index,corpus):\n    sent=[]\n    for text in tqdm(corpus):\n        li=[]\n        for word in text.split():\n            try:\n                li.append(word_index[word])\n            except KeyError:\n                li.append(0)\n        sent.append(li)\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab=get_vocab(train.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_feat=dict(list(vocab.items())[:MAX_FEATURES])\n#top_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=get_word_index(top_feat)\nlen(word_index.keys()) #-- should be MAX_FEATURES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_docs=fit_one_hot(word_index,train.text)\npadded_doc=pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")\n\n### preprocessing the text\ntrain_text=Preprocess(train.text)\ntest_text=Preprocess(test.text)\nvalidation_text=Preprocess(validation.text)\n\nvocab=get_vocab(train_text)\ntop_feat=dict(list(vocab.items())[:MAX_FEATURES])\nword_index=get_word_index(top_feat)\n\n### encoding the training set\nencoded_docs=fit_one_hot(word_index,train_text)\ntrain_padded=pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")\n\n### encoding the test set\nencoded_docs=fit_one_hot(word_index,test_text)\ntest_padded=pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")\n\n### encoding the Validation (holdout) set\nencoded_docs=fit_one_hot(word_index,validation_text)\nvalidation_padded=pad_sequences(encoded_docs,maxlen=MAX_LENGTH,padding=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(MAX_LENGTH,))\nx = Embedding(MAX_FEATURES + 1, FEATURE_VEC)(inp)\nx = Bidirectional(GRU(128, return_sequences=True))(x)\nx = Conv1D(64,5,activation=\"relu\")(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(128, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\n\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### defining some callbacks\nopt=Adam(learning_rate=0.002)\nbin_loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.2, name='binary_crossentropy')\n\n## defining the call backs\n\n#see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=3,mode=\"min\",restore_best_weights=True)\n\n### Now reducing the learning rate when the model is not improvinig \nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.2,patience=2, verbose=1,  mode=\"auto\")\n\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmy_callbacks=[early_stopping,reduce_lr, tensorboard_callback]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=bin_loss, optimizer=opt, metrics=['accuracy'])\n#with GPU/CPU\n    \nhistory = model.fit(train_padded, train.target, \n                        batch_size=BATCH_SIZE,  #512\n                        epochs=NUM_EPOCHS,\n                        validation_data=(test_padded, test.target),\n                        callbacks=my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlen(history.history['val_loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot history: MAE\nimport matplotlib.pyplot as plt\nplt.plot(history.history['loss'], label='MAE (training data)')\nplt.plot(history.history['val_loss'], label='MAE (test data)')\nplt.title('MAE for Clickbait')\nplt.ylabel('MAE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(test_padded, test.target, verbose=1)\nprint(f'Test loss: {score[0]} / Test accuracy: {score[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the model against the holdout set\nvalidation_results = model.predict(validation_padded)\ntarget = validation.target.array\nright = 0\nwrong = 0\ntotal = len(validation)\n\nfor i in range(total):\n    guessed = int(np.round((validation_results[i]),0))\n    if target[i] == guessed:\n        right += 1\n    else:\n        wrong += 1\n        if VERBOSE:\n            print(f\"Text: {validation_text[i]}\\n\\tGuessed Wrong: {guessed}({validation_results[i]}), Actual: {target[i]}\")\n\nprint(f'Right: {right}, Wrong: {wrong}, Accuracy {right*100/total:2.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%time\ns1 = pd.merge(  train, validation, how='inner', on=['text'])\ns2 = pd.merge( validation, test, how='inner', on=['text'])\nprint(f'Interractions between train and validation: {s1.size}')\nprint(f'Interractions between test and validation: {s1.size}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = [round(i[0]) for i in model.predict(test_padded)]\ncm = confusion_matrix(test.target, preds)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Not clickbait', 'Clickbait'], fontsize=16)\nplt.yticks(range(2), ['Not clickbait', 'Clickbait'], fontsize=16)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}