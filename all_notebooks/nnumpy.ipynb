{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.genfromtxt('../input/data-banknote-authentication/data_banknote_authentication.txt', delimiter = ',')\nX = data[:,:4]\ny = data[:, 4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X[:, 0], X[:, 1], alpha=0.2,\n c=y, cmap='viridis')\nplt.xlabel('variance of wavelet');\nplt.ylabel('skewness of wavelet');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train = X_train.T\ny_train = y_train.reshape(1, y_train.shape[0])\nX_test = X_test.T\ny_test = y_test.reshape(1, y_test.shape[0])\nprint ('Train X Shape: ', X_train.shape)\nprint ('Train Y Shape: ', y_train.shape)\nprint ('I have m = %d training examples!' % (X_train.shape[1]))\n\nprint ('\\nTest X Shape: ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 0.05\nnum_iterations = 50\nc = eval(input('quantidade de neuronios na camada oculta: '))\n\ndef define_structure(X, Y):\n    input_unit = X.shape[0] # size of input layer\n    hidden_unit = c #hidden layer of size 4\n    output_unit = Y.shape[0] # size of output layer\n    return (input_unit, hidden_unit, output_unit)\n(input_unit, hidden_unit, output_unit) = define_structure(X_train, y_train)\nprint(\"The size of the input layer is:  = \" + str(input_unit))\nprint(\"The size of the hidden layer is:  = \" + str(hidden_unit))\nprint(\"The size of the output layer is:  = \" + str(output_unit))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def parameters_initialization(input_unit, hidden_unit, output_unit):\n    np.random.seed(2) \n    W1 = np.random.randn(hidden_unit, input_unit)*0.01\n    b1 = np.zeros((hidden_unit, 1))\n    W2 = np.random.randn(output_unit, hidden_unit)*0.01\n    b2 = np.zeros((output_unit, 1))\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(z):\n    return 1/(1+np.exp(-z))\n\ndef forward_propagation(X, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = sigmoid(Z2)\n    cache = {\"Z1\": Z1,\"A1\": A1,\"Z2\": Z2,\"A2\": A2}\n    \n    return A2, cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_entropy_cost(A2, Y, parameters):\n    # number of training example\n    m = Y.shape[1] \n    # Compute the cross-entropy cost\n    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1 - A2))\n    cost = - np.sum(logprobs) / m\n    cost = float(np.squeeze(cost))\n                                    \n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef rmse(A2, Y, parameters):\n    differences = A2 - Y                       #the DIFFERENCEs.\n\n    differences_squared = differences ** 2                    #the SQUAREs of ^\n\n    mean_of_differences_squared = differences_squared.mean()  #the MEAN of ^\n\n    rmse_val = np.sqrt(mean_of_differences_squared)           #ROOT of ^\n    \n    mse = mean_squared_error(A2, Y)\n\n    return rmse_val, mse  \n                                    \n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward_propagation(parameters, cache, X, Y):\n    #number of training example\n    m = X.shape[1]\n    \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    A1 = cache['A1']\n    A2 = cache['A2']\n   \n    dZ2 = A2-Y\n    dW2 = (1/m) * np.dot(dZ2, A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n    dW1 = (1/m) * np.dot(dZ1, X.T) \n    db1 = (1/m)*np.sum(dZ1, axis=1, keepdims=True)\n    \n    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2,\"db2\": db2}\n    \n    return grads","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(parameters, grads, learning_rate = learning_rate):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n   \n    dW1 = grads['dW1']\n    db1 = grads['db1']\n    dW2 = grads['dW2']\n    db2 = grads['db2']\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    \n    parameters = {\"W1\": W1, \"b1\": b1,\"W2\": W2,\"b2\": b2}\n    \n    return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def neural_network_model(X, Y, hidden_unit, num_iterations = num_iterations):\n    np.random.seed(3)\n    input_unit = define_structure(X, Y)[0]\n    output_unit = define_structure(X, Y)[2]\n    rmse1 = list()\n    epocas = list()\n    mse = list()\n    \n    parameters = parameters_initialization(input_unit, hidden_unit, output_unit)\n   \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    for i in range(0, num_iterations):\n        A2, cache = forward_propagation(X, parameters)\n        cost = cross_entropy_cost(A2, Y, parameters)\n        grads = backward_propagation(parameters, cache, X, Y)\n        parameters = gradient_descent(parameters, grads)\n        erro, errmse = rmse(A2,Y,parameters) \n        rmse1.append(erro)\n        mse.append(errmse)\n        \n        epocas.append(i+1)\n        \n        if i % 1 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            print (\"erro %i: %f\" %(i, erro))\n    return parameters, rmse1, epocas, mse\n\nparameters, rmse, epocas, mse = neural_network_model(X_train, y_train, hidden_unit, num_iterations= num_iterations)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(parameters, X):\n    A2, cache = forward_propagation(X, parameters)\n    predictions = np.round(A2)\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = prediction(parameters, X_train)\nprint ('Accuracy Train: %d' % float((np.dot(y_train, predictions.T) + np.dot(1 - y_train, 1 - predictions.T))/float(y_train.size)*100) + '%')\npredictions = prediction(parameters, X_test)\nprint ('Accuracy Test: %d' % float((np.dot(y_test, predictions.T) + np.dot(1 - y_test, 1 - predictions.T))/float(y_test.size)*100) + '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsetotal(predictions, targets): #y - prediction\n\n    differences = predictions - targets                       #the DIFFERENCEs.\n\n    differences_squared = differences ** 2                    #the SQUAREs of ^\n\n    mean_of_differences_squared = differences_squared.mean()  #the MEAN of ^\n\n    rmse_val = np.sqrt(mean_of_differences_squared)           #ROOT of ^\n\n    return rmse_val   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = prediction(parameters, X_train)\nr1 = rmsetotal(y_train,predictions.T)\nprint ('Erro Train: ', r1)\npredictions = prediction(parameters, X_test)\nr = rmsetotal(predictions.T,y_test)\nprint ('Erro Test: ', r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.scatter(r1, r, alpha=0.2,\n #c=X_train, cmap='viridis')\n#plt.xlabel('variance of wavelet')\n#plt.ylabel('skewness of wavelet');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epocas, rmse) \nplt.xlabel('Época(s)')\nplt.ylabel('Erro Médio Quadrado (MSE)')\n#plt.title('Gráfico do comportamento do erro de acordo com as épocas')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}