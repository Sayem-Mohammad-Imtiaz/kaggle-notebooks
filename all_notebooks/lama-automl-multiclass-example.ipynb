{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install lightautoml","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[More info about LAMA](https://github.com/sberbank-ai-lab/LightAutoML)"},{"metadata":{},"cell_type":"markdown","source":"# Step 0.1. Import necessary libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\n\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# Imports from our package\nfrom lightautoml.automl.base import AutoML\nfrom lightautoml.automl.blend import WeightedBlender\nfrom lightautoml.ml_algo.boost_lgbm import BoostLGBM\nfrom lightautoml.ml_algo.linear_sklearn import LinearLBFGS\nfrom lightautoml.ml_algo.tuning.optuna import OptunaTuner\nfrom lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures, LGBAdvancedPipeline\nfrom lightautoml.pipelines.features.linear_pipeline import LinearFeatures\nfrom lightautoml.pipelines.ml.base import MLPipeline\nfrom lightautoml.pipelines.selection.importance_based import ModelBasedImportanceEstimator, ImportanceCutoffSelector\nfrom lightautoml.reader.base import PandasToPandasReader\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\nfrom lightautoml.utils.timer import PipelineTimer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.2. Parameters "},{"metadata":{"trusted":true},"cell_type":"code","source":"N_THREADS = 8 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 600 # Time in seconds for automl run\nTARGET_NAME = 'TARGET' # Target column name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.3. Fix torch number of threads and numpy seed "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.4. Change profiling decorators settings "},{"metadata":{},"cell_type":"markdown","source":"By default, profiling decorators are turned off for speed and memory reduction. If you want to see profiling report after using LAMA, you need to turn on the decorators using command below: "},{"metadata":{"trusted":true},"cell_type":"code","source":"p = Profiler()\np.change_deco_settings({'enabled': True})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.5. Example data load "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata = pd.read_csv('../input/lama-datasets/sampled_app_train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.6. (Optional) Some user feature preparation "},{"metadata":{},"cell_type":"markdown","source":"Cell below shows some user feature preparations to create task more difficult (this block can be omitted if you don't want to change the initial data):"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata['BIRTH_DATE'] = (np.datetime64('2018-01-01') + data['DAYS_BIRTH'].astype(np.dtype('timedelta64[D]'))).astype(str)\ndata['EMP_DATE'] = (np.datetime64('2018-01-01') + np.clip(data['DAYS_EMPLOYED'], None, 0).astype(np.dtype('timedelta64[D]'))\n                    ).astype(str)\n\ndata['constant'] = 1\ndata['allnan'] = np.nan\n\ndata['report_dt'] = np.datetime64('2018-01-01')\n\ndata.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.7. Create fake multiclass target "},{"metadata":{"trusted":true},"cell_type":"code","source":"data[TARGET_NAME] = np.where(np.random.rand(data.shape[0]) > .5, 2, data[TARGET_NAME].values)\ndata[TARGET_NAME].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.8. (Optional) Data splitting for train-test "},{"metadata":{},"cell_type":"markdown","source":"Block below can be omitted if you are going to train model only or you have specific train and test files:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_data, test_data = train_test_split(data, \n                                         test_size=TEST_SIZE, \n                                         stratify=data[TARGET_NAME], \n                                         random_state=RANDOM_STATE)\nprint('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n              .format(train_data.shape, test_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ========= AutoML creation =========\n\n## Step 1. Create Timer for pipeline\n\nHere we are going to use strict timer for AutoML pipeline, which helps not to go outside the limit:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntimer = PipelineTimer(600, mode=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2. Create feature selector"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntimer_gbm = timer.get_task_timer('gbm') # Get task timer from pipeline timer \nfeat_sel_0 = LGBSimpleFeatures()\nmod_sel_0 = BoostLGBM(timer=timer_gbm)\nimp_sel_0 = ModelBasedImportanceEstimator()\nselector_0 = ImportanceCutoffSelector(feat_sel_0, mod_sel_0, imp_sel_0, cutoff=0, )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3.1. Create GBMs pipeline for AutoML "},{"metadata":{},"cell_type":"markdown","source":"Our GBMs ML pipeline:\n- Advanced features for gradient boosting built on selected features (using step 2) \n- 2 different models:\n    * LightGBM with params tuning (using OptunaTuner)\n    * LightGBM with heuristic params\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nfeats_gbm_0 = LGBAdvancedPipeline(top_intersections=4, \n                                  output_categories=True, \n                                  feats_imp=imp_sel_0)\ntimer_gbm_0 = timer.get_task_timer('gbm')\ntimer_gbm_1 = timer.get_task_timer('gbm')\n\ngbm_0 = BoostLGBM(timer=timer_gbm_0)\ngbm_1 = BoostLGBM(timer=timer_gbm_1)\n\ntuner_0 = OptunaTuner(n_trials=20, timeout=30, fit_on_holdout=True)\ngbm_lvl0 = MLPipeline([\n        (gbm_0, tuner_0),\n        gbm_1\n    ],\n    pre_selection=selector_0,\n    features_pipeline=feats_gbm_0, \n    post_selection=None\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3.2. Create linear pipeline for AutoML "},{"metadata":{},"cell_type":"markdown","source":"Our linear pipeline:\n- Using features, special for linear models\n- LinearLBFGS as a model\n- Without feature selection here"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfeats_reg_0 = LinearFeatures(output_categories=True, \n                             sparse_ohe='auto')\n\ntimer_reg = timer.get_task_timer('reg')\nreg_0 = LinearLBFGS(timer=timer_reg)\n\nreg_lvl0 = MLPipeline([\n        reg_0\n    ],\n    pre_selection=None,\n    features_pipeline=feats_reg_0, \n    post_selection=None\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4. Create multiclass task and reader"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\ntask = Task('multiclass', metric = 'crossentropy', ) \nreader = PandasToPandasReader(task = task, samples = None, max_nan_rate = 1, max_constant_rate = 1,\n                              advanced_roles = True, drop_score_co = -1, n_jobs = 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5. Create blender for 2nd level "},{"metadata":{},"cell_type":"markdown","source":"To combine predictions from different models into one vector we use WeightedBlender:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nblender = WeightedBlender()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6. Create AutoML pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nautoml = AutoML(reader=reader, levels=[\n    [gbm_lvl0, reg_lvl0]\n], timer=timer, blender=blender, skip_conn=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 7. Train AutoML on loaded data "},{"metadata":{},"cell_type":"markdown","source":"In cell below we train AutoML with target column `TARGET` to receive fitted model and OOF predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\noof_pred = automl.fit_predict(train_data, roles={'target': TARGET_NAME})\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 8. Predict to test data and check scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntest_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'\n              .format(test_pred, test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(log_loss(train_data[TARGET_NAME].values, oof_pred.data)))\nprint('TEST score: {}'.format(log_loss(test_data[TARGET_NAME].values, test_pred.data)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 9. Check AUCs for each class in train and test data "},{"metadata":{"trusted":true},"cell_type":"code","source":"for dat, df, name in zip([oof_pred, test_pred], [train_data, test_data], ['train', 'test']):\n    print('Check aucs {0}...'.format(name))\n    for cl in range(3):\n        sc = roc_auc_score((df[TARGET_NAME].values == cl).astype(np.float32), dat.data[:, cl])\n        print('Class {0} {1} auc score: {2}'.format(cl, name, sc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}