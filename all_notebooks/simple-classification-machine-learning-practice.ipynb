{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings(action='ignore')\n\nimport pandas as pd \nimport numpy as np\n# 지도 학습 : 훈련 데이터로부터 하나의 함수를 유추해내기 위한 기계 학습의 한 방법 \n# 분류 예측 : 주어진 입력 벡터가 어떤 종류의 값인지 표식하는 것을 분류라고 함\n\n# 이해하기 쉬운 분류 알고리즘 4 : \n# K-Nearest Neighbors\n# Decision Tree/Random Forest \n# Support Vector Machine(SVM)\n# Neural Network \n\n# 알고리즘 import  \nfrom sklearn.neighbors import KNeighborsClassifier # KNN 모델\nfrom sklearn.tree import DecisionTreeClassifier # 의사결정나무 모델\nfrom sklearn.ensemble import RandomForestClassifier # 랜덤포레스트 모델\nfrom sklearn.svm import SVC # 서포트백터머신\n\n# 모델을 테스트하기 위해 필요한 iris 데이터 불러오기\nfrom sklearn.datasets import load_iris\niris = load_iris()\ndf = pd.DataFrame(iris['data'],columns=iris['feature_names'])\ndf['target']=iris['target']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터셋을 test와 valid셋으로 분리 \nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns=['target'])\ny = df['target']\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델별로 데스트 해보기\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'KNeighborsClassifier':{\n        # n_neighbors, n개의 근접 이웃을 기준으로 판단할지 결정 \n        'n_neighbors':[i for i in np.arange(5,20)],\n        # 근접 이웃에 따라서 동일하게 고려할지, 거리에 따라서 가중치를 고려할지를 결정 \n        'weights':['uniform','distance'],\n        # 데이터간 거리를 측정하는 방법을 결정, kdtree를 좀 더 최적화한 것이 balltree\n        'algorithm':['auto','ball_tree','kd_tree']\n    }\n}\n    \nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\nmodel, model_name = KNeighborsClassifier(),'KNeighborsClassifier'\ngcv = GridSearchCV(model,param_grid=param_grid[model_name],scoring='accuracy')\ngcv.fit(X_train,Y_train)\n# print(gcv.best_params_)\n# print(gcv.best_score_)\nmodel = gcv.best_estimator_\nresult = model.predict(X_test)\naccuracy_score(Y_test,result)\n\n# 정확도(accuracy) : 예측이 얼마나 정확한가\n# 정밀도(precision) : 예측한 것 중에서 정답의 비율은? \n# 재현율(recall) : 찾아야할 것 중에서 실제로 찾은 비율은? \n# f1 스코어 : 정밀도와 재현율의 평균 \n\n# 번호 : [  1,    2,    3,    4,    5,    6  ]\n# 정답 : [음치,음치,음치,음치,정상,정상] \n# 예측 : [음치,음치,정상,정상,정상,정상]\n\n# 정확도 : 예측이 맞은 비율은?\n#         1,2,5,6 번 맞추고 3,4번은 틀렸다. 6명중 4명 맞췄으므로 4/6 = 2/3 = 0.66 \n#정밀도 : 음치라고 예측한 사람들 중에 진짜 음치가 얼마나 있는가?\n#          내가 음치라고 예측한 1,2번 이 둘다 음치가 맞았다. 2/2 = 1.00\n#재현율 : 전체 음치 중에서 내가 맞춘 음치의 비율은?\n#          원래 음치가 4명 있는데 나는 그중에서 2명을 맞췄다. 2/4 = 0.5\n#F1 Score : 정밀도와 재현율의 평균 \n#            2 * 정밀도 * 재현율 /(정밀도+재현율) = 2 * 1.00 * 0.5 / (1.00 + 0.5) = 0.66\n\n\n# sklearn 을 이용하면 전부 계산해준다.\nprint('accuracy', accuracy_score(Y_test,result) )\nprint('precision', precision_score(Y_test,result,average=None) )\nprint('recall', recall_score(Y_test,result,average=None) )\nprint('f1', f1_score(Y_test,result,average=None) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델별로 데스트 해보기\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'DecisionTreeClassifier':{\n        # 트리를 만들 때 불순도 impurity가 가장 낮은 방향으로 트리를 만들어야함\n        'criterion':['gini', 'entropy'],\n        'splitter':['best','random'],\n        'max_depth':[i for i in np.arange(1,5,1)],\n        # 노트를 분할하기 위한 최소한의 샘플 수 \n        'min_samples_split':[i for i in np.arange(5,11,1)],\n        # 리프 노드가 되기 위한 최소한의 샘플 수\n        'min_samples_leaf':[i for i in np.arange(5,11,1)]\n        \n    }\n}\n    \nfrom sklearn.metrics import accuracy_score\nmodel, model_name = DecisionTreeClassifier(),'DecisionTreeClassifier'\ngcv = GridSearchCV(model,param_grid=param_grid[model_name],scoring='accuracy')\ngcv.fit(X_train,Y_train)\nprint(gcv.best_params_)\nprint(gcv.best_score_)\nmodel = gcv.best_estimator_\nresult = model.predict(X_test)\naccuracy_score(Y_test,result)\n\n# sklearn 을 이용하면 전부 계산해준다.\nprint('accuracy', accuracy_score(Y_test,result) )\nprint('precision', precision_score(Y_test,result,average=None) )\nprint('recall', recall_score(Y_test,result,average=None) )\nprint('f1', f1_score(Y_test,result,average=None) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델별로 데스트 해보기\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'RandomForestClassifier':{\n        'criterion':['gini', 'entropy'],\n        'max_depth':[i for i in np.arange(1,5,1)],\n        'min_samples_split':[i for i in np.arange(5,11,1)]\n#         'min_samples_leaf':[i for i in np.arange(5,11,1)]\n        \n    }\n}\n    \nfrom sklearn.metrics import accuracy_score\nmodel, model_name = RandomForestClassifier(),'RandomForestClassifier'\ngcv = GridSearchCV(model,param_grid=param_grid[model_name],scoring='accuracy')\ngcv.fit(X_train,Y_train)\nprint(gcv.best_params_)\nprint(gcv.best_score_)\nmodel = gcv.best_estimator_\nresult = model.predict(X_test)\naccuracy_score(Y_test,result)\n\n# sklearn 을 이용하면 전부 계산해준다.\nprint('accuracy', accuracy_score(Y_test,result) )\nprint('precision', precision_score(Y_test,result,average=None) )\nprint('recall', recall_score(Y_test,result,average=None) )\nprint('f1', f1_score(Y_test,result,average=None) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델별로 데스트 해보기\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {\n    'SVC':{\n        'kernel':['linear', 'poly','rbf'],\n        'degree':[i for i in np.arange(5,11,1)],\n        'gamma':['scale','auto']\n    }\n}\n    \nfrom sklearn.metrics import accuracy_score\nmodel, model_name = SVC(),'SVC'\ngcv = GridSearchCV(model,param_grid=param_grid[model_name],scoring='accuracy')\ngcv.fit(X_train,Y_train)\nprint(gcv.best_params_)\nprint(gcv.best_score_)\nmodel = gcv.best_estimator_\nresult = model.predict(X_test)\naccuracy_score(Y_test,result)\n\n# sklearn 을 이용하면 전부 계산해준다.\nprint('accuracy', accuracy_score(Y_test,result) )\nprint('precision', precision_score(Y_test,result,average=None) )\nprint('recall', recall_score(Y_test,result,average=None) )\nprint('f1', f1_score(Y_test,result,average=None) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 실험용 데이터 셋 정제 \n# 타이타닉 데이터셋 \ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\n# 성별, 승선항구를 숫자로 변경\ndf['Sex'] = df['Sex'].astype('category').cat.codes\ndf['Embarked'] = df['Embarked'].astype('category').cat.codes\n# 불필요한 컬럼은 drop \ndf = df.drop(columns=['Name','Ticket','Cabin']).copy() \n# 빈 값은 0으로 대체 \ndf = df.fillna(0).copy() \ntitanic = df.copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 버섯 분류 데이터셋\ndf = pd.read_csv('/kaggle/input/mushroom-classification/mushrooms.csv')\ndf = df.astype('category').apply(lambda x: x.cat.codes).copy() \ndf = df.fillna(0).copy()\ndf.head()\nmushroom = df.copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IBM HR 데이터 셋 \ndf = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nfor col in df.columns:\n    if df[col].dtype.name == 'object':\n        df[col]=df[col].astype('category').cat.codes\ndf.head()\nibmhr = df.copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 심장병 사망 사례 데이터셋 \ndf= pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf = df.fillna(0).copy()\ndf.head()\nheart = df.copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 머신러닝 모델링 - 타이타닉 데이터 활용\n# 데이터셋을 test와 valid셋으로 분리 \nfrom sklearn.model_selection import train_test_split\n# 모델별로 데스트 해보기\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nX = titanic.drop(columns=['Survived'])\ny = titanic['Survived']\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33)\n\nparam_grid = {\n    'KNeighborsClassifier':{\n        # n_neighbors, n개의 근접 이웃을 기준으로 판단할지 결정 \n        'n_neighbors':[i for i in np.arange(1,20)],\n        # 근접 이웃에 따라서 동일하게 고려할지, 거리에 따라서 가중치를 고려할지를 결정 \n        'weights':['uniform','distance'],\n        # 데이터간 거리를 측정하는 방법을 결정, kdtree를 좀 더 최적화한 것이 balltree\n        'algorithm':['auto','ball_tree','kd_tree']\n    },\n    'DecisionTreeClassifier':{\n        # 트리를 만들 때 불순도 impurity가 가장 낮은 방향으로 트리를 만들어야함\n        'criterion':['gini', 'entropy'],\n        'splitter':['best','random'],\n        'max_depth':[i for i in np.arange(1,5,1)],\n        # 노트를 분할하기 위한 최소한의 샘플 수 \n        'min_samples_split':[i for i in np.arange(5,11,1)]\n        # 리프 노드가 되기 위한 최소한의 샘플 수\n#         'min_samples_leaf':[i for i in np.arange(5,11,1)]\n    },\n    'RandomForestClassifier':{\n        'criterion':['gini', 'entropy'],\n        'max_depth':[i for i in np.arange(1,5,1,)],\n        'min_samples_split':[i for i in np.arange(5,11,1)]\n#         'min_samples_leaf':[i for i in np.arange(5,11,1)]  \n    },\n    'SVC':{\n        'kernel':['linear','rbf'],\n#         'degree':[i for i in np.arange(5,11,2)]\n#         'gamma':['scale','auto']\n    }\n}\n    \n\nmodels = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),SVC()] \nmodel_names = ['KNeighborsClassifier','DecisionTreeClassifier','RandomForestClassifier','SVC']\n    \nfor model, model_name in zip(models,model_names):\n    print()\n    print('======='+model_name+'========')\n    gcv = GridSearchCV(model,param_grid=param_grid[model_name],scoring='accuracy')\n    gcv.fit(X_train,Y_train)\n    print('train set의 최고 예측 정확도 : {}'.format(gcv.best_score_))\n    print('train set 예측 정확도가 가장 높은 경우의 파라미터 : {}'.format(gcv.best_params_))\n    model = gcv.best_estimator_\n    result = model.predict(X_test)\n\n    # sklearn 을 이용하면 전부 계산해준다.\n    print(model_name,'test set의 예측 정확도 accuracy', accuracy_score(Y_test,result) )\n    print(model_name,'test set의 예측 정확도 precision', precision_score(Y_test,result,average=None) )\n    print(model_name,'test set의 예측 정확도 recall', recall_score(Y_test,result,average=None) )\n    print(model_name,'test set의 예측 정확도 f1', f1_score(Y_test,result,average=None) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 머신러닝 모델링 - 타이타닉 데이터 활용\n# 데이터셋을 test와 valid셋으로 분리 \nfrom sklearn.model_selection import train_test_split\n# 모델별로 데스트 해보기\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nX = mushroom.drop(columns=['class'])\ny = mushroom['class']\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33)\n\nparam_grid = {\n    'KNeighborsClassifier':{\n        # n_neighbors, n개의 근접 이웃을 기준으로 판단할지 결정 \n        'n_neighbors':[i for i in np.arange(1,20)],\n        # 근접 이웃에 따라서 동일하게 고려할지, 거리에 따라서 가중치를 고려할지를 결정 \n        'weights':['uniform','distance'],\n        # 데이터간 거리를 측정하는 방법을 결정, kdtree를 좀 더 최적화한 것이 balltree\n        'algorithm':['auto','ball_tree','kd_tree']\n    },\n    'DecisionTreeClassifier':{\n        # 트리를 만들 때 불순도 impurity가 가장 낮은 방향으로 트리를 만들어야함\n        'criterion':['gini', 'entropy'],\n        'splitter':['best','random'],\n        'max_depth':[i for i in np.arange(1,5,1)],\n        # 노트를 분할하기 위한 최소한의 샘플 수 \n        'min_samples_split':[i for i in np.arange(5,11,1)]\n        # 리프 노드가 되기 위한 최소한의 샘플 수\n#         'min_samples_leaf':[i for i in np.arange(5,11,1)]\n    },\n    'RandomForestClassifier':{\n        'criterion':['gini', 'entropy'],\n        'max_depth':[i for i in np.arange(1,5,1,)],\n        'min_samples_split':[i for i in np.arange(5,11,1)]\n#         'min_samples_leaf':[i for i in np.arange(5,11,1)]  \n    },\n    'SVC':{\n        'kernel':['linear','rbf'],\n#         'degree':[i for i in np.arange(5,11,2)]\n#         'gamma':['scale','auto']\n    }\n}\n    \n\nmodels = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),SVC()] \nmodel_names = ['KNeighborsClassifier','DecisionTreeClassifier','RandomForestClassifier','SVC']\n    \nfor model, model_name in zip(models,model_names):\n    print()\n    print('======='+model_name+'========')\n    gcv = GridSearchCV(model,param_grid=param_grid[model_name],scoring='accuracy')\n    gcv.fit(X_train,Y_train)\n    print('train set의 최고 예측 정확도 : {}'.format(gcv.best_score_))\n    print('train set 예측 정확도가 가장 높은 경우의 파라미터 : {}'.format(gcv.best_params_))\n    model = gcv.best_estimator_\n    result = model.predict(X_test)\n\n    # sklearn 을 이용하면 전부 계산해준다.\n    print(model_name,'test set의 예측 정확도 accuracy', accuracy_score(Y_test,result) )\n    print(model_name,'test set의 예측 정확도 precision', precision_score(Y_test,result,average=None) )\n    print(model_name,'test set의 예측 정확도 recall', recall_score(Y_test,result,average=None) )\n    print(model_name,'test set의 예측 정확도 f1', f1_score(Y_test,result,average=None) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 머신러닝 모델링 - 타이타닉 데이터 활용\n# 데이터셋을 test와 valid셋으로 분리 \nfrom sklearn.model_selection import train_test_split\n# 모델별로 데스트 해보기\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nX = ibmhr.drop(columns=['Attrition'])\ny = ibmhr['Attrition']\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33)\n\nparam_grid = {\n    'KNeighborsClassifier':{\n        # n_neighbors, n개의 근접 이웃을 기준으로 판단할지 결정 \n        'n_neighbors':[i for i in np.arange(1,20)],\n        # 근접 이웃에 따라서 동일하게 고려할지, 거리에 따라서 가중치를 고려할지를 결정 \n        'weights':['uniform','distance'],\n        # 데이터간 거리를 측정하는 방법을 결정, kdtree를 좀 더 최적화한 것이 balltree\n        'algorithm':['auto','ball_tree','kd_tree']\n    },\n    'DecisionTreeClassifier':{\n        # 트리를 만들 때 불순도 impurity가 가장 낮은 방향으로 트리를 만들어야함\n        'criterion':['gini', 'entropy'],\n        'splitter':['best','random'],\n        'max_depth':[i for i in np.arange(1,5,1)],\n        # 노트를 분할하기 위한 최소한의 샘플 수 \n        'min_samples_split':[i for i in np.arange(5,11,1)]\n        # 리프 노드가 되기 위한 최소한의 샘플 수\n#         'min_samples_leaf':[i for i in np.arange(5,11,1)]\n    },\n    'RandomForestClassifier':{\n        'criterion':['gini', 'entropy'],\n        'max_depth':[i for i in np.arange(1,5,1,)],\n        'min_samples_split':[i for i in np.arange(5,11,1)]\n#         'min_samples_leaf':[i for i in np.arange(5,11,1)]  \n    },\n    'SVC':{\n        'kernel':['linear','rbf'],\n#         'degree':[i for i in np.arange(5,11,2)]\n#         'gamma':['scale','auto']\n    }\n}\n    \n\nmodels = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),SVC()] \nmodel_names = ['KNeighborsClassifier','DecisionTreeClassifier','RandomForestClassifier','SVC']\n    \nfor model, model_name in zip(models,model_names):\n    print()\n    print('======='+model_name+'========')\n    gcv = GridSearchCV(model,param_grid=param_grid[model_name],scoring='accuracy')\n    gcv.fit(X_train,Y_train)\n    print('train set의 최고 예측 정확도 : {}'.format(gcv.best_score_))\n    print('train set 예측 정확도가 가장 높은 경우의 파라미터 : {}'.format(gcv.best_params_))\n    model = gcv.best_estimator_\n    result = model.predict(X_test)\n    print(model_name,'test set의 예측 정확도 : {}'.format(accuracy_score(Y_test,result)))\n    \n    # sklearn 을 이용하면 전부 계산해준다.\n    print(model_name,'test set의 예측 정확도 accuracy', accuracy_score(Y_test,result) )\n    print(model_name,'test set의 예측 정확도 precision', precision_score(Y_test,result,average=None) )\n    print(model_name,'test set의 예측 정확도 recall', recall_score(Y_test,result,average=None) )\n    print(model_name,'test set의 예측 정확도 f1', f1_score(Y_test,result,average=None) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}