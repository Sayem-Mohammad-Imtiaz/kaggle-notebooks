{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option(\"max_columns\", None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tracks = pd.read_csv(\"../input/spotify-dataset-19212020-160k-tracks/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks['decade'] = tracks.year.apply(lambda year : year-(year%10))\nf, ax = plt.subplots(figsize=(18, 7))\nax=sns.set_style('darkgrid')\nax=sns.distplot(tracks['decade'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sample =tracks.loc[(tracks['year'] >= 1950) & (tracks['year'] < 2000)]\nf, ax = plt.subplots(figsize=(18, 7))\nax=sns.set_style('darkgrid')\nax=sns.distplot(tracks_sample['decade'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda = tracks_sample.drop(columns=['year', 'key', 'artists', 'release_date', 'name', 'explicit', 'mode', 'id'])\ntracks_eda.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf=tracks_sample\n\nfig, ax=plt.subplots(1,2, figsize=(16, 5))\nsns.barplot(x=\"mode\", y=\"mode\", data=df, estimator=lambda x: len(x) / len(df) * 100, ax=ax[0])\nsns.barplot(x=\"explicit\", y=\"explicit\", data=df, estimator=lambda x: len(x) / len(df) * 100, ax=ax[1])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sample['key'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sample['artists'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ntracks_eda_scaled = pd.DataFrame(MinMaxScaler().fit_transform(tracks_eda)).rename(columns={0:'valence',1: 'acousticness',2: 'danceability',\n                                                                 3:'duration_ms',4:'energy', 5:'instrumentalness',\n                                                                 6:'liveness',7:'loudness',8:'popularity',\n                                                                 9:'speechiness', 10:'tempo', 11:'decade'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nax = sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(tracks_eda_scaled))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_scaled_w_orig = pd.merge(tracks_eda,tracks_eda_scaled,how = 'left',left_index = True, right_index = True)\ntracks_scaled_w_orig.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_scaled_w_orig.to_csv('tracks_scaled_w_orig.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_scaled_w_orig_melt=pd.melt(tracks_scaled_w_orig)\ntracks_scaled_w_orig_melt.to_csv('tracks_scaled_w_orig_melt.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=tracks_eda_scaled.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(10, 8))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean it up\n#create a list of our conditions\ndecade_conditions = [\n    (tracks_eda_scaled['decade'] == 0.00),\n    (tracks_eda_scaled['decade'] == .25),\n    (tracks_eda_scaled['decade'] == .50),\n    (tracks_eda_scaled['decade'] == .75),\n    (tracks_eda_scaled['decade'] == 1.00)\n    ]\n\n# create a list of the values we want to assign for each conditionen = ['1', '2', '3', '4']\ndecade_values = ['1950', '1960', '1970', '1980', '1990']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntracks_eda_scaled['decade2'] = np.select(decade_conditions, decade_values)\ntracks_eda_scaled.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = tracks_eda_scaled.groupby(['decade']).quantile(0.50).columns\n#labels = [\"{:02d}'s\".format(l%100) for l in sorted(tracks_eda_scaled.decade2.unique())]\nfig, ax = plt.subplots(figsize=(20,5)) \nsns.heatmap(tracks_eda_scaled.groupby(['decade']).quantile(0.50).iloc[:,1:20])\nplt.ylabel(\"Release Decade\")\nplt.xlabel(\"Features (Mean)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot(query, y):\n    f, ax = plt.subplots(figsize=(10, 5))\n    ax = sns.boxplot(x=\"decade\", y=y, data=tracks.query(query))\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n    plt.show()\n    \nboxplot('year >= 1950 & year <= 2000', \"acousticness\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('year >= 1950 & year <= 2000', \"loudness\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('year >= 1950 & year <= 2000', \"energy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot('year >= 1950 & year <= 2000', \"danceability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(7, 3))\nax=sns.set_style('darkgrid')\nax=sns.distplot(tracks_eda_scaled['acousticness'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"25th percentile: \"+tracks_eda_scaled.acousticness.quantile(0.25).astype(str)) \nprint(\"50th percentile: \"+tracks_eda_scaled.acousticness.quantile(0.50).astype(str)) \nprint(\"75th percentile: \"+tracks_eda_scaled.acousticness.quantile(0.75).astype(str)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of our conditions\nac_conditions = [\n    (tracks_eda_scaled['acousticness'] <= .122),\n    (tracks_eda_scaled['acousticness'] > .122) & (tracks_eda_scaled['acousticness'] <= .520),\n    (tracks_eda_scaled['acousticness'] > .520) & (tracks_eda_scaled['acousticness'] <= .842),\n    (tracks_eda_scaled['acousticness'] > .842)\n    ]\n\n# create a list of the values we want to assign for each condition\nac_values = ['1', '2', '3', '4']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntracks_eda_scaled['ac'] = np.select(ac_conditions, ac_values)\n\n# display updated DataFrame\ntracks_eda_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(7, 3))\nax=sns.set_style('darkgrid')\nax=sns.distplot(tracks_eda_scaled['energy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"25th percentile: \"+tracks_eda_scaled.energy.quantile(0.25).astype(str)) \nprint(\"50th percentile: \"+tracks_eda_scaled.energy.quantile(0.50).astype(str)) \nprint(\"75th percentile: \"+tracks_eda_scaled.energy.quantile(0.75).astype(str)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of our conditions\nen_conditions = [\n    (tracks_eda_scaled['energy'] <= .272),\n    (tracks_eda_scaled['energy'] > .272) & (tracks_eda_scaled['energy'] <= .472),\n    (tracks_eda_scaled['energy'] > .472) & (tracks_eda_scaled['energy'] <= .691),\n    (tracks_eda_scaled['energy'] > .691)\n    ]\n\n# create a list of the values we want to assign for each conditionen = ['1', '2', '3', '4']\nen_values = ['1', '2', '3', '4']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntracks_eda_scaled['en'] = np.select(en_conditions, en_values)\n\n# display updated DataFrame\ntracks_eda_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(7, 3))\nax=sns.set_style('darkgrid')\nax=sns.distplot(tracks_eda_scaled['loudness'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"25th percentile: \"+tracks_eda_scaled.loudness.quantile(0.25).astype(str)) \nprint(\"50th percentile: \"+tracks_eda_scaled.loudness.quantile(0.50).astype(str)) \nprint(\"75th percentile: \"+tracks_eda_scaled.loudness.quantile(0.75).astype(str)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of our conditions\nld_conditions = [\n    (tracks_eda_scaled['loudness'] <= .711),\n    (tracks_eda_scaled['loudness'] > .711) & (tracks_eda_scaled['loudness'] <= .764),\n    (tracks_eda_scaled['loudness'] > .764) & (tracks_eda_scaled['loudness'] <= .809),\n    (tracks_eda_scaled['loudness'] > .809)\n    ]\n\n# create a list of the values we want to assign for each conditionen = ['1', '2', '3', '4']\nld_values = ['1', '2', '3', '4']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntracks_eda_scaled['ld'] = np.select(ld_conditions, ld_values)\n\n# display updated DataFrame\ntracks_eda_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda_scaled['cluster']=tracks_eda_scaled['ac'].astype(str)+tracks_eda_scaled['en'].astype(str)+tracks_eda_scaled['ld'].astype(str)\ntracks_eda_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda_scaled.to_csv('tracks_w_simple_cluster.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda_scaled['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_clus = pd.DataFrame(tracks_eda_scaled.groupby(['decade', 'cluster']).count().valence)\ntracks_clus.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sum = pd.DataFrame(tracks_eda_scaled.groupby(['decade']).count().valence)\ntracks_sum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_clus=tracks_clus.reset_index()\ntracks_sum=tracks_sum.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_clus.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sum.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sum_all = pd.merge(tracks_clus, tracks_sum, on='decade', how='left')\ntracks_sum_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sum_all['pct_clus']=tracks_sum_all['valence_x']/tracks_sum_all['valence_y']\ntracks_sum_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_sum_all.to_csv('tracks_sum_all.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_max_clus = pd.DataFrame(tracks_sum_all.groupby(['decade']).max().pct_clus)\ntracks_max_clus=tracks_max_clus.reset_index()\ntracks_max_clus.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_max_clus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_max_clus2 = pd.merge(tracks_sum_all, tracks_max_clus, on='pct_clus', how='inner')\ntracks_max_clus2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean it up\n# create a list of our conditions\ndecade_conditions = [\n    (tracks_max_clus2['decade_x'] == 0.00),\n    (tracks_max_clus2['decade_x'] == .25),\n    (tracks_max_clus2['decade_x'] == .50),\n    (tracks_max_clus2['decade_x'] == .75),\n    (tracks_max_clus2['decade_x'] == 1.00)\n    ]\n\n# create a list of the values we want to assign for each conditionen = ['1', '2', '3', '4']\ndecade_values = ['50s', '60s', '70s', '80s', '90s']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntracks_max_clus2['decade'] = np.select(decade_conditions, decade_values)\ntracks_max_clus2['cluster_count'] = tracks_max_clus2['valence_x']\ntracks_max_clus2['total_decade_song_count'] = tracks_max_clus2['valence_y']\n\n#drop all of the extra columns\ntracks_max_clus2=tracks_max_clus2.drop(columns=['decade_x', 'decade_y', 'valence_x', 'valence_y'])\n\n# display updated DataFrame\ntracks_max_clus2 = tracks_max_clus2[['decade', 'cluster', 'cluster_count', 'total_decade_song_count', 'pct_clus']]\ntracks_max_clus2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#name the clusters\n#create a list of our conditions\ncname_conditions = [\n    (tracks_max_clus2['cluster'] == '411'),\n    (tracks_max_clus2['cluster'] == '144')\n    \n    ]\n\n#create a list of the values we want to assign for each conditionen = ['1', '2', '3', '4']\ncname_values = ['Acoustic,Quiet,Low Energy', 'Not Acoustic, Loud, High Energy']\n\n#create a new column and use np.select to assign values to it using our lists as arguments\ntracks_max_clus2['cluster_name'] = np.select(cname_conditions, cname_values)\ntracks_max_clus2 = tracks_max_clus2[['decade', 'cluster', 'cluster_name','cluster_count', 'total_decade_song_count', 'pct_clus']]\ntracks_max_clus2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_max_clus2.to_csv('tracks_largest_cluster.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kneed\nfrom sklearn.cluster import KMeans\nfrom kneed import KneeLocator\nfrom sklearn.metrics import silhouette_score\n\nkmeans_kwargs = {\"init\": \"k-means++\",\"n_init\": 10,\"max_iter\": 300,\"random_state\": 0}\n\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 11):\n        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n        kmeans.fit(tracks_eda_scaled.drop(columns=['decade', 'popularity']))\n        sse.append(kmeans.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.labels_[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 11), sse)\nplt.xticks(range(1, 11))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kl = KneeLocator(range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n\nkl.elbow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set number of clusters\nkclusters = 3\n\n# run k-means clustering\nkmeans = KMeans(init=\"k-means++\",n_clusters=kclusters,n_init=10,max_iter=300,random_state=0).fit(tracks_eda_scaled.drop(columns=['decade', 'popularity']))\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tracks_eda_scaled.drop(columns=['Kmeans'], inplace=True)\n#tracks_eda_scaled.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#kmeans.labels_[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add clustering labels\ntracks_eda_scaled.insert(0, 'Kmeans', kmeans.labels_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2).fit(tracks_eda_scaled.drop(columns=['decade', 'Kmeans', 'popularity']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_pca = pca.transform(tracks_eda_scaled.drop(columns=['decade', 'Kmeans', 'popularity']))\nprint(tracks_eda_scaled.drop(columns=['decade', 'Kmeans']).shape, x_pca.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent = pca.explained_variance_ratio_\nprint(percent)\nprint(sum(percent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pca_explained(X, threshold):\n    '''\n    prints optimal principal components based on threshold of PCA's explained variance\n\n    Parameters\n    ----------\n    X : dataframe or array\n        of features\n    threshold : float < 1\n        percentage of explained variance as cut off point\n    '''\n\n    # find total no. of features\n    features = X.shape[1]\n    # iterate till total no. of features,\n    # and find total explained variance for each principal component\n    for i in range(2, features):\n        pca = PCA(n_components = i).fit(X)\n        sum_ = pca.explained_variance_ratio_\n        # add all components explained variances\n        percent = sum(sum_)\n        print('{} components at {:.2f}% explained variance'.format(i, percent*100))\n        if percent > threshold:\n            break\n\npca_explained(tracks_eda_scaled.drop(columns=['decade', 'Kmeans', 'popularity']), 0.85)\n# 2 components at 61.64% explained variance\n# 3 components at 77.41% explained variance\n# 4 components at 86.63% explained variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0], x_pca[:,1], c=tracks_eda_scaled['Kmeans'], cmap='plasma', alpha=0.4, edgecolors='black', s=65);\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 4))\nplt.imshow(pca.components_, interpolation = 'none', cmap = 'plasma')\nfeature_names = list(tracks_eda_scaled.drop(columns=['decade', 'Kmeans', 'popularity']).columns)\n\nplt.gca().set_xticks(np.arange(-.5, len(feature_names)));\nplt.gca().set_yticks(np.arange(0.5, 2));\nplt.gca().set_xticklabels(columns, rotation=90, ha='left', fontsize=12);\nplt.gca().set_yticklabels(['First PC', 'Second PC'], va='bottom', fontsize=12);\n\nplt.colorbar(orientation='horizontal', ticks=[pca.components_.min(), 0,\n                                              pca.components_.max()], pad=0.65);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the target feature for decade 1990\n# create a list of our conditions\ntarget_conditions = [\n    (tracks_eda_scaled['decade2'] == '1950'),\n    (tracks_eda_scaled['decade2'] == '1960'),\n    (tracks_eda_scaled['decade2'] == '1970'),\n    (tracks_eda_scaled['decade2'] == '1980'),\n    (tracks_eda_scaled['decade2'] == '1990')\n    ]\n\n# create a list of the values we want to assign for each conditionen = ['1', '2', '3', '4']\ntarget_values = [0, 0, 0, 0, 1]\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ntracks_eda_scaled['target'] = np.select(target_conditions, target_values)\ntracks_eda_scaled_modeling=tracks_eda_scaled.drop(columns=['decade', 'decade2'])\ntracks_eda_scaled_modeling.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tracks_eda_scaled_modeling.groupby(['decade2']).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracks_eda_scaled.to_csv('check_target_tags2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tracks_eda_scaled_modeling=tracks_eda_scaled.drop(columns=['decade2'])\ntracks_eda_scaled_modeling = tracks_eda_scaled_modeling.apply(pd.to_numeric)\ntracks_eda_scaled_modeling.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into X and y\nX = tracks_eda_scaled_modeling.drop(columns=['target', 'popularity'])\nY = tracks_eda_scaled_modeling.drop(columns=['acousticness','Kmeans', 'valence', 'loudness', 'danceability', 'energy','duration_ms', 'instrumentalness', 'liveness','popularity', 'speechiness', 'tempo', 'ac', 'en', 'ld', 'cluster'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Y=np.ravel(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total records:\" + str(len(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% True:\" + np.mean(Y).astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train and test sets\nseed = 7\ntest_size = 0.70\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model on training data\nmodel = XGBClassifier()\neval_set = [(X_test, y_test)]\nmodel.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions for test data\ny_hats = model.predict(X_test)\npredictions = [round(value) for value in y_hats]\npredictions[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hats_df=pd.DataFrame(y_hats, columns = ['y_hats'])\ny_hats_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(X_test)\npred_df = pd.DataFrame(preds).rename(columns={0:'prob_no', 1:'prob_yes'})\npred_df.loc[(check2[1]>0),:].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_merge = pd.merge(pred_df, y_hats_df, left_index = True, right_index = True)\npred_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hats_df = pd.DataFrame(data = y_hats, columns = ['y_hats'], index = X_test.index.copy())\ndf_out_test = pd.merge(tracks_eda_scaled_modeling, pred_merge, left_index = True, right_index = True)\ndf_out_test.to_csv('testing_y_hats_merge.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_out_test2 = pd.merge(df_out_test, tracks_eda_scaled, left_index = True, right_index = True)\ndf_out_test2.to_csv('testing_y_hats_merge2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importance(model)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define data_dmatrix\ndata_dmatrix = xgb.DMatrix(data=X,label=Y)\n\nfrom xgboost import cv\n\nparams = {\"objective\":\"binary:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\nxgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50, early_stopping_rounds=10, metrics=\"auc\", as_pandas=True, seed=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_cv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nxg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\nxgb.plot_tree(xg_reg,num_trees=0,rankdir='LR')\nplt.rcParams['figure.figsize'] = [25, 5]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}