{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a> \n# Salinity CalCOFI: Data Clean, Correlation, Visualizations and Folium Map\n\nThe CalCOFI data set represents the longest (1949-present) and most complete (more than 50,000 sampling stations) time series of oceanographic and larval fish data in the world. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term “El Niño” into the scientific literature. \n\nThis analysis involves CalCOFI data cleaning, correlation, visualization and folium map.\n\n\n### Table Of Content\n1.  [Data Collection](#coll)<br>\n\n2.  [Understanding the Data](#data)<br>\n2.1  [Data Types](#data_info)<br>\n2.2  [Statistical Summary](#data_summ)<br>\n\n3.  [Data Cleaning](#prep)<br>\n3.1  [Check for NULLs/Duplicates](#prep_nulls)<br>\n3.2  [Extract Month/Year from Depth_ID](#prep_extract)<br>\n3.3  [Drop columns that cannot be Normalized](#prep_drop)<br>\n\n4.  [Correlations](#corr)<br>\n4.1  [Normalization](#corr_norm)<br>\n4.2  [Correlation - Salinity](#corr_saln)<br>\n\n5. [Data Visualization](#eda)<br>\n5.1  [Salinity Plots](#eda_saln)<br>\n5.2  [Distribution Plots - Correlation](#eda_dist)<br>\n5.3  [Regression Plots - Correlation](#eda_regr)<br>\n\n6.  [Map - Collection Station Locations](#map)<br>   \n"},{"metadata":{},"cell_type":"markdown","source":"---\n#  1.  Data Collection <a id=\"coll\"></a>\n###  Import Python Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n#  maps\nimport folium\nfrom folium.plugins import MarkerCluster\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 80)\n\n#  Kaggle directories\nimport os\nprint(os.listdir(\"../input\"))\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Load the Datasets"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#  bottle.csv contains information on ocean conditions\n#  cast.csv   contains information on collecting stations\ndf = pd.read_csv('../input/bottle.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[go to top of document](#top)     \n\n---\n#  2.  Understanding the Data <a id=\"data\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##  2.1  Data Types<a id=\"data_info\"></a>\n-  **Categorical data** - collection station information - for map\n-  **Numerical data** - scientific data\n\n\nThe collection station locations will be used for the Folium map, but the main attributes for this analysis will be:\n\n*  **Depth_ID**  - extract months and years\n*  **Salnty**  - Salinity in g of salt per kg of water (g/kg).  _Target_ variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  2.2  Statistical Summary <a id=\"data_summ\"></a>\nSummarize descriptive statistics of the dataset for *numerical* and *categorical* features. "},{"metadata":{},"cell_type":"markdown","source":"**Statistical Summary - NUMERICAL DATA**   \nSummarize the central tendency, dispersion and shape of numeric features, excluding categorical and NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()   #  NUMERICAL DATA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Statistical Summary - CATEGORICAL DATA**   \nSummarize the count, uniqueness and frequency of categorical features, excluding numerical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=['O'])   #  CATEGORICAL DATA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[go to top of document](#top)     \n\n---\n#  3.  Data Cleaning <a id=\"prep\"></a>\nClean the data before begining any type of analysis."},{"metadata":{},"cell_type":"markdown","source":"## 3.1  Check for NULLs/Duplicates <a id=\"prep_nulls\"></a>\nCleaning up the NULL and duplicate values in the dataset:\n\n*  3.1.1  Check for NULL percentages\n*  3.1.2  Drop attributes with more than 30% data missing\n*  3.1.3  Fill remaining NULLs with **mean** values\n*  3.1.4  Re-check NULL Percentages\n*  3.1.5  Check for duplicated"},{"metadata":{},"cell_type":"markdown","source":"###  3.1.1  Check for NULL percentages"},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls = df.isnull().sum().sort_values(ascending = False)\nprcet = round(nulls/len(df)*100,2)\n\ndf_null = pd.DataFrame(columns =  ['Attr','Total','Percent'])\ndf_null.Attr  = nulls.index\ndf_null.Total = nulls.values\ndf_null.Percent = prcet.values\nprint(df_null.head(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.1.2  Drop attributes with more than 30% data missing"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df_null.Attr[df_null['Percent'] > 30]:\n    df = df.drop([i], axis=1)\n    #print(df.shape,i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.1.3  Fill remaining NULLs with **mode** values\nSome attributes have more than one mode.  Take mean of the multiple modes for the 'fillna' value."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    if df[i].isnull().sum() > 0:\n        df[i].fillna(df[i].mode().mean(), inplace=True)\n        #print('filled',i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.1.4  Re-check NULL Percentages\nShows attributes for that can be used for EDA and Correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls = df.isnull().sum().sort_values(ascending = False)\nprcet = round(nulls/len(df)*100,2)\n\ndf_null = pd.DataFrame(columns =  ['Attr','Total','Percent'])\ndf_null.Attr  = nulls.index\ndf_null.Total = nulls.values\ndf_null.Percent = prcet.values\nprint(df_null.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  3.1.5  Check for Duplicated values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('COUNT OF DUPLICATES:  {}'.format(df.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2  Extract Month/Year from Depth_ID <a id=\"prep_extract\"></a>\n_cast.csv_ file has the month/year data, but due to the size of the _bottle.csv_, it's be easier to extract it from here."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Depth_ID = [Century]-[YY][MM][ShipCode]-etc\n#  19-4903CR-HY-060-0930-05400560-0020A-7\ndf['Year'] = (df['Depth_ID'].str.split('-', expand=True)[0] + \\\n                df['Depth_ID'].str.split('-', expand=True)[1]). \\\n                map(lambda x: str(x)[:4])\ndf['Month'] = (df['Depth_ID'].str.split('-', expand=True)[1]). \\\n                 map(lambda x: str(x)[2:4])\n                 \ndf[['Depth_ID','Year','Month']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3  Drop columns that cannot be Normalized <a id=\"prep_drop\"></a>\nDropping columns that cannot be normalized.\n   - Cst_Cnt   Auto-numbered Cast Count\n   - Btl_Cnt   Auto-numbered Bottle count\n   - Sta_ID    CalCOFI Line and Station\n   - Depth_ID  [Century]-[YY][MM][ShipCode]"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"drop_cols = ['Cst_Cnt', 'Btl_Cnt', 'Sta_ID', 'Depth_ID', 'Depthm','Year','Month']\ndf_norm = df.drop(drop_cols, axis=1)  #  data for normalization\ndf_scale = df_norm.copy(deep=True)    #  backup data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[go to top of document](#top)     \n\n---\n# 4.  Correlation<a id=\"corr\"></a>\nCorrelation is a statistical metric for measuring to what extent different variables are interdependent.  In order to perform correlation, we need to first normalize the data."},{"metadata":{},"cell_type":"markdown","source":"## 4.1  Normalization<a id=\"corr_norm\"></a>\nNormalization is a rescaling of the data from the original range so that all values are within a certain range, typically between 0 and 1. Normalized data is essential in machine learning. Correlation and models will not produce good results if the scales are not standardized.\n\nData in **df_corr** will be normalized and the **df** data frame will be updated with the encoded and normalized data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scale = StandardScaler().fit_transform(df_scale)\n\n#  create dataframe\ndf_norm = pd.DataFrame(df_scale, index=df_norm.index, columns=df_norm.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2  Correlation - Salinity<a id=\"corr_saln\"></a>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df_norm.corr()\n\n#  Drop columns with mode = \"0.0\".  No impact on correlation\nfor i in df_norm.columns.tolist():\n    if (df_norm[i].mode()[0] == 0.0):\n        print(' - ',i,df_norm[i].mode()[0])\n        df_norm = df_norm.drop(i,axis=1)\n\n#  Create correlation dataframe\ndf_corr = pd.DataFrame(columns=['Attributes','Correlation'])\ndf_corr.Attributes = df_norm.corr()['Salnty'].sort_values(ascending=False).index\ndf_corr.Correlation = df_norm.corr()['Salnty'].sort_values(ascending=False).values\nprint(df_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[go to top of document](#top)     \n\n---\n#  5.  Data Visualization <a id=\"eda\"></a>\nSome of the attribute values will result in similar plots, i.e. R_O2Sat and O2Sat.  In that case, only one observation will be selected for plotting the distribution and regression plots.  For this project, nine observations were selected.\n\n**df_sample** - dataframe size will be sampled (reduced) in order to have cleaner plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  observations for plotting\nplot_attr = ['R_DYNHT', 'R_SIGMA', 'R_Depth', 'RecInd', 'NH3q',  'T_prec', 'T_degC', 'R_POTEMP', 'O2ml_L']\n\nfor i in plot_attr:\n    if plot_attr[0] == i:\n        df_plot = df_corr[df_corr.Attributes == i]\n    else:\n        df_plot = df_plot.append(df_corr[df_corr.Attributes == i])\nprint(df_plot)\n\n#  take sample of data for plotting\ndf_sample = df_norm.sample(n=int(round(len(df)*.002,0)), random_state=0)\nprint('\\n\\nPlotting data shape: {}'.format(df_sample.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  5.1  Salinity Plots <a id=\"eda_saln\"></a>\nSalinity plots will use the complete dataset."},{"metadata":{},"cell_type":"markdown","source":"###  5.1.1  Salinity Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Salinity distribution\nplt.figure(figsize=(8,6))\nplt.xlim([32, 36])#  Salinity distribution\nplt.title('Salinity Distribution (g/Kg)', fontsize=14)\nsns.distplot(df['Salnty'], color='darkgreen')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  5.1.2  Plot of Salinity over Time"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#  Yearly change in Salinity\nfig = plt.figure(figsize=(12,6))\nfig.autofmt_xdate()\nfig.add_subplot(121)\nplt.title('Yearly Change in Salinity (g/Kg)', fontsize=14)\nsns.scatterplot(data=df, x='Year', y='Salnty', color='darkgreen')\n\n#  Seasonal change in Salinity\nfig.add_subplot(122)\nplt.title('Seasonal Change in Salinity (g/Kg)', fontsize=14)\nsns.scatterplot(data=df, x='Month', y='Salnty', color='darkgreen')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  5.2  Distribution Plots for Correlations <a id=\"eda_dist\"></a>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,60))\ncol = 3\nrow  = int(len(df_corr.Attributes)/col)\ncount = 1\n\nfor i, j in zip(df_plot.Attributes,df_plot.Correlation):\n    fig.add_subplot(row, col, count)\n    plt.title('Salinity vs {} (corr = {:.4})\\nnormalized distribution'.format(i,j))\n    plt.xlim(-4,4)\n    sns.distplot(df_sample.Salnty)\n    sns.distplot(df_sample[i])\n    count = count + 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  5.3  Regression Plots - Correlation <a id=\"eda_regr\"></a>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,60))\ncol = 3\nrow  = int(len(df_corr.Attributes)/col)\ncount = 1\n\nfor i, j in zip(df_plot.Attributes,df_plot.Correlation):\n    fig.add_subplot(row, col, count)\n    plt.title('Salinity vs {} (corr = {:.4f})\\nnormalized distribution'.format(i,j))\n    sns.regplot(x=df_sample[i],y=\"Salnty\",data=df_sample,order=2, scatter_kws={'alpha':0.25},color='green');\n    count = count + 1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[go to top of document](#top)     \n\n---\n#  6.  Map - Collection Station Locations<a id=\"map\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Load the Dataset\ndfLOC = pd.read_csv('../input/cast.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#  select location points\ndfLOC = dfLOC[['Lat_Dec', 'Lon_Dec','Date']]\ndfLOC = dfLOC.tail(1000)\ndfLOC = dfLOC.reset_index(drop=True)  # reset index after tail\n\n#  create folium map\nsalinity_map   = folium.Map(location=[dfLOC.Lat_Dec.mean(),dfLOC.Lon_Dec.mean()], zoom_start=6)\nmarker_cluster = MarkerCluster().add_to(salinity_map)\n\nfor i in range(len(dfLOC)):\n    folium.Marker(location=[dfLOC.Lat_Dec[i],dfLOC.Lon_Dec[i]],\n            popup = (dfLOC.Date[i]),         # dates in popups\n            icon = folium.Icon(color='green')  # green popup icon\n    ).add_to(marker_cluster)\n\nsalinity_map.add_child(marker_cluster)\nsalinity_map         #  display map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\nPlease upvote if you found this helpful :-)\n###  END\n[go to top of document](#top)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}