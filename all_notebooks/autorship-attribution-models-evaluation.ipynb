{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2222fbb3-6d51-b6b1-b0eb-328063069c1b"},"source":"In this notebook I work on the question  whether the author of a tweet (very short text) can be successfully identified.\nI try to choose the best classification method its parameters set and features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc468d47-0253-f2ee-a7d8-3361bf71d15f"},"outputs":[],"source":"import pandas as pd\nfrom pandas import Series,DataFrame\nimport numpy as np"},{"cell_type":"markdown","metadata":{"_cell_guid":"afb38dcb-25f1-4f5e-546e-539bf58aad64"},"source":"I choose binary classification problem for the start and use Kim Kardashian and Hillary Clinton tweets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abd7721d-001b-5f30-d54e-4965dda55d5b"},"outputs":[],"source":"#KimKardashianTweets data\ndf_kk=pd.read_csv('../input/KimKardashianTweets.csv')\nlen(df_kk)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a0ff274-f221-b641-3f74-a4445a049003"},"outputs":[],"source":"#HillaryClintonTweets data\ndf_hc=pd.read_csv('../input/HillaryClintonTweets.csv')\nlen(df_hc)"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc3bb467-9c60-abae-9f22-0599d078b418"},"source":"First let's choose the same random number of tweets from both authors"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"599a0fe7-43fa-d19e-6120-a6faeca53444"},"outputs":[],"source":"import random\n#2000 random sample rows for KK\nrows = random.sample(list(df_kk.index), 2000)\ndf_kk = df_kk.ix[rows]\n#2000 random sample rows for HC\nrows = random.sample(list(df_hc.index), 2000)\ndf_hc = df_hc.ix[rows]\n#join back together\ndf=df_kk.append(df_hc,ignore_index=True)\nlen(df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"61b34803-c5f3-489a-13a7-a897168fa0a8"},"source":"For sparsity reasons I pre-process data before analysis:\n\n 1.  removing re-tweets\n 2. removing short messages (less then 4 words)\n 3. replacing @ with REF\n 4. replacing any url with URL\n 5. replacing any date with DATE\n 6. replacing any time with TIME\n 7. replace digits with NUM"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7535b0f-0cd1-ff75-5f6e-592dac0b48a4"},"outputs":[],"source":"#data pre-processing\ndf.drop(df[df.retweet==True].index, inplace=True)\ndf['num_of_words'] = df[\"text\"].str.split().apply(len)\ndf.drop(df[df.num_of_words<4].index, inplace=True)\ndf[\"text\"].replace(r\"http\\S+\", \"URL\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"@\\S+\", \"REF\", regex=True ,inplace=True)\ndf[\"text\"].replace(r\"(\\d{1,2})[/.-](\\d{1,2})[/.-](\\d{2,4})+\", \"DATE\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"(\\d{1,2})[/:](\\d{2})?(am|pm)+\", \"TIME\", regex=True,inplace=True)\ndf[\"text\"].replace(r\"\\d+\", \"NUM\", regex=True,inplace=True)\nlen(df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"70e04bca-9dba-84fd-bfeb-e21d42e90816"},"source":"To avoid overfitting let's hold out a part of the available data as a test set twt_test (X), author_test (Y)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aea09d1c-b450-fd13-47cd-e48dde94197f"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\ntwt_train, twt_test, author_train, author_test = train_test_split(df['text'], df['author'], test_size=0.4, random_state=42)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f2fc9829-6c7d-a052-4b99-ad4e58df2927"},"source":"Train set length:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddee1b7d-5647-f6bc-4dc0-5f3873caa7a8"},"outputs":[],"source":"len(twt_train)"},{"cell_type":"markdown","metadata":{"_cell_guid":"59221c09-911f-8760-a3e5-33312bf87e8f"},"source":"Test set length:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2f84b50-9c78-baa3-9f68-acfe8926ed2b"},"outputs":[],"source":"len(twt_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"278642d1-f46b-df28-aa7a-6535d92a9c65"},"source":"The place to start is to get better results from known classification method that perform well on the problem.\nI test several algorithms using its default parameters as well as CountVectorizer() and TfidfTransformer() with default parameters\nI also apply ten-fold cross validation on the training set to select the best method"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d330c2ea-5b95-cd44-0667-12be398fbca2"},"outputs":[],"source":"ScoreSummaryByModel = list()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c22b3de8-8231-705f-f708-ffe6523ea44c"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cross_validation import cross_val_score\n\ndef ModelEvaluation (model,comment):\n    pipeline = Pipeline([('vect', CountVectorizer())\n                  , ('tfidf', TfidfTransformer())\n                  , ('model', model)])\n    scores = cross_val_score(pipeline, df['text'], df['author'], cv=10)\t\n    mean = scores.mean()\t\n    std = scores.std()\t\n    #The mean score and the 95% confidence interval of the score estimate (accuracy)\n    ScoreSummaryByModel.append([comment,mean, std, \"%0.3f (+/- %0.3f)\" % (mean, std * 2)])\n    print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c1a51bd-ded5-d24a-9f9d-f9d29ea30b91"},"outputs":[],"source":"from sklearn.naive_bayes import MultinomialNB\nModelEvaluation (MultinomialNB(),'Naive Bayes classifier')"},{"cell_type":"markdown","metadata":{"_cell_guid":"96081c9c-7183-014b-3087-a7f5d67bce4c"},"source":"Naive Bayes classifier results are not very impressive. It's just a baseline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad530b62-e7d3-6ace-a8e3-14e8fc8cd731"},"outputs":[],"source":"from sklearn.naive_bayes import BernoulliNB\nModelEvaluation (BernoulliNB(binarize=0.0),'Bernoulli Naive Bayes')"},{"cell_type":"markdown","metadata":{"_cell_guid":"bebee3f8-e1b9-61a2-c3ad-fecce1040a6a"},"source":"Bernoulli Naive Bayes reasults looks better"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8b0dcf9-ed0f-4388-6afd-06b071b40382"},"outputs":[],"source":"from sklearn.svm import LinearSVC\nModelEvaluation (LinearSVC(),'LinearSVC')"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b26bdb5-4327-093d-1dd4-dba0db679129"},"source":"LinearSVC looks Ok Maybe parameters tuning of the method improve the results"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99a2f2a7-81ba-63ea-6588-d95cffb78c17"},"outputs":[],"source":"from sklearn.svm import SVC\nModelEvaluation (SVC(),'SVC, default rbf kernel')"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d538fc1-6f8e-cb1e-00c4-030596941514"},"source":"Ups :( Default rbf kernel definetly does not work"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d98b2fb1-2d2d-2253-45dc-78ea2a456e7f"},"outputs":[],"source":"ModelEvaluation (SVC(kernel='linear'),'SVC, linear kernel')"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d976c75-e7d8-0cc3-1e57-5485b35fafee"},"source":"Well, SVC with linear kernel should be not worse then LinearSVC"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c7fb2a6-20b7-209d-75af-a819d56f3fa3"},"outputs":[],"source":"from sklearn.linear_model import SGDClassifier\nModelEvaluation (SGDClassifier(),'SGD')"},{"cell_type":"markdown","metadata":{"_cell_guid":"ab8ed0d2-4010-4a0d-8938-e262f8c72704"},"source":"Linear classifiers (SVM, logistic regression, a.o.) with SGD training should also be good for the problem. Let's try to optimize its parameters"},{"cell_type":"markdown","metadata":{"_cell_guid":"670b15a0-a475-cbf3-faa8-99f3ec4f298b"},"source":"Here is the summary. Bernoulli Naive Bayes with default parameters returned teh best accuracy"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc2b175c-6371-5a76-ed2c-43f7222b7f77"},"outputs":[],"source":"df_ScoreSummaryByModel=DataFrame(ScoreSummaryByModel,columns=['Method','Mean','Std','Accuracy'])\ndf_ScoreSummaryByModel.sort_values(['Mean'],ascending=False,inplace=True)\ndf_ScoreSummaryByModel"},{"cell_type":"markdown","metadata":{"_cell_guid":"10005404-cda4-0c7a-8499-9455489e6832"},"source":"For better features extractions let's try tokenize the text, remove stopwords (including the placeholders we added at the stage of data pre-processing) and stem the words\nI will use the function in farther analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"816a90e8-91ec-2c41-2f39-eaae011e64c6"},"outputs":[],"source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\ndef text_process(text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Tokenizes and removes punctuation\n    2. Removes  stopwords\n    3. Stems\n    4. Returns a list of the cleaned text\n    \"\"\"\n\n    # tokenizing\n    tokenizer = RegexpTokenizer(r'\\w+')\n    text_processed=tokenizer.tokenize(text)\n    \n    # removing any stopwords\n    stoplist = stopwords.words('english')\n    stoplist.append('twitter')\n    stoplist.append('pic')\n    stoplist.append('com')\n    stoplist.append('net')\n    stoplist.append('gov')\n    stoplist.append('tv')\n    stoplist.append('www')\n    stoplist.append('twitter')\n    stoplist.append('num')\n    stoplist.append('date')\n    stoplist.append('time')\n    stoplist.append('url')\n    stoplist.append('ref')\n\n    text_processed = [word.lower() for word in text_processed if word.lower() not in stoplist]\n    \n    # steming\n    porter_stemmer = PorterStemmer()\n    \n    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n    \n\n    return text_processed"},{"cell_type":"markdown","metadata":{"_cell_guid":"b1d017a7-23e8-ded0-df28-d29d3c350e39"},"source":"I use the grid search approach for parameter tuning. It will methodically build and evaluate a model for each combination of \nalgorithm parameters and feature sets."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5817d4de-54b2-0581-4707-f6dc2c6060ed"},"outputs":[],"source":"ScoreSummaryByModelParams = list()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a733d82-ec5d-3b65-c5b1-fb73542a0167"},"outputs":[],"source":"from sklearn.grid_search import GridSearchCV\ndef ModelParamsEvaluation (vectorizer,model,params,comment):\n    pipeline = Pipeline([\n    ('vect', vectorizer),\n    ('tfidf', TfidfTransformer()),\n    ('clf', model),\n    ])\n    grid_search = GridSearchCV(estimator=pipeline, param_grid=params, verbose=1)\n    grid_search.fit(df['text'], df['author'])\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(params.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n        ScoreSummaryByModelParams.append([comment,grid_search.best_score_,\"\\t%s: %r\" % (param_name, best_parameters[param_name])])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cc55a33-5b24-0710-f948-08904222dfa4"},"outputs":[],"source":"p = {'vect__analyzer':('char', 'char_wb'),\n    'vect__max_df': (0.5, 0.75, 1.0),\n    'vect__ngram_range': ((2, 2), (3, 3)), \n    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\nModelParamsEvaluation(CountVectorizer(),BernoulliNB(),p,'Bernoulli Naive Bayes')"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc82a07d-c24c-39ab-1d14-4ca111d80268"},"source":"It makes sense. 3 chars is almost a word (in actual use, the average is 4.79 letters per word, and 80% are between 2 and 7 letters long according to http://norvig.com/mayzner.html). Other classification methods tests provided the same result - using 3 chars is the best approach. I do not include the results for other methods in the notebook. I tested them on my local computer. Interesting, the score is higher then for word analyzer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de8f8060-d5ab-ab3b-087c-de23fe932675"},"outputs":[],"source":"p = {\n    'vect__max_df': (0.5, 0.75, 1.0),\n    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)), \n    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\nModelParamsEvaluation(CountVectorizer(analyzer='word'),BernoulliNB(),p,'Bernoulli Naive Bayes, analyzer=word')"},{"cell_type":"markdown","metadata":{"_cell_guid":"931eeeb2-59b6-95dd-a7d6-8b0462d05138"},"source":"I, actually, expected better results for longer ngrams. Maybe becasue tweets are short messages, unigrams make more sense. Other classification methods tests provided the same result - using unigrams is the best approach. I do not include the results for other methods in the notebook. I tested them on my local computer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dcfe4bd6-b413-f091-835f-30e458e3bdac"},"outputs":[],"source":"p = {\n    'vect__max_df': (0.5, 0.75, 1.0),\n    'vect__ngram_range': ((1, 1), (3, 3), (5,5),(2,5)), \n    'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\nModelParamsEvaluation(CountVectorizer(analyzer='word',tokenizer=text_process),BernoulliNB(),p,'Bernoulli Naive Bayes, analyzer=word, tokenizer=text_process')"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e5c809d-dd54-d045-8416-5ed22770969c"},"source":"Not a big difference if I use tokenizer. The result even slightly worse. Other classification methods tests provided the same result. I do not include the results for other methods in the notebook. I tested them on my local computer"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6ffbacc-3df1-8d3e-b101-4d59d1dd137f"},"source":"Now let's try the combination of all vectors with discovered parameters. FeatureUnion concatenates results of multiple transformer objects."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d9090b7-f80d-0c4f-93ce-97a71dc5af04"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import FeatureUnion\nword_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 1),max_df=0.5)\nchar_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\nvectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector)])\np = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\nModelParamsEvaluation(vectorizer,BernoulliNB(),p,'Bernoulli Naive Bayes, vectorizer')"},{"cell_type":"markdown","metadata":{"_cell_guid":"484786f3-197d-569a-8b73-9b868922f06f"},"source":"The result is becomes better on the combination.\nLet's try to add tokenized vector"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"937bd0ae-277d-8d90-d861-7a48c8398f7c"},"outputs":[],"source":"word_vector =  CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\nchar_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\ntext_vector = CountVectorizer(analyzer='word',tokenizer=text_process,ngram_range=(3, 3),max_df=0.75)\nvectorizer = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector),(\"text\", text_vector)])\np = {'clf__alpha': (1,0.1,0.01,0.001,0.0001,0)}\nModelParamsEvaluation(vectorizer,BernoulliNB(),p,'Bernoulli Naive Bayes, vectorizer+text_vector')"},{"cell_type":"markdown","metadata":{"_cell_guid":"46650358-e001-77b9-9b19-f9690adb7504"},"source":"The result improves a little bit."},{"cell_type":"markdown","metadata":{"_cell_guid":"d28e426a-c1f9-67a8-6c57-b17b9813c4d0"},"source":"Here is the summary. And the winners are:\n\n 1. LinearSVC (C=1, char 3-ngrams)\n 2. SVC with linear kernel (C=1, char 3-ngams)\n 3. SGDClassifier (alpha: 0.0001, char 3-ngams)\n 4. Bernoulli Naive Bayes (alpha = 0.001 based on combination of character 3-ngrams, word unigrams and tokenized unigram)\n\nAll 4 of them returns approximately the same accuracy - 0.967 - 0.970\nYou can find the full summary on my GitHub project page\nhttps://github.com/KaterynaD/TweetsAutorshipAttributionModelsEvaluation/blob/master/Autorship%2Battribution%2Bmodels%2Bevaluation.ipynb\nI do not include all tests in Kaggle notebook due to the long running time \nThe data may be slightly different of course each run"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"157dfaef-9984-97e2-8667-589d6014d3ef"},"outputs":[],"source":"df_ScoreSummaryByModelParams=DataFrame(ScoreSummaryByModelParams,columns=['Method','BestScore','BestParameter'])\ndf_ScoreSummaryByModelParams.sort_values(['BestScore'],ascending=False,inplace=True)\ndf_ScoreSummaryByModelParams"},{"cell_type":"markdown","metadata":{"_cell_guid":"30de1a5f-a390-b461-76d6-7f0248f36230"},"source":"Now let's apply the discovered best approach to test data set\nI use Bernoulli Naive Bayes and SGDClassifier for comparison"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c8b1456-8d42-cd51-6fbc-dfcb6dcc964a"},"source":"We need score metrics and few functions now"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42f0b05a-8168-afff-ef4a-e373f8606dc8"},"outputs":[],"source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\nfrom scipy import interp\n#Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39455671-6e6e-4c95-206a-4d4a4e8551e8"},"outputs":[],"source":"def ROCCurves (Actual, Predicted):\n    '''\n    Plot ROC curves for the multiclass problem\n    based on http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n    '''\n    # Compute ROC curve and ROC area for each class\n    n_classes=2\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(Actual, Predicted)\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Actual.ravel(), Predicted.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    ##############################################################################\n    # Plot ROC curves for the multiclass problem\n\n    # Compute macro-average ROC curve and ROC area\n\n    # First aggregate all false positive rates\n\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr /= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure()\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         linewidth=2)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         linewidth=2)\n\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Some extension of Receiver operating characteristic to multi-class')\n    plt.legend(loc=\"lower right\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8cd2da40-91db-b9b7-627c-14e2d106579b"},"outputs":[],"source":"def ConfusionMatrix(author_test_b,author_predictions_b):\n    cm=confusion_matrix(author_test_b,author_predictions_b)\n    plt.matshow(cm)\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a3fbb85-ed65-ddab-4952-ae9946962114"},"outputs":[],"source":"def PredictionEvaluation(author_test_b,author_predictions_b):\n    print ('Precision: %0.3f' % (precision_score(author_test_b,author_predictions_b)))\n    print ('Accuracy: %0.3f' % (accuracy_score(author_test_b,author_predictions_b)))\n    print ('Recall: %0.3f' % (recall_score(author_test_b,author_predictions_b)))\n    print ('F1: %0.3f' % (f1_score(author_test_b,author_predictions_b)))\n    print ('Confussion matrix:')\n    print (confusion_matrix(author_test_b,author_predictions_b))\n    print ('ROC-AUC: %0.3f' % (roc_auc_score(author_test_b,author_predictions_b)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8feffaf8-54dd-5a13-198f-46f07cd44b2b"},"source":"Feature Vectors"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f34e52c9-e857-ea64-65d8-5a657fe13508"},"outputs":[],"source":"word_vector = CountVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 5),max_df=0.5)\nchar_vector = CountVectorizer(analyzer='char_wb',ngram_range=(3, 3),max_df=0.75)\ntext_vector = CountVectorizer(analyzer='word',tokenizer=text_process,ngram_range=(3, 3),max_df=0.75)\nvectorizer  = FeatureUnion([(\"chars\", char_vector),(\"words\", word_vector),(\"text\", text_vector)])"},{"cell_type":"markdown","metadata":{"_cell_guid":"ce632c1c-773a-18ee-8f18-34d30d073f99"},"source":"Bernoulli Naive Bayes pipeline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d55852cb-843a-2aa9-528c-ce2e10ce6495"},"outputs":[],"source":"pipeline = Pipeline([\n    ('vect', vectorizer),\n    ('tfidf', TfidfTransformer()),\n    ('clf', BernoulliNB(alpha=0.001)),\n    ])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff51c623-fe7f-80f9-5ccb-8bd035eddd87"},"outputs":[],"source":"pipeline.fit(twt_train,author_train)\nauthor_predictions = pipeline.predict(twt_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"76471f77-c7e5-ca22-5c38-5b23d87dddaf"},"source":"To evaluate the results we need to binarize labels(authors) - 'HillaryClinton' is 0 and 'KimKardashian' is 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae071041-58b5-bf08-e7b0-195d26be9146"},"outputs":[],"source":"from sklearn.preprocessing import LabelBinarizer\nlb = LabelBinarizer()\nauthor_test_b = lb.fit_transform(author_test.values)\nauthor_predictions_b  = lb.fit_transform(author_predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9cb37736-9b59-81d1-5376-80c8587abb89"},"outputs":[],"source":"PredictionEvaluation(author_test_b,author_predictions_b)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1d383d9-06ee-ec9b-6bec-cb46c893ea70"},"outputs":[],"source":"ROCCurves (author_test_b,author_predictions_b)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"857a3d03-8c5e-6483-bf9e-5f532ed66f56"},"outputs":[],"source":"ConfusionMatrix(author_test,author_predictions)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8485bc66-61c0-2f6d-ea43-94d4d907afcb"},"source":"SGDClassifier pipeline on the same test data with slightly higher precision but less accuracy"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6813754b-42aa-610a-f2e3-970edf901d46"},"outputs":[],"source":"pipeline = Pipeline([\n    ('vect', CountVectorizer(analyzer='char',ngram_range=(3, 3),max_df=1)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier(alpha=0.0001, penalty='elasticnet')),\n    ])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d157a30-24e5-0291-3e8a-c450a0431171"},"outputs":[],"source":"pipeline.fit(twt_train,author_train)\nauthor_predictions = pipeline.predict(twt_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19881b9a-04ad-4bd2-8135-cdddf695321e"},"outputs":[],"source":"author_test_b = lb.fit_transform(author_test.values)\nauthor_predictions_b  = lb.fit_transform(author_predictions)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c562f32-ba2e-b18a-bab7-dc44728d7272"},"outputs":[],"source":"PredictionEvaluation(author_test_b,author_predictions_b)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc1d1330-0c36-43d5-59f8-37ffa4131128"},"outputs":[],"source":"ROCCurves (author_test_b,author_predictions_b)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49daa8de-d6c5-a62e-3b51-cb2f2ea490f1"},"outputs":[],"source":"ConfusionMatrix(author_test_b,author_predictions_b)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}