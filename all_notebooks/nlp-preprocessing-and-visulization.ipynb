{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import Counter\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nimport sys\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.stem.porter import *\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nfrom sklearn import metrics\nimport pandas as pd \nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import jaccard_similarity_score\ncv = CountVectorizer()\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics.pairwise import cosine_similarity\nstop = set(stopwords.words(\"english\"))\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/obama-white-house.csv\"]).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/obama-white-house.csv\",nrows=1000)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d67dce4090cfa63b3962a2ec71fbde392cb6ea96"},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"021788fc1c36024634b583c0d3cc8b82ed057a9f"},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(data['title']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title(\"Title\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6de33d77d7279d645b60fa2e4c3bed2dea8e01a"},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(data['content']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Content')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35b23e4315859fbdbd6ec2e08523612186317d18"},"cell_type":"markdown","source":"### Data Cleaning"},{"metadata":{"trusted":true,"_uuid":"395fb73bee3bd86b820bc4f3440bb3a3ca7401af"},"cell_type":"code","source":"%%timeit\ndef cleaning(s):\n    s = str(s)\n    s = s.lower()\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(r'[^\\w]', ' ', s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\",\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s\ndata['content'] = [cleaning(s) for s in data['content']]\ndata['title'] = [cleaning(s) for s in data['title']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80fa795548a94fcba1803a8416e76c511a593544"},"cell_type":"markdown","source":"### Tf-idf and Kmeans"},{"metadata":{"trusted":true,"_uuid":"5a6eddd042afdba20855ebeb1d5badff3f86ebd8"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words='english',use_idf=True)\nmodel = vectorizer.fit_transform(data['content'].str.upper())\nkm = KMeans(n_clusters=5,init='k-means++',max_iter=200,n_init=1)\n\nk=km.fit(model)\nterms = vectorizer.get_feature_names()\norder_centroids = km.cluster_centers_.argsort()[:,::-1]\nfor i in range(5):\n    print(\"cluster of words %d:\" %i)\n    for ind in order_centroids[i,:10]:\n        print(' %s' % terms[ind])\n    print() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eb0f4a7a9298c387c09ddceb3d90380b6046f57"},"cell_type":"markdown","source":"### Building corpus from Title and Contents"},{"metadata":{"trusted":true,"_uuid":"b0a15378d2399944ea0f98ae148ff44ceaeaca75"},"cell_type":"code","source":"def build_corpus(data):\n    \"Creates a list of lists containing words from each sentence\"\n    corpus = []\n    for col in ['title', 'content']:\n        for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(data)        \ncorpus[0:2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ab5af08e560943e36e5d868dc998950a9a06ab8"},"cell_type":"markdown","source":"### Words to Vector"},{"metadata":{"trusted":true,"_uuid":"e95979fa01dff389c6ef405d785c0f651b7633ce"},"cell_type":"code","source":"model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=400, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7de997955de1dbe2d5fcb220d87ca65c7d3d80e2"},"cell_type":"code","source":"model.wv['states']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba4fbcab09b484492591e2aa3518e7ab185684a9"},"cell_type":"markdown","source":"### Data Visualization"},{"metadata":{"trusted":true,"_uuid":"acbcbea86b3b2df75b01378c465ca98c5313b7cf"},"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ed7ca4e0a79c91664eff8be8af1930717884c1a"},"cell_type":"code","source":"tsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5906ab166a8966926e2891ba6a213de660e7f506"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}