{"cells":[{"metadata":{"id":"1C6ahTLgeoIN"},"cell_type":"markdown","source":"# Regression predictive modelling on Boston House Prices (Linear/Lasso/Ridge Regression)\n[Boston House Prices Dataset on Kaggle](https://www.kaggle.com/vikrishnan/boston-house-prices)","execution_count":null},{"metadata":{"id":"Frs_jr3WeoIO"},"cell_type":"markdown","source":"## 1. Import data for analysis","execution_count":null},{"metadata":{"trusted":true,"id":"w9RMFww2eoIP","outputId":"dd8129fd-6122-4c06-bbad-d1d01651269a"},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nos.chdir('/kaggle/input')\nos.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"0NZIaCuZeoIS","outputId":"3b9fb681-e53e-497b-be01-2bf471a9f904"},"cell_type":"code","source":"df=pd.read_csv('boston-house-prices/housing.csv')\ndf.head() #all values are in the first column and header is missing","execution_count":null,"outputs":[]},{"metadata":{"id":"dhS5zmQZeoIV"},"cell_type":"markdown","source":"**X: Predictors** \n* CRIM: per capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to ﬁve Boston employment centers\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10k\n* PTRATIO: pupil-teacher ratio by town 12. \n* B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town 13. \n* LSTAT:%lower status of the population\n\n**Y: Outcome** \n* MEDV: Median value of owner-occupied homes in $1000s","execution_count":null},{"metadata":{"trusted":true,"id":"gmhzum4zeoIW","outputId":"fb85aa72-6b71-48dd-8901-900bbf8a4d76"},"cell_type":"code","source":"names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'] \ndf=pd.read_csv('boston-house-prices/housing.csv',delim_whitespace=True,names=names) \n\n#df.head()\n#df.columns\n#df.shape #506*14\ndf.info()  #no missing value ","execution_count":null,"outputs":[]},{"metadata":{"id":"sYPues-ZeoIY"},"cell_type":"markdown","source":"## 2. Data Cleaning & Wrangling ","execution_count":null},{"metadata":{"id":"bi-NtVlAeoIZ"},"cell_type":"markdown","source":"#### 2.1 Descriptive Analysis\n**View decriptive statistics for all variables**","execution_count":null},{"metadata":{"trusted":true,"id":"cqdYT_3_eoIZ","outputId":"f0683995-768c-4676-ac1f-807f752a0728"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"inaSnmaIeoIc"},"cell_type":"markdown","source":"#### 2.2 Check relationship between predictors and outcome variable\n* **1.scatter plot**\n* According to the plots on the last row, we can observe moderate to strong relationship between each predictor and median house price, suggesting these predictors could explain the house prices to some extent. ","execution_count":null},{"metadata":{"trusted":true,"id":"bzUheA4NeoIc","outputId":"85b62991-a829-48ba-bffa-778dff0ddc92"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nscatterplots=sns.PairGrid(df)\nscatterplots.map_offdiag(plt.scatter) \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"id":"NQMuSPPoeoIf"},"cell_type":"markdown","source":"* **2.correlation matrix / Heatmap**","execution_count":null},{"metadata":{"trusted":true,"id":"5Z0d7AQ7eoIg","outputId":"e4ae895d-3a5b-4b5b-939c-cdf0ba9d3ed8"},"cell_type":"code","source":"#correlation matrix\n#df.corr()\n\nplt.figure(figsize=(25, 12))\nsns.heatmap(df.corr(), vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm', annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"W2_J6BHueoIj"},"cell_type":"markdown","source":"## 3.Build Regression Model from Scikit-learn\n* Train the model: .fit()\n* Predit of new data: .predit()","execution_count":null},{"metadata":{"trusted":true,"id":"9CxwdOMzeoIj"},"cell_type":"code","source":"#split the data into predictors X and Y \n#df.info() #X:0-12; Y:13\nX=df.iloc[:,:12]\ny=df.iloc[:,13]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"9d5AQ73ieoIl"},"cell_type":"code","source":"#Splitting to training and testing data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"6PUhtcV8eoIo"},"cell_type":"markdown","source":"### 3.1 Linear regression\n* Y=aX+b\n* Y=target, X=features\n* a,b=paremeters of model\n* best line of fit: minimize the error function (SSE) --> best a,b","execution_count":null},{"metadata":{"trusted":true,"id":"2lN6tZ-7eoIq"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr_all=LinearRegression()  \nlr_all.fit(X_train, y_train) \n\ny_pred1=lr_all.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"glCKbL1DeoIt","outputId":"d5c41005-a4e3-4474-cf8a-1792ca165b06"},"cell_type":"code","source":"# coefficient of intercept\nlr_all.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"rvws3UFqeoIv","outputId":"34e1bd86-57ff-4077-b440-a8146f3569fe"},"cell_type":"code","source":"#Converting the coefficient values to a dataframe\nlr_all_coeffcients = pd.DataFrame([X_train.columns,lr_all.coef_]).T\nlr_all_coeffcients = lr_all_coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'}) #put into dataframe\nlr_all_coeffcients #print out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model Evaluation**","execution_count":null},{"metadata":{"trusted":true,"id":"UQ5FtAB_eoIy","outputId":"cab9c742-faad-4793-d773-ac3f1413bd75"},"cell_type":"code","source":"#accuracy score \nlr_all.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"bNwDBwBPeoI3"},"cell_type":"markdown","source":"* 𝑅^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\n* Adjusted 𝑅^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\n* MAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y. \n\n* MSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. \n\n* RMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. ","execution_count":null},{"metadata":{"trusted":true,"id":"gi8xlxOxeoI0","outputId":"498fdc4d-f7b7-43ea-816b-83c88b13e90d"},"cell_type":"code","source":"# other evaluation metrics\nfrom sklearn import metrics\nprint('R^2:',metrics.r2_score(y_test, y_pred1))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred1))*(len(y_test)-1)/(len(y_test)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_pred1))\nprint('MSE:',metrics.mean_squared_error(y_test, y_pred1))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))","execution_count":null,"outputs":[]},{"metadata":{"id":"E6EmS13VeoI3"},"cell_type":"markdown","source":"## 4.Overfitting, Regularization \n* Default Performance Metrics: accuracy=correct prediction/ total # of prediction\n* The loss fuction: OLS:minimize sum of squares of residuals\n* :) the smaller the loss function, the better the model","execution_count":null},{"metadata":{"id":"6krXtCE1eoI3"},"cell_type":"markdown","source":"* Regularization: Penalizing large coefficients","execution_count":null},{"metadata":{"id":"HV4ye6IIeoI4"},"cell_type":"markdown","source":"### 4.1 Ridge Regression\n* Ridge regression is one of the simple techniques to reduce model complexity and prevent over-fitting which may result from linear regression\n* The loss function is altered by adding a penalty equivalent to square of the magnitude of the coefficients \n* **One parameter: Alpha (also called 'lambda')**\n* **higher the alpha value --> more restriction on the coeffs**\n* **lower alpha --> more generalization**\n* **Normal pratice: alpha>1** (e.g. 150;230)","execution_count":null},{"metadata":{"trusted":true,"id":"Ga248xeseoI4"},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge=Ridge(alpha=100)\nridge.fit(X_train, y_train)\ny_pred2=ridge.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"pRmBHSq2eoI7","outputId":"f931bec7-f09e-4529-8fd3-89a86f58a335"},"cell_type":"code","source":"ridge.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"famtDfp8eoI9"},"cell_type":"markdown","source":"* ***compare Linear regression vs Ridge(alpha=0.1) vs Ridge(alpha=100)*** ","execution_count":null},{"metadata":{"id":"OU8B2_MGeoI9"},"cell_type":"markdown","source":"* 1. in terms of test score: Ridge regression with high alpha has lowest test score","execution_count":null},{"metadata":{"trusted":true,"id":"oq3AfprdeoI-","outputId":"0e976bf3-c7c3-4ca5-99d9-a9b02f55af1d"},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nrr1=Ridge(alpha=0.01)\nrr1.fit(X_train,y_train)\n\nrr2=Ridge(alpha=100)\nrr2.fit(X_train,y_train)\n\nprint('Linear regression test score:',lr_all.score(X_test,y_test))\nprint('Ridge regression test score with low alpha(0.1):',rr1.score(X_test,y_test))\nprint('Ridge regression test score with high alpha(100):',rr2.score(X_test,y_test)) #high alpha对score的penalty很高","execution_count":null,"outputs":[]},{"metadata":{"id":"i1dYhUYNeoJA"},"cell_type":"markdown","source":"* 2. in terms of magnitude of coefficients: Rigde regression with high alpha penalizes the coefficients on CHAS, NOX, and RM a lot","execution_count":null},{"metadata":{"trusted":true,"id":"bYWgiEGaeoJA","outputId":"5a614f4a-b7d5-440f-8090-8c84358a29da"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(names[0:12],lr_all.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\nplt.plot(names[0:12],rr1.coef_,alpha=0.4,linestyle='none',marker='*',markersize=7,color='red',label=r'Ridge;$\\alpha=0.01$')\nplt.plot(names[0:12],rr2.coef_,alpha=0.4,linestyle='none',marker='d',markersize=7,color='blue',label=r'Ridge;$\\alpha=100$')\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"7cEFB9L7eoJC"},"cell_type":"markdown","source":"### 4.2 Lasso Regression\n* Lasso regression is another simple technique to reduce model complexity and prevent over-fitting which result from lienar regression\n* Lasso regression not only helps in **reducing over-fitting** but it can help us in **feature selection** \n* **Normal practice: alpha<1** (e.g. 0.1, 0.03) ","execution_count":null},{"metadata":{"trusted":true,"id":"Mj70taLNeoJD","outputId":"7636cbd5-2ee8-4472-f410-9f0f44d41efa"},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso=Lasso(alpha=0.8)\nlasso.fit(X_train, y_train)\ny_pred3=lasso.predict(X_test)\n\nlasso.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"7gOmMpiCeoJE"},"cell_type":"markdown","source":"#### :) Feature Selection \n* Removing the predictors with zero coefficients: **CHAS and NOX**","execution_count":null},{"metadata":{"trusted":true,"id":"OwTZMceJeoJF","outputId":"c9a0e9a0-52b9-43d0-c094-e54eedb4e5cd"},"cell_type":"code","source":"#print(lasso.coef_) \n\n#Converting the coefficient values to a dataframe\nlasso_coeffcients = pd.DataFrame([X_train.columns,lasso.coef_]).T\nlasso_coeffcients = lasso_coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'}) #put into dataframe\nlasso_coeffcients #print out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"MRXlgnFseoJH","outputId":"b63f9625-6376-402c-b765-6e6a72f8ecbc"},"cell_type":"code","source":"#Viewing by comparing linear and lasso regression coefficient plots \nimport matplotlib.pyplot as plt\nplt.plot(names[0:12],lasso.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Lasso Regression')\nplt.plot(names[0:12],lr_all.coef_,alpha=0.4,linestyle='none',marker='d',markersize=7,color='blue',label='Linear Regression')\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YyZY9xOAeoJJ"},"cell_type":"markdown","source":"### 4.3 Hyperparameter tunning \n* Ridge and Lasso regression: Choosing alpha\n* Hyperparameters cannot be learned by fitting he model\n* **Solution: GridSearch/RandomizedSearch**","execution_count":null},{"metadata":{"trusted":true,"id":"a9RawcGueoJK","outputId":"2132adaa-c688-49f2-cde3-1ad0b9d727b3"},"cell_type":"code","source":"#find best alpha for Ridge Regression\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'alpha':np.arange(1,10,500)} #range from 1-500 with equal interval of 10 \nridge=Ridge() \nridge_best_alpha=GridSearchCV(ridge, param_grid)\nridge_best_alpha.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"WmuRur3reoJM","outputId":"000e407b-057f-4fe2-e147-cf212cc80cd7"},"cell_type":"code","source":"print(\"Best alpha for Ridge Regression:\",ridge_best_alpha.best_params_)\nprint(\"Best score for Ridge Regression with best alpha:\",ridge_best_alpha.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"MTbmxyzjeoJP","outputId":"e1c5ea49-8812-4a0c-bdeb-10de5c1f4494"},"cell_type":"code","source":"#find best alpha for Lasso Regression\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'alpha':np.arange(0,0.1,1)} #range from 0-1 with equal interval of 0.1 \nlasso=Lasso() \nlasso_best_alpha=GridSearchCV(lasso, param_grid) \nlasso_best_alpha.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"gyVpf-zneoJR","outputId":"ed27e2fc-798b-49bf-bfe4-55d4244dd0b8"},"cell_type":"code","source":"print(\"Best alpha for Lasso Regression:\",lasso_best_alpha.best_params_)\nprint(\"Best score for Lasso Regression with best alpha:\",lasso_best_alpha.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"id":"nB-yhpwseoJT"},"cell_type":"markdown","source":"## 5.Preprocessing Data + Pipeline \n* 1. Handling missing value: dropna; fillna; Imputer\n* 2. Normalizing(Centering and scaling): Features on larger scales can unduly in uence the model\n* 3. pipeline：missing value+normalizaiton+fit model+predict+score","execution_count":null},{"metadata":{"trusted":true,"id":"qSSXMJw_eoJT","outputId":"c76491b2-9da0-49fd-a8bb-498c31d828c6"},"cell_type":"code","source":"#Preprocessin data \n\n#1. handling with missing value (fill up by mean value)\nfrom sklearn.impute import SimpleImputer \nimport numpy as np\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\n\n#2. Normalizing raw data\nfrom sklearn.preprocessing import StandardScaler\n\n#3. Select a prediction model \nfrom sklearn.linear_model import LinearRegression\n\n#4. Set up pipeline \nfrom sklearn.pipeline import Pipeline\nsteps=[('imputation',imputer),('scaler',StandardScaler()),('predict',LinearRegression())]\npipeline=Pipeline(steps) \n\n#5. Fit data into pipeline \nreg=pipeline.fit(X_train, y_train)\ny_pred4=reg.predict(X_test)\nreg.score(X_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}