{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\nLet's say you are planning your trip from or to the United States, definitely you need to book your tickets with the best airlines to avoid anything that disturbs your mood during your vacation.\nAdvertisments can be tricky and misleading, so the best way to build your impression on an airline are people's feedback and review, thankfully we have social media that made accessing people's point of views and feedbacks easier than ever.\n\nIn this notebook, we are going to anaylyze 15k tweets from people about 6 american airlines, in order to have better intuiton and view about the quality of their services and the positive and negative reviews of their clients.\nAlso, we are going to train machine learning and deep learning models to predict and classify if a given tweet is a negative or a positive review or neither."},{"metadata":{},"cell_type":"markdown","source":"**Starting by importing the necessary library**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#starting by importing the necessary libraries\nimport numpy as np #The library that handles all the numbers and matrices\nimport pandas as pd # This library is for reading and manipulating the data, can do a lot of things\nimport matplotlib.pyplot as plt #for data visualization \n\nfrom sklearn.naive_bayes import MultinomialNB #mainly based on bayes theorem; not complicated but has its advantages\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression #The most simple classifier\n#ensemble models are worth trying \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier #although it does not need training but still brilliant\nfrom sklearn.svm import SVC #most popular machine learning classifier \nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score #for accuracy and scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading the data**\n\nStarting by reading the data and exploring it"},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the data from CSV file using pandas\ndata = pd.read_csv('../input/Tweets.csv')\n#have a look on the data\n#data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data visualization**\n\nIn this section, some exploratory analysis and visualization on the data will be done and presented.\nPandas and matplotlib are great libraries for data visualization ans statistics\nWe will start by showing the number of negative, positive and neutral reviews from the data we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = data.airline_sentiment.value_counts()\nindex = [1,2,3]\nplt.figure(1,figsize=(12,6))\nplt.bar(index,counter,color=['green','red','blue'])\nplt.xticks(index,['negative','neutral','positive'],rotation=0)\nplt.xlabel('Sentiment Type')\nplt.ylabel('Sentiment Count')\nplt.title('Count of Type of Sentiment')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like people are not having pleasant flights these days.\nIt is important to know which airline pleases their costumers the most and vice versa, so we sill be looking at the percentage of the negative reviews for each airline."},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_tweets = data.groupby(['airline','airline_sentiment']).count().iloc[:,0]\ntotal_tweets = data.groupby(['airline'])['airline_sentiment'].count()\n\nmy_dict = {'American':neg_tweets[0] / total_tweets[0],'Delta':neg_tweets[3] / total_tweets[1],'Southwest': neg_tweets[6] / total_tweets[2],\n'US Airways': neg_tweets[9] / total_tweets[3],'United': neg_tweets[12] / total_tweets[4],'Virgin': neg_tweets[15] / total_tweets[5]}\nperc = pd.DataFrame.from_dict(my_dict, orient = 'index')\nperc.columns = ['Percent Negative']\nprint(perc)\nax = perc.plot(kind = 'bar', rot=0, colormap = 'Greens_r', figsize = (15,6))\nax.set_xlabel('Airlines')\nax.set_ylabel('Percentage of negative tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next time I am flying either with Virgin or Delta airlines, but definitely not with the US Airways.\nLooking on the brught side, we will show the numbers of each type of review for each line in the next graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"figure_2 = data.groupby(['airline', 'airline_sentiment']).size()\nfigure_2.unstack().plot(kind='bar', stacked=True, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(figure_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Last but not least, people complain for many reasons about their flights; 10 reasons to be specific. In the next figure, we will break down these reason for each airline."},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_reasons = data.groupby('airline')['negativereason'].value_counts(ascending=True)\nnegative_reasons.groupby(['airline','negativereason']).sum().unstack().plot(kind='bar',figsize=(22,12))\nplt.xlabel('Airline Company')\nplt.ylabel('Number of Negative reasons')\nplt.title(\"The number of the count of negative reasons for airlines\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except for Delta airlines, Costumer service is the leading reason for people's complaints on twitter.\nUS airways should really consider doing a fundemental change in their customer service."},{"metadata":{},"cell_type":"markdown","source":"**Data cleaning and Preprocessing**\n\nThis process is mandatory for training any machine learning and deep learning models, and the results differs significantly without cleaning and preprocessing, as computers are not as smart as humans (so far), so we somehow need to spoon-feed them the data, and this stage is mandatory for this purpose."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The preprocessing and cleaning stages consists of removing the stopwords, removing mentions; @VirginArilines, removing any links, punctuation and convert the data to lower case; for the purpose of word representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(input_text):\n    stopwords_list = stopwords.words('english')\n    #some words might give us something important for the sentiment analysis like not, so we keep them\n    wl = [\"not\", \"no\"]\n    words = input_text.split() \n    clean_words = [word for word in words if (word not in stopwords_list or word in wl) and len(word) > 1] \n    return \" \".join(clean_words)\n\ndef remove_mentions(input_text):\n    for i in range(len(input_text)):\n        input_text[i] = re.sub(r'@\\w+', '', input_text[i])\n    return input_text\n\ndef lower_case(input_text):\n    for i in range(len(input_text)):\n        input_text[i] = input_text[i].lower()\n    return input_text\n\ndef remove_http(input_text):\n    for i in range(len(input_text)):\n        input_text[i] = re.sub(r'http\\S+', '',input_text[i])\n    return input_text\n\ndef remove_punctuation(input_text):\n    for i in range(len(input_text)):\n        input_text[i] = re.sub(r'[^\\w\\s]','',input_text[i])\n    return input_text\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After applying the preprocessing on the data we need; the sentiment and the tweet itself, we will be splitting our data to training (90%) and testing (10%) after mapping the sentiment to numbers, because again the computers only understand numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_2 = data[['text', 'airline_sentiment']]\npreprocessed_data = data_2.apply(remove_mentions).apply(remove_http).apply(remove_punctuation).apply(lower_case)\nclean_text = []\nfor tweet in preprocessed_data.text:\n    clean = remove_stopwords(tweet)\n    clean_text.append(clean)\n\nX = clean_text\nY = preprocessed_data['airline_sentiment']\nfrom sklearn.model_selection import train_test_split\nY = Y.map({'negative':0, 'positive':1, 'neutral':2}).astype(int)\nX_train,X_test,y_train,y_test = train_test_split(X, Y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word Representation**\n\nWord representation is actually a very powerful and important stage in natural language processing, and the move from one hot encoders to more efficient and meaningful word representations dramaticaly enhanced the performance of NLP models, actually in a paper published in 2019, researchers from CMU and facebook attempted to do sentence classification without training the encoders and relying only on pretrained embeddings, and it achieved pretty good results, [the paper](https://arxiv.org/abs/1901.10444). \nIn this section, we will explore different word representation techniuqes.\n\nTerm frequency-inverse document frequency (TF-IDF) has emerged to be a powerful word representation technique until today, [this blog explains it brilliantly](https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\ntext_features_train = vectorizer.fit_transform(X_train)\ntext_features_test = vectorizer.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another breakthrough in the field of NLP is [Word2vec](https://arxiv.org/abs/1301.3781), proposed by google. It gives word representation based on the context of the word. On the other hand, it has some limitations like the out of vocab, but since we are using a small dataset, it will not be an issue now.\nThe following code train a word2vec model on our data, and you can also [download pretrained](https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/) word2vec embeddings for many languages."},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\nsentences = [line.split() for line in clean_text]\nw2v = Word2Vec(sentences, size=50, min_count = 0, window = 5,workers=4,iter=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"visualization of what word2vec actually learn, you can notice that words that are similar are actually close in the space."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nX = w2v[w2v.wv.vocab]\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X[0:100])\nplt.rcParams[\"figure.figsize\"] = (20,20)\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nlabels = list(w2v.wv.vocab.keys())\nfor label, x, y in zip(labels, X_tsne[:, 0], X_tsne[:, 1]):\n    plt.annotate(\n        label,\n        xy=(x, y), xytext=(-1, -1),\n        textcoords='offset points', ha='right', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For deep learning techniques, every word should be replaced by a token, and stored in a dictionary that holds for every word its embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nt = Tokenizer()\nt.fit_on_texts(clean_text)\nvocab_size = len(t.word_index) + 1\nencoded_docs = t.texts_to_sequences(clean_text)\npadded_docs = pad_sequences(encoded_docs, maxlen=20, padding='post')\nembedding_dict = dict()\nfor i in w2v.wv.vocab:\n    embedding_dict[i] = w2v[i]\n\nembedding_matrix = np.zeros((vocab_size, 50))\nfor word, i in t.word_index.items():\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After word2vec, companies and universities entered a competetion in the field of word embeddings, facebook proposed fastText that overcomes the issue of out of vocabulary, while Samsung with the University of edingburgh proposed Byte Per Encoding (BPE), and recently, researchers made another breakthrough my proposing contexual word embeddings (this was also a limitation in word2vec that it does not take into consideration that words may have multiple meanings) by proposing Elmo."},{"metadata":{},"cell_type":"markdown","source":"**Machine Learning Techniques**\n\nMachine Learning techniques perform very well as classifiers. In fact, they usually surpass deep learning techniques when the training set is relatively small. In this section we will be exploring some of these techniques.\nWe will be using sklearn to implement these techniques.\nAlso, for machine learning techniques, we will use TF-IDF as word representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(training_features, labels_train, test_features, labels_test):\n    for c in [0.01, 0.05, 0.25, 0.5, 1, 5]:\n    #changing the parameter C to get the optimal classification\n        lr = LogisticRegression(C=c)\n        lr.fit(training_features, labels_train)\n        print (\"Accuracy of logistic regression for C=%s: %s\" \n           % (c, accuracy_score(labels_test, lr.predict(test_features))))\n        results(labels_test, lr.predict(test_features))\n    \n  \ndef svm(training_features, labels_train, test_features, labels_test):\n    for c in [1, 5, 10, 50]:\n    #changing the parameter C to get the optimal classification\n        model = LinearSVC(C=c)\n        model.fit(training_features, labels_train)\n        print (\"Accuracy of svm for C=%s: %s\" \n           % (c, accuracy_score(labels_train, model.predict(training_features))))\n        results(labels_test, model.predict(test_features))\n    \ndef naiive_bayes(training_features, labels_train, test_features, labels_test):\n    clf = MultinomialNB()\n    clf.fit(training_features, labels_train)\n    print (\"Accuracy of Naiive Bayes: %s\" \n         % ( accuracy_score(labels_test, clf.predict(test_features))))\n    results(labels_test, clf.predict(test_features))\n\ndef GB(training_features, labels_train, test_features, labels_test):\n    model = GradientBoostingClassifier()\n    model.fit(training_features,y_train)\n    print (\"Accuracy of GBM: %s\" \n          % (accuracy_score(y_test, model.predict(test_features))))\n    results(y_test, model.predict(test_features))\n        \ndef RF(training_features, labels_train, test_features, labels_test):\n    model = RandomForestClassifier(n_estimators=200)\n    model.fit(training_features,y_train)\n    print (\"Accuracy of Random Forest for: %s\" \n          % (accuracy_score(y_test, model.predict(test_features))))\n    results(y_test, model.predict(test_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to get better insights on the results, we will calculate the confusion matrix, accuracy, precision, recall and F1 score for each classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def results(labels, pred):\n    print(confusion_matrix(labels,pred))  \n    print(classification_report(labels,pred))  \n    print(accuracy_score(labels, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**\nTo start with, Logistic regression is a simple classification techniuqe but yet powerful in sometimes, for multiclass classification, it uses the ONe-VS-rest technique.\nThe C parameter is varied in order to get the best result."},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_regression(text_features_train,y_train, text_features_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machines**\n\nSupport Vector machines is a very popular technique for classification, it simply tries to find the line (vector) that best seperates the classes.\nAlso, the C parameter is varied to get the best results."},{"metadata":{"trusted":true},"cell_type":"code","source":"svm(text_features_train,y_train, text_features_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Naiive Bayes**\n\nNaiive Bayes technique is totally dependant on Bayes theorem of probability for classification. It has its advantages and disadvantages; such as it is simple, fast, and has low computation cost and can work on large datasets, but on the other hand, the assumption of independent features is a drawback as in practice it is almost impossible that model will get a set of predictors which are entirely independent."},{"metadata":{"trusted":true},"cell_type":"code","source":"naiive_bayes(text_features_train,y_train, text_features_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting algorithm**\n\nGradient Boosting is a type of ensemble models, it is fast, accurate and the algorithm is parallelizable. It combines a set of weak learners and delivers improved prediction accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"GB(text_features_train,y_train, text_features_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest**\n\nRandom forest depends mainly on decision trees for prediction, it is accurate and robust and rarely overfits, but it is slow as it has so many decisions to make for prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"RF(text_features_train,y_train, text_features_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Deep Learning techniques**\n\nThis section was made possible by the great efforts of Geoffry Hinton, the man who came up with back porbagation; how neural networks actually learn.\nIn this section, we will explore the feed forward neural network, Long Short Term Memory (LSTM), and convolutional Neural Networks (CNN) for text classification. \nFor Deep Learning, the embeddings extracted by word2vec will be used.\n\nAt first, the labels should be in the form of one hot encoders.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ny_one_hot = tf.keras.utils.to_categorical(\n    Y,\n    num_classes=3,\n    dtype='int32'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_DL, x_test_DL,y_train_DL,y_test_DL = train_test_split(padded_docs,y_one_hot,test_size=0.1,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feed Forward Neural Networks**\n\nFeed Forward neural networks can perform solid classifications given moderate amount of data. \nIn this model, we will use the trained word2vec embeddings as the initialized embeddings for the first layer, followed by two dense layers each containing 128 hidden nodes, in addition to dropout to reduce the effect of overfitting. finally, softmax actiavtion is used to predict the most probable output."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding,LSTM, Flatten, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom sklearn.metrics import accuracy_score\nmodel = Sequential()\nembedding_layer = Embedding(vocab_size,50,embeddings_initializer = Constant(embedding_matrix),input_length=20, trainable=False)\nmodel.add(embedding_layer)\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(x_train_DL,y_train_DL,epochs = 50, batch_size=128)\nans = model.predict(x_test_DL)\nlabels = [np.where(r==1)[0][0] for r in y_test_DL]\nans = np.argmax(ans,axis=1)\naccuracy_score(ans,labels)\nresults(ans,labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Long Short Term Memory (LSTM)**\n\nHumans don’t start their thinking from scratch every second. As you read this sentence, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence. However, feed forward neural networks are not able to do this, which rised the need for recurrent neural network, but they still suffered from the vanishing gradients problem, which in turn led researchers to propose Gated recurrent Unit (GRU) and LSTM, which are better at caputring the meaning of a whole sequence. Until now in this paragraph, LSTM's sound like a great fit for NLP problems, which they really are. In this section we will explore them for this task."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential() \nembedding_layer = Embedding(vocab_size,50,embeddings_initializer = Constant(embedding_matrix),input_length=20, trainable=True)\nmodel.add(embedding_layer)\nmodel.add(LSTM(32, activation = 'relu',return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3,activation = 'softmax'))\nprint(model.summary())\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.fit(x_train_DL,y_train_DL,epochs = 20,batch_size=256)\nans = model.predict(x_test_DL)\nlabels = [np.where(r==1)[0][0] for r in y_test_DL]\nans = np.argmax(ans,axis=1)\naccuracy_score(ans,labels)\nresults(ans,labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conolutional Neural Networks (CNN)**\n\nCNN are well known in the tasks of image processing, but they also perform pretty good in the field of NLP. In fact, in machine translation, facebook proposed CNN, and it is considered one of the state of the art architectures, as they achieved high results on all the bench marks and actually they are leading in some languages. Also, they are well known for their ability of parallel computation. In this section, we will explore the ability of CNN for this task, the used filters are 1D, [this blog](https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f) explains how text classification with CNN works."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Convolution1D, GlobalMaxPooling1D\nfrom keras.layers import Dropout\nmodel = Sequential()\nembedding_layer = Embedding(vocab_size,50,embeddings_initializer = Constant(embedding_matrix),input_length=20, trainable=False)\nmodel.add(embedding_layer)\nmodel.add(Convolution1D(nb_filter=100,\n                        filter_length=5,\n                        border_mode='valid',\n                        activation='relu',\n                        subsample_length=1))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.fit(x_train_DL,y_train_DL,epochs=20)\nans = model.predict(x_test_DL)\nlabels = [np.where(r==1)[0][0] for r in y_test_DL]\nans = np.argmax(ans,axis=1)\naccuracy_score(ans,labels)\nresults(ans,labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Results Summary, Discussion and Error Analysis**\n\nEight machine learning and deep learning techniques were implemented for the task of sentiment analysis for tweets for 6 US airlines. The training data is 13.1k while the testing data 1.4k. \n\nThe machine learning techniques implemented were; Logistic regression, Support Vector Machines (SVM), Naiive Bayes, Gradient Boosting and Random Forest. Tf-IDF was utilized as a word representation for \nThe reported results for best model of each technique are shown in the table below.\n\n| Technique  | Accuracy | Precision | Recall | F1Score|\n| ---------- | -------- |-------- |-------- |-------- |\n| Logistic Regression  | 79.5%  | 0.78 | 0.8 | 0.78|\n| Support Vector Machines  | 79%  | 0.78 | 0.79| 0.78|\n| Naive Bayes  | 69.6%  | 0.75 | 0.7 | 0.62|\n| Gradient Boosting | 72%  | 0.73 | 0.73 |0.67 |\n| Random Forest| 78.3%  | 0.77 |0.78 |0.77 |\n\nFor further analysis, confusion matrix is useful to gain some insights on the results.\n\nLogistic regression\n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 862 | 22 | 40 |\n| positive  |50  | 158 | 30|\n| neutral  | 127 | 31| 144 |\n\nSupport Vector Machines\n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 847 | 24 | 53 |\n| positive  |44  | 161 | 33|\n| neutral  | 115 | 38| 149 |\n\nNaiive Bayes\n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 920 | 0 | 44 |\n| positive  |178 | 55 | 5|\n| neutral  | 253 | 4| 45 |\n\nGradient Boosting\n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 888 | 28 | 8 |\n| positive  | 96  | 138 | 4|\n| neutral  | 240 | 25| 37 |\n\nRandom Forest \n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 864 | 18 | 42 |\n| positive  |72  | 140 | 26|\n| neutral  | 134 | 25| 143 |\n\n\n\nThe implemented deep learning techniques were; feed forward neural network, LSTM model and CNN model. Word2vec embeddings were used for these techniques.\nThe reported results for each technique are shown in the table below.\n\n| Technique  | Accuracy | Precision | Recall | F1Score|\n| ---------- | -------- |-------- |-------- |-------- |\n| Feed Forward | 78.7%  | 0.79 | 0.79 | 0.79|\n| LSTM  | 78%  | 0.78 | 0.78| 0.78|\n| CNN  |74.6%  | 0.75 | 0.74 | 0.74|\n\nConfusion Matrix\n\nFeed Forward Neural Network\n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 813 | 35 | 97 |\n| positive  | 37  | 170 | 37|\n| neutral  | 74 | 33| 168 |\n\nLong Short Term Memory (LSTM)\n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 800 | 23 | 93 |\n| positive  |42  | 179 | 45|\n| neutral  | 82 | 36| 164 |\n\nConvolutional Neural Networks (CNN)\n\n| | negative | positive | neutral|\n| ---------- | -------- |-------- |--------|\n| negative | 777 | 38 | 95 |\n| positive  |60 | 165 | 61|\n| neutral  | 87 | 35| 146 |\n"},{"metadata":{},"cell_type":"markdown","source":"**Error Analysis**\n\n* The accuracy of all the implemented techniques were better at classifying the negative type of reviews, which is due to the fact that the training data has more negative reviews than neutral or positive reviews.\n* The highest error rate is in the neutral class, especially in the ML techniques, where the accuracy of classifying the neutral class was around 50%, deep learning models were better at classifying neutral classes.\n* The classifiers were much better in classifying sentences which consists of obvious words indicating any sentiment, such as; good, bad, awesome, terrible and the list goes on.\n* The performance of the models on small sentences was better overall than the long ones, however, Deep learning techniques were better at classifying long sentences than ML algorithms.\n* Machine learning techniques; especially logistic regression and SVM performed at the same level as deep learning techniques, due to the fact the dataset is considered small, and deep learning techniques have more trainable and learnable parameters and hence more data is required to give better performance.\n* The problem of overfitting appears more in Deep learning techniques than machine learning techniques.\n* Deep learning handles errors in training set better than machine learning.\n* Deep learning models needs more tuning as they have more parameters to vary and tune.\n* Deep learning models were better at handling words that can vary in meaning depending on the context, especially LSTM models (sequential), such as; blow my mind, blow up.\n* Padding sequences negatively effects the learning of DL models.\n\n**Future Work**\n\n* Trying Bert for sentence classification.\n* Replacing word2vec with Elmo/ FastText which can handle context and subwords respectively.\n* Optimize and fine-tune the deep learning techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}