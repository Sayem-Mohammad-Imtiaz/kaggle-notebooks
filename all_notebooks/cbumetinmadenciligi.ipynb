{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # datayi CSV vb okutmak icin kullaniriz\nimport matplotlib.pyplot as plt\n\ndatapath = '../input/student-mat.csv' # dizini veriyolu olarak degiskene atadik \ndataset = pd.read_csv(datapath)\ndf = pd.DataFrame(dataset)\ndf\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b93c11774dbefe09a812c7f6576882c0a9c8e3eb"},"cell_type":"code","source":"# Veri Keşfi ve Görselleştirme\n#satır ve sütun sayısı\nprint(dataset.shape)\n\nprint(\"First 10 lines:\")#ilk 10 satır --> deger belirtidliği ici, yoksa 5 satır\ndf.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d0fbd834dc0ebe37564372be79db936a174fa2e"},"cell_type":"code","source":"\nprint(\"Tail\")#son 5 satır-->deger belirtilmediği icin 5\ndf.tail()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d635562635bb893e6f3b4ceddb53602292ba617"},"cell_type":"code","source":"\nprint(\"describe: \")#basit belirli istatistikler\ndf.describe()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f4658e507da4ba02d28ac83a08929b0ab5be8a2"},"cell_type":"code","source":"print(\"info: \")#bellek kullanımı ve veri türleri\ndf.info()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e53defab18fadeab04cdda74f94b652f4705ead"},"cell_type":"code","source":"#histogramı cizer\ndf.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e8cc1f348199260f3b22a42cd41f5b82f31e7a"},"cell_type":"code","source":"#histogramı cizer, parametrelerini degistirdik, korelasyonu en yüksek iki feature un hisyogramını cizdik\n#hist = df.hist(bins=3)\nax = df.hist(column='G2', bins=25, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n\nax = ax[0]\nfor x in ax:\n\n    # Despine\n    x.spines['right'].set_visible(False)\n    x.spines['top'].set_visible(False)\n    x.spines['left'].set_visible(False)\n\n    # Switch off ticks\n    x.tick_params(axis=\"True\", which=\"True\", bottom=\"False\", top=\"False\", labelbottom=\"True\", left=\"True\", right=\"False\", labelleft=\"True\")\n\n    # Draw horizontal axis lines\n    vals = x.get_yticks()\n    for tick in vals:\n        x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n\n    # Remove title\n    x.set_title(\"Histogram g2-g3\")\n\n    # Set x-axis label\n    x.set_xlabel(\"G3\", labelpad=20, weight='bold', size=12)\n\n    # Set y-axis label\n    x.set_ylabel(\"G2\", labelpad=20, weight='bold', size=12)\n\n    # Format y-axis label\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7277809a6b2562abda8a0682a830d75b335196ee"},"cell_type":"code","source":"\n#eksik veri yok, 395 de 395 hepsi dolu, \"Ön İşleme\" kısmında da göreceğiz\ndataset.corr()#1 e yakın sonuc verenlerin korelasyonu yüksektir. \n#g1 g2 ve g2 g3 arasında, pozitif yönlü güçlü bir korelasyon görülüyor\n#g2-g3 0.904868\n#g1-g2 0.852118\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c93e39e2faef9565901b882a77dd8e5101d6c6d"},"cell_type":"code","source":"import seaborn as sns\n\n# Compute the correlation matrix\ncorr = dataset.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\n#en koyu olanların korelasyonları en yüksektir. g2 ve g3 ile g1 ve g2 burada da kendilerini göstermişler\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b98afdcf78693b51e46c67e832b187a25d199b7","_kg_hide-input":true},"cell_type":"code","source":"import seaborn as sns#baska bir gösterim, bunda g1-g3 ile g2-g3 daha net\ncorr = df.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bec1f9bb90bc01924a92ed113b0230eee7d80968"},"cell_type":"code","source":"df.plot(x='G2', y='G3', style='o') #g2-g3 korelasyon degeri: 0.904868\n#net bir sekilde yüksek oldugu görülüyor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1c616a5b8782c48affd608c40896ae0c44a2029"},"cell_type":"code","source":"df.plot(x='G1', y='G2', style='o')#g1-g2 korelasyon degeri : 0.852118\n#net bir sekilde yüksek oldugu görülüyor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3619ca5bb57d907ad8640208e4302178d62f075b"},"cell_type":"code","source":"# Ön İşleme\ndataset.isnull().sum()#eksik veri olmadığı görülüyor hepsinde 0\ndataset.isnull().sum().sum()#bu da toplam eksikleri veriyor. ikna olmazsak bunu deneyebiliriz.\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b86068d91e148dba5e45592113616824255ff890"},"cell_type":"code","source":"#2.Aykırı Değer Tespiti\nimport seaborn as sns\nsns.boxplot(x=df['G2'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc5d0bac3f6791768e229a2e383cd3c3bcf74144"},"cell_type":"code","source":"P = np.percentile(df.Medu, [10, 100])\nP\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7d458353e3dc7a2d287527a9364e8899743732a"},"cell_type":"code","source":"#1 ve 4 uç değerler, bu iki değer arasındakiler uygun\nnew_df = df[(df.Medu > P[0]) & (df.Medu < P[1])]\nnew_df\n#sns.boxplot(x=df['G3'])\n#P = np.percentile(df.Medu, [10, 100])\n\n#seklinde tüm featurelar icin uc degerleri bulabiliriz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8be928e40533e19327e3335b203dd0122ee3acc"},"cell_type":"code","source":"#Yeni Öznitelik oluşturma--> 2 adet feature birlestirildi ve baska bir feature olarak atandı\ndf[\"NewFeature\"] = df[\"Mjob\"]+ df[\"Fjob\"]\ndf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9da86a989ab327fbb91124cab96931b80ac244"},"cell_type":"code","source":"#Normalleştirme\nfrom sklearn import preprocessing\n\n#age özniteliğini normalleştirmek istiyoruz\nx = df[['age']].values.astype(float)\n\n#Normalleştirme için MinMax normalleştirme yöntemini kullanıyoruz.\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf['age2'] = pd.DataFrame(x_scaled)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5aaba577d8df276ca4fa7437c6b43e15323ac88d"},"cell_type":"code","source":"\nprint(dataset)\n#Model Eğitimleri\n#X = dataset.iloc[:, [2, 3]].values\n#X = df.iloc[:, :-1].values son sürun haric tamamı\nX = df.iloc[:, 30:32]#korelasyonları yüksek olan veriler feature olarak secildi\nY = df['G2'] #ismen sutun alma\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b0636db5cc570cb7e0dfd485f85b0ba7879cbb4"},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37bb816f1555ad8995ac8bc4d30eddb4be0de04c"},"cell_type":"code","source":"Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75c2de591dbf6959bf69946e8d8849a8ebd70db0"},"cell_type":"code","source":"# Veri setini test ve eğitim olarak 2'ye ayırıyoruz. %25 e %75 seklinde\nfrom sklearn import model_selection\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size = 0.25, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25eac1e79388f722c82583c42129d3a11f09e44f"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Random Forest algoritmasını uyguluyoruz \nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, Y_train)\n\n# X_test ile sonucu tahmin etmeye calısıyoruz\nY_pred = classifier.predict(X_test)\n\n#Confusion matrisimizi oluşturuyoruz. Bu esnada Classification Raporunu da import ediyoruz. \n#Burada dogru ve yanlıs sayıları net bir sekilde gorunecek\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ncm = confusion_matrix(Y_test, Y_pred)\n\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Accuracy sonucu\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy:\")\nprint(\"ACC: \",accuracy_score(Y_pred,Y_test))\n\n\nprint(\"Precision, Recall Değerleri:\")\nprint(classification_report(Y_test, Y_pred))\n\n#buradan gozlemlenen sonuclar oldukca kotu. bir de naive bayes i deneyelim.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4473abf770213279b14e626f6292808fb7b7ef65"},"cell_type":"code","source":"#sonuc iyi görünüyor, bir de grafiksel olarak inceleyelim\n# Eğitim sonuçları gözlemliyoruz\nfrom matplotlib.colors import ListedColormap\n\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Sınıflama (Eğitim seti)')\nplt.xlabel('Feature')\nplt.ylabel('G3')\nplt.legend()\nplt.show()\n\n# Test sonuçlarını gözlemliyoruz.\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_test, Y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Sınıflama (Test seti)')\nplt.xlabel('Feature')\nplt.ylabel('G3')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8cd2cda84f818dee8b352bde82fb7cac9f8c82d"},"cell_type":"code","source":"#2. MODEL ile eğitilmesi -Naive Bayes\n\n\n# eğitim setine Naive Bayes uyguluyoruz \nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, Y_train)\n\n# Test veri setini kullanarak sonuçları tahmin ediyoruz\ny_pred = classifier.predict(X_test)\n\n# Confusion matrisimizi oluşturuyoruz.\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, Y_pred)\n\n\nprint(\"Confusion Matrix naive_bayes:\")\nprint(cm)\n\n# Accuracy sonucu\nfrom sklearn.metrics import accuracy_score\nprint(\"naive_bayes-ACC: \",accuracy_score(Y_pred,Y_test))\n\n\nprint(\"Precision, Recall Değerleri: naive_bayes\")\nprint(classification_report(Y_test, Y_pred))\n\n#buradan gozlemlenen sonuclar oldukca kotu. bir de naive bayes i deneyelim.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28c8f3aa5662868fd0753ae19d1e03abb4859777"},"cell_type":"code","source":"\n# Eğitim sonuçları gözlemliyoruz\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_train, Y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Naive Bayes (Training set)')\nplt.xlabel('Feature')\nplt.ylabel('G3')\nplt.legend()\nplt.show()\n\n# Test sonuçlarını gözlemliyoruz.\nfrom matplotlib.colors import ListedColormap\nX_set, Y_set = X_test, Y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(Y_set)):\n    plt.scatter(X_set[Y_set == j, 0], X_set[Y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Naive Bayes (Test set)')\nplt.xlabel('Feature')\nplt.ylabel(' G3')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}