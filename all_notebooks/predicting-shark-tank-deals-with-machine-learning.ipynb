{"cells":[{"source":"# Using the data published in Shark Tank (US) pitches and deals dataset to train Machine Learning algorithms","metadata":{"_uuid":"7c261db74829e6d5c8acf0325288eb19adb08952","_cell_guid":"a4c0e659-167b-495d-bebf-58436ee8a08e"},"cell_type":"markdown"},{"source":"After I published the Shark tank pitches and deals dataset, couple of my colleagues asked me how exactly can this dataset be used for predicting a deal on Shark tank.\n\nI am writing this kernel so that it can be an example for them, as well as others, on how the dataset can be used. It can also be considered as a basic NLP and text vectorization starter code for new machine learning students.\n\nMany aspects of this code are based on my personal preference, and they are changeable:  for example, I have used count vectorization from sklearn and selected ngrams. Alternatively, one may also use tfidf vectorization. ","metadata":{"_uuid":"8debd24c62c26a9329ffd76d8f3d03e40aa872e3","_cell_guid":"e07fa5e9-c9c6-48f5-aa13-61ecc273f25f"},"cell_type":"markdown"},{"source":"## Step 1 - Importing all libraries and data ##","metadata":{"_uuid":"8c78e470e4c15ce33bf11c61037c63e5591f59ea","_cell_guid":"d122f854-53c1-4299-93e5-0b9a57216c74"},"cell_type":"markdown"},{"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom ggplot import *\nimport nltk\ndf = pd.read_csv('../input/Sharktankpitchesdeals.csv')\n## Check whether dataset is loaded\ndf.head()","execution_count":null,"cell_type":"code","metadata":{"_uuid":"9aef6e784e0b23050699fc6c02670731d1c7b07c","_cell_guid":"4241fd9a-d704-422a-8d4d-d558987d6a16"},"outputs":[]},{"source":"## Step  2 - Cleaning Data","metadata":{"_uuid":"96e3c57435899c4c86ae60063d4c16e66877d000","_cell_guid":"4c3e6b23-7bd4-47aa-9ad7-138638137546"},"cell_type":"markdown"},{"source":"def data_cleaning(corpus):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", corpus) \n    words = letters_only.lower().split()                            \n    return( \" \".join( words ))     ","execution_count":null,"cell_type":"code","metadata":{"_uuid":"2d9b46d576156e3fc382e024932156664970974b","collapsed":true,"_cell_guid":"065bce73-fda1-4167-ad67-341b89acab55"},"outputs":[]},{"source":"### Tips - ready code examples if you want to make changes such as removing stopwords or other specific words from the dataset \n\nHere one may want to remove stopwords\n> from nltk.corpus import stopwords\n\nYou may also want to remove some additional words, which you have observed in the dataset, which may not contribute to the learning  \n\n> addedwords = ('service','use','product','line','allow','make','offer','make','provide','products','design','made')\n\n> stop = stopwords.words('english')+list(addedwords)\n\n> df['Pitched_Business_Desc'] = df['Pitched_Business_Desc'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","metadata":{"_uuid":"2ccb6355a3e5a0efa0f22ad5ccc7803d657b5e26","_cell_guid":"e72b0e0b-3fcb-42c1-9a46-4c32ac41e882"},"cell_type":"markdown"},{"source":"df['Pitched_Business_Desc'] = df['Pitched_Business_Desc'].apply(lambda x:data_cleaning(x))\ndf = df[['Deal_Status','Pitched_Business_Desc']]\nfor i in range(5):\n    print(df['Pitched_Business_Desc'][i])","execution_count":null,"cell_type":"code","metadata":{"_uuid":"1a4a6ddf682b4a5bb719702fa9c8784c9b8bde6d","_cell_guid":"e8e52a46-455d-49cf-a771-7edc74c55a33"},"outputs":[]},{"source":"## Step 3 - Vectorize the data & split in training and testing datasets","metadata":{"_uuid":"682552d5d7f3cd40398ba9ed8b2da278f1216691","_cell_guid":"49a2d3da-29e0-4c43-9258-56f61e503ab2"},"cell_type":"markdown"},{"source":"## Split into train/test sets\nfrom sklearn.cross_validation import train_test_split\ntrain, test = train_test_split(df,test_size=0.2)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"a72bc84ea394b7742429eca91b744d5a466a8bd4","_cell_guid":"82874e71-c234-454a-a2aa-22eddbcbc694"},"outputs":[]},{"source":"## Vectorize\ntrain_corpus = []\ntest_corpus = []\nfor each in train['Pitched_Business_Desc']:\n    train_corpus.append(each)\nfor each in test['Pitched_Business_Desc']:\n    test_corpus.append(each)\n## Start creating them\nfrom sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(ngram_range=(2,2))\ntrain_features = v.fit_transform(train_corpus)\ntest_features=v.transform(test_corpus)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"0dfa2e4dc2ee77ccd7649ee1dae392c8c044e0ba","collapsed":true,"_cell_guid":"a18dcca1-a3f9-4443-acc2-d3ae3b6111c6"},"outputs":[]},{"source":"print(train_features.shape)\nprint(test_features.shape)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"c4e7178186d93d3fb80bf5753ae889e24084fa7f","_cell_guid":"41fa40c2-520d-4aaf-922c-2a580a9dcda1"},"outputs":[]},{"source":"## Step 4 - Initiate machine learning algorithms","metadata":{"_uuid":"2599f9c3046379394d27f509bfdd2f8f9679f11f","_cell_guid":"bc7ba233-61c9-4d70-8dbf-f6da785a1cad"},"cell_type":"markdown"},{"source":"# Import ML models from sklearn\nfrom sklearn.linear_model import LogisticRegression # Regression classifier\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree classifier\nfrom sklearn import svm # Support Vector Machine\nfrom sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent Classifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Random Forest and Gradient Boosting Classifier\nfrom sklearn.naive_bayes import MultinomialNB # Naive Bayes Classifier \nfrom sklearn.metrics import accuracy_score, recall_score, confusion_matrix # Some metrics to check the performance of the models","execution_count":null,"cell_type":"code","metadata":{"_uuid":"97fff185a944a29ffa556a0dcfb59e18679a651b","collapsed":true,"_cell_guid":"1d540980-bb7c-4418-8d1a-1b42e27efae2"},"outputs":[]},{"source":"# Setting parameters for each algorithm - these are tunable to achieve max accuracy\n\nClassifiers = {'LR':LogisticRegression(random_state=10,C=5,max_iter=200),\n               'DTC':DecisionTreeClassifier(random_state=10,min_samples_leaf=2),\n               'RF':RandomForestClassifier(random_state=10,n_estimators=100,n_jobs=-1),\n               'GBC':GradientBoostingClassifier(random_state=10,n_estimators=400,learning_rate=0.2),\n               'SGD':SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n               'SVM':svm.SVC(kernel='linear', C=0.1),\n               'NB':MultinomialNB(alpha=.05)}\n","execution_count":null,"cell_type":"code","metadata":{"_uuid":"30afa1a5442827460e667cf6e8aad70846aa4684","collapsed":true,"_cell_guid":"3ce370cd-1032-4828-98e8-68fef92552d8"},"outputs":[]},{"source":"# Create a pipeline so you can reuse the code\ndef ML_Pipeline(clf_name):\n    clf = Classifiers[clf_name]\n    fit = clf.fit(train_features,train['Deal_Status'])\n    pred = clf.predict(test_features)\n    Accuracy = accuracy_score(test['Deal_Status'],pred)\n    Confusion_matrix = confusion_matrix(test['Deal_Status'],pred)\n    print('==='*20)\n    print('Accuracy = '+str(Accuracy))\n    print('==='*20) \n    print(Confusion_matrix)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"10a5f453a3449a6fd9a4517f8131b63c60dbe601","collapsed":true,"_cell_guid":"cc65efc0-d878-4d18-8a41-5e14e64d3b8a"},"outputs":[]},{"source":"## Step 5 - Run the machine learning algorithms in individual blocks","metadata":{"_uuid":"254c146bf77979f06554d3823426214cbe49fbf1","_cell_guid":"d0e4d51d-2c04-44c1-944d-b19d415bc885"},"cell_type":"markdown"},{"source":"ML_Pipeline('LR')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"7516ad344befdfdc08ff0c29a7866b887bd2c6d2","_cell_guid":"7199fec9-4092-4c36-afe5-03aaccbcc30a"},"outputs":[]},{"source":"ML_Pipeline('DTC')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"9fe4f9aaa780e5f55d645e7d69e1a02e929dcf4b","_cell_guid":"72698d48-9c21-40e8-815e-3a31af7c1281"},"outputs":[]},{"source":"ML_Pipeline('RF')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"5c1f72e31be1ec949032f017ecddf661160f0809","_cell_guid":"54115e47-3a3f-486b-b20c-d7851beacb2f"},"outputs":[]},{"source":"ML_Pipeline('GBC')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"f2e857c0496212714c55643c0fafbd417e7f1cb0","_cell_guid":"ae41183d-5dd9-4b0c-824b-e76661666a37"},"outputs":[]},{"source":"ML_Pipeline('NB')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"fdfbdb5ac778a86851b6053d36b6b8b62d0a6b24","_cell_guid":"99ae8834-14d3-4d4f-8b9d-2cbf07daab36"},"outputs":[]},{"source":"ML_Pipeline('SVM')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"93388148c27144d89ced4c0da6ead9223d3c7466","_cell_guid":"9fee112b-7e7a-4749-ab1a-22f93f32074f"},"outputs":[]},{"source":"ML_Pipeline('SGD')","execution_count":null,"cell_type":"code","metadata":{"_uuid":"c065a3559b02e830081e740bc529b8af21a3dfb8","_cell_guid":"019b4000-eb63-4b15-83ed-fffc50f9ac16"},"outputs":[]},{"source":"# Concluding remarks\n\nAs of the current setup, some combinations of ngrams and parameters help us reach ~60% accuracy.\n\nThis, I believe, can be further optimized to reach higher accuracy numbers.\n\nI would invite others to work with the data and try to achieve a higher accuracy. \n\nThis kernel uses only the 'Deal_Status' column; Perhaps, someone can also try their hand on the 'Deal_Shark' column to find a model that can give a higher accuracy for a certain Shark/Sharkette or their combination.  ","metadata":{"_uuid":"9f546c8d464b8cefd3379215b3889566300c3d36","_cell_guid":"ee47780b-3077-4e42-a3b2-b1ef1ca8d95d"},"cell_type":"markdown"},{"source":"## Step 6 - Parameter Optimization & Tuning Using Grid Search","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\nftwo_scorer = make_scorer(fbeta_score, beta=2)\nftwo_scorer\nmake_scorer(fbeta_score, beta=2)\nparameters = {'kernel':('linear', 'rbf'), 'C':[0.01, 0.1, 1, 10, 100]}\nsvc = svm.SVC()\nclf = GridSearchCV(svc, parameters, scoring=ftwo_scorer)\nclf.fit(train_features,train['Deal_Status'])\nprint(clf.best_params_)","execution_count":null,"cell_type":"code","metadata":{},"outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.1"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":1}