{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\nfrom scipy import sparse\nfrom tqdm import tqdm\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, GRU\nfrom keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Im Folgenden werden die bereits zu csv konvertierten DatensÃ¤tze geladen:","metadata":{}},{"cell_type":"code","source":"business = pd.read_csv('../input/yelp-csv/yelp_academic_dataset_business.csv')\nreview_all = pd.read_csv('../input/yelp-csv/yelp_academic_dataset_review.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-------------------------------------------------------------- DATA PREPARATION --------------------------------------------------------------------**","metadata":{}},{"cell_type":"code","source":"a = business[business['categories'].str.contains('Restaurant') == True]\nrev = review_all[review_all.business_id.isin(a['business_id']) == True]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rev_samp = rev.sample(n = 350000, random_state = 42)\ntrain = rev_samp[0:280000]\ntest = rev_samp[280000:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[['text', 'stars']]\ntrain['stars'].hist();train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.get_dummies(train, columns = ['stars'])\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test[['text', 'stars']]\ntest = pd.get_dummies(test, columns = ['stars'])\ntrain.shape, test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_samp = train.sample(frac = .1, random_state = 42)\ntest_samp = test.sample(frac = .1, random_state = 42)\ntrain_samp.shape, test_samp.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-------------------------------------------------------------- NAIVE BAYES MODEL --------------------------------------------------------------------**","metadata":{}},{"cell_type":"code","source":"max_features = 2000\ntfidf = TfidfVectorizer(max_features = max_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NBFeatures(BaseEstimator):\n    '''Class implementation of Jeremy Howards NB Linear model'''\n    def __init__(self, alpha):\n        # Smoothing Parameter: always going to be one for my use\n        self.alpha = alpha\n        \n    def preprocess_x(self, x, r):\n        return x.multiply(r)\n    \n    # calculate probabilities\n    def pr(self, x, y_i, y):\n        p = x[y == y_i].sum(0)\n        return (p + self.alpha)/((y==y_i).sum()+self.alpha)\n    \n    # calculate the log ratio and represent as sparse matrix\n    # ie fit the nb model\n    def fit(self, x, y = None):\n        self._r = sparse.csr_matrix(np.log(self.pr(x, 1, y) /self.pr(x, 0, y)))\n        return self\n    \n    # apply the nb fit to original features x\n    def transform(self, x):\n        x_nb = self.preprocess_x(x, self._r)\n        return x_nb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create pipeline using sklearn pipeline:\n    # I basically create my tfidf features which are fed to my NB model \n    # for probability calculations. Then those are fed as input to my \n    # logistic regression model.\nlr = LogisticRegression()\nnb = NBFeatures(1)\np = Pipeline([\n    ('tfidf', tfidf),\n    ('nb', nb),\n    ('lr', lr)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['stars_1', 'stars_2', 'stars_3', 'stars_4', 'stars_5']\nscores = []\npreds = np.zeros((len(test_samp), len(class_names)))\nfor i, class_name in enumerate(class_names):\n    train_target = train_samp[class_name]    \n    cv_score = np.mean(cross_val_score(estimator = p, X = train_samp['text'].values, \n                                      y = train_target, cv = 3, scoring = 'accuracy'))\n    scores.append(cv_score)\n    print('CV score for class {} is {}'.format(class_name, cv_score))\n    p.fit(train_samp['text'].values, train_target)\n    preds[:,i] = p.predict_proba(test_samp['text'].values)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'][744909]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = metrics.classification_report(np.argmax(test_samp[class_names].values, axis = 1),np.argmax(preds, axis = 1))\nprint(t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-------------------------------------------------------------- NEURAL NETWORK MODEL --------------------------------------------------------------------**\n","metadata":{}},{"cell_type":"markdown","source":"**EINBETTUNG UND TOKEN**","metadata":{}},{"cell_type":"code","source":"# I'm using GLoVe word vectors to get pretrained word embeddings\nembed_size = 200 \n# max number of unique words \nmax_features = 20000\n# max number of words from review to use\nmaxlen = 200\n\n# File path\nembedding_file = '../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt'\n\n# read in embeddings\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['stars_1', 'stars_2', 'stars_3', 'stars_4', 'stars_5']\n# Splitting off my y variable\ny = train_samp[class_names].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_samp['text'].values))\nX_train = tokenizer.texts_to_sequences(train_samp['text'].values)\nX_test = tokenizer.texts_to_sequences(test_samp['text'].values)\nx_train = pad_sequences(X_train, maxlen = maxlen)\nx_test = pad_sequences(X_test, maxlen = maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\n\nnb_words = min(max_features, len(word_index))\n# create a zeros matrix of the correct dimensions \nembedding_matrix = np.zeros((nb_words, embed_size))\nmissed = []\nfor word, i in word_index.items():\n    if i >= max_features: break\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        missed.append(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(missed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missed[0:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missed[1000:1010]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modellierung: SPEZIFIZIERUNG**","metadata":{}},{"cell_type":"code","source":"inp = Input(shape = (maxlen,))\nx = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = True)(inp)\nx = SpatialDropout1D(0.5)(x)\nx = Bidirectional(LSTM(40, return_sequences=True))(x)\nx = Bidirectional(GRU(40, return_sequences=True))(x)\navg_pool = GlobalAveragePooling1D()(x)\nmax_pool = GlobalMaxPooling1D()(x)\nconc = concatenate([avg_pool, max_pool])\noutp = Dense(5, activation = 'sigmoid')(conc)\n\nmodel = Model(inputs = inp, outputs = outp)\n# patience is how many epochs to wait to see if val_loss will improve again.\nearlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3)\ncheckpoint = ModelCheckpoint(monitor = 'val_loss', save_best_only = True, filepath = 'yelp_lstm_gru_weights.hdf5')\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modellierung: ANPASSUNG**","metadata":{}},{"cell_type":"code","source":"model.fit(x_train, y, batch_size = 512, epochs = 20, validation_split = .1,\n          callbacks=[earlystop, checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modellierung: BEWERTUNG**","metadata":{}},{"cell_type":"code","source":"y_test = model.predict([x_test], batch_size=1024, verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test, test_samp[class_names].values, verbose = 1, batch_size=1024)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = metrics.classification_report(np.argmax(test_samp[class_names].values, axis = 1),np.argmax(y_test, axis = 1))\nprint(v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modellierung: SPEICHERN**","metadata":{}},{"cell_type":"code","source":"model.save('yelp_nn_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modellierung: VERBESSERUNG => nur TEXT kein CODE**","metadata":{}},{"cell_type":"markdown","source":"**-------------------------------------------------------------- Nutzen der Modelle --------------------------------------------------------------------**","metadata":{}},{"cell_type":"markdown","source":"**nur TEXT kein CODE**","metadata":{}}]}