{"cells":[{"metadata":{"_uuid":"f481165ede559946cfbc3d0225f391b4edc3e1fc"},"cell_type":"markdown","source":"![](https://www.cenozon.com/wp-content/uploads/2018/07/Computer_visioning.jpg)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Goal of this kernel is to introduce the reader to NN aspects of computer vision\n\n\n## I wanted to teach myself more about the subject hence this kernel & paper\n\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# 1.Theory(math) behind image processing and cNN can be found in my subject sumarization [Theory of image processing](https://drive.google.com/file/d/1AJHe5nV1qAI7zE6TGYhzxPrsKedTEcRr/view?usp=sharing)\n\n### I heavily relied on [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)"},{"metadata":{"_uuid":"2def9c413bbe87acf230a31a1c610b1234b181ff"},"cell_type":"markdown","source":"Now that we got the basic idead what happens behind the scenes we can tackle some problems. First and most basic one will be the MINST data set. in the nutshell we have gray-scale images of 10 possible digits (0-9) and our job is to correctly classify them. \n[Data-set description](https://www.kaggle.com/c/digit-recognizer/data). Now this data-set is some what idealistic in a sense that we already have quantified images, i.e. images are already represented in a pixaleted form via matrix (as explained in the link). So only thing that we need to think about is some pre-processing and building the model. Other more realistic data-set that we are going to analyse is the [Humpback-whale](https://www.kaggle.com/c/humpback-whale-identification) data set. Goal is pretty straight-foreward: Given an image of a tail of a whale, predict its unique id. Now this is a bit specific competition since we have only a few images for every id. But more on that in a different kernel (to avoid over-stuffing this kernel)"},{"metadata":{"_uuid":"098069f168a8ca9fd299d6640c551a05c1f1a547"},"cell_type":"markdown","source":"## a) Digit recognizer"},{"metadata":{"_uuid":"0be91bc8e53c517a98cbdcf44c395a9c40f1fa6f"},"cell_type":"markdown","source":"**Necessary modules**"},{"metadata":{"trusted":true,"_uuid":"c86e51ff782ab22d8c702defe4520b8e9d3a3061"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical #one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6aa69aa8724b6046e140491cc431859dcccc0177"},"cell_type":"markdown","source":"Now let us load the data and extract the independent variable Y (standard with unsupervised learning...)"},{"metadata":{"trusted":true,"_uuid":"08e8dd0cbac103b13f1bf8e980f9c69a5d6bd338"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nY_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\nsns.countplot(Y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ac2f4f5e8384b1f167bf356ed308820d37fda95"},"cell_type":"markdown","source":"As we can see the distribution of the classes is pretty much balanced. That is good since we wont need to use stratified partitioning (i.e. making sure that every subset represents the whole set) when subsetting the train and test set."},{"metadata":{"trusted":true,"_uuid":"7fff7f1f037f4ee420dd4a63684d1d522424e871"},"cell_type":"code","source":"X_train.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb4c3c2682cde2b252af377f4cf21ca648142577"},"cell_type":"markdown","source":"No NAN values (corrupted images) this is just a standard Data Science pipeline of pre-processing..."},{"metadata":{"trusted":true,"_uuid":"b1302e123f0bc67b5b2a2bb646747961a1937d4d"},"cell_type":"code","source":"test.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3fbfb41c6d9c776d3882bca34c9736b4dbcd775"},"cell_type":"markdown","source":"We should also normalize the data, right now we have values in the range of 0 to 255. Reason being is that we only have black-white images. When we translate it to numerical values this coresponds to one gray-level where pixel values can take values between 0 and 255 (depending on the position). We can normalize these values to speed up the algorithm."},{"metadata":{"trusted":true,"_uuid":"19f69f55f3a2cd9c879f8cf80a9aa3d3f44e468a"},"cell_type":"code","source":"X_train = X_train / 255.0\ntest = test / 255.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2872836f9008d133388441c6edaace9caa008bb4"},"cell_type":"markdown","source":"Now if read the teory we can notice that one thing thats beneficial to cNN (among others regarding image processing) ist that input values (images) are forwarded in the exactly the same fashion as they are pixelated. What I mean by that is that images are usually represented as 3 dimensional objects (array containing numerical values). Now in this data set one row is one picture, but is actually a 784 (28*28*1) dimensional vector. We need to transform it"},{"metadata":{"trusted":true,"_uuid":"96710281afa1494b365fe429cdc1bbb975aed74c"},"cell_type":"code","source":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9213d9e24efce5a74a744e8e8fc6b6805ac3aff8"},"cell_type":"markdown","source":"NExt thing we have to think about (again standard toguht process in ML pipeline) is categorical variables. We ought to label on one-hot encode these variables. WE already have target variable as label encoded one, but we do not want that (in short because 9 is not more \"valuable\" than 1 and with label encodign we are going to achieve that) hence we are going to one-hot encode target-y variable."},{"metadata":{"trusted":true,"_uuid":"d5e705ce03958d530897201e0a481939d6b80e28"},"cell_type":"code","source":"Y_train = to_categorical(Y_train, num_classes = 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e40b0c3a3ef2c2e215a53a6e63745af569b10bdc"},"cell_type":"markdown","source":"Now splitting our into our training and testing set (validation is already given and with bad notaiton also called test). Note that because of even distribution of classes we do not need to use stratified partitioning and we can use simple train_test_split function from skicit learn with random_state parameter to ensure reproducability."},{"metadata":{"trusted":true,"_uuid":"60e26f1b98d85bdda768eb3b932878184f24bdd2"},"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b07f8c4b01b8b1e243f305fd394a90d035fb4b9"},"cell_type":"markdown","source":"But can we acutally see a picture that is encoded numerically?  Sure, plotly library that we already imported helps us with that"},{"metadata":{"trusted":true,"_uuid":"078efd35464c4cad0088a7ba6b170408d2c63af4"},"cell_type":"code","source":"plt.imshow(X_train[5][:,:,0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"849d239cf737c3fe2182544e5aaa0ac541f35363"},"cell_type":"markdown","source":"Cool, now we can start thinking about building a model (cNN!) now we already discussed theoretically what woud it look like.  Pooling and convolution layers are novice things, but also what would it look like as a whole (assuming one has no experiance with NN in python-keras)?"},{"metadata":{"trusted":true,"_uuid":"7692da5e3d5b2ba7b860e950ebe65cce78f91560"},"cell_type":"code","source":"model = Sequential()\n\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7beffd7a9828dd156fbd1f901a927772d766ff8b"},"cell_type":"markdown","source":"WE already imported\n\n\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n\nnecessary modules from keras, they should already be familiar from the paper. What we basically built is a pipeline cNN. What each and every part does is already discussed in the paper as well as the input parameters into each individual part. For example \n\nConv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1))\n                 \n   \n   \n  filters=32 specifies number of filters, kernel_size size of the convolution matrix, padding= do we add 0 in order to control convolution, activation=\"Relu\" specifies with what function do we activate the output (remember it is still a NN), and inpute_shape tells us the what shape should we expect as input (remember we transformed it!)\n  \n  Helpful is also [keras documunetation cNN](https://keras.io/layers/convolutional/)\n  \n  \n  # Important\n  last layer is softmax function (with 10 output values). Why? Well we feed our cNN wit hone picture of a number than the last layer (with softmax) will compress the values to values between 0 and 1. Which coresponds to probability. Than in the end we are going to say that this number is the number with the highest probability (just like with binary classification-we get probabilities that for which we set a tresh hold level that determines wether it is 0 or 1)"},{"metadata":{"_uuid":"74a8db4d9837698afceed73b3600b355f42c4c24"},"cell_type":"markdown","source":"**But why do we choose exactly THIS cNN architecture?** IT looks rather arbitrary. Well that is an issue and there are not a lot of quantitative methods around that tackel these issues (Bayesian optimization,Grid search, random search ...) But there are good starting points with following google paper\n[Recommended cNN architecture](https://arxiv.org/abs/1512.00567)"},{"metadata":{"_uuid":"602b99a3cd2079876fe33b08126f46fe0954364d"},"cell_type":"markdown","source":"**Optimizer** First I started with adam, but I did not achieve desired speed (sgb would be even slower) hence I opted for RMSProp. The RMSProp update adjusts the Adagrad method in a very simple way in an attempt to reduce its aggressive, monotonically decreasing learning rate."},{"metadata":{"trusted":true,"_uuid":"ee1405afb1011523d6aeae2ab9f64be3b7ae92b7"},"cell_type":"code","source":"rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15c0bdf3473d189571e5593ce84f748fb62b6e52"},"cell_type":"markdown","source":"Let us proceede, what else do we need? Backpropagation is still the main algo of NN and we need loss function. Since we perform multiclassification we will need categorical (instead binary) crossentropy. Optimizer (a standard choice) is adam, and standard accuracy measure."},{"metadata":{"_uuid":"91f9027709c3b32d8137ad24218c778b92eeaa3e","trusted":true},"cell_type":"code","source":"model.compile(optimizer = rmsprop , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ba0919c58c7ebfb31a71ae50fd4f15f5f9f990a"},"cell_type":"markdown","source":"Now we want to accelrate our algo even further, hence we ough to modify the lr (learning rate) Idea is quite simple. In the begininng we should learn rather quickly but as we more towards the optimum (of the loss function) we should decrease the learning rate. (to avoid oscilation)\nTo keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).(Waiting 2 epochs to see if it changes)\n\nWith the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy is not improved after 2 epochs, setting the minimal possible lr to 0.0001\n"},{"metadata":{"trusted":true,"_uuid":"5904d459d4f0002a2261241ed6fcec15a14e7170"},"cell_type":"code","source":"lr_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28ed6550f288451691357e6444d8b40e57df3b03"},"cell_type":"markdown","source":"**Data augmentation**\n\nWhat else do we need, well if we think about it and have a look at the couple of images we can see that they are a too much idealistic. In reality our model could be tested on a set that has rotated, re-shaped, increased etc... nubmers (images in general) hence we ought make sure that also this modifications of images are to be found in our data-set. It benefits us in couple of ways. When we are done we are going to increase our training sample, we wont over-fit and we will be able to generalize."},{"metadata":{"_uuid":"b982e6a0e113d2a5b5d74ebea9bd8b79d2ad8844","trusted":true},"cell_type":"code","source":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d18308b850b1a0b7a34bf0c6285bd41286d411db"},"cell_type":"markdown","source":"Let us apply this to our model now"},{"metadata":{"trusted":true,"_uuid":"49cef85d7fa544f913b72de548c4fcae85f0f639"},"cell_type":"code","source":"epochs = 1\nbatch_size = 86","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f66d51b387f70bb6b035cb37d7a72bf56a82952"},"cell_type":"code","source":"\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n                              , callbacks=[lr_reduction])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d94e4f4c03828adc858f0c4dc31232813df6a061"},"cell_type":"markdown","source":"Still with only one foreward and backward pass (1 epoch) we get relatively big run time. To achieve better results (with even bigger run time) one should increase the number of epochs (not to much we still want the model to generalize)"},{"metadata":{"_uuid":"0d0d9055362d29133a2aed97b0909fa66de453c1"},"cell_type":"markdown","source":"**Confusion Matrix**\n\nAs with every classification task confusion matrix is very good way to see where did the model make mistakes. Overall we can see that results are good but in some cases model mistakes certain numbers for others"},{"metadata":{"trusted":true,"_uuid":"30672b4af9b74cabe1b136b4adc29562ceb6fe73"},"cell_type":"code","source":"# Look at confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fa3ac63c51911ff6552f19f87bdd6e24691327c"},"cell_type":"markdown","source":"Now we see that algo often mistakens 9 and 4. We want to further analyze it. But how? Note that predictions are not classes right away, with argmax function we decide which class do we want to predict (based on highes probability). So we can argue that those that misssed the most had the highest probabilty for the actual class. Those cases we want to observe..."},{"metadata":{"trusted":true,"_uuid":"cbd1a537921f2e887f9e49b2881365c311fd8f61"},"cell_type":"code","source":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08699610d3f57a5b09180f5ff3690be219246161"},"cell_type":"markdown","source":"I do not know about you but some of the would fool even a human...."},{"metadata":{"trusted":true,"_uuid":"d2d73c7dde94ad89ef9c67aeabe211bd5a2233bf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c515653707a3a63f449c34f408939b6c1902930b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63fc39ff1a6bb34f89d41c80cec925e8bbb67d67"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7b3029e11f56291c803464b5290f9a1f111d0d4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3ed295bdea5e49031accea2af709c2169b5d300"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09e05ecea376649d968e6c3853e43118ca4837d7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3977610c4dced0fd02d0e8eb920a7ef52129f4c3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}