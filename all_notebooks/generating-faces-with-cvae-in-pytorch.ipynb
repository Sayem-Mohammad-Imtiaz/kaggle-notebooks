{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Generating Faces with CVAE in PyTorch [TRAIN]"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom torchvision import datasets, transforms, models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom torchvision.utils import save_image\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nimport math\nfrom PIL import Image\nfrom IPython.display import display\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformObj = transforms.Compose([\n    transforms.Resize(64),\n    transforms.CenterCrop(64),\n    transforms.ToTensor(),\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataroot = \"../input/celeba-dataset/img_align_celeba/\"\n\ndataset = datasets.ImageFolder(root=dataroot, transform=transformObj)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self, latent_size=100):\n        super(VAE, self).__init__()\n        \n        self.latent_size = latent_size\n        \n        self.l1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=2, padding=1)\n        self.l1b = nn.BatchNorm2d(32)\n        self.l2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)\n        self.l2b = nn.BatchNorm2d(64)\n        self.l3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n        self.l3b = nn.BatchNorm2d(128)\n        self.l4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1)\n        self.l4b = nn.BatchNorm2d(256)\n        \n        self.l41 = nn.Linear(256*4*4, self.latent_size)\n        self.l42 = nn.Linear(256*4*4, self.latent_size)\n        \n        self.f = nn.Linear(self.latent_size, 256*4*4)\n        \n        self.l5 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1)\n        self.l6 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1)\n        self.l7 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n        self.l8 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=4, stride=2, padding=1)\n        \n    def encoder(self, x_in):\n        h = F.leaky_relu(self.l1b(self.l1(x_in)))\n        h = F.leaky_relu(self.l2b(self.l2(h)))\n        h = F.leaky_relu(self.l3b(self.l3(h)))\n        h = F.leaky_relu(self.l4b(self.l4(h)))\n        \n        h = h.view(h.size(0), -1)\n        \n        return self.l41(h), self.l42(h)\n    \n    def decoder(self, z):\n        z = self.f(z)\n        z = z.view(-1, 256, 4, 4)\n        \n        z = F.leaky_relu(self.l5(z))\n        z = F.leaky_relu(self.l6(z))\n        z = F.leaky_relu(self.l7(z))\n        z = torch.sigmoid(self.l8(z))\n        \n        return z\n    \n    def sampling(self, mu, log_var):\n        std = torch.exp(0.5*log_var)\n        eps = torch.randn_like(std)\n        return torch.add(eps.mul(std), mu)\n    \n    def forward(self, x_in):\n        mu, log_var = self.encoder(x_in)\n        z = self.sampling(mu, log_var)\n        return self.decoder(z), mu, log_var","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae = VAE()\n    \nvae.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(vae.parameters(), lr=0.0005)\n\ndef loss_function(recon_x, x, mu, log_var):\n    \n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n    #MSL = F.mse_loss(recon_x, x)\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) # KL Divergence from MIT 6.S191\n    #return (MSL + KLD)\n    \n    return (BCE + KLD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epoch):\n    vae.train()\n\n    train_loss = 0\n    for batch_idx, (data, _) in enumerate(dataloader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        \n        r_batch, mu, log_var = vae(data)\n\n        loss = loss_function(r_batch, data, mu, log_var)\n\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        if batch_idx%2500==0:\n            print(\"Batch no. finished in Epoch: \", batch_idx)\n    print(\"-------------------------------------------------\")\n    print('Epoch: {} Train mean loss: {:.8f}'.format(epoch, train_loss / len(dataloader.dataset)))\n    print(\"-------------------------------------------------\")\n    return train_loss","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"n_epoches = 8\n\nloss_hist = []\n\nfor epoch in range(1, n_epoches+1):\n    loss_epoch = train(epoch)\n    loss_hist.append(loss_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with torch.no_grad():\n    counter = 0\n    for i in range(100): \n        counter += 1\n        z = (torch.rand(100)*2).to(device)\n        sample = vae.decoder(z).to(device)\n        save_image(sample.view(3, 64, 64), './sample' + str(counter) + '.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(loss_hist)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img in glob.glob(\"*.png\"):\n    display(img)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}