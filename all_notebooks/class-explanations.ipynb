{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Explanations of .py files for project\n\nThis notebook is part of my capstone project on hierarchichal classification:\nhttps://github.com/luka5132/NLPToS\nIn this notebook you can find some additional information on what the classes in the respective .py files do.\nThe files discussed are:\n1. pytorch_classification.py\n2. data_processing.py\n3. hierarchical_classification.py","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# import module we'll need to import our custom module (needed for kaggle)\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/privbert-data/data_processing.py\", dst = \"../working/data_processing.py\")\ncopyfile(src = \"../input/privbert-data/pytorch_classifier.py\", dst = \"../working/pytorch_classifier.py\")\ncopyfile(src = \"../input/privbert-data/hierarchical_data.py\", dst = \"../working/hierarchical_data.py\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch_classification\n\nThis file was initially planned to contain all code related to training a BERT model. However with memory constraints on kaggle I was forced to take out large bits, mainly everything that was used a GPU 'device', i.e. all of training and testing. Remains of the class are still used as *shortcuts* for loading data.","metadata":{}},{"cell_type":"code","source":"from pytorch_classifier import BertClassification\n\nexample_class = BertClassification()\nattributes = vars(example_class)\nattnames = [item[0] for item in attributes.items()]\nprint(\"All variables: \\n\")\nprint(''.join(\"%s \\n\" % attname for attname in attnames))\nall_dir = dir(example_class)\nall_functions = [method for method in all_dir if not method.startswith('__') and method not in attnames]\nprint(\"All functions: \\n\")\nprint(''.join(\"%s \\n\" % funcname for funcname in all_functions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above one can see the functions and variables for this class\nThe *init_data, init_optimizer and init_tokenizer*  speak mostly for themselves as they initializes the respecitve data optimizer and tokenizer.\nThe only function really worth explaining here is *encode_texts*","metadata":{}},{"cell_type":"code","source":"encode_texts = example_class.encode_texts\n# variables: max_length = 128, trunc = True, ptml = True,stratify = None, batchsize = 32, rs = 2021, valsize = 0.1, with_labels = True):\n# max_length is the token legnth of the data\n# trunc stands for trunctuation and is defaulted to true\n# ptml = pad to max length and means that we fill the the token array to the max length\n# batch size speaks for itself\n# with_labels decides whether the dataloader contains the labels for the segments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data_processing\n\nThis class is used for creating the one hot vectors for each segment. Because we are working with a lot of classification models this requires some work.","metadata":{}},{"cell_type":"code","source":"# let's first have a look at the data:\n\nall_data = pd.read_csv('../input/privbert-data/op115_processed.csv')\nall_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The information in this dataset that is most relevant to us is the *'category_name', 'attribute_value_pairs and segment_text*.\nIn the explanation of the class one will see how this data is used to create one hot vectors.\nTo see how the data was stratified please consult the following notebook: 'Stratify_data.ipynb'","metadata":{}},{"cell_type":"code","source":"from data_processing import Op115OneHots\n\nexample_class = Op115OneHots(all_data) # the class is initialized with a dataframe\nattributes = vars(example_class)\nattnames = [item[0] for item in attributes.items()]\nprint(\"All variables: \\n\")\nprint(''.join(\"%s \\n\" % attname for attname in attnames))\nall_dir = dir(example_class)\nall_functions = [method for method in all_dir if not method.startswith('__') and method not in attnames]\nprint(\"All functions: \\n\")\nprint(''.join(\"%s \\n\" % funcname for funcname in all_functions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The functions used for this classed are summed up mostly in *'go2'* This function calls the following functions in order:\n- majority_vote() # if majority vote is true\n- sort_df_polseg()  \n- getcats()\n- getdicts()\n- processtuples()\n- getsub()\n- set_oh_names(class_tup) # if the function was loaded with a name of classes\n- tuplist_per_segment()","metadata":{}},{"cell_type":"code","source":"#Majority vote filters annonations that were only annotated once (out of 3 times)\nprint('Number of annotations before majority vote :',len(example_class.df))\nprint()\nexample_class.majority_vote()\nprint('Number of annotations after majority vote :',len(example_class.df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_class.df[:5].columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It then sorts the dataframe based on the policy-segment number\nf5 = example_class.df[:5]\nf5_id = f5[['policy_uid', 'segment_id']].values\nprint(\"First 5 id's before sorting: \")\nprint(''.join(\"%s \\n\" % f for f in f5_id))\nexample_class.sort_df_polseg()\nf5 = example_class.df[:5]\nf5_id = f5[['policy_uid', 'segment_id']].values\nprint(\"First 5 id's after sorting: \")\nprint(''.join(\"%s \\n\" % f for f in f5_id))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with get_cats() we see how many unique categories there are\nprint(\"self.categories before getcats() : \",example_class.categories)\nexample_class.getcats()\ncats = example_class.categories[:5]\nprint(\"after the fucntion we now have a list with the corresponding category values\")\nprint(''.join(\"%s \\n\" % cat for cat in cats))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get dicts turn the type = string 'dictionaires' into actualy dictionaries\nprint(\"attribute value pairs: \\n \", example_class.df.attribute_value_pairs.values[0])\nprint(\"\\n of type: \", type(example_class.df.attribute_value_pairs.values[0]))\nexample_class.getdicts()\nprint()\nprint(\"after calling getdicts(): \\n\", example_class.dicts[0])\nprint(\"\\n of type: \", type(example_class.dicts[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# process tuples crate the tuple list per segment, these tuples are used to create the one hot vectors\nexample_class.processtuples()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"after calling this function we now have a list of all the tuples that contain information on the respective segment\")\nprint(\"one can see this as: Category, Subcategory, Value \\n\")\nf5 = example_class.tuple_list[:5]\nprint(''.join(\"%s \\n\" % f for f in f5))\n\nprint(\"it also produces a list that sees if the values are 'enacted' or true\")\nf5 = example_class.dictvalues[:5]\nprint(''.join(\"%s \\n\" % f for f in f5))\n\nprint(\"and finally it produces a list with all uqniue tuples found, these are the values\")\nprint(\"in total {} unqiue tuples / values are in the data\".format(len(example_class.unique_tups)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we want to do something similar like unique tuples for the category and subcategory classes\nprint(\"To get the unique categories and subcategories we call 'getsub()' \\n\")\nexample_class.getsub()\nprint(\"we then see we have a total number of {} categories\".format(len(example_class.unique_cats)))\nprint(\"and a total number of {} subcategories \\n\".format(len(example_class.unique_atts)))\n\nprint(\"an example of a category: \\n {} \\n\".format(example_class.unique_cats[0]))\nprint(\"an example of a subcategory: \\n {}\".format(example_class.unique_atts[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"then if 'set_oh_name' has an input in the function we overwrite these unique tuples. This is done since some values are rare \\nand thus might not apear in either the training or testing set\")\nprint(\"one can create such a list by calling 'return_oh_names()' \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"finally instead of having the data per annotation the data is grouped into segments\")\nprint(\"one can see that we had {} rows, i.e. tuples in the tuplist created ealier\".format(len(example_class.tuple_list)))\nexample_class.tuplist_per_segment()\nprint(\"\\nafter calling 'tuplist_per_segment()' we now have a list of information per segment\")\nprint(\"this list has length {}\".format(len(example_class.segments_vals)))\nprint(\"an example of a segment is: \\n{}\".format(example_class.segments_vals[0]))\nprint(\"\\none can see it contains the annotations per annotator\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then afther all this preprocessing the *main* function is called, namely new_onehots(). This function return the text segments and one hot vectors for all classificaiton models","metadata":{}},{"cell_type":"code","source":"cat_to_sub, cat_to_val, sub_to_val, all_cats, all_subcats, all_vals, all_texts = example_class.new_onehots()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"calling new_onehots returns a tuple with 7 elements. Namely: \\n\")\nprint(\"1) A dictionary that contains the texts and labels for a category to a subcategory \")\nprint(\"2) A dictionary that contains the texts and labels for a category to the values \")\nprint(\"3) A dictionary that contains the texts and labels for a subcategory to the values \")\nprint(\"4) A list of onehot vectors for the categories per segement \")\nprint(\"5) A list of onehot vectors for the subcategories per segement \")\nprint(\"6) A list of onehot vectors for the values per segement \")\nprint(\"7) A list texts/ segments \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(cat_to_sub['Data Retention'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hierarchical_data\n\nThis class is used to store the advices for the advice system. It also is used for running a gridsearch","metadata":{}},{"cell_type":"code","source":"from hierarchical_data import HierarchicalData\n\nexample_class = HierarchicalData() # the class is initialized with a dataframe\nattributes = vars(example_class)\nattnames = [item[0] for item in attributes.items()]\nprint(\"All variables: \\n\")\nprint(''.join(\"%s \\n\" % attname for attname in attnames))\nall_dir = dir(example_class)\nall_functions = [method for method in all_dir if not method.startswith('__') and method not in attnames]\nprint(\"All functions: \\n\")\nprint(''.join(\"%s \\n\" % funcname for funcname in all_functions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO","metadata":{},"execution_count":null,"outputs":[]}]}