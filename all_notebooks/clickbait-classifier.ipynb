{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Clickbait Classifier\n\nLet's build a simple ClickBait Classifier.\nWe'll try several algorithms and select the one that performs b \nWe'll use the [ClicBait Dataset](https://www.kaggle.com/amananandrai/clickbait-dataset) created by Aman Anand. \nIt contains ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom pandas import DataFrame, Series, read_csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Get the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = read_csv(\"/kaggle/input/clickbait-dataset/clickbait_data.csv\")\ntitles_len = len(titles)\nclckbt_ratio = len(titles[titles[\"clickbait\"]==0])/titles_len\nprint(\"Database lenght : {} \\nClickbait ratio: {}\".format(titles_len, clckbt_ratio))\ntitles.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here some clickbait headlines"},{"metadata":{"trusted":true},"cell_type":"code","source":"for head in titles['headline'][titles['clickbait']==1][:10]:\n    print(head)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Preprocessing and Analysis\n\nBefore doing any analysis let's split the dataset into train and test sets. \nThe test size is the 10% of the total set (about 6000 headlines).\nWee further split the train set into train plus validation (10% of the initial train set). "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(titles['headline'], titles[\"clickbait\"],\n                                                    test_size=.1, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parse the text\n\nImplement the following operations on each string:\n\n- converts to lower-case,\n- expand contractions,\n- remove punctuation,\n- lemmatize words\n\nWe also count the __number of contractions__ for each headline. This is because, usually clickbait headlines have an higer ratio of contracted words. "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom contractions import contractions_dict\nfrom nltk import word_tokenize\nfrom nltk.tag import pos_tag\nimport contractions\nimport string\n\n_lem = WordNetLemmatizer()\ncontractions_set = set(contr.lower() for contr in contractions_dict)\n\n\ndef remove_contractions(string):\n    ''' Expand and count the contractions in a given string.\n        Return string and the contractions number '''\n    string = string.lower()\n    contr_num = sum(1 for contr in contractions_set if contr in string)\n    parsed_string = ' '.join(contractions.fix(word) for word in string.split())\n    return parsed_string, contr_num\n\n\ndef lemmatise_sentence(sentence):\n    \n    # remove contarctions and convert to lower case\n    sentence, contr_num = remove_contractions(sentence.lower())\n    \n    # remove punctuation\n    sentence = sentence.translate(str.maketrans('', '', string.punctuation+'’‘'))  \n    \n    # lemmatize words\n    lemm_str = \"\"\n    for word, tag in pos_tag(word_tokenize(sentence.lower())):\n        if tag.startswith('NN'):\n            word_1 = word \n            pos = 'n'\n        elif tag.startswith('VB'):\n            word_1 = word\n            pos = 'v'\n        elif tag.startswith('CD'):\n            word_1 = 'NUM'\n            pos = 'a'\n        else:\n            word_1 = word\n            pos = 'a'\n        lemm_str += ' '+_lem.lemmatize(word_1, pos)\n    \n    return lemm_str, contr_num","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have defined all the parsing functions we need.\nLet's build a class that implement them on the database."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ParseString(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self = True\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X_prep, contr_list = [], []\n        for string in X:\n            lemm_str, contr_num = lemmatise_sentence(string)\n            X_prep.append(lemm_str)\n            contr_list.append(contr_num)\n        return DataFrame({\"headline\": X_prep, \"contr num\":contr_list})","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_prep = ParseString().fit_transform(X=X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bag of Words\n\nTime to make the features.We create a bag of words that counts 200 most common clickbait words. \nTo better generalize, we then remove some common words related to actuality, as Trum, Donald, Obama, ... "},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nfrom sklearn.feature_extraction.text import CountVectorizer\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')    # shut up bs4 URL warning\n\n\ndef make_vocabulary(X, length, rm_words=False, to_del_words_list=None):\n    vectorizer = CountVectorizer(max_features=length)\n    vectorizer.fit(X)\n    vocab = vectorizer.get_feature_names()\n    if rm_words:\n        for word in to_del_words_list:\n            if word in vocab:\n                vocab.remove(word)\n    return vocab\n\n\ndef save_vocabulary(words_list, txt_file):\n    file = open(txt_file, 'w+')\n    for word in words_list:\n            file.write(str(key)+'\\n')\n    file.close()\n    \n    print('Vocabulary stored in \"{}\"'.format(txt_file))\n    \n\ndef load_vocabulary(length, txt_file, to_del=None):\n    file = open(txt_file, 'r')\n    vocab = np.array([file.readline().rstrip().lower() for line in range(length)])\n    file.close()\n    print('Dictionary loaded.')\n    return vocab\n    \n\nto_del = ['trump','donald','christmas','obama','president','america','harry','russian','russia','china',\n          'american']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make the vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_clckb = X_train_prep.headline.values[y_train==1]\nX_train_noclckbt = X_train_prep.headline.values[y_train==0]\n\nfull_vocab = make_vocabulary(X_train_prep.headline, length=20)\nclckbt_vocab = make_vocabulary(X_train_clckb, length=21, rm_words=True, to_del_words_list=to_del)\nno_clckbt_vocab = make_vocabulary(X_train_noclckbt, length=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Here the 20 most common clickbait words (in alphabetical order):"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"common_words = DataFrame({'No Clickbait': no_clckbt_vocab[:20], \n                          'Clickbait': clckbt_vocab[:20], \n                          'Full': full_vocab[:20]})\ncommon_words.transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create the Pipeline\n\nTime to create a Pipeline that generates all the features. Together with the __words count__ we add 4 additional features \n- headline length (number of words)\n- stopwords ratio \n- contractions ratio\n- a flag if the headline starts with a number"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import coo_matrix, hstack\nfrom sklearn.pipeline import Pipeline\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n\nclass PreProcess(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary):\n        self.vocabulary = vocabulary\n        self.vectorizer = CountVectorizer(vocabulary=self.vocabulary)\n        self.stopwords_set = set(stopwords.words('english'))\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        # bag of words\n        X_bag = self.vectorizer.transform(X.headline)\n        # meta data\n        meta_arr = []\n        for i in range(len(X)):\n            d = Counter(X.headline.iloc[i].split())\n            num_flag = 1 if list(d)[0]=='NUM' else 0\n            n_of_words = sum(d.values())\n            contr_r = X['contr num'].iloc[i]/n_of_words\n            stop_r = sum(d[key] for key in set(d.keys())&self.stopwords_set) / n_of_words\n            meta_arr.append([num_flag, contr_r, stop_r, n_of_words])\n        meta_arr = coo_matrix(meta_arr)\n        return hstack([X_bag, meta_arr])\n\n    \n    \nfull_pipeline = Pipeline([\n    (\"parse text\", ParseString()),\n    (\"gen features\", PreProcess(vocabulary=clckbt_vocab))\n])\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_prep = full_pipeline.fit_transform(X_train)\nX_train_prep.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a mini train set for exploratory analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_mini = X_train_prep.toarray()[:1000]\ny_train_mini = y_train[:1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Train some classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, make_scorer\n\nscorers = {\n    'precision_score': make_scorer(precision_score),\n    'recall_score': make_scorer(recall_score),\n    'accuracy_score': make_scorer(accuracy_score)\n}\n\n\ndef print_scores(clf_cv, acc=True, prec=True, rec=True):\n    '''Print cross validation results.\n       Valid only for CrossValidation.\n    '''\n    if acc:\n        print('accuracy: %.3f' % clf_cv['test_accuracy_score'].mean())\n    if prec:\n        print('precision: %.3f' % clf_cv['test_precision_score'].mean())\n    if rec:\n        print('recall: %.3f' % clf_cv['test_recall_score'].mean())\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's train some classifier and see which one performs the best."},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Train Naive Bayes only on the Bag of Words\n\nFirst we try it on Train_mini"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmnb_clf = MultinomialNB()\n\nmnb_clf_cv = cross_validate(mnb_clf, X_train_mini[:,:200], y_train_mini, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(mnb_clf_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... then on the full rain set"},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb_clf_cv = cross_validate(mnb_clf, X_train_prep.toarray()[:,:200], y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(mnb_clf_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we fit the model on the full train set. We'll use it's output on as a feature for a SVC and Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb_clf.fit(X_train_prep.toarray()[:,:200], y_train)\nprobabilities = mnb_clf.predict_proba(X_train_prep.toarray()[:,:200])[:, 1]\nprobabilities = probabilities.reshape((len(probabilities), 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Train Random Forest on Naive Bayes probabilities and non-word features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_forest = np.concatenate([probabilities, X_train_prep.toarray()[:, 200:]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we train it on train_mini"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(random_state=42)\n\nforest_cv = cross_validate(forest_clf, X_train_forest[:1000], y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(forest_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we fine tune some hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distrib = {\"n_estimators\": list(range(1,500)), \"max_depth\": reciprocal(2,100)}\n\nrnd_srch_forest = RandomizedSearchCV(forest_clf, param_distributions=param_distrib,\n                                     cv=5, scoring='accuracy', random_state=42,\n                                     n_iter=100, verbose=5, n_jobs=-1)\n\nrnd_srch_forest.fit(X_train_forest[:1000], y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_forest.best_score_)\nprint('Best params:', rnd_srch_forest.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now use __Cross Validation__ on the full train set to get a estimate of the performances."},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_clf = rnd_srch_forest.best_estimator_\n\nforest_cv = cross_validate(forest_clf, X_train_forest, y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(forest_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Train SVM on Naive Bayes probabilities and non-word features\n\nLet's do the same thing of Sec.4.2 but using a SVM instead.\n\nExploratory analysis on train_mini."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nsvc_clf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('svm', SVC())\n])\n\nsvc_cv = cross_validate(svc_clf, X_train_forest[:1000], y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(svc_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine tune"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"param_distrib = {'svm__kernel': ['rbf', 'poly'],\n                 'svm__C': uniform(1,20),\n                 'svm__gamma': reciprocal(.0001, .1),\n                }\n                 \n\nrnd_srch_svc = RandomizedSearchCV(svc_clf, param_distributions=param_distrib,\n                                  cv=5, scoring='accuracy', n_iter=1000, verbose=5, n_jobs=-1)\n\nrnd_srch_svc.fit(X_train_forest[:1000], y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_svc.best_score_)\nprint('Best params:', rnd_srch_svc.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross Validation on train_set"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf = rnd_srch_svc.best_estimator_\n\nsvc_clf_cv = cross_validate(svc_clf, X_train_forest, y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(svc_clf_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Train naive Random Forest on all the features\n\nNow let's try a blind Random forest on all the 204 features. \n\nFirst on train_mini"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_clf_v1 = RandomForestClassifier(random_state=42)\n\nforest_cv_v1 = cross_validate(forest_clf_v1, X_train_mini, y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(forest_cv_v1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine tune"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distrib = {\"n_estimators\": list(range(1,500)), \"max_depth\": reciprocal(2,100)}\n\nrnd_srch_forest_v1 = RandomizedSearchCV(forest_clf_v1, param_distributions=param_distrib,\n                                     cv=5, scoring='accuracy', random_state=42,\n                                     n_iter=100, verbose=5, n_jobs=-1)\n\nrnd_srch_forest_v1.fit(X_train_mini, y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_forest_v1.best_score_)\nprint('Best params:', rnd_srch_forest_v1.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cross validation on train_set"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_clf_v1 = rnd_srch_forest_v1.best_estimator_\n\nforest_cv_v1 = cross_validate(forest_clf_v1, X_train_prep, y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(forest_cv_v1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Train naive SVM on all the features"},{"metadata":{},"cell_type":"markdown","source":"Now let's train a blind SVC on all the 204 features. Before doing that do not forget to __rescale the features__!"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf_v1 = Pipeline([\n    ('scaler', StandardScaler(with_mean=False)),\n    ('svm', SVC())\n])\n\nsvc_cv_v1 = cross_validate(svc_clf_v1, X_train_mini, y_train_mini, cv=5, scoring=scorers, n_jobs=-1)\nprint_scores(svc_cv_v1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine tune"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_distrib = {'svm__kernel': ['rbf', 'poly'],\n                 'svm__C': uniform(1,20),\n                 'svm__gamma': reciprocal(.0001, .1),\n                }\n\nrnd_srch_svc_v1 = RandomizedSearchCV(svc_clf_v1, param_distributions=param_distrib,\n                                  cv=5, scoring='accuracy', n_iter=100, verbose=5, n_jobs=-1)\n\nrnd_srch_svc_v1.fit(X_train_mini, y_train_mini)\n\nprint('Best score: %.3f' % rnd_srch_svc_v1.best_score_)\nprint('Best params:', rnd_srch_svc_v1.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf_v1 = rnd_srch_svc_v1.best_estimator_\n\nsvc_clf_cv_v1 = cross_validate(svc_clf_v1, X_train_prep.toarray(), y_train, cv=5, scoring=scorers, n_jobs=5)\nprint_scores(svc_clf_cv_v1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5 Test on Validation set\n\nEvaluate the 5 classifiers we've built on the Validation set to see which performs better\n\n### Prepare titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val_prep = full_pipeline.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_pred = mnb_clf.predict(X_val_prep.toarray()[:,:200])\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_proba = mnb_clf.predict_proba(X_val_prep.toarray()[:,:200])[:,1]\ny_val_proba = y_val_proba.reshape((len(y_val), 1))\nX_val_forest = np.concatenate([y_val_proba, X_val_prep.toarray()[:, 200:]], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest on 5 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_clf.fit(X_train_forest, y_train)\ny_val_pred = forest_clf.predict(X_val_forest)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVC on 5 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf.fit(X_train_forest, y_train)\ny_val_pred = svc_clf.predict(X_val_forest)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest on 204 features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"forest_clf_v1.fit(X_train_prep, y_train)\ny_val_pred = forest_clf_v1.predict(X_val_prep)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVC on 204 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_clf_v1.fit(X_train_prep, y_train)\ny_val_pred = svc_clf_v1.predict(X_val_prep)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_val, y_val_pred))\nprint('Precision: %.3f' % precision_score(y_val, y_val_pred))\nprint('Recall: %.3f' % recall_score(y_val, y_val_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Final evaluation on the Test Set\n\nChoose the best algorithm: __SVC classifier on 204 features__."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_test_prep = full_pipeline.transform(X_test)\ny_test_pred = svc_clf_v1.predict(X_test_prep)\n\nprint('Score on the test set: %.3f' % accuracy_score(y_test, y_test_pred))\nprint('Precision: %.3f' % precision_score(y_test, y_test_pred))\nprint('Recall: %.3f' % recall_score(y_test, y_test_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}