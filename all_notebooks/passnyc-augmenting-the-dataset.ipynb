{"cells":[{"metadata":{"_uuid":"e00a00e99306637c3c06e41f7f7727772fb54ae8"},"cell_type":"markdown","source":"\nIn this kernel, I look at augmenting the data provided in the 2016 NYC School Explorer, using datasets pulled both from Socrata's NYC Open Data, as well as other datasets pulled or scraped from the web. The goal of this work is to add nuance and depth to the feature set provided by the original school explorer so that we can build a more comprehensive model for estimating SHSAT test-takers and specialized HS acceptance rates. \n\nI will be adding  the following data to our dataset:\n1.  Population density of the zip code where the school is located\n2.  Demographic breakdown of the school district where the school is located\n3.  Total count of auto collisions within a 0.5 km radius from the school location for the years 2012-2018\n4.  Grade 6 acceptance rates for each school from 2014-15\n\nI'll be describing each in detail later. First, let's load the starting-point dataset, which is a pruned version of the School Explorer and which I've made public already [here](http://www.kaggle.com/gtreen/nyc-2016-school-explorer-refined)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# function for printing out top schools for a given parameter\n\ndef print_top_N_schools (data, target_key, N, order='Top'):\n    if (target_key in data.keys()):\n        print ('%s schools for parameter %s:' % (order, target_key))\n        d = np.array(data[target_key])\n        for i in range(N):\n            if (order=='Top'):\n                index = np.argmax(d)\n                value = np.max(d)\n                d[index] = 0.0\n            else:\n                index = np.argmin(d)\n                value = np.min(d)\n                d[index] = 1000000.0\n            print ('%d. %s, value = %f' % ((i+1), data['School Name'][index], value))\n\n\n\n\nschools_data    = pd.read_csv ('../input/nyc-2016-school-explorer-refined/nyc_school_explorer_refined.csv')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"435ee1a0185137fe320af5dc6b2e3aea025ee8e8"},"cell_type":"markdown","source":"\n**1. Zipcode Population Density:**\n\nIt occurred to me that understanding the population density in the school's vicinity might be an interesting parameter to fit into a model. Intuitively, you might expect denser areas to have higher economic needs and poorer-performing schools. I grabbed the Zip code densities from [this site](https://blog.splitwise.com/2014/01/06/free-us-population-density-and-unemployment-rate-by-zip-code/). The data is from 2010, but I figure it's recent enough to use for a model using 2016 school data.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"407f6bb74ce5428ceb1fcbb04ea9479e3818629f","collapsed":true},"cell_type":"code","source":"zip_code_density_data = pd.read_csv ('../input/us-population-density-by-zip-code-2010/Zipcode-ZCTA-Population-Density-And-Area-Unsorted.csv')\n\n\ndef get_density (zip_code):\n    for i in range(len(zip_code_density_data)):\n        if (zip_code_density_data['Zip/ZCTA'][i]==zip_code):\n            return zip_code_density_data['Density Per Sq Mile'][i]\n\ndensities = []\nfor i in range(len(schools_data)):\n    zipcode = schools_data['Zip'][i]\n    densities.append (get_density(zipcode))\n\n# create a new schools data parameter: Zip Density\n    \nschools_data['Zip Density'] = densities\n\nprint_top_N_schools (schools_data, 'Zip Density', 10, 'Top')\nprint ('')\nprint_top_N_schools (schools_data, 'Zip Density', 10, 'Bottom')\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f83229c4ed9926733d72c7358fafb04a1c3cda89"},"cell_type":"markdown","source":"**2. School District Demographic Breakdown:**\n\nSimilarly, I want to add population demographics for all of the school districts. This is privided by the Socrata NYC School District Breakdowns dataset. I chose to focus on the following demographics data:\n* Percent Asian\n* Percent Black\n* Percent Hispanic\n* Percent White\n* Percent Receives Public Assistance\n* Percent U.S. Citizen\n* Percent Permanent Resident Alien\n"},{"metadata":{"trusted":true,"_uuid":"6b157c095fee118f249aa1ead13666bc59e0a91a","collapsed":true},"cell_type":"code","source":"school_district_breakdowns = pd.read_csv ('../input/nyc-school-district-breakdowns/school-district-breakdowns.csv')\n\n\n# get the district breakdown data\n\ndistrict_asian_pct = []\ndistrict_black_pct = []\ndistrict_hispanic_pct = []\ndistrict_white_pct = []\ndistrict_public_assistance_pct = []\ndistrict_us_citizen_pct = []\ndistrict_permanent_res_alien_pct = []\nfor i in range(len(schools_data)):\n    district  = schools_data['District'][i]\n    district_asian_pct.append(school_district_breakdowns['PERCENT ASIAN NON HISPANIC'][district-1])\n    district_black_pct.append(school_district_breakdowns['PERCENT BLACK NON HISPANIC'][district-1])\n    district_hispanic_pct.append(school_district_breakdowns['PERCENT HISPANIC LATINO'][district-1])\n    district_white_pct.append(school_district_breakdowns['PERCENT WHITE NON HISPANIC'][district-1])\n    district_public_assistance_pct.append(school_district_breakdowns['PERCENT RECEIVES PUBLIC ASSISTANCE'][district-1])\n    district_us_citizen_pct.append(school_district_breakdowns['PERCENT US CITIZEN'][district-1])\n    district_permanent_res_alien_pct.append(school_district_breakdowns['PERCENT PERMANENT RESIDENT ALIEN'][district-1])\n\n# add to the schools dataset\n\nschools_data['District Asian %'] = district_asian_pct\nschools_data['District Black %'] = district_black_pct\nschools_data['District Hispanic %'] = district_hispanic_pct\nschools_data['District White %'] = district_white_pct\nschools_data['District Public Assistance %'] = district_public_assistance_pct\nschools_data['District U.S. Citizen %'] = district_us_citizen_pct\nschools_data['District Permanent Resident Alien %'] = district_permanent_res_alien_pct\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0fa334ac596aa62e2d719b0db09f1fa9a1c924d"},"cell_type":"markdown","source":"\n**3. Nearby Auto Collisions Data:**\n\nOne of the Socrata datasets lists every motor vehicle collision reported by the NYPD from 2012-2018 ([NYPD Motor Vehicle Collisions](https://www.kaggle.com/new-york-city/nypd-motor-vehicle-collisions)). As you can imagine, this dataset is huge, with over 1.3 million entries. The following code demonstrates how I parsed the collision location data, mapped it to distances in km from each school, and counted the number of entries that fell within the 0.5 km threshold I set. It takes a long time, so **I don't recommend you run it here** (that's why it's commented out). Rather, I've uploaded the resulting data to a public .csv dataset ([NYC Middle Schools and Nearby Auto Collisions](https://www.kaggle.com/gtreen/nyc-middle-schools-and-nearby-auto-collisions)). \n\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"\n\n#collisions_data = pd.read_csv ('../input/nypd-motor-vehicle-collisions/nypd-motor-vehicle-collisions.csv')\n\n#def get_distance_in_km (ll1, ll2):\n#   # approximate radius of earth in km\n#    R = 6373.0\n#    lat1 = radians(ll1[0])\n#    lon1 = radians(ll1[1])\n#    lat2 = radians(ll2[0])\n#    lon2 = radians(ll2[1])\n#    dlon = lon2 - lon1\n#    dlat = lat2 - lat1\n#    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n#    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n#    distance = R * c\n#    return distance\n\n## find the number of collisions reported within the vicinity of a school\n\n#collision_dist_thresh = 0.5\n#collisions_count = []\n#for i in range(len(schools_data)):\n#    nearby_collisions = 0\n#    print (time.ctime())\n#    print ('School: %s:' % schools_data['School Name'][i])\n#    for j in range(len(collisions_data)):\n#        dist = get_distance_in_km ((schools_data['Latitude'][i], schools_data['Longitude'][i]),\n#                                   (collisions_data['LATITUDE'][j], collisions_data['LONGITUDE'][j]))\n#        if (dist < collision_dist_thresh):\n#            nearby_collisions+=1\n#    print ('Found %d nearby collisions.' % nearby_collisions)\n#    collisions_count.append(nearby_collisions)\n#    np.save ('collisions_count', collisions_count)\n    \n#school_df = pd.DataFrame(schools_data['School Name'])\n#school_df['Nearby Auto Collisions 2012-18'] = collisions_count\n#school_df.to_csv('nyc_middle_schools_collisions.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3186bc9b25e520c53f66cdb2ef045b9b496da538"},"cell_type":"markdown","source":"Here's the shortcut: simply read the resulting CSV and add it to the schools dataset.\n\nIn the next code fragment, I've plotted the schools data on a map of New York City, with red indicating a very high nearby collisions count and blue indicating a low count. As you can see, the schools with the worst traffic problems are concentrated in lower Manhattan and near the bridges to New Jersey or the other boroughs."},{"metadata":{"trusted":true,"_uuid":"f099f40f9be942f05153228cb8287bca479c69fb","collapsed":true},"cell_type":"code","source":"schools_collisions_data = pd.read_csv ('../input/nyc-middle-schools-and-nearby-auto-collisions/nyc_middle_schools_collisions_2012_2018.csv')\n\n\nschools_data['Nearby Auto Collisions'] = schools_collisions_data['Nearby Auto Collisions 2012-18']\n\n\n# plot the collisions data on a map for visualization\n\nimport folium\nfrom folium import plugins\nfrom io import StringIO\nimport folium \n\n\ncollisions = schools_data['Nearby Auto Collisions']\nmap_data = collisions\n\ninterval = (max(map_data)-min(map_data))/255.0\nred_val = ((map_data-min(map_data))/interval).astype('int')\n\nm = folium.Map([schools_data['Latitude'][0], schools_data['Longitude'][0]], zoom_start=10.3,tiles='stamentoner')\n\n#for lat, long, col in zip(schools_data['Latitude'], schools_data['Longitude'], cols):\nfor lat, long, red in zip(schools_data['Latitude'], schools_data['Longitude'], red_val):\n    #rown = list(rown)\n    #folium.CircleMarker([lat, long], color='#0000ff', fill=True, radius=2).add_to(m)\n    colourString = '#%0.2x00%0.2x' % (red, (255-red))\n    folium.CircleMarker([lat, long], color=colourString, fill=True, radius=2).add_to(m)\n\n\nm\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a12120b4b8c71492f216785856a3788f3653820b"},"cell_type":"markdown","source":"**4. Grade 6 Acceptance Rates **\n\nNot only are New York City's public specialized high schools elite, but so are some of their middle schools. According to [this](https://www.dnainfo.com/new-york/20141028/morningside-heights/map-these-are-most-popular-middle-schools-new-york-city/) article, \"Across the city, 65 middle school programs accepted less than 10 percent of their applicants last year, making the public schools just as selective as Ivy League universities like Brown and UPenn.\" Knowing the relative exclusivity of the middle school may explain why some high-achieving students are chosen over others for the specialized high schools, and therefore this data is a potentially useful addition to our dataset.\n\nI scraped the grade 6 applications/offers data from the link I posted above, then combined all of the programs for each school into a single row (one row per school), with the total of all program applications received and offers given (I made the resulting dataset public [here](https://www.kaggle.com/gtreen/nyc-middle-schools-grade-6-applicationsoffers). This data is from the school year 2014-15, which isn't concurrent with our School Explorer dataset, but recent enough that the relative admissions rates may not be so different from 2016.\n\nIn the code below, I used the 'DBN' entry in this grade 6 dataset to match with the 'Location Code' parameter in the main schools dataset.\n"},{"metadata":{"trusted":true,"_uuid":"cddadab6546ead35f1bd12e53d4434e9eeab6b27","collapsed":true},"cell_type":"code","source":"grade6_applications_offers = pd.read_csv ('../input/nyc-middle-schools-grade-6-applicationsoffers/nyc_middle_school_grade6_offers.csv')\n\n# set the school grade 6 acceptance rate\n\nacceptance_rates=[]\nnot_found_cnt = 0\nnot_found_charter_cnt = 0\n\nfor i in range(len(schools_data)):\n    acc_rate = 0.0\n    for j in range(len(grade6_applications_offers)):\n        if (schools_data['Location Code'][i]==grade6_applications_offers['DBN'][j]):\n            acc_rate = float(grade6_applications_offers['Offers'][j]) / (\n                       float(grade6_applications_offers['Applications'][j]))\n            break\n    if (acc_rate==0.0):\n        not_found_cnt += 1\n        if ('CHARTER' in schools_data['School Name'][i]):\n            not_found_charter_cnt += 1\n        print ('Warning: could not find applications and offers data for school %s.' % schools_data['School Name'][i])\n    acceptance_rates.append(acc_rate)\n\nprint ('Could not find application/offer data for %d schools (%d charter schools).' % (not_found_cnt, not_found_charter_cnt))\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01857a58289dae8f94df8d995c5f2dedbaada03d"},"cell_type":"markdown","source":"We can see a problem here. We only have applications/offers data for 462 of the 588 middle schools in our School Explorer dataset. Fortunately, most of those are charter schools, which, according to [www.nyccharterschools.org](http://www.nyccharterschools.org/enrollment-faq#2), are not allowed to be selective in their admissions and must admit students on a first-come-first-served basis. So we can set all charter school admission rates to 1.0. That still leaves 31 public non-charter schools for which we don't have applications/offers data. Just upon cursory examination, these appear primarily to be schools that did not exist, were too new or did not admit grade 6 students in 2014-15 when the data was accumulated. It was a tough decision, but I decided ultimately to remove these 31 schools from the model, since they represent only about 5.3% of the total."},{"metadata":{"trusted":true,"_uuid":"e7bdd96149a1fd8a305a335ac3c7eb7e778d3add","collapsed":true},"cell_type":"code","source":"# set the charter school acceptance rate to 1.0\n\nfor i in range(len(schools_data)):\n    if (acceptance_rates[i]==0.0):\n        if ('CHARTER' in schools_data['School Name'][i]):\n            acceptance_rates[i] = 1.0\n\n# remove the schools for which we have no acceptance rate data\n\n\nacceptance_rates = np.array(acceptance_rates)\nvalid_acceptance_rates = acceptance_rates[acceptance_rates>0.0]\nschools_data = schools_data[acceptance_rates>0.0]\n\n# re-construct the DataFrame\nschools_data = pd.DataFrame(data=np.array(schools_data),columns=schools_data.keys())\nschools_data['Grade 6 Acceptance Rate'] = valid_acceptance_rates\n\n\nprint_top_N_schools (schools_data, 'Grade 6 Acceptance Rate', 10, 'Lowest')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f07c85e0e895be003fa611b9c199f7e368766342","collapsed":true},"cell_type":"markdown","source":"\n**Analysis:**\n\nHere I'm going to look at some variable correlation and derive some insights. We can see that higher densities in the areas surrounding the schools are most closely correlated with:\n    a) nearby auto collisions, \n    b) economic needs and a high percentage of the population on public assistance and \n    c) minority (hispanic and black) populations.\nThis seems to make intuitive sense. Those with means are more likely to reside in less-dense areas with safer streets. \n\nNext, we look at the variables most correlated with elite (low acceptance rate) middle schools. Therefore, we want a negative correlation. \nIt appears that there are no strong negative correlations with elite middle schools, but what does stand out are the percentage of high-achieving Asian students, which suggests that Asian students are likely overrepresented in the city's elite middle schools, which are likely gateways to the elite specialized high schools.\n\n\n\n"},{"metadata":{"trusted":true,"_uuid":"f4d5ade10a6922755d4cf97e0e5d0ac0c0ea587b","collapsed":true},"cell_type":"code","source":"from scipy.stats.stats import pearsonr\n\n\nadmin_index = 10\nX=np.array(schools_data)[:,admin_index:]\nX_keys=schools_data.keys()[admin_index:]\n\n\n\ndef print_top_N_correlations (keys, data, target_key, N, order='Highest'):\n    coefficients = []\n    target_index = -1\n    for i in range(len(keys)):\n        if (keys[i] == target_key):\n            target_index=i\n            break\n    if (target_index<0):\n        print ('Could not find key %s in key list.' % target_key)\n        return\n    for i in range(len(keys)):\n        coeff = pearsonr (data[:,i], data[:,target_index])[0]\n        coefficients .append (coeff)\n    print ('%s Pearson\\'s correlation with %s:' % (order, target_key))\n    c = np.array(coefficients)\n    for i in range(N):\n        if (order=='Highest'):\n            index = np.argmax(c)\n        else:\n            index = np.argmin(c)\n        print ('%d. Key: %s, correlation = %f' % ((i+1), keys[index], c[index]))\n        c[index] = 0.0\n\n\n\n\nprint_top_N_correlations (X_keys, X, 'Zip Density', 10, 'Highest')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"164c0734347d65041aa2d539c7f48ced8b7bcc52"},"cell_type":"markdown","source":"Next, we look at the variables most correlated with elite (i.e. low acceptance rate) middle schools. Therefore, we want a negative correlation with the grade 6 acceptance rate.\n\nIt appears that there are no strong negative correlations with elite middle schools, but what does stand out are the percentage of high-achieving Asian students. This suggests that Asian students are likely overrepresented in the city's elite middle schools, which are likely gateways to the specialized high schools."},{"metadata":{"trusted":true,"_uuid":"e6e3966f338052e23fe4316482ee349ba968ce84","collapsed":true},"cell_type":"code","source":"print_top_N_correlations (X_keys, X, 'Grade 6 Acceptance Rate', 10, 'Lowest')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61853ff9fbb7fe6b53c64f5af8f646b03cd8459e"},"cell_type":"markdown","source":"Finally, let's save the augmented schools dataset to CSV for model-building."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c4136699a4d66f997473d7fea43141445addc8c1"},"cell_type":"code","source":"schools_data.to_csv('nyc_augmented_school_explorer.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}