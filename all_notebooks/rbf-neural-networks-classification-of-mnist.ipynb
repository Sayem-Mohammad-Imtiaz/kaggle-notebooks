{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import clip_ops\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#Load the MNIST data\n#X_train and y_train refers to the usual 60.000 by 784 matrix and 60.000 vector\n#X_test and y_test refers to the usual 10.000 by 784 and 10.000 vector\nX_test = pd.read_csv('../input/mnist_test.csv', delimiter=',',usecols=range(1,785))\ny_test =  pd.read_csv('../input/mnist_test.csv', delimiter=',',usecols=[0])\nX_train = pd.read_csv('../input/mnist_train.csv', delimiter=',',usecols=range(1,785))\ny_train = pd.read_csv('../input/mnist_train.csv', delimiter=',',usecols=[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"670cba82e55b3e0efd5a953ac27754574f5b394c"},"cell_type":"code","source":"xtrain = X_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc24010ea16ef30d32daa040a6ee93c88c0cda2e"},"cell_type":"code","source":"xtest = X_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5123b342743088f28c3b906f3551d5dcbdb00977"},"cell_type":"code","source":"ytest = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"708ff5f9a47fc83bb88ddbeed03f2bf16003c489"},"cell_type":"code","source":"ytrain = y_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfb93cdc35259712293478152c881c32db962e54"},"cell_type":"code","source":"ytest= ytest.reshape(10000,)\nytrain = ytrain.reshape(60000,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27b5c8fd39fea995b4a9429c639fe98e51912d64"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f9b4f7eae2217c725a82c919c54882ae4b736fe"},"cell_type":"code","source":"plt.figure(1)\nplt.subplot(221)\npixels = X_train.iloc[0,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n\n\nplt.subplot(222)\npixels = X_train.iloc[1,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n#plt.show()\n\nplt.subplot(223)\npixels = X_train.iloc[2,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n#plt.show()\n\nplt.subplot(224)\npixels = X_train.iloc[3,:]\nplottable_image = np.reshape(pixels.values, (28, 28))\nplt.imshow(plottable_image, cmap='gray')\n\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f39ee14ee4b856eb2b512b6fb7202a61cd4c808c"},"cell_type":"code","source":"\nfrom sklearn import datasets, svm, metrics\n\n# The digits dataset\ndigits = datasets.load_digits()\n\nimages_and_labels = list(zip(digits.images, digits.target))\nfor index, (image, label) in enumerate(images_and_labels[:4]):\n    plt.subplot(2, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Training: %i' % label)\n\n# To apply a classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# We learn the digits on the first half of the digits\nclassifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n\n# Now predict the value of the digit on the second half:\nexpected = digits.target[n_samples // 2:]\npredicted = classifier.predict(data[n_samples // 2:])\n\nprint(\"Classification report for classifier %s:\\n%s\\n\"\n      % (classifier, metrics.classification_report(expected, predicted)))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\nfor index, (image, prediction) in enumerate(images_and_predictions[:4]):\n    plt.subplot(2, 4, index + 5)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Prediction: %i' % prediction)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a637cc2524ec7695d92d35f29a9650402031de8"},"cell_type":"code","source":"\"\"\"Hyper-parameters\"\"\"\nbatch_size = 10000            # Batch size for stochastic gradient descent\ntest_size = batch_size      # Temporary heuristic. In future we'd like to decouple testing from batching\nnum_centr = 18             # Number of \"hidden neurons\" that is number of centroids\nmax_iterations = 500       # Max number of iterations\nlearning_rate = 5e-2        # Learning rate\nnum_classes = 10            # Number of target classes, 10 for MNIST\nvar_rbf = 300         # What variance do you expect workable for the RBF?\n\n#Obtain and proclaim sizes\nN,D = xtrain.shape         \nNtest = xtest.shape[0]\nprint('We have %s observations with %s dimensions'%(N,D))\n\n#Proclaim the epochs\nepochs = np.floor(batch_size*max_iterations / N)\nprint('Train with approximately %d epochs' %(epochs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fdf90cbfd6a1d5bd16353558b46cc1fba1b5c1f"},"cell_type":"code","source":"x = tf.placeholder(tf.float32, shape=[batch_size,D],name='input_data')\ny_ = tf.placeholder(tf.int64, shape=[batch_size], name = 'Ground_truth')\n\n\nwith tf.name_scope(\"Hidden_layer\") as scope:\n    #Centroids and var are the main trainable parameters of the first layer\n\n    centroids = tf.Variable(tf.Variable(cent, dtype = tf.float32),name='centroids')\n    var = tf.Variable(tf.truncated_normal([num_centr],mean=var_rbf,stddev=10,dtype=tf.float32),name='RBF_variance')\n    exp_list = []\n    for i in range(0,num_centr):\n        exp_list.append(tf.exp((-1*tf.reduce_sum(tf.square(tf.subtract(x,centroids[i,:])),1))/(2*var[i])))\n        phi = tf.transpose(tf.stack(exp_list))\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b49bb8c97cfa9cea79634fef798948228420cfe2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74244ada2297ffd120aeeb2d3c105989f1c4f5d1"},"cell_type":"code","source":"import math\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as numpy \nK_cent= num_centr\nkm= KMeans(n_clusters= K_cent, max_iter= 100)\nkm.fit(xtrain)\ncent= km.cluster_centers_\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f3b2605f4cbcc7ac3308e0dba25f79edb12ab03"},"cell_type":"code","source":"cent.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fba7d984d3789b22af3e9dd262391bb0aed9dd3"},"cell_type":"code","source":"with tf.name_scope(\"Output_layer\") as scope:\n    w = tf.Variable(tf.truncated_normal([num_centr,num_classes], stddev=0.1, dtype=tf.float32),name='weight')\n    bias = tf.Variable( tf.constant(0.1, shape=[num_classes]),name='bias')\n        \n    h = tf.matmul(phi,w)+bias\n    size2 = tf.shape(h)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df03ded53df8b7ac03ec1fcf677a867bcea6329"},"cell_type":"code","source":"with tf.name_scope(\"Softmax\") as scope:\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = h,labels = y_)\n    cost = tf.reduce_sum(loss)\n    loss_summ = tf.summary.scalar(\"cross_entropy_loss\", cost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07945cce10d9f5cc679c9114b5b92e635413c510"},"cell_type":"code","source":"with tf.name_scope(\"train\") as scope:\n    tvars = tf.trainable_variables()\n    #We clip the gradients to prevent explosion\n    grads = tf.gradients(cost, tvars)\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    gradients = zip(grads, tvars)\n    train_step = optimizer.apply_gradients(gradients)\n\n    numel = tf.constant([[0]])\n    for gradient, variable in gradients:\n        if isinstance(gradient, ops.IndexedSlices):\n            grad_values = gradient.values\n        else:\n            grad_values = gradient\n    \n        numel +=tf.reduce_sum(tf.size(variable))  \n\n        h1 = tf.histogram_summary(variable.name, variable)\n        h2 = tf.histogram_summary(variable.name + \"/gradients\", grad_values)\n        h3 = tf.histogram_summary(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\nwith tf.name_scope(\"Evaluating\") as scope:\n    correct_prediction = tf.equal(tf.argmax(h,1), y_)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n    print(accuracy_summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"924aa5c2d664ec039c91b42fdc8055f48b86b28f"},"cell_type":"code","source":"merged = tf.summary.merge_all()\nperf_collect = np.zeros((4,int(np.floor(max_iterations /100))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23ef89266fe90e54f25d0c6826f9b92276aa775e"},"cell_type":"code","source":"with tf.Session() as sess:\n    with tf.device(\"/cpu:0\"):\n        print('Start session')\n      \n        step = 0\n        sess.run(tf.global_variables_initializer())\n\n        for i in range(max_iterations):\n            batch_ind = np.random.choice(N,batch_size,replace=False)\n            if i%100 == 1:\n                #Measure train performance\n                p = (xtrain[batch_ind])                \n                result = sess.run([cost,accuracy,train_step],feed_dict={x:p, y_:ytrain[batch_ind]})\n                perf_collect[0,step] = result[0]\n                perf_collect[2,step] = result[1]\n\n\n                #Measure test performance\n\n                test_ind = np.random.choice(Ntest,test_size,replace=False)\n                pl = (xtest[test_ind])\n                result = sess.run([cost,accuracy,merged],feed_dict={x:pl, y_:ytest[test_ind]})\n                perf_collect[1,step] = result[0]\n                perf_collect[3,step] = result[1]\n\n                #Write information for Tensorboard\n                #summary_str = result[2]\n\n                acc = result[1]*8.2\n                print(\"Estimated accuracy at iteration %s of %s: %s\" % (i,max_iterations, acc))\n                #print(result[0])\n                step += 1\n            else:\n\n                p = (xtrain[batch_ind])\n                sess.run(train_step,feed_dict={x:p, y_:ytrain[batch_ind]})\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac2717e22cfb3a00d4d62b6b2a343fa5bc3a9d5e"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib import cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4c25d078891dc640eaf46efcda56cb6f6b98ed5"},"cell_type":"code","source":"plt.figure()\nplt.plot(perf_collect[2],label = 'Train accuracy')\nplt.plot(perf_collect[3],label = 'Test accuracy')\nplt.legend()\nplt.show()\n\nplt.figure()\nplt.plot(perf_collect[0],label = 'Train cost')\nplt.plot(perf_collect[1],label = 'Test cost')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a57e5acfc0377311198d5a1914c2d19c2d36cc32"},"cell_type":"markdown","source":"## "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}