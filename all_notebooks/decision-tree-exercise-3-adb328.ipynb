{"cells":[{"metadata":{"_cell_guid":"cbf7ca05-8871-4237-a741-930925ba3003","_uuid":"5df31c603dd75d38793692236eeeba4e057cd189"},"cell_type":"markdown","source":"## 用决策树来分类贷款是否优良"},{"metadata":{"_cell_guid":"3cb68e77-e44a-4a12-bebb-180fbd4b36ca","_uuid":"17706883ba03560cb2e39bd2ad62cba4675e3ee8"},"cell_type":"markdown","source":"[LendingClub](https://www.lendingclub.com/) 是一家贷款公司. 在本次作业中,我们需要手动实现决策树来预测一份贷款是否安全，并对比不同复杂度下决策树的表现"},{"metadata":{"_cell_guid":"c1770b14-da67-4ff3-a5da-e9cb93190ae1","_uuid":"b52f0ce13e088aa9488baaf9b09a539988824774","collapsed":true,"trusted":false},"cell_type":"code","source":"%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\n\n\n","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"862db2de-3ce3-4d81-8f72-a6968c8984c7","_uuid":"fe1191e114bbf7353f13bb9792266a2b0713a1e2"},"cell_type":"markdown","source":"## 读取数据"},{"metadata":{"_cell_guid":"b0f8dbc5-4785-4a4f-8ff8-388572936d54","_uuid":"7044fc9ec1ac2a3aeefe9d553e398caebe00d839","trusted":false},"cell_type":"code","source":"#data = pd.read_csv(os.path.join(\"data\", \"loan_sub.csv\"), sep=',')\ndata = pd.read_csv(os.path.join(\"../input\", \"loan_sub.csv\"), sep=',')","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"a9121967-470a-42b7-a422-f3b1c75bdff0","_uuid":"17d2e96acc79be8f51051ff71cc804caadcae9b2"},"cell_type":"markdown","source":"## 打印可用特征"},{"metadata":{"_cell_guid":"9d1959ff-8c0a-4772-a37d-9c4e6805f4d1","_uuid":"a9c1f4cc4249ee4ce672f6dc0e081d1018f3b20f","trusted":false},"cell_type":"code","source":"data.columns","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"a0dfc505-8a02-41ea-ba2c-1c781f561649","_uuid":"23a50a996d5cc564b0a248508701d7c38c923019"},"cell_type":"markdown","source":"## 预处理预测目标\n\n预测目标是一列'bad_loans'的数据。其中**1**表示的是不良贷款，**0**表示的是优质贷款。\n\n将预测目标处理成更符合直觉的标签，创建一列 `safe_loans`. 并且: \n\n* **+1** 表示优质贷款, \n* **-1** 表示不良贷款. "},{"metadata":{"_cell_guid":"952788a0-a064-4fa5-8f2c-6184eb4fc64f","_uuid":"c97e524bdc835ef5cc9d9567a53bce69c2462d88","collapsed":true,"trusted":false},"cell_type":"code","source":"# safe_loans =  1 => safe\n# safe_loans = -1 => risky\n#TODO\ndata['safe_loans'] = data.bad_loans.apply(lambda x: +1 if x==0 else -1)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"d8430fdb-370a-490d-871a-6b4008d5a506","_uuid":"956af464c8d6f8340a0d14b8a1adb6647a28fca4"},"cell_type":"markdown","source":"## 打印优质贷款与不良贷款的比例"},{"metadata":{"_cell_guid":"2ca945a1-fd6f-4f2d-aa21-759841f38530","_uuid":"58781c2a2022c5ac519705fb464e36bd6d33162f","trusted":false},"cell_type":"code","source":"data['safe_loans'].value_counts(normalize=True)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"b37d2287-328a-4969-b825-9369fb9ac416","_uuid":"d4db5078f21aa0b68287cc6687cfff62eee254cd"},"cell_type":"markdown","source":"#### 这是一个不平衡数据， 好的贷款远比坏的贷款要多. "},{"metadata":{"_cell_guid":"6c4a7676-83fb-4120-9edf-2ec3974dea12","_uuid":"4bead2914b6d9200e7f54c95f1dc4cfef247ec4e"},"cell_type":"markdown","source":"## 选取用于预测的特征"},{"metadata":{"_cell_guid":"5e5f7d04-32c0-4406-ba0e-36f36713c3ac","_uuid":"f16f2ce0e1278023c11814f30c2ef01cd02c0e16","trusted":false},"cell_type":"code","source":"cols = ['grade', 'term','home_ownership', 'emp_length']\ntarget = 'safe_loans'\n\ndata = data[cols + [target]]#TODO\ndata.head()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"3333c950-602b-44a8-89c1-2962409f723c","_uuid":"e19dfcb51e380d0585e372029a99c77e6bba59b3"},"cell_type":"markdown","source":"## 创建更为平衡的数据集  \n\n* 对占多数的标签进行下采样  \n* 注意有很多方法处理不平衡数据，下采样只是其中之一"},{"metadata":{"_cell_guid":"b7617a97-67d9-4beb-95c1-48b06e7c3876","_uuid":"51dca67fec51e2cc260c0ede9c8032544fda3195","trusted":false},"cell_type":"code","source":"data['safe_loans'].value_counts()\n","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"c06def41-d03e-4105-9b25-b6319b16c361","_uuid":"4e9e4076f543aefe1b7e86a93470d99205c3a71f","collapsed":true,"trusted":false},"cell_type":"code","source":"\n# use the percentage of bad and good loans to undersample the safe loans.\nbad_ones = data[data['safe_loans'] == -1]# TODO\nsafe_ones = data[data['safe_loans'] == +1]# TODO\npercentage = len(bad_ones)/len(safe_ones)#TODO\n\nrisky_loans = bad_ones\nsafe_loans = safe_ones.sample(frac=percentage, random_state=33)#TODO\n\n# combine two kinds of loans\ndata_set = pd.concat([risky_loans, safe_loans], axis=0)#TODO","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"fec84bdc-5b6f-431c-9629-d7deffa1cf72","_uuid":"4a62a030d4225aedddc2184edcbd52db832391ce"},"cell_type":"markdown","source":"Now, let's verify that the resulting percentage of safe and risky loans are each nearly 50%."},{"metadata":{"_cell_guid":"342bc506-1915-4ee1-a4ab-2c9b26b05eb8","_uuid":"4f4699a58c1a6105a68245db67c1f96c5e0245f0","trusted":false},"cell_type":"code","source":"data_set[target].value_counts(normalize=True)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"d39bcf69-6f27-4c6a-8a68-c8027e977d1a","_uuid":"39dffd3c49dea40545d1eccdf78060e4b47d3fbf"},"cell_type":"markdown","source":"## Preprocessing your features"},{"metadata":{"_cell_guid":"a68571a3-0684-40fa-b205-abdac5260d55","_uuid":"9d3fd72e15633094f06183aaa6bda157ca3d1303","collapsed":true,"trusted":false},"cell_type":"code","source":"def dummies(data, columns=['pclass','name_title','embarked', 'sex']):\n    for col in columns:\n        data[col] = data[col].apply(lambda x: str(x))\n        new_cols = [col + '_' + i for i in data[col].unique()]\n        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)[new_cols]], axis=1)\n        del data[col]\n    return data","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"a7bc7d38-62ed-41b6-87d1-70ed2f878ef1","_uuid":"07fc7fcfc42fd2332d846e323862823c29602047","trusted":false},"cell_type":"code","source":"#grade, home_ownership, target\ncols = ['grade', 'term','home_ownership', 'emp_length']\ndata_set = dummies(data_set, columns=cols)#TODO\ndata_set.head()","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"ea2ab39c-e904-4583-80f9-ecefbb111d1a","_uuid":"758594d8eb40c978ff84262708741ebd40fb56b6"},"cell_type":"markdown","source":"## 将数据分成训练集和测试集"},{"metadata":{"_cell_guid":"cafbe746-a07f-4d02-9c45-e0a0669d3b47","_uuid":"17675e6251b913d2d2ee0f3aebb51b4904619ad7"},"cell_type":"markdown","source":"重要的事情说三遍!!  \n\n**把你的爪子从TEST DATA上拿开!!**   \n**把你的爪子从TEST DATA上拿开!!**  \n**把你的爪子从TEST DATA上拿开!!**  \n"},{"metadata":{"_uuid":"fc5186a4fc66bf6833c019e2e24d9884f1392e23"},"cell_type":"markdown","source":"对于trainY的赋值为什么在课上的代码使用pd.DataFrame(train_data[target])的问题郭助教已经给出了解答:\n\ntrain_data[target] -> shape: (37040,) 得到pd.Series\n\npd.DataFrame(train_data[target]) -> shape: (37040,1) 得到pd.DataFrame\n\n两种皆可但是DataFrame操作上更加灵活，以下尝试了另几种得到Series和DaataFrame的不同做法"},{"metadata":{"trusted":false,"_uuid":"dea38286436641dbe52645e4ec304f15f21803ec"},"cell_type":"code","source":"#做切片时，使用.iloc[:,0]得到Series, .iloc[:,:1]得到DataFrame\ntrain_data, test_data = train_test_split(data_set,\n                                        test_size=0.2,\n                                        random_state=33)#TODO\ntrainX, trainY = train_data.iloc[:,1:], train_data.iloc[:,:1]#TODO\ntestX, testY = test_data.iloc[:,1:], test_data.iloc[:,:1]\nprint(trainX.shape, trainY.shape, testX.shape, testY.shape)","execution_count":31,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dcd53455317e5ee93a846f00c972a3b438ff929b"},"cell_type":"code","source":"#此处原理与上相同，不同点在于不是对原数据集做切片，而是对索引做切片\ntrain_data, test_data = train_test_split(data_set,\n                                        test_size=0.2,\n                                        random_state=33)#TODO\ntrainX, trainY = train_data[train_data.columns[1:]], train_data[train_data.columns[:1]]#TODO\ntestX, testY = test_data[test_data.columns[1:]], test_data[test_data.columns[:1]]\nprint(trainX.shape, trainY.shape, testX.shape, testY.shape)","execution_count":32,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e82ab906bfec303fd4865f56cbc62ff38f6808d7"},"cell_type":"code","source":"#尝试了以column_name做切片，原理同上\n#同时在取label时，用单括号[]索引得到Series,用双括号[[]]索引得到DataFrame\ntrain_data, test_data = train_test_split(data_set,\n                                        test_size=0.2,\n                                        random_state=33)#TODO\ntrainX, trainY = train_data.loc[:,'grade_C':], train_data[[target]]#TODO\ntestX, testY = test_data.loc[:,'grade_C':], test_data[[target]]\nprint(trainX.shape, trainY.shape, testX.shape, testY.shape)","execution_count":39,"outputs":[]},{"metadata":{"_cell_guid":"d76b802c-a9b3-4ca9-8acc-0617cbaf5596","_uuid":"8e90801bb1e53a778749e2f53b50a63682c51827"},"cell_type":"markdown","source":"## 建自己的决策树!  \n\n任务：  \n1 实现根据error来选择最佳划分特征的函数best_split()  \n2 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n3 实现树节点的类TreeNode  \n4 实现模型的类MyDecisionTree  "},{"metadata":{"_cell_guid":"95a2979c-6497-4719-ac64-6af9ddad3ae3","_uuid":"2d4997336da3d66e289652f9bb6a87544e77912c"},"cell_type":"markdown","source":"#### 任务1， 实现根据error来选择最佳划分特征的函数best_split()  \n约定树的左边对应target == 0， 树的右边对应target == 1"},{"metadata":{"_cell_guid":"096a9270-cf52-4674-aaf2-bb0b7deeddb5","_uuid":"d9b1496e0577033d6375cb0efff6caa72cc1e7c7","collapsed":true,"trusted":false},"cell_type":"code","source":"def count_errors(labels_in_node):\n    if len(labels_in_node) == 0:\n        return 0\n    \n    positive_ones = len(labels_in_node[labels_in_node == +1])#TODO\n    negative_ones = len(labels_in_node[labels_in_node == -1])#TODO\n    \n    return min(positive_ones, negative_ones)# TODO\n\n\ndef best_split(data, features, target):\n    # return the best feature\n    best_feature = None\n    best_error = 2.0 \n    num_data_points = float(len(data))  \n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split = data[data[feature]==0]# TODO\n        \n        # 右分支对应当前特征为1的数据点\n        right_split = data[data[feature]==1]#TODO\n        \n        # 计算左边分支里犯了多少错\n        left_misses = count_errors(left_split[target])#TODO            \n\n        # 计算右边分支里犯了多少错\n        right_misses = count_errors(right_split[target])#TODO\n            \n        # 计算当前划分之后的分类犯错率\n        error = (left_misses + right_misses) / num_data_points#TODO\n\n        # 更新应选特征和错误率，注意错误越低说明该特征越好\n        if error < best_error:\n            best_error = error#TODO\n            best_feature = feature#TODO\n    return best_feature","execution_count":40,"outputs":[]},{"metadata":{"_cell_guid":"c030bb31-2bec-40e2-a446-e1e06e72e53d","_uuid":"12daddfd025b340eedd431a0009d02cd2f4f9835"},"cell_type":"markdown","source":"#### 任务2， 实现根据entropy来选择最佳特征的函数best_split_entropy()  \n"},{"metadata":{"_cell_guid":"4393f84f-1f2b-4424-816b-12b46655745c","_uuid":"b6ad2f8e15bd3446781f4134d37868ed79a4771e","collapsed":true,"trusted":false},"cell_type":"code","source":"def entropy(labels_in_node):\n    # 二分类问题: 0 or 1\n    n = len(labels_in_node)\n    s1 = (labels_in_node==1).sum()\n    if s1 == 0 or s1 == n:\n        return 0\n    \n    p1 = float(s1) / n\n    p0 = 1 - p1\n    return -p0 * np.log2(p0) - p1 * np.log2(p1)\n\n\ndef best_split_entropy(data, features, target):\n    \n    best_feature = None\n    best_info_gain = float('-inf') \n    num_data_points = float(len(data))\n    # 计算划分之前数据集的整体熵值\n    entropy_original = entropy(data[target])#TODO\n\n    for feature in features:\n        \n        # 左分支对应当前特征为0的数据点\n        left_split = data[data[feature] == 0]#TODO\n        \n        # 右分支对应当前特征为1的数据点\n        right_split = data[data[feature] == 1]#TODO \n        \n        # 计算左边分支的熵值\n        left_entropy = entropy(left_split[target])#TODO           \n\n        # 计算右边分支的熵值\n        right_entropy = entropy(right_split[target])#TODO\n            \n        # 计算左边分支与右分支熵值的加权和（数据集划分后的熵值）\n        entropy_split = len(left_split)/num_data_points * left_entropy + len(right_split)/num_data_points * right_entropy#TODO\n        \n        # 计算划分前与划分后的熵值差得到信息增益\n        info_gain = entropy_original - entropy_split#TODO\n\n        # 更新最佳特征和对应的信息增益的值\n        if info_gain > best_info_gain:\n            best_info_gain = info_gain\n            best_feature = feature\n    return best_feature\n    ","execution_count":41,"outputs":[]},{"metadata":{"_cell_guid":"34d625a1-e5fd-42a8-9e27-fd5e329d937f","_uuid":"1054a895ce42c3a5b067a8eebfaa879934d72eff"},"cell_type":"markdown","source":"#### 任务3，实现树节点的类TreeNode，每个树节点应该包含如下信息:  \n\n   3.1 is_leaf: True/False  表示当前节点是否为叶子节点  \n   \n   3.2 prediction: 当前节点做全民公投的预测结果\n   \n   3.3 left: 左子树  \n   \n   3.4 right: 右子树 \n   \n   3.5 split_feature: 当前节点用来划分数据时所采用的特征"},{"metadata":{"_cell_guid":"bbb7c96b-3f4f-47ae-8647-4e506ce81c35","_uuid":"db3fd70be8a7e5e5faea5f931d0d7d595e2c58d0","collapsed":true,"trusted":false},"cell_type":"code","source":"class TreeNode:\n    def __init__(self, is_leaf, prediction, split_feature):\n        self.is_leaf = is_leaf\n        self.prediction = prediction\n        self.split_feature = split_feature\n        self.left = None\n        self.right = None\n    # TODO\n        \n        ","execution_count":42,"outputs":[]},{"metadata":{"_cell_guid":"aae5e29c-7313-4d04-bf54-54d6335d17bc","_uuid":"34826c20fad54475201c106fa83367e8c752eafc"},"cell_type":"markdown","source":"#### 任务4，实现模型的类MyDecisionTree， 实现如下主要函数  \n  \n  \n   4.1 fit(): 模型在训练集上的学习  \n   \n   4.2 predict(): 模型在数据集上的预测\n   \n   4.3 score(): 模型在测试集上的得分   \n   \n   \n   \n   \n   为了实现4.1 - 4.3的方法， 需要实现如下辅助函数  \n   4.4 create_tree(): 创建一棵树  \n   \n   4.5 create_leaf(): 创建叶子节点  \n   \n   4.6 predict_single_data(): 模型预测单个数据  \n   \n   4.7 count_leaves(): 统计模型的叶子数"},{"metadata":{"_cell_guid":"1297a729-41f3-4fb8-8bb4-4a236f5c1fa9","_uuid":"e7a852a0ccbc6f6d88cc6e3d1c9c9e038062a94f","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nfrom sklearn.metrics import accuracy_score\nclass MyDecisionTree(BaseEstimator):\n    \n    def __init__(self, max_depth, min_error):\n        self.max_depth = max_depth\n        self.min_error = min_error\n    \n    def fit(self, X, Y, data_weights = None):\n        \n        data_set = pd.concat([X, Y], axis=1)\n        features = X.columns#TODO\n        target = Y.columns[0]\n        self.root_node = self.create_tree(data_set, features, target, current_depth = 0, max_depth = self.max_depth, min_error = self.min_error)# TODO\n        \n        \n    def predict(self, X):\n        prediction = X.apply(lambda row: self.predict_single_data(self.root_node, row), axis=1)\n        return prediction\n        \n        \n    def score(self, testX, testY):\n        target = testY.columns[0]# TODO\n        result = self.predict(testX)\n        return accuracy_score(testY[target], result)\n    \n    \n    def create_tree(self, data, features, target, current_depth = 0, max_depth = 10, min_error=0):\n        \"\"\"\n        探索三种不同的终止划分数据集的条件  \n  \n        termination 1, 当错误率降到min_error以下, 终止划分并返回叶子节点  \n        termination 2, 当特征都用完了, 终止划分并返回叶子节点  \n        termination 3, 当树的深度等于最大max_depth时, 终止划分并返回叶子节点\n        \"\"\"\n        \n    \n        # 拷贝以下可用特征\n        remaining_features = features[:]#TODO\n\n        target_values = data[target]# TODO\n\n        # termination 1\n        if count_errors(target_values) <= min_error:\n            print(\"Termination 1 reached.\")     \n            return self.create_leaf(target_values)# TODO\n\n        # termination 2\n        if len(remaining_features) == 0:\n            print(\"Termination 2 reached.\")    \n            return self.create_leaf(target_values)# TODO    \n\n        # termination 3\n        if current_depth >= max_depth: \n            print(\"Termination 3 reached.\")\n            return self.create_leaf(target_values)# TODO\n\n\n\n        # 选出最佳当前划分特征\n        #split_feature = # TODO   #根据正确率划分\n        split_feature = best_split_entropy(data, features, target)# TODO  # 根据信息增益来划分\n\n        # 选出最佳特征后，该特征为0的数据分到左边，该特征为1的数据分到右边\n        left_split = data[data[split_feature] == 0]# TODO\n        right_split = data[data[split_feature] == 1]# TODO\n\n        # 剔除已经用过的特征\n        remaining_features = remaining_features.drop(split_feature)# TODO\n        print(\"Split on feature %s. (%s, %s)\" % (split_feature, str(len(left_split)), str(len(right_split))))\n\n        # 如果当前数据全部划分到了一边，直接创建叶子节点返回即可\n        if len(left_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(left_split[target])\n        if len(right_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(right_split[target])\n\n        # 递归上面的步骤\n        left_tree = self.create_tree(left_split, remaining_features, target, current_depth + 1, max_depth, min_error)# TODO     \n        right_tree = self.create_tree(right_split, remaining_features, target, current_depth + 1, max_depth, min_error)# TODO\n\n        #生成当前的树节点\n        result_node = TreeNode(False, None, split_feature)\n        result_node.left = left_tree\n        result_node.right = right_tree\n        return result_node    \n    \n    \n    \n    def create_leaf(self, target_values):\n        # 用于创建叶子的函数\n\n        # 初始化一个树节点\n        leaf = TreeNode(True, None, None)# TODO\n\n        # 统计当前数据集里标签为+1和-1的个数，较大的那个即为当前节点的预测结果\n        num_positive_ones = len(target_values[target_values == +1])#TODO\n        num_negative_ones = len(target_values[target_values == -1])# TODO\n\n        if num_positive_ones > num_negative_ones:\n            leaf.prediction = +1\n        else:\n            leaf.prediction = -1\n\n        # 返回叶子        \n        return leaf \n    \n    \n    \n    def predict_single_data(self, tree, x, annotate = False):   \n        # 如果已经是叶子节点直接返回叶子节点的预测结果\n        if tree.is_leaf:\n            if annotate: \n                print(\"leaf node, predicting %s\" % tree.prediction)\n            return tree.prediction# TODO \n        else:\n            # 查询当前节点用来划分数据集的特征\n            split_feature_value = x[tree.split_feature]# TODO\n\n            if annotate: \n                print(\"Split on %s = %s\" % (tree.split_feature, split_feature_value))\n            if split_feature_value == 0:\n                #如果数据在该特征上的值为0，交给左子树来预测\n                return self.predict_single_data(tree.left, x, annotate)# TODO\n            else:\n                #如果数据在该特征上的值为1，交给右子树来预测\n                return self.predict_single_data(tree.right, x, annotate)# TODO    \n    \n    def count_leaves(self):\n        return self.count_leaves_helper(self.root_node)\n    \n    def count_leaves_helper(self, tree):\n        if tree.is_leaf:\n            return 1\n        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)\n        # TODO\n    \n","execution_count":69,"outputs":[]},{"metadata":{"_cell_guid":"e09cb420-2ee9-4c63-8c73-8d9d5db17b8d","_uuid":"a5418df4a9ab2493442a89f7dc0e374328f39780","collapsed":true,"trusted":false},"cell_type":"code","source":"m = MyDecisionTree(max_depth = 10, min_error = 1e-15)","execution_count":70,"outputs":[]},{"metadata":{"_cell_guid":"075a6909-945a-4a78-8856-0af85b091ce9","_uuid":"eba3f893824748c2e9dda841bf8586248d09d875","trusted":false,"scrolled":true},"cell_type":"code","source":"m.fit(trainX, trainY)","execution_count":71,"outputs":[]},{"metadata":{"_cell_guid":"023cb42d-047a-4d64-9c4b-437fe1b11528","_uuid":"35519847a084d8cf97518a3dc174cd14f518053b","trusted":false},"cell_type":"code","source":"m.score(testX, testY)","execution_count":72,"outputs":[]},{"metadata":{"_cell_guid":"42a90f3c-c99b-4108-9ae3-646e1c0e0133","_uuid":"cc3ee6840fd3e020e419251e2f509d211453f0b8","collapsed":true},"cell_type":"markdown","source":"### 决策树复杂度的探讨"},{"metadata":{"_cell_guid":"c14d4418-f133-490f-a5b2-e335e3872d22","_uuid":"3e6fb6cad69a098932956cfe578973619497348f","trusted":false},"cell_type":"code","source":"m.count_leaves()","execution_count":73,"outputs":[]},{"metadata":{"_cell_guid":"ee230737-2aef-403e-a7a8-582ae6985630","_uuid":"38b8ae1cf325676403d32b93e575a5d6b2543413"},"cell_type":"markdown","source":"#### 探索不同树深度对决策树的影响  \n\n1 max_depth = 3  \n2 max_depth = 7  \n3 max_depth = 15\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9037b94822e9d47b28ba33227e4fb395bbd492ec"},"cell_type":"code","source":"model_1 = MyDecisionTree(max_depth=3, min_error=1e-15)# TODO\nmodel_2 = MyDecisionTree(max_depth=7, min_error=1e-15)# TODO\nmodel_3 = MyDecisionTree(max_depth=15, min_error=1e-15)# TODO","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"579b24a0-3038-4468-9236-5318d1f0b407","_uuid":"c3f66cc76a7ecd6417031d29991522ade474a9af","trusted":false,"scrolled":true},"cell_type":"code","source":"model_1.fit(trainX, trainY)\nmodel_2.fit(trainX, trainY)\nmodel_3.fit(trainX, trainY)","execution_count":75,"outputs":[]},{"metadata":{"_cell_guid":"b92be199-47ef-4090-8403-7fed771ca8cc","_uuid":"cb7d2f6c6b323ed7ad253b86e1249b75e39fec9d","trusted":false},"cell_type":"code","source":"print(\"model_1 training accuracy :\", model_1.score(trainX, trainY))\nprint(\"model_2 training accuracy :\", model_2.score(trainX, trainY))\nprint(\"model_3 training accuracy :\", model_3.score(trainX, trainY))","execution_count":76,"outputs":[]},{"metadata":{"_cell_guid":"9321d9d7-8dac-4394-8e3b-e8bdbc66b1a0","_uuid":"686b7a26500e842346fb84b1a43a929958203fc0","trusted":false},"cell_type":"code","source":"print(\"model_1 testing accuracy :\", model_1.score(testX, testY))\nprint(\"model_2 testing accuracy :\", model_2.score(testX, testY))\nprint(\"model_3 testing accuracy :\", model_3.score(testX, testY))","execution_count":77,"outputs":[]},{"metadata":{"_cell_guid":"6d555cbf-90a8-4be2-915d-71c4b28bcc14","_uuid":"d99262b1aaf612f79dafc09f099540452dd93fa9","trusted":false},"cell_type":"code","source":"print(\"model_1 complexity is: \", model_1.count_leaves())\nprint(\"model_2 complexity is: \", model_2.count_leaves())\nprint(\"model_3 complexity is: \", model_3.count_leaves())","execution_count":78,"outputs":[]},{"metadata":{"_cell_guid":"1647744f-bb9b-455c-893a-d26f64ef8c73","_uuid":"f7836cc1a458bf99741380ee769e2d64a581afad","collapsed":true},"cell_type":"markdown","source":"根据正确率划分"},{"metadata":{"trusted":true,"_uuid":"280315df2684060b688495ff82c49ee0202cd4e5"},"cell_type":"code","source":"class MyDecisionTree1(BaseEstimator):\n    \n    def __init__(self, max_depth, min_error):\n        self.max_depth = max_depth\n        self.min_error = min_error\n    \n    def fit(self, X, Y, data_weights = None):\n        \n        data_set = pd.concat([X, Y], axis=1)\n        features = X.columns#TODO\n        target = Y.columns[0]\n        self.root_node = self.create_tree(data_set, features, target, current_depth = 0, max_depth = self.max_depth, min_error = self.min_error)# TODO\n        \n        \n    def predict(self, X):\n        prediction = X.apply(lambda row: self.predict_single_data(self.root_node, row), axis=1)\n        return prediction\n        \n        \n    def score(self, testX, testY):\n        target = testY.columns[0]# TODO\n        result = self.predict(testX)\n        return accuracy_score(testY[target], result)\n    \n    \n    def create_tree(self, data, features, target, current_depth = 0, max_depth = 10, min_error=0):\n        \"\"\"\n        探索三种不同的终止划分数据集的条件  \n  \n        termination 1, 当错误率降到min_error以下, 终止划分并返回叶子节点  \n        termination 2, 当特征都用完了, 终止划分并返回叶子节点  \n        termination 3, 当树的深度等于最大max_depth时, 终止划分并返回叶子节点\n        \"\"\"\n        \n    \n        # 拷贝以下可用特征\n        remaining_features = features[:]#TODO\n\n        target_values = data[target]# TODO\n\n        # termination 1\n        if count_errors(target_values) <= min_error:\n            print(\"Termination 1 reached.\")     \n            return self.create_leaf(target_values)# TODO\n\n        # termination 2\n        if len(remaining_features) == 0:\n            print(\"Termination 2 reached.\")    \n            return self.create_leaf(target_values)# TODO    \n\n        # termination 3\n        if current_depth >= max_depth: \n            print(\"Termination 3 reached.\")\n            return self.create_leaf(target_values)# TODO\n\n\n\n        # 选出最佳当前划分特征\n        split_feature = best_split(data, features, target)# TODO   #根据正确率划分\n        #split_feature = best_split_entropy(data, features, target)# TODO  # 根据信息增益来划分\n\n        # 选出最佳特征后，该特征为0的数据分到左边，该特征为1的数据分到右边\n        left_split = data[data[split_feature] == 0]# TODO\n        right_split = data[data[split_feature] == 1]# TODO\n\n        # 剔除已经用过的特征\n        remaining_features = remaining_features.drop(split_feature)# TODO\n        print(\"Split on feature %s. (%s, %s)\" % (split_feature, str(len(left_split)), str(len(right_split))))\n\n        # 如果当前数据全部划分到了一边，直接创建叶子节点返回即可\n        if len(left_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(left_split[target])\n        if len(right_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(right_split[target])\n\n        # 递归上面的步骤\n        left_tree = self.create_tree(left_split, remaining_features, target, current_depth + 1, max_depth, min_error)# TODO     \n        right_tree = self.create_tree(right_split, remaining_features, target, current_depth + 1, max_depth, min_error)# TODO\n\n        #生成当前的树节点\n        result_node = TreeNode(False, None, split_feature)\n        result_node.left = left_tree\n        result_node.right = right_tree\n        return result_node\n    \n    \n    \n    def create_leaf(self, target_values):\n        # 用于创建叶子的函数\n\n        # 初始化一个树节点\n        leaf = TreeNode(True, None, None)# TODO\n\n        # 统计当前数据集里标签为+1和-1的个数，较大的那个即为当前节点的预测结果\n        num_positive_ones = len(target_values[target_values == +1])#TODO\n        num_negative_ones = len(target_values[target_values == -1])# TODO\n\n        if num_positive_ones > num_negative_ones:\n            leaf.prediction = +1\n        else:\n            leaf.prediction = -1\n\n        # 返回叶子        \n        return leaf \n    \n    \n    \n    def predict_single_data(self, tree, x, annotate = False):   \n        # 如果已经是叶子节点直接返回叶子节点的预测结果\n        if tree.is_leaf:\n            if annotate: \n                print(\"leaf node, predicting %s\" % tree.prediction)\n            return tree.prediction# TODO \n        else:\n            # 查询当前节点用来划分数据集的特征\n            split_feature_value = x[tree.split_feature]# TODO\n\n            if annotate: \n                print(\"Split on %s = %s\" % (tree.split_feature, split_feature_value))\n            if split_feature_value == 0:\n                #如果数据在该特征上的值为0，交给左子树来预测\n                return self.predict_single_data(tree.left, x, annotate)# TODO\n            else:\n                #如果数据在该特征上的值为1，交给右子树来预测\n                return self.predict_single_data(tree.right, x, annotate)# TODO    \n\n    def count_leaves(self):\n        return self.count_leaves_helper(self.root_node)\n\n    def count_leaves_helper(self, tree):\n        if tree.is_leaf:\n            return 1\n        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)\n    \n","execution_count":83,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"394f8787caf6d216b9f303f4333176ead186a403"},"cell_type":"code","source":"m1 = MyDecisionTree1(max_depth = 10, min_error = 1e-15)","execution_count":84,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce824ca249cabe1631fd4394a677ead3cb8d597f"},"cell_type":"code","source":"m1.fit(trainX, trainY)","execution_count":85,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"610181c6648b62ed4d1921ffd4470108ea262ae9"},"cell_type":"code","source":"m1.score(testX, testY)","execution_count":86,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d422551d2637107e94d7f3998e54678dd156eb1e"},"cell_type":"code","source":"m1.count_leaves()","execution_count":90,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1c9a7fe5c4e5ca3c291c3f40bc80928eaa21d743"},"cell_type":"code","source":"model1_1 = MyDecisionTree1(max_depth=3, min_error=1e-15)# TODO\nmodel1_2 = MyDecisionTree1(max_depth=7, min_error=1e-15)# TODO\nmodel1_3 = MyDecisionTree1(max_depth=15, min_error=1e-15)# TODO","execution_count":91,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ab8a24d4253081998a8a97bea2b6e918da60cb34"},"cell_type":"code","source":"model1_1.fit(trainX, trainY)\nmodel1_2.fit(trainX, trainY)\nmodel1_3.fit(trainX, trainY)","execution_count":92,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45171735d38163689b08270834a5f3abb81d266f"},"cell_type":"code","source":"print(\"model1_1 training accuracy :\", model1_1.score(trainX, trainY))\nprint(\"model1_2 training accuracy :\", model1_2.score(trainX, trainY))\nprint(\"model1_3 training accuracy :\", model1_3.score(trainX, trainY))","execution_count":94,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fae63d8304841d26de3ec17e68a2249d848ae910"},"cell_type":"code","source":"print(\"model1_1 testing accuracy :\", model1_1.score(testX, testY))\nprint(\"model1_2 testing accuracy :\", model1_2.score(testX, testY))\nprint(\"model1_3 testing accuracy :\", model1_3.score(testX, testY))","execution_count":95,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd70364f6e95d0abf5667b3f5caed731f019e162"},"cell_type":"code","source":"print(\"model1_1 complexity is: \", model1_1.count_leaves())\nprint(\"model1_2 complexity is: \", model1_2.count_leaves())\nprint(\"model1_3 complexity is: \", model1_3.count_leaves())","execution_count":96,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}