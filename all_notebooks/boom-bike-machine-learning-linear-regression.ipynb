{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <font color=\"blue\">Importing packages</font>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Supress Warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:31.836409Z","iopub.execute_input":"2021-06-10T15:50:31.836898Z","iopub.status.idle":"2021-06-10T15:50:31.841314Z","shell.execute_reply.started":"2021-06-10T15:50:31.836867Z","shell.execute_reply":"2021-06-10T15:50:31.840304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## increasing the column view\npd.set_option('display.max_column',999)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:31.846109Z","iopub.execute_input":"2021-06-10T15:50:31.846452Z","iopub.status.idle":"2021-06-10T15:50:31.856461Z","shell.execute_reply.started":"2021-06-10T15:50:31.846421Z","shell.execute_reply":"2021-06-10T15:50:31.855456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 1: Reading and Understanding the Data</font>\n\nLet's start with the following steps:\n\n1. Importing data using the pandas library\n2. Understanding the structure of the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/boom-bike-dataset/day.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:32.021797Z","iopub.execute_input":"2021-06-10T15:50:32.022136Z","iopub.status.idle":"2021-06-10T15:50:32.048252Z","shell.execute_reply.started":"2021-06-10T15:50:32.022106Z","shell.execute_reply":"2021-06-10T15:50:32.047247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:32.050155Z","iopub.execute_input":"2021-06-10T15:50:32.05047Z","iopub.status.idle":"2021-06-10T15:50:32.071027Z","shell.execute_reply.started":"2021-06-10T15:50:32.050439Z","shell.execute_reply":"2021-06-10T15:50:32.069502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:32.073015Z","iopub.execute_input":"2021-06-10T15:50:32.074099Z","iopub.status.idle":"2021-06-10T15:50:32.140354Z","shell.execute_reply.started":"2021-06-10T15:50:32.073886Z","shell.execute_reply":"2021-06-10T15:50:32.139296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:32.141741Z","iopub.execute_input":"2021-06-10T15:50:32.142344Z","iopub.status.idle":"2021-06-10T15:50:32.14862Z","shell.execute_reply.started":"2021-06-10T15:50:32.14228Z","shell.execute_reply":"2021-06-10T15:50:32.147842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### droppping instant & dteday column as its not required for model analysis\ndf.drop(columns=[\"instant\",\"dteday\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:32.151516Z","iopub.execute_input":"2021-06-10T15:50:32.152173Z","iopub.status.idle":"2021-06-10T15:50:32.160796Z","shell.execute_reply.started":"2021-06-10T15:50:32.15213Z","shell.execute_reply":"2021-06-10T15:50:32.15978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### checking null count of each column\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:32.163068Z","iopub.execute_input":"2021-06-10T15:50:32.163543Z","iopub.status.idle":"2021-06-10T15:50:32.177873Z","shell.execute_reply.started":"2021-06-10T15:50:32.163497Z","shell.execute_reply":"2021-06-10T15:50:32.176861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 2: Visualising the Data</font>\n\nLet's now visualise our data using seaborn. We'll first make scatterplot of all the contineous variables present to visualise which variables are most correlated to `cnt`.","metadata":{}},{"cell_type":"code","source":"### scatter plot for contineous vars\ncatvars = ['temp', 'atemp', 'hum','windspeed','casual','registered']\nplt.figure(figsize=(20,20))\nfor i in range(1,len(catvars)+1):\n    plt.subplot(3,2,i)\n    sns.scatterplot(data=df , y=\"cnt\" , x=catvars[i-1])\n    \n\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:32.179334Z","iopub.execute_input":"2021-06-10T15:50:32.179844Z","iopub.status.idle":"2021-06-10T15:50:33.14759Z","shell.execute_reply.started":"2021-06-10T15:50:32.179806Z","shell.execute_reply":"2021-06-10T15:50:33.14613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### box plot for categorical vars\ncatvars = [\"season\",\"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weathersit\"]\nplt.figure(figsize=(20,20))\nfor i in range(1,len(catvars)+1):\n    plt.subplot(4,2,i)\n    sns.boxplot(data=df , y=\"cnt\" , x=catvars[i-1])\n    \n\nplt.show()    \n    \n    \n# season : season (1:spring, 2:summer, 3:fall, 4:winter)\n# yr : year (0: 2018, 1:2019)\n# mnth : month ( 1 to 12)\n# holiday : weather day is a holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n# weekday : day of the week\n# workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n# weathersit : \n#  1: Clear, Few clouds, Partly cloudy, Partly cloudy\n#  2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n#  3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n#  4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog    \n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:33.149456Z","iopub.execute_input":"2021-06-10T15:50:33.149837Z","iopub.status.idle":"2021-06-10T15:50:34.2882Z","shell.execute_reply.started":"2021-06-10T15:50:33.149795Z","shell.execute_reply":"2021-06-10T15:50:34.286894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 3: Data Preparation</font>","metadata":{}},{"cell_type":"markdown","source":"1. We need to convert season , mnth , weekday , weathersit into dummy columns as there are categorical variables\n\n2. As yr , holiday and workingday are binary categorical column no need to convert those","metadata":{}},{"cell_type":"code","source":"## create Dummy for season\n## 1. 000 - spring\n## 2. 100 - summer\n## 3. 010 - fall\n## 4. 001 - winter\n\nseason = pd.get_dummies(df.season, drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.289762Z","iopub.execute_input":"2021-06-10T15:50:34.290172Z","iopub.status.idle":"2021-06-10T15:50:34.297151Z","shell.execute_reply.started":"2021-06-10T15:50:34.290129Z","shell.execute_reply":"2021-06-10T15:50:34.296139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convert column names \nseason.columns = [\"Summer\",\"Fall\",\"Winter\"]\nseason.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.298487Z","iopub.execute_input":"2021-06-10T15:50:34.298879Z","iopub.status.idle":"2021-06-10T15:50:34.313663Z","shell.execute_reply.started":"2021-06-10T15:50:34.298844Z","shell.execute_reply":"2021-06-10T15:50:34.312423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging the columns into original dataframe and droppping season column\ndf = pd.concat([df,season] , axis=1)\n\ndf.drop(columns=\"season\",inplace=True)\n        \ndf.head() ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.315256Z","iopub.execute_input":"2021-06-10T15:50:34.315765Z","iopub.status.idle":"2021-06-10T15:50:34.338861Z","shell.execute_reply.started":"2021-06-10T15:50:34.315723Z","shell.execute_reply":"2021-06-10T15:50:34.337456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## create Dummy for weekdays\n## 0. 000000 - Monday\n## 1. 100000 - Tuesday\n## 2. 010000 - Wednesday\n## 3. 001000 - Thursday\n## 4. 000100 - Friday\n## 5. 000010 - Saturday\n## 6. 000001 - Sunday\n\nweekday = pd.get_dummies(df.weekday , drop_first=True) ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.340448Z","iopub.execute_input":"2021-06-10T15:50:34.340904Z","iopub.status.idle":"2021-06-10T15:50:34.349583Z","shell.execute_reply.started":"2021-06-10T15:50:34.340872Z","shell.execute_reply":"2021-06-10T15:50:34.348456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convert column names \nweekday.columns = [\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\nweekday.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.350902Z","iopub.execute_input":"2021-06-10T15:50:34.351164Z","iopub.status.idle":"2021-06-10T15:50:34.369169Z","shell.execute_reply.started":"2021-06-10T15:50:34.35114Z","shell.execute_reply":"2021-06-10T15:50:34.368033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging the columns into original dataframe and droppping weekday column\ndf = pd.concat([df,weekday] , axis=1)\n\ndf.drop(columns=\"weekday\",inplace=True)\n        \ndf.head() ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.370536Z","iopub.execute_input":"2021-06-10T15:50:34.370827Z","iopub.status.idle":"2021-06-10T15:50:34.398048Z","shell.execute_reply.started":"2021-06-10T15:50:34.370799Z","shell.execute_reply":"2021-06-10T15:50:34.397223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## create Dummy for weathersit\n## 1. 00 - Clear, Few clouds, Partly cloudy, Partly cloudy\n## 2. 10 - Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n## 3. 01 - Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n## 4. N.A - Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n##(Not Considered as there is no data for this as seen in the box plot)\n\nweathersit = pd.get_dummies(df.weathersit , drop_first=True) ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.399575Z","iopub.execute_input":"2021-06-10T15:50:34.40003Z","iopub.status.idle":"2021-06-10T15:50:34.405726Z","shell.execute_reply.started":"2021-06-10T15:50:34.399998Z","shell.execute_reply":"2021-06-10T15:50:34.404765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convert column names \nweathersit.columns = [\"Mist\",\"LightSnow\"]\nweathersit.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.407175Z","iopub.execute_input":"2021-06-10T15:50:34.407585Z","iopub.status.idle":"2021-06-10T15:50:34.534279Z","shell.execute_reply.started":"2021-06-10T15:50:34.407538Z","shell.execute_reply":"2021-06-10T15:50:34.53308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging the columns into original dataframe and droppping weathersit column\ndf = pd.concat([df,weathersit] , axis=1)\n\ndf.drop(columns=\"weathersit\",inplace=True)\n        \ndf.head() ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.535602Z","iopub.execute_input":"2021-06-10T15:50:34.535912Z","iopub.status.idle":"2021-06-10T15:50:34.563202Z","shell.execute_reply.started":"2021-06-10T15:50:34.535883Z","shell.execute_reply":"2021-06-10T15:50:34.56221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## create Dummy for mnth\n## 1. 00000000000 - Jan\n## 2. 10000000000 - Feb\n## 3. 01000000000 - Mar\n## 4. 00100000000 - Apr\n## 5. 00010000000 - May\n## 6. 00001000000 - June\n## 7. 00000100000 - July\n## 8. 00000010000 - Aug\n## 9. 00000001000 - Sep\n## 10. 00000000100 - Oct\n## 11. 00000000010 - Nov\n## 12. 00000000001 - Dec\n\nmnth = pd.get_dummies(df.mnth , drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.564446Z","iopub.execute_input":"2021-06-10T15:50:34.564714Z","iopub.status.idle":"2021-06-10T15:50:34.569607Z","shell.execute_reply.started":"2021-06-10T15:50:34.564689Z","shell.execute_reply":"2021-06-10T15:50:34.568891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convert column names \nmnth.columns = [\"Feb\",\"Mar\",\"Apr\",\"May\",\"June\",\"July\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\nmnth.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.570599Z","iopub.execute_input":"2021-06-10T15:50:34.570978Z","iopub.status.idle":"2021-06-10T15:50:34.592543Z","shell.execute_reply.started":"2021-06-10T15:50:34.570942Z","shell.execute_reply":"2021-06-10T15:50:34.591622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging the columns into original dataframe and droppping mnth column\ndf = pd.concat([df,mnth] , axis=1)\n\ndf.drop(columns=\"mnth\",inplace=True)\n        \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.593698Z","iopub.execute_input":"2021-06-10T15:50:34.59397Z","iopub.status.idle":"2021-06-10T15:50:34.629398Z","shell.execute_reply.started":"2021-06-10T15:50:34.593942Z","shell.execute_reply":"2021-06-10T15:50:34.628179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## As yr is already a binary categorical column , just converting the column name to the year will be helpful\ndf = df.rename(columns={\"yr\":\"2019\"})\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.630996Z","iopub.execute_input":"2021-06-10T15:50:34.631416Z","iopub.status.idle":"2021-06-10T15:50:34.658943Z","shell.execute_reply.started":"2021-06-10T15:50:34.631369Z","shell.execute_reply":"2021-06-10T15:50:34.657721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 4: Splitting the Data into Training and Testing Sets</font>\n\nAs you know, the first basic step for regression is performing a train-test split.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.660293Z","iopub.execute_input":"2021-06-10T15:50:34.66067Z","iopub.status.idle":"2021-06-10T15:50:34.841924Z","shell.execute_reply.started":"2021-06-10T15:50:34.66064Z","shell.execute_reply":"2021-06-10T15:50:34.841114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train,df_test = train_test_split(df , train_size=0.7 ,test_size=0.3, random_state=100)\nprint(df_train.shape)\nprint(df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.846639Z","iopub.execute_input":"2021-06-10T15:50:34.847164Z","iopub.status.idle":"2021-06-10T15:50:34.854855Z","shell.execute_reply.started":"2021-06-10T15:50:34.847131Z","shell.execute_reply":"2021-06-10T15:50:34.854096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 5: Rescaling the Features </font>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.856613Z","iopub.execute_input":"2021-06-10T15:50:34.857113Z","iopub.status.idle":"2021-06-10T15:50:34.864765Z","shell.execute_reply.started":"2021-06-10T15:50:34.857079Z","shell.execute_reply":"2021-06-10T15:50:34.863944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### rescaling temp , atemp , hum , windspeed , casual , registered , cnt\nscale_feature = [\"temp\" , \"atemp\" , \"hum\" , \"windspeed\" , \"casual\" , \"registered\",\"cnt\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.866066Z","iopub.execute_input":"2021-06-10T15:50:34.866458Z","iopub.status.idle":"2021-06-10T15:50:34.875682Z","shell.execute_reply.started":"2021-06-10T15:50:34.86643Z","shell.execute_reply":"2021-06-10T15:50:34.874925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.876768Z","iopub.execute_input":"2021-06-10T15:50:34.877189Z","iopub.status.idle":"2021-06-10T15:50:34.8874Z","shell.execute_reply.started":"2021-06-10T15:50:34.877161Z","shell.execute_reply":"2021-06-10T15:50:34.886382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[scale_feature] = scaler.fit_transform(df_train[scale_feature])\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.888575Z","iopub.execute_input":"2021-06-10T15:50:34.888953Z","iopub.status.idle":"2021-06-10T15:50:34.93084Z","shell.execute_reply.started":"2021-06-10T15:50:34.888918Z","shell.execute_reply":"2021-06-10T15:50:34.929833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### checking the impact of scaling on the train set\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:34.932063Z","iopub.execute_input":"2021-06-10T15:50:34.932348Z","iopub.status.idle":"2021-06-10T15:50:35.038623Z","shell.execute_reply.started":"2021-06-10T15:50:34.932306Z","shell.execute_reply":"2021-06-10T15:50:35.037448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 6: Checking correlation coefficients in the Training Set </font>","metadata":{}},{"cell_type":"code","source":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (30, 20))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:35.039806Z","iopub.execute_input":"2021-06-10T15:50:35.040181Z","iopub.status.idle":"2021-06-10T15:50:39.697364Z","shell.execute_reply.started":"2021-06-10T15:50:35.040139Z","shell.execute_reply":"2021-06-10T15:50:39.69626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 7: Dividing into X and Y sets for the model building</font>","metadata":{}},{"cell_type":"code","source":"y_train = df_train.pop('cnt')\nX_train = df_train","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:39.69852Z","iopub.execute_input":"2021-06-10T15:50:39.698799Z","iopub.status.idle":"2021-06-10T15:50:39.70484Z","shell.execute_reply.started":"2021-06-10T15:50:39.698772Z","shell.execute_reply":"2021-06-10T15:50:39.703669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 8: Building a linear model</font>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:39.706377Z","iopub.execute_input":"2021-06-10T15:50:39.706764Z","iopub.status.idle":"2021-06-10T15:50:39.861741Z","shell.execute_reply.started":"2021-06-10T15:50:39.706732Z","shell.execute_reply":"2021-06-10T15:50:39.860873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(X_train,y_train)\n### selecting top 20 features as per idea from heatmap above\nrfe = RFE(lm,20)\nrfe = rfe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:39.863161Z","iopub.execute_input":"2021-06-10T15:50:39.863563Z","iopub.status.idle":"2021-06-10T15:50:39.918404Z","shell.execute_reply.started":"2021-06-10T15:50:39.863523Z","shell.execute_reply":"2021-06-10T15:50:39.917208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### checking the list of features as er RFE\nlist(zip(X_train.columns , rfe.support_,rfe.ranking_))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:39.919788Z","iopub.execute_input":"2021-06-10T15:50:39.920418Z","iopub.status.idle":"2021-06-10T15:50:39.932203Z","shell.execute_reply.started":"2021-06-10T15:50:39.920375Z","shell.execute_reply":"2021-06-10T15:50:39.930907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### selecing the top RFE columns\ntopRFEcolumns = X_train.columns[rfe.support_]\ntopRFEcolumns","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:39.934142Z","iopub.execute_input":"2021-06-10T15:50:39.934907Z","iopub.status.idle":"2021-06-10T15:50:39.944664Z","shell.execute_reply.started":"2021-06-10T15:50:39.934863Z","shell.execute_reply":"2021-06-10T15:50:39.943682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### creating the linear model with these RFE selected features\n\nimport statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:39.946799Z","iopub.execute_input":"2021-06-10T15:50:39.947533Z","iopub.status.idle":"2021-06-10T15:50:40.694212Z","shell.execute_reply.started":"2021-06-10T15:50:39.947489Z","shell.execute_reply":"2021-06-10T15:50:40.693438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[topRFEcolumns])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:40.695647Z","iopub.execute_input":"2021-06-10T15:50:40.696061Z","iopub.status.idle":"2021-06-10T15:50:40.707913Z","shell.execute_reply.started":"2021-06-10T15:50:40.696019Z","shell.execute_reply":"2021-06-10T15:50:40.706904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:40.709094Z","iopub.execute_input":"2021-06-10T15:50:40.70938Z","iopub.status.idle":"2021-06-10T15:50:40.721399Z","shell.execute_reply.started":"2021-06-10T15:50:40.709353Z","shell.execute_reply":"2021-06-10T15:50:40.720006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### checking the sumary of the Linear Model\nlm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:40.723099Z","iopub.execute_input":"2021-06-10T15:50:40.72367Z","iopub.status.idle":"2021-06-10T15:50:40.780804Z","shell.execute_reply.started":"2021-06-10T15:50:40.723625Z","shell.execute_reply":"2021-06-10T15:50:40.779781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## creating function to calculate VIF\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef showVIF(data,cols):\n    vif = pd.DataFrame()\n    vif['Features'] = cols\n    vif['VIF'] = [variance_inflation_factor(data[cols].values , i) for i in range(data[cols].shape[1])]\n    vif['VIF'] = round(vif['VIF'] ,2)\n    vif = vif.sort_values(by =\"VIF\" , ascending=False)\n    return vif","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:40.782474Z","iopub.execute_input":"2021-06-10T15:50:40.782982Z","iopub.status.idle":"2021-06-10T15:50:40.793901Z","shell.execute_reply.started":"2021-06-10T15:50:40.782937Z","shell.execute_reply":"2021-06-10T15:50:40.792837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## show VIF\nprint(showVIF(X_train_sm,topRFEcolumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:40.795611Z","iopub.execute_input":"2021-06-10T15:50:40.796278Z","iopub.status.idle":"2021-06-10T15:50:40.887848Z","shell.execute_reply.started":"2021-06-10T15:50:40.796232Z","shell.execute_reply":"2021-06-10T15:50:40.886822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column temp thats having very high VIF from the model & then rebuild the Model","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019','atemp', 'hum', 'windspeed', 'casual', 'registered',\n       'Fall', 'Winter', 'Wednesday', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Feb', 'Mar', 'Apr', 'July', 'Aug', 'Dec']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:40.889524Z","iopub.execute_input":"2021-06-10T15:50:40.890184Z","iopub.status.idle":"2021-06-10T15:50:41.021553Z","shell.execute_reply.started":"2021-06-10T15:50:40.89014Z","shell.execute_reply":"2021-06-10T15:50:41.020483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column registered thats having very high VIF from the model & then rebuild the Model","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019','atemp', 'hum', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Wednesday', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Feb', 'Mar', 'Apr', 'July', 'Aug', 'Dec']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.023217Z","iopub.execute_input":"2021-06-10T15:50:41.023895Z","iopub.status.idle":"2021-06-10T15:50:41.151125Z","shell.execute_reply.started":"2021-06-10T15:50:41.023853Z","shell.execute_reply":"2021-06-10T15:50:41.150055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column Dec thats having very high P-Value from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019','atemp', 'hum', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Wednesday', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Feb', 'Mar', 'Apr', 'July', 'Aug']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.152865Z","iopub.execute_input":"2021-06-10T15:50:41.153556Z","iopub.status.idle":"2021-06-10T15:50:41.269729Z","shell.execute_reply.started":"2021-06-10T15:50:41.153511Z","shell.execute_reply":"2021-06-10T15:50:41.268722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column Mar thats having very high P-Value from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019','atemp', 'hum', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Wednesday', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Feb','Apr', 'July', 'Aug']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.271529Z","iopub.execute_input":"2021-06-10T15:50:41.272189Z","iopub.status.idle":"2021-06-10T15:50:41.382777Z","shell.execute_reply.started":"2021-06-10T15:50:41.272144Z","shell.execute_reply":"2021-06-10T15:50:41.381747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column Feb thats having very high P-Value from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019','atemp', 'hum', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Wednesday', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Apr', 'July', 'Aug']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.384462Z","iopub.execute_input":"2021-06-10T15:50:41.385126Z","iopub.status.idle":"2021-06-10T15:50:41.452272Z","shell.execute_reply.started":"2021-06-10T15:50:41.385081Z","shell.execute_reply":"2021-06-10T15:50:41.450888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column Wednesday thats having very high P-Value from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019','atemp', 'hum', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Apr', 'July', 'Aug']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.45361Z","iopub.execute_input":"2021-06-10T15:50:41.453915Z","iopub.status.idle":"2021-06-10T15:50:41.519038Z","shell.execute_reply.started":"2021-06-10T15:50:41.453888Z","shell.execute_reply":"2021-06-10T15:50:41.518003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column atemp thats having very high VIF from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019','hum', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Apr', 'July', 'Aug']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.520288Z","iopub.execute_input":"2021-06-10T15:50:41.520592Z","iopub.status.idle":"2021-06-10T15:50:41.591153Z","shell.execute_reply.started":"2021-06-10T15:50:41.520564Z","shell.execute_reply":"2021-06-10T15:50:41.59044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column hum thats having very high p-Value & VIF from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Apr', 'July', 'Aug']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.592149Z","iopub.execute_input":"2021-06-10T15:50:41.592563Z","iopub.status.idle":"2021-06-10T15:50:41.645814Z","shell.execute_reply.started":"2021-06-10T15:50:41.592533Z","shell.execute_reply":"2021-06-10T15:50:41.644977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-  ### We will remove the column Aug thats having very high P-Value from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Apr', 'July']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.64681Z","iopub.execute_input":"2021-06-10T15:50:41.647204Z","iopub.status.idle":"2021-06-10T15:50:41.697994Z","shell.execute_reply.started":"2021-06-10T15:50:41.647175Z","shell.execute_reply":"2021-06-10T15:50:41.697271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"-  ### We will remove the column July thats having very high P-Value from the model & then rebuild the Model\n","metadata":{}},{"cell_type":"code","source":"## selecting new list of feature columns\nFeatureColumns = ['2019', 'windspeed', 'casual',\n       'Fall', 'Winter', 'Friday', 'Saturday', 'Mist',\n       'LightSnow', 'Apr']\n\n## adding constant to the training vars\n\nX_train_sm = sm.add_constant(X_train[FeatureColumns])\n\n## creating the Linear Model\n\nlm_sm = sm.OLS(y_train , X_train_sm)\n\nlm_model = lm_sm.fit()\n\n### checking the sumary of the Linear Model\nprint(lm_model.summary())\n\n## check VIF\nprint('\\n',showVIF(X_train_sm,FeatureColumns))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.698974Z","iopub.execute_input":"2021-06-10T15:50:41.699394Z","iopub.status.idle":"2021-06-10T15:50:41.74699Z","shell.execute_reply.started":"2021-06-10T15:50:41.69936Z","shell.execute_reply":"2021-06-10T15:50:41.746261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"- ### Finally we have the list of 10 top features that give very good R2 value in the model and none have P-Value > 0.05 and VIF > 5\n\n- ### Features : <font color=\"green\">casual , windspeed , 2019 , Fall, Winter, Mist, Apr, Friday, Saturday, LightSnow </font>","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 9: Residual Analysis of the train data</font>\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","metadata":{}},{"cell_type":"code","source":"## getting predicted value from the model\ny_train_predict = lm_model.predict(X_train_sm)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.748033Z","iopub.execute_input":"2021-06-10T15:50:41.748465Z","iopub.status.idle":"2021-06-10T15:50:41.752205Z","shell.execute_reply.started":"2021-06-10T15:50:41.748426Z","shell.execute_reply":"2021-06-10T15:50:41.751487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_predict))\nfig.suptitle('Error Terms', fontsize = 15)\nplt.xlabel('Errors', fontsize = 18)  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.753158Z","iopub.execute_input":"2021-06-10T15:50:41.753469Z","iopub.status.idle":"2021-06-10T15:50:41.979716Z","shell.execute_reply.started":"2021-06-10T15:50:41.753441Z","shell.execute_reply":"2021-06-10T15:50:41.978667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ### We can that the error terms of the predicted values from the model on our training data is normally distributed and mean is at 0\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"##  <font color=\"blue\">Step 10: Making Predictions Using the Final Model</font>","metadata":{}},{"cell_type":"markdown","source":"#### Applying the scaling on the test sets","metadata":{}},{"cell_type":"code","source":"df_test[scale_feature] = scaler.fit_transform(df_test[scale_feature])\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:41.981381Z","iopub.execute_input":"2021-06-10T15:50:41.981789Z","iopub.status.idle":"2021-06-10T15:50:42.018053Z","shell.execute_reply.started":"2021-06-10T15:50:41.981746Z","shell.execute_reply":"2021-06-10T15:50:42.016986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### checking the impact of scaling on the test set\ndf_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:42.019426Z","iopub.execute_input":"2021-06-10T15:50:42.019716Z","iopub.status.idle":"2021-06-10T15:50:42.125296Z","shell.execute_reply.started":"2021-06-10T15:50:42.019687Z","shell.execute_reply":"2021-06-10T15:50:42.124132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"#### Dividing into X and Y vars from test sets","metadata":{}},{"cell_type":"code","source":"y_test = df_test.pop('cnt')\nX_test = df_test","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:42.126723Z","iopub.execute_input":"2021-06-10T15:50:42.127104Z","iopub.status.idle":"2021-06-10T15:50:42.131657Z","shell.execute_reply.started":"2021-06-10T15:50:42.127072Z","shell.execute_reply":"2021-06-10T15:50:42.130737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"#### Adding constant variable to test dataframe","metadata":{}},{"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test[FeatureColumns])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:42.132599Z","iopub.execute_input":"2021-06-10T15:50:42.132893Z","iopub.status.idle":"2021-06-10T15:50:42.149022Z","shell.execute_reply.started":"2021-06-10T15:50:42.132856Z","shell.execute_reply":"2021-06-10T15:50:42.147958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"#### Making predictions","metadata":{}},{"cell_type":"code","source":"y_test_predict = lm_model.predict(X_test_sm)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:42.150235Z","iopub.execute_input":"2021-06-10T15:50:42.150535Z","iopub.status.idle":"2021-06-10T15:50:42.161407Z","shell.execute_reply.started":"2021-06-10T15:50:42.150507Z","shell.execute_reply":"2021-06-10T15:50:42.160417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"blue\">Step 11: Model Evaluation</font>","metadata":{}},{"cell_type":"code","source":"# Plotting y_test and y_test_predict to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_test_predict)\nfig.suptitle('y_test vs y_test_predict', fontsize = 20) \nplt.xlabel('y_test', fontsize = 18) \nplt.ylabel('y_test_predict', fontsize = 16)  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:42.162659Z","iopub.execute_input":"2021-06-10T15:50:42.162933Z","iopub.status.idle":"2021-06-10T15:50:42.325539Z","shell.execute_reply.started":"2021-06-10T15:50:42.162903Z","shell.execute_reply":"2021-06-10T15:50:42.324451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the R2-score of the Test data\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_test_predict)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T15:50:42.327273Z","iopub.execute_input":"2021-06-10T15:50:42.327705Z","iopub.status.idle":"2021-06-10T15:50:42.334923Z","shell.execute_reply.started":"2021-06-10T15:50:42.327663Z","shell.execute_reply":"2021-06-10T15:50:42.33372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"## <font color=\"green\">We can see that the R2 value of the Train (0.772) and Test (0.708) data is very close . So we can conclude the model is best fit.</font>","metadata":{}}]}