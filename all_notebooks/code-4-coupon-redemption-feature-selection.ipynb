{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import mode\nimport pandas_profiling\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.metrics import auc\n\nimport os\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_colwidth',500)\npd.set_option('display.max_columns',5000)\n\nencoder = LabelEncoder()\nfrom IPython.display import Image\nimport os\n!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ncampaign = pd.read_csv('../input/campaign_data.csv')\nitems = pd.read_csv('../input/item_data.csv')\ncoupons = pd.read_csv('../input/coupon_item_mapping.csv')\ncust_demo = pd.read_csv('../input/customer_demographics.csv')\ncust_tran = pd.read_csv('../input/customer_transaction_data.csv')\ntest = pd.read_csv('../input/test.csv')\ndf=pd.read_csv('../input/final_train.csv')\ndata=pd.read_csv('../input/smote.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, campaign.shape, items.shape, coupons.shape, cust_demo.shape, cust_tran.shape, test.shape,df.shape,data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Dataframe')\nprint(train.isnull().sum())\nprint('======================')\nprint('Campaign Dataframe')\nprint(campaign.isnull().sum())\nprint('======================')\nprint('Items Dataframe')\nprint(items.isnull().sum())\nprint('======================')\nprint('Coupons Dataframe')\nprint(coupons.isnull().sum())\nprint('======================')\nprint('Customer Demographics Dataframe')\nprint(cust_demo.isnull().sum())\nprint('======================')\nprint('Customer Transaction Dataframe')\nprint(cust_tran.isnull().sum())\nprint('======================')\n\nprint(test.isnull().sum())\nprint('======================')\nprint(df.isnull().sum())\nprint('======================')\nprint(data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(\"../input/Schema.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All datasets merged by following steps:\n\n1. Merge coupon item data and items data on item_id\n2. Aggregate transactions by item_id\n3. Merge 1 and 2 on item_id\n4. Aggregate 3 on coupon_id\n5. Merge 4 and train on coupon_id\n6. Aggregate transactions on customer_id\n7. Merge 5 with campaign data on campaign_id\n8. Merge 7 with customer demographic data on customer_id\n9. Merge 6 with 8 on customer_id respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['Unnamed: 0'],inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"## Recursive Feature Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nX = df.drop(['redemption_status'], axis = 1)\ny = df['redemption_status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nrfe = RFE(model,41)\nX_rfe = rfe.fit_transform(X,y)\nmodel.fit(X_rfe,y)\nprint(rfe.support_)\nprint(rfe.ranking_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no of features\nnof_list = np.arange(1,41)\nhigh_score = 0\n#Variable to store the optimum feature\nnof = 0\nscore_list=[]\nfor n in range(len(nof_list)):\n    X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,Y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,Y_train)\n    Y_pred = model.predict(X_test_rfe)\n    score = roc_auc_score(Y_test,Y_pred)\n    score_list.append(score)\n    if(score>high_score):\n        high_score=score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" %(nof, high_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(X.columns)\nmodel=LinearRegression()\n#Initializing RFE Model\nrfe = RFE(model,32)\n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)\n#Fitting data to model\nmodel.fit(X_rfe,y)\ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names=['coupon_id', 'brand', 'brand_type', 'category', 'cd_sum','coupon_discount_x', 'coupon_used_x', 'item_counts', 'no_of_customers',\n   'od_sum', 'other_discount_x', 'quantity_x', 'selling_price_x','total_discount_mean',\n   'total_discount_sum', 'campaign_type','campaign_duration',\n   'age_range', 'marital_status', 'rented','family_size',\n   'no_of_children', 'income_bracket', 'coupon_discount_y','coupon_used_y','day', 'dow',\n   'month', 'other_discount_y','quantity_y', 'selling_price_y', 'cdd_sum']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df[col_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1990)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(x_train,y_train)\ny_pred_LR = LR.predict(x_test)\nprint(classification_report(y_test,y_pred_LR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(y_test,y_pred_LR))\nModel = ['Logistic Regression']\nROC_AUC_Accuracy = [roc_auc_score(y_test,y_pred_LR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \nresults=confusion_matrix(y_test,y_pred_LR)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_LR) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_LR) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_LR ):\n    cm = metrics.confusion_matrix( y_test,y_pred_LR )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_LR )\n\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_LR):\n    if (len(y_test.shape) != len(y_pred_LR.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_LR.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_LR)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    conf_mat = create_conf_mat(y_test,y_pred_LR)\n    sns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.title('Actual vs. Predicted Confusion Matrix')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = LR.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_nb))\nModel.append('Naive Bayes')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_nb)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_nb) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_nb) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_nb ):\n    cm = metrics.confusion_matrix( y_test,y_pred_nb )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_nb )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_nb):\n    if (len(y_test.shape) != len(y_pred_nb.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_nb.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_nb)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_nb)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = nb.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':range(1,10),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\ndt = DecisionTreeClassifier()\n\nrs = RandomizedSearchCV(estimator=dt,n_jobs=-1,cv=3,param_distributions=params,scoring='recall')\nrs.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(**rs.best_params_)\ndt.fit(x_train,y_train)\ny_pred_dt = dt.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_dt))\nModel.append('Decision Tree')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_dt):\n    cm = metrics.confusion_matrix( y_test,y_pred_dt )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_dt)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_dt) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_dt) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_dt )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_dt):\n    if (len(y_test.shape) != len(y_pred_dt.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_dt.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_dt)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_dt)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = dt.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'n_estimators':range(10,100,10),\n    'criterion':['gini','entropy'],\n    'max_depth':range(2,10,1),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\nrf = RandomForestClassifier()\n\nrs = RandomizedSearchCV(estimator=rf,param_distributions=params,cv=3,scoring='recall',n_jobs=-1)\nrs.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(**rs.best_params_)\nrf.fit(x_train,y_train)\ny_pred_rf = rf.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_rf))\nModel.append('Random Forest')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_rf)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_rf) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_rf) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_rf ):\n    cm = metrics.confusion_matrix( y_test,y_pred_rf )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_rf )\nprint(\"confusion matrix = \\n\",mat_pruned)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_rf):\n    if (len(y_test.shape) != len(y_pred_rf.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_rf.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_rf)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = rf.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'Model':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now using the Bagging Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Bag = BaggingClassifier(base_estimator=LR,n_estimators=100,n_jobs=-1,random_state=1)\nnb_Bag = BaggingClassifier(base_estimator=nb,n_estimators=100,n_jobs=-1,random_state=1)\ndt_Bag = BaggingClassifier(base_estimator=dt,n_estimators=100,n_jobs=-1,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = LR_Bag\nname = 'Bagged-LR'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Bag\nname = 'Bagged-NB'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = dt_Bag\nname = 'Bagged-DT'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now Moving Towards Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMModel,LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Boost = AdaBoostClassifier(base_estimator=LR,n_estimators=100,learning_rate=0.01,random_state=1)\nnb_Boost = AdaBoostClassifier(base_estimator=nb,n_estimators=100,learning_rate=0.01,random_state=1)\ndt_Boost = AdaBoostClassifier(base_estimator=dt,n_estimators=100,learning_rate=0.01,random_state=1)\nrf_Boost = AdaBoostClassifier(base_estimator=rf,n_estimators=100,learning_rate=0.01,random_state=1)\ngb_Boost = GradientBoostingClassifier(n_estimators=100,learning_rate=0.01)\nlgbm = LGBMClassifier(objective='binary',n_estimators=100,reg_alpha=2,reg_lambda=5,random_state=1,learning_rate=0.01,is_unbalance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = LR_Boost\nname = 'Boosted-LR'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Boost\nname = 'Boosted-NB'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = dt_Boost\nname = 'Boosted-DT'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = lgbm\nname = 'LGBM'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'Model - RFE Data':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VIF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n[variance_inflation_factor(X.values, j) for j in range(1, X.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function definition\n\ndef calculate_vif(X):\n    thresh = 5.0\n    output = pd.DataFrame()\n    k = X.shape[1]\n    vif = [variance_inflation_factor(X.values, j) for j in range(X.shape[1])]\n    for i in range(1,k):\n        print(\"Iteration no.\",i)\n        print(vif)\n        a = np.argmax(vif)\n        print(\"Max VIF is for variable no.:\",a)\n        if vif[a] <= thresh :\n            break\n        if i == 1 :          \n            output = X.drop(X.columns[a], axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        elif i > 1 :\n            output = output.drop(output.columns[a],axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n    return(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_out = calculate_vif(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## includes only the relevant features.\ntrain_out.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_out.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_out.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(train_out,y,test_size=0.3,random_state=1990)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape,y_train.shape,x_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(x_train,y_train)\ny_pred_LR = LR.predict(x_test)\nprint(classification_report(y_test,y_pred_LR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test,y_pred_LR)\nModel = ['Logistic Regression']\nROC_AUC_Accuracy = [roc_auc_score(y_test,y_pred_LR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_LR)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_LR) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_LR) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_LR ):\n    cm = metrics.confusion_matrix( y_test,y_pred_LR )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_LR )\n\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_LR):\n    if (len(y_test.shape) != len(y_pred_LR.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_LR.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_LR)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    conf_mat = create_conf_mat(y_test,y_pred_LR)\n    sns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.title('Actual vs. Predicted Confusion Matrix')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = LR.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_nb))\nModel.append('Naive Bayes')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_nb)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_nb) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_nb) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_nb ):\n    cm = metrics.confusion_matrix( y_test,y_pred_nb )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_nb )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_nb):\n    if (len(y_test.shape) != len(y_pred_nb.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_nb.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_nb)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_nb)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = nb.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':range(1,10),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\ndt = DecisionTreeClassifier()\n\ngs = GridSearchCV(estimator=dt,n_jobs=-1,cv=3,param_grid=params,scoring='recall')\ngs.fit(train_out,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(**gs.best_params_)\ndt.fit(x_train,y_train)\ny_pred_dt = dt.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_dt))\nModel.append('Decision Tree')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_dt):\n    cm = metrics.confusion_matrix( y_test,y_pred_dt )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_dt)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_dt) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_dt) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_dt )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_dt):\n    if (len(y_test.shape) != len(y_pred_dt.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_dt.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_dt)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_dt)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = dt.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'n_estimators':range(10,100,10),\n    'criterion':['gini','entropy'],\n    'max_depth':range(2,10,1),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\nrf = RandomForestClassifier()\n\nrs = RandomizedSearchCV(estimator=rf,param_distributions=params,cv=3,scoring='recall',n_jobs=-1)\nrs.fit(train_out,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(**rs.best_params_)\nrf.fit(x_train,y_train)\ny_pred_rf = rf.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_rf))\nModel.append('Random Forest')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_rf)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_rf) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_rf) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_rf ):\n    cm = metrics.confusion_matrix( y_test,y_pred_rf )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_rf )\nprint(\"confusion matrix = \\n\",mat_pruned)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_rf):\n    if (len(y_test.shape) != len(y_pred_rf.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_rf.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_rf)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = rf.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'Model - VIF Data':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now using the Bagging Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Bag = BaggingClassifier(base_estimator=LR,n_estimators=100,n_jobs=-1,random_state=1)\nnb_Bag = BaggingClassifier(base_estimator=nb,n_estimators=100,n_jobs=-1,random_state=1)\ndt_Bag = BaggingClassifier(base_estimator=dt,n_estimators=100,n_jobs=-1,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = LR_Bag\nname = 'Bagged-LR'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Bag\nname = 'Bagged-NB'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = dt_Bag\nname = 'Bagged-DT'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now Moving Towards Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMModel,LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Boost = AdaBoostClassifier(base_estimator=LR,n_estimators=100,learning_rate=0.01,random_state=1)\nnb_Boost = AdaBoostClassifier(base_estimator=nb,n_estimators=100,learning_rate=0.01,random_state=1)\ndt_Boost = AdaBoostClassifier(base_estimator=dt,n_estimators=100,learning_rate=0.01,random_state=1)\nrf_Boost = AdaBoostClassifier(base_estimator=rf,n_estimators=100,learning_rate=0.01,random_state=1)\ngb_Boost = GradientBoostingClassifier(n_estimators=100,learning_rate=0.01)\nlgbm = LGBMClassifier(objective='binary',n_estimators=100,reg_alpha=2,reg_lambda=5,random_state=1,learning_rate=0.01,is_unbalance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = LR_Boost\nname = 'Boosted-LR'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Boost\nname = 'Boosted-NB'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = dt_Boost\nname = 'Boosted-DT'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = lgbm\nname = 'LGBM'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'Model - RFE Data':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV, Lasso\nreg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  \n      str(sum(coef == 0)) + \" variables\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_coef = coef.sort_values()\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (16.0, 20.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got 8 columns by performing the Lasso they are:\n\n [qu_sum, qa_sum, pprice_sum, price_sum, total_discount_sum, cd_sum, odd_sum, cdd_sum]"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names=['qu_sum', 'qa_sum', 'pprice_sum', 'price_sum', 'total_discount_sum', 'cd_sum', 'odd_sum', 'cdd_sum']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df[col_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1990)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape,y_train.shape,x_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR = LogisticRegression()\nLR.fit(x_train,y_train)\ny_pred_LR = LR.predict(x_test)\nprint(classification_report(y_test,y_pred_LR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test,y_pred_LR)\nModel = ['Logistic Regression']\nROC_AUC_Accuracy = [roc_auc_score(y_test,y_pred_LR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_LR)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_LR) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_LR) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_LR ):\n    cm = metrics.confusion_matrix( y_test,y_pred_LR )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_LR )\n\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_LR):\n    if (len(y_test.shape) != len(y_pred_LR.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_LR.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_LR)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    conf_mat = create_conf_mat(y_test,y_pred_LR)\n    sns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\n    plt.xlabel('Predicted Values')\n    plt.ylabel('Actual Values')\n    plt.title('Actual vs. Predicted Confusion Matrix')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = LR.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_nb))\nModel.append('Naive Bayes')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_nb)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_nb) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_nb) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_nb ):\n    cm = metrics.confusion_matrix( y_test,y_pred_nb )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_nb )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_nb):\n    if (len(y_test.shape) != len(y_pred_nb.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_nb.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_nb)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_nb)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = nb.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':range(1,10),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\ndt = DecisionTreeClassifier()\n\nrs = RandomizedSearchCV(estimator=dt,n_jobs=-1,cv=3,param_distributions=params,scoring='recall')\nrs.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(**rs.best_params_)\ndt.fit(x_train,y_train)\ny_pred_dt = dt.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_dt))\nModel.append('Decision Tree')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_dt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_dt):\n    cm = metrics.confusion_matrix( y_test,y_pred_dt )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_dt)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_dt) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_dt) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_dt )\nprint(\"confusion matrix = \\n\",mat_pruned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_dt):\n    if (len(y_test.shape) != len(y_pred_dt.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_dt.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_dt)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_dt)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = dt.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \n    'n_estimators':range(10,100,10),\n    'criterion':['gini','entropy'],\n    'max_depth':range(2,10,1),\n    'max_leaf_nodes':range(2,10,1),\n    'max_features':['auto','log2']\n    \n}\n\nrf = RandomForestClassifier()\n\nrs = RandomizedSearchCV(estimator=rf,param_distributions=params,cv=5,scoring='recall',n_jobs=-1)\nrs.fit(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(**rs.best_params_)\nrf.fit(x_train,y_train)\ny_pred_rf = rf.predict(x_test)\nprint(roc_auc_score(y_test,y_pred_rf))\nModel.append('Random Forest')\nROC_AUC_Accuracy.append(roc_auc_score(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=confusion_matrix(y_test,y_pred_rf)\nprint ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test,y_pred_rf) )\nprint ('Report : ')\nprint (classification_report(y_test,y_pred_rf) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to visulise confusion matrix\ndef draw_cm( y_test,y_pred_rf ):\n    cm = metrics.confusion_matrix( y_test,y_pred_rf )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [\"0\", \"1\"] , yticklabels = [\"0\", \"1\"] , cmap=\"Greens\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nfrom sklearn.metrics import classification_report,confusion_matrix\nmat_pruned = confusion_matrix(y_test,y_pred_rf )\nprint(\"confusion matrix = \\n\",mat_pruned)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_conf_mat(y_test,y_pred_rf):\n    if (len(y_test.shape) != len(y_pred_rf.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (y_test.shape != y_pred_rf.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics\n        test_crosstb_comp = pd.crosstab(index = y_test,\n                                       columns = y_pred_rf)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_mat = create_conf_mat(y_test,y_pred_rf)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get predicted probabilites\ntarget_probailities_log = rf.predict_proba(x_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create true and false positive rates\nlog_false_positive_rate,log_true_positive_rate,log_threshold = roc_curve(y_test,target_probailities_log)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot ROC Curve\nsns.set_style('whitegrid')\nplt.figure(figsize=(10,6))\nplt.title('Reciver Operating Characterstic Curve')\nplt.plot(log_false_positive_rate,log_true_positive_rate)\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Model,ROC_AUC_Accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'Model':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Now using the Bagging Classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Bag = BaggingClassifier(base_estimator=LR,n_estimators=100,n_jobs=-1,random_state=1)\nnb_Bag = BaggingClassifier(base_estimator=nb,n_estimators=100,n_jobs=-1,random_state=1)\ndt_Bag = BaggingClassifier(base_estimator=dt,n_estimators=100,n_jobs=-1,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.array(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = LR_Bag\nname = 'Bagged-LR'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Bag\nname = 'Bagged-NB'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3,shuffle=True,random_state=1)\nmodel = dt_Bag\nname = 'Bagged-DT'\n#for model,name in zip([LR_Bag,knn_Bag,nb_Bag,dt_Bag],['Bagged-LR','Bagged-kNN','Bagged-NB','Bagged-DT']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now Moving Towards Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMModel,LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_Boost = AdaBoostClassifier(base_estimator=LR,n_estimators=100,learning_rate=0.01,random_state=1)\nnb_Boost = AdaBoostClassifier(base_estimator=nb,n_estimators=100,learning_rate=0.01,random_state=1)\ndt_Boost = AdaBoostClassifier(base_estimator=dt,n_estimators=100,learning_rate=0.01,random_state=1)\nrf_Boost = AdaBoostClassifier(base_estimator=rf,n_estimators=100,learning_rate=0.01,random_state=1)\ngb_Boost = GradientBoostingClassifier(n_estimators=100,learning_rate=0.01)\nlgbm = LGBMClassifier(objective='binary',n_estimators=100,reg_alpha=2,reg_lambda=5,random_state=1,learning_rate=0.01,is_unbalance=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = LR_Boost\nname = 'Boosted-LR'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = nb_Boost\nname = 'Boosted-NB'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = dt_Boost\nname = 'Boosted-DT'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5,shuffle=True,random_state=1)\nmodel = lgbm\nname = 'LGBM'\n#for model,name in zip([LR_Boost,nb_Boost,dt_Boost,rf_Boost,lgbm],['Boosted-LR','Boosted-NB','Boosted-DT','Boosted - Random Forest','LGBM']):\nroc_acc = []\nfor train,test in kf.split(x,y):\n    x_train = x[train,:]\n    x_test = x[test,:]\n    y_train = y[train]\n    y_test = y[test]\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    score = roc_auc_score(y_test,y_pred)\n    fpr,tpr,_ = roc_curve(y_test,y_pred)\n    roc_acc.append(auc(fpr,tpr))\nModel.append(name)\nROC_AUC_Accuracy.append(np.mean(roc_acc))\nprint('The AUC Score for')\nprint('%s is %0.02f with variacne of (+/-) %0.5f'%(name,np.mean(roc_acc),np.var(roc_acc,ddof=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.DataFrame({'Model - RFE Data':Model,'Accuracy':ROC_AUC_Accuracy})\nfinal_result","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}