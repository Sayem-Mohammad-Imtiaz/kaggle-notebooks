{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Helper function, used these for debugging purposes\n# detector2 build only succeeds if CUDA version is correct\n\n#!nvidia-smi\n#!nvcc --version\n\n#import torch\n#torch.__version__\n#import torchvision\n#torchvision.__version__\n\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.7/index.html\n!pip install fastai","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-23T08:47:42.008277Z","iopub.execute_input":"2021-06-23T08:47:42.011073Z","iopub.status.idle":"2021-06-23T08:48:03.517099Z","shell.execute_reply.started":"2021-06-23T08:47:42.011034Z","shell.execute_reply":"2021-06-23T08:48:03.516066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Underneed you'll find the extra libraries we'll use in this notebook. More libraries will be added througout the notebook when needed.","metadata":{}},{"cell_type":"code","source":"# Base setup:\n# detectron2 logger\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# common libraries\nimport numpy as np\nimport os, json, cv2, random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog\nfrom detectron2.structures import BoxMode","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-23T08:48:03.522406Z","iopub.execute_input":"2021-06-23T08:48:03.522991Z","iopub.status.idle":"2021-06-23T08:48:04.646145Z","shell.execute_reply.started":"2021-06-23T08:48:03.522951Z","shell.execute_reply":"2021-06-23T08:48:04.638313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=traincustom> </a>\n## 3. Data Preparation\nLet's first check our training data! Ofcourse we'll use the **Visualizer** class again.","metadata":{}},{"cell_type":"code","source":"# loading of data\n# challenge1_path = \n\ntraining_path = \"/kaggle/input/d/ykviki/dsta-object-detection-til-2021-dataset/merged/merged\"\ntrain_annotation = os.path.join(training_path, \"annotations/train.json\")\nval_annotation = os.path.join(training_path, \"annotations/val.json\")\nimage_path = os.path.join(training_path,\"images\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:04.647673Z","iopub.execute_input":"2021-06-23T08:48:04.648037Z","iopub.status.idle":"2021-06-23T08:48:04.658195Z","shell.execute_reply.started":"2021-06-23T08:48:04.648001Z","shell.execute_reply":"2021-06-23T08:48:04.657091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.path.exists(train_annotation)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:04.659443Z","iopub.execute_input":"2021-06-23T08:48:04.659879Z","iopub.status.idle":"2021-06-23T08:48:04.677063Z","shell.execute_reply.started":"2021-06-23T08:48:04.659825Z","shell.execute_reply":"2021-06-23T08:48:04.676147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from detectron2.structures import BoxMode\n# if your dataset is in COCO format, this cell can be replaced by the following three lines:\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"train_data\", {}, train_annotation, image_path)\nregister_coco_instances(\"val_data\", {}, val_annotation, image_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:04.678567Z","iopub.execute_input":"2021-06-23T08:48:04.679009Z","iopub.status.idle":"2021-06-23T08:48:04.689945Z","shell.execute_reply.started":"2021-06-23T08:48:04.678972Z","shell.execute_reply":"2021-06-23T08:48:04.688583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualize training data\nmy_dataset_train_metadata = MetadataCatalog.get(\"train_data\")\ndataset_dicts = DatasetCatalog.get(\"train_data\")\n\nmy_dataset_val_metadata = MetadataCatalog.get(\"val_data\")\nval_dicts = DatasetCatalog.get(\"val_data\")\n\nimport random\nfrom detectron2.utils.visualizer import Visualizer\nimport cv2\nimport matplotlib.pyplot as plt\n\nfor d in random.sample(dataset_dicts, 3):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n    vis = visualizer.draw_dataset_dict(d)\n    plt.imshow(vis.get_image()[:, :, ::-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:04.691433Z","iopub.execute_input":"2021-06-23T08:48:04.69179Z","iopub.status.idle":"2021-06-23T08:48:06.100715Z","shell.execute_reply.started":"2021-06-23T08:48:04.691755Z","shell.execute_reply":"2021-06-23T08:48:06.099808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training","metadata":{}},{"cell_type":"code","source":"# # DATA AUG\n\n# from detectron2.data import transforms as T\n# # Define a sequence of augmentations:\n# augs = T.AugmentationList([\n#     T.RandomBrightness(0.9, 1.1),\n#     T.RandomFlip(prob=0.5),\n#     T.RandomCrop(\"absolute\", (640, 640))\n# ])  # type: T.Augmentation\n\n# # Define the augmentation input (\"image\" required, others optional):\n# input = T.AugInput(image, boxes=boxes, sem_seg=sem_seg)\n\n# # Apply the augmentation:\n# transform = augs(input)  # type: T.Transform\n# image_transformed = input.image  # new image\n# sem_seg_transformed = input.sem_seg  # new semantic segmentation\n\n# # For any extra data that needs to be augmented together, use transform, e.g.:\n# image2_transformed = transform.apply_image(image2)\n# polygons_transformed = transform.apply_polygons(polygons)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:06.102078Z","iopub.execute_input":"2021-06-23T08:48:06.102603Z","iopub.status.idle":"2021-06-23T08:48:06.11001Z","shell.execute_reply.started":"2021-06-23T08:48:06.102557Z","shell.execute_reply":"2021-06-23T08:48:06.109093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Run training\n\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator\n\n\nclass CocoTrainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            os.makedirs(\"coco_eval\", exist_ok=True)\n            output_folder = \"coco_eval\"\n        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:06.111663Z","iopub.execute_input":"2021-06-23T08:48:06.113214Z","iopub.status.idle":"2021-06-23T08:48:06.127054Z","shell.execute_reply.started":"2021-06-23T08:48:06.113175Z","shell.execute_reply":"2021-06-23T08:48:06.125992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### FASTERCNN\n","metadata":{}},{"cell_type":"markdown","source":"Running the model","metadata":{}},{"cell_type":"code","source":"# LOADING PREV FORMAT\n\n# from detectron2.config.config import CfgNode as CN\n\n# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_data\",)\n# cfg.DATASETS.TEST = (\"val_data\",)\n# cfg.DATALOADER.NUM_WORKERS = 4\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo\n# cfg.SOLVER.IMS_PER_BATCH = 4\n# cfg.SOLVER.BASE_LR = 0.001\n# cfg.SOLVER.WARMUP_ITERS = 1000\n# cfg.SOLVER.MAX_ITER = 20000 #adjust up if val mAP is still rising, adjust down if overfit\n# cfg.SOLVER.STEPS = [1000,8000,16000]\n# cfg.SOLVER.GAMMA = 0.05\n# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7\n# # cfg.TEST.EVAL_PERIOD = 1000\n\n\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = CocoTrainer(cfg) \n# # trainer.resume_or_load(resume=False)\n\n# trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:06.131283Z","iopub.execute_input":"2021-06-23T08:48:06.131678Z","iopub.status.idle":"2021-06-23T08:48:06.138657Z","shell.execute_reply.started":"2021-06-23T08:48:06.131638Z","shell.execute_reply":"2021-06-23T08:48:06.137545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RetinaNet","metadata":{}},{"cell_type":"code","source":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_data\",)\n# cfg.DATASETS.TEST = (\"val_data\",)\n# cfg.DATALOADER.NUM_WORKERS = 4\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_1x.yaml\")  # Let training initialize from model zoo\n# cfg.SOLVER.IMS_PER_BATCH = 4\n# cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n# cfg.SOLVER.MAX_ITER = 300    # 300 iterations enough for this dataset; Train longer for a practical dataset\n# cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, enough for this dataset (default: 512)\n# # cfg.MODEL.ROI_HEADS.NUM_CLASSES = 5  # classes for RCNN\n# cfg.MODEL.RETINANET.NUM_CLASSES = 5 # Classes for Retina\n# cfg.TEST.EVAL_PERIOD = 500\n\n\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = DefaultTrainer(cfg) \n# trainer.resume_or_load(resume=False)\n# trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:06.140584Z","iopub.execute_input":"2021-06-23T08:48:06.141013Z","iopub.status.idle":"2021-06-23T08:48:06.154831Z","shell.execute_reply.started":"2021-06-23T08:48:06.140978Z","shell.execute_reply":"2021-06-23T08:48:06.15336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"modelevaluation\" ></a>\n## 4.4. Model evaluation\nLet's check out the performance of our model!\n\nFirst of all let's make some predictions! We're going to use the [**DefaultPredictor**](https://detectron2.readthedocs.io/en/latest/modules/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor) class. Ofcourse we'll use the same cfg that we used during training. We'll change two parameters for our inferencing.","metadata":{}},{"cell_type":"markdown","source":"### Loading model","metadata":{}},{"cell_type":"code","source":"# RELOADING MODEL\nfrom detectron2.config.config import CfgNode as CN\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\ncfg.MODEL.WEIGHTS = os.path.join(\"/kaggle/input/detectron-dsta-model\", \"model_final (3).pth\")  # path to the model we trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\ncfg.DATASETS.TRAIN = (\"train_data\",)\ncfg.DATASETS.TEST = (\"val_data\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.005\ncfg.SOLVER.WARMUP_ITERS = 1000\ncfg.SOLVER.MAX_ITER = 9000 #adjust up if val mAP is still rising, adjust down if overfit\ncfg.SOLVER.STEPS = [1000,3000,6000]\ncfg.SOLVER.GAMMA = 0.05\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 7\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a testing threshold\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = CocoTrainer(cfg) \ntrainer.resume_or_load(resume=False)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:50:17.838876Z","iopub.execute_input":"2021-06-23T08:50:17.839282Z","iopub.status.idle":"2021-06-23T08:50:19.795277Z","shell.execute_reply.started":"2021-06-23T08:50:17.839244Z","shell.execute_reply":"2021-06-23T08:50:19.791713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor = DefaultPredictor(cfg)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:28.367586Z","iopub.status.idle":"2021-06-23T08:48:28.368363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that by using the [**ColorMode.IMAGE_BW**](https://detectron2.readthedocs.io/en/latest/modules/utils.html?highlight=ColorMode#module-detectron2.utils.visualizer) we we're capable of removing the colors from objects which aren't detected!","metadata":{}},{"cell_type":"code","source":"from detectron2.utils.visualizer import ColorMode\nval_dict = DatasetCatalog.get(\"val_data\")\n\nfor d in random.sample(val_dict, 3):    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im) \n    v = Visualizer(im[:, :, ::-1],\n                   metadata=my_dataset_train_metadata, \n                   scale=0.5, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. Only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.figure(figsize=(15,7))\n    plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:28.374783Z","iopub.status.idle":"2021-06-23T08:48:28.37549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we can see that our models performs pretty well! Let's now evaluate our custom model with [Evaluators](https://detectron2.readthedocs.io/en/latest/modules/engine.html?highlight=DefaultPredictor#detectron2.engine.defaults.DefaultPredictor). Two evaluators can be used:\n* [**COCOEvaluator**](https://detectron2.readthedocs.io/en/latest/modules/evaluation.html#detectron2.evaluation.COCOEvaluator) can evaluate AP (Average Precision) for box detection, instance segmentation and keypoint detection.\n* [**SemSegEvaluator**](https://detectron2.readthedocs.io/en/latest/modules/evaluation.html#detectron2.evaluation.SemSegEvaluator) can evaluate semantic segmentation metrics.\n\nAfterwards we'll use the [**build_detection_test_loader**](https://detectron2.readthedocs.io/en/latest/modules/data.html?highlight=build_detection_test_loader#detectron2.data.build_detection_test_loader) which returns a torch DataLoader, that loads the given detection dataset.\n\nAt last we'll use the model, evaluated and dataloader within the [inference_on_dataset](https://detectron2.readthedocs.io/en/latest/modules/evaluation.html#detectron2.evaluation.inference_on_dataset) function. It runs the model on the dataloader and evaluates the metric with the evaluator.","metadata":{}},{"cell_type":"code","source":"from detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import build_detection_test_loader\nfrom detectron2.modeling import build_model\n\n\nevaluator = COCOEvaluator(\"val_data\", None, False, output_dir=\"./output/\")\n# evaluator = COCOEvaluator(\"val_data\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n\n# Loading model\nmodel_uploaded = build_model(cfg)\n\nval_loader = build_detection_test_loader(cfg, \"val_data\")\n# print(inference_on_dataset(trainer.model, val_loader, evaluator))\nprint(inference_on_dataset(model_uploaded, val_loader, evaluator))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:28.380207Z","iopub.status.idle":"2021-06-23T08:48:28.380994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n# cfg.MODEL.WEIGHTS = os.path.join(\"/kaggle/input/objectron-retinanetv1/model_final.pth\")\n\n\nc3_test_path = \"/kaggle/input/d/ykviki/dsta-object-detection-til-2021-dataset/challenge_3_test_dataset/challenge_3_test_dataset\"\n\ncfg.DATASETS.TEST = (\"my_dataset_test\", )\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\ntest_metadata = MetadataCatalog.get(\"my_dataset_test\")\n\nfrom detectron2.utils.visualizer import ColorMode\nimport glob\n\nou_test = []\nfor imageName in glob.glob(os.path.join(c3_test_path,\"images/*.jpg\")):\n  im = cv2.imread(imageName)\n  outputs = predictor(im)\n  ou_test.append(outputs)\n  v = Visualizer(im[:, :, ::-1],\n                metadata=my_dataset_train_metadata, \n                scale=0.8\n                 )\n  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n  plt.imshow(out.get_image()[:, :, ::-1])\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T08:48:28.382244Z","iopub.status.idle":"2021-06-23T08:48:28.38304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ou_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\nim = cv2.imread(os.path.join(c3_test_path,\"images/44.jpg\"))\noutputs = predictor(im)\nv = Visualizer(im[:, :, ::-1],\n            metadata=my_dataset_train_metadata, \n            scale=0.8\n             )\nout = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\nplt.imshow(out.get_image()[:, :, ::-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate detections on the folder of test images (this will be used for submission)\nfrom PIL import Image, ImageDraw\nfrom torchvision import transforms\nfrom torchvision.ops import batched_nms\nfrom torchvision.transforms import functional as F\nimport torch\n\ndetections = []\n\nfor imageName in glob.glob(os.path.join(c3_test_path,\"images/*.jpg\")):\n\n        im = cv2.imread(imageName)\n        outputs = predictor(im)\n        classes = outputs[\"instances\"].pred_classes.tolist()\n        box_round = outputs[\"instances\"].pred_boxes.tensor.tolist()\n        score_output = outputs[\"instances\"].scores.tolist()\n        head, tail = os.path.split(imageName)\n        img_id = int(tail.split('.')[0])\n\n        for i in range(len(box_round)):\n\n            x1, y1, x2, y2 = box_round[i]\n            label = int(classes[i]) + 1\n            score = float(score_output[i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})\n\ntest_pred_json = os.path.join(\"/kaggle/working\", \"test_pred_2.json\")\nwith open(test_pred_json, 'w') as f:\n    json.dump(detections, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check \nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nsample_json_path = os.path.join(c3_test_path,\"c2_test_sample.json\")\n\ncoco_gt = COCO(sample_json_path)\ncoco_dt = coco_gt.loadRes(test_pred_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"othermodels\" ></a>\n# 5. Other models\n\nIt's possible to use other high-end object detection models aswell. Let's check it out!\n\n<a id=\"keypoint\" ></a>\n## 5.1. Keypoint detection","metadata":{}},{"cell_type":"markdown","source":"Reload the data.","metadata":{"trusted":true}},{"cell_type":"code","source":"# !wget http://images.cocodataset.org/val2017/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\"./input.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cfg = get_cfg()   # fresh config\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# outputs = predictor(im)\n# v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n# plt.figure(figsize=(15,7))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"panoptic\" ></a>\n## 5.2. Panoptic segmentation","metadata":{}},{"cell_type":"code","source":"# !wget http://images.cocodataset.org/val2017/000000282037.jpg -q -O input.jpg\n# im = cv2.imread(\"./input.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n# predictor = DefaultPredictor(cfg)\n# panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n# out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n# plt.figure(figsize=(25,15))\n# plt.imshow(out.get_image()[:, :, ::-1][..., ::-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"semantic\" ></a>\n## 5.3. Semantic, Densepose, ...\n\nWill be added in a future version! Stay tuned!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"video\" ></a>\n# 6. Video\n\nSo up until now we've been working with images only. Can we quickly use the models for videos? The answer is YES!\n\n<a id=\"videolib\" ></a>\n## 6.1. Libraries\nAs you can see we actually don't need many other libraries. Lets import a library to handle the video.","metadata":{}},{"cell_type":"code","source":"# from IPython.display import YouTubeVideo, display, Video # for viewing the video\n# !pip install youtube-dl # for downloading the video","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"thevideo\" ></a>\n## 6.2. The video","metadata":{}},{"cell_type":"code","source":"# #video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)#7HaJArMDKgI\n# video = YouTubeVideo(\"7HaJArMDKgI\", width=750, height= 450)#\n# display(video)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Downloading the video and cropping 6 seconds for processing\n","metadata":{}},{"cell_type":"code","source":"# !youtube-dl https://www.youtube.com/watch?v=7HaJArMDKgI -f 22 -o video.mp4\n# !ffmpeg -i video.mp4 -t 00:00:10 -c:v copy video-clip.mp4 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"videoinference\" ></a>\n## 6.3. Inference on the video\nLet's now run an panoptic model over the video above.\n\n*note: For now I'll be using some [demo](https://github.com/facebookresearch/detectron2/tree/master/demo) files, I'll later add the code implementations to this notebook.*","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/facebookresearch/detectron2\n# !python detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output 1video-output.mkv \\\n#   --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the result! \n\n*I've ran into some trouble with video encoding opencv and ffmpeg (fix in future version of this notebook).*","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/vandeveldemaarten/tempdetector2video.git\n# Video(\"./tempdetector2video/myvideo.mkv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# That's all for now!\n\nThank you for reading this notebook! If you enjoyed it, please upvote!\n\n*More coming soon!*","metadata":{"trusted":true}}]}