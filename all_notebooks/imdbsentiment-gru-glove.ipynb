{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":" # The model is for Sentiment classification on IMBD dataset.\n # It is based on neural network - GRU RNN and pre-trained GloVe word embeddings.\n # It uses tensorflow keras APIs.\n    \n # Steps:\n # Step1. load corpus data, pre-process and split into train and test dataset. [pending HTML tags removal]\n # Step2. create a vocabulary index on corpus. tokenize and vectorize corpus data.\n # Step3. load pre-trained GloVe word embeddings.\n # Step4. build and evaluate model on training, validation and test data.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import required libraries\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport pathlib\nimport codecs\nimport re\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, GRU, LSTM\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step1. load corpus data, pre-process and split into train and test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import dataset and analyze dataset\n\ndf = pd.read_csv('../input/movie-review/labelled_full_dataset.csv')\ndf.isnull().values.any()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x='label', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view one sample\n\ndf['review'][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #splitting training data into validation and training\n\n#limiting data to less values to run in less then 5 minutes\nx_train = df.loc[:9999, 'review'].values\ny_train = df.loc[:9999, 'label'].values\nx_test = df.loc[10000:12499, 'review'].values\ny_test = df.loc[10000:12499, 'label'].values\n\n# #total data split\n# x_train = df.loc[:39999, 'review'].values\n# y_train = df.loc[:39999, 'label'].values\n# x_test = df.loc[40000:, 'review'].values\n# y_test = df.loc[40000:, 'label'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(x_train), len(x_test), len(y_train), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Step2. create a vocabulary index on corpus. tokenize and vectorize corpus data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a vocabulary index\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\n#vocabulary = tokenizer.fit_on_texts(df['review'])\nvocabulary = tokenizer.fit_on_texts(x_train) \n#running tokenizer on x_train only for limiting values, else would be run on raw text data\nprint(tokenizer)\nprint(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define vocabulary size\n\nvocabulary_size_max = len(tokenizer.word_index) + 1\nprint(vocabulary_size_max)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max length for padding\n\n#max_length = max([len(s.split()) for s in df['review']])\nmax_length = max([len(s.split()) for s in x_train]) \n#limiting padding to x_train data else would be run on raw text data\nprint(max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#vectorize tokens\n\nx_train_vector = tokenizer.texts_to_sequences(x_train)\nx_test_vector = tokenizer.texts_to_sequences(x_test)\nprint(\"train vector is:\", x_train_vector[1])\nprint(\"test vector is:\", x_test_vector[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pad sequences\n#sequences shorter than the length are padded in the beginning and \n#sequences longer are truncated at the beginning.\n\nx_train_pad = pad_sequences(x_train_vector, maxlen = max_length, padding = 'post')\nx_test_pad = pad_sequences(x_test_vector, maxlen = max_length, padding = 'post')\nprint(\"train padding is:\", x_train_pad[1])\nprint(\"test padding is:\", x_test_pad[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizer.word_index.items()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Step3. load pre-trained GloVe word embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"#load pre-trained word embedding in a dictionary\n#dictionary with key = word and value = embedding in the file\n\nglove_file = '../input/glove6b50dtxt/glove.6B.50d.txt'\nembedding_dict = {}\nglove = codecs.open(glove_file, encoding = 'utf8')\nfor line in glove:\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_dict[word] = coef\nglove.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embedding_dict[\"the\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#embedding matrix with ONLY the words present in the input vocabulary i.e. corpus and\n#their corresponding embedding vector\n#vocab_size = len(token.word_index)+1\n#shape of embedding matrix: vocabulary_size_max, glove_dimension\n\nembedding_matrix = np.zeros((vocabulary_size_max,50))\nfor word,i in tokenizer.word_index.items():\n    embedding_value = embedding_dict.get(word)\n    if embedding_value is not None:\n        embedding_matrix[i] = embedding_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(embedding_matrix), embedding_matrix.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embedding_matrix[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step4. build and evaluate model on training, validation and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the pre-trained word embeddings matrix into an Embedding layer\n# Note that we set trainable=False so as to keep the embeddings fixed\n# (we don't want to update them during training).\n\nembedding_dim = 50 #dimensions of embedding layer\n\nfrom tensorflow.keras.layers import Embedding\n\nembedding_layer = Embedding(\n    input_dim = vocabulary_size_max,\n    output_dim = embedding_dim,\n    input_length = max_length,\n    embeddings_initializer= tf.keras.initializers.Constant(embedding_matrix),\n    trainable = False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model\n#GRU default with tanh activation, recurrent activation default sigmoid\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(GRU(units = 16, dropout = 0.2, recurrent_dropout = 0.2, activation = 'tanh'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # model to learn new word embeddings and NOT use pre-trained\n# #build model\n# #GRU default with tanh activation, recurrent activation default sigmoid\n# model = Sequential()\n# model.add(Embedding(input_dim = vocabulary_size_max, output_dim = embedding_dim, input_length = max_length))\n# model.add(GRU(units = 16, dropout = 0.2, recurrent_dropout = 0.2, activation = 'tanh'))\n# model.add(Dense(1, activation = 'sigmoid'))\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compile model\n\nmodel.compile(optimizer = 'adam', loss = tf.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train model\n#training on less epochs to run it in less time. ideal would be to increase epochs.\n\nhistory = model.fit(x = x_train_pad, y = y_train, batch_size = 512, epochs = 5, validation_split = 0.25, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot loss and accuracy of training and validation\n\nhistory_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate the model\n\nevaluate = model.evaluate(x = x_test_pad, y = y_test, batch_size = 512, verbose = 1, return_dict = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate.keys()\ntest_acc = evaluate['accuracy']\ntest_loss = evaluate['loss']\nprint(\"Test loss: \", test_loss*100, '%')\nprint(\"Test accuracy: \", test_acc*100, '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}