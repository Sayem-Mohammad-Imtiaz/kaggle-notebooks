{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tweepy\nfrom textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nimport  json\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/pfizer-vaccine-tweets/vaccination_tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.replace(False,0,inplace=True)\ndf.replace(True,1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['id','text','user_verified']]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Twitter Handles(@User)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_pattern(text,pattern):\n    \n    # finding the pattern\n    r = re.findall(pattern ,text)\n    \n    for i in r:\n        text = re.sub(i,'',text)\n        \n    return text    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tidy_tweets'] = np.vectorize(remove_pattern)(df['text'],'@[\\w]*')\n\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Punctuation, Numbers, and Special Characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tidy_tweets'] = df['tidy_tweets'].str.replace('[^a-zA-Z#]',' ')\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Short Words "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tidy_tweets'] = df['tidy_tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization\n*Tokenization is the process of splitting a string of text into tokens*"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokined_tweet = df['tidy_tweets'].apply(lambda x: x.split())\ntokined_tweet.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming\n\nStemmming is a rule based process of stripping the suffixes('ing','es' etc.)from a word\n\nfi=or example - 'play','player','played','plays','playing'are different variation of 'play'"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import PorterStemmer\n\nps = PorterStemmer()\n\ntokenized_tweet = tokined_tweet.apply(lambda x: [ps.stem(i) for i in x])\n\ntokenized_tweet.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Switching these tokens back together*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ''.join(tokenized_tweet[i])\n    \n    \ndf['tidy_tweets'] = tokenized_tweet\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\nimport urllib\nimport requests","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_verified_user = ''.join(text for text in df['tidy_tweets'][combine['user_verified']==0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine the image with dataset\nMask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n\nimage_colors = ImageColorGenerator(Mask)\n\nwc = WordCloud(background_color='black', height=1500, width=5000, mask=Mask).generate(non_verified_user)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,20))\n\nplt.imshow(wc.recolor(color_func=image_colors), interpolation='hamming')\n\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"verified_user = ''.join(text for text in df['tidy_tweets'][combine['user_verified']==1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combine the image with dataset\nMask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n\nimage_colors = ImageColorGenerator(Mask)\n\nwc = WordCloud(background_color='black', height=1500, width=4000, mask=Mask).generate(verified_user)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,20))\n\nplt.imshow(wc.recolor(color_func=image_colors), interpolation='hamming')\n\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Impact of Hashtag "},{"metadata":{"trusted":true},"cell_type":"code","source":"def Hastags_Extract(x):\n    hashtags = []\n    \n    for i in x:\n        ht = re.findall(r'#(\\w+)',i)\n        hashtags.append(ht)\n        \n    return hashtags    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ht_positive = Hastags_Extract(df['tidy_tweets'][df['user_verified']==1])\n\nht_positive","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# List Unnestting"},{"metadata":{"trusted":true},"cell_type":"code","source":"ht_positive_unnest = sum(ht_positive,[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ht_positive_unnest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A nested list of all hastags from non_verified_user"},{"metadata":{"trusted":true},"cell_type":"code","source":"ht_negative = Hastags_Extract(df['tidy_tweets'][df['user_verified']==0])\n\nht_negative","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ht_negative_unnest = sum(ht_negative,[])\nht_negative_unnest\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Counting Frequency of words by verified_user"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq_positive = nltk.FreqDist(ht_positive_unnest)\n\nword_freq_positive","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataframe of most frequently words by verified_user"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_positive  = pd.DataFrame({'Hashtags':list(word_freq_positive.keys()),'Count':list(word_freq_positive.values())})\ndf_positive.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Barplot for the 20 most frequent words used for hashtags"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ndf_positive_plot = df_positive.nlargest(20,columns='Count')\n\nsns.barplot(data = df_positive_plot, y='Hashtags',x='Count')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Similarly for non_verified_user"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq_negative = nltk.FreqDist(ht_negative_unnest)\n\nword_freq_negative\n\ndf_negative  = pd.DataFrame({'Hashtags':list(word_freq_negative.keys()),'Count':list(word_freq_negative.values())})\ndf_negative.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_negative_plot = df_negative.nlargest(20,columns='Count')\n\nsns.barplot(data = df_negative_plot, y='Hashtags',x='Count')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nbow_vectorizer = CountVectorizer(max_df = 0.90, min_df = 2, max_features=325, stop_words='english')\n\nbow = bow_vectorizer.fit_transform(combine['tidy_tweets'])\ndf_bow = pd.DataFrame(bow.todense())\ndf_bow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bow = bow\ntrain_bow.todense()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Refrences\n# Towards Data Science Newsletter\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}