{"cells":[{"metadata":{},"cell_type":"markdown","source":"## __The importance of insulin__\n\n- Diabetes is a disease in which your body either can't produce insulin or can't properly use the insulin it produces. Insulin is a hormone produced by your pancreas.\n\n- Insulin's role is to regulate the amount of glucose (sugar) in the blood. Blood sugar must be carefully regulated to ensure that the body functions properly. Too much blood sugar can cause damage to organs, blood vessels, and nerves. Your body also needs insulin in order to use sugar for energy.\n\nThere are generally 2 types of diabetes:\n\n1. Type 1 diabetes is an autoimmune disease and is also known as insulin-dependent diabetes. People with type 1 diabetes aren't able to produce their own insulin (and can't regulate their blood sugar) because their body is attacking the pancreas. Roughly 10 per cent of people living with diabetes have type 1, insulin-dependent diabetes. Type 1 diabetes generally develops in childhood or adolescence, but can also develop in adulthood. People with type 1 need to inject insulin or use an insulin pump to ensure their bodies have the right amount of insulin. \n\n\n2. People with type 2 diabetes can't properly use the insulin made by their bodies, or their bodies aren't able to produce enough insulin. Roughly 90 per cent of people living with diabetes have type 2 diabetes.Type 2 diabetes is most commonly developed in adulthood, although it can also occur in childhood. Type 2 diabetes can sometimes be managed with healthy eating and regular exercise alone, but may also require medications or insulin therapy.  \n\n(Source: https://www.diabetes.ca/diabetes-basics/what-is-diabetes)"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Read in the dataset, and giving back their headers\ndata = pd.read_csv('../input/pima-indians-diabetes.csv', header = None, names = \n                   ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n                    'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- No missing values, 768 entries, all numbers\n- Class is the target variable"},{"metadata":{},"cell_type":"markdown","source":"## __Heat Map__"},{"metadata":{"trusted":false},"cell_type":"code","source":"colormap = plt.cm.inferno\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Take aways from the heatmap:__\n- Glucose has the highest correlation with our target variable. High glucose would likely cause diabetes\n- BMI, Age and Pregnancies has the secondary highest correlation with target variable\n- However, age is relatively highly correlated with number of times of pregnancies as people ages they will be able to pregenant more times\n- Insulin and skinthickness also has relatively high correlation"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Separate the predictors and response variables\ny = data['Class'].ravel()\nX = data.drop(['Class'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Defining some variables\nSEED = 2019 # for reproducibility\nNFOLDS = 3\nkfold = KFold(n_splits = NFOLDS, random_state = SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Separate training and testing dataset\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ntrain = x_train.shape[0]  #Number of rows per training column (recall: train.shape = (3, 4), then shape[0] is 3)\nntest = x_test.shape[0]  #testing column\nx_train = x_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))  #Establish an array of 0s that has the same length of the training data\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))  #Establish an array consist of 5 rows and number of test columns\n\n    for i, (train_index, test_index) in enumerate(kfold.split(x_train)):  #Elocate the splited data into their dataset\n        x_tr = x_train[train_index]   #train index of train dataset\n        y_tr = y_train[train_index]   #train index of test dataset\n        x_te = x_train[test_index]  #test index of train dataset\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 300,\n     'warm_start': True, \n     'max_features': 0.7,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':300,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.1\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     'max_features': 0.7,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking feature importance\nrf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now plotling the heatmap of our second level models:\nWe want them to be as diverse as possible"},{"metadata":{"trusted":false},"cell_type":"code","source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data = [\n    go.Heatmap(\n        z = base_predictions_train.astype(float).corr().values ,\n        x = base_predictions_train.columns.values,\n        y = base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gbm = xgb.XGBClassifier(learning_rate = 0.02,\n     n_estimators= 2000,\n     max_depth= 4,\n     min_child_weight= 2,\n     #gamma=1,\n     gamma=0.9,                        \n     subsample=0.8,\n     colsample_bytree=0.8,\n     objective= 'binary:logistic',\n     nthread= -1,\n     scale_pos_weight=1).fit(x_train, y_train)\n\npredictions = gbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}