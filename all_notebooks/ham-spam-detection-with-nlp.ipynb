{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference\n* [Fares Sayah - Natural Language Processing (NLP) 🧾 for Beginners](https://www.kaggle.com/faressayah/natural-language-processing-nlp-for-beginners)\n* [Ravi Chaubey - Natural Language Processing with Python](https://www.kaggle.com/ravichaubey1506/natural-language-processing-with-python)\n* [Madz2000 - Simple EDA with Data Cleaning & GloVe(98%Accuracy)](https://www.kaggle.com/madz2000/simple-eda-with-data-cleaning-glove-98-accuracy)\n* [adityapatil - Spam detector using NLP and Random Forest](https://www.kaggle.com/adityapatil673/spam-detector-using-nlp-and-random-forest)\n* [Shekhar - Spam Detection using NLP and Random Forest](https://www.kaggle.com/shekhart47/spam-detection-using-nlp-and-random-forest)"},{"metadata":{},"cell_type":"markdown","source":"# Dataset overview    \n* 探索式資料分析 (EDA)\n    * EDA (Exploratory Data Analysis) uses **visualization** and **basic statistics** to get an overview of the data we have, in order to do more complicated and thorough analysis to it.\n    * EDA should let us be able to achieve the following three main things:\n        1. To Know the Data - what information does the data provide, the structure of the data, etc.\n        2. Check the Data - if there’s any outliers or unusual value.\n        3. Correlation between Data - find out important variables.\n    * We can also check if the data meet our assumption of it and figure out latent errors before actually building the model, so as to do adjusts for the further analysis."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading a text-based dataset into pandas\ndef readData_rawSMS(filepath):\n    data_rawSMS = pd.read_csv(filepath, header=0, usecols=[0,1], encoding='latin-1')\n    data_rawSMS.columns = ['label', 'content']\n    return data_rawSMS\n\ndata_rawSMS = readData_rawSMS(os.path.join(dirname, filename))\ndata_rawSMS.head()\n\n# 垃圾訊息(spam) / 有效訊息(ham)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 定義 `readData_rawSMS` function\n    * header\n        * `0`: 第一列(橫)為欄位名稱\n            > 即 v1, v2\n        * `1`: 第二列(橫)為欄位名稱\n            > 即 ham, Go until jurong point, crazy.. Available only ...\n        * `None`: 本資料(spam.csv)沒有欄位名稱\n    * usecols\n        * `[0,1]`: 僅使用第一行(直)和第二行(直)的資料，其他行(直)略過不讀取且不使用。\n    * data_rawSMS.columns = ['label', 'content']\n        > 重新命名欄位名稱：由 `v1, v2` 改為 `label, content`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate descriptive statistics\ndata_rawSMS.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group large amounts of data and compute operations on these groups\ndata_rawSMS.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a new column `length` to detect how long the content are.\ndata_rawSMS['length'] = data_rawSMS['content'].apply(len)\ndata_rawSMS.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `data_rawSMS['length'] = data_rawSMS['content'].apply(len)`\n    * 使用 len 函數 計算 `data_rawSMS` DataFrame 的 `content`，並將結果給予另增的欄位 `length`。\n* `DataFrame.apply(func, axis=0)`\n    * Apply a function along an axis of the DataFrame.\n    * Example: `df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])`\n        ```\n           A  B\n        0  4  9\n        1  4  9\n        2  4  9\n        ```\n        * df.apply(np.sum, axis=0)\n            ```\n            A    12\n            B    27\n            dtype: int64\n            ```\n        * df.apply(np.sum, axis=1)\n            ```\n            0    13\n            1    13\n            2    13\n            dtype: int64\n            ```"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS.groupby('label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS.label=='ham']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"ggplot\")\n\nplt.figure(figsize=(6, 4))\n\ndata_rawSMS[data_rawSMS.label=='ham'].length.plot(\n    bins=35, kind='hist', color='blue', \n    label='Ham messages', alpha=0.6)\ndata_rawSMS[data_rawSMS.label=='spam'].length.plot(\n    kind='hist', color='red', \n    label='Spam messages', alpha=0.6)\n\nplt.legend()\nplt.xlabel(\"Message Length\")\n#Through just basic EDA we've been able to discover a trend that `spam messages` tend to have `more characters`.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nDataFrame.plot(x=None, y=None, kind='line', ax=None, subplots=False, sharex=None, sharey=False, layout=None, figsize=None, \n    use_index=True, title=None, grid=None, legend=True, style=None, logx=False, logy=False, loglog=False, \n    xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, \n    yerr=None, xerr=None, secondary_y=False, sort_columns=False, **kwds)\n```\n* bins: int or sequence or str, default: rcParams[\"hist.bins\"] (default: 10)\n    * 把分佈(長條)切成 N 等分，N 預設為 10。\n        > 即長條總共有 10 個。\n* legend: Place legend on axis subplots\n    * 放置圖例"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS.length.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS.label=='ham'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS['length'] == 910]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_rawSMS[data_rawSMS['length'] == 910]['content'].iloc[0]\n# data_rawSMS[data_rawSMS.length == 910].content.iloc[0]     #same result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* https://ithelp.ithome.com.tw/articles/10194006\n* `pandas.DataFrame.iloc`\n    * Purely integer-location based indexing for selection by position. (用 index 位置來取我們要的資料)\n* `pandas.DataFrame.loc`\n    * Access a group of rows and columns by label(s) or a boolean array. (用 標籤 來取出資料)"},{"metadata":{},"cell_type":"markdown","source":"# Text Pre-processing\n* The simplest is the `詞袋模型 (Bag-of-words Model, BoW)` approach, where each unique word in a text will be represented by one number.\n    ![](https://miro.medium.com/max/875/1*ujkZ3JrQ6ubSuEpepHE4Aw.png)\n    > [NLP 入門 (1) — Text Classification (Sentiment Analysis) — 極簡易情感分類器 Bag of words + Naive Bayes](https://sfhsu29.medium.com/nlp-%E5%85%A5%E9%96%80-1-text-classification-sentiment-analysis-%E6%A5%B5%E7%B0%A1%E6%98%93%E6%83%85%E6%84%9F%E5%88%86%E9%A1%9E%E5%99%A8-bag-of-words-naive-bayes-e40d61de9a7f)\n    \n    * 如何利用 bag-of-words 方法將文字轉換成數字？\n        > [NLP的基本執行步驟(II) — Bag of Words 詞袋語言模型](https://medium.com/@derekliao_62575/nlp%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%9F%B7%E8%A1%8C%E6%AD%A5%E9%A9%9F-ii-bag-of-words-%E8%A9%9E%E8%A2%8B%E8%AA%9E%E8%A8%80%E6%A8%A1%E5%9E%8B-3b670a0c7009)\n        * Example: \n            * a. 看到他我就不爽。(先斷詞) => 看到/他/我/就/不爽\n            * b. 看到他我就火大。(先斷詞) => 看到/他/我/就/火大\n        * 這個語料庫的詞袋長這樣：(就 ,他 , 看到, 我 , 火大, 不爽)\n            * Notice! \n                * 詞袋裡的詞「沒有絕對的順序關係」，此為隨機排法。\n        * 接著，如果一個詞在句子中有出現，我們就幫他做一個記號，如下：\n            * a. 表示成：[1, 1, 1, 1, 0, 1] (a 句沒有「火大」，標示為 0)\n            * b. 表示成：[1, 1, 1, 1, 1, 0] (b 句沒有「不爽」，標示為 0)\n        * 像這樣用 1 跟 0 來表示句子中詞語有沒有出現的方式，它有個酷炫名字：『獨熱編碼 (One-hot encoding)』\n            * 利用獨熱編碼的方式，一個句子可以被轉換成一個向量的形式表達(向量 \"vector\" 就是一列數字而已)，就可以達成簡單的文字轉換成數字。\n    * BoW 的衍生模型：\n        * TF-IDF\n            * 獲取一個能代表一個詞在文件中重要程度的數值。\n        * CBoW (Continuous Bag of Words, 連續詞袋模型)\n            * 這個模型是一個淺層的類神經網路。\n            * 相較於傳統詞袋模型，CBoW 的輸入一樣是獨熱的形式，但不同的點是，CBoW 模型會將一開始每個詞都透過中間的隱藏層作轉換，讓每個詞的詞向量中不再只包含0與1，而是有意義的數值。\n                * 中間的隱藏層轉換是怎麼進行的？\n                    * CBoW 會同時參考**一個詞前後的語境**來決定那個詞所代表的詞向量是什麼。\n                        > Ex:「不爽」和「火大」前面所接的詞都是「看到」、「他」、「我」、「就」\n                        > 因此模型就會判斷：「不爽」和「火大」可能表現出相似的語意和句法結構，這個兩個詞就會被賦予非常接近的詞向量。\n        * Word2vec\n            * 由「連續詞袋模型 CBoW」和「跳字模型 skip-gram」構成 word2vec 模型。\n            * 由 Google 的 Mikolov 等人在 2013 年提出。"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nprint(stopwords.words('english'))\n\nimport string\n\nprint(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n    \n    # Check characters to see if they are in punctuation\n    # 非標點符號的 characters，就存進 list\n    nopunc = [char for char in mess if char not in string.punctuation] \n    \n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    # 非停用字的 word，轉成小寫字母後就存進 list\n    return ' '.join([word.lower() for word in nopunc.split() if word.lower() not in STOPWORDS]) \n\ndata_rawSMS['clean_msg'] = data_rawSMS.content.apply(text_process)\ndata_rawSMS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unigram Analysis\nfrom collections import Counter\n\ndef get_words(content):\n    words = []\n    for row in content:\n        for j in row.split():\n            words.append(j.strip())\n    return words\n\ncounter = Counter(get_words(data_rawSMS['clean_msg']))\nmost_common = dict(counter.most_common(20))\nprint(most_common)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unigram Analysis\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.barplot(x=list(most_common.values()), y=list(most_common.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nwords = data_rawSMS[data_rawSMS.label=='ham'].clean_msg.apply(lambda x: [word for word in x.split()])\nham_words = Counter()\n\nfor msg in words:\n    ham_words.update(msg)\n    \nprint(ham_words.most_common(50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nwords = data_rawSMS[data_rawSMS.label=='spam'].clean_msg.apply(lambda x: [word for word in x.split()])\nspam_words = Counter()\n\nfor msg in words:\n    spam_words.update(msg)\n    \nprint(spam_words.most_common(50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Example] Vectorization\n* We'll do that in 3 steps using the `bag-of-words model`:\n  1. Count how many times does a word occur in each message (**term frequency**)\n  2. Weigh the counts, so that frequent tokens get lower weight (**inverse document frequency**)\n  3. Normalize the vectors to unit length, to abstract from the original text length (**L2 norm**)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use `CountVectorizer` to \"convert text into a matrix of token counts\"\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n# The raw data, a sequence of symbols cannot be fed directly to the machine learning algorithms.\n# They require numerical feature vectors with a fixed size.\nsimple_train = ['call you tonight call', 'Call me a cab', 'Please call me... PLEASE!']\n\n# 1. import and instantiate CountVectorizer (with the default parameters)\nvect = CountVectorizer()\n\n# 2. learn the 'vocabulary' of the training data (occurs in-place)\nvect.fit(simple_train)\n\n# 3. examine the fitted vocabulary\nprint(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(simple_train)\nprint(simple_train_dtm)\n\"\"\"\n   0       1      2       3        4         5\n['cab', 'call', 'me', 'please', 'tonight', 'you'] = vect.get_feature_names()\n\n-> 'call you tonight'\n->   0    5     4\n\n(docID, wordID)  word count\n  (0, 1)\t1\n  (0, 4)\t1\n  (0, 5)\t1\n  \nindex  0 1 2 3 4 5\n====> [0 1 0 0 1 1]\n\"\"\"\nprint()\nprint(simple_train_dtm.toarray())\n\n# 5. examine the vocabulary and document-term matrix together\npd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. example text for model testing\nsimple_test = [\"please don't call me\"]\n\n# 7. transform testing data into a document-term matrix (using existing vocabulary)\nsimple_test_dtm = vect.transform(simple_test)\n\"\"\"\n   0       1      2       3        4         5\n['cab', 'call', 'me', 'please', 'tonight', 'you'] = vect.get_feature_names()\n\n-> \"please don't call me\"\n->     3    x     1   2\nvector -> [[0, 1, 1, 1, 0 , 0]]\n\"\"\"\n\npd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning workflow with Vectorization\n## 1. Divided into training set and testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert label to a numerical variable\ndata_rawSMS['label_num'] = data_rawSMS.label.map({'ham':0, 'spam':1})\ndata_rawSMS.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data_rawSMS.clean_msg\ny = data_rawSMS.label_num\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0, train_size=0.8)\n\nprint('【Training set】')\nprint('Row_count: {}\\n\\nData content:\\n{}\\n'.format(X_train.shape, X_train))\nprint('Row_count: {}\\n\\nData label:\\n{}\\n'.format(y_train.shape, y_train))\nprint('------------')\nprint('【Testing set】')\nprint('Row_count: {}\\n\\nData:\\n{}\\n'.format(X_test.shape, X_test))\nprint('Row_count: {}\\n\\nData label:\\n{}\\n'.format(y_test.shape, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `train_test_split()`\n    1. `random_state`\n        * This ensures that if I have to rerun my code, I’ll get the exact same train-test split, so my results won’t change.\n    2. `stratify=y`\n        * This tells train_test_split to make sure that the training and test datasets contain examples of each class **in the same proportions as in the original dataset**. \n        * 由於 classes 的不平衡性(imbalanced)，因此特別重要！\n        * 若完全隨機地拆成 Train 和 Test，容易造成某個小類別在 Test 有、但在 Train 沒有的情況發生，使得 model 無法辨識那個小類別(因為 Train 沒有，所以沒辦法學習)！\n            > A random split could easily end up with all examples of the smallest class in the test set and none in the training set, and then the model would be unable to identify that class."},{"metadata":{},"cell_type":"markdown","source":"## 2. Vectorization (+ Feature Engineering)\n### Training data\n* CountVectorizer\n* TfidfTransformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Method1 ###\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\nvect.fit(X_train)\n# print(len(vect.get_feature_names()))\n# print(vect.get_feature_names())\n\n\n# learn training data vocabulary, then use it to create a document-term matrix\nX_train_dtm = vect.transform(X_train)\n\n# word's TF\npd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Method2 ###\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\n# equivalently: combine fit and transform into a single step\nX_train_dtm = vect.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\npd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = \"{:,.10f}\".format\n\n# word's TF-IDF weight\nTFIDF = tfidf_transformer.transform(X_train_dtm).toarray()\npd.DataFrame(TFIDF, columns=vect.get_feature_names()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(tfidf_transformer.transform(X_train_dtm).toarray(), columns=vect.get_feature_names()) #.iloc[4170]     #TF-IDF (L2 norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF Explain"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names()) #['ìï'].iloc[4170]\ndf.loc[df['ìï'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TF = pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())['ìï'].iloc[4170]\nTF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())['ìï'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.float_format = \"{:,.4f}\".format\n\n# word's IDF weight\npd.DataFrame([tfidf_transformer.idf_], columns=vect.get_feature_names()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(vect.get_feature_names())):\n    if 'ìï' == vect.get_feature_names()[i]:\n        print(i)\n        # 'ìï' => index 8147","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the vocabulary and document-term matrix together\npd.DataFrame(tfidf_transformer.transform(X_train_dtm).toarray(), columns=vect.get_feature_names()).iloc[4170]\n\n\n\n### TF-IDF (L2 norm) ###\n# ìï             0.4197\n\n### TF ###\n# \"ìï\" 在第 4170 篇，僅出現一次。\n\n### IDF ###\n# import math\n# TF = 1\n# DF = 36\n# total_Doc = 4179\n# math.log((4179 + 1)/(36 + 1)) + 1 = 5.727148612874577","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing data\n* TfidfTransformer\n    * transform testing data (using fitted vocabulary) into a document-term matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# --- start --- Use Method2 --- #\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nvect = CountVectorizer()\nX_train_dtm = vect.fit_transform(X_train)\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(X_train_dtm)\ntfidf_transformer.transform(X_train_dtm)\n# --- end --- Use Method2 --- #\n\n\nX_test_dtm = vect.transform(X_test)\nprint(X_test_dtm.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Building and Evaluating a model (依據特徵資料訓練分類器)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=20, random_state=0)\n\n# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n%time clf = rf.fit(X_train_dtm, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\n# Extract single tree\nestimator = rf.estimators_[5]\n\nn_nodes = rf.estimators_[4].tree_.node_count\nprint(estimator, n_nodes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n\n# rf = RandomForestClassifier(n_estimators=20, oob_score=True, random_state=0)\n\n# # train the model using X_train_dtm (timing it with an IPython \"magic command\")\n# %time clf = rf.fit(X_train_dtm, y_train)\n\n# ### OOB ###\n# print(rf.oob_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rf.feature_importances_.shape\n# rf.feature_importances_\n\n\ndf = pd.DataFrame([rf.feature_importances_], columns=vect.get_feature_names())\ndf.nlargest(20, 'ìï')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate (Testing)\n* accuracy_score\n* precision_score\n* recall_score\n* f1_score"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\n# calculate accuracy of class predictions\nprint(\"【 Testing 】\")\nprint('Accuracy score: {}'.format(accuracy_score(y_test, y_predTest_class)))\nprint('Precision score: {}'.format(precision_score(y_test, y_predTest_class)))\nprint('Recall score: {}'.format(recall_score(y_test, y_predTest_class)))\nprint('F1 score: {}'.format(f1_score(y_test, y_predTest_class)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### 補充：calculate AUC (機器學習的效能衡量指標) ###\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, clf.predict_proba(X_test_dtm)[:,1])\n\n\n# 當AUC = 1時，代表分類器非常完美，但這畢竟是理想狀況。\n# 當AUC > 0.5時，代表分類器分類效果優於隨機猜測，模型有預測價值。\n# https://ithelp.ithome.com.tw/articles/10229049\n# https://medium.com/marketingdatascience/%E5%88%86%E9%A1%9E%E5%99%A8%E8%A9%95%E4%BC%B0%E6%96%B9%E6%B3%95-roc%E6%9B%B2%E7%B7%9A-auc-accuracy-pr%E6%9B%B2%E7%B7%9A-d3a39977022c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, y_predTest_class))\nprint()\nprint(confusion_matrix(y_test, y_predTest_class))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_test, y_predTest_class), index=['Ham','Spam'] , columns=['Ham','Spam'])\nplt.figure(figsize=(3,3))\nsns.heatmap(cm, cmap=\"Blues\", linecolor='black', linewidth=1, annot=True, fmt='', xticklabels=['Ham','Spam'], yticklabels=['Ham','Spam'])\n\n# x 軸：預測(Predict)\n# y 軸：實際(Actual)\n\n### Type I error  (嚴重) ###    Predict: Ham (0) & Actual: Spam (1)\nprint(len(X_test[y_predTest_class < y_test]), '個\\t=> false negatives (spam incorrectly classifier)\\n') # X_test[y_predTest_class < y_test]\n\n### Type II error  (輕微) ###   Predict: Spam (1) & Actual: Ham (0)\nprint(len(X_test[y_predTest_class > y_test]), '個\\t=> false positives (ham incorrectly classifier)\\n') # X_test[y_predTest_class > y_test]\nprint(X_test[(y_predTest_class==1) & (y_test==0)]) # same result\nprint()\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support as score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\nprecision, recall, fscore, support = score(y_test, y_predTest_class, pos_label=0, average='binary')\nprint('Precision : {} / Recall : {} / fscore : {} / Accuracy: {}'.format(round(precision,3),round(recall,3),round(fscore,3),round((y_predTest_class==y_test).sum()/len(y_test),3)))\n\nprecision, recall, fscore, support = score(y_test, y_predTest_class, pos_label=1, average='binary')\nprint('Precision : {} / Recall : {} / fscore : {} / Accuracy: {}'.format(round(precision,3),round(recall,3),round(fscore,3),round((y_predTest_class==y_test).sum()/len(y_test),3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Input SMS and Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# vect = CountVectorizer()\n# X_train_dtm = vect.fit_transform(X_train)\n\n# tfidf_transformer = TfidfTransformer()\n# tfidf_transformer.fit(X_train_dtm)\n# tfidf_transformer.transform(X_train_dtm)\n    \n# rf = RandomForestClassifier() #n_estimators=20\n# rf.fit(X_train_dtm, y_train)\n\nSMS = 'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.'\nclean_text = text_process(SMS)\n# print(clean_text)\nsimple_test_dtm = vect.transform([clean_text])\nprint(simple_test_dtm.toarray(), simple_test_dtm.reshape(1,-1).shape)\n\ny_predSimpleTest_class = rf.predict(simple_test_dtm.reshape(1,-1))\nif int(y_predSimpleTest_class) == 1:\n    print ('SPAM: {}'.format(SMS))\nelse:\n    print ('ham: {}'.format(SMS))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Pipeline (補充) ###\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import metrics\n\nX = data_rawSMS.clean_msg\ny = data_rawSMS.label_num\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n\npipe = Pipeline([('bow', CountVectorizer()),\n                 ('tfid', TfidfTransformer()),  \n                 ('model', RandomForestClassifier(n_estimators=20, bootstrap=True, oob_score=False, random_state=1))])\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)\n\ny_pred = pipe.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))\nprint(metrics.confusion_matrix(y_test, y_pred))\n\n\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# make class predictions for X_test_dtm\ny_predTest_class = rf.predict(X_test_dtm)\n\n# calculate accuracy of class predictions\nprint(\"【 Testing 】\")\nprint('Accuracy score: {}'.format(metrics.accuracy_score(y_test, y_pred)))\nprint('Precision score: {}'.format(metrics.precision_score(y_test, y_pred)))\nprint('Recall score: {}'.format(metrics.recall_score(y_test, y_pred)))\nprint('F1 score: {}'.format(metrics.f1_score(y_test, y_pred)))\n\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}