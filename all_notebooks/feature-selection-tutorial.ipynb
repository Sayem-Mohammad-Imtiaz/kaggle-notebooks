{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"When creating machine learning models, the perfomance of the model will not always increase on creating new features. We  all may have faced this problem of identifying the good features from the set of features we have or the features we created. Feature selection techniques will comes to rescue in this case. It is one of the core concepts in machine learning which hugely impacts the performance of your model. \n\nThe data features that you use to train your machine learning models have a huge influence on the performance you can achieve.Irrelevant or partially relevant features can negatively impact model performance.Feature selection and Data cleaning should be the first and most important step of your model designing.\n\n\n**Advantages of Feature selection:**\n\n<b>Reduces Overfitting:</b> Less redundant data means less opportunity to make decisions based on noise.<br>\n\n<b>Improves Accuracy:</b>Less misleading data means modeling accuracy improves.<br>\n\n<b>Reduces Training Time:</b> fewer data points reduce algorithm complexity and algorithms train faster.<br>\n\nIn this notebook, we will familirize with some of the commonly used feature selection techniques.\n\n**1. Filter methods**\n```\n    - chi2 test\n    - Anova F test\n    - Using Pearsons coorelation matrix\n```\n**2. Wrapper methods**\n``` \n    - Forward feature selection\n    - Backward selection\n    - Recursive feature elimination\n```\n**3. Embeddeded methods**\n```   \n    - Lasso\n    - Ridge\n    - Elastic net\n```   ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import auc,roc_auc_score,roc_curve\nfrom sklearn.model_selection import GridSearchCV","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T02:59:08.142152Z","iopub.execute_input":"2021-06-02T02:59:08.142577Z","iopub.status.idle":"2021-06-02T02:59:09.436722Z","shell.execute_reply.started":"2021-06-02T02:59:08.142491Z","shell.execute_reply":"2021-06-02T02:59:09.435731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## train test file path\ndata = '../input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n\ndf = pd.read_csv(data)\nprint(df.shape)\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:59:09.438332Z","iopub.execute_input":"2021-06-02T02:59:09.438658Z","iopub.status.idle":"2021-06-02T02:59:09.551075Z","shell.execute_reply.started":"2021-06-02T02:59:09.43863Z","shell.execute_reply":"2021-06-02T02:59:09.550123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:59:09.552042Z","iopub.execute_input":"2021-06-02T02:59:09.552324Z","iopub.status.idle":"2021-06-02T02:59:09.583314Z","shell.execute_reply.started":"2021-06-02T02:59:09.552297Z","shell.execute_reply":"2021-06-02T02:59:09.582242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#basic EDA\n\n\ndf = df.drop(columns=['customerID'])\ndf['TotalCharges'] = df['TotalCharges'].apply(lambda x: -1 if x == ' ' else float(x))\ndf['TotalCharges'] = df['TotalCharges'].replace(-1,df['TotalCharges'].mean())\n\n\nnum_cols = ['TotalCharges','MonthlyCharges','tenure']\nfor col in num_cols:\n    df[col] = df[col].astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:59:14.229584Z","iopub.execute_input":"2021-06-02T02:59:14.229975Z","iopub.status.idle":"2021-06-02T02:59:14.249106Z","shell.execute_reply.started":"2021-06-02T02:59:14.229939Z","shell.execute_reply":"2021-06-02T02:59:14.248104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [1]Filter Methods","metadata":{}},{"cell_type":"markdown","source":"Most of the people prefer to use warpper methods like forward feature selection,backward elimenation etc for feature selection, but while doing EDA, while proceeding to next step the easiest way to do feature selection is using univariate methods like ch2 test, ANOVA test, using coorelation matrix etc.","metadata":{}},{"cell_type":"markdown","source":"## [1.1] Chi square test (For categorical data)\n\nIn the case of classification problems where input variables are also categorical, we can use statistical tests to determine whether the output variable is dependent or independent of the input variables. If independent, then the input variable is a candidate for a feature that may be irrelevant to the problem and removed from the dataset. The Pearsonâ€™s chi-squared statistical hypothesis is an example of a test for independence between categorical variables.\n\nChi2 test can be used to know the feature importance of categorical variables in classification problems.Basically it will find wheather a relationship exist or there is dependency between two features. Let us take some categorical variables and see how it performs","metadata":{}},{"cell_type":"code","source":"df['SeniorCitizen'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:59:19.362315Z","iopub.execute_input":"2021-06-02T02:59:19.36268Z","iopub.status.idle":"2021-06-02T02:59:19.370432Z","shell.execute_reply.started":"2021-06-02T02:59:19.362649Z","shell.execute_reply":"2021-06-02T02:59:19.369488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. Design the hypothesis**\n\n**Null hypothesis (H0) : Two features are independend**\n\n**Alternate hypothesis (H1): Two features are dependent**\n\nIf we proved that input and target variables are independent, then there is no strong relation with the target and we can remove that feature on moving forward.\n\n**2. Choose a alpha value**\n\nWe need to choose a alpha-value (significance value) which indicates how confident are we in saying two features are independent.Here we choose alpha value = 0.05 which indicates the probability of rejecting null hypothesis if it is true.\n\nIf the p value obtained is greater than alpha value the null hypotheis (H0) is true.","metadata":{}},{"cell_type":"code","source":"# contigency table\nalpha = 0.05\ncont_table = pd.crosstab(index=df['SeniorCitizen'],columns=df['Churn'])\ncont_table","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:59:44.418573Z","iopub.execute_input":"2021-06-02T02:59:44.418924Z","iopub.status.idle":"2021-06-02T02:59:44.465268Z","shell.execute_reply.started":"2021-06-02T02:59:44.418896Z","shell.execute_reply":"2021-06-02T02:59:44.464211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import chi2_contingency,chi2\n\n# chi2 value, p value, degree of freedom , expected_table\nchi2_value, p, dof, expected_table = chi2_contingency(cont_table)\n\nprint(f'chi2 value: {chi2_value}')\nprint(f'p value: {p}')\nprint(f'degree of freedom: {dof}')\nprint(f'expected table/array : \\n {expected_table}')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:59:56.09931Z","iopub.execute_input":"2021-06-02T02:59:56.099789Z","iopub.status.idle":"2021-06-02T02:59:56.110225Z","shell.execute_reply.started":"2021-06-02T02:59:56.099758Z","shell.execute_reply":"2021-06-02T02:59:56.109303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## calculated value of chi2 >= crirical value from table(found using dof and alpha) --> Ho is rejected\n## ie,  abs(ch2_value) > chi2.ppf(0.95, dof) -->Ho rejected\n\nif p <= alpha:\n    print(f'Reject null hypothesis. There exist some relation between features')\nelse:\n    print(f'Accept null hypothesis. Two features are not related')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T02:59:59.218644Z","iopub.execute_input":"2021-06-02T02:59:59.219026Z","iopub.status.idle":"2021-06-02T02:59:59.224774Z","shell.execute_reply.started":"2021-06-02T02:59:59.21899Z","shell.execute_reply":"2021-06-02T02:59:59.223352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we rejected null hypothesis which means features are not independent. There exist some relationship between SeniorCitizen and Churn prediction.","metadata":{}},{"cell_type":"markdown","source":"Note: Usually in hypothesis testing values below critical value are acepted and values above it are rejected. (one tail test)","metadata":{}},{"cell_type":"code","source":"def chi2_test(X,target,alpha=0.05):\n    \"\"\"\n    X = input dataframe\n    target= target frame\n    alpha = significant value\n    \"\"\"\n    useful_cols = {}\n    for col in X.columns: \n        cont_table = pd.crosstab(index=X[col],columns=target)\n        chi2_value, p, dof, expected_table = chi2_contingency(cont_table)\n        if p <= alpha:\n            # reject null hypothesis # so, important feature\n            useful_cols[col] = p\n    print(f'Total {len(useful_cols)} features selected')\n    return useful_cols\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:00:16.950649Z","iopub.execute_input":"2021-06-02T03:00:16.951018Z","iopub.status.idle":"2021-06-02T03:00:16.9575Z","shell.execute_reply.started":"2021-06-02T03:00:16.950985Z","shell.execute_reply":"2021-06-02T03:00:16.956516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chi2_test(df[['gender','SeniorCitizen', 'Partner','PhoneService','PaperlessBilling']],df['Churn'])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:00:19.677545Z","iopub.execute_input":"2021-06-02T03:00:19.677901Z","iopub.status.idle":"2021-06-02T03:00:19.747244Z","shell.execute_reply.started":"2021-06-02T03:00:19.677857Z","shell.execute_reply":"2021-06-02T03:00:19.74648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Sklearn library","metadata":{}},{"cell_type":"markdown","source":"First we have to label encode categorical features","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest,chi2\n\ndff = df[['gender','SeniorCitizen', 'Partner','PhoneService','PaperlessBilling','Churn']]\n\n# label encod cat features\ndff['gender'] = dff['gender'].map({v:i for i,v in enumerate(dff['gender'].value_counts().index)})\ndff['SeniorCitizen'] = dff['SeniorCitizen'].map({v:i for i,v in enumerate(dff['SeniorCitizen'].value_counts().index)})\ndff['Partner'] = dff['Partner'].map({v:i for i,v in enumerate(dff['Partner'].value_counts().index)})\ndff['PhoneService'] = dff['PhoneService'].map({v:i for i,v in enumerate(dff['PhoneService'].value_counts().index)})\ndff['PaperlessBilling'] = dff['PaperlessBilling'].map({v:i for i,v in enumerate(dff['PaperlessBilling'].value_counts().index)})\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:00:38.723615Z","iopub.execute_input":"2021-06-02T03:00:38.724001Z","iopub.status.idle":"2021-06-02T03:00:38.762129Z","shell.execute_reply.started":"2021-06-02T03:00:38.723949Z","shell.execute_reply":"2021-06-02T03:00:38.761288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will give k = 5 to show case scores of all features.If we want top 3 features we can directly give k = 3 ","metadata":{}},{"cell_type":"code","source":"best = SelectKBest(chi2,k=5)\nbest.fit(dff[['gender','SeniorCitizen', 'Partner','PhoneService','PaperlessBilling']],dff['Churn'])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:00:48.146601Z","iopub.execute_input":"2021-06-02T03:00:48.147187Z","iopub.status.idle":"2021-06-02T03:00:48.187763Z","shell.execute_reply.started":"2021-06-02T03:00:48.147135Z","shell.execute_reply":"2021-06-02T03:00:48.186995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_score = pd.DataFrame(best.pvalues_,columns=['p_values'])\ndf_score['chi2_values'] = best.scores_\ndf_score['columns'] = ['gender','SeniorCitizen', 'Partner','PhoneService','PaperlessBilling']\ndf_score.sort_values(by='p_values')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:01:35.962282Z","iopub.execute_input":"2021-06-02T03:01:35.962673Z","iopub.status.idle":"2021-06-02T03:01:35.977643Z","shell.execute_reply.started":"2021-06-02T03:01:35.962644Z","shell.execute_reply":"2021-06-02T03:01:35.976932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have value of significance, alpha =0.05. so we have to choose those features with p value <= alpha","metadata":{}},{"cell_type":"code","source":"df_score[df_score['p_values'] <= 0.05]['columns']","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:02:01.540521Z","iopub.execute_input":"2021-06-02T03:02:01.541099Z","iopub.status.idle":"2021-06-02T03:02:01.548977Z","shell.execute_reply.started":"2021-06-02T03:02:01.541063Z","shell.execute_reply":"2021-06-02T03:02:01.548026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [1.2] Using Pearsons coorelation matrix","metadata":{}},{"cell_type":"code","source":"df_cor = df[['TotalCharges','MonthlyCharges','Churn']].corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(df_cor,annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:02:30.150267Z","iopub.execute_input":"2021-06-02T03:02:30.150645Z","iopub.status.idle":"2021-06-02T03:02:30.380369Z","shell.execute_reply.started":"2021-06-02T03:02:30.150591Z","shell.execute_reply":"2021-06-02T03:02:30.37929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we donot find any strong coorelation between any features.So it is not helpful in this case. If we find any variables with strong positive or negative coorelation we can remove any one of them.\n\nie if a feature is important:\n\n* It will have weak coorelation with other independent features\n* It will have strong coorelatio with target(dependent feature)","metadata":{}},{"cell_type":"markdown","source":"## [1.3] ANOVA F-test\n\n\nOne way ANOVA test can be used to find relationship between numeric and a categorical variable\n\n\nHere, <br> \n**Null hypothesis H0 : two groups have same variance.** <br>\n**Alternate hypothesis H1: aleast one of the group have different variance**\n\n\n\nie, if two groups have same variance it indicates that those feature is not important. We can drop them on feature selection. otherwise we wont drop the feature.\n\nThe basic idea is that we will find \n```\nFscore = (variance_between groups/ variance_within groups) \n```\n\nand compare it with critical value obtained from F value table to accept or reject null hypothesis.\n\n\nSklearn provides method called f_classif to do Anova F test and we can use it with Select K best for faster results. We dont have to do seperate for each input feature. If the value â€˜variance_between / variance_withinâ€™ is less than the critical value (evaluated using log table). The library returns score and p value, for p<0.05 we mean that the confidence>95% for them to belong to the same population and hence are co-related. We select top k co-related features according to the score returned by Anova.\n","metadata":{}},{"cell_type":"code","source":"dff = df[['TotalCharges','MonthlyCharges','Churn']]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:06:39.208276Z","iopub.execute_input":"2021-06-02T03:06:39.209018Z","iopub.status.idle":"2021-06-02T03:06:39.220098Z","shell.execute_reply.started":"2021-06-02T03:06:39.208965Z","shell.execute_reply":"2021-06-02T03:06:39.219015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\n# select top 3 features\nselector = SelectKBest(f_classif,k=2)\nselector.fit(dff[['TotalCharges','MonthlyCharges']],dff['Churn'])\n\n\ndf_score = pd.DataFrame(selector.pvalues_,columns=['p_values'])\ndf_score['score'] = selector.scores_\ndf_score['columns'] = ['TotalCharges','MonthlyCharges']\n\ndf_score","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:06:39.571465Z","iopub.execute_input":"2021-06-02T03:06:39.571805Z","iopub.status.idle":"2021-06-02T03:06:39.599988Z","shell.execute_reply.started":"2021-06-02T03:06:39.571776Z","shell.execute_reply":"2021-06-02T03:06:39.598888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_score[df_score['p_values'] <= 0.05]['columns']","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:06:57.383708Z","iopub.execute_input":"2021-06-02T03:06:57.384129Z","iopub.status.idle":"2021-06-02T03:06:57.395011Z","shell.execute_reply.started":"2021-06-02T03:06:57.384093Z","shell.execute_reply":"2021-06-02T03:06:57.393994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [2]Wrapper Methods\n\nEventhough We have filter methods they are not much accurate, so we have wrapper methods like forward feature selection, backward elemination etc.\n\n## [2.1] Forward Selection\n\nIt is an iterative method in which we start with zero features at the beginning and in each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.","metadata":{}},{"cell_type":"markdown","source":"Before Proceeding further Let us create a basic model first. We will build a decision tree classifier.","metadata":{}},{"cell_type":"code","source":"num_cols = ['TotalCharges','MonthlyCharges','tenure']\n\nfor col in df.columns:\n    if col not in num_cols:\n        df[col] = df[col].map({v:i for i,v in enumerate(df[col].value_counts().index)})\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:07:16.790851Z","iopub.execute_input":"2021-06-02T03:07:16.791435Z","iopub.status.idle":"2021-06-02T03:07:16.878359Z","shell.execute_reply.started":"2021-06-02T03:07:16.791388Z","shell.execute_reply":"2021-06-02T03:07:16.877253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ny = df['Churn']\nX = df.drop(columns=['Churn'])\nprint(X.shape,y.shape)\nprint('-'*50)\n\n#60-20-20 split\nx_train,x_test,y_train,y_test = train_test_split(X,y,random_state=100,stratify=y,test_size=0.2)\n\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:07:20.438407Z","iopub.execute_input":"2021-06-02T03:07:20.43904Z","iopub.status.idle":"2021-06-02T03:07:20.462726Z","shell.execute_reply.started":"2021-06-02T03:07:20.438992Z","shell.execute_reply":"2021-06-02T03:07:20.461815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before going further let us build and tune a model","metadata":{}},{"cell_type":"code","source":"\nclf = DecisionTreeClassifier()\nsamplesplits = [5, 10, 100, 500]\nmaximumdepth = [1, 5, 10, 50, 100, 500, 1000]\nparameters = {'min_samples_split':samplesplits ,'max_depth':maximumdepth}\n\nmodel = GridSearchCV(estimator=clf, param_grid=parameters, cv=3, n_jobs=-1, scoring='roc_auc',return_train_score=True)\nmodel.fit(x_train,y_train)\nprint(\"Model with best parameters :\\n\",model.best_params_)\n\n### model\nbest_est = DecisionTreeClassifier(**model.best_params_)\nbest_est = best_est.fit(x_train,y_train)\ntrain_fpr, train_tpr, thresholds = roc_curve(y_train, best_est.predict_proba(x_train)[:,1])\ntest_fpr, test_tpr, thresholds = roc_curve(y_test, best_est.predict_proba(x_test)[:,1])\n\nprint('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\nprint('Area under test roc {}'.format(auc(test_fpr, test_tpr)))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:07:23.687532Z","iopub.execute_input":"2021-06-02T03:07:23.688142Z","iopub.status.idle":"2021-06-02T03:07:26.598072Z","shell.execute_reply.started":"2021-06-02T03:07:23.688094Z","shell.execute_reply":"2021-06-02T03:07:26.597182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:07:55.432057Z","iopub.execute_input":"2021-06-02T03:07:55.432555Z","iopub.status.idle":"2021-06-02T03:07:55.437639Z","shell.execute_reply.started":"2021-06-02T03:07:55.432521Z","shell.execute_reply":"2021-06-02T03:07:55.436979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Forward feature selection**\n\nCurrently we have 20 features. Let us pick up top 17 features and see how it performs.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SequentialFeatureSelector\n\n\nmodel = DecisionTreeClassifier(max_depth=5,min_samples_split=100)\nsfs = SequentialFeatureSelector(model,n_features_to_select=17,scoring='roc_auc',direction='forward')\nsfs.fit(x_train,y_train)\n\n\nidxes = sfs.get_support(indices=True)\ntop_feats = x_train.columns[idxes]\nprint(f'Selected features are {top_feats}')\n\n\n### model\nbest_est = DecisionTreeClassifier(max_depth=5,min_samples_split=100)\nbest_est = best_est.fit(x_train[top_feats],y_train)\ntrain_fpr, train_tpr, thresholds = roc_curve(y_train, best_est.predict_proba(x_train[top_feats])[:,1])\ntest_fpr, test_tpr, thresholds = roc_curve(y_test, best_est.predict_proba(x_test[top_feats])[:,1])\n\n\nprint(f'Results after reducing features from 20 to 17')\nprint('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\nprint('Area under test roc {}'.format(auc(test_fpr, test_tpr)))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:10:52.385167Z","iopub.execute_input":"2021-06-02T03:10:52.385525Z","iopub.status.idle":"2021-06-02T03:10:57.598118Z","shell.execute_reply.started":"2021-06-02T03:10:52.385497Z","shell.execute_reply":"2021-06-02T03:10:57.597042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that on selecting top 17 features our model perfomance increased from 0.814637422821566 to 0.8163347025239608","metadata":{}},{"cell_type":"markdown","source":"## [2.2] Backward selection\n\nIn this method, we start with the all the features, and remove features one by one if their absence increases the score of the model. We do this until no improvement is observed on removing any feature.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SequentialFeatureSelector\n\nmodel = DecisionTreeClassifier(max_depth=5,min_samples_split=100)\nsfs = SequentialFeatureSelector(model,n_features_to_select=17,scoring='roc_auc',direction='backward')\nsfs.fit(x_train,y_train)\n\n\nidxes = sfs.get_support(indices=True)\ntop_feats = x_train.columns[idxes]\nprint(f'Selected features are {top_feats}')\n\n\n### model\nbest_est = DecisionTreeClassifier(max_depth=5,min_samples_split=100)\nbest_est = best_est.fit(x_train[top_feats],y_train)\ntrain_fpr, train_tpr, thresholds = roc_curve(y_train, best_est.predict_proba(x_train[top_feats])[:,1])\ntest_fpr, test_tpr, thresholds = roc_curve(y_test, best_est.predict_proba(x_test[top_feats])[:,1])\n\n\nprint(f'Results with reduced features')\nprint('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\nprint('Area under test roc {}'.format(auc(test_fpr, test_tpr)))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:11:55.803048Z","iopub.execute_input":"2021-06-02T03:11:55.80343Z","iopub.status.idle":"2021-06-02T03:11:57.866087Z","shell.execute_reply.started":"2021-06-02T03:11:55.8034Z","shell.execute_reply":"2021-06-02T03:11:57.865216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case also removing features improved the perfomance of the model.","metadata":{}},{"cell_type":"markdown","source":"## [2.3] Recursive feature elimination\n\n\nIn RFE, it is a recursive process of feature selection. Initially a model is build with all the features. Now the feature with least importance is removed and again the model is fitted in remaining features. Inorder to determine important features algorithms like decision tree,xgboost etc have its own ways. Otherwise it internally uses statistical methods to achieve the same. This process is recursively done until we get required number of features.\n\n\nHere we will familirize RFE with cross validation as it will makes better since since we will also do cross validation while building each model.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\n\n\n\n\nmodel = DecisionTreeClassifier(max_depth=5,min_samples_split=100)\nrfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(10), scoring='roc_auc')\nrfecv.fit(x_train,y_train)\n\n\nprint('Optimal number of features: {}'.format(rfecv.n_features_))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:12:31.73753Z","iopub.execute_input":"2021-06-02T03:12:31.737864Z","iopub.status.idle":"2021-06-02T03:12:33.75113Z","shell.execute_reply.started":"2021-06-02T03:12:31.737835Z","shell.execute_reply":"2021-06-02T03:12:33.750323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(np.where(rfecv.support_ == True)[0])\ntop_feats = x_train.columns[np.where(rfecv.support_ == True)[0]]\n\n\n\nbest_est = DecisionTreeClassifier(max_depth=5,min_samples_split=100)\nbest_est = best_est.fit(x_train[top_feats],y_train)\ntrain_fpr, train_tpr, thresholds = roc_curve(y_train, best_est.predict_proba(x_train[top_feats])[:,1])\ntest_fpr, test_tpr, thresholds = roc_curve(y_test, best_est.predict_proba(x_test[top_feats])[:,1])\n\n\nprint(f'Results with reduced features')\nprint('Area under train roc {}'.format(auc(train_fpr, train_tpr)))\nprint('Area under test roc {}'.format(auc(test_fpr, test_tpr)))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T03:12:39.306112Z","iopub.execute_input":"2021-06-02T03:12:39.306574Z","iopub.status.idle":"2021-06-02T03:12:39.32742Z","shell.execute_reply.started":"2021-06-02T03:12:39.306544Z","shell.execute_reply":"2021-06-02T03:12:39.326312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that with just 3 features our score improved to 0.8283","metadata":{}},{"cell_type":"markdown","source":"Thus we can see that feature selection helps in improving our model perfomance.","metadata":{}}]}