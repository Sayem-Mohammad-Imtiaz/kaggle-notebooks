{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Regression Analysis - Flat Price in Moscow\nThis notebook will analyse the price of housing in Moscow. Regressions will first be used to understand the data. Prediction and optimisation will also be presented.\n\nThe methods used are:\n* linear regression of multiple variables  \n* decision tree regression\n* XGBoost regression","metadata":{}},{"cell_type":"code","source":"#importing the necessary libraries\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:06:00.87588Z","iopub.execute_input":"2021-05-31T07:06:00.876474Z","iopub.status.idle":"2021-05-31T07:06:02.040467Z","shell.execute_reply.started":"2021-05-31T07:06:00.87639Z","shell.execute_reply":"2021-05-31T07:06:02.039478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/price-of-flats-in-moscow/flats_moscow.csv', index_col = 0)   #Importing data \ndf","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:06:02.0419Z","iopub.execute_input":"2021-05-31T07:06:02.042186Z","iopub.status.idle":"2021-05-31T07:06:02.098354Z","shell.execute_reply.started":"2021-05-31T07:06:02.042158Z","shell.execute_reply":"2021-05-31T07:06:02.09724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data description, translated by one of the available translators, showed something non-intuitive. More specifically, it concerns a column named 'floor'.\n\nThe description reads: \n> 1 - floor other than the first and last floor, 0 - otherwise.\n\nPersonally, I would prefer the reverse designation. 1 for first and last floor, 0 otherwise. The code below will replace this column to make it easier to read. In addition, the commands will remove the remaining columns of categorical data. \"walk\" and \"brick\" do not appear to be a significant factor affecting housing prices. ","metadata":{}},{"cell_type":"code","source":"def floor_changer(f):    #easy function, just changing 1 to 0, and 0 to 1. \n    if f == 1:      \n        f = 0\n    else: \n        f = 1 \n    return f\n\ndf.floor = df.floor.apply(floor_changer)   #using this function on 'floor' column\n\ndf = df.drop(columns=['brick', 'walk', 'code'])   #deleting of unnecessary columns\n\ndf #showing dataframe","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:06:02.100067Z","iopub.execute_input":"2021-05-31T07:06:02.100382Z","iopub.status.idle":"2021-05-31T07:06:02.122481Z","shell.execute_reply.started":"2021-05-31T07:06:02.100353Z","shell.execute_reply":"2021-05-31T07:06:02.121386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to get any information about the data presented and, if necessary, to remove outliers, each variable can be presented graphically in relation to price. Price will be the predicted value in later regression models. Only the categorical variable 'floor' will be shown as a bar chart with the mean divided into house on first or last floor (1) and others. ","metadata":{}},{"cell_type":"code","source":"plt.subplots(2,3 , figsize=(20,15))\n\ni = 1    #to display one graph next to another\nfor variable in df.columns[1:]:\n    plt.subplot(2,3,i)\n    if variable == 'floor':     #logical test to present categorical data in different form than a scatter plot\n        plt.title(variable)\n        categorical_value_plot = sns.barplot(x='floor', y='price', data=df, color='#191970')   \n        categorical_value_plot.axhline(df.price.mean(), color='black', lw=2)    #adding a line on the level of general mean\n        i +=1    #to display one graph next to another\n    else:\n        plt.scatter(x = df[variable], y=df.price, c='#191970')  \n        plt.title(variable)\n        plt\n        i +=1   #to display one graph next to another","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:06:09.014546Z","iopub.execute_input":"2021-05-31T07:06:09.014901Z","iopub.status.idle":"2021-05-31T07:06:10.098371Z","shell.execute_reply.started":"2021-05-31T07:06:09.014872Z","shell.execute_reply":"2021-05-31T07:06:10.097722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, the graphs form a more or less linear relationship. It can also be seen, which is consistent with people's subjective assessment of ground floor flats as potentially more dangerous, that the price of these flats is on average lower than houses on other floors. \n\nOne outlier observation was also located.  This is the highest priced flat (over $700,000). It also has the largest living space, but performs average in other categories. With an observation count of 2040, this outlier can be removed without major inaccuracies for the rest of the analysis. \n\nUnfortunately, there is a different data problem here. With regard to the graphs and the descriptions of the variables, it has to be said that there is a good chance that there is multicollinearity between the variables 'totsp' and 'livesp'. This is not a big problem for predictive models, but it can make the data difficult to understand. \n\nTo test this it will be run a series of linear models in which each predictor variable becomes an predicted variable. In this way, it will be possible to calculate the Variance inflation Factor. The VIF allows to check whether collinearity actually exists. Depending on the literature, different indicators are given as limits. In general, a VIF greater than 5 indicates collinearity between the variables. ","metadata":{}},{"cell_type":"code","source":"df = df[df.price<700]   #deleting outlier. Only one value has a price above 700. \n\n\ndef VIF(R_2):              #function to calcute VIF, in which R_2 is R^2 score for every model \n    return (1/(1-R_2))\n\nfor i in range(len(df.columns)):       #every variable (so every column) has to be treated as explained variable \n    model = LinearRegression()         #used from scikitlearn library -  Ordinary least squares Linear Regression\n    Y = df.iloc[:,i]                   #just one column at a time should be picked as explained variable \n    X = df.drop(columns = df.columns[i])    #explained variable must be deleted from other predictors \n    model.fit(X, Y)                         #fitting the model \n    R_2 = model.score(X, Y)                 #finding R^2 score\n    print('VIF in case of ' + df.columns[i] + \" treated as outcome variable: \" + str(VIF(R_2)))  #showing VIF","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:06:19.311122Z","iopub.execute_input":"2021-05-31T07:06:19.311458Z","iopub.status.idle":"2021-05-31T07:06:19.464586Z","shell.execute_reply.started":"2021-05-31T07:06:19.311426Z","shell.execute_reply":"2021-05-31T07:06:19.457659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As said, a large VIF can be observed with the variables 'totsp' and 'livesp'. There are several ways to deal with collinearity. In this paper a slightly unusual approach will be used, that is, the variable \"totsp\" will be modified to retain as much information as possible, minimising collinearity. \n\nBecause \"totsp\", as the description implies, is the extra space of the flat in relation to the living space (\"livesp\"). It seems that additional space understood in this way can be e.g. balconies, gardens, cellars. Therefore, if \"livesp\" is subtracted from \"totsp\", the value of \"extra square metres\" is obtained. The column thus created will be called \"extrasp\" and will replace the existing \"totsp\". ","metadata":{}},{"cell_type":"code","source":"df['extrasp'] = df.totsp-df.livesp    #modifying \"totsp\" somehow to reduce a collinearity \ndf = df.drop(columns='totsp')         #deleting unwanted column ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:07:13.338737Z","iopub.execute_input":"2021-05-31T07:07:13.3391Z","iopub.status.idle":"2021-05-31T07:07:13.452089Z","shell.execute_reply.started":"2021-05-31T07:07:13.339069Z","shell.execute_reply":"2021-05-31T07:07:13.450414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#perfoming the previous test once again to check for changes in the VIF\n\nfor i in range(len(df.columns)):       \n    model = LinearRegression()        \n    Y = df.iloc[:,i]                  \n    X = df.drop(columns = df.columns[i])    \n    model.fit(X, Y)                         \n    R_2 = model.score(X, Y)                 \n    print('VIF in case of ' + df.columns[i] + \" treated as outcome variable: \" + str(VIF(R_2))) ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:07:24.734666Z","iopub.execute_input":"2021-05-31T07:07:24.735025Z","iopub.status.idle":"2021-05-31T07:07:24.852221Z","shell.execute_reply.started":"2021-05-31T07:07:24.734991Z","shell.execute_reply":"2021-05-31T07:07:24.851215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The situation with collinearity has improved. Several predictions will now be made in order to achieve the highest possible R2 and the lowest possible MAPE (Mean Absolute Percent Error)","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:07:31.831853Z","iopub.execute_input":"2021-05-31T07:07:31.832169Z","iopub.status.idle":"2021-05-31T07:07:31.851064Z","shell.execute_reply.started":"2021-05-31T07:07:31.832142Z","shell.execute_reply":"2021-05-31T07:07:31.850226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns='price')  #price will be outcome variable\nY = df.price","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:07:34.072028Z","iopub.execute_input":"2021-05-31T07:07:34.07239Z","iopub.status.idle":"2021-05-31T07:07:34.077868Z","shell.execute_reply.started":"2021-05-31T07:07:34.072363Z","shell.execute_reply":"2021-05-31T07:07:34.076635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2) #division into training and test data sets ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:07:40.709566Z","iopub.execute_input":"2021-05-31T07:07:40.709937Z","iopub.status.idle":"2021-05-31T07:07:40.715826Z","shell.execute_reply.started":"2021-05-31T07:07:40.709903Z","shell.execute_reply":"2021-05-31T07:07:40.71506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model = LinearRegression()          #OLS Model\nlr_model.fit(X_train, y_train)         #using just training data \ny_predicted = lr_model.predict(X_test) #prediction \n\n\n\nparameters = list(zip(X.columns, lr_model.coef_))  #creating list with the variable and its parameter\nparameters #printing","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:07:42.791438Z","iopub.execute_input":"2021-05-31T07:07:42.791939Z","iopub.status.idle":"2021-05-31T07:07:42.803251Z","shell.execute_reply.started":"2021-05-31T07:07:42.791909Z","shell.execute_reply":"2021-05-31T07:07:42.802398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All values appear to be logically correct. The larger the size of a flat, the higher its price. The distance from the centre and from the metro decreases the price of the flat. If a house is located on the ground or top floor, its price is on average lower by more than 6 thousand dollars. ","metadata":{}},{"cell_type":"code","source":"lr_model.score(X_test, y_test), mean_absolute_error(y_test, y_predicted)/df.price.mean()  #Showing R2 and MAPE","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:07:56.858164Z","iopub.execute_input":"2021-05-31T07:07:56.858475Z","iopub.status.idle":"2021-05-31T07:07:56.869953Z","shell.execute_reply.started":"2021-05-31T07:07:56.858447Z","shell.execute_reply":"2021-05-31T07:07:56.869083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model explains approximately 65% of housing prices. The relative error of the forecast is about 15%. \n\nBelow is an attempt to get a better result using decision tree regression.","metadata":{}},{"cell_type":"code","source":"tree_regr = DecisionTreeRegressor(max_depth=3) #Predictors are amounted to just 6 so depth on level 3 could be seen as fairly reasonable \ntree_regr.fit(X_train, y_train) #fitting \ny_tree_predicted = tree_regr.predict(X_test) #Prediction using the same test sample which was in Linear Regression\ntree_regr.score(X_test, y_test), mean_absolute_error(y_test, y_tree_predicted)/df.price.mean() #Printing R2 and MAPE to compare with lr_model","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:08:34.504181Z","iopub.execute_input":"2021-05-31T07:08:34.504526Z","iopub.status.idle":"2021-05-31T07:08:34.518464Z","shell.execute_reply.started":"2021-05-31T07:08:34.50448Z","shell.execute_reply":"2021-05-31T07:08:34.51762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is inferior to linear regression. It is mainly based on the variables 'livesp' and 'kitsp'. This is not particularly surprising, with the linear regression model these two variables also had high parameters. \n\nBelow is a graphical representation of the tree using a dedicated method from the scikitlearn library. ","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(25,20))   #adjusting size\ntree.plot_tree(tree_regr, filled=True)  ","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:08:47.127402Z","iopub.execute_input":"2021-05-31T07:08:47.127892Z","iopub.status.idle":"2021-05-31T07:08:48.067812Z","shell.execute_reply.started":"2021-05-31T07:08:47.127861Z","shell.execute_reply":"2021-05-31T07:08:48.066783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To improve Decision Tree Regressor this code cell will try to find best fitted tree model changing max_depth parameter\n\nMAX_DEPTH = 10\n\nfor i in range(1, MAX_DEPTH+1):\n    tree_model = DecisionTreeRegressor(max_depth=i)\n    tree_model.fit(X_train, y_train)\n    y_prediction = tree_model.predict(X_test)\n    print('For depth {deep_value} R_2: {r_score} and MAPE: {mape}'.format\n          (deep_value=i, \n           r_score=tree_model.score(X_test, y_test), \n           mape = mean_absolute_error(y_test, y_prediction)/df.price.mean()))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:08:52.762233Z","iopub.execute_input":"2021-05-31T07:08:52.762587Z","iopub.status.idle":"2021-05-31T07:08:52.841526Z","shell.execute_reply.started":"2021-05-31T07:08:52.762557Z","shell.execute_reply":"2021-05-31T07:08:52.840633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Increasing the depth of the decision tree does not significantly affect the level of predictive validity of the model. Still the linear model with an R2 of 65% is the best. In order to beat this value, the XGBoost model will be used, which is, to a large simplification, a more advanced variation of decision trees.","metadata":{}},{"cell_type":"code","source":"xgb_model = xgb.XGBRegressor(n_estimators = 100, max_depth = 1, )\nX_train = np.ascontiguousarray(X_train)    #Changing format to avoid XGBoost Warning\ny_train = np.ascontiguousarray(y_train) \nX_test = np.ascontiguousarray(X_test)\ny_test = np.ascontiguousarray(y_test)\n\nxgb_model.fit(X_train, y_train)\n\nxgb_prediction = xgb_model.predict(X_test)\nxgb_model.score(X_test, y_test), mean_absolute_error(y_test, xgb_prediction)/df.price.mean()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:12:52.119875Z","iopub.execute_input":"2021-05-31T07:12:52.120206Z","iopub.status.idle":"2021-05-31T07:12:52.171343Z","shell.execute_reply.started":"2021-05-31T07:12:52.120177Z","shell.execute_reply":"2021-05-31T07:12:52.170474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost performs far better than a simple decision tree. It is now necessary to find the most suitable parameters. ","metadata":{}},{"cell_type":"code","source":"r_score_results = []   #In this list will be gathered all R2 scores for every number of \"n_estimators\"\nMAPE_result = []       #In this list will be gathered all R2 scores for every number of \"n_estimators\"\n\nfor i in range(5,100):\n    xgb_model = xgb.XGBRegressor(n_estimators = i, max_depth = 4)  #After some trials this depth should be defined as most effective\n    xgb_model.fit(X_train, y_train)\n    xgb_prediction = xgb_model.predict(X_test)\n    r_score_results.append(xgb_model.score(X_test, y_test))\n    MAPE_result.append(mean_absolute_error(y_test, xgb_prediction)/df.price.mean())","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:12:58.818126Z","iopub.execute_input":"2021-05-31T07:12:58.818742Z","iopub.status.idle":"2021-05-31T07:13:02.811Z","shell.execute_reply.started":"2021-05-31T07:12:58.818692Z","shell.execute_reply":"2021-05-31T07:13:02.810215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting how R2 and MAPE change in relation to n_estimators\nplt.figure(figsize=(12,12))\nplt.plot(r_score_results, c='#191970')\nplt.plot(MAPE_result, c = '#4e6e81')\nplt.legend(labels=['R2', 'MAPE'])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:13:24.58229Z","iopub.execute_input":"2021-05-31T07:13:24.582663Z","iopub.status.idle":"2021-05-31T07:13:24.749907Z","shell.execute_reply.started":"2021-05-31T07:13:24.582632Z","shell.execute_reply":"2021-05-31T07:13:24.749266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(r_score_results), min(MAPE_result)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:13:55.065248Z","iopub.execute_input":"2021-05-31T07:13:55.065648Z","iopub.status.idle":"2021-05-31T07:13:55.070845Z","shell.execute_reply.started":"2021-05-31T07:13:55.065613Z","shell.execute_reply":"2021-05-31T07:13:55.070075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_score_results.index(max(r_score_results)) #Finding idex of max value \nn = list(range(5,100))[r_score_results.index(max(r_score_results))]  #Recreating a list given in code before to get a value of n_estimators\nprint(n)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:14:51.027142Z","iopub.execute_input":"2021-05-31T07:14:51.027496Z","iopub.status.idle":"2021-05-31T07:14:51.032837Z","shell.execute_reply.started":"2021-05-31T07:14:51.027467Z","shell.execute_reply":"2021-05-31T07:14:51.031858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Last try \nxgb_model = xgb.XGBRegressor(n_estimators = n, max_depth = 4)\nxgb_model.fit(X_train, y_train)\nxgb_prediction = xgb_model.predict(X_test)\nxgb_model.score(X_test, y_test), mean_absolute_error(y_test, xgb_prediction)/df.price.mean()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:14:58.088843Z","iopub.execute_input":"2021-05-31T07:14:58.089359Z","iopub.status.idle":"2021-05-31T07:14:58.125281Z","shell.execute_reply.started":"2021-05-31T07:14:58.089325Z","shell.execute_reply":"2021-05-31T07:14:58.124287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To sum up. A simple linear regression model performs quite well in predicting the value of housing in Moscow based on the analysed data set. The simplest decision tree regression model does not do so well (it is possible that the sample is a bit too small). The highest R2 was achieved using XGBRegressor with parameters as in the last cell with code. However, an improvement of this percentage points in R2 over linear regression is probably not worth the loss of model readability. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}