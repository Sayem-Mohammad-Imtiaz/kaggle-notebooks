{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\n\nimport numpy as np \nimport requests\nfrom PIL import Image\nfrom io import BytesIO \n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(12,8)})\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:05:19.823985Z","iopub.execute_input":"2021-06-14T14:05:19.824554Z","iopub.status.idle":"2021-06-14T14:05:19.841606Z","shell.execute_reply.started":"2021-06-14T14:05:19.824512Z","shell.execute_reply":"2021-06-14T14:05:19.840573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/schopenhauer-work-corpus/Schopenhauer_works_corpus.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:05:19.843968Z","iopub.execute_input":"2021-06-14T14:05:19.844263Z","iopub.status.idle":"2021-06-14T14:05:20.016376Z","shell.execute_reply.started":"2021-06-14T14:05:19.844233Z","shell.execute_reply":"2021-06-14T14:05:20.015079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Isolate the book wanted: Beyond Good and Evil\nwill = df[df['book_title']=='The World As Will And Idea (Vol. 1 of 3)']['text_clean'][8]\ntokens = word_tokenize(will)\nfreq = Counter(tokens)\nsorted_freq = dict(sorted(freq.items(), key=lambda x: x[1], reverse=True))\ntop_25_words = list(sorted_freq.keys())[:25]\ntop_25_freq = list(sorted_freq.values())[:25]\nsns.barplot(y=top_25_words, x=top_25_freq)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:05:20.018698Z","iopub.execute_input":"2021-06-14T14:05:20.019117Z","iopub.status.idle":"2021-06-14T14:05:21.310057Z","shell.execute_reply.started":"2021-06-14T14:05:20.019076Z","shell.execute_reply":"2021-06-14T14:05:21.309061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordcloud","metadata":{}},{"cell_type":"code","source":"def plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(12, 8))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");\n    \n\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, \n                      background_color='black', colormap='Set2', \n                      collocations=False, stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(sorted_freq)\nplot_cloud(wordcloud)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:05:21.312107Z","iopub.execute_input":"2021-06-14T14:05:21.31258Z","iopub.status.idle":"2021-06-14T14:05:34.646271Z","shell.execute_reply.started":"2021-06-14T14:05:21.312529Z","shell.execute_reply":"2021-06-14T14:05:34.64547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Summarization","metadata":{}},{"cell_type":"code","source":"will = df[df['book_title']=='The World As Will And Idea (Vol. 1 of 3)']['text'][8]\nwill = will.encode(encoding=\"ascii\", errors=\"ignore\").decode()\nwill = re.sub(\"[\\r\\n]\", \" \", will)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:33:35.404262Z","iopub.execute_input":"2021-06-14T14:33:35.404668Z","iopub.status.idle":"2021-06-14T14:33:35.445008Z","shell.execute_reply.started":"2021-06-14T14:33:35.404634Z","shell.execute_reply":"2021-06-14T14:33:35.44383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt') # one time execution\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:37:35.678997Z","iopub.execute_input":"2021-06-14T14:37:35.679407Z","iopub.status.idle":"2021-06-14T14:37:36.015206Z","shell.execute_reply.started":"2021-06-14T14:37:35.679362Z","shell.execute_reply":"2021-06-14T14:37:36.014317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\nsentences = []\ntext = will.split(\". \")\nfor s in text:\n    sentences.append(sent_tokenize(s))\n\nsentences = [y for x in sentences for y in x] # flatten list","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:39:32.849583Z","iopub.execute_input":"2021-06-14T14:39:32.849924Z","iopub.status.idle":"2021-06-14T14:39:33.21445Z","shell.execute_reply.started":"2021-06-14T14:39:32.849896Z","shell.execute_reply":"2021-06-14T14:39:33.21345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:40:13.87556Z","iopub.execute_input":"2021-06-14T14:40:13.87592Z","iopub.status.idle":"2021-06-14T14:43:23.970007Z","shell.execute_reply.started":"2021-06-14T14:40:13.875891Z","shell.execute_reply":"2021-06-14T14:43:23.967846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:43:36.645248Z","iopub.execute_input":"2021-06-14T14:43:36.645752Z","iopub.status.idle":"2021-06-14T14:43:54.029799Z","shell.execute_reply.started":"2021-06-14T14:43:36.645711Z","shell.execute_reply":"2021-06-14T14:43:54.028785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(word_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:43:54.031604Z","iopub.execute_input":"2021-06-14T14:43:54.031908Z","iopub.status.idle":"2021-06-14T14:43:54.03893Z","shell.execute_reply.started":"2021-06-14T14:43:54.031879Z","shell.execute_reply":"2021-06-14T14:43:54.037867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove punctuations, numbers and special characters\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:43:58.155878Z","iopub.execute_input":"2021-06-14T14:43:58.156431Z","iopub.status.idle":"2021-06-14T14:43:58.280012Z","shell.execute_reply.started":"2021-06-14T14:43:58.15638Z","shell.execute_reply":"2021-06-14T14:43:58.279144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to remove stopwords\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new\n# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:44:41.548134Z","iopub.execute_input":"2021-06-14T14:44:41.54878Z","iopub.status.idle":"2021-06-14T14:44:41.826928Z","shell.execute_reply.started":"2021-06-14T14:44:41.548744Z","shell.execute_reply":"2021-06-14T14:44:41.825985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence_vectors = []\nfor i in clean_sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:45:25.796454Z","iopub.execute_input":"2021-06-14T14:45:25.797088Z","iopub.status.idle":"2021-06-14T14:45:26.029308Z","shell.execute_reply.started":"2021-06-14T14:45:25.79705Z","shell.execute_reply":"2021-06-14T14:45:26.02845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics.pairwise import cosine_similarity\n# sim_mat = np.zeros([len(sentences), len(sentences)])\n# for i in range(len(sentences)):\n#     for j in range(len(sentences)):\n#         if i != j:\n#             sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), \n#                                               sentence_vectors[j].reshape(1,100))[0,0]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:49:33.197984Z","iopub.execute_input":"2021-06-14T14:49:33.198366Z","iopub.status.idle":"2021-06-14T14:49:33.203269Z","shell.execute_reply.started":"2021-06-14T14:49:33.198319Z","shell.execute_reply":"2021-06-14T14:49:33.20218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import networkx as nx\n\n# nx_graph = nx.from_numpy_array(sim_mat)\n# scores = nx.pagerank(nx_graph)","metadata":{},"execution_count":null,"outputs":[]}]}