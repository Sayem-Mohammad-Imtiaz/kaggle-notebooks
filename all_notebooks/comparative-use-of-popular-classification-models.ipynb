{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv(\"../input/social-network-ads/Social_Network_Ads.csv\")\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr() # Lets look at statistical correlation\n#There is positive high correlation between Age and Purchased items","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap=\"jet\",annot=True)\n#here we visualize the correlations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(data=df, x=\"Age\",hue=\"Purchased\")\n#We can see that the effects of ads is highest between ages 26 and 40\n#Therefore these ge groups are more suitable to be target group for the commercial ads","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum() # we do not have any missing values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info() # we do not have any non numerical values in the columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B. Preparing Data For Algorithms","metadata":{}},{"cell_type":"markdown","source":"1. Splitting Data into Train and Test Sets","metadata":{}},{"cell_type":"code","source":"X = df.drop(\"Purchased\",axis=1).values\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df[\"Purchased\"].values\ny.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.Feature Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\nX_train= ss.fit_transform(X_train)\nX_test= ss.transform(X_test)\nX_train[0]\n#We rescale all of the features with standart scaler which produces values between -1 and 1\n#This secures there is no value gap between features ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## C. Training Classification Algorithms","metadata":{}},{"cell_type":"markdown","source":"## 1. Logistic Regression:","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogistic= LogisticRegression()\nlogistic.fit(X_train, y_train)\npredictions_logistic= logistic.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation of the Performance of Logistic Regression","metadata":{}},{"cell_type":"code","source":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(predictions_logistic,columns=[ \"Predictions of Logistic regression\"])\npd.concat([df1,df2],axis=1).head()\n#Here we can compare the predictions of our model with the actual values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\nprint(classification_report(y_test, predictions_logistic))\nprint(confusion_matrix(y_test, predictions_logistic))\nprint(accuracy_score(y_test, predictions_logistic))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, logistic.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n#Visualization of the predictions of the Logistic Regression in Train Set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, logistic.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n#Visualization of the predictions of the Logistic Regression in Test Set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. K Nearest Neighbors:","metadata":{}},{"cell_type":"markdown","source":"How Algorithm Works:\n\n*K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).\n\nFirstly we store all the dataSecondly we calculate the data from x to all points in our data set, x indicationg particular new data point Then we sort the points near data by increasing distance from xFinally we predict the majority label of K, which is number and represent closest points\n\n*Choosing a K will effect what class a new point is assigned to: if we choose k=3, then the algorithm looks at the three nearest neighbors to this new point if we set k=6, then the algorithm looks at the six nearest neighbors to this new point and decide according to the majority of these 6 neighbors. If we set larger k values,we get a cleaner cutoff at the expense of mislabelling some points","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"../input/knneigbor/knn.PNG\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, we have data points of Class A and B. We want to predict what the star (test data point) is. If we consider a k value of 3 (3 nearest data points) we will obtain a prediction of Class B. Yet if we consider a k value of 6, we will obtain a prediction of Class A.Therefore, the value of k is very important for our model's success.","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choosing true k value is very important. Instead of using different k vlaues which will be time consuming, we can use a function in order to choose the best k.","metadata":{}},{"cell_type":"code","source":"error_rate=list()\n#here we iterate meny different k values and plot their error rates \n#and discover which one is better than others and has the lowest error rate\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    prediction_i=knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i != y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we will plot the prediction error rates of different k values\nplt.figure(figsize=(15,10))\nplt.plot(range(1,40),error_rate, color=\"blue\", linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate vs K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel(\"Error Rate\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see in the figure above, k between 5 and 37 gives the least error rate,so we will use it for better predictions","metadata":{}},{"cell_type":"code","source":"knn=KNeighborsClassifier(n_neighbors=5) # we choose 5 as neigbor parameter\nknn.fit(X_train,y_train)\nknn_predictions=knn.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation of the Performance of  K Neares Neighbors","metadata":{}},{"cell_type":"code","source":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(knn_predictions,columns=[ \"Predictions of KNN\"])\npd.concat([df1,df2],axis=1).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, knn_predictions))\nprint(confusion_matrix(y_test, knn_predictions))\nprint(accuracy_score(y_test, knn_predictions))\n#KNN has higher performance than Logistic Regression in this dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising the Test set results","metadata":{}},{"cell_type":"code","source":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\nplt.contourf(X1, X2, knn.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('blue', 'purple')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('blue', 'purple'))(i), label = j)\nplt.title('K-NN (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree=DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\ndtree_predictions= dtree.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(dtree_predictions,columns=[ \"Predictions of Decision Tree Classifier\"])\npd.concat([df1,df2],axis=1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, dtree_predictions))\nprint(confusion_matrix(y_test, dtree_predictions))\nprint(accuracy_score(y_test, dtree_predictions))\n#Decision has higher performance than Logistic Regression, but lower than KNN  in this dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising the Test set results\n","metadata":{}},{"cell_type":"code","source":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, dtree.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Decision Tree Classification (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrandom=RandomForestClassifier()\nrandom.fit(X_train,y_train)\nrandom_predictions= random.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(random_predictions,columns=[ \"Predictions of Random Forest Classifier\"])\npd.concat([df1,df2],axis=1).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, random_predictions))\nprint(confusion_matrix(y_test, random_predictions))\nprint(accuracy_score(y_test, random_predictions))\n#Random Forest has the second best position after K Nearest Neighbors Algorithm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising the Test set results","metadata":{}},{"cell_type":"code","source":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, random.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising the Test set results","metadata":{}},{"cell_type":"markdown","source":"## 5. Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nbayes=GaussianNB()\nbayes.fit(X_train, y_train)\nbayes_predictions=bayes.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(random_predictions,columns=[ \"Predictions of Naive Bayes Classifier\"])\npd.concat([df1,df2],axis=1).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, bayes_predictions))\nprint(confusion_matrix(y_test, bayes_predictions))\nprint(accuracy_score(y_test, bayes_predictions))\n#Naive Bayes has better predictions than logistic regression, but worse than the restof algorithms","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising the Test set results","metadata":{}},{"cell_type":"code","source":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, bayes.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Support Vector Machines","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier= SVC(kernel=\"linear\")\nsvclassifier.fit(X_train, y_train)\nsvc_predictions= svclassifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.DataFrame(y_test,columns=[\"Original Values\"])\ndf2=pd.DataFrame(svc_predictions,columns=[ \"Predictions of Support Vector Machines\"])\npd.concat([df1,df2],axis=1).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, svc_predictions))\nprint(confusion_matrix(y_test,svc_predictions))\nprint(accuracy_score(y_test, svc_predictions))\n#Support Vector Machines has almost the same results as Naive Bayes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualization of Test Results","metadata":{}},{"cell_type":"code","source":"X_set, y_set = ss.inverse_transform(X_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, svclassifier.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM (Test set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}