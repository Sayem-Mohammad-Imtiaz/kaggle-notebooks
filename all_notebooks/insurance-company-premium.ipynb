{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Insurance Company Premium Default Prediction:\n![Insurance Premium](https://www.charitytaxgroup.org.uk/wp-content/uploads/insurance-premium.jpg)"},{"metadata":{},"cell_type":"markdown","source":"This is a case of predictive modeling. I'll use basic ML algorithms but I have researched a bit for this dataset so I'll be going as detailed & informative as possible. Please forgive me if it feels lengthy :P"},{"metadata":{},"cell_type":"markdown","source":"## The model building is a step-by-step process with following 3 major steps:\n1. Algorithm Selection\n2. Training Model\n3. Predicting"},{"metadata":{},"cell_type":"markdown","source":"Starting with the first step which is the Algorithm Selection:\n\n## 1. Algorithm Selection:"},{"metadata":{},"cell_type":"markdown","source":"The most important task to begin with this step is to identify whether the given dataset has dependent(target) variables or not. So I'll be uploading the train & test datasets below. Now to read those datasets, python requires a library called **Pandas**. So I'll first upload that library:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Now loading the train & test datasets individually:\ntrain=pd.read_csv(\"../input/insurance-company-dataset/train.csv\")\ntest=pd.read_csv(\"../input/insurance-company-dataset/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the dimensions of both the datasets side-by-side:\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means there are 13 columns in train dataset, whereas 11 in the test dataset. One of the missing columns in the test dataset is the \"target\" variable and I'll train my model to predict that variable. Also, since the target variable is present in the train dataset so the predictive model that is going to be built below can also be called a **Supervised Learning model**."},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration:"},{"metadata":{},"cell_type":"markdown","source":"Analysing the dataset is a part of a big process called Data Exploration. The various stages of this process are:\n\n1. Reading the data\n2. Variable identification\n3. Univariate analysis\n4. Bivariate analysis\n5. Missing value treatment\n6. Outlier treatment\n7. Variable transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Starting with reading the data. The first five rows of train dataset are:\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'target' column is present on the extreme right of the dataset. This column is a dependent column(dependent variable) as its values are dependent on other columns(independent variables). Notice that its values are not continuous as it only contains either 1s or 0s. So this is a **Classification problem**. This means the following algorithms suit this problem:\n\n1. Logistic Regression\n2. Decision Tree\n3. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# And first five rows of the test dataset:\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'target' column is not present in the test dataset. It is this column which I have to predict using my model. Also, the 'Premium' column too is absent in the test dataset. So I'll try dropping it from the train data as well, and see what result I get without it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop('premium',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now checking the datatype of all the columns/variables:\ntrain.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows there are 5 columns of int type, 5 columns of float type, & 2 columns of object type in the train dataset.\n\nThe categorical variables are stored as 'object'. Whereas continuous variables are stored as 'int' or 'float'. The int type can only take discrete/fixed integer values. Whereas float type can take any real values."},{"metadata":{},"cell_type":"markdown","source":"Now before starting the Univariate Analysis, I'd like to write the **details of all the columns** in the dataset:\n1. **id**: Unique ID of the policy\n2. **perc_premium_paid_by_cash_credit**: Percentage of premium amount paid by cash or credit card\n3. **age_in_days**: Age in days of policy holder\n4. **Income**: Monthly Income of policy holder\n5. **Count_3-6_months_late**: No of premiums late by 3 to 6 months\n6. **Count_6-12_months_late**: No of premiums late by 6 to 12 months\n7. **Count_more_than_12_months_late**: No of premiums late by more than 12 months\n8. **application_underwriting_score**: Underwriting Score of the applicant at the time of application (No applications under the score of 90 are insured)\n9. **no_of_premiums_paid**: Total premiums paid on time till now\n10. **sourcing_channel**: Sourcing channel for application\n11. **residence_area_type**: Area type of Residence (Urban/Rural)\n12. **target**: 1 - premium paid on time, 0 - otherwise"},{"metadata":{},"cell_type":"markdown","source":"I'll check for the twelfth column, ie; did customers pay the insurance premium on time or not\n![Premium paid](https://www.healthcare.gov/assets/health-insurance-monthly-premium.png)"},{"metadata":{},"cell_type":"markdown","source":"## Univariate analysis:\nNow I'll be exploring one variable at a time. The univariate analysis is different for continuous & categorical variables. In continuous variables I'll be looking for the following:\n\n1. Statistical properties like central tendency & dispersion (mean,median & standard deviation)\n2. Distribution of variable (symmetric/right skewed/left skewed)\n3. Presence of missing values\n4. Presence of outliers"},{"metadata":{},"cell_type":"markdown","source":"The methods that I am going to use for the **univariate analysis of continuous variables** are:\n\n1. Tabular methods- to represent mean, median, standard deviation & presence of missing values\n2. Graphical methods- to represent the distribution of variables & presence of outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Statistical properties of all the continuous variables can be checked by using 'describe' function as follows:\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below the standard deviation row there are 5 other rows that represent the data distribution in the dataset. It's starting with minimum value, the 3 Quartile points, and maximum value of the dataset. Quartiles are values that divide the data distribution into 4 parts. The 25% is representing the **25th percentile** of data. It is called the **1st Quartile**. While 75% is the **75th percentile** and is called **3rd Quartile**. The 50% is the **50th percentile** & it is called the **2nd Quartile**. This 50th percentile also represents the median value of the corresponding column of the dataset."},{"metadata":{},"cell_type":"markdown","source":"For example, the median value of 1st column 'id' is almost equal to its 'mean' value. This means the data of this column has a symmetric distribution. Same is true for the 3rd column 'age_in_days'. While for the 2nd column 'perc_premium_paid_by_cash' the mean is 0.314288 and median is 0.167000. This difference tells that the data distribution is not symmetric for this column. So I'll explore this column data and all other columns(like 'income', all 3 'count' columns, etc) that represent such unequal data symmetry via plots. Plus, the statistics shown above can be visualized by the help of boxplots. A **boxplot** shows the minimum value, 1st Quartile, median value, 3rd Quartile & the maximum value of a variable. Beyond the minimum and maximum values on the boxplot, we have outliers. These outliers are unusually small or large values in the dataset. These can arise due to the following errors:\n\n1. Data entry errors\n2. Measurement errors\n3. Processing errors\n4. Change in the underlying population."},{"metadata":{},"cell_type":"markdown","source":"I'll first plot the boxplots for various variables, then explore the distribution plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now before plotting the boxplot, I require a library called matplotlib. So I'm importing it:\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Also importing 'seaborn' library to better visualize the distributions:\nimport seaborn as sn\n# This library uses matplotlib underneath to plot graphs but has better graphics than matplotlib.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting boxplot for 'perc_premium_paid_by_cash_credit' column using the 'seaborn' library:\nsn.boxplot(train['perc_premium_paid_by_cash_credit'],color='brown')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Starting from the left(0.0):\n\n1. The first vertical line is called Lower Fence. It is the minimum value of 'perc_premium_paid_by_cash_credit' column data.\n2. Then the blue box starts. This 1st vertical line of the box is called 1st Quartile. It is 25 percentile of the data.\n3. The vertical line inside the blue box is showing the median value or mid value of this column data.\n4. The last vertical line of the box is 3rd Quartile. This is 75 percentile of the data.\n5. The end line of the plot at point 1.0 is called the Upper Fence. This is the maximum value of this column."},{"metadata":{},"cell_type":"markdown","source":"By using the describe function above I've already got the exact values of all the columns. From boxplot also I can get the approximate values as follows:\n\n1. The minimum value is exactly 0.0\n2. The value of 1st Quartile is just a little more than 0.0\n3. Median is just a little less than 0.2\n4. The value of 3rd Quartile is more than 0.5 but less than 0.6\n5. The maximum value is exactly 1.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarly another example of a boxplot representing the 'age_in_days' column:\nsn.boxplot(train['age_in_days'],color='green')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows the following:\n\n1. The minimum age of customers on the dataset is less than 10,000 days. ie; the youngest customer is of age less than 10,000 days.\n2. 1st Quartile of the age data is 15,000 days.\n3. Median age is less than 20,000 days.\n4. 3rd Quartile of the age data is near about 22,500 days.\n5. The maximum age of customers is more than 35,000 days. ie; the oldest customer is of age more than 35,000 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"# One more example of boxplot reading is as follows:\nsn.boxplot(train['Income'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This boxplot shows that majority of customers get income under Rs 1x10^7 (1 crore) per month. Two points between 4 & 6 on the plot show incomes below 5 crores & above 5 crores per month. The maximum income on the dataset is more than Rs 8 crores per month."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Visualizing the distribution of various variables may give better understanding of the data on the dataset. The distribution of continuous variables is visualized by using **Histograms**. I'll plot some histograms now:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the distribution of the 2nd column of train dataset using Histogram:\nsn.distplot(train['perc_premium_paid_by_cash_credit'],color='green')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is almost a right-skewed distribution. But it also looks like a little bimodal distribution as there's a small peak at the extreme right as well. But what this really shows is that the percentage of premium amount paid by cash or credit card by maximum customers is around 0. While the percentage of premium amount paid by cash or credit card by a few amount of customers is around 1. This means the mode of the distribution is 0. Let me check it using 'mode' function also:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['perc_premium_paid_by_cash_credit'].mode()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The reason why the plot for 'perc_premium_paid_by_cash_credit' column has so many values in between 0 & 1 is that this column has 'float' values. Next I'll check the distribution of 'Income' column which has all 'int' values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the 'income' distribution:\nsn.distplot(train['Income'],color='brown')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is extremely right skewed distribution. So to make some sense out of this data I'll have to use variable transformation. For this I'll take logarithm of the Income data & then plot its distribution. Now is the time to import '**numpy**' library:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing numpy library:\nimport numpy as np\n# Now taking logarithm of the Income data and then plotting its distribution:\nsn.distplot(np.log(train[\"Income\"]),color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This seems a better distribution. I can read it now & tell various statistics about it. The min & max are 10 & 14 respectively\n# Also it looks like the mode is 12. Let's confirm it using mode function:\nnp.log(train['Income']).mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I'll check distribution for another 'int' variable named 'no_of_premiums_paid':\nsn.distplot(train['no_of_premiums_paid'],color='cyan')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I'll check the 'Count_3-6_months_late' column distribution:\nplt.hist(train['Count_3-6_months_late'],histtype='stepfilled',label=str,color='purple',bins=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice from the distribution plot that although the 'Count_3-6_months_late' variable is considered float by python but it is actually an int variable as it has discrete values & not continuous! It reminds me of categorical variable. So now is the time I introduce the categorical variables & their analysis:\n\nThe categorical variables are discrete in nature & are stored as 'object' datatype. During the Univariate analysis of categorical variables, the task is to look for 'count' and 'count%'.\n\n"},{"metadata":{},"cell_type":"markdown","source":"Count gives the absolute frequency of each category in a categorical variable. While Count% shows the proportion of different categories in a categorical variable, expressed as %.\n\nTo analyse categorical variables the plotting methods used are frequency table & barplots. I'll make a **frequency table** now:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Count_3-6_months_late'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows the various 'categories' in 'Count_3-6_months_late' column. For instance, the 1st category tells that there are 66801 customers on the train dataset who don't have any number of premiums late by 3-6 months. But many of them are on either 'Count_6-12_months_late column' or on 'Count_more_than_12_months_late' column. This means they have some premiums due in 6-12month and/or more than 12 months categories. To be more clear with the frequency table, notice that there are 8826 customers who have 1 premium late by 3-6 months. For another example, 4 customers have 9 premiums late by 3-6 months. Finally there's 1 customer who has as much as 10 premiums late by 3-6 months! That's a lot!\n\nSimilar counts can be seen for 6-12 months and more than 12 months categories as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Count_6-12_months_late'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Count_more_than_12_months_late'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll plot the barplot for Count_6-12_months_late variable to visualize the above frequency plot:\ntrain['Count_6-12_months_late'].value_counts().plot.bar(color='green')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarly for 'Count_more_than_12_months_late' column:\ntrain['Count_more_than_12_months_late'].value_counts().plot.bar(color='brown')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll now visualize the 'target' data to check how many customers paid the premium on time:\ntrain['target'].value_counts().plot.bar(color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# But this plot doesn't quite explain the actual figures. So I'll check it using the value_counts function:\ntrain['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay so 74855 customers paid their premium on time. Only 4998 customers didn't pay their premium on time. Till now I've been examining a single variable at a time. Now I'll examine two variables at a time:"},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------------------------------------\n## Bivariate Analysis:\n\n![Bivariate Analysis](http://image.slideserve.com/182834/bivariate-distribution-n.jpg)\n\nHere I'll study two variables together to check if they are associated with each other. It may help me detect anomalies in the dataset (if present)."},{"metadata":{},"cell_type":"markdown","source":"There are 3 types of Bivariate Analysis. They are:\n\n1. Continuous-Continuous Analysis\n2. Categorical-Continuous Analysis\n3. Categorical-Categorical Analysis."},{"metadata":{},"cell_type":"markdown","source":"I'll start with the first one, ie; **Continuous-Continuous Analysis**: Here I'll examine two continuous variables together. I'll start with 'age_in_days' & 'Income' columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For this analysis I'll use scatter plot:\nsn.scatterplot(train['age_in_days'],train['Income'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot is telling the following facts:\n\n1. The maximum age of customers in the train dataset is around 38000 days & minimum age is around 4000 days.\n2. Majority of the customers earn income less than Rupees 1x10^7 (1 crore) monthly.\n3. Few anomalies are present in the income as some customers do earn a lot ranging from Rs 1 crore to Rs 9 crore monthly!\n4. The age range of these highly earning individuals lie between 15000 days to 25000 days."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I'll see the relation between income of customers and the number of premium they pay by cash or credit card:\nsn.scatterplot(train['perc_premium_paid_by_cash_credit'],train['Income'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows the following:\n\n1. The customers who earn less than Rs 1 crore monthly as salary pay 0 to 100 % of their premium by either cash or credit card.\n2. The high income customers pay in range 0% to 50% of their premium by either cash or credit card.\n3. The customer who earns around 9 crore a month pays 0% premium either by cash or credit card.\n4. The customer who earns around 5 crore a month pays 20% premium by cash or credit card.\n5. The customer who earns over 3 crore a month pays 50% premium by cash or credit card.\n6. It seems the high income group must be using some other means to pay majority of their premium."},{"metadata":{},"cell_type":"markdown","source":"**Categorical-Continuous Analysis**:\n\nNow I'll check the relation between different categorical variables with continuous variables using Barplots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll start with income of customers & how many premiums they are late by 3-6 months:\nsn.barplot(train['Count_3-6_months_late'],train['Income'],color='b',errcolor='c',errwidth='.26')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The black vertical lines on the tip of some bars represent uncertainty in the data. Anyway, this plot shows the following:\n\n1. All the customers who are late to pay their premiums by 3-6 months earn below Rs 4,00,000 (4 Lakhs) a month.\n2. Some customers who earn around Rs 3,50,000 a month but still they are as much as 13 premiums late by 3-6 months.\n3. Some customers who earn Rs 3 Lakh a month are 8 premiums late by 3-6 months.\n4. Some customers who earn Rs 2 Lakh a month are 0 premiums late.\n5. Some customers who earn little less than Rs 1 Lakh per month are upto 10 premiums late."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll check the similar relation between Income & 6-12 month late category:\nsn.barplot(train['Count_6-12_months_late'],train['Income'],errcolor='c',errwidth='.26')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows that only those customers who earn Rs 4 lakh & less are in the 6-12 months late category. There are some customers who earn around Rs 75,000 are as much as 17 premiums late by 6-12 months. This category shows even more late-payers than previous category. Let me see the next category to find if there are even more in it than these two categories:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the relation between Income of customers & how many premiums they are late by 12 months or more:\nsn.barplot(train['Count_more_than_12_months_late'],train['Income'],color='brown',errcolor='c',errwidth='.26')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this gives a sigh of relief that there are customers in this category who are atleast not more than 11 premium late unlike the previous category. Some customers who earn around Rs 3,50,000 are 11 premiums late by more than 12 months. And the similar story is told here."},{"metadata":{},"cell_type":"markdown","source":"One thing is clear from above 3 plots though that the customers who earn more than Rs 4 lakh a month have been paying their premiums on time. So the insurance company should not be worried atleast about all the rich customers as they seem to be sticking around with the company in near future. Great news! :D"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To make sure about the rich customer category I'll make a final barplot to see the relation between income & 'target' column:\nsn.barplot(train['target'],train['Income'],palette='rainbow',errcolor='c',errwidth='.26')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows that the customers who earn under Rs 1,80,000 a month are the ones the insurance company should be worried about. As they are the ones who did not pay their premiums on time."},{"metadata":{},"cell_type":"markdown","source":"**Categorical-Categorical Analysis**:\n\nNow I'll check relationship between various categorical variables with other categorical variables using matrix type Two-Way Tables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'm starting with 'target' & 'residence' columns:\npd.crosstab(train['target'],train['residence_area_type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This **two-way table** is displaying the following:\n\n1. The number of customers from both rural & urban residence area type who didn't pay their premium on time & also those who payed their premium on time.\n2. The number 0 on the target column represents those customers who didn't pay their premium on time.\n3. The number 1 on the target column represents the customers who paid their premium on time.\n4. The Rural column is showing that there are 1,998 rural customers of the insurance company who didn't pay their premiums on time. Whereas 29,672 rural customers paid their premiums on time.\n5. The urban column is showing that there are 3,000 urban customers who didn't pay their premiums on time. On the other hand, there are 45,183 urban customers of the insurance company who paid their premium on time."},{"metadata":{},"cell_type":"markdown","source":"Now I'll check the relation between the 'Sourcing channel' & residence area type to see if residents of the two area types prefer different types of sourcing channel:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train['residence_area_type'],train['sourcing_channel'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows the following:\n\n1. 17,115 rural customers prefer the sourcing channel A, whereas 26,019 urban customers prefer channel A.\n2. 6,506 rural customers prefer the sourcing channel B, whereas 10,006 urban customers prefer channel B.\n3. 4,780 rural customers prefer the sourcing channel C, whereas 7,259 urban customers prefer channel C.\n4. 2,999 rural customers prefer the sourcing channel D, whereas 4,560 urban customers prefer channel D.\n5. 270 rural customers prefer the sourcing channel E, whereas 339 urban customers prefer channel E."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I'll see in what proportion they prefer a specific sourcing channel:\ntrain['sourcing_channel'].value_counts()/len(train['sourcing_channel'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So 54% of total customers prefer channel A, 20.67% customers prefer channel B, 15% customers prefer channel C, and so on. Clearly A is the most preferred channel whereas E is the least preferred channel among the customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I want to see how many customers of this company live in urban area type & how many of them live in rural area:\ntrain['residence_area_type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the company has 48,183 urban customers & 31,670 rural customers. Now I'll have to see what percentage of total customers are urban/rural:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['residence_area_type'].value_counts() / len(train['residence_area_type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So 60.33% of all customers are urban & 39.66% of them are rural. I'll now finally check if the proportion of rural to urban is same across all the channels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"17115/(17115+26019) , 6506/(6506+10006) , 4780/(4780+7259) , 2999/(2999+4560) , 270/(270+339)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen from above result that other than channel E, all other channels are preferred in equal proportion by the customers. But the channel E is comparatively more preferred by the rural customers."},{"metadata":{},"cell_type":"markdown","source":"Now I'll check the relationship between 'number of premiums paid' & 'target' columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train['no_of_premiums_paid'],train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This Two-Way table shows the following:\n\n* The urban customers paid a lot more premiums than the rural customers.\n* There's an increasing trend of number of premiums paid by urban customers from 2 premiums to 8 premiums.\n* Maximum number of urban customers paid 8 premiums.\n* Maximum number of rural customers paid 7 premiums.\n* Then there's a decreasing trend from 8 premiums to 60.\n* 511 urban customers & 215 rural customers paid 2 premiums.\n* 1,511 urban customers & 235 rural customers paid 3 premiums.\n* 3,890 urban customers & 325 rural customers paid 5 premiums.\n* 6,547 urban customers & 326 rural customers paid 10 premiums.\n* 3,085 urban customers & 179 rural customers paid 15 premiums.\n* 1,055 urban customers & 79 rural customers paid 20 premiums.\n* 282 urban customers & 23 rural customers paid 25 premiums.\n* 85 urban customers & 6 rural customers paid 30 premiums.\n* 1 urban customer paid 50 premiums but 2 rural customers paid 50 premiums! This is good.\n* 1 urban customer paid 60 premiums on the other hand 1 rural customer paid 59 premiums!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I'm moving on to check relationship between 'residence area type' & 'count 3-6 months late' columns:\npd.crosstab(train['residence_area_type'],train['Count_3-6_months_late'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows the following:\n\n* 40,234 urban customers & 26,567 rural customers have 0 premiums due by 3-6 months.\n* 5,401 urban customers & 3,425 rural customers have 1 premium due to pay by 3-6 months.\n* 106 urban customers & 62 rural customers are 5 premiums late by 3-6 months.\n* 1 urban customer & no rural customer is 13 premiums late by 3-6 months."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarly I'm checking relationship between 'residence area type' & 'count 6-12 months late' columns:\npd.crosstab(train['residence_area_type'],train['Count_6-12_months_late'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here 3 urban & 1 rural customers are late by 10 premiums, 2 urban customers are late by 13 premiums, 1 urban & 1 rural customers are late by 14, 1 urban customer is late by as much as 17 premiums by 6-12 months, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finally the relation between residence area type & 'count more than 12 months' columns:\npd.crosstab(train['residence_area_type'],train['Count_more_than_12_months_late'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This table shows that the in 'more than 12 month' category, the maximum number of premiums late by an urban customer is 11. While the maximum number of premiums late by a rural customer is 8."},{"metadata":{},"cell_type":"markdown","source":"I'll now represent a Correlation plot to display the relationship between all the numerical variables together at ones. A characteristic of Correlation plot is that it only includes numerical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr= train.corr()\nmask=np.array(corr)\nmask[np.tril_indices_from(mask)] = False\nfig,ax = plt.subplots()\nfig.set_size_inches(20,10)\nsn.heatmap(corr, mask=mask, vmax=0.9, square=True, annot=True, cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This correlation plot is representing the strength as well as direction of correlation between all the variables. For example:\n\n* The 'target' and 'id' variables/columns have a correlation coefficient of -0.0051. This means that these two variables are negatively correlated with each other by a strength of 0.0051. This strength is very weak.\n* The 'target' & 'perc_premium_paid_by_cash_credit' variables have a correlation coefficient of -0.24. This means they are also negatively correlated but by a strength of 0.24. Although this strength too is weak but it is comparatively stronger than the previous pair.\n*  Negative correlation coefficient means when the value of one variable increases the value of the other one decreases. While positive correlation coefficient means when value of one variable increases the value of other variable also increases.\n* 'target' and 'age_in_days' are positively correlated by a weak strength of 0.095.\n* It is clear that the strongest strength between target & any other variable is of 0.29 units only. This is a negative correlation between 'target' & 'Count_more_than_12_months_late' columns. This means the more the value of 'count' a customer has, the less he's going to a '1' in his target column. Though the strength of the counts is not that high to give any noticeable difference."},{"metadata":{},"cell_type":"markdown","source":"I'll need to further investigate this dataset to check if it has any missing values."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n## Missing Value Investigation:\n\n![Missing Values](https://datascienceplus.com/wp-content/uploads/2015/08/missing-values-r.png)\n\nThere may be missing or empty values in the dataset due to either error in data collection or due to some error in reading data. Also, since 'income' is involved in this dataset so there may be some customers who may not be willing to reveal their true income. Whatever the reason, I'll have to investigate for the missing values because I have to train my model from the 'train' dataset to predict the 'target' values in the 'test' dataset. So if I leave the missing values in the 'train' dataset and train my model the predictions of test dataset may get highly affected. Resulting in totally wrong prediction results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Firstly, checking the missing values in the whole train dataset using 'isnull' function:\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is telling the following:\n\n1. There are 97 missing values in Count_3-6_months_late column\n2. There are 97 missing values in Count_6-12_months_late column\n3. There are 97 missing values in Count_more_than_12_months_late column\n4. There are 2,974 missing values in application_underwriting_score column\n5. No other column in the dataset has any missing value."},{"metadata":{},"cell_type":"markdown","source":"Now I have to treat these missing values otherwise they'll affect my final predictions. I'll treat the missing values of all the three \"count\" columns by filling up zero in place of the missing values. This is due to the fact that I don't know whether a customer is 0 premium late or 1 or 2 or more premiums late. The customer can be practically any number of premiums late which is impossible to find out. So it'd be better if I assume he/she is 0 premium late. Because if I put 1 in front of someone who has really paid all of his/her premiums on time & I treat my model with these wrong values, then it'll result in bad predictions in the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# So, filling 0 as missing values using 'fillna' function in all 3 'Count' columns:\ntrain['Count_3-6_months_late'].fillna(0,inplace=True)\ntrain['Count_6-12_months_late'].fillna(0,inplace=True)\ntrain['Count_more_than_12_months_late'].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for the missing values in 'application_underwriting_score' column, I'll fill them with the mean of this column. There's a condition that applications under the score of 90 do not get insured. So obviously if I fill the missing values of this column with 0 & train my model then similar kind of data in test dataset will get affected. There are various methods to fill up a missing value, apart from using 0. Few of them are mean, median, mode, minimum value, maximum value, etc. The mean is the average of all the values of a particular column. The median is the middle value of all the values of the column. The mode is the most common value in the entire column. So filling the underwriting score with mean seems to be the best option."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling up missing values in 'application_underwriting_score column' with the mean of this column:\ntrain['application_underwriting_score'].fillna(train['application_underwriting_score'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verifying if all missing values got filled up or not:\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if 'test' dataset has any missing values:\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Yes it does! And it has missing values on same columns as train dataset.\n# So I'll do the same filling on the test dataset as done on train dataset:\ntest['Count_3-6_months_late'].fillna(0,inplace=True)\ntest['Count_6-12_months_late'].fillna(0,inplace=True)\ntest['Count_more_than_12_months_late'].fillna(0,inplace=True)\ntest['application_underwriting_score'].fillna(test['application_underwriting_score'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verifying missing values in test dataset:\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now is the time to adjust the dataset according to the scikit learn implementation. So here starts the next part called training the model:"},{"metadata":{},"cell_type":"markdown","source":"---------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------\n# 2. Training the Model\n\n![Training model](https://www.recordedfuture.com/wp-content/uploads/machine-learning-cybersecurity-applications.png)"},{"metadata":{},"cell_type":"markdown","source":"The **scikit learn algorithms** take two separate arguments. This means they need independent variables separately & the dependent variable (or target variable) separately. But since in the train dataset both independent & dependent variables are present together so I need to separate them out.\n\nFirstly, I'll create a set of independent variables from the train dataset. So I'm dropping the 'target' variable from it using axis=1. This axis=1 specifies that the drop shall happen from the column. I'll store this set in an object called \"x\" as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.drop('target',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I need my separate target variable. So keeping only the '**target**' variable in an object y:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a 'train.csv' file, I have just a single dataframe. But for the purpose of modeling I need separate train & test sets from this main train.csv file. So I'll split this original train dataset into further train & test datasets.\n\nSo for the purpose of splitting I'll use a **module called \"Model Selection\"**. In that module I'll use the \"train_test_split\" function as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This split function accepts an independent variable 'x' & a dependent variable 'y', and then split them into two parts. First is independent variable for train and independent variable for test. And then dependent variable for train and dependent variable for test. I'm naming them as **train_x, test_x, train_y & test_y** sets respectively as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(x, y, random_state=11111, shuffle=True, train_size=None, test_size=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the brackets there are various **parameters**. During the scoring stage, these parameters can be adjusted to get the **best model score or accuracy**.\n\nOne thing about sklearn is that it needs input in terms of numbers only. ie; either as int datatype or as float datatype. This means **sklearn doesn't take string as input**. So to get rid of strings I need to create numeric features out of these categorical or string features. So I'll now use a concept called \"**Dummification**\" where I'll create dummies of variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummies of both train_x and test_x sets:\ntrain_x = pd.get_dummies(train_x)\ntest_x = pd.get_dummies(test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll check the proportion of 1s and 0s in the dependent variables of train & test that I just created:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y.value_counts()/len(train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means there are 93.6% 1s & 6.3% 0s in train_y set. I'll now check the same for test_y set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_y.value_counts()/len(test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means there are 94% 1s & 5.9% 0s in test_y set. So I've got almost similar proportions of 1s & 0s in both dependent sets. This is good because now whatever performance I'll get on the train will get emulated on the test."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Model:\n\n![Logistic Regression](https://1.bp.blogspot.com/_Tndn7IbKcao/Syu0vkRlGtI/AAAAAAAAAIk/TQ-K2fOr9w0/s400/SigmoidPlot1.png)\n\nNow I'll train my model on train_x & train_y and predict on test_x. For this purpose I'll import the **LogisticRegression** which is inside a module named **linear_model** in scikit learn:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll create an object for this so that I can use the \"**fit**\" & the \"**predict**\" functions on it. I'm naming this object as logr:"},{"metadata":{"trusted":true},"cell_type":"code","source":"logr = LogisticRegression(n_jobs=1,max_iter=100,random_state=11111)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The parameters inside the brackets can be adjusted(increased or decreased) to improve model score or **accuracy** during the scoring stage.\n\nNow I'll run the fit function on train_x & train_y using my LogisticRegression model as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"logr.fit(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scikit learn provides a function called \"**score**\" which can be used **to check the performance of my model**. This shows the accuracy for how well my model fits. It takes 2 arguments one independent variable & one dependent variable. I'll use this function 1st on train_x & train_y and then on test_x & test_y as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"logr.score(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logr.score(test_x,test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This means there's 93.6 % accuracy on my train dataset & 94 % accuracy on the test dataset. This also means that the test sample is really a representative of the train sample. However, a training score of 93.6 % is not that great for a good prediction of an unseen test dataset. The test set that I've been using till now was made out of the train dataset. But the test dataset that I've got as \"test.csv\" file has data of totally new customers. So it's like an unseen data for my model. For this reason, my LogisticRegression model will not give true predictions for the test.csv dataset."},{"metadata":{},"cell_type":"markdown","source":"So I'll try another model called **DecisionTreeClassifier** and see if I can achieve better training score from that:"},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier Model:\n\n![Decision Tree](https://www.xoriant.com/blog/wp-content/uploads/2017/08/a-decisionTreesforClassification-AMachineLearningAlgorithm.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Again I'll train my model on train_x & train_y and predict on test_x. This time I'm importing the Decision Tree Classifier which is inside the **module called Tree** in scikit learn:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll create an object for this also so that I can use the \"fit\" & the \"predict\" functions on it as well. I'm naming the object as dtc."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc= DecisionTreeClassifier(max_depth=None, max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll run the fit function on train_x & train_y using my DecisionTreeClassifier model as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc.fit(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This time I'll use the score function with dtc. So using it 1st on train_x & train_y and then on test_x & test_y as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc.score(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc.score(test_x,test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there's 100 % accuracy on my train dataset & 90.3 % accuracy on the test dataset. This means my model is 100% accurate on the train dataset now. But still, a test accuracy of 90.3% is not that great. So I'll take up one more model which is called **Random Forest**:"},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Model:\n\n![Random Forest](https://cdn-images-1.medium.com/max/1024/0*DrAmS5L2ekBlAXJy.png)"},{"metadata":{},"cell_type":"markdown","source":"As before, I'll be training my model on train_x & train_y and predicting on test_x. This time I'm importing the Random Forest Classifier which is inside the **module called Ensemble** in scikit learn:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating an object for the Random Forest Classifier so that I can use the \"fit\" & the \"predict\" functions on it. I'm naming the object as rfc:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the fit function on train_x & train_y using my **RandomForestClassifier** model as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.fit(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now using the score function with rfc. Using it 1st on train_x & train_y and then on test_x & test_y as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(test_x,test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that's a pretty **good accuracy** on both sets. These scores look close enough to obtain **predictions** now:"},{"metadata":{},"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n# 3. Predicting the target values\n\n![target Predictions](https://fintech4us.files.wordpress.com/2017/12/making-predictions-clipart-1.jpg?w=720&h=540)"},{"metadata":{},"cell_type":"markdown","source":"As my Random Forest model has achieved significant score, I can use this model to get the predictions. I'll start with **getting predictions on train_x** set as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.predict(train_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarly getting predictions on test_x as follows:\nrfc.predict(test_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll pick up the main test.csv dataset. I've already filled the missing values on this dataset. I'll first **compare the size**s of this and train_x set to see how different is test dataset currently from the train_x:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape , train_x.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay so this shows that I have less number of columns in the test dataset as compared to train_x set. This is due to the use of that 'dummies' function on train_x set. So I'll have to use this dummies function on test dataset also:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll again check the size of test dataset to verify if now I **have equal number of columns** on both train_x & test datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! So now I can use my rfc model to get predictions on this test dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm storing these predictions in an object called '**test_prediction**':"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prediction = rfc.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I'll create a **new csv file**. Firstly I'll create a 'target' column in a csv file, then I'll **submit** the test predictions that I've got above in that column. And finally I'll save this csv file with a name. This process is as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = test_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('customer_premium_on_time.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Success](https://www.highschoolillustrated.com/wp-content/uploads/2013/01/success_sign.png)"},{"metadata":{},"cell_type":"markdown","source":"The task has been completed successfully. It's my first detailed work so **please upvote** if you liked it or if it helped you in any way :)"},{"metadata":{},"cell_type":"markdown","source":"**Thank you** very much for looking into my notebook. **Best regards**,\n* Rachit Shukla :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}