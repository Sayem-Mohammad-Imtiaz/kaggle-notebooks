{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Car Price Prediction Using Keras","metadata":{}},{"cell_type":"markdown","source":"#### Below are the steps which we will be basically following:\n\n1. [Step 1: Reading and Understanding the Data](#1)\n1.  [Step 2: Cleaning the Data](#2)\n    - Missing Value check\n    - Data type check\n    - Duplicate check\n1. [Step 3: Data Visualization](#3)\n    - Heatmap\n1. [Step 4: Data Preprocessing](#4) \n   - One-Hot Encoding\n1. [Step 5: Splitting the Data into Training and Testing Sets](#5)\n1. [Step 6: Normalizing the Data](#6)\n1. [Step 7: Building a Model](#7)\n1. [Step 8: K-Fold Validation](#8)\n1. [Step 9: Training](#9)\n1. [Step 10: Model Evaluation](#10)\n   - MSE Score\n1. [Step 11: Prediction](#11)","metadata":{}},{"cell_type":"markdown","source":"## Setting-up Envoirnment ","metadata":{}},{"cell_type":"markdown","source":"Firstly, we will import all the required libraries.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nplt.rcParams['figure.figsize']=(12,5)\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras import regularizers\n#!pip install openpyxl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## Loading Data","metadata":{}},{"cell_type":"code","source":"df_car = pd.read_csv(\"../input/car-price-prediction/CarPrice_Assignment.csv\")\n#data_car = pd.read_excel(\"../input/car-price-prediction/Data Dictionary - carprices.xlsx\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_car.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Checking Shape and Size","metadata":{}},{"cell_type":"code","source":"print(df_car.shape)\nprint(df_car.size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## Cleaning the Data","metadata":{}},{"cell_type":"code","source":"df_car.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing value.","metadata":{}},{"cell_type":"code","source":"df_car.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dropping Useless Column","metadata":{}},{"cell_type":"code","source":"df_car.drop(columns = ['car_ID'], inplace= True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n## Data Visualization","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.heatmap(df_car.select_dtypes(include=['int','float']).corr(),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Price is highly(positively) correlated with wheelbase, carlength, carwidth, curbweight and enginesize. And negatively correlated with citympg and highwaympg.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df_car.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coverting categorical data to dummy variables\ncar_dummies = pd.get_dummies(df_car,columns=['symboling','CarName', 'fueltype', 'aspiration', 'doornumber','carbody', \n                                             'drivewheel','enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"car_dummies.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## Splitting the Data\nSplitting data into training and testing data.","metadata":{}},{"cell_type":"code","source":"# Training Data\nnp.random.seed(11111) \nmsk = np.random.rand(len(car_dummies)) < 0.72\nX_train = car_dummies[msk]\nX_test = car_dummies[~msk]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(X_train))\nprint(len(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target Data \ny_train = X_train.pop('price')\ny_test = X_test.pop('price')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"{X_train.columns.get_loc(c): c for idx, c in enumerate(X_train.columns)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n## Normalizing the Data\nHere we are normalizing data by subtracting data by mean of the data and then dividing by standard deviation of the data.","metadata":{}},{"cell_type":"code","source":"X_mean = X_train.iloc[:,0:13].mean(axis=0) # taking mean of training data\nX_train.iloc[:,0:13] -= X_mean # subtracting the mean from training data\nX_std = X_train.iloc[:,0:13].std(axis=0) # taking std of training data\nX_train.iloc[:,0:13] /= X_std # dividing train data by std\nX_test.iloc[:,0:13] -= X_mean # subrating the mean from testing data\nX_test.iloc[:,0:13] /= X_std # dividing test data by std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_mean = y_train.mean() \ny_train -= y_mean\ny_std = y_train.std()\ny_train /= y_std\ny_test -= y_mean\ny_test /= y_std","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Changing Data Type To Float","metadata":{}},{"cell_type":"code","source":"X_train = np.asarray(X_train).astype(float)\nX_test = np.asarray(X_test).astype(float)\n\ny_train = np.asarray(y_train).astype(float)\ny_test = np.asarray(y_test).astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n## Building a Model","metadata":{}},{"cell_type":"code","source":"def build_model():\n    model = Sequential()\n    model.add(Dense(80 , activation='relu', input_shape=(X_train.shape[-1],))) # Input Layer\n    model.add(Dropout(0.5)) # Dropout Layer\n    model.add(Dense(40 , activation='relu'))\n    model.add(Dropout(0.5)) # Dropout Layer\n    model.add(Dense(20 , activation='relu'))\n    model.add(Dropout(0.5)) # Dropout Layer\n    model.add(Dense(10 , activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # Compiling Model\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note** the network with the mse loss functionâ€”mean squared error,\nthe square of the difference between the predictions and the targets. This is a widely\nused loss function for regression problems.","metadata":{}},{"cell_type":"code","source":"build_model().summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"8\"></a> <br>\n## K- Fold Validation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nk =  4 # no of folds\nnum_val_samples = len(X_train) // k\nnum_epochs = 100\nall_scores_relu = []\nfor i in range(k):\n    print('processing fold #', i)\n    val_X = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate([X_train[:i * num_val_samples],X_train[(i + 1) * num_val_samples:]],  axis=0)\n    # print(partial_train_data)\n    partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)\n    model = build_model()\n    model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_X, val_y, verbose=0)\n    all_scores_relu.append(val_mae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Validation MSE","metadata":{}},{"cell_type":"code","source":"val_mse","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_scores_relu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n## Training\nHere we will call model and train on the training data and evaluate on the test data.","metadata":{}},{"cell_type":"code","source":"model_relu = build_model()\nmodel_relu.fit(X_train, y_train,epochs= 80, batch_size=1, verbose=0)\ntest_mse_score, test_mae_score = model_relu.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n## Model Evaluation","metadata":{}},{"cell_type":"code","source":"# MSE Score\ntest_mse_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAE Score\ntest_mae_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"11\"></a> <br>\n## Prediction","metadata":{}},{"cell_type":"code","source":"x_relu = model_relu.predict(X_test[5].reshape(1,X_test.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **Note** that here we will use the reverse process of Normalization to retrieve our values of price in thousand of dollars i.e. x = (y - mean)/ std ==>> we will calculate y = x * std + mean and then we will compare it with our target values.","metadata":{}},{"cell_type":"code","source":"x_relu * y_std + y_mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Actual Value","metadata":{}},{"cell_type":"code","source":" y_test[5] * y_std + y_mean ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If this Kernel helped you in any way, some <span style=\"color:red\">UPVOTES !!!</span> would be very much appreciated.","metadata":{}}]}