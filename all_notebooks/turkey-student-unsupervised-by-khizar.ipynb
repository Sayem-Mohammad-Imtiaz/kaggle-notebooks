{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport collections\nfrom collections import Counter as count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Data Ingestion :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"data= pd.read_csv(\"../input/uci-turkiye-student-evaluation-data-set/turkiye-student-evaluation_generic.csv\")\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Null Values :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are no null values in a given dataset.**"},{"metadata":{},"cell_type":"markdown","source":"### *Descriptive Statistics :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference :**\n* Descrpitive statistics of columns difficulty, Q1, Q2....Q28 are almost same.\n* Mean and Median of class column approximately same.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *Exploratory Data Analysis :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='class',data=data)\n# sns.pairplot(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.boxplot(data=data.iloc[:,6:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It was observed that Q14,Q15,Q17,Q19:Q22 and Q25 questions with good rating.**"},{"metadata":{},"cell_type":"markdown","source":"# *Scaling :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\ndata = pd.DataFrame(sc.fit_transform(data),columns=data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *K Means Clustering :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_range = range(1,20)\ncluster_errors = []\nfor num_cluster in cluster_range:\n    clusters = KMeans(num_cluster)\n    clusters.fit(data)\n    cluster_errors.append(clusters.inertia_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'No of Clusters':cluster_range, 'Cluster Error':cluster_errors})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Elbow Plot :* "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(cluster_range,cluster_errors,marker = 'o')\nplt.title('Elbow Plot')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Error of Clusters')\nplt.xticks(cluster_range)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Based on the elbow graph we can go for 3 clusters.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3)\ny_kmeans = kmeans.fit_predict(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_clusters = count(y_kmeans)\nk_clusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Above count was the count of 3 clusters.**"},{"metadata":{},"cell_type":"markdown","source":"# *Hierarchical Clustering :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, linkage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nZ = linkage(data, method='ward')\ndendrogram(Z, leaf_rotation=90, p=10, truncate_mode='level', leaf_font_size=6, color_threshold=8)\nplt.title('Dendogram')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By the Dendogram we can see that there are 3 optimal number of clusters.**"},{"metadata":{},"cell_type":"markdown","source":"**Now fit Hierarchical clustering to the data**"},{"metadata":{},"cell_type":"markdown","source":"## *Agglomerative Clustering :*\n#### it is Hierarchical clustering algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac.fit(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac.labels_ ## clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h_clusters = count(ac.labels_)\nh_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h_clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clusters = ['Kmean','Hierarchical']\npd.DataFrame({'K_Clusters':k_clusters, 'Hierarchical':h_clusters})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference :**\n* From the above dataframe we can compare the clusters of both the algorithmns.\n* Third cluster number from both methods was almost close."},{"metadata":{},"cell_type":"markdown","source":"# *Convert Unsupervised data into Supervised data :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3, max_iter=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.fit(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count(kmeans.labels_) # clusters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'] = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no outlier \ndf.label.plot(kind='box')\n# sns.boxplot(x='label',data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.label.value_counts().plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df,hue='label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *PCA :*"},{"metadata":{},"cell_type":"markdown","source":"**Since the data is already scaled , now apllying PCA fro dimensionality reduction :**\n#### only apply on features "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pca = pca.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.cumsum is used to calculate the accumulative sum of array\npca.explained_variance_ratio_ \n# The pca.explained_variance_ratio_ parameter returns a vector of the variance explained by each dimension.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cumsum=np.cumsum(pca.explained_variance_ratio_)\ncumsum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nplt.plot(range(1,34), cumsum, color='k', lw=2)\n\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\n\nplt.axvline(8, c='b')\nplt.axhline(0.9, c='r')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**90 percent of data variance consists 8 components.**\n#### if i chosse 8 columns than i save 90% variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=8)\npca.fit(data)\ndata_pca = pd.DataFrame(pca.transform(data))\ndata_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pca.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In statistics, kernel density estimation is a non-parametric \n# way to estimate the probability density function of a random variable. \n\nsns.pairplot(data_pca, diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Kmeans Clustering :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_range = range(1,16)\ncluster_errors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters, n_init=10, max_iter=100)\n    clusters.fit(data_pca)\n    \n    cluster_errors.append(clusters.inertia_)\n    \npd.DataFrame({'num_clusters':cluster_range, 'Error': cluster_errors})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Elbow Plot :**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.plot(cluster_range, cluster_errors, marker = \"o\" )\nplt.title('Elbow Plot')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Error')\nplt.xticks(cluster_range)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_df = data_pca.copy()\nkmeans = KMeans(3, n_init=10, max_iter=100)\nkmeans.fit(pca_df)\npca_df['label'] = kmeans.labels_\npca_df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Agglomerative Clustering :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nlink = linkage(data_pca, method='ward')\ndendrogram(link, leaf_rotation=90, p=10, truncate_mode='level', leaf_font_size=6, color_threshold=8)\nplt.title('Dendogram')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above dendogram we can see 3 clusters.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean',  linkage='ward')\nac.fit(data_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_ac=ac.fit_predict(data_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count(y_ac)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first0=[2226,2756]\nsecond1=[1231,2379]\nthird2=[2363,685]\nclusters=['Kmeans','Agglm Cluster']\nd=pd.DataFrame({'Clusters':clusters,'FirstC':first0,'SecondC':second1,'ThirdC':third2})\nd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference :**\n* First cluster is some what nearer in both the methods."},{"metadata":{},"cell_type":"markdown","source":"# *Splitting the data before PCA :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(columns='label')\ny=df['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=1)\n\nprint(Xtrain.shape)\nprint(Xtest.shape)\nprint(ytrain.shape)\nprint(ytest.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Logistic Regression :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(Xtrain, ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training score =', lr.score(Xtrain, ytrain))\nprint('Test score =', lr.score(Xtest, ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred1=lr.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc1=(metrics.accuracy_score(ytest,ypred1))\nacc1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(ytest, ypred1)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is good fit.**"},{"metadata":{},"cell_type":"markdown","source":"### *Decision Tree Classifier :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(Xtrain, ytrain)\n\nprint('Training score =', dt.score(Xtrain, ytrain))\nprint('Test score =', dt.score(Xtest, ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred2=dt.predict(Xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc2=(metrics.accuracy_score(ytest,ypred2))\nacc2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(ytest, ypred2)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is under fit.**"},{"metadata":{},"cell_type":"markdown","source":"### *KNN :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nscore=[]\nfor k in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(Xtrain, ytrain)\n    ypred3=knn.predict(Xtest)\n    accuracy=metrics.accuracy_score(ypred3,ytest)\n    score.append(accuracy*100)\n    print (k,': ',accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.index(max(score))+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(max(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(Xtrain, ytrain)\n\nprint('Training score =', knn.score(Xtrain, ytrain))\nprint('Test score =', knn.score(Xtest, ytest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is good fit:**"},{"metadata":{},"cell_type":"markdown","source":"### *Naive Bayes :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ngnb.fit(Xtrain, ytrain)\n\nprint('Training score =', gnb.score(Xtrain, ytrain))\nprint('Test score =', gnb.score(Xtest, ytest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is Best Fit.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Algorithm=['LogisticRegression','Decision Tree','KNN','Naive Bayes']\nTrain_Accuracy=[0.985,1.00,0.977,0.988]\nTest_Accuracy=[0.975,0.939,0.963,0.988]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Before_PCA = pd.DataFrame({'Algorithm': Algorithm,'Train_Accuracy': Train_Accuracy,'Test_Accuracy':Test_Accuracy})\nBefore_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference :**\n* Naive Bayes algorithm has performed well with an accuracy 0f 98.8 percent.\n* Decision Tree has not performed well and it is under fit."},{"metadata":{},"cell_type":"markdown","source":"# *Splitting the data after PCA :*"},{"metadata":{},"cell_type":"raw","source":"data_pca is the dataset got after PCA. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=data_pca.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(3, n_init=5, max_iter=100)\nkmeans.fit(df1)\ndf1['label'] = kmeans.labels_\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1=df1.drop(columns='label')\ny1=df1['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *Logistic Regression :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_pca = LogisticRegression()\nlr_pca.fit(X_train, y_train)\nprint('Training score =', lr_pca.score(X_train, y_train))\nprint('Test score =', lr_pca.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is good fit.**"},{"metadata":{},"cell_type":"markdown","source":"### *Decision Tree Classifier :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_pca = DecisionTreeClassifier()\ndt_pca.fit(X_train, y_train)\nprint('Training score =', dt_pca.score(X_train, y_train))\nprint('Test score =', dt_pca.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is somewhat underfit.**"},{"metadata":{},"cell_type":"markdown","source":"### *KNN :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"score=[]\nfor k in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    ypred=knn.predict(X_test)\n    accuracy=metrics.accuracy_score(ypred,y_test)\n    score.append(accuracy*100)\n    print (k,': ',accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score.index(max(score))+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(max(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_pca = KNeighborsClassifier(n_neighbors=7)\nknn_pca.fit(X_train, y_train)\n\nprint('Training score =', knn_pca.score(X_train, y_train))\nprint('Test score =', knn_pca.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is good fit.**"},{"metadata":{},"cell_type":"markdown","source":"### *Naive Bayes :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb_pca = GaussianNB()\ngnb_pca.fit(X_train, y_train)\nprint('Training score =', gnb_pca.score(X_train, y_train))\nprint('Test score =', gnb_pca.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model is good fit.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Algorithm=['LogisticRegression','Decision Tree','KNN','Naive Bayes']\nTrain_Accuracy=[0.987,1.00,0.987,0.975]\nTest_Accuracy=[0.979,0.995,0.980,0.967]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"After_PCA = pd.DataFrame({'Algorithm': Algorithm,'Train_Accuracy': Train_Accuracy,'Test_Accuracy':Test_Accuracy})\nAfter_PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference :**\n* All the models performed well.\n* Decision Tree has 100% on training and 99.5% on testing."},{"metadata":{},"cell_type":"markdown","source":"# *Final Model :*"},{"metadata":{"trusted":true},"cell_type":"code","source":"Algorithm=['LR BPCA','DT BPCA','KNN BPCA','NB BPCA','LR APCA','DT APCA','KNN APCA','NB APCA']\nTrain_Accuracy=[0.985,1.00,0.977,0.988,0.987,1.00,0.987,0.975]\nTest_Accuracy=[0.975,0.939,0.963,0.988,0.979,0.995,0.980,0.967]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Final = pd.DataFrame({'Algorithm': Algorithm,'Train_Accuracy': Train_Accuracy,'Test_Accuracy':Test_Accuracy})\nFinal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"BPCA indicates BEFORE PCA\nAPCA indicates AFTER PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,6))\nsns.lineplot(x=\"Algorithm\", y=\"Train_Accuracy\",data=Final,palette='hot',label='Train Accuracy')\nsns.lineplot(x=\"Algorithm\", y=\"Test_Accuracy\",data=Final,palette='hot',label='Test Accuracy')\n\nplt.xticks(rotation=90)\nplt.title('MLA Accuracy Comparison')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Inference :**\n* Naive Bayes before PCA  performed well.\n* Logistic Regression after PCA performed well.\n* Naive Bayes(Before PCA) is the best model from all the model where training and testing sores are equal."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}