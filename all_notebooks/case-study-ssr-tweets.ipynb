{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries/Modules Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import of Data and Pre-Processing\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/tweets-on-sushant-singh-rajput/SSR_tweets.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\ndef preprocess(textdata):\n    processedText = []\n    \n    # Create Lemmatizer and Stemmer.\n    wordLemm = WordNetLemmatizer()\n    \n    # Defining regex patterns.\n    userPattern       = '@[^\\s]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in textdata:\n        tweet = tweet.lower()\n        \n     \n        # Replace @USERNAME to ' '.\n        tweet = re.sub(userPattern,' ', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n        tweetwords = ''\n        for word in tweet.split():\n            # Checking if the word is a stopword.\n            #if word not in stopwordlist:\n            if len(word)>1:\n                # Lemmatizing the word.\n                word = wordLemm.lemmatize(word)\n                tweetwords += (word+' ')\n            \n        processedText.append(tweetwords)\n        \n    return processedText","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We removed all Username and non alphabets pattern ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text = data['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nt = time.time()\ncorpus = preprocess(text)\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.Series(corpus)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_list = [word for line in df for word in line.split()]\n\nsns.set(style=\"darkgrid\")\ncounts = Counter(word_list).most_common(50)\ncounts_df = pd.DataFrame(counts)\ncounts_df\ncounts_df.columns = ['word', 'frequency']\n\nfig, ax = plt.subplots(figsize = (12, 12))\nax = sns.barplot(y=\"word\", x='frequency', ax = ax, data=counts_df)\nplt.savefig('wordcount_bar.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(\n    background_color='black',\n    max_words=50,\n    max_font_size=40, \n    scale=5,\n    random_state=1,\n    collocations=False,\n    normalize_plurals=False\n).generate(' '.join(word_list))\n\n\nplt.figure(figsize = (12, 10), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}