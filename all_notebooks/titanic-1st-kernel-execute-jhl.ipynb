{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"markdown","source":"# Titanic Survival - Exploration + Baseline Model\n\nThis is a simple notebook on exploration and baseline model to predict who will survive the sinking of the Titanic\n\n## **Contents**   \n[1. Set Environment, Define  Functions, Load Data](#1)  \n[2. Examine Basic Structure of Data set and Features](#2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.1 Basic Dataframe Info](#2.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [2.2 Feature Values](#2.2)  \n[3. Hot-Code Categorical Data]((#3)  \n[4. RF Check of Feature Importance](#4)  \n[5. Create Test Data Set](#5)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.1 Check working director files](#5.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.2 Roeve any desired working director files](#5.2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.3 Roeve any desired working director files](#5.3)  \n[6. Impute Missing Feature Values](#6)  \n[7. Treat Feature Imblances](#7)\n\n\n&nbsp;&nbsp;&nbsp;&nbsp; [5.1 Number of Relatives & CabinYN](#5.1)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.2 Titles (from Name)](#5.2)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.3 Age Categories](#5.3)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.4 Fare Categories](#5.4)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.5 Age | Class](#5.5)  \n&nbsp;&nbsp;&nbsp;&nbsp; [5.6 Fare/Person](#5.6)  \n[6. 2nd Data Exploration](#6)  \n[7. Feature Engineering](#7)  \n[8. 3rd Data Exploration](#8)  \n[9. Model Testing](#9)  \n[10. ???](#10)\n\n## <a id=\"1\">1. Initialize </a>\n\n[//]: # (This syntax works like a comment, and won't appear in any output.)"},{"metadata":{"trusted":true,"_uuid":"965ef1637f3bad085d36563e70344a0bda5a5c6a","scrolled":false},"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy.stats\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n## Function to Hot-Code a categorical variable_\n    # Takes as parameters 1) a dataframe 2) a string variable with the column name to recode\n    # Leaves in tack the initial variable that was recoded\n\ndef HotC(dframe,col):   # Function to Hot-Code a categorical variable\n    if not(isinstance(dframe,pd.DataFrame)):\n        print('!!ERROR!! The first variable in the HotC function must be a dataframe')\n        return\n    if not(isinstance(col,str)):\n        print('!!ERROR!! The second variable in the HotC function must be a string representing a column in the dataframe')\n        return\n    #df2=pd.DataFrame(dframe[col].str.get_dummies())\n    df2=pd.get_dummies(dframe[col],prefix=col)\n    df3=pd.concat([dframe,df2],axis=1)\n    return df3\n\nprint('Modules and User Defined Functions set')\n\n# Input Titanic Data\npath='../input/titanic-machine-learning-from-disaster/'\n\ntrain_df = pd.read_csv(path + \"train.csv\")\ntest_df = pd.read_csv(path + 'test.csv')\n\n# Write out Data sets for download\ntrain_df.to_csv('train_df_raw.csv', index = False)\ntest_df.to_csv('test_df_raw.csv', index = False)\n\n# Create variables with column header names\ntrain_colnames = list(train_df.columns.values)\ntest_colnames = list(test_df.columns.values)\nprint(\"Data Loaded\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d7559a3ccc31548394c6da87ae05bbd7c6a05a1"},"cell_type":"markdown","source":"## <a id=\"2\">2.  Initial Data Exploration </a> \n### &nbsp;&nbsp;  <a id=\"2.1\">2.1  Basic DataFrame Info </a>\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"## Basic Dataframe structure\n# Widen display\npd.options.display.max_columns=20   ## Force number of coluns to show\npd.options.display.max_rows=1000     ## Force number of rows to show\n\n##     Display train_df\nprint('BASIC STRUCTURE: train_df')\ntrain_df.info()\nprint('SAMPLE DATA: train_df')\nprint(train_df.head(5))\n\n##    Display test_df\nprint('\\n','\\n','============================================')\nprint('BASIC STRUCTURE: test.df')\ntest_df.info()\nprint(train_df.head(5))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b2cd3cb45f1a25d0c9e70372d98ce7f21e6ef2a"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"2.2\">2.2  Check Feature Values </a>"},{"metadata":{"trusted":true,"_uuid":"dd479e9c6b436681ff7ad3250cee4de0aae8d6b6"},"cell_type":"code","source":"## Value Counts for Features\nprint('Value Counts for Dataset Features','\\n')\nfor elem in train_df.columns.values:\n    print(elem)\n    if train_df[elem].nunique()<8:\n        if train_df[elem].dtype != np.object:\n            if train_df[elem].nunique()>7:\n                print(train_df[elem].value_counts(bins=8))\n            else:\n                print(train_df[elem].value_counts())\n        else:\n            print(train_df[elem].value_counts())\n    else:\n        print('Unique values:',train_df[elem].nunique())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dde3271b2b0e4d9e3d07e355d77349c4bb8ae0d"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"2.3\">2.3  Check Distributions of Numerical Features </a>"},{"metadata":{"trusted":true,"_uuid":"2e2d2eae2c29af935a2bec875f9015703a39a0ee"},"cell_type":"code","source":"## Generate distributions of numberical features\ntrain_df.hist(bins=50, figsize=(20,15))\nplt.show\nprint('Distributions of train_df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c06de7b9c2b6729b91c5ed30a6bd5948ffb47610"},"cell_type":"markdown","source":"## <a id=\"3\">4. Create Validation Data Set </a>\n### &nbsp;&nbsp;  <a id=\"4.1\">4.1  Check files in working directory </a>"},{"metadata":{"trusted":true,"_uuid":"50edf963c0a59d5aee6692c7eca8d9a993cf34f1"},"cell_type":"code","source":"## Module to allow the user to remove any data files they want before creatring the data validation_set\nwhile not(input(\"Are you sure? (y/n): \").lower().strip()[:1] == \"y\" or input(\"Are you sure? (y/n): \").lower().strip()[:1] == \"n\"):\n    print('invalid')\nif input(\"Are you sure? (y/n): \").lower().strip()[:1] == \"y\" or input(\"Are you sure? (y/n): \").lower().strip()[:1] == \"n\":\n    print('T')\nelse:\n    print('F')\nimport os.path\nfileList = os.listdir(\"../working/\")\nprint(fileList)\n\n\nquestion = lambda q: raw_input(q).lower().strip()[0] == \"y\" or question(q)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"982b07d3e012c6c3344fcf888dc1ce7b12e06f43"},"cell_type":"code","source":"## Take straight percentage of the data set as a validation set\n#  To avoid generating a new set evertime, save the validation set or set the random_state=# parameter\n#  Or, you can save the train and validation sets and relod them next time\nfrom sklearn.model_selection import train_test_split\nimport os.path\ndef errorAlert():   # Function to present error meassage if validation file exists\n    print('''validation_set.csv and train_set.csv already exist.\n    No new file will be created. To create a new validation file\n    delete validation_set.csv AND train_set.csv in the working direcotry and rerun this module.\n    Current directories & files in the working directory for this Notebook are:\\n''')\n    print('NOTEBOOK DIRECTORIES')\n    print(os.listdir(\"../\"))\n    print('\\nWORKING DIRECTORY FILES')\n    fileList = os.listdir(\"../working/\")\n    for f in fileList: \n        print(f)\ndef selectPerc(perc):         # select straight percent & save new train & validation files\n    train_set,validation_set = train_test_split(train_df, test_size=perc,random_state=42)     # Split off .2 of training set \n    print('NEWLY CREATED train_set')\n    print(train_set.info()\n    print('NEWLY CREATED test_set')\n    print(validation_set.info())\n    test_set=test_df   # To keep naming conventions similar, create and a data set called test_set\n    \n    # Save the newly created datasets\n    test_set.to_csv('test_set',index=False)    # Save the newly named test set\n    train_set.to_csv('train_set',index=False)    # Save the new training set\n    validation_set.to_csv('validation_set.csv',index=False)  # Save the new Validation set\n    \n    \n# First check if validation set has already been created\nif os.path.isfile('../working/validation_set.csv') or os.path.isfile('../working/validation_set.csv'):   # Test for validation_set.csv and print message if exists\n    errorAlert()   # if exists, call error alert message\nelse:\n    selectPerc(0.2)  # if doesn't exist, create desired validation set\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3535f2f8c93971eebc3bc408e43db94c86f821"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3acc86ec09cfcb4fb6896421ba2a6e68a738185"},"cell_type":"markdown","source":"## <a id=\"3\">3.  Hot-Code Categorical Data </a> "},{"metadata":{"trusted":true,"_uuid":"667385bd461caaa5411926431ad062954f8a1442"},"cell_type":"code","source":"## Hot Code All Categorical Data\n# Copy data set\n\n\nhc_train=train_df\nhc_train=hc_train.dropna()\nhc_test=test_df\nhc_test=hc_test.dropna()\nhc_data=[hc_train,hc_test]\n\nfor elem in hc_train:\n    if hc_train[elem].dtype == np.object and hc_train[elem].nunique()<8:\n        hc_train=HotC(hc_train,elem)\n        hc_train=hc_train.drop([elem], axis=1)\n        print(elem,'hot-coded')\n    else:\n        if hc_train[elem].dtype == np.object:\n            hc_train=hc_train.drop([elem], axis=1)\n            print('DROPPED',elem)\n            \nfor elem in hc_test:\n    if hc_test[elem].dtype == np.object and hc_test[elem].nunique()<8:\n        hc_test=HotC(hc_test,elem)\n        hc_test=hc_test.drop([elem], axis=1)\n        print(elem,'hot-coded')\n    else:\n        if hc_test[elem].dtype == np.object:\n            hc_test=hc_test.drop([elem], axis=1)\n            print('DROPPED',elem)\n\n# Drop Passenger id\n#hc_train=hc_train.drop(['PassengerId'], axis=1)\n#hc_train=hc_test.drop(['PassengerId'], axis=1)\n            \nprint(hc_train.info())\nprint(hc_test.info())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c81223e11fd4e2c38ba5b17e31a9ab733b1d2204"},"cell_type":"code","source":"## Apply Random Forest for all Features for an idea of importance\n# Define testing dataframes\n## Fit Several Models to compare effectiveness\n# Define testing dataframes\nX_train = hc_train.drop(\"Survived\", axis=1)\nY_train = hc_train[\"Survived\"]\nX_test  = hc_test\n\n#Apply Random Forest and Logistical Regression\n#Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n\n#Logistic Regression:\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n\nresults = pd.DataFrame({\n    'Model': ['Logistic Regression','Random Forest'],\n    'Score': [acc_log,acc_random_forest]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\n\nfeature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nprint(result_df)\nprint(feature_importances)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7ffce614d989beecc7713a88fffeeaf98abd0bd"},"cell_type":"markdown","source":"## <a id=\"4\">4.  Convert Features to Floating Point (or Drop) </a> "},{"metadata":{"trusted":true,"_uuid":"85f67d647781c552b117dd345b7a887cc64bc855"},"cell_type":"code","source":"## Convert Fare from float to int64 using 'astype()'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n## Hot-Code Sex feature\ntrain_df=HotC(train_df,'Sex')\ntrain_df=train_df.drop(['Sex'], axis=1)\n\ntest_df=HotC(test_df,'Sex')\ntest_df=test_df.drop(['Sex'], axis=1)\n\n## Hot-Code Embarked and re-name columns\ntrain_df=HotC(train_df,'Embarked')\ntest_df=HotC(test_df,'Embarked')\n\n##Rename Embarked Hot-codes\ntrain_df=train_df.rename(index=str, columns={'C':'Emb_C','Q':'Emb_Q','S':'Emb_S'})\ntest_df=test_df.rename(index=str, columns={'C':'Emb_C','Q':'Emb_Q','S':'Emb_S'})\n\n##Drop Embarked Column\ntrain_df=train_df.drop(['Embarked'], axis=1)\ntest_df=test_df.drop(['Embarked'], axis=1)\n\n##Drop cabin as mostly available for survivers so sample very biased\n#train_df=train_df.drop(['Cabin'], axis=1)\n#test_df=test_df.drop(['Cabin'], axis=1)\n#pd.options.display.max_columns=20\n\nprint('Converted Values')\nprint('Train_df')\ntrain_df.info()\nprint(train_df.head(10))\nprint('test_df')\ntest_df.info()\nprint(test_df.head(10)) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"293b6663c0f0160367dce43b091926cacbac07ae"},"cell_type":"markdown","source":"## <a id=\"5\">5.  Feature Engineering </a> \n### &nbsp;&nbsp;  <a id=\"5.1\">5.1  Number of Relatives  & CabinYN</a>"},{"metadata":{"trusted":true,"_uuid":"14ffc40cf71ca3f20d859de8be2bbd41542db582"},"cell_type":"code","source":"## Feature to indicated number of relatives & if traveling alone. \ndata = [train_df, test_df]\nfor dataset in data:\n    # Relatives & CabinNoYN\n    dataset['Relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['Relatives'] > 0, 'Not_alone'] = 0\n    dataset.loc[dataset['Relatives'] == 0, 'Not_alone'] = 1\n    dataset['Not_alone'] = dataset['Not_alone'].astype(int)\n    # Code Existence of Cabin number as 0/1   \n    dataset['CabinYN'] = dataset['Cabin'].apply(lambda x: 1 if not pd.isnull(x) else np.nan)\n    dataset['CabinYN'] = dataset['CabinYN'].fillna(0)\n    dataset['CabinYN'] = dataset['CabinYN'].astype(int)\n## Lookk at relationships with Survived \ng = sns.catplot(\"Not_alone\", \"Survived\", 'male',col='Pclass', col_wrap=3, data=train_df, kind=\"bar\", height=6, palette=\"autumn\",aspect=.8,legend=True)\nprint('IMPACT OF TRAVEL COMPANIONS AND KNOWING CABIN #')\ng = sns.catplot(\"Relatives\", \"Survived\", col='male', col_wrap=2, data=train_df, kind=\"bar\", height=6, palette=\"autumn\",aspect=.8,legend=True)\ng = sns.catplot(\"CabinYN\", \"Survived\", col='male', col_wrap=2,data=train_df, kind=\"bar\", height=6, palette=\"autumn\",aspect=.8,legend=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"431c285141f4a55b5920bc9275eec6cb5e5a35d3","scrolled":true},"cell_type":"code","source":"\n\n### &nbsp;&nbsp;  <a id=\"2.4\">2.4  Missing Data Imputations </a>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da54efe664ccb65651720c57acf3d55bff4a32f3"},"cell_type":"code","source":"## Age missing values\n# Use random numbers based on the mean age value in regards to the standard deviation and is_null\nembarked_common_value='S'\ndata = [train_df, test_df]\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\n    \n    # Fill the 2 embarked missing features with the most common values from embarked\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)\n    \n# Ck For missing values in Age and Embarked\nprint('Sum of missing values in Age and Embarked')\ndataset['Age','Embarked'].isnull().sum\n  \ntrain_df.info()\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0261be04e125fb846a51dc8fd4a9a7bfdd97040c","collapsed":true},"cell_type":"code","source":"\n\n\ntrain_df.info()\ntest_df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da276e1d051774ed8cd6ad983c6edb4965fc4aef"},"cell_type":"markdown","source":"### &nbsp;&nbsp;  <a id=\"2.6\">2.6  Feature Extraction </a>"},{"metadata":{"trusted":true,"_uuid":"b0c077bef03f391fa9a3cb8d4ce2a9187955d9d1","collapsed":true},"cell_type":"code","source":"## Use the Name feature to extract the titles from the Name to build a new feature\ndata = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8971320f764611d98013e8f69d83bab0db0fba53","collapsed":true},"cell_type":"markdown","source":"\n## &nbsp;&nbsp;  <a id=\"5.1\">5.1  Remove any Unwanted Features</a>"},{"metadata":{"trusted":true,"_uuid":"dbd698f59bae067f83d8d6060329c313a5391c87","collapsed":true},"cell_type":"code","source":"# Drop Ticket from the data set\ntrain_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)\n\n# Delete PassengerId from train_df (not there, so does not need to be deleted)\n# train_df = train_df.drop(['PassengerId'], axis=1)    Don't need to drop; not there\n# Drop Passenger Name\n#train_df = train_df.drop(['Name'], axis=1)   # Don't need to drop; not there\n\n#train_df = train_df.drop(['PassengerId'], axis=1)   # Don't need to drop; not there","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba5fb009612dfabebc25b8fea09a877d3b262303","collapsed":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb8acb2fa1a72a55d832a7d20b5d090d3896c10f","collapsed":true},"cell_type":"code","source":"## Create Categories for Age Feature\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed\nprint('distribution of train_df')\ntrain_df['Age'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a301201ecbd4062224bbf2f9965ea0eb74ff1e12","collapsed":true},"cell_type":"code","source":"## Create categories for Fare\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45a3ddc209a7eda3c58f4c25b4b3df76995b6ad7","collapsed":true},"cell_type":"code","source":"## Create some additional variables\n\n# Age X Class\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n\n# Fare per Person\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(10)\n\nprint(train_df.head(10))\nprint(test_df.head(10)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3409ce0b303b43326a18256229557dbb0c80ec9b"},"cell_type":"code","source":"# Look at correlation between key variables\n\n'''\nprint('Correlation of Pclass & relatives')\n#print(train_df.corr().loc['relatives','Pclass'])\nimport scipy.stats\nprint(scipy.stats.pearsonr(train_df['Pclass'].values,train_df['relatives'].values)[0],'    --using scipy.stats pearsonr')\nprint(train_df.corr().loc['Pclass','relatives'],'    --using pandas pearsonr \\n')\n\nprint('Correlation of relatives & Age')\nprint(train_df.corr().loc['relatives','Age'])\nprint('\\n')\n'''\n## Create a string with all variable names\ncol_name=list(train_df.columns.values)\n\ntrain_df[col_name]\n\ncor_dataset=train_df[col_name]\nprint('Correlation Matrix')\nprint(cor_dataset.corr())\nprint(sns.heatmap(cor_dataset.corr()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2db223b00df810ce677021a20691c358b451d589"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f8b99824222cfc45e4110fa58b45793514bef5f","collapsed":true},"cell_type":"code","source":"print(\"Results\")\n## Fit Several Models to compare effectiveness\n# Define testing dataframes\nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df\n\n#print(X_test.head(10))\n\n#SGD-Stochastic Gradient Descent\nsgd = linear_model.SGDClassifier(max_iter=50, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nsgd.score(X_train, Y_train)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n#Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n\n#Logistic Regression:\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n\n# K Nearest Neighbor\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n\n# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n    \n# Perceptron:\nperceptron = Perceptron(max_iter=10)\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n\n# Linear Support Vector Machine\nlinear_svc = LinearSVC(max_iter=2000)\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n\n'''\n# lgb_light\n# params = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n          #'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n          #'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n          #'min_split_gain':.01, 'min_child_weight':1}\n\n# lgb_light = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)\nlgb_light = lgb\nlgb_light.fit(x_train,Y_train)\nY_pred = lgb_light.predict(X_test)\nacc_lgb_light = round(lgm_light.score(X_train, Y_train) * 100, 2)\n'''\n# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n\nresults = pd.DataFrame({\n    'Model': ['LinearSVC','KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)\n\n  \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}