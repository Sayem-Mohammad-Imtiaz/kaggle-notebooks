{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing essential packages\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading dataset\ndf=pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1)EDA**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# A.Understanding the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **B.Visualizing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding correlation\ncorrel=df.corr()\nplt.subplots(figsize=(16,10))\nfig1=sns.heatmap(correl, annot=True)\nbottom, top = fig1.get_ylim()\nfig1.set_ylim(bottom + 0.5, top - 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"UNIVARIATE ANALYSIS-HISTOGRAMS:\")\nfor i in [0,1,2,9,10]:\n    plt.subplots(figsize=(10,5))\n    sns.distplot(df.iloc[:,i],color='purple',bins=15)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BOXPLOTS:\")\nfor i in range(0,12):\n    plt.subplots(figsize=(8,4))\n    sns.boxplot(x=df.iloc[:,11],y = df.iloc[:,i],palette=\"cool\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BIVARIATE ANALYSIS: Scatter plots-\\n\")\nl1=[1,2,9,10]\nfor i in l1:\n    for j in l1:\n        if(j!=i):\n            sns.scatterplot(x=df.iloc[:,i],y=df.iloc[:,j],hue=df[\"quality\"])\n            plt.show()\n    l1.remove(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **C.Feature selection and data scaling**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier \n#Feature selection\ny=df.iloc[:,11]\nX=df.iloc[:,[1,2,9,10]] #Using only top 4 columns with highest correlation to quality\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX=ss.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2.Model creation**\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Note: Usually the random state is not a parameter to be tuned, and will produced 'optimistically biased' results. I have done it here anyway to give a best case scenario.\nYou can find out more from random state manipulation from these links:\n[https://towardsdatascience.com/manipulating-machine-learning-results-with-random-state-2a6f49b31081](http://)\nand \n[https://stats.stackexchange.com/questions/263999/is-random-state-a-parameter-to-tune](http://)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Finding best suitable random state between state 1-43\n\n\"\"\"This step fits multiple KNN models with number of neighbours ranging from 1-35 and random states 1-43\nto give the best case accuracy. Random state manipulation is NOT A GOOD PRACTICE but FINDING OPTIMAL NUMBER OF NEIGHBOURS\nfor the particular problem is important. \n\nPLEASE NOTE THIS STEP TAKES AROUND 1 MINUTE TO RUN\"\"\"\n\nstate_macc=list(np.empty(43))\nfor i in range(1,43):\n    from sklearn.model_selection import train_test_split\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=i,stratify=y)\n    \n    neighbors = np.arange(1,35)\n    train_accuracy = np.empty(len(neighbors))\n    test_accuracy = np.empty(len(neighbors))\n    for j,k in enumerate(neighbors):\n        knn2 = KNeighborsClassifier(n_neighbors=k,weights='distance',metric='manhattan')\n        knn2.fit(X_train,y_train)\n        train_accuracy[j] = knn2.score(X_train, y_train)\n        test_accuracy[j] = knn2.score(X_test, y_test)\n    macc=np.amax(test_accuracy)\n    x=np.where(test_accuracy==np.amax(test_accuracy))\n    x=x[0].tolist()\n    x = [i+1 for i in x]\n    data=[x,float(macc)]\n    state_macc[i-1]=data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=0.0\npos=0\nfor i in range(42):\n    state_macc[i][-1] = float(state_macc[i][-1])\n    if (state_macc[i][-1] > temp):\n        temp = state_macc[i][-1]\n        pos=i+1\nideal_k=state_macc[pos-1][0]\nprint(\"Ideal random state= {} yielding max accuracy {} with number of neighbours: \".format(pos,temp),ideal_k)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Model:- Accuracy of about 75%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"THEREFORE Final model is prepared using ideal Random State and Best K number of neighbours \"\"\"\n\nfrom sklearn.neighbors import KNeighborsClassifier \ny=df.iloc[:,11]\nX=df.iloc[:,[1,2,9,10]] #Using only top 4 columns with highest correlation to quality\n\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nX=ss.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=pos,stratify=y)\n\nknn = KNeighborsClassifier(n_neighbors=ideal_k[0],weights='distance',metric='manhattan')\nknn.fit(X_train,y_train)\n\nfacc=knn.score(X_test,y_test)\nprint(\"\\n\\nAccuracy of final model is: \", facc*100, \"%\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Prediction table:\")\na=knn.predict(X_test)\nb=y_test\nz=list(enumerate(a))\nz=[x[0] for x in z]\ntable={'Index':z,'Predicted value':a,'Real value':b}\nnf=pd.DataFrame(table)\nnf.style.hide_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n**As we can see the KNN model gives us an accuracy of about 75% on the test data, which means that makes correct predictions for roughly 3 in 4 datapoints. Another point to be noted from the prediction table is that most of the wrong predictions aren't off by much (i.e. a 6 might be predicted as a 5, not as a 3) which indicates that the predictions are still pretty close to the true data. We can note that though the KNN model is 'decent'in performance, there are other algorithms that can be used and improvements that can be made.**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}