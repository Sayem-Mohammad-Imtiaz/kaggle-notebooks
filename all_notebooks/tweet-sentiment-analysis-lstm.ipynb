{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport chardet \nimport operator\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense, Bidirectional\nfrom tensorflow.keras.layers import Dropout, SpatialDropout1D\n\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwith open(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", 'rb') as f:\n    result = chardet.detect(f.read())\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f.close()\n#can be tried cp1254, latin and iso-8859-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_data = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", encoding='iso-8859-1')\ntest_data = pd.read_csv(\"../input/covid-19-nlp-text-classification/Corona_NLP_test.csv\", encoding='iso-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape, test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.Sentiment.value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ntrain_data.OriginalTweet[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract hashtag \n\ntrain_data[\"hashtag\"] = train_data[\"OriginalTweet\"].apply(lambda x: re.findall(r\"#(\\w+)\", x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data pre processing\n#extract url used \ntrain_data[\"uri\"] = train_data[\"OriginalTweet\"].apply(lambda x : re.findall(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"handler\"] = train_data[\"OriginalTweet\"].apply(lambda x: re.findall(r\"@(\\w+)\", x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lemmatize Words\n\n\ndef get_pos_tag(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        # As default pos in lemmatization is Noun\n        return wordnet.NOUN\n\nlemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tag([\"going\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean the data now \n\n\nregex = [\n    r'<[^>]+>', #HTML tags\n    r'@(\\w+)', # @-mentions\n    r\"#(\\w+)\", # hashtags\n    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n    r'[^0-9a-z #+_\\\\r\\\\n\\\\t]', #BAD SYMBOLS\n]\n\nREPLACE_URLS = re.compile(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+')\nREPLACE_HASH = re.compile(r'#(\\w+)')\nREPLACE_AT = re.compile(r'@(\\w+)')\nREPLACE_HTML_TAGS = re.compile(r'<[^>]+>')\n#REPLACE_DIGITS = re.compile(r'\\d+')\n#REPLACE_BY = re.compile(r\"[/(){}\\[\\]\\|,;.:?\\-\\'\\\"$]\")\nREPLACE_BY = re.compile(r\"[^a-z0-9\\-]\")\n\nSTOPWORDS = set(stopwords.words('english'))\n\n#tokens_re = re.compile(r'('+'|'.join(regex)+')', re.VERBOSE | re.IGNORECASE)\n\nsentences = [] #for Word2Vec model\n\ndef clean_text(text):\n    text = text.lower()\n    text = REPLACE_HTML_TAGS.sub(' ', text)\n    text = REPLACE_URLS.sub('', text)\n    text = REPLACE_HASH.sub('', text)\n    text = REPLACE_AT.sub('', text)\n    #text = REPLACE_DIGITS.sub(' ', text)\n    text = REPLACE_BY.sub(' ', text)\n    \n    \n    text = \" \".join(lemmatizer.lemmatize(word.strip(), get_pos_tag(pos_tag([word.strip()])[0][1])) for word in text.split() if word not in STOPWORDS and len(word)>3)\n    \n    #sentences.append(text.split())\n    return (text, text.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"Tweet\"], train_data[\"sentences\"] = zip(*train_data[\"OriginalTweet\"].apply(clean_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from itertools import izip\ntest_data[\"Tweet\"], test_data[\"sentences\"] = zip(*test_data[\"OriginalTweet\"].apply(clean_text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets do some eda on the data \n#how sentiments are related with the hashtag and user handler\n#ext_pos = train_data[train_data['Sentiment'] == 'Extremely Positive']\npos = train_data[train_data[\"Sentiment\"] == \"Positive\"]\nneu = train_data[train_data[\"Sentiment\"] == \"Neutral\"]\nneg = train_data[train_data[\"Sentiment\"] == \"Negative\"]\n#ext_neg = train_data[train_data[\"Sentiment\"] == \"Extremely Negative\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize=(20, 20), subplot_kw=dict(aspect=\"equal\"))\nfor i, param in enumerate([\"hashtag\", \"handler\"]): #, \"uri\"]):\n    for j, df in enumerate([ pos, neu, neg]): #, ext_neg, ext_pos,]:\n        hash_count = {}\n        for tag in df[param]:\n            for value in tag:\n                hash_count[value] = hash_count.get(value, 0) + 1\n        data = sorted(hash_count.items(), key=operator.itemgetter(1))[-10:]\n        axs[i,j].pie([value[1] for value in data], labels=[value[0] for value in data], autopct='%1.1f%%', shadow=True, startangle=90)\n        axs[i,j].axis('equal')\n        axs[i,j].set_title(\"--Top {0} in {1}--\".format(param, df[\"Sentiment\"].values[0].upper()), fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ntrain_data.Tweet[50:55]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word Cloud\n\n#for color, df in {\"green\":ext_pos, \"yellow\":pos, \"white\":neu, \"pink\":neg, \"orange\":ext_neg}.items(): \nfor color, df in { \"yellow\":pos, \"white\":neu, \"pink\":neg}.items():\n    plt.figure(figsize=(18,18))\n    wc_pos = WordCloud(width=400, height=250, min_font_size=5, background_color=color, max_words=10000).generate(\" \".join(df[\"Tweet\"]))\n    plt.title(\"word cloud for {0}\".format(df[\"Sentiment\"].values[0]), fontsize=25)\n    plt.imshow(wc_pos, interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max len of clean data\nimport numpy as np\nmax_len = np.max(train_data[\"Tweet\"].apply(lambda x: len(x)))\nmax_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(train_data[\"Tweet\"].values)\n\nvocab_size = len(tokenizer.word_index) + 1\nX = tokenizer.texts_to_sequences(train_data[\"Tweet\"].values)\nX = pad_sequences(X, maxlen=max_len, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pre process test data with param of train data\nX_test = tokenizer.texts_to_sequences(test_data[\"Tweet\"].values)\nX_test = pad_sequences(X_test, maxlen=max_len, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\n#ValueError: could not convert string to float: '.' this is why using ingore\nembeddings_index = {}\nglovefile = open('../input/glove42b300dtxt/glove.42B.300d.txt','r',encoding='utf-8')\nfor line in tqdm(glovefile):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nglovefile.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\nfor word, index in tqdm(tokenizer.word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoding = {'Extremely Negative': 0,\n            'Negative': 0,\n            'Neutral': 1,\n            'Positive':2,\n            'Extremely Positive': 2\n           }\n\nlabels = ['Negative', 'Neutral', 'Positive']\n           \n\ntrain_data[\"Sentiment\"].replace(encoding, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data[\"Sentiment\"].replace(encoding, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = pd.get_dummies(train_data[\"Sentiment\"]).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.get_dummies(train_data[\"Sentiment\"]).values\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = pd.get_dummies(test_data[\"Sentiment\"]).values\ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train valid split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=12)\nprint(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector_features = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, vector_features, input_length=X.shape[1], weights=[embedding_matrix], trainable=False))\nmodel.add(SpatialDropout1D(0.2))\n\nmodel.add(Bidirectional(LSTM(300, activation='relu', dropout=0.3, recurrent_dropout=0.3), input_shape=(vector_features, vocab_size)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\nbatch_size = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_valid, y_valid),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Accuracy\naccuracy = history.history[\"accuracy\"]\nloss = history.history[\"loss\"]\n\nval_accuracy = history.history[\"val_accuracy\"]\nval_loss = history.history[\"val_loss\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"training acuuracy {0}% and training loss {1}%\".format(accuracy[-1]*100, loss[-1]*100))\nprint(\"validation acuuracy {0}% and validation loss {1}%\".format(val_accuracy[-1]*100, val_loss[-1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot \n\nplt.plot(accuracy,'g',label='training accuracy')\nplt.plot(val_accuracy, 'r', label='validation accuracy')\nplt.legend()\nplt.show()\n\n\nplt.plot(loss,'g',label='training loss')\nplt.plot(val_loss, 'r', label='validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = model.predict_classes(X_test)\n#y_pred = np.argmax(model.predict(X_test), axis=-1)\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labels[np.argmax(y_test, 1)][100:120])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labels[np.argmax(y_pred, 1)][100:120])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\ncm = confusion_matrix(np.argmax(y_test, 1), np.argmax(y_pred, 1))\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets go for Word2Vec\nimport time\nimport multiprocessing\nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cores = multiprocessing.cpu_count()\ncores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = Word2Vec(min_count=5,\n                     window=5,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.01, \n                     min_alpha=0.0007, \n                     negative=7,\n                     workers=cores-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pass list of words of data\n#it can be nested list \nt = time.time()\nw2v_model.build_vocab(train_data[\"sentences\"], progress_per=10000)\nprint('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nw2v_model.train(train_data[\"sentences\"], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\nprint('Time to train the model: {} mins'.format(round((time.time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#memory efficient\nw2v_model.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"walmart\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"covid\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"food\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors = w2v_model.wv\nprint(\"length of word vector vocab {}\".format(len(word_vectors.vocab)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"walmart\" in word_vectors.vocab.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word2idx(sent):\n    sent_id = []\n    for word in sent:\n        if word in w2v_model.wv.vocab.keys():\n            sent_id.append(w2v_model.wv.vocab[word].index)\n    return sent_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def idx2word(sent_id):\n    sent = []\n    for idx in sent_id:\n       sent.append(w2v_model.wv.index2word[idx])\n    return sent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_w2v = train_data[\"sentences\"].apply(word2idx)\nX_test_w2v = test_data[\"sentences\"].apply(word2idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_w2v_pad = pad_sequences(X_train_w2v, maxlen=max_len, padding='post')\nX_test_w2v_pad = pad_sequences(X_test_w2v, maxlen=max_len, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_w2v_pad.shape, X_test_w2v_pad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_w2v = w2v_model.wv.syn0\n#w2v_model.wv.vectors.shape\nvocab_size_w2v, vector_features_w2v = weights_w2v.shape\nvocab_size_w2v, vector_features_w2v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train valid split\n\nX_train_w2v, X_valid_w2v, y_train_w2v, y_valid_w2v = train_test_split(X_train_w2v_pad, y, test_size=0.20, random_state=12)\nprint(X_train_w2v.shape, y_train_w2v.shape)\nprint(X_valid_w2v.shape, y_valid_w2v.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_w2v = Sequential()\nmodel_w2v.add(Embedding(vocab_size_w2v, vector_features_w2v, input_length=X_train_w2v_pad.shape[1], weights=[weights_w2v], trainable=False))\nmodel_w2v.add(SpatialDropout1D(0.2))\n\nmodel_w2v.add(Bidirectional(LSTM(300, activation='relu', dropout=0.3, recurrent_dropout=0.3), input_shape=(vector_features_w2v, vocab_size_w2v)))\n\nmodel_w2v.add(Dense(1024, activation='relu'))\nmodel_w2v.add(Dropout(0.8))\n\nmodel_w2v.add(Dense(1024, activation='relu'))\nmodel_w2v.add(Dropout(0.8))\n\nmodel_w2v.add(Dense(3, activation='softmax'))\n\nmodel_w2v.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_w2v.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\nbatch_size = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_w2v.fit(X_train_w2v, y_train_w2v, epochs=epochs, batch_size=batch_size,validation_data=(X_valid_w2v, y_valid_w2v),callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seems like word2vec was not trained good enough\n#may be due  to limited vocab","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}