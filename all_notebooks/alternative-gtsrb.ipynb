{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Prepare all our necessary libraries\nimport numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\n\n#pytorch libraries\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Pad, Compose,CenterCrop, ToPILImage, Normalize, ConvertImageDtype, Resize\n\nfrom torchvision.models import resnet50\nfrom torch import nn\nfrom torch.nn import init, Linear, ReLU, Softmax\nfrom torch.nn.init import xavier_uniform_\nfrom torch.optim import SGD, Adam\nimport torch.nn.functional as F\n\nimport datetime\n\n# libs fÃ¼r AE\n\n!pip install cleverhans --upgrade\nfrom cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare our Dataset Structure, as it has to be normalized for pytorch\n\nfrom torchvision.io import read_image\n\nclass GTSRB(Dataset):\n    def __init__(self, annotations_file, img_dir , transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)[[\"Path\",\"ClassId\"]]\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        label = self.img_labels.iloc[idx, 1]\n        image = read_image(img_path)\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating training dataset; Normalize pictures them to (3,90,90) size\nimg_dir = \"/kaggle/input/gtsrb-german-traffic-sign/\"\ntrain_file = \"/kaggle/input/gtsrb-german-traffic-sign/Train.csv\"\ntrain_data = GTSRB(img_dir = img_dir, annotations_file = train_file,\n                   transform = Compose([Resize((30,30)), ConvertImageDtype(torch.float32)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare DataLoader\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display image and label.\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0]\nlabel = train_labels[0]\nimg = ToPILImage()(img).convert(\"RGB\")\nplt.imshow(img)\nplt.show()\nprint(f\"Label: {label}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class conv_net(nn.Module):\n    def __init__(self):\n        super(conv_net,self).__init__()\n        self.conv1 = nn.Conv2d(3,32,(5,5))\n        self.conv2 = nn.Conv2d(32,64, (3,3))\n        self.pool = nn.MaxPool2d((2,2))\n        self.dropout1 = nn.Dropout(p=0.25)\n        self.conv3 = nn.Conv2d(64,3,(3,3))\n        self.linear1 = Linear(75,256)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.linear2 = Linear(256,43)\n\n        \n    def forward(self, X):\n        X = F.relu(self.conv1(X))\n        X = self.pool(F.relu(self.conv2(X)))\n        X = self.dropout1(X)\n        X = self.pool(F.relu(self.conv3(X)))\n        X = self.dropout1(X)\n        X = torch.flatten(X,1)\n        X = F.relu(self.linear1(X))\n        X = self.dropout2(X)\n        X = self.linear2(X)\n        \n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = conv_net()\ncriterion = nn.CrossEntropyLoss(reduction = \"mean\")\noptim = Adam(model.parameters(), lr = 0.001)\n\n#put on cuda if possible\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nnb_epochs = 30\n\nae_predictions = []\nae_accuracy = []\naccuracy = []\n\neps = 0.085\neps_iter = eps/10\nsteps = 15\n\nfor epoch in range(nb_epochs):\n    running_loss = 0\n    ae_count = 0\n    for i, data in enumerate(train_dataloader, 0):\n        train_features, train_labels = data\n        train_features, train_labels = train_features.to(device), train_labels.to(device)\n        \n        #ae training\n        model.eval()\n            \n        ae = projected_gradient_descent(model,train_features[-7:], eps, eps_iter, steps,np.inf, clip_min=0, clip_max=1).detach()\n        \n        if i == 100:\n            image = ToPILImage()(ae[0]).convert(\"RGB\")\n            original = ToPILImage()(train_features[-7]).convert(\"RGB\")\n            \n            fig, axs = plt.subplots(2, 1, constrained_layout=True)\n            axs[0].imshow(image)\n            axs[0].set_title('Adversarial example')\n            fig.suptitle('Comparison AE vs. Original', fontsize=12)\n\n            axs[1].imshow(original)\n            axs[1].set_title('Original')\n            plt.show()\n            print(\"Maximum pixel distance: \", torch.max(train_features[-7] - ae[0]))\n        \n        train_features = torch.cat((train_features[:-7], ae)).to(device)\n        model.train()\n        \n        optim.zero_grad()\n        prediction = model(train_features).to(torch.float32)\n        loss = criterion(prediction, train_labels.to(torch.long)) \n        loss.backward()\n        optim.step()\n\n        running_loss += loss.item()\n        \n        #calculate accuracy\n        corrects = 0\n        ae_corrects = 0\n        pred = torch.argmax(prediction, dim = 1)\n        \n        for i, p,l in zip(range(32), pred, train_labels):\n            if i > (32-7):\n                ae_predictions.append((p,l))\n                ae_corrects += (p==l)\n                ae_count +=1\n            corrects += (p == l)\n        ae_accuracy.append(int(ae_corrects)/7)\n        accuracy.append(int(corrects)/len(pred))\n        \n    print(\"Epoch: {j}/{total_epochs} \\t Time: {time} \\t Loss: {Loss} \\t Accuracy: {acc}\".format(j = epoch+1,Loss = running_loss/len(train_dataloader),total_epochs = nb_epochs,time = datetime.datetime.now().time(), acc=np.mean(accuracy[-50:])))\n    print(\"{number} adversarial examples have been created. The accuracy against ae's is {ae_acc}\".format(number=ae_count, ae_acc = np.mean(ae_accuracy[-50:])))    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save accuracy information \naccuracy_data = pd.DataFrame({\"accuracy\":accuracy, \"ae_accuracy\" : ae_accuracy})\naccuracy_data.to_csv((\"/kaggle/working/training_eps{}_epsiter{}_st{}\".format(eps*10, eps_iter*10, steps).replace(\".\",\"\") + \".csv\"), decimal = \",\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model for our test notebook    \nPATH = '/kaggle/working/alt_gtsrb.pth'\ntorch.save(model.state_dict(), PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}