{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reddit Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.read_csv(\"../input/cricket-on-reddit/reddit_cricket.csv\")\n\nprint(data.shape)\ndata.head","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:42.578702Z","iopub.execute_input":"2021-08-27T14:36:42.579076Z","iopub.status.idle":"2021-08-27T14:36:42.625562Z","shell.execute_reply.started":"2021-08-27T14:36:42.579044Z","shell.execute_reply":"2021-08-27T14:36:42.624463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess The Comments\nWhen dealing with raw or uncleaned data, we would first have to preprocess or “clean” it before modelling or in our case, to apply a sentiment analysis.\n\nIn our instance, we would include the following steps for our preprocessing:\n* Removing Emojis\n* Tokenizing, removing links etc.\n* Removing stopwords\n* Normalizing words via lemmatizing","metadata":{}},{"cell_type":"code","source":"import emoji\nimport re\n\ndata['emojiless'] = data['body'].apply(lambda x: emoji.get_emoji_regexp().sub(u'', str(x)))\n# data['emojiless']","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:42.627139Z","iopub.execute_input":"2021-08-27T14:36:42.62745Z","iopub.status.idle":"2021-08-27T14:36:45.864305Z","shell.execute_reply.started":"2021-08-27T14:36:42.627419Z","shell.execute_reply":"2021-08-27T14:36:45.863408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer  \ntokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+http\\S+')\ndata['token'] = data['emojiless'].apply(lambda x: tokenizer.tokenize(x))\n# print(data['token'])\ndata['lower_token'] = data['token'].apply(lambda x: [word.lower() for word in x])\n# data['lower_token']","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:45.865933Z","iopub.execute_input":"2021-08-27T14:36:45.866368Z","iopub.status.idle":"2021-08-27T14:36:45.97225Z","shell.execute_reply.started":"2021-08-27T14:36:45.866337Z","shell.execute_reply":"2021-08-27T14:36:45.971418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nall_stopwords = nlp.Defaults.stop_words\n\ndata['token_without_sw'] = data['lower_token'].apply(lambda x: [word for word in x if not word in all_stopwords and len(word)>2 and not word in [\"www\", \"http\", \"https\", \"reddit\", \"cricket\", \"nan\"]])\n# data['token_without_sw']","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:45.973519Z","iopub.execute_input":"2021-08-27T14:36:45.97395Z","iopub.status.idle":"2021-08-27T14:36:47.270772Z","shell.execute_reply.started":"2021-08-27T14:36:45.973921Z","shell.execute_reply":"2021-08-27T14:36:47.269912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The last pre-processing step is one of **Lemmatizing** or **Stemming**.\n\nBoth processes are used to trim words down to their root words. However, stemming might return a root word that is not an actual word whereas, lemmatizing returns a root word that is an actual language word.","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndata['lemmatizer_tokens'] = data['token_without_sw'].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n# print(data['lemmatizer_tokens'])","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:47.271766Z","iopub.execute_input":"2021-08-27T14:36:47.272198Z","iopub.status.idle":"2021-08-27T14:36:47.615781Z","shell.execute_reply.started":"2021-08-27T14:36:47.272169Z","shell.execute_reply":"2021-08-27T14:36:47.614903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For visualization purposes, we can see that the number of words pre and post-processing has reduced dramatically!","metadata":{}},{"cell_type":"code","source":"print(f'Original number of words: ', sum([len(str(x)) for x in data['body']]))\nprint(f'Emojiless number of words: ', sum([len(str(x)) for x in data['emojiless']]))\nprint(f'Tokenized number of words: ', sum([len(str(x)) for x in data['lower_token']]))\nprint(f'Stopwordless number of words: ', sum([len(str(x)) for x in data['token_without_sw']]))\nprint(f'Lemmatized number of words: ', sum([len(str(x)) for x in data['lemmatizer_tokens']]))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:47.616817Z","iopub.execute_input":"2021-08-27T14:36:47.617226Z","iopub.status.idle":"2021-08-27T14:36:47.667215Z","shell.execute_reply.started":"2021-08-27T14:36:47.617197Z","shell.execute_reply":"2021-08-27T14:36:47.66625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply A Sentiment Analyzer (VADER)","metadata":{}},{"cell_type":"markdown","source":"Once we obtain our cleaned output, we would calculate each tokenized word's polarity scores using the **VADER** (Valence Aware Dictionary for Sentiment Reasoning) model.\n\nThe polarity scores measure the positivity and negativity for each word. We are mostly interested in the **compound score**, which is normalized to be between -1 (most extreme negative sentiment) and +1 (most extreme positive sentiment). This provides a single unidimensional measure of sentiment for a given word.","metadata":{}},{"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nimport numpy as np\n\nsia = SIA()\nresults = []\n\ndata['pool_score'] = data['lemmatizer_tokens'].apply(lambda x: [sia.polarity_scores(w).get('compound') for w in x]) \n# data['pool_score']\ndata['label'] = 0\ndata.loc[data['pool_score'].apply(lambda x: np.mean(x) > 0.1), 'label'] = 1\ndata.loc[data['pool_score'].apply(lambda x: np.mean(x) < 0.1), 'label'] = -1\n# data['label']","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:47.668259Z","iopub.execute_input":"2021-08-27T14:36:47.668683Z","iopub.status.idle":"2021-08-27T14:36:49.734203Z","shell.execute_reply.started":"2021-08-27T14:36:47.668653Z","shell.execute_reply":"2021-08-27T14:36:49.73311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Representation Of Sentiment Results","metadata":{}},{"cell_type":"markdown","source":"Using a quick value count, we can see that the sentiment for most of the words is nagative; they are also more positive than neutral in sentiment.","metadata":{}},{"cell_type":"code","source":"print(data.label.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:49.735504Z","iopub.execute_input":"2021-08-27T14:36:49.735807Z","iopub.status.idle":"2021-08-27T14:36:49.742917Z","shell.execute_reply.started":"2021-08-27T14:36:49.735767Z","shell.execute_reply":"2021-08-27T14:36:49.741442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8,8))\n\ncounts = data.label.value_counts(normalize=True) * 100\n\nsns.barplot(x=counts.index, y=counts, ax=ax)\n\nax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\nax.set_ylabel('Percentage')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:36:49.745496Z","iopub.execute_input":"2021-08-27T14:36:49.745952Z","iopub.status.idle":"2021-08-27T14:36:49.897556Z","shell.execute_reply.started":"2021-08-27T14:36:49.745908Z","shell.execute_reply":"2021-08-27T14:36:49.896375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Visualizations\n\nLastly, before ending this post, we can perform some simple visualizations and explore the word frequency of positive and negative words.\n\nFrequency distribution of the 20 most common positive words:","metadata":{}},{"cell_type":"code","source":"from nltk import FreqDist \n\nall_words = \" \".join(x for x in data['lemmatizer_tokens'].apply(lambda x: \" \".join(x))).split(\" \")\npositive_words = \" \".join(x for x in data[data['label'] == 1]['lemmatizer_tokens'].apply(lambda x: \" \".join(x))).split(\" \")\nnegative_words = \" \".join(x for x in data[data['label'] == -1]['lemmatizer_tokens'].apply(lambda x: \" \".join(x))).split(\" \")\n\npos_freq = FreqDist(positive_words).most_common(5)\nneg_freq = FreqDist(negative_words).most_common(5)\nprint('pos_freq\\n', pos_freq)\nprint('neg_freq\\n', neg_freq)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:42:38.463772Z","iopub.execute_input":"2021-08-27T14:42:38.464159Z","iopub.status.idle":"2021-08-27T14:42:38.554338Z","shell.execute_reply.started":"2021-08-27T14:42:38.46413Z","shell.execute_reply":"2021-08-27T14:42:38.553578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also visualize them via WordClouds.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud_total = WordCloud(background_color=\"white\").generate(\" , \".join(all_words))\nwordcloud_posit = WordCloud(background_color=\"white\").generate(\" , \".join(positive_words))\nwordcloud_negat = WordCloud(background_color=\"white\").generate(\" , \".join(negative_words))\n\nplt.imshow(wordcloud_total, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\nplt.imshow(wordcloud_posit, interpolation='bilinear')\nplt.axis('off')\nplt.show()\n\nplt.imshow(wordcloud_negat, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T14:43:32.870933Z","iopub.execute_input":"2021-08-27T14:43:32.871303Z","iopub.status.idle":"2021-08-27T14:43:35.388087Z","shell.execute_reply.started":"2021-08-27T14:43:32.871268Z","shell.execute_reply":"2021-08-27T14:43:35.387118Z"},"trusted":true},"execution_count":null,"outputs":[]}]}