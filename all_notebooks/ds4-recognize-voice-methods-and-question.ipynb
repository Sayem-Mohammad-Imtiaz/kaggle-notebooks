{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"bd757c8f-1aa2-89b3-a809-bb609d257154"},"source":"<H1>DS4: Recognize voice: are you male or female??</H1>\nWe will see the question the author what are:\n<ul>\n<li>What other features differ between male and female voices?</li>\n<li>Can we find a difference in resonance between male and female voices?</li>\n<li>Can we identify falsetto from regular voices? (separate data-set likely needed for this)</li>\n<li>Are there other interesting features in the data?</li>\n</ul>\nbut i wanna know how classified methods do with dataset to classify. Then first i 'll ask my question and try to ask the question: what features are differ between male and female voices?."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"addc3993-e8a3-8524-8805-7f8fb54b0ac7"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as pyplot # plot in python\nimport seaborn as sns # data visualization in python \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a63a66da-5a1d-bd38-24cd-5da875096a6a"},"outputs":[],"source":"data = pd.read_csv(\"../input/voice.csv\")\ndata.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"39f70aa1-1c50-fceb-a909-6623da1549aa"},"source":"<h1>Cleaning and analyze Data</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0f34b0e-7c20-ae15-7725-74c6ccecff50"},"outputs":[],"source":"#type of data\ndata.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32a320c4-d139-1f45-7a27-e18d40a5039c"},"outputs":[],"source":"#count the null value \ndata.isnull().sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1c82c6e-eb90-22b7-b960-f07c00b00481"},"outputs":[],"source":"#share of data\nprint( \"dimension of dataframe is: \",data.shape)\n#how many men and women are in data?\nprint(\"Number of male: {}\".format(data[data.label == 'male'].shape[0]))\nprint(\"Number of female: {}\".format(data[data.label == 'female'].shape[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2f4d7f4-b58b-0fa1-777c-43e9b20de7ec"},"outputs":[],"source":"#Library what i will use\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.ensemble import RandomForestClassifier"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ca3d6df-7655-bc6b-621e-b2afb9283fc3"},"source":"<h1>Data Standardization</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"395122e2-9c1a-30f1-4e53-346d94b1f12f"},"outputs":[],"source":"#separate in features and label data\nfeatures = data.iloc[  : , :-1]\nlabels = data.iloc[ : , -1 ]\n#preprocessing of data: i think that raw data can affect to performance of algorithms\ngender_encoder = LabelEncoder()\ny = gender_encoder.fit_transform(labels)\nscaler = StandardScaler()\nscaler.fit(features)\nx = scaler.transform(features)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3d0bc28a-7fdc-bc93-ad28-53e23eb59427"},"source":"<h1>Features selection : see the value of features </h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d6faa4c-1008-6211-a4ec-8874e9a41137"},"outputs":[],"source":"#create a selection\nselection = SelectKBest( f_classif,  k = 5 )\nmodel = selection.fit( x , y )\n#preparate the scores\nscores_features = pd.DataFrame()\nscores_features[\"Value\"] = selection.scores_\nscores_features[\"Attribute\"] = data.drop(\"label\", axis = 1 ).columns\nscores_features = scores_features.sort_values( [\"Attribute\"] , ascending = False )\n#plots the value\nplot1 = sns.barplot( x = scores_features['Value'] , y = scores_features['Attribute'] , data = scores_features   )\nplot1.set_title('Feature Importance')"},{"cell_type":"markdown","metadata":{"_cell_guid":"43f86055-5d07-61a9-783a-f794e97ccfad"},"source":"we say the meanfun , IQR , Q25 , sd and sp.ent features are most important than other"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc4bc275-2e9b-c7e4-2b14-f0deae6cf542"},"outputs":[],"source":"#create a dataset using features selection\nfeatures_selection = data[ [\"IQR\", \"Q25\" , \"sd\" , \"sp.ent\" , \"meanfun\" , \"sfm\" , \"label\" ] ]\nfeatures_fs = features_selection.iloc[  : , :-1]\nlabels_fs = features_selection.iloc[ : , -1 ]\n#normalize the data\ngender_encoder = LabelEncoder()\ny_fs = gender_encoder.fit_transform( labels_fs )\nscaler = StandardScaler()\nscaler.fit( features_fs )\nx_fs = scaler.transform( features_fs )\n#create trainset and testset\nx_fs_train , x_fs_test , y_fs_train  , y_fs_test = train_test_split( x_fs  , y_fs , test_size = 0.2 , random_state = 1 )\n# the future plot\ndata_plot = pd.DataFrame( columns =(\"Method\",\"Accuracy\") )"},{"cell_type":"markdown","metadata":{"_cell_guid":"c3b1ff64-dbc0-9f36-ffab-78841bac5de4"},"source":"<h1>Note</h1>\nthis is a benchmark point , if you wanna know the stability or accuracy of algorithm, you need to drawn evaluate each algorithm."},{"cell_type":"markdown","metadata":{"_cell_guid":"bd3cba2a-4d15-5168-fec3-51c7eb6002e0"},"source":"<h1>Classified Methods: SVM</h1>\n<br>I selected linear SVM because i classify the unseen data in two class: male or female."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8dda98f-41a1-2c02-2d8b-3ce1cd0be688"},"outputs":[],"source":"#create a SVM and see the precision\nx_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=1)\nsvm_linear = SVC( kernel = 'linear' )\nsvm_linear.fit( x_train, y_train )\nprediction = svm_linear.predict( x_test )\nprint(\"Accuracy Score ( default linear ): \",metrics.accuracy_score( y_test, prediction ) )\n#with cross-validation \nscore_cv = cross_val_score( svm_linear , x_test , y_test , scoring = \"accuracy\",cv = 10 )\nprint(\"-->Average accuracy score ( default linear ): \",score_cv.mean() )\ndata_plot.loc[ len(data_plot) ] = [ \"SVM( default + linear) \", score_cv.mean( ) ]\n# with tuned hiperparameter with gridsearch\ntuned_parameter = { 'C': (np.arange(0.1 , 1 , 0.1 ) )  , 'kernel':[ 'linear' ] }\ntuned_svm_linear = GridSearchCV( SVC() , tuned_parameter , cv = 10 , scoring = 'accuracy'  )\ntuned_svm_linear.fit( x_train , y_train )\nprint(\"-->Accuracy score ( GridSearchCV defaultlinear ):\",tuned_svm_linear.best_score_ )\nprint(\"Best parameter ( GridSearchCV default linear ):\",tuned_svm_linear.best_params_)\ndata_plot.loc[ len(data_plot) ] = [ \"SVM ( GridSearchCv + linear + default )\" , tuned_svm_linear.best_score_ ]\n#with C = 0.1\ncustomer_svm_linear = SVC( kernel = 'linear', C= 0.1 )\ncustomer_svm_linear.fit( x_train, y_train )\nprediction = customer_svm_linear.predict( x_test )\nprint(\"Accuracy Score ( customer linear ) : \",metrics.accuracy_score( y_test, prediction ) )\n#with cross-validation\nscore_cv = cross_val_score( customer_svm_linear , x_test , y_test , scoring = \"accuracy\",cv = 10 )\nprint(\"-->Average accuracy score ( customer linear ): \",score_cv.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"SVM ( Customer + linear )\" , score_cv.mean() ]\n#with rbf kernel default + cross validation\nsvm_rbf = SVC( kernel = \"rbf\")\nsvm_rbf.fit(x_train , y_train )\nscore_cv = cross_val_score( svm_rbf , x_test , y_test , scoring = \"accuracy\" , cv= 10 )\nprint(\"-->Average accuracy score( default RBF ): \",score_cv.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"SVM ( default + rbf )\" , score_cv.mean() ]\n#with rbf kernel + gridsearch\ntuned_parameter = { 'C' : np.arange( 0.1 , 1 , 0.1) , 'gamma' : [ 0.01 , 0.02, 0.03 , 0.04 , 0.05] , 'kernel':['rbf'] }\ntuned_svm_rbf = GridSearchCV( SVC() , tuned_parameter , cv = 10 , scoring = \"accuracy\" )\ntuned_svm_rbf.fit( x_train , y_train )\nprint(\"-->Acurracy score ( GridSearchCV RBF ): \",tuned_svm_rbf.best_score_ )\nprint(\"Best parameter ( GridSearchCV rbf ): \",tuned_svm_rbf.best_params_)\ndata_plot.loc[ len( data_plot ) ] = [ \"SVM ( GridSearchCV + rbf )\" , tuned_svm_rbf.best_score_ ]"},{"cell_type":"markdown","metadata":{"_cell_guid":"ff230370-b9b3-7a58-250e-00a49d0ddf77"},"source":"We can say that the GridSearchCV get me a better accuracy than others. The RBF SVM ( with Grid Search and Cross Validation ) is better than other in accuracy and next is the linear SVM ( with Grid Search and Cross Validation )."},{"cell_type":"markdown","metadata":{"_cell_guid":"c35a5596-873d-f16b-91b2-66eaa57d262c"},"source":"<h1>Classified Methods: CART</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e97bd671-6cb8-4897-5415-0d6a3ececef2"},"outputs":[],"source":"from sklearn.tree import DecisionTreeClassifier\n#CART using default parameter and use CV\ndt = DecisionTreeClassifier()\ndt.fit( x_train , y_train )\nscore_dt = cross_val_score(dt , x_test , y_test , cv = 10 , scoring = \"accuracy\")\nprint(\"--> Average Accuracy score ( default ): \",score_dt.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"CART ( default ) \", score_dt.mean() ]\n#using features selection\ndt_fs = DecisionTreeClassifier()\ndt_fs.fit( x_fs_train , y_fs_train )\nscore_dt = cross_val_score(dt_fs , x_fs_test , y_fs_test , cv = 10 , scoring = \"accuracy\")\nprint(\"--> Average Accuracy score ( feature selection default ): \",score_dt.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"CART ( features selection ) \", score_dt.mean() ]\n#using GridSearchCV\ntune_parameter = { 'criterion':[\"gini\",\"entropy\"] , 'max_depth': [ 3, 4 ,5 ,6 , 7 , 8, 9 ,10 ] }\ntuned_dt = GridSearchCV( DecisionTreeClassifier() , tune_parameter , cv = 10 , scoring = \"accuracy\" )\ntuned_dt.fit( x_train , y_train )\nprint(\"-->Acurracy score ( GridSearchCV default ): \",tuned_dt.best_score_ )\nprint(\"Best parameter ( GridSearchCV default ): \",tuned_dt.best_params_)\ndata_plot.loc[ len( data_plot ) ] = [ \"CART ( GridSearchCV + default ) \", tuned_dt.best_score_ ]\n#using GridSearchCV + feature selection\ntune_parameter = { 'criterion':[\"gini\",\"entropy\"] , 'max_depth': [ 3, 4 ,5 ,6 , 7 , 8, 9 ,10 ] }\ntuned_dt_fs = GridSearchCV( DecisionTreeClassifier() , tune_parameter , cv = 10 , scoring = \"accuracy\" )\ntuned_dt_fs.fit( x_fs_train , y_fs_train )\nprint(\"-->Acurracy score ( GridSearchCV feature selection ): \",tuned_dt_fs.best_score_ )\nprint(\"Best parameter ( GridSearchCV feature selection ): \",tuned_dt_fs.best_params_) \ndata_plot.loc[ len( data_plot ) ] = [ \"CART ( GridSearchCV + feature selection ) \", tuned_dt_fs.best_score_ ]\n#using entropy default\ndt_entropy = DecisionTreeClassifier( criterion = 'entropy')\ndt_entropy.fit( x_train , y_train )\nscore_dt_entropy = cross_val_score( dt_entropy ,x_test , y_test , cv = 10 , scoring = \"accuracy\" )\nprint(\"--> Average accuracy score ( entropy default ): \",score_dt_entropy.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"CART ( entropy + default ) \", score_dt_entropy.mean()]\n#using entropy + features selection\ndt_entropy = DecisionTreeClassifier( criterion = 'entropy')\ndt_entropy.fit( x_fs_train , y_fs_train )\nscore_dt_entropy = cross_val_score( dt_entropy ,x_fs_test , y_fs_test , cv = 10 , scoring = \"accuracy\" )\nprint(\"--> Average accuracy score ( entropy feature selection ): \",score_dt_entropy.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"CART ( entropy + features selection ) \", score_dt_entropy.mean()]"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc7b5981-dada-9a66-86ea-73854e11609a"},"source":"The tuned parameter + features selection version is better than other and next the tuned parameter version."},{"cell_type":"markdown","metadata":{"_cell_guid":"02fcdc68-9774-27ba-1be5-dc20cbf13311"},"source":"<h1>Methods: Logistic Regression </h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63bc6417-00f8-b440-49e0-501ec82ef351"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\n#Logistic Regression default\nlr_default = LogisticRegression()\nlr_default.fit( x_train , y_train )\nprediction = lr_default.predict( x_test )\nprint(\"Accuracy ( default ): \",metrics.accuracy_score( y_test, prediction) )\nscores = cross_val_score( lr_default , x_test , y_test , cv = 10 , scoring = \"accuracy\")\nprint(\"--> Average Accuracy ( default ): \",scores.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"Logistic Regression ( default ) \", scores.mean() ]\n#Logistic Regression + GridSearchCV\ntuned_parameter = { 'penalty': ['l1','l2'], 'solver':['liblinear'] , 'C': np.arange( 0.1 , 5 , 0.1 ) }\ntuned_lr = GridSearchCV( LogisticRegression() , tuned_parameter, cv = 10 , scoring = \"accuracy\")\ntuned_lr.fit( x_train , y_train )\nprint(\"--> Accurancy ( GridSearch default ): \",tuned_lr.best_score_ )\nprint(\"Best Parameter: \", tuned_lr.best_params_ )\ndata_plot.loc[ len( data_plot ) ] = [ \"Logistic Regression ( GridSearchCV + default ) \", tuned_lr.best_score_ ]\n#Logistic Regression + Features selection\nlr_fs = LogisticRegression()\nlr_fs.fit( x_fs_train , y_fs_train )\nprediction = lr_fs.predict( x_fs_test )\nprint(\"Accuracy ( features selection ): \",metrics.accuracy_score( y_fs_test , prediction ) )\nscores = cross_val_score( lr_fs , x_fs_test , y_fs_test , cv = 10 , scoring = \"accuracy\")\nprint(\"--> Average Accuracy ( features selection default ): \",scores.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"Logistic Regression ( features selection ) \", scores.mean()  ]\n#Logistic Regression + Features selection + GridSearchCV\ntuned_parameter = { 'penalty': ['l1','l2'], 'solver':['liblinear'] , 'C': np.arange( 0.1 , 5 , 0.1 ) }\ntuned_lr_fs = GridSearchCV( LogisticRegression() , tuned_parameter, cv = 10 , scoring = \"accuracy\")\ntuned_lr_fs.fit( x_fs_train , y_fs_train )\nprint(\"--> Accurancy ( GridSearch features selection ): \",tuned_lr_fs.best_score_ )\nprint(\"Best Parameter: \", tuned_lr_fs.best_params_ )\ndata_plot.loc[ len( data_plot ) ] = [ \"Logistic Regression ( GridSearchCV + features selection ) \", tuned_lr_fs.best_score_  ]"},{"cell_type":"markdown","metadata":{"_cell_guid":"a4054bff-3ce3-e124-7368-46789ceaa077"},"source":"we say the gridsearchcv + features selection version is better than other and next is gridsearchcv + default version."},{"cell_type":"markdown","metadata":{"_cell_guid":"b2a70c0b-c3cd-ae1e-dba6-8b780beccd94"},"source":"<h1>Method: Random Forest </h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"469318f2-fd10-8cec-3ec1-0e86d650e6db"},"outputs":[],"source":"#default\nrf_default = RandomForestClassifier()\nrf_default.fit( x_train , y_train )\nprediction = rf_default.predict( x_test )\nprint(\"Accuracy ( default ): \",metrics.accuracy_score( prediction, y_test ) )\n#with CV\nscore = cross_val_score( rf_default , x_test , y_test, cv= 20 , scoring = \"accuracy\" )\nprint(\"--> Average Accurancy ( default ): \",score.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"RandomForest ( default )\" , score.mean() ]\n#with GridSearchCV\ntuned_parameter = {'criterion': [\"gini\",\"entropy\"] , 'max_features': [\"auto\",\"log2\"] , \"max_depth\": [ 3 , 4 ,5 , 6, 7 , 8 ,9 ,10,11,12 ] }\ntuned_rf = GridSearchCV( RandomForestClassifier() , tuned_parameter , cv = 15 , scoring = \"accuracy\" )\ntuned_rf.fit( x_train , y_train )\nprint(\"-->Accuracy ( GridSearch Default ): \",tuned_rf.best_score_ )\nprint(\"Best Parameter: \",tuned_rf.best_params_)\ndata_plot.loc[ len( data_plot ) ] = [ \"RandomForest ( GridSearchCV + default )\" , tuned_rf.best_score_   ]\n#with features selection\nrf_default = RandomForestClassifier()\nrf_default.fit( x_fs_train , y_fs_train )\nprediction = rf_default.predict( x_fs_test )\nprint(\"Accuracy ( features selection ): \",metrics.accuracy_score( prediction, y_fs_test ) )\nscore = cross_val_score( rf_default , x_fs_test , y_fs_test, cv= 20 , scoring = \"accuracy\" )\nprint(\"--> Average Accurancy ( features selection ): \",score.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"RandomForest ( features selection )\" , score.mean() ]\n#features selection + Gridsearch\ntuned_parameter = {'criterion': [\"gini\",\"entropy\"] , 'max_features': [\"auto\",\"log2\"] , \"max_depth\": [ 3 , 4 ,5 , 6, 7 , 8 ,9 ,10,11,12 ] }\ntuned_rf = GridSearchCV( RandomForestClassifier() , tuned_parameter , cv = 15 , scoring = \"accuracy\" )\ntuned_rf.fit( x_fs_train , y_fs_train )\nprint(\"-->Accuracy ( GridSearch Default + features selection ): \",tuned_rf.best_score_ )\nprint(\"Best Parameter: \",tuned_rf.best_params_)\ndata_plot.loc[ len( data_plot ) ] = [ \"RandomForest ( GridSearchCV + features selection )\" , tuned_rf.best_score_   ]\n# Customed\nrf_tuned = RandomForestClassifier( criterion=\"gini\", max_features = \"log2\", max_depth = 9 )\nrf_tuned.fit( x_train , y_train )\nscore = cross_val_score( rf_tuned , x_test , y_test , cv = 20 , scoring = \"accuracy\" )\nprint(\"-->Average Accuracy ( customed default ): \",score.mean() )\nrf_tuned.fit(x_fs_train , y_fs_train )\nscore = cross_val_score( rf_tuned, x_fs_test , y_fs_test , cv = 20 , scoring = \"accuracy\" )\nprint(\"-->Average accuracy (customed + features selection ): \", score.mean() )\ndata_plot.loc[ len( data_plot ) ] = [ \"RandomForest ( Customer )\" , score.mean()   ]"},{"cell_type":"markdown","metadata":{"_cell_guid":"a91bcdee-7874-084a-abdb-c1c48d8071f2"},"source":"the GridSearchVersion + default is better than other and next is GridSearchVersion + features selection."},{"cell_type":"markdown","metadata":{"_cell_guid":"a9512220-629e-2e05-4684-5a18693523bb"},"source":"<h1>Methods: Emsemble : AdaBoost </h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e0f2f49-b8ce-3a0b-7cea-2aef4c40ced1"},"outputs":[],"source":"from sklearn.ensemble import  AdaBoostClassifier\n#defaul\nab = AdaBoostClassifier()\nab.fit( x_train , y_train )\nscore = cross_val_score(ab , x_test , y_test , cv = 10 , scoring = \"accuracy\" )\nprint(\"--> Average accuracy ( default ): \",score.mean() )\ndata_plot.loc[ len( data_plot ) ] = [\" adaboost ( default ) \" , score.mean() ]\n#GridSearchCV + default\ntuned_parameter = {\"n_estimators\":[40 , 50 ,60 ,70 ,80 ] , 'algorithm':[\"SAMME\",\"SAMME.R\"]}\nab_tuned = GridSearchCV( AdaBoostClassifier() , tuned_parameter , cv = 10 , scoring = \"accuracy\" )\nab_tuned.fit( x_train , y_train )\nprint(\"--> Accuracy ( GridSearchCV + default ): \", ab_tuned.best_score_ )\nprint(\"best parameter: \",ab_tuned.best_params_)\ndata_plot.loc[ len( data_plot ) ] = [\" adaboost ( GridSearchCV + default ) \" , ab_tuned.best_score_ ]\n# with feature selection\nab = AdaBoostClassifier()\nab.fit( x_fs_train , y_fs_train )\nscore = cross_val_score(ab , x_fs_test , y_fs_test , cv = 10 , scoring = \"accuracy\" )\nprint(\"--> Average accuracy ( feature selection ): \",score.mean() )\ndata_plot.loc[ len( data_plot ) ] = [\" adaboost ( feature selection ) \" , score.mean() ]\n#GridSearchCV + feature selection\ntuned_parameter = {\"n_estimators\":[40 , 50 ,60 ,70 ,80 ] , 'algorithm':[\"SAMME\",\"SAMME.R\"]}\nab_tuned = GridSearchCV( AdaBoostClassifier() , tuned_parameter , cv = 10 , scoring = \"accuracy\" )\nab_tuned.fit( x_fs_train , y_fs_train )\nprint(\"--> Accuracy ( GridSearchCV + selection ): \", ab_tuned.best_score_ )\nprint(\"best parameter: \",ab_tuned.best_params_)\ndata_plot.loc[ len( data_plot ) ] = [\" adaboost ( GridSearchCV +  features selection ) \" , ab_tuned.best_score_ ]"},{"cell_type":"markdown","metadata":{"_cell_guid":"4219e654-73db-468e-764a-55acd42654e1"},"source":"the GridSearch + default version is better than other and next is the GridSearch +features selection version."},{"cell_type":"markdown","metadata":{"_cell_guid":"446822bd-9987-9b22-0585-8d991d1b5a1a"},"source":"<h1> Methods: General Plot</h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57f3d044-77db-eafa-ec8a-aff2903f568b"},"outputs":[],"source":"data_plot.sort_values( by = \"Accuracy\", ascending = False ).reset_index(drop = True )"},{"cell_type":"markdown","metadata":{"_cell_guid":"17cb6b45-adb8-c3c0-b71f-2317121678da"},"source":"we can say following idea:\n<ul>\n<li> the less accuracy is 95.5% and most is 98.1 </li>\n<li> We can't say nothing about the top 3 of the most accuracy because all change in each iteration</li>\n<li> the 3 last accuracy are CART ( entropy + default , features selection ,  default)</li>\n<li> the CART method is worst to accuracy in this dataset</li>\n<li> the GridSearchCV give to improve to method </li>\n<li> the SVM , RandomForest and AdaBoost are between top 10 of list </li>\n</ul>"},{"cell_type":"markdown","metadata":{"_cell_guid":"9ef53503-26c1-be7f-7bb4-e8d7eaad3601"},"source":"<h1>Question: what features are </h1>"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"268cbeae-8d13-1cf8-088e-7aa270ec4e8b"},"outputs":[],"source":"# will separate data\ndata_female = data.loc[  data[ \"label\" ] == \"female\", : ]\ndata_male = data.loc[ data[ \"label\" ] == \"male\", :]\n#plot\nplotf1 = sns.distplot( data_female[\"meanfreq\"] , color = \"blue\") \nplotm1 = sns.distplot( data_male[\"meanfreq\"] , color = \"red\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30ceea3c-01b3-0b75-0899-2f9e362cbf29"},"outputs":[],"source":"#see the relationship between some selected features\nplot2 = sns.pairplot( data[ [\"meanfreq\",\"dfrange\",\"meanfun\",\"meandom\",\"sfm\",\"label\"] ]  , hue = \"label\" , size = 3 )"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}