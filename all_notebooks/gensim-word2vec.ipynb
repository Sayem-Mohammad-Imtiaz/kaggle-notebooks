{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/news-summary/news_summary.csv', encoding = 'latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv('../input/news-summary/news_summary_more.csv', encoding = 'latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re  \nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\n\nimport spacy  \n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.shape, df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum(), df2.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.dropna().reset_index(drop = True)\ndf1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en', disable=['ner', 'parser']) \n\ndef cleaning(doc):\n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    if len(txt) > 2:\n        return ' '.join(txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"brief_cleaning1 = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df1['text'])\nbrief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df1['ctext'])\nbrief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df1['headlines'])\nbrief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df2['headlines'])\nbrief_cleaning2 = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df2['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\ntxt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning1, batch_size=5000, n_threads=-1)]\n\nprint('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\ntxt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning2, batch_size=5000, n_threads=-1)]\n\nprint('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = pd.DataFrame({'clean': txt})\ndf_clean = df_clean.dropna().drop_duplicates()\ndf_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.phrases import Phrases, Phraser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent = [row.split() for row in df_clean['clean']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = Phrases(sent, min_count=30, progress_per=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram = Phraser(phrases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = bigram[sent]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nlen(word_freq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\n\nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cores = multiprocessing.cpu_count()\ncores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"upgrad\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"bjp\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"iiit\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"amit_shah\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.similarity(\"bjp\", 'congress')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive = ['bjp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive = ['cristiano_ronaldo'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 300), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    reduc = PCA(n_components=19).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(8, 8)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'football', ['dog', 'bird', 'ronaldo', 'wenger', 'iit', 'sky', 'fill', 'hey'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'finance', [i[0] for i in w2v_model.wv.most_similar(negative=[\"football\"])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsnescatterplot(w2v_model, 'finance', [i[0] for i in w2v_model.wv.most_similar(positive=[\"finance\"])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.syn0.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(\"worst\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to average all word vectors in a paragraph\ndef featureVecMethod(words, model, num_features):\n    # Pre-initialising empty numpy array for speed\n    featureVec = np.zeros(num_features,dtype=\"float32\")\n    nwords = 0\n    \n    #Converting Index2Word which is a list to a set for better speed in the execution.\n    index2word_set = set(model.wv.index2word)\n    \n    for word in  words:\n        if word in index2word_set:\n            nwords = nwords + 1\n            featureVec = np.add(featureVec,model[word])\n    \n    # Dividing the result by number of words to get average\n    featureVec = np.divide(featureVec, nwords)\n    return featureVec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAvgFeatureVecs(news, model, num_features):\n    counter = 0\n    newsFeatureVecs = np.zeros((len(news),num_features),dtype=\"float32\")\n    for article in news:\n        # Printing a status message every 1000th review\n        if counter%1000 == 0:\n            print(\"Article %d of %d\"%(counter,len(news)))\n            \n        newsFeatureVecs[counter] = featureVecMethod(news, model, num_features)\n        counter = counter+1\n        \n    return newsFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDataVecs = getAvgFeatureVecs(df_clean, w2v_model, num_features = 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a random forest classifier to the training data\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators = 100)\n    \nprint(\"Fitting random forest to training data....\")    \nforest = forest.fit(trainDataVecs, df_clean.iloc[:,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = df1.iloc[:,5]\n# testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.dropna().reset_index(drop = True)\ntest_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\ntest_txt = [cleaning(doc) for doc in nlp.pipe(test_cleaning, batch_size=5000, n_threads=-1)]\n\nprint('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test = pd.DataFrame({'clean': test_txt})\nclean_test = clean_test.dropna().drop_duplicates()\nclean_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDataVecs = getAvgFeatureVecs(clean_test, w2v_model, num_features = 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the sentiment values for test data and saving the results in a csv file \n\nresult = forest.predict(testDataVecs)\noutput = pd.DataFrame(data={\"sentiment\":result})\noutput.to_csv( \"output.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}