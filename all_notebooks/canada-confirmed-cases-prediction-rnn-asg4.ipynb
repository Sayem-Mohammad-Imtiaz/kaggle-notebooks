{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This code is submitted by Jason Hsu(20835712) and Kasra Sadatsharifi(20812219) "},{"metadata":{"id":"uTjbuespKVUU"},"cell_type":"markdown","source":"# Canada Confirmed Cases Predictions\n>In this prediction, our goal is to predict daily confirmed cases in Canada. Seven days of daily confirmed cases would be predicted based on our model. \n\n>A recurrent neural network (RNN) based long short-term memory (LSTM) structure has been implemented. We have scaled the features in order to be in the same range as a preprocessing step. Furthermore, one layer of LSTM was implemented and dense and adam optimization were designed to get better results. In the training mode, past five-days of the data were fed to the train data and prediction is done on the sixth-day of data. Assessing the accuracy of the model was attained by mean absolute percentage error. In the end, seven days of daily confirmed cases in Canada were forecast.\n\n\n\n"},{"metadata":{"id":"2nW3LEXBNc_P","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"kDCL_zU3YAzO","outputId":"0beeeb8d-0d55-49bb-80a1-ea73cf576f61","trusted":true},"cell_type":"code","source":"url = \"../input/time-series-covid19-confirmed-global/time_series_covid19_confirmed_global.csv\"\ndf_confirmed = pd.read_csv(url)\n# df_confirmed.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"ad8hmZg2YDu2","outputId":"12d79672-48a6-47cc-c43a-c83e7d194dff","trusted":true},"cell_type":"code","source":"country = \"Canada\"\ndf_confirmed1 = df_confirmed[df_confirmed[\"Country/Region\"] == country]\ndf_confirmed1","execution_count":null,"outputs":[]},{"metadata":{"id":"Nqhe5OUbYLWN","outputId":"10d87d48-ffa9-4d43-e123-ea926811b6bb","trusted":true},"cell_type":"code","source":"df_confirmed2 = pd.DataFrame(df_confirmed1[df_confirmed1.columns[4:]].sum(),columns=[\"confirmed\"])\ndf_confirmed2.index = pd.to_datetime(df_confirmed2.index,format='%m/%d/%y')\ndf_new = df_confirmed2[[\"confirmed\"]]\ndf_new.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"id":"GNij7zG_Y04f","outputId":"b10ec3bc-5ab5-4a47-8320-31b475188ce8","trusted":true},"cell_type":"code","source":"# Get train and test\nlen(df_new)\nx = len(df_new)-5\nprint(x)\ntrain=df_new.iloc[:x]\ntest = df_new.iloc[x:]\nprint(len(df_new))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Lqe8raZ0Syc1"},"cell_type":"markdown","source":"## Data preprocessing\n\n>We tried the other moethod for scaling the data, standard scaler, However it did not work so well on our data thus we switched to minmaxscaler. The accuracy using minmax was 97% and 23% for standard scaler. We can conclude that scaling the data is a very impactful method that need to be chosen wisely.  "},{"metadata":{"id":"UGsei9NdY-fS","outputId":"41b9a737-20dc-4684-b013-9edfe233196c","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = MinMaxScaler()\nscaler.fit(train) #find max value\nMinMaxScaler(copy=True, feature_range=(0, 1))\nscaled_train = scaler.transform(train) # divide every point by max value\nscaled_test = scaler.transform(test)\nprint(scaled_train[-5:])","execution_count":null,"outputs":[]},{"metadata":{"id":"1lxudk-DZT5i","outputId":"f82f3b7a-7254-4f81-fd56-d86ad129705e","trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import TimeseriesGenerator\nscaled_train.shape\nn_input = 5   ## number of steps five days to train the model\nn_features = 1  ## number of features you want to predict (for univariate time series n_features=1)\ngenerator = TimeseriesGenerator(scaled_train,scaled_train,length = n_input,batch_size=1) #generates batches of temporal data.\ngenerator[0][0].shape,generator[0][1].shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"JFr4prGpSxUv"},"cell_type":"markdown","source":"## LSTM model\n>As shown the model is a simple RNN with a LSTM and a dense layer. The last dense layer is to output one number which is our prediction for the future days. "},{"metadata":{"id":"ZotK8I8jZIQY","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Activation\n\nmodel = Sequential()\nmodel.add(LSTM(150,activation=\"relu\",input_shape=(n_input,n_features)))\nmodel.add(Dense(75,activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer=\"adam\",loss=\"mse\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"s7VzJ0RmZOeP","outputId":"8362f4ea-c112-426e-94a0-40ba68672842","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"xM___pKzZbUq","outputId":"44d185e3-3aa8-4e13-eeea-91c14e49737c","trusted":true},"cell_type":"code","source":"# creating a validation set in order to validate the model\nvalidation_set = np.append(scaled_train[55],scaled_test)\nvalidation_set = validation_set.reshape(6,1)\nvalidation_set","execution_count":null,"outputs":[]},{"metadata":{"id":"tItXWPROZg96","outputId":"837e512c-4e9c-4885-e1e7-89f17dbc8a12","trusted":true},"cell_type":"code","source":"n_input = 5\nn_features = 1\nvalidation_gen = TimeseriesGenerator(validation_set,validation_set,length=5,batch_size=1)\nvalidation_gen[0][0].shape,validation_gen[0][1].shape","execution_count":null,"outputs":[]},{"metadata":{"id":"T9LkXtGyS5_b"},"cell_type":"markdown","source":"## Model fit"},{"metadata":{"id":"8yVKii9OZmVx","outputId":"9dbdf89b-1bd4-451a-84dd-5e928bcf9b72","trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nmodel.fit_generator(generator,validation_data=validation_gen,epochs=100,steps_per_epoch=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"0fj4ooURZr5s","outputId":"eb63536f-608a-4e57-e479-38d0159678a7","trusted":true},"cell_type":"code","source":"pd.DataFrame(model.history.history).plot(title=\"loss vs epochs curve\")\nmodel.history.history.keys()\nmyloss = model.history.history[\"val_loss\"]\nplt.title(\"validation loss vs epochs\")\nplt.plot(range(len(myloss)),myloss)","execution_count":null,"outputs":[]},{"metadata":{"id":"nXEqG35BTA2Y"},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"id":"geQfisFOZ6vx","outputId":"ced2715a-39ab-4836-a6e6-d74826cc733e","trusted":true},"cell_type":"code","source":"test_prediction = []\n\n##last n points from training set\nfirst_eval_batch = scaled_train[-n_input:]\ncurrent_batch = first_eval_batch.reshape(1,n_input,n_features)\ncurrent_batch.shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"5zLqTPWzaGBl","outputId":"1c697f35-0dbc-49eb-fa0b-4ab533d9e959","trusted":true},"cell_type":"code","source":"for i in range(len(test)+7):\n    current_pred = model.predict(current_batch)[0]\n    test_prediction.append(current_pred)\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\ntest_prediction","execution_count":null,"outputs":[]},{"metadata":{"id":"_TMiNwT4TFjl"},"cell_type":"markdown","source":"## Result of the prediction"},{"metadata":{"id":"XAppU7dxaJY5","outputId":"545273dc-d1d1-40f3-ce61-97144f8f2308","trusted":true},"cell_type":"code","source":"true_prediction = scaler.inverse_transform(test_prediction)\nprint(true_prediction[:,0])\ntime_series_array = test.index\nfor k in range(0,7):\n    time_series_array = time_series_array.append(time_series_array[-1:] + pd.DateOffset(1))\n\ndf_forecast = pd.DataFrame(columns=[\"confirmed\",\"confirmed_predicted\"],index=time_series_array)\ndf_forecast.loc[:,\"confirmed_predicted\"] = true_prediction[:,0]\ndf_forecast.loc[:,\"confirmed\"] = test[\"confirmed\"]\ndf_forecast","execution_count":null,"outputs":[]},{"metadata":{"id":"vn1OP-02TNTS"},"cell_type":"markdown","source":"## Accuracy"},{"metadata":{"id":"6hKiWnLmaZyG","outputId":"20709970-9182-49d0-8897-f676ba30fdf5","trusted":true},"cell_type":"code","source":"MAPE = 1 - np.mean(np.abs(np.array(df_forecast[\"confirmed\"][:5]) - np.array(df_forecast[\"confirmed_predicted\"][:5]))/np.array(df_forecast[\"confirmed\"][:5]))\nprint(\"accuracy is \" + str(MAPE*100) + \" %\")\nsum_errs = np.sum((np.array(df_forecast[\"confirmed\"][:5]) - np.array(df_forecast[\"confirmed_predicted\"][:5]))**2)\nprint('sum of errors: ' + str(sum_errs))\nstdev = np.sqrt(1/(5-2) * sum_errs)\nprint('standard deviation:'+ str(stdev))\n# calculate prediction interval\ninterval = 1.96 * stdev\nprint('interval: '+ str(interval))","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Asg4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}