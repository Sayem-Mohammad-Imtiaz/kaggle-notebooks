{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:56:48.148658Z","iopub.execute_input":"2021-06-12T15:56:48.149067Z","iopub.status.idle":"2021-06-12T15:56:48.82353Z","shell.execute_reply.started":"2021-06-12T15:56:48.148965Z","shell.execute_reply":"2021-06-12T15:56:48.822535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lib import\n---","metadata":{}},{"cell_type":"code","source":"# !pip uninstall transformers -y &> /dev/null\n!pip install transformers==3.5.1 &> /dev/null\n# !pip uninstall torch &> /dev/null\n!pip install torch==1.4.0 &> /dev/null\nprint(\"finished installing requirements\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T15:57:18.365924Z","iopub.execute_input":"2021-06-12T15:57:18.36631Z","iopub.status.idle":"2021-06-12T15:57:29.375397Z","shell.execute_reply.started":"2021-06-12T15:57:18.366273Z","shell.execute_reply":"2021-06-12T15:57:29.374407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:56:55.182096Z","iopub.execute_input":"2021-06-12T15:56:55.182468Z","iopub.status.idle":"2021-06-12T15:56:58.294172Z","shell.execute_reply.started":"2021-06-12T15:56:55.182428Z","shell.execute_reply":"2021-06-12T15:56:58.293221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport re\nimport time\nimport string\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nimport nltk\nfrom tqdm import tqdm\nimport os\nimport spacy\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch import optim\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:57:29.377294Z","iopub.execute_input":"2021-06-12T15:57:29.377658Z","iopub.status.idle":"2021-06-12T15:57:31.160378Z","shell.execute_reply.started":"2021-06-12T15:57:29.377615Z","shell.execute_reply":"2021-06-12T15:57:31.159156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:57:31.162438Z","iopub.execute_input":"2021-06-12T15:57:31.16279Z","iopub.status.idle":"2021-06-12T15:57:31.167944Z","shell.execute_reply.started":"2021-06-12T15:57:31.162752Z","shell.execute_reply":"2021-06-12T15:57:31.167131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Load\n---","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/indonesiahatespeechpreprocessed/notsoclean/data_preprocessed.csv\")\ndata.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:57:53.13077Z","iopub.execute_input":"2021-06-12T15:57:53.131112Z","iopub.status.idle":"2021-06-12T15:57:53.169447Z","shell.execute_reply.started":"2021-06-12T15:57:53.131068Z","shell.execute_reply":"2021-06-12T15:57:53.16848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model class section (Using IndoBert SmSA - Sentence-level Sentiment Analysis)\n---\n1. Requirement Imports","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom transformers import BertForSequenceClassification, BertConfig, BertTokenizer\nfrom nltk.tokenize import TweetTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:58:11.444365Z","iopub.execute_input":"2021-06-12T15:58:11.44471Z","iopub.status.idle":"2021-06-12T15:58:11.450456Z","shell.execute_reply.started":"2021-06-12T15:58:11.444678Z","shell.execute_reply":"2021-06-12T15:58:11.449164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Include:\n    - DocumentSentimentDataset\n    - DocumentSentimentDataLoader","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nimport torch\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nclass DocumentSentimentDataset(Dataset):\n    # Static constant variable\n    LABEL2INDEX = {'positive': 1, 'negative': 0}\n    INDEX2LABEL = {1: 'positive', 0: 'negative'}\n    NUM_LABELS = 2\n    \n    def load_dataset(self, path): \n        df = pd.read_csv(path)\n        # df.columns = ['sentiment','text']\n        df['hs_class'] = df['hs_class'].apply(lambda lab: self.LABEL2INDEX[lab])\n        return df\n    \n    def __init__(self, dataset_path, tokenizer, no_special_token=False, *args, **kwargs):\n        self.data = self.load_dataset(dataset_path)\n        self.tokenizer = tokenizer\n        self.no_special_token = no_special_token\n    \n    def __getitem__(self, index):\n        data = self.data.loc[index,:]\n        text, hs_class = data['text'], data['hs_class']\n        subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n        return np.array(subwords), np.array(hs_class), data['text']\n    \n    def __len__(self):\n        return len(self.data)    \n        \nclass DocumentSentimentDataLoader(DataLoader):\n    def __init__(self, max_seq_len=512, *args, **kwargs):\n        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = self._collate_fn\n        self.max_seq_len = max_seq_len\n        \n    def _collate_fn(self, batch):\n        batch_size = len(batch)\n        max_seq_len = max(map(lambda x: len(x[0]), batch))\n        max_seq_len = min(self.max_seq_len, max_seq_len)\n        \n        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n        \n        seq_list = []\n        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n            subwords = subwords[:max_seq_len]\n            subword_batch[i,:len(subwords)] = subwords\n            mask_batch[i,:len(subwords)] = 1\n            sentiment_batch[i,0] = sentiment\n            \n            seq_list.append(raw_seq)\n            \n        return subword_batch, mask_batch, sentiment_batch, seq_list","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:03:32.567211Z","iopub.execute_input":"2021-06-12T16:03:32.567587Z","iopub.status.idle":"2021-06-12T16:03:32.597497Z","shell.execute_reply.started":"2021-06-12T16:03:32.567551Z","shell.execute_reply":"2021-06-12T16:03:32.596497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Include:\n    - forward_sequence_classification","metadata":{}},{"cell_type":"code","source":"def forward_sequence_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n    # Unpack batch data\n    if len(batch_data) == 3:\n        (subword_batch, mask_batch, label_batch) = batch_data\n        token_type_batch = None\n    elif len(batch_data) == 4:\n        (subword_batch, mask_batch, token_type_batch, label_batch) = batch_data\n    \n    # Prepare input & label\n    subword_batch = torch.LongTensor(subword_batch)\n    mask_batch = torch.FloatTensor(mask_batch)\n    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n    label_batch = torch.LongTensor(label_batch)\n    if device == \"cuda\":\n        subword_batch = subword_batch.cuda()\n        mask_batch = mask_batch.cuda()\n        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n        label_batch = label_batch.cuda()\n\n    # Forward model\n    outputs = model(subword_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n    loss, logits = outputs[:2]\n    # generate prediction & label list\n    list_hyp = []\n    list_label = []\n    hyp = torch.topk(logits, 1)[1]\n    for j in range(len(hyp)):\n        list_hyp.append(i2w[hyp[j].item()])\n        list_label.append(i2w[label_batch[j][0].item()])\n        \n    return loss, list_hyp, list_label","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:59:07.038638Z","iopub.execute_input":"2021-06-12T15:59:07.039323Z","iopub.status.idle":"2021-06-12T15:59:07.048843Z","shell.execute_reply.started":"2021-06-12T15:59:07.039275Z","shell.execute_reply":"2021-06-12T15:59:07.048046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Metrics Include :\n    - document_sentiment_metrics_fn","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\ndef document_sentiment_metrics_fn(list_hyp, list_label):\n    metrics = {}\n    metrics[\"ACC\"] = accuracy_score(list_label, list_hyp)\n    metrics[\"F1\"] = f1_score(list_label, list_hyp, average='macro')\n    metrics[\"REC\"] = recall_score(list_label, list_hyp, average='macro')\n    metrics[\"PRE\"] = precision_score(list_label, list_hyp, average='macro')\n    return metrics","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:00:33.693227Z","iopub.execute_input":"2021-06-12T16:00:33.693577Z","iopub.status.idle":"2021-06-12T16:00:33.699122Z","shell.execute_reply.started":"2021-06-12T16:00:33.693537Z","shell.execute_reply":"2021-06-12T16:00:33.698064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Methods Include:\n    - get_lr\n    - count_param\n    - metrics_to_string\n    - set_seed","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \ndef count_param(module, trainable=False):\n    if trainable:\n        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n    else:\n        return sum(p.numel() for p in module.parameters())\n\ndef metrics_to_string(metric_dict):\n    string_list = []\n    for key, value in metric_dict.items():\n        string_list.append('{}:{:.2f}'.format(key, value))\n    return ' '.join(string_list)\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    \nset_seed(26092020)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:01:00.85151Z","iopub.execute_input":"2021-06-12T16:01:00.851864Z","iopub.status.idle":"2021-06-12T16:01:00.861756Z","shell.execute_reply.started":"2021-06-12T16:01:00.85183Z","shell.execute_reply":"2021-06-12T16:01:00.860332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pretrained Model Load\n---","metadata":{}},{"cell_type":"code","source":"# Load Tokenizer and Config\ntokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\nconfig = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\nconfig.num_labels = DocumentSentimentDataset.NUM_LABELS\n\n# Instantiate model\nmodel = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)\n\n# tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n# model = BertForMultiLabelClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)\n\n'''\nif you are loading saved model\nloaded_model = torch.load(\"./model.pkl\")\nmodel = loaded_model\n'''\n# model","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:00:17.518016Z","iopub.execute_input":"2021-06-12T16:00:17.518394Z","iopub.status.idle":"2021-06-12T16:00:26.637251Z","shell.execute_reply.started":"2021-06-12T16:00:17.518359Z","shell.execute_reply":"2021-06-12T16:00:26.636252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_param(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:01:03.283933Z","iopub.execute_input":"2021-06-12T16:01:03.284336Z","iopub.status.idle":"2021-06-12T16:01:03.291757Z","shell.execute_reply.started":"2021-06-12T16:01:03.284286Z","shell.execute_reply":"2021-06-12T16:01:03.29084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_path = \"../input/indonesiahatespeechpreprocessed/notsoclean/train_split.csv\"\ntest_dataset_path = \"../input/indonesiahatespeechpreprocessed/notsoclean/test_split.csv\"\nvalid_dataset_path = \"../input/indonesiahatespeechpreprocessed/notsoclean/validate_split.csv\"\n\ntrain_na = pd.read_csv(train_dataset_path).dropna().to_csv(\"train_new.csv\", index=False)\ntest_na = pd.read_csv(test_dataset_path).dropna().to_csv(\"test_new.csv\", index=False)\nvalid_na = pd.read_csv(valid_dataset_path).dropna().to_csv(\"validate_new.csv\", index=False)\n\ntrain_dataset_path = './train_new.csv'\ntest_dataset_path = './test_new.csv'\nvalid_dataset_path = './validate_new.csv'\n\ntrain_dataset = pd.read_csv(train_dataset_path)\ntest_dataset = pd.read_csv(test_dataset_path)\nvalid_dataset = pd.read_csv(valid_dataset_path)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:03:36.908429Z","iopub.execute_input":"2021-06-12T16:03:36.908783Z","iopub.status.idle":"2021-06-12T16:03:37.023987Z","shell.execute_reply.started":"2021-06-12T16:03:36.908751Z","shell.execute_reply":"2021-06-12T16:03:37.023151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(train_dataset_path, header=None)\ndf.columns = ['sentiment', 'text']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:02:15.296868Z","iopub.execute_input":"2021-06-12T16:02:15.29727Z","iopub.status.idle":"2021-06-12T16:02:15.320193Z","shell.execute_reply.started":"2021-06-12T16:02:15.297218Z","shell.execute_reply":"2021-06-12T16:02:15.319217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\ntest_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\nvalid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n\ntrain_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \ntest_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)\nvalid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)  \n\n\nw2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\nprint(w2i)\nprint(i2w)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:03:39.414366Z","iopub.execute_input":"2021-06-12T16:03:39.414719Z","iopub.status.idle":"2021-06-12T16:03:39.449839Z","shell.execute_reply.started":"2021-06-12T16:03:39.414688Z","shell.execute_reply":"2021-06-12T16:03:39.448895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing single row data without training\n---","metadata":{}},{"cell_type":"code","source":"text = 'Woi dasar kau anjing babi antek komunis'\nsubwords = tokenizer.encode(text)\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\nlogits = model(subwords)[0]\nlabel = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n\nprint(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:04:12.86785Z","iopub.execute_input":"2021-06-12T16:04:12.868301Z","iopub.status.idle":"2021-06-12T16:04:12.996344Z","shell.execute_reply.started":"2021-06-12T16:04:12.868249Z","shell.execute_reply":"2021-06-12T16:04:12.995249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = 'Budi pergi ke pondok indah mall membeli cakwe'\nsubwords = tokenizer.encode(text)\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\nlogits = model(subwords)[0]\nlabel = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n\nprint(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:05:07.766259Z","iopub.execute_input":"2021-06-12T16:05:07.766615Z","iopub.status.idle":"2021-06-12T16:05:07.896726Z","shell.execute_reply.started":"2021-06-12T16:05:07.766583Z","shell.execute_reply":"2021-06-12T16:05:07.895669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning\n---","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=3e-6)\nmodel = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:05:24.188476Z","iopub.execute_input":"2021-06-12T16:05:24.188807Z","iopub.status.idle":"2021-06-12T16:05:26.217349Z","shell.execute_reply.started":"2021-06-12T16:05:24.188776Z","shell.execute_reply":"2021-06-12T16:05:26.216451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\nn_epochs = 10\nfor epoch in range(n_epochs):\n    model.train()\n    torch.set_grad_enabled(True)\n \n    total_train_loss = 0\n    list_hyp, list_label = [], []\n\n    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n    for i, batch_data in enumerate(train_pbar):\n        # Forward model\n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n\n        # Update model\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        tr_loss = loss.item()\n        total_train_loss = total_train_loss + tr_loss\n\n        # Calculate metrics\n        list_hyp += batch_hyp\n        list_label += batch_label\n\n        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n            total_train_loss/(i+1), get_lr(optimizer)))\n\n    # Calculate train metric\n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n\n    # Evaluate on validation\n    model.eval()\n    torch.set_grad_enabled(False)\n    \n    total_loss, total_correct, total_labels = 0, 0, 0\n    list_hyp, list_label = [], []\n\n    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n    for i, batch_data in enumerate(pbar):\n        batch_seq = batch_data[-1]        \n        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n        \n        # Calculate total loss\n        valid_loss = loss.item()\n        total_loss = total_loss + valid_loss\n\n        # Calculate evaluation metrics\n        list_hyp += batch_hyp\n        list_label += batch_label\n        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n\n        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n        \n    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n        total_loss/(i+1), metrics_to_string(metrics)))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:05:47.553993Z","iopub.execute_input":"2021-06-12T16:05:47.554352Z","iopub.status.idle":"2021-06-12T16:11:34.435894Z","shell.execute_reply.started":"2021-06-12T16:05:47.554322Z","shell.execute_reply":"2021-06-12T16:11:34.434468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test fine-tuned model on sample sentences","metadata":{}},{"cell_type":"code","source":"correct, incorrect = 0, 0\nfor i in range(10):\n    print('='*40)\n#     print()\n    single_row = data.sample()\n    test_text = single_row['text'].values[0]\n    test_res = single_row['hs_class'].values[0]\n    print(f'test sentence:\\n{test_text}\\n')\n    print(f'actual class : {test_res}')\n\n    text = test_text\n\n    subwords = tokenizer.encode(text)\n    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\n    logits = model(subwords)[0]\n    labels = [torch.topk(logit, k=1, dim=-1)[1].squeeze().item() for logit in logits]\n    \n    for i, label in enumerate(labels):\n        print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')\n        if(i2w[label]==test_res):\n            print(\"> CORRECT PREDICTION!\\n\")\n            correct+=1\n        else:\n            print(\"> INCORRECT PREDICTION!\\n\")\n            incorrect+=1\nprint(f'\\n\\ncorrect result: {correct}/10')\nprint(f'incorect result: {incorrect}/10')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:13:24.482728Z","iopub.execute_input":"2021-06-12T16:13:24.483068Z","iopub.status.idle":"2021-06-12T16:13:24.619939Z","shell.execute_reply.started":"2021-06-12T16:13:24.483035Z","shell.execute_reply":"2021-06-12T16:13:24.61921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def return_prediction_result(text):\n    subwords = tokenizer.encode(text)\n    subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\n    logits = model(subwords)[0]\n    labels = [torch.topk(logit, k=1, dim=-1)[1].squeeze().item() for logit in logits]\n\n    for i, label in enumerate(labels):\n        return i2w[label]","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:11:44.408197Z","iopub.execute_input":"2021-06-12T16:11:44.408522Z","iopub.status.idle":"2021-06-12T16:11:44.415226Z","shell.execute_reply.started":"2021-06-12T16:11:44.408494Z","shell.execute_reply":"2021-06-12T16:11:44.414241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(test_dataset_path)\ntotal_len, correct = len(test_data.values), 0\nfor row in test_data.values:\n    actual = row[0]\n    text = row[1]\n    pred = (return_prediction_result(text))\n    if actual == pred:\n        correct+=1\nprint(f'Score based on test data: {correct}/{total_len}  Percentage:{round(correct/total_len * 100,2)}%')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:11:46.021952Z","iopub.execute_input":"2021-06-12T16:11:46.022323Z","iopub.status.idle":"2021-06-12T16:11:57.65109Z","shell.execute_reply.started":"2021-06-12T16:11:46.022288Z","shell.execute_reply":"2021-06-12T16:11:57.650204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving Model to PKL \n---","metadata":{}},{"cell_type":"code","source":"torch.save(model, \"model.pkl\")\nloaded_model = torch.load(\"./model.pkl\")\nsingle_row = data.sample()\ntest_text = single_row['text'].values[0]\ntest_res = single_row['hs_class'].values[0]\nprint(f'test sentence:\\n{test_text}\\n')\nprint(f'actual class : {test_res}')\n\ntext = test_text\n\nsubwords = tokenizer.encode(text)\nsubwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n\nlogits = loaded_model(subwords)[0]\nlabels = [torch.topk(logit, k=1, dim=-1)[1].squeeze().item() for logit in logits]\n\nfor i, label in enumerate(labels):\n    print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T16:14:44.827794Z","iopub.execute_input":"2021-06-12T16:14:44.828223Z","iopub.status.idle":"2021-06-12T16:14:46.288055Z","shell.execute_reply.started":"2021-06-12T16:14:44.828183Z","shell.execute_reply":"2021-06-12T16:14:46.287009Z"},"trusted":true},"execution_count":null,"outputs":[]}]}