{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Oppimispäiväkirja 1:\n\nJohdanto datatieteeseen 20201 oppimispäiväkirja. Ensimmäinen luento tällä kurssilla alkoi johdatuksella kurssin aiheeseen sekä ohjeisiin (oppimispäiväkirja ja harjoitustyö). Osallistuin reaaliaikaisesti ensimmäiselle luennolle, koska se töiden puolesta sopi aikatauluihin. Aineistona tässä oppimispäiväkirjassa käytän pitkälti luentoaineistoa, koska se oli kattava ja selkeä. Eli lähtökohtaisesti käsittelemäni asiat oppimispäiväkirjassa pohjautuvat luennoitsijan materiaaliin ellen toisin mainitse. \n\nLuennon pääteemat olivat datatieteen käsittelyssä eli siinä, miten datatiede määritellään, missä sitä tarvitaan ja millaiset tulevaisuuden näkymät \"alalla\" on. Datatiede koostuu neljästä eri kokonaisuudesta, jotka luennolla määriteltiin seuraaviksi: liiketoimintaosaaminen, ohjelmointi- ja tietokantaosaaminen, tilastollinen analyysi sekä datalähtöinen visualisointi. CRISP-DM -malli oli myös yksi ensimmäisellä luennolla käsitellyistä aiheista. Malli esittää prosessikuvauksen datatieteen prosesseista, mikä koostuu seuraavista kohdista: data - business understanding <-> dataunderstanding - data preparation - modeling- evaluation -deployment. \"Dataunderstanding\" tuntuu itselle ehkä näistä kaikista vieraammalta ja toivon, että kurssin aikana opin tästä lisää eli kuinka ymmärstää paremmin dataa. Monesti datan käsittely jää hyvin pintapuoliseksi,jolloin syvällinen ymmärrys sen käyttömahdollisuuksista heikkenee. Lisäksi käsiteltävinä aiheina olivat datan määrän suuri kasvu ja sen käsittely - nykyisen datan käsittely ja tekoäly painottuu koneoppimiseen. Teemana koneoppiminen ei ollut itselle aikaisemmin yhtään tuttua, vaikka termiä paljon kuuleekin. Luulen, että tästä luentokerrasta eniten minulle käteen jäi se, kuinka paljon oikeasti dataa on ja miten sen \"oikeanlainen\" käsittely voi tuoda haluttuja lopputuloksia etenkin liike-elämässä, joka itselleni on tärkeä teema opiskelujen ja työn kannalta.\n\nKäytin aikaa aika paljon alussa näiden eri ohjelmien asentamiseen, sekä ensimmäiset koodien toimintaan saamiseksi. Katsoin myös Youtubesta videoita, jotta pääsin alkuun. Anacondasin sivuilta löysin myös ohjevideon Anacondan lataamiseen, joka helpotti huomattavan paljon. Myös Kaggle tuli ensimmäistä kertaa tutuksi ja siellä oli datan lisäksi hyviä opetusvinkkejä Pythonin käyttöön. Kuitenkin datan tuominen sellaisessa muodossa, että pystyn alkaa tekemään siitä data-analyysiä ei ainakaan vielä onnistunut tässä vaiheessa, joten sitä pitää harjoitella. \n\nTärkeimmät \"oivallukset\" luentokerralta:\n- Jupyter Notebook, jonka päätin ottaa käyttöön ensimmäistä kertaa\n- Datan suuri määrä ja se, kuinka sitä voidaan käsitellä\n- Erilaiset työtehtävät datan parissa (Data Scientist tms.)\n- CRISP-DM -malli oli täysin uusi termi itselleni\n- Hans Roslingin video oli hyvin mielenkiintoinen ja avasi omat silmät\n\n\"Parannusaiheita\" on hankala keksiä, mielestäni luentoa oli mukava kuunnella ja pysyi suhteellisen hyvin kärryillä. Ehkä sellainen parannusehdotus, että kaikki uudet järjestelmät eivät olleet itselleni tuttuja ennestään, joten ne vähän sekottivat alkuun. Toisaalta oma pohjatietoni ei kurssille vastaa vaadittua, joten sikäli ymmärrän, että aivan \"nollasta\" ei voi lähteä liikkeelle. Toinen parannusehdotus on, että mielestäni luentomateriaalit ovat hyvin hajanaisesti eri paikoissa, joka hankaloittaa vähän tiedonhakua. \n\nKatsoin täältä alkuun ensin apua koodeihin:\nhttps://dev.socrata.com/blog/2016/02/01/pandas-and-jupyter-notebook.html \n-> olen katsonut täältä, koska oma lähtötasoni on aivan aloittelija, joten siksi päädyin katsomaan \"valmiita\" materiaaleja aluksi. Kyseisellä sivustolla on perus tilastolliseen analyysiin liittyviä koodeja ja ohjeita, jotka mielestäni olivat selkeitä.\n-> lisäksi osan koodeista olen katsonut demo-klinkan materiaaleista, ja koin sen ihan hyödylliseksi. En ennen tätä tiennyt, kuin print ('hello world'), ja nyt ainakin jollain tasolla ymmärsin import, print (head) ja muut perus käskyt, joiden avulla pääsin alkuun. \n-> data on peräisin Kagglesta, jossa käsitellään ETF Fundeja. \n","metadata":{}},{"cell_type":"code","source":"print ('hello world')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print ('mita kuuluu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nprint(pd.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declare the libraries that will be used\nimport pandas as pd\n# Used to plot the results\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/mutual-funds-and-etfs/ETFs.csv')\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.dtypes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caps = lambda x: x.upper()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caps ('moi')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Oppimispäiväkirja 2. \n\nTällä kerralla luennon aiheena oli datan kerääminen ja jalostaminen. En päässyt osallistumaan reaaliaikaisesti opetukseen, koska olin töissä mutta katsoin luennon jälkeenpäin itse. Pääasiassa käytän hyväksi Jupyter Notebookissa ollutta materiaalia eli materiaalia, mitä luennoitsija on koonnut luennolle. Mikäli käytän muuta materiaalia, ilmoitan siitä erikseen. Lisäksi katsoin jälkikäteen demo-klinikan videon, koska töiden vuoksi en sitäkään pystynyt katsomaan oikeassa aikataulussa. Katsoin heti alussa myös Datacampin linkin, jossa oli hyvin selkeästi selitetty käskyt, joita Pythonilla voidaan käyttään. Luulen, että tämä auttaa ainakin tulevaisuudessa, sillä käskyt ovat itselläni tässä vaiheessa edelleen aika hukassa. \n\nData Science Workflow artikkeli oli ihan mielenkiintoinen ja se käytiin myös luennolla läpi hyvin. Datan siivoamiseen käytettävä aika yllätti ehkä itseni, ja se olikin hyvä nosto. Luennolla käytiin läpi myös datatieteenprosessi, joka jakautuu seuraaviin päävaiheisiin: tiedon esikäsittely, vuoropuhelu analyysin ja reflektion välillä ja lopputulosten viestiminen vastaanottajalle soveltuvassa muodossa. Tutuin vaihe itselleni näistä on viimeinen, eli vastaanottajalle viestiminen. Tiedon esikäsittely on vaikeampaa, koska en omaa tällä hetkellä ainakaan vielä sellaisia taitoja, joiden avulla osaisin käsitellä erityisen paljon tietoa alussa. Mielenkiintoista oli se, kuinka dataa ja tietoa voidaan hyödyntää liike-elämässä, koska se on itselleni läheinen aihe. Aika pienessä osassa yrityksistä on oma data-analytiikan tiimi, joka varmasti näkyy siinä, että dataa ei osata yrityksissä hyödyntää täysissä määrin ja oikein. \n\nHarward Business Reviewin artikkeli siitä, että datatieteen tiimit tarvitsevat \"generalisteja\", ei vain \"specialisteja\" oli mielenkiintoinen. Poiketen yleisestä ajatuksesta, että pitäisi olla erikoistunut yhteen tiettyyn aihealueeseen haastetaan kyseisessä artikkelissa. Myös artikkelin perustelut siitä, miksi näin olivat mielenkiintoisia. Etenkin datatieteen parissa oppiminen on jatkuvaa ja ehkä siksi myös kokonaisuuksien hahmottaminen suuressa määrin on tärkeää - tai näin ainakin itse ajattelen asiasta. Itsestä kuitenkin tuntuu siltä, että yritykset silti etsivät \"specialisteja\" tai olettavat tämän osaavan kaikki järjestelmät tai monet järjestelmät asiantuntijan tavoin. Ehkä tärkeämpää on se, että henkilö tietää, missä kohti mitäkin on järkevää käyttää ja mitä eri järjestelmillä voidaan tehdä. Toki uskon siltikin, että datatieteen tiimit tarvitsevat kovan luokan asiantuntijoita juuri yhden osa-alueen osalta, mutta siksi juuri tiimissä onkin hyvä olla erilaisia henkilöitä. \n\nRyömijät ja raapijat olivat termeinä tässä yhteydessä itselleni uusia. Siihen liittyvät koodit vaikuttivat hieman monimutkaisilta. Verkkokauppa.com esimerkki luennolla havainnollisti kuitenkin niiden käyttöä hyvin, mutta en usko, että tällä hetkellä itse osaisin hyödyntää niitä millään tavalla. Tämä kuitenkin avasi silmiäni, mitä kaikkea yritykset voi tehdä tai saatta tehdä, ja mikä on eettisesti ja laillisesti oikein (Power-esimerkki). Lopussa \"alkuohjeet\" Pandasin käyttöön olivat hyvät, koska tuskailin sen kanssa ennen kuin kerkesin katsoa luennon ja diat. Tämä ainakin auttoi siinä, että sain Pandasin käyttöön ja \"yhdistettyä\" Jupyterin kanssa. \n\nTietotekniikkaa opiskeleva kaverini on auttanut jonkin verran alussa ja kertounut esimerkiksi aikaisemmin BeautifulSoup -järjestelmästä. En silloin ymmärtänyt, mitä se meinaa, joten hauska oli huomata, että nämä asiat käytiin nyt luennolla läpi ja ymmärsin ainakin hieman paremmmin, mitä sillä voidaan tehdä. \n\nTärkeimmät oivallukset:\n- Datatieteenprosessi: mitä itse osaan ja mitä en todellakaan osaa\n- Se, kuinka paljon datan siivoamiseen täytyy käyttää aikaa (voisiko tätä prosessia tehostaa jollain tapaa tulevaisuudessa, vaikka datan määrä kasvaa kokoajan)\n- Datan hyödyntäminen liike-elämässä: yritykset hyödyntävät sitä vielä \"liian\" vähän ainakin omasta mielestä -> toivottavasti tulevaisuudessa voin olla vaikuttamassa tähän!\n- Pandas, luentomateriaalit auttoivat ymmärtämään alkuun pääsemisessä Pandasin kanssa\n- Minua kiinnostaa datan hyödyntäminen liiketoiminnassa, mutta ei ehkä suoranaisesti menetelmät tai itse datan esikäsittely\n- Positiivisempi kuva siitä, että voin itsekin opetella käyttämään Pythonia, Jupyterya, Pandasia tms. Aikaisemmin ajattelin, että ne ovat liian vaikeita\n\nParannettavaa ehkä luennolle: olisin voinut tarvita lisäapua raapijoiden ja ryömijöiden kanssa, mutta muuten ei mitään! Verkkokauppa.com esimerkki oli hyvä ja tässä vaiheessa varmaan riittää minulle. ","metadata":{}},{"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print ('moi')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('1+3=4')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/climate-change-earth-surface-temperature-data/GlobalLandTemperaturesByCountry.csv')\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf['AverageTemperature'].is_unique","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfZone=df[['Country', 'AverageTemperature']]\ndfZone.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oppimispäiväkirja 3.\n\nTällä kerralla aiheena oli koneoppiminen. Tämä kerran luennon katsoin myös jälkikäteen, koska töiden puolesta en voinut osallistua reaaliaikaisesti luennolle. Tällä kerralla aiheena oli käydä läpi koneoppimisen perusteita. Luin artikkelin myös siitä, kuinka laadullinen tieto jalostuu laskennalliseksi: piirteet sosiaalisen median analytiikassa. Artikkeli oli silmiä avaava, koska en etukäteen tiennyt, kuinka kehittynyttä koneoppiminen voi olla ja kuinka se tunnistaa esimerkiksi ihmiset kuvasta prosentuaalisella todennäköisyydellä. Myös luennolla lyhyesti läpi käyty esimerkki siitä, että miten koneoppimisen myötä saatetaan olettaa tiettyjä asioita mm. iPhonen ja Androitin käyttäjistä. \n\nKoneoppiminen terminä on kuultu monessa yhteydessä, mutta itselleni sen syvempi ymmärtäminen on jäänyt hieman auki. Luennon aikana ja materiaaleista koneoppiminen terminä avautui hyvin, sekä se, mihin koneoppimistä tarvitaan. Luennolla käytiin läpi myös sitä, kuinka tekoälyn kehitys on jaettu kolmeen eri osaan DARPAn John Launchburyn ehdotuksen mukaan: käsin rakennettuun toteutukseen, tilastolliseen oppimiseen sekä tilanteeseen mukautuvaan oppivaan tekoälyyn. Oli siis ihan mielenkiintoista perehtyä lyhyesti myös tekoälyn kehityksen historiaan sekä vaiheisiin. Viimeinen kohta eli ohjattu oppiminen oli luennolla pääosassa käsittelyä - ja mielestäni se olikin tärkeää, koska uskon näille taidoille olevan kysyntää myös työelämässä. Esimerkiksi luennolla mainittu lineraainen regressio ja sen käyttäminen hintojen tai muiden asioiden ennustamisessa on tärkeä osa mm. analyytikon tehtäviä, joka ainakin itselleni voisi olla mielenkiintoinen urakokeilu. Lineaarista regressiota olen käyttänyt muilla tilasto-ohjelmilla, kuten Statalle, mutta Pythonilla sen tekeminen ja opettelu on pitkässä juoksussa varmasti fiksumpaa.\n\nLuennolla käydyt esimerkit puhelinliittymistä ja kuinka uutta \"halvempaa\" liittymää tarjomalla saadaan uusi datapiste, ja tärkeimpänä tietenkina asiakkaan asiakkuuden jatkaminen. Mielestäni tämä avasi hyvin konkreettisesti sitä, missä kaikkialla ja miten koneoppimista käytetään. Minun on itse hankala tarkemmin ottaa kantaa syvällisemmin koneoppimiseen, koska asia on täysin uutta. Esimerkkien avulla aiheesta saa kuitenkin mielenkiintoisemman, sillä ajattelin aikaisemmin sen olevan niin kauaskantoista asiaa, josta minä en voi ymmärtää yhtään mitään. \n\nLuennolla käyty malli esimerkki mallista, kannattaako lainaa antaa asiakkaalle oli hyvä ja sai ainakin itseni innostumaan enemmän aiheesta, koska päästiin hieman lähemmäksi kaupallisuutta ja pankkien toimintaa. Tämän kerran luentoesimerkki oli tähän astisista esimerkeistä paras sekä hyödyllisin, toki myös laajin. Käytän tätä varmasti hyödykseni paljon harjoitustyötä tehdessäni ja palaan aiheeseen vielä uudestaan, koska luennon aikana en koe oppineeni asiaa kunnolla. Pitää myös itse kokeilla mallin tekemistä, jolloin jokainen kohta aukeaa varmasti paremmin. Sanaselityksiä ei tällä luennolla kauheammin tullut, mutta se ei mielestäni mitään haitannut. Yleisesti koneoppimista käytiin esimerkein läpi, joka on hyvä asia, koska pelkkä termien läpikäyminen ei sinällään opeta kauheasti.\n\nTärkeimmät oivallukset:\n- Koneoppiminen yleisesti ja sen käyttö\n- Teköälyn kehityksen vaiheet\n- Lineaarinen regressio Pythonilla (aikaisemmin tuttu vain Statalla) -> mielenkiinto opetella myös Pythonilla1\n- Luotonanto esimerkki ja oivallukset, miten pankit voivat käyttää vielä enemmän hyödyksi tätä\n- Onko mallin muuttujat merkittäviä vai ei, miten niiden merkittävyyttä tulisi tulkita\n\nParannettavaa voisi olla \"luentodiojen\" selkeydessä - välillä en aina ihan hahmota kaavioiden ja tekstien ideaa. Siksi joitakin termejä voisi avata enemmän dioissa.","metadata":{}},{"cell_type":"markdown","source":"# Oppimispäiväkirja 4. \n\nTämä luento käsitteli pitkälti harjoitustyön tekemistä ja sen vaiheita. Mielestäni tämä luento oli hyvä ja auttoi ainakin itseäni harjoitustyön kanssa, koska ennen luentoa en oikein tiennyt, mitä minun tulisi käytännössä tehdä ja miten. Niklas Stephensonin mahdollisuuskehikko oli täysin uusi, mutta tulen sitä käyttämäänt harjoitustyössä hyväksi. Tämä oli hyvin konkreettinen esimerkki ja uskon, että tätä käytetään myös oikeassa yrityselämässä ja se tekikin tästä hyvin mielenkiintoisen! En päässyt valitettavasti mukaan ryhmäkeskusteluihin, koska olin töissä luennon aikana. \n\nPäätin tämän perusteella, että teen harjoitustyön todennäköisesti AirBnB -aineistosta, koska luulen ymmärtäväni yhtiön liiketoimintaa ainakin jollain tasolla, ja se oli yksi tärkeä osa Stephensonin mallia. Vastaan tässä myös luennolla käytyihin kysymyksiin omasta näkökulmasta:\n- Miten näkisitte että Airbnb tapauskuvauksessa voisi yllä mainittu mittakehikko toimia?\n- Miten laatu, nopeus ja robustisuus voisi muuttua datan avulla?\n\nEnsimmäiseen kysymykseen vastauksena koen, että Airbnb:N tapauksessa mittakehikko, joka luentomateriaalissa on voi toimia, koska saatavilla oleva data tukee laatua, nopeutta ja robustisuutta. \n\nTärkeimmät oivallukset:\n- Mahdollisuuskehikko: täysin uusi juttu mutta kuulostaa hyödylliseltä!\n- Liiketoiminnan syvällinen ymmärtäminen, jotta datan analysoimisessa olisi ylipäätään mitään järkeä tai että saadaan oikeat lopputulokset\n- Datan visualisoinnin tärkeys: katsoja saa paremman käsityksen tutkimuksesta ja hahmottaa eri muuttujat (mikä oleellista ja mikä ei)\n- Mallintaminen eli kuinka ratkaista kysymys ohjelmallisesti\n- Iterointi, eli aina voi keksiä uusia kysymyksiä datan käsittelyn jälkeen\n\nParannusehdotus: ehkä lisää vielä esimerkkejä, jos jotain pakko keksiä. Minusta tämä luento oli kuitenkin todella hyvä ja selkeä! Kiitos.\n\n","metadata":{}},{"cell_type":"markdown","source":"Oppimispäiväkirja 5.\n\nTämä luento oli vierailijaluento, jonka kävin läpi jälkikäteen töiden takia. Kiva, että saatiin kurssille myös työelämän edustusta. Oikeastaan kaikki luennolla käydyt asiat olivat uusia itselleni termit mukaan lukien. Aiheena oli luonnollisen kielen analyysi. Koen siis oppineeni aika paljon tämän luennon aikana ja tässä oppimispäiväkirjassa käyn läpi myös termejä, jotka käyn itse läpi. Aluksi luennolla käytiin läpi termiä NLP, joka tarkoittaa datatieteen alaa, jossa datalähteenä on luonnollinen kieli. NLP:tä käytetään esimerkiksi chatboteissa ja automaattisissa konekäännöksissä. Kieli linkittyykin koneoppimiseen useimmiten. Google Colaboraty on Googlen ylläpitämä Notebook-palvelu. Luennolla käytiin pikakomentoja läpi, jotka toki tässä vaiheessa ei mieleen jäänyt kauhean tarkasti.\n\nTärkeimmät oivallukset:\nUudet termit\nStepwordit eli hukkasanat, kuinka niitä voidaan lukea ja tulkita\nDatasettien tasapainotus, kun ryhmissä tapahtuu muutoksia (ministeriöesimerkki)\nOli mielenkiintoista nähdä, kuinka sanoja voi muokata\nTwiittien yhdistäminen dataan!\n\nParannettavaa ehkä siinä, että luento koostui \"koodista\" pitkälti, olisin kaivannut hieman enemmän myös kirjallista selitystä aiheesta, jotta olisin ymmärtänyt paremmin asiat.","metadata":{}},{"cell_type":"markdown","source":"Oppimispäiväkirja 6.\n\nTämä luento käsitteli ohjaamatonta oppimista poiketen ohjatusta oppimisesta, jota aikaisemmin ollaan käsitelty. Kävin luennon teemat läpi itsenäisesti, koska olin töissä päivän. Ohjaamattomasta oppimisesta on kyse silloin, kun ennalta ennustettua piirrettä ei ole - välillä data-analyysissä tarvitaan myös ohjaamatonta oppimista! Ohjaamattoman oppimisen piirteitä ovat ostoskorianalyysi, verkostoanalyysi, ryvästäminen sekä aihemallinnus. Ennestään tuttu termi oli ryvästäminen sekä ostoskorianalyysi, muutta verkostoanalyysi sekä aihemallinnus olivat uusia termejä.\n\nMuuttujien normalisointia ja siihen liittyvää datan pyörittämistä (vinkkejä siihen ja koodia) auttoi ymmärtämään luentomateriaaleissa ollut linkki Alex Greenin blogitekstiin, jonka kävin lukemassa luentomateriaalien läpi käynnin jälkeen. Luennolla käytiin enemmän läpi ryvästämistä, joka aiheena oli hieman jo tuttua, joten pääsin perehtymään asiaan tarkemmin luentomateriaaleista. K-means algortimi oli kuitenkin uusi termi, ja tutkin aihetta hieman lisää itse Googlen avulla, koska en alkuun tajunnut mistä oli kyse.\nKaiken kaikkiaan tämä luento ei ollut ehkä edellisiin nähden niin laaja, enkä ole varma kuinka paljon tulevaisuudessa käytän näitä asioita. Luultavasti ohjaamaton oppiminenkin ja siihen liittyvä data-analyysi tulee vastaan jossain tilanteessa, mitä en tällä hetkellä itse välttämättä edes ymmärrä.\n\nTärkeimmät oivallukset:\nohjaamaton oppiminen ja missä kaikessa sitä voidaan käyttää (osa vielä vähän auki edelleen!)\nohjaamattoman oppimisen eri piirteet (ryvästämminen ja ostoskorianalyysi hieman tuttuja)\nmuuttujien normalisointi\nK-means algoritmin tarkoitus\nSe, että paljon voi etsiä lisätietoa eri aiheista myös itse\n\nParannusehdotuksena ehkä tulevaisuutta ajatellen se, että aukaistaisiin enemmän vielä materieeleihin termejä. Toki varmasti ihan hyvä aktivoida opiskelijoita etsimään itse myös tietoa. Ja vaikka videoilla käydäänkin läpi laajasti aiheita, en itse oppimisen kannalta kuuntele kaikkia videoita kokonaan, koska opin paremmin lukemalla.","metadata":{}},{"cell_type":"markdown","source":"Oppimispäiväkirja 7.\n\nViimeisen luennon aiheena oli visuaalinen analytiikka - ja tämä ainakin kiinnosti aiheen perusteella itseäni paljon! Ihan alkuun visuaalisella analyytikalla tarkoitetaan datatieteessä kahta asiaa: raakadatan tutkiva kartoittaminen sekä lopputuloksen esittäminen sellaisessa muodossa, että kohdeyleisö ymmärtää asian. Etenkin jälkimmäinen sopii perusajatukseen hyvin, sillä se miten lopputuloksen esittää on paljon merkitystä. Haluaisin myös itse oppia lisää siitä, kuinka data-analyysin ratkaisut voidaan esittää paremmin ja tehokkaammin yleisölle visuaalisesta näkökulmasta katsottuna.\n\nBen Fryn (2008) kehittelemä seitsämän askeleen kohta datan visualisoinnille oli mielestäni selkeä, kun se luentomateriaalissa ja luennolla käytiin läpi. Etenkin viimeinen kohta vuorovaikuttaminen oli mielestäni tärkeää, sillä sen, millä tavalla lopputuloksen esittää on merkitystä. Lopputuloksen esittämisen pitäisi toimia vuorovaikutuksessa yleisön kanssa ja siksi datan visualisointi onkin hyvin tärkeää.\n\nHuonosta datan visualisoinnista ollut esimerkki oli hyvä, koska siitä kävi ilmi, että visualisoinnin merkitys oli suuri. Mielestäni esitetty data oli tehty huonosti, epäselvästi, eikä siihen jaksanut kiinnittää tarkempaa huomiota edes. Esimerkiksi palkkien suuri ero, vaikka prosentuaalinen ero oli todella pieni, väärensi illuusiota datan oikeasta erosta. Myös liiallinen värien käyttö ja materiaali, sekä vääränlainen kuvaaja vaikuttavat siihen, kuinka laadukasta visualisointi on. Nämä olivat ennestään jo tuttuja asioita, mutta toisaalta hyvä muistuttaa itseään myös aina näistä.\n\nTärkeimmät oivallukset:\nvisualisoinnin merkitys\naihe kiinnostaa itseäni tämän jälkeen vielä enemmän\nseitsämän askeleen kohta datan visualisoinnille\nkuinka pythonilla voi visualisoida dataa\nhuonot visualisoinnin tavat (liikaa aineistoa, värejä tms.)\ntutstuin paremmin Tableuhun\n\nTämä oli mielestäni kokonaisuudessaan hyvä luento ja materiaalit, joten hankala keksiä parannettavaa. Ehkä jonkin reaalimaailman yritys-casen lisäisin tähän.\n","metadata":{}}]}