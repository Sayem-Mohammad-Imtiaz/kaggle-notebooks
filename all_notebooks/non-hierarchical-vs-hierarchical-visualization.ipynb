{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first kernel and I am trying to implment all the skills I have learnt during my work on Credit Risk and Credit Card Marketing.\nThe approach I have followed is:\n1.     Do EDA on the Data and understand what variables makes sense to take into clustering.\n1.     k-mean clustering with certain clusters and then combining them to arrive at n distinct clusters\n1.     2-D & 3-D visualizations with Plotly to understand how clusters look different\n1.     Try Agglomerative Clustering which is a type Hierarchical Clustering \n1.     Visualize the results on some variable\n    \n\n**Clustering & its benefits?**\nClustering is the grouping of objects together so that objects belonging in the same group (cluster) are more similar to each other than those in other groups (clusters). Clustering solves a lot of purpose:\n*     Clustering is useful to find similar groups of stores for retail chains to help an organization with increase of sales in the same cluster\n*     Clustering helps in the test and control strategies in marketing\n*     Clustering helps in disburment of loans of similar types and mitigate risk\n*     Clustering can also help in developing recommendation engines\n*     Clustering is also useful in social network analysis like finding targets on Facebook, Twitter or Instagram\n  \nIn here, the objective is to market or rather personalize Credit Cards products/offers to one cluster and diversify on the basis of strategy. It will help to remain focus on each clusters and implement strategies as per their Credit Card Behaviour. In my own experience, I have used clustering a lot both in retail and banking projects\n\n**Types of clustering**\nThere are many types of clustering but in this kernel we will implement two types:\n*     **Non-Hierarchical Clustering**: In this the groups are formed in comparison to the distance between each other. In the non-hierarchical method a position in the measurement is taken as central place and distance is measured from such central point (seed). The distance used is Eucleadian Distance. A popular method is k-means which is based on selecting random central points (Centroids) and then calculating the distance of all other points from these random centroids uptill there is no change in centroids. All the points closer to one centroid will be considered a cluster. In this there is no relationship or a hierarchy considered with the other point\n    \n*     **Hierarchical Clustering**: Hierarchical clustering, as the name suggests is an algorithm that builds hierarchy of clusters. This algorithm starts with all the data points assigned to a cluster of their own. Then two nearest clusters are merged into the same cluster. In the end, this algorithm terminates when there is only a single cluster left. Thus we are left with a hierarchy for each of the cluster\n\nI will be trying to do clustering with some other techniques when I get time.\nI would appreciate your feedback and please upvote if you like my work","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_   = pd.read_csv('../input/ccdata/CC GENERAL.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def missings_(data):\n    miss      = data.isnull().sum()\n    miss_pct  = 100 * data.isnull().sum()/len(data)\n    \n    miss_pct      = pd.concat([miss,miss_pct], axis=1)\n    missings_cols = miss_pct.rename(columns = {0:'Missings', 1: 'Missing pct'})\n    missings_cols = missings_cols[missings_cols.iloc[:,1]!=0].sort_values('Missing pct', ascending = False).round(1)\n    \n    return missings_cols  \n\nmissings = missings_(data_)\nmissings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,4,figsize =(20,4))\nax0, ax1, ax2, ax3 = ax.flatten()\n\nax0.hist(data_['BALANCE'], bins = 60, alpha =0.8 )\nax1.hist(data_['ONEOFF_PURCHASES'], bins = 60, color=\"green\" ,alpha =0.8 )\nax2.hist(data_['PURCHASES'], bins = 60, color=\"red\",alpha =0.8 )\nax3.hist(data_['PAYMENTS'], bins = 60, color=\"orange\",alpha =0.8 )\n\nax0.set_title(\"BALANCES\")\nax1.set_title(\"ONEOFF_PURCHASES\")\nax2.set_title(\"PURCHASES\")\nax3.set_title(\"PAYMENTS\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols   = data_.columns\nfig , ax = plt.subplots(1,4, figsize = (20,8))\nax0, ax1, ax2, ax3 = ax.flatten() \n\nfor i in range(0,4):\n    \n    X   = data_[cols[i+2]]\n    Y   = data_[cols[1]]\n    ax[i].plot(X, Y, marker = 'o', linestyle = \"None\")\n    ax[i].set_xlabel(cols[i+2])\n    ax[0].set_ylabel(cols[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (50,20))\ndata_sub = data_[(data_['BALANCE_FREQUENCY']>=0.3)]\ndata_sub['BALANCE_FREQ'] = round(data_['BALANCE_FREQUENCY'],2)\n\nsns.violinplot(y='BALANCE',x='BALANCE_FREQ',data=data_sub)\nplt.xlabel('BALANCE FREQ',fontsize=40)\nplt.ylabel('BALANCE',fontsize=40)\nplt.tick_params(labelsize=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring how Purchases behave for these customers\n#### Creating bins to understand the distributions of these variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_['Balance_decile'] = pd.qcut(data_['BALANCE'], q=10)\ndata_grp   = data_.groupby('Balance_decile', as_index=False).mean()\ndata_grp   = data_grp[['Balance_decile', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES']]\ndata_grp_t = pd.melt(data_grp, id_vars = 'Balance_decile')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nsns.barplot(x= \"Balance_decile\" , y = \"value\", hue = 'variable', data =data_grp_t)\nplt.ylabel(\"Average Purchase Amount\", fontsize=20)\nplt.xlabel(\" Balance Groups\", fontsize =20)\nplt.tick_params(labelsize=12)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding average Balance left by Frequency of Purchases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_['freq_purchase_decile'] = pd.qcut(data_['PURCHASES_FREQUENCY'], q=4)\ndata_bal   = data_.groupby('freq_purchase_decile', as_index=False).mean()\nfig = plt.figure(figsize=(10,5))\nsns.barplot(x= \"freq_purchase_decile\" , y = \"BALANCE\", data =data_bal)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Credit Card Utilization: It is quite useful metric while working on Banking projects or in a Fintech. It tells us basically how much unused balance is left in the credit card of the customer. It is calcualated as CC Utilization = (Credit Card Limit - Balance Used)/Credit Card Limit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_['CREDIT_LIMIT'].fillna(1, inplace=True)\ndata_['CC_utilisation']     = (data_['CREDIT_LIMIT'] - data_['BALANCE'])/data_['CREDIT_LIMIT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_['CC_util_decile']     = pd.qcut(data_['CC_utilisation'], q=10)\ndata_cc_grp                 = data_.groupby('CC_util_decile', as_index=False).mean()\ndata_cc_grp                 = data_cc_grp[['CC_util_decile', 'PAYMENTS' , 'MINIMUM_PAYMENTS']]\ndata_cc_grp_t               = pd.melt(data_cc_grp, id_vars = 'CC_util_decile')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\nsns.barplot(x= \"CC_util_decile\" , y = \"value\", hue = \"variable\" ,data =data_cc_grp_t)\nplt.xlabel(\"Credit Card Utilization\")\nplt.ylabel(\"Average Payments\")\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating clusters for Marketing and Risk Strategy\n#### Also, we had outliers in our columns so it will be better to bin the variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_n  = data_.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES','CASH_ADVANCE',\n         'CREDIT_LIMIT', 'PAYMENTS']\nfor c in cols:\n    bins = c+'_bin'\n    max_ = max(data_n[c])\n    data_n[bins] = pd.cut(data_n[c], bins=[0,500,1000,3000,5000,10000,15000,max_],labels = [1,2,3,4,5,6,7], include_lowest= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY','PURCHASES_INSTALLMENTS_FREQUENCY',\n         'CASH_ADVANCE_FREQUENCY']\nfor c in cols:\n    bins = c+'_bin'\n    max_ = max(data_[c])\n    data_n[bins] = pd.cut(data_n[c], bins=[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,max_],labels = [1,2,3,4,5,6,7,8,9,10], include_lowest= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['CASH_ADVANCE_TRX', 'PURCHASES_TRX']\n\nfor c in cols:\n    bins = c+'_bin'\n    max_ = max(data_[c])\n    data_n[bins] = pd.cut(data_n[c], bins=[0,20,40,60,80,100,max_],labels = [1,2,3,4,5,6], include_lowest= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping columns before doing k-means clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model  = data_n.drop(['CUST_ID', 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'Balance_decile',\n       'freq_purchase_decile', 'CC_utilisation', 'TENURE', 'PURCHASES_TRX_bin', 'CASH_ADVANCE_TRX_bin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model  = data_model.drop(['CC_util_decile'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stand_         = StandardScaler()\ndata_model_std = stand_.fit_transform(data_model)\n\nrandom.seed(234)\nn_clusters=20\nsse=[]\nfor i in range(1,n_clusters+1):\n    kmean= KMeans(i)\n    kmean.fit(data_model_std)\n    sse.append([i, kmean.inertia_]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1])\nplt.title(\"Elbow Curve\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(234)\nkmean= KMeans(8)\nkmean.fit(data_model_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmean.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_kmeans = kmean.predict(data_model_std)\ny_kmeans\n\ndata_model['Cluster']       = y_kmeans\ndata_model_std              = pd.DataFrame(data_model_std)\ndata_model_std['Cluster']   = y_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model['Cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in data_model:\n    g   = sns.FacetGrid(data_model, col='Cluster')\n    g.map(plt.hist, c, color = \"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seeing the clusters we try to regroup them on the basis of their variable distributions\n#### Cluster 1 and 3 could be combined\n#### Cluster 0 and 2 could be combined\n#### Cluster 4 and 5 could be combined","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_model[\"Cluster\"].replace({3: 1, 2: 0, 5:4}, inplace=True)\ndata_model['Cluster'].value_counts()\ndata_model_std[\"Cluster\"].replace({3: 1, 2: 0, 5:4}, inplace=True)\nclusters_   = data_model[\"Cluster\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2-D & 3-D Visualization of clusters using PCA and plotly. Here we will visualize the cluster using seaborn and Plotly library. Plotly allows us to draw some amazing and cutting edge interactive visualizations. We wil be using PCA as we had to get 2 and 3 axis which is a combination of the other variables in the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(32)\npca = PCA()\npca.fit(data_model_std)\n\n\nfig = plt.figure(figsize =(12,6))\nplt.plot(range(0,12),pca.explained_variance_ratio_.cumsum(), marker ='o', linestyle = \"--\")\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Variance Explained\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = 3)\npca.fit(data_model_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = pca.transform(data_model_std)\n\n\nx,y = scores[:,0] , scores[:,1]\ndf_data = pd.DataFrame({'x': x, 'y':y, 'clusters':clusters_})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouping_ = df_data.groupby('clusters')\nfig, ax = plt.subplots(figsize=(20, 13))\n\nnames = {0: 'Cluster 1', \n         1: 'Cluster 2', \n         4: 'Cluster 3',\n         6: 'Cluster 4',\n         7: 'Cluster 5'}\n\nfor name, grp in grouping_:\n    ax.plot(grp.x, grp.y, marker='o', label = names[name], linestyle='')\n    ax.set_aspect('auto')\n\nax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y,z = scores[:,0] , scores[:,1], scores[:,2]\n\ndf_data = pd.DataFrame({'x': x, 'y':y, 'z':z, 'clusters':clusters_})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize cluster shapes in 3d.\n\ncluster1=df_data.loc[df_data['clusters'] == 0]\ncluster2=df_data.loc[df_data['clusters'] == 1]\ncluster3=df_data.loc[df_data['clusters'] == 4]\ncluster4=df_data.loc[df_data['clusters'] == 6]\ncluster5=df_data.loc[df_data['clusters'] == 7]\n\n\nscatter1 = dict(\n    mode = \"markers\",\n    name = \"Cluster 1\",\n    type = \"scatter3d\",    \n    x = cluster1.to_numpy()[:,0], y = cluster1.to_numpy()[:,1], z = cluster1.to_numpy()[:,2],\n    marker = dict( size=2, color='green')\n)\nscatter2 = dict(\n    mode = \"markers\",\n    name = \"Cluster 2\",\n    type = \"scatter3d\",    \n    x = cluster2.to_numpy()[:,0], y = cluster2.to_numpy()[:,1], z = cluster2.to_numpy()[:,2],\n    marker = dict( size=2, color='blue')\n)\nscatter3 = dict(\n    mode = \"markers\",\n    name = \"Cluster 3\",\n    type = \"scatter3d\",    \n    x = cluster3.to_numpy()[:,0], y = cluster3.to_numpy()[:,1], z = cluster3.to_numpy()[:,2],\n    marker = dict( size=2, color='red')\n)\n\nscatter4 = dict(\n    mode = \"markers\",\n    name = \"Cluster 4\",\n    type = \"scatter3d\",    \n    x = cluster4.to_numpy()[:,0], y = cluster4.to_numpy()[:,1], z = cluster4.to_numpy()[:,2],\n    marker = dict( size=2, color='orange')\n)\n\nscatter5 = dict(\n    mode = \"markers\",\n    name = \"Cluster 5\",\n    type = \"scatter3d\",    \n    x = cluster5.to_numpy()[:,0], y = cluster5.to_numpy()[:,1], z = cluster5.to_numpy()[:,2],\n    marker = dict( size=2, color='yellow')\n)\n\n\n################## Clusters  ##############\n\ncluster1 = dict(\n    alphahull = 5,\n    name = \"Cluster 1\",\n    opacity = .1,\n    type = \"mesh3d\",    \n    x = cluster1.to_numpy()[:,0], y = cluster1.to_numpy()[:,1], z = cluster1.to_numpy()[:,2],\n    color='green', showscale = True\n)\ncluster2 = dict(\n    alphahull = 5,\n    name = \"Cluster 2\",\n    opacity = .1,\n    type = \"mesh3d\",    \n    x = cluster2.to_numpy()[:,0], y = cluster2.to_numpy()[:,1], z = cluster2.to_numpy()[:,2],\n    color='blue', showscale = True\n)\ncluster3 = dict(\n    alphahull = 5,\n    name = \"Cluster 3\",\n    opacity = .1,\n    type = \"mesh3d\",    \n    x = cluster3.to_numpy()[:,0], y = cluster3.to_numpy()[:,1], z = cluster3.to_numpy()[:,2],\n    color='red', showscale = True\n)\n\ncluster4 = dict(\n    alphahull = 5,\n    name = \"Cluster 4\",\n    opacity = .1,\n    type = \"mesh3d\",    \n    x = cluster4.to_numpy()[:,0], y = cluster4.to_numpy()[:,1], z = cluster4.to_numpy()[:,2],\n    color='orange', showscale = True\n)\n\ncluster5 = dict(\n    alphahull = 5,\n    name = \"Cluster 5\",\n    opacity = .1,\n    type = \"mesh3d\",    \n    x = cluster5.to_numpy()[:,0], y = cluster5.to_numpy()[:,1], z = cluster5.to_numpy()[:,2],\n    color='yellow', showscale = True\n)\n\nlayout = dict(\n    title = '3D visualisation of Clusters',\n    scene = dict(\n        xaxis = dict( zeroline=True ),\n        yaxis = dict( zeroline=True ),\n        zaxis = dict( zeroline=True ),\n    )\n)\nfig = dict( data=[scatter1, scatter2, scatter3, scatter4, scatter5, cluster1, cluster2, cluster3, cluster4, cluster5], layout=layout )\n# Use py.iplot() for IPython notebook\nplotly.offline.iplot(fig, filename='mesh3d_sample')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig  = plt.figure(figsize = (7200,30))\nfor c in data_model:\n    g   = sns.FacetGrid(data_model, col='Cluster')\n    g.map(plt.hist, c, color = \"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Profiling of Clusters\n### Cluster 0: Who do not purchase but have good credit limit. Also miss payments \n### Cluster 1: Who have a good balance, make average purchases and do make payments\n### Cluster 4: Who buy frequntly and have a high credit limit\n### Cluster 6: Who buy very small, keeps low balance but frequently pay dues\n### Cluster 7: Who buy in installments only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **Agglomerative Clustering(Hierarchical Clustering)**\nLet us also validate our results with some other types of clustering too. k-mean is Non-heirarchial but let us also try hierarchial clustering using AgglomerativeClustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  \ncl = cluster.fit_predict(data_model_std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, We will visualize our clusters and compare the results with k-means clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 7))\n\nplt.scatter(x,y, c=cluster.labels_)\nplt.title(\"Agglomerative Clustering with % Clusters\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the resulting clusters from Agglomerative Clustering doesn't show that good results. The three clusters on the right are separted but that is not the case with the left most clusters. The results match up with k-means but there is no improvement in the clusters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Comparison of between Non-Hierarchial and Hierarchial Clustering\n### Here we will compare both the type sof clusters to see which one gives better results. The way it is done is after getting the clusters, these will be profiled using Average and median of the variable and will be seen visually.\n### k-means (Non-Hierarchial)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"### Getting the required \ndata_k_means  = pd.concat([data_,data_model['Cluster']], axis=1)\ndata_k_means  = data_k_means.drop(['CUST_ID', 'Balance_decile', 'CC_utilisation','CC_util_decile'], axis =1)\n\n### Checking some basic stats/distributions from our clusters\ndata_freq     = data_k_means.filter(regex=\"FREQUENCY\")\ndata_amount   = data_k_means.drop(list(data_freq.columns), axis=1)\n\n#Finding Mean\ndata_amnt_m   = data_amount.groupby(['Cluster'],as_index=False).mean()\n\n#Finding Median\ndata_freq     = pd.concat([data_freq,data_model['Cluster']], axis=1)\ndata_freq_m   = data_freq.groupby(['Cluster'],as_index=False).median()\n\n#Join both of them\ndata_all_kmeans = pd.merge(data_amnt_m, data_freq_m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all_kmeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agglomerative Clustering (Non-Hierarchical)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting the Data\ncl            = pd.DataFrame(cl)\ncl.columns    = ['Cluster']\ndata_agg      = pd.concat([data_,cl], axis=1)\n\ndata_agg          = data_agg.drop(['CUST_ID', 'Balance_decile', 'CC_utilisation','CC_util_decile'], axis =1)\ndata_freq_agg     = data_agg.filter(regex=\"FREQUENCY\")\ndata_amount_agg   = data_agg.drop(list(data_freq_agg.columns), axis=1)\n\n#Finding Mean\ndata_amnt_agg_m   = data_amount_agg.groupby(['Cluster'],as_index=False).mean()\n\n#Finding Median\ndata_freq_agg         = pd.concat([data_freq_agg,cl], axis=1)\ndata_freq_agg_m       = data_freq_agg.groupby(['Cluster'],as_index=False).median()\n\n#Join both of them\ndata_all_agg = pd.merge(data_amnt_agg_m, data_freq_agg_m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all_agg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In order to plot them the data needs to be converted into a long format from a wider format.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Changing the Data Structure so that we could compare them visually\n## k-means\n\n## Amount \ndata_amnt_m_sub1     = data_amnt_m.drop(['TENURE','PRC_FULL_PAYMENT','CASH_ADVANCE_TRX','PURCHASES_TRX'], axis =1)\ncols  = list(data_amnt_m_sub1.columns)[1:9]\ndata_amnt_m_sub11   = pd.melt(data_amnt_m_sub1, id_vars = ['Cluster'],value_vars=cols ,var_name='cols')\n\n## Frequency\ncols1               = list(data_freq_m.columns)[1:5]\ndata_freq_m_sub11   = pd.melt(data_freq_m, id_vars = ['Cluster'],value_vars=cols1 ,var_name='cols')\n\n\n## Agglomerative\n\n## Amount \ndata_amnt_m_sub2     = data_amnt_agg_m.drop(['TENURE','PRC_FULL_PAYMENT','CASH_ADVANCE_TRX','PURCHASES_TRX'], axis =1)\ncols  = list(data_amnt_m_sub2.columns)[1:9]\ndata_amnt_m_sub22   = pd.melt(data_amnt_m_sub2, id_vars = ['Cluster'],value_vars=cols ,var_name='cols')\n\n## Frequency\ncols2               = list(data_freq_agg_m.columns)[1:5]\ndata_freq_m_sub22   = pd.melt(data_freq_agg_m, id_vars = ['Cluster'],value_vars=cols2 ,var_name='cols')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating catplots for both the graphs\nfig = plt.figure(figsize = (22,12))\n\n## k-means Clustering\n\nax1 = fig.add_subplot(221)\ng = sns.pointplot(x=\"cols\", y=\"value\", hue='Cluster', data=data_amnt_m_sub11, kind =\"point\", ax = ax1)\nplt.title(\"Clusters across avg of Amount Variables\", fontsize = 18)\nplt.xlabel(\"Variables related to Amount\", fontsize = 14)\nplt.ylabel(\"Average\", fontsize = 14)\ng.set_xticklabels(g.get_xticklabels(), rotation=60)\n\nax2 = fig.add_subplot(222)\ng = sns.pointplot(x=\"cols\", y=\"value\", hue='Cluster', data=data_freq_m_sub11, kind =\"point\", ax = ax2)\nplt.title(\"Clusters across median of Frequency Variables\", fontsize = 18)\nplt.xlabel(\"Variables related to Frequency\", fontsize = 14)\nplt.ylabel(\"Median\", fontsize = 14)\ng.set_xticklabels(g.get_xticklabels(), rotation=60)\n\n\n## Agglomerative Clustering\n\nax3 = fig.add_subplot(223)\ng = sns.pointplot(x=\"cols\", y=\"value\", hue='Cluster', data=data_amnt_m_sub22, kind =\"point\", ax = ax3)\nplt.title(\"Agglomerative Clustering\", fontsize = 18)\nplt.xlabel(\"Variables related to Amount\", fontsize = 14)\nplt.ylabel(\"Average\", fontsize = 14)\ng.set_xticklabels(g.get_xticklabels(), rotation=60)\n\nax4 = fig.add_subplot(224)\ng = sns.pointplot(x=\"cols\", y=\"value\", hue='Cluster', data=data_freq_m_sub22, kind =\"point\", ax = ax4)\nplt.title(\"Agglomerative Clustering\", fontsize = 18)\nplt.xlabel(\"Variables related to Frequency\", fontsize = 14)\nplt.ylabel(\"Median\", fontsize = 14)\ng.set_xticklabels(g.get_xticklabels(), rotation=60)\n\nfig.tight_layout() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion: We conclude that k-means and Agglomerative clustering almost gives similar results even when we see them visually or see them when we profile them. Both of them have their pros and cons:\n\n* k-means doesn't tell you the number of clusters. This has to be found by the user and then move ahead with the clusters\n* Agglomerative Clustering can be used with large sized data while k-means doesn't give accuracte results with large sized data","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}