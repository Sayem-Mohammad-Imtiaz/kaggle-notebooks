{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploration"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/dnd_monsters.csv')\ndf.info(verbose=True)\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessig / Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"Adopting the following strategy;\n\n* Drop all rows with missing values\n* Encode the \"size\" column - we can safely do this using standard label encoding as the sizes have a very clear order / are all related to one another."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert 'cr' column to float in seperate column\ndf['cr_float'] = df['cr']\ndf['cr_float'] = df['cr_float'].replace('1/8', 0.125)\ndf['cr_float'] = df['cr_float'].replace('1/4', 0.25)\ndf['cr_float'] = df['cr_float'].replace('1/2', 0.5)\ndf['cr_float'] = df['cr_float'].astype('float')\ndf[\"cr\"] = df[\"cr\"].astype('category')\n\n# Change the \"legendary\" column to a boolean 0/1\ndf[\"legendary\"] = df[\"legendary\"].fillna(0)\ndf[\"legendary\"] = df[\"legendary\"].replace('Legendary', 1)\n\n# Change the \"size\" column to an integer\nsizes = [\"Tiny\", \"Small\", \"Medium\", \"Large\", \"Huge\", \"Gargantuan\"]\ndf[\"size\"] = df[\"size\"].astype(pd.CategoricalDtype(sizes, ordered=True))\ndf[\"size\"] = df[\"size\"].cat.codes\n\n# These are the features we'll use to train \nfeatures = [\n    'str', 'dex', 'con', 'int', 'wis', 'cha', 'hp', 'ac', 'size', 'legendary'\n]\n\n# Drop any rows with missing features or that are missing the target value (cr)\ndf = df.dropna(subset=features + ['cr'])\ndf.info(verbose=True)\ndf[features].head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification approach\nIs this a classificaiton problem? Evidently not. The best accuracy I was able to achieve (without any hyperparameter tuning) was around 0.3 - so one in every 3-ish creatures were being given the wrong challenge rating.\n\nThe fact it was able to get that close is pretty good anyway, however I think this is almost definately more of a regression problem - if we get \"close\" to the challenge rating we want then that still counts as a win."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn import tree\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX = df[features]\ny = df[\"cr\"]\n\n# Train\nmodels = {\n    \"SVM\": svm.SVC(),\n    \"SGD\": SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=500),\n    \"Tree\": tree.DecisionTreeClassifier(),\n    \"RandomForest\": RandomForestClassifier(max_depth=2, random_state=0)\n}\n\ndef train_and_evaluate(model_type):\n    # Train\n    test_size = 0.3\n    seed = 42\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n    model = model_type.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = model.predict(X_test)    \n    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nfor key in models:\n    print(\"\\n\")\n    print(key)\n    print(\"==========\")\n    train_and_evaluate(models[key])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression approach\n\nSome ridiculously impressive scores for a few regression models.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import max_error\nfrom sklearn.model_selection import train_test_split\n\nX = df[features]\ny = df[\"cr_float\"]\n\n# Train\nmodels = {\n    \"SVC\": svm.SVR(),\n    \"SGD\": linear_model.SGDRegressor(),\n    \"Bayesian Ridge\": linear_model.BayesianRidge(),\n    \"Lasso\": linear_model.LassoLars(),\n    \"ARD\": linear_model.ARDRegression(),\n    \"PassAggr\": linear_model.PassiveAggressiveRegressor(),\n    \"Theil\": linear_model.TheilSenRegressor(),\n    \"Linear\": linear_model.LinearRegression()\n}\n\ndef train_and_evaluate(model_type):\n    # Train\n    test_size = 0.3\n    seed = 42\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n    model = model_type.fit(X_train, y_train)\n\n    # Evaluate\n    y_pred = model.predict(X_test)\n\n    print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n    print(\"Mean squared error:\", mean_squared_error(y_test, y_pred))\n    print(\"Explained Variance:\", explained_variance_score(y_test, y_pred))\n\nfor key in models:\n    print(\"\\n\")\n    print(key)\n    print(\"==========\")\n    train_and_evaluate(models[key])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}