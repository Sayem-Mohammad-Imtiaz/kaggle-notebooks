{"cells":[{"metadata":{"_uuid":"7d05944a90982275c61c0162b8b7dedf19e76c3e"},"cell_type":"markdown","source":"# Deep Clustering for Unsupervised Learning 0f Visual Features\nhttps://arxiv.org/pdf/1807.05520.pdf"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"from collections import Counter\nfrom pathlib import Path\n\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models import resnet18\nfrom torchvision import transforms as T\nfrom torchvision.utils import make_grid\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.neighbors import NearestNeighbors\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"210bb092f2f2017eb90652a628a4ab52edc734e7"},"cell_type":"markdown","source":"## Utils"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e12340cdb39826fb1c3d7e6ab5021fbcef214a82"},"cell_type":"code","source":"def show_cluster(cluster, labels, dataset, limit=32):\n    images = []\n    labels = np.array(labels)\n    indices = np.where(labels==cluster)[0]\n    \n    if not indices.size:\n        print(f'cluster: {cluster} is empty.')\n        return None\n    \n    for i in indices[:limit]:\n        image, _ = dataset[i]\n        images.append(image)\n        \n    gridded = make_grid(images)\n    plt.figure(figsize=(15, 10))\n    plt.title(f'cluster: {cluster}')\n    plt.imshow(gridded.permute(1, 2, 0))\n    plt.axis('off')\n    \n    \ndef show_neighbors(neighbors, dataset):\n    images = []\n    for n in neighbors:\n        images.append(dataset[n][0])\n\n    gridded = make_grid(images)\n    plt.figure(figsize=(15, 10))\n    plt.title(f'image and nearest neighbors')\n    plt.imshow(gridded.permute(1, 2, 0))\n    \n    \ndef extract_features(model, dataset, batch_size=32):\n    \"\"\"\n    Gets the output of a pytorch model given a dataset.\n    \"\"\"\n    loader = DataLoader(dataset, batch_size=batch_size)\n    features = []\n    for image, _ in tqdm(loader, desc='extracting features'):\n        output = model(Variable(image).cuda())\n        features.append(output.data.cpu())\n    return torch.cat(features).numpy() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a02f37287bffad0d0ed1f1e24c80d74fd18a30b3"},"cell_type":"markdown","source":"## Dataset and transforms"},{"metadata":{"trusted":true,"_uuid":"4c43b1920f834b1b924c724b831335918ed6b210","collapsed":true},"cell_type":"code","source":"class FoodDataset(Dataset):\n    def __init__(self, root, transforms=None, labels=[], limit=None):\n        self.root = Path(root)\n        self.image_paths = list(Path(root).glob('*/*.jpg'))\n        if limit:\n            self.image_paths = self.image_paths[:limit]\n        self.labels = labels\n        self.transforms = transforms\n        self.classes = set([path.parts[-2] for path in self.image_paths])\n        \n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        label = self.labels[index] if self.labels else 0\n        image = Image.open(image_path)\n        if self.transforms:\n            return self.transforms(image), label\n        return image, label\n            \n    def __len__(self):\n        return len(self.image_paths)    \n    \ntransforms = T.Compose([T.Resize(224),\n                        T.CenterCrop(224),\n                        T.ToTensor()])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4b8a131e69be1c196c910c9b9f09d6981e17810"},"cell_type":"markdown","source":"## Config"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6fe25dca2797838544b04c45e32e8b9e0a562212"},"cell_type":"code","source":"# data\nroot = '../input/images'\nlimit_images = 10000\n\n# clustering\npca_dim = 50\nkmeans_clusters = 100\n\n# convnet\nbatch_size = 64\nnum_classes = 100\nnum_epochs = 2\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e52b22d670ea18f6dcaddde8dedf8155275c0053"},"cell_type":"markdown","source":"## Data\nFood Dataset, 101 different foods, 1000 samples each.\n\nWe will use then first 10 classes to test this method."},{"metadata":{"trusted":true,"_uuid":"9bf2874f385bb8ec11e936acd18569e2ba283e0d"},"cell_type":"code","source":"dataset = FoodDataset(root=root, limit=limit_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e9e7998e8a885fcb4854da93ebf4fbd99cf7d9f"},"cell_type":"code","source":"dataset.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a87be606b70f0350c255caeff41896a99163acf"},"cell_type":"code","source":"image, _ = dataset[9000]\nimage","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d8f7b2b2b52c823f64eaace0839d45c62692617"},"cell_type":"markdown","source":"## Models"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b1c6eb4b829ab5e1eaba7101e67d6afb22b45bf1"},"cell_type":"code","source":"# load resnet and alter last layer\nmodel = resnet18()\nmodel.fc = nn.Linear(512, num_classes)\nmodel.cuda();\n\npca = IncrementalPCA(n_components=pca_dim, batch_size=512, whiten=True)\nkmeans = MiniBatchKMeans(n_clusters=kmeans_clusters, batch_size=512, init_size=3*kmeans_clusters)\noptimizer = Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6baddc2c5131447801a1b51781a420acfbaa9142"},"cell_type":"markdown","source":"# clustering loop"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"259ba1a788b17f42fbd2545cfaa420d20453fb0a"},"cell_type":"code","source":"def cluster(pca, kmeans, model, dataset, batch_size, return_features=False):\n    features = extract_features(model, dataset, batch_size)  \n    reduced = pca.fit_transform(features)\n    pseudo_labels = list(kmeans.fit_predict(reduced))\n    if return_features:\n        return pseudo_labels, features\n    return pseudo_labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8b6433ffc4d3dd66d9d459bee782aa21c303293"},"cell_type":"markdown","source":"## Training loop"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"959aa392073410874f33caa17d349251c1ecf4cb"},"cell_type":"code","source":"def train_epoch(model, optimizer, train_dataset, batch_size):\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    total_loss = 0\n    pbar = tqdm(train_loader)\n    for batch, (images, labels) in enumerate(pbar):\n        optimizer.zero_grad()\n        images = Variable(images).cuda()\n        labels = Variable(labels).cuda().long()\n        out = model(images)\n        loss = F.cross_entropy(out, labels)\n        total_loss += loss.data[0]\n        pbar.set_description(f'training - loss: {total_loss / (batch + 1)}')\n        loss.backward()\n        optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdc9478cc215a08f6ca4fb94ad187e2eafc60df2"},"cell_type":"markdown","source":"## Check how images are clustered with random convnet"},{"metadata":{"trusted":true,"_uuid":"36e24afd4c011e18b8b682ca7830ef86254c5f14","scrolled":true},"cell_type":"code","source":"raw_dataset = FoodDataset(root=root, transforms=transforms, limit=limit_images)\npseudo_labels, features = cluster(pca, kmeans, model, raw_dataset, batch_size, return_features=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6a4eaca7a7702c7545a77176818b81bd16a90a0"},"cell_type":"markdown","source":"### Cluster distributions"},{"metadata":{"trusted":true,"_uuid":"f10c0407c059d52b83901939c397a6f0bfd94100"},"cell_type":"code","source":"plt.hist(pseudo_labels, bins=kmeans_clusters)\nplt.title('cluster membership counts');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"518ce592c8323968ec37ba9393b0197c70074a2b"},"cell_type":"markdown","source":"### largest clusters"},{"metadata":{"trusted":true,"_uuid":"326478697ef70abf20e96efce78e23bbdf3371c3"},"cell_type":"code","source":"raw_dataset.classes ## all food types we have sampled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2dee5e310862973bb1bb7eb0fdefa2caf3bd6cf"},"cell_type":"code","source":"counts = Counter(pseudo_labels)\nshow_cluster(counts.most_common()[0][0], pseudo_labels, raw_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2be7b20c6f84badedc74749cc8fa21fa31c15b12"},"cell_type":"code","source":"show_cluster(counts.most_common()[1][0], pseudo_labels, raw_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eb95fb61486bf57f7e60f1f01309437177fc3a8"},"cell_type":"markdown","source":"## image retrieval on with random model"},{"metadata":{"trusted":true,"_uuid":"4b3dcfa87440cc23f30db11857d5884b369eee86"},"cell_type":"code","source":"knn = NearestNeighbors(metric='cosine')\nknn.fit(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a313247567f61edf559646a4d78fd0d876a8147c"},"cell_type":"code","source":"anchor_image = 0\nneighbors = knn.kneighbors([features[anchor_image]], n_neighbors=4, return_distance=False)[0]\nshow_neighbors(neighbors, raw_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1215f55c97b6976f3188929b985562bbe6fae93"},"cell_type":"markdown","source":"## Full Cycle"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"279b5377ffece1f1eff1e6b9902e5a526309f5f1"},"cell_type":"code","source":"for i in range(num_epochs):\n    pseudo_labels = cluster(pca, kmeans, model, raw_dataset, batch_size) # generate labels\n    labeled_dataset = FoodDataset(root=root, labels=pseudo_labels, transforms=transforms, limit=limit_images) # make new dataset with labels matched to images\n    train_epoch(model, optimizer, labeled_dataset, batch_size) # train for one epoch","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9aebe374841c65db00e741b4250ac1c98b16049d"},"cell_type":"markdown","source":"## Check new clusters"},{"metadata":{"trusted":true,"_uuid":"ebcc4fb4cf87f4ac02fa9146e192ebd4fcb35280","collapsed":true},"cell_type":"code","source":"pseudo_labels, features = cluster(pca, kmeans, model, raw_dataset, batch_size, return_features=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"536b127738073719fd3b5ff7cc0348e437b8290b","collapsed":true},"cell_type":"code","source":"plt.hist(pseudo_labels, bins=kmeans_clusters)\nplt.title('cluster membership counts');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e09f8f65e615db195b2404915fcdbe32114823c6","collapsed":true},"cell_type":"code","source":"counts = Counter(pseudo_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af8f35a53bd0747ba75168372d5b12a4b0c1e40c","collapsed":true},"cell_type":"code","source":"show_cluster(counts.most_common()[0][0], pseudo_labels, raw_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc6bc7aded4b1c0acb8d3d8181713d40b2b7acae","collapsed":true},"cell_type":"code","source":"show_cluster(counts.most_common()[1][0], pseudo_labels, raw_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd0f6232696e6050b2f0ab13d975e1499c3c0670"},"cell_type":"markdown","source":"## Image retrieval"},{"metadata":{"trusted":true,"_uuid":"515949b5e5326997885021847cfca129e2a43bdf","collapsed":true},"cell_type":"code","source":"knn = NearestNeighbors(metric='cosine')\nknn.fit(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4df9084b5923d6c2808bd6f9bfd125f67141253","collapsed":true},"cell_type":"code","source":"anchor_image = 0\nneighbors = knn.kneighbors([features[anchor_image]], n_neighbors=4, return_distance=False)[0]\nshow_neighbors(neighbors, raw_dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e3d622e76751c569fa80b463349a810e27c82ba"},"cell_type":"markdown","source":"## Train some more"},{"metadata":{"trusted":true,"_uuid":"da4d377f9ef61eabe6a0cdcc0fc998a5ba6e2e2f","collapsed":true},"cell_type":"code","source":"for i in range(4):\n    pseudo_labels = cluster(pca, kmeans, model, raw_dataset, batch_size) # generate labels\n    labeled_dataset = FoodDataset(root=root, labels=pseudo_labels, transforms=transforms, limit=limit_images) # make new dataset with labels matched to images\n    train_epoch(model, optimizer, labeled_dataset, batch_size) # train for one epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5cec45df70342314cbb52fd8227d68b02b4d3856"},"cell_type":"code","source":"features = extract_features(model, raw_dataset, batch_size)  \nknn = NearestNeighbors(metric='cosine')\nknn.fit(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fa61418596d43eacbcb644132d7fe8340e651051"},"cell_type":"code","source":"anchor_image = 0\nneighbors = knn.kneighbors([features[anchor_image]], n_neighbors=4, return_distance=False)[0]\nshow_neighbors(neighbors, raw_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5fbbd0296eb189e52f208095b9b8c7e2a4410520"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}