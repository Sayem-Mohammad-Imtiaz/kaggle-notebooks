{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n# Porter stemmer\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n# Snowball stemmer\nfrom nltk.stem import SnowballStemmer\nsnowball = SnowballStemmer('english')\n# Wordnet lemmatizer\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore textual data","metadata":{}},{"cell_type":"code","source":"tweets_data = \"../input/disaster-tweets/tweets.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read data\ntweets = pd.read_csv(tweets_data)\ntweets.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean up Data ","metadata":{}},{"cell_type":"code","source":"def preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply the preprocess function to all tweets\ntweets['text'] = tweets['text'].apply(preprocessor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tweets['text']\ny = tweets['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorization","metadata":{}},{"cell_type":"markdown","source":"## Count Vectorizer","metadata":{}},{"cell_type":"code","source":"bag_of_words_vectorizer = CountVectorizer(min_df=5)\nbow_vectors = bag_of_words_vectorizer.fit_transform(tweets['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bag_of_words_vectorizer.vocabulary_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For bag of words\npca = PCA(n_components=2)\nx_pca = pca.fit_transform(bow_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=tweets['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF Vectorizer","metadata":{}},{"cell_type":"code","source":"TFIDF_vectorizer = TfidfVectorizer(min_df=5)\ntfidf_vectors = TFIDF_vectorizer.fit_transform(tweets['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TFIDF_vectorizer.get_feature_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For TFIDF vectors\npca_tfidf = PCA(n_components=2)\nx_pca = pca_tfidf.fit_transform(tfidf_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=tweets['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline (TFIDF + LR)","metadata":{}},{"cell_type":"code","source":"def tokenizer(text):\n    return text.split()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\ndef tokenizer_snowball(text):\n    return [snowball.stem(word) for word in text.split()]\n\ndef tokenizer_wordnet_lemmatizer(text):\n    return [lemmatizer.lemmatize(word) for word in text.split()]\n\nTFIDF_vectorizer = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n    {\n        'vect__ngram_range': [(1, 2)],\n        'vect__stop_words': [stop, None],\n        'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_wordnet_lemmatizer],\n        'clf__penalty': ['l1', 'l2'],\n        'clf__C': [1.0, 10.0, 100.0]        \n    },\n    {\n        'vect__ngram_range': [(1, 2)],\n        'vect__stop_words': [stop, None],\n        'vect__tokenizer': [tokenizer, tokenizer_porter, tokenizer_snowball, tokenizer_wordnet_lemmatizer],\n        'vect__use_idf': [False],\n        'vect__norm': [None],\n        'clf__penalty': ['l1', 'l2'],\n        'clf__C': [1.0, 10.0, 100.0]\n    }\n]\n\nlr_tfidf = Pipeline([('vect', TFIDF_vectorizer),\n                     ('clf', LogisticRegression(random_state=0))])\n\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy',\n                           cv=5, verbose=1, n_jobs=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_lr_tfidf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best parameters\ngs_lr_tfidf.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best score\ngs_lr_tfidf.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine the score of the best model on the test set (We use here TFIDF vectorizer + LogisticRegression)\nclf = gs_lr_tfidf.best_estimator_\nclf.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the Pipeline","metadata":{}},{"cell_type":"code","source":"print(tweets['text'][0])\nprint(clf.predict([preprocessor(tweets['text'][0])]))\nprint('True target: ', tweets['target'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}