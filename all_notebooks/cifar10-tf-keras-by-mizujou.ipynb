{"cells":[{"metadata":{},"cell_type":"markdown","source":"The goal of this notebook is to present simple approaches for classifying images.\n\nI will start with a very simple CNN and make it deeper, and more complex as I move forward. Feel free to leave some comments as I am still a student in ML and I could be doing things with mistakes.\n\n---\n\n# 1. Import Librairies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Basics\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nimport IPython\nimport random\nfrom random import getrandbits\nimport os\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras import callbacks\n\nfrom sklearn.model_selection import train_test_split\n\n# Metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# Graphs\nimport matplotlib.pyplot as plt\n\nprint(\"Numpy: \" + np.__version__)\nprint(\"Tensorflow: \" + tf.__version__)\nprint(\"Keras: \" + keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 2. Loading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n\n# Normalize pixel values to be between 0 and 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verify Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    # The CIFAR labels happen to be arrays, \n    # which is why you need the extra index\n    plt.xlabel(class_names[train_labels[i][0]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 3. Re-Create a Basic CNN from TensorFlow\n\nThis first, and simplest model, come from an architecture found of the website of tensorflow. You can find the model, [and the notebook here](https://www.tensorflow.org/tutorials/images/cnn)."},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = keras.Sequential([\n    # First Convolutional Block\n    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n    layers.MaxPool2D((2, 2)),\n\n    # Second Convolutional Block\n    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n    layers.MaxPool2D((2, 2)),\n\n    # Third Convolutional Block\n    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n\n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(10)\n])\n\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\nhistory1 = model1.fit(\n    train_images, train_labels, \n    epochs=10, \n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 4. Evaluate Model 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history1.history['accuracy'], label='accuracy')\nplt.plot(history1.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 5. Create a CNN from Kaggle Course\n\nIn this second model, I make the CNN a bit wilder and deeper. I also took the architecture from a tutorial, this one comes from the Kaggle Course, [the notebook can be found here](https://www.kaggle.com/ryanholbrook/custom-convnets)."},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = keras.Sequential([\n    # First Convolutional Block\n    layers.Conv2D(32, kernel_size=5, activation=\"relu\", padding='same',\n                  # give the input dimensions in the first layer\n                  # [height, width, color channels(RGB)]\n                  input_shape=[32, 32, 3]),\n    layers.MaxPool2D(),\n\n    # Second Convolutional Block\n    layers.Conv2D(64, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n\n    # Third Convolutional Block\n    layers.Conv2D(128, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n\n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(10)\n])\n\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\nhistory2 = model2.fit(\n    train_images, train_labels, \n    epochs=10, \n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 6. Evaluate Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history2.history['accuracy'], label='accuracy')\nplt.plot(history2.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First Conclusion\n * We can see that the second model is overfitting. The validation accuracy isn't following the train accuracy.\n * First, create One Version with DropOut and BatchNormalization Layers.\n * Then, create One Version with Augmentation Techniques.\n \n ---\n \n # 7. Create a Model with DropOut Technics\n \n In this model, I added DropOut and BatchNormalization Layers. The Kaggle Course introducting them [can be found here](https://www.kaggle.com/ryanholbrook/dropout-and-batch-normalization)."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model3 = keras.Sequential([\n    # First Convolutional Block\n    layers.Conv2D(32, kernel_size=5, activation=\"relu\", padding='same',\n                  # give the input dimensions in the first layer\n                  # [height, width, color channels(RGB)]\n                  input_shape=[32, 32, 3]),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Second Convolutional Block\n    layers.Conv2D(64, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Third Convolutional Block\n    layers.Conv2D(128, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(10)\n])\n\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model3.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\nhistory3 = model3.fit(\n    train_images, train_labels, \n    epochs=10, \n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 8. Evaluate Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history3.history['accuracy'], label='accuracy')\nplt.plot(history3.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 9. Create a Model with Augmentation Technics\n\nFor this model, I tried to implement Augmentation Layers. I used two notebooks to learn how to implement them:\n* [The notebook from TensorFlow](https://www.tensorflow.org/tutorials/images/data_augmentation)\n* [The notebook from Kaggle's Course](https://www.kaggle.com/ryanholbrook/data-augmentation)"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model4 = keras.Sequential([\n    # First Convolutional Block\n    layers.InputLayer(input_shape=(32, 32, 3)),\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    layers.experimental.preprocessing.RandomRotation(0.2),\n    layers.Conv2D(32, kernel_size=5, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Second Convolutional Block\n    layers.Conv2D(64, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Third Convolutional Block\n    layers.Conv2D(128, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(10)\n])\n\nmodel4.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\nhistory4 = model4.fit(\n    train_images, train_labels, \n    epochs=10, \n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 10. Evaluate Model 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history4.history['accuracy'], label='accuracy')\nplt.plot(history4.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Second Conclusion\n\n* The Drop Layer made the Model generalize better and not overfitting anymore but we probably need to let him train on more epochs.\n* We lost accuracy while implementing and using Augmentation Technics.\n\n---\n\n# 11. Creation of a Model with more epochs\n\nHere, I did some little modifications to my previous model, and added more epochs."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model5 = keras.Sequential([\n    # First Convolutional Block\n    layers.Conv2D(32, kernel_size=5, activation=\"relu\", padding='same',\n                  # give the input dimensions in the first layer\n                  # [height, width, color channels(RGB)]\n                  input_shape=[32, 32, 3]),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Second Convolutional Block\n    layers.Conv2D(64, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Third Convolutional Block\n    layers.Conv2D(128, kernel_size=3, activation=\"relu\", padding='same'),\n    layers.MaxPool2D(),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    \n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(10)\n])\n\nmodel5.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model5.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\nhistory5 = model5.fit(\n    train_images, train_labels, \n    epochs=50, \n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n# 12. Evaluate Model 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history5.history['accuracy'], label='accuracy')\nplt.plot(history5.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Third Conclusion\n\n* We successfully improved the accuracy of our Model.\n* Taking off the DropOut and BatchNormalization in the head of the Model also helped: 71% -> 75% in 10 epochs\n* Adding more epochs also helped: 10 -> 50\n* Now, let's move on to an other technic: **Transfer Learning**.\n\n---\n\n# 13. Creation of a Model using a Pre-trained Model\n\nFor this model, I tried, for the first time to use a Transfer Learning Technic. For that I used two notebooks:\n* [The notebook from TensorFlow](https://www.tensorflow.org/tutorials/images/transfer_learning)\n* [The notebook from Kaggle](https://www.kaggle.com/ryanholbrook/data-augmentation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = tf.keras.applications.MobileNetV2(\n    input_shape=(32, 32, 3),\n    include_top=False,\n    weights='imagenet'\n)\n\nbase_model.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"base_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model6 = keras.Sequential([\n    layers.InputLayer(input_shape=(32, 32, 3)),\n    \n    # Preprocessing\n    #preprocess_input,\n    \n    # Base Model: Efficient Net\n    base_model,\n    \n    # Classifier Head\n    layers.Flatten(),\n    layers.Dense(64, activation=\"relu\"),\n    layers.Dense(10)\n])\n\nmodel6.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model6.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\nhistory6 = model6.fit(\n    train_images, train_labels, \n    epochs=50, \n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fourth Conclusion\n\n* Our first attempt on using Tranfer Learning was a failure. It is probably due to the small size of our images, while most Big Models to use with transfer learning are trained on bigger images.\n\n---\n\n# 14. Performances on the Test Set\n\nTo check the performance on the test set, we are going to reuse our best model which is the model 5\n\n## A. Performances on the Training Set\n\nplt.xlabel(class_names[train_labels[i][0]])"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ntrain_pred = model5.predict(train_images)\n#train_pred = train_pred.reshape(-1)\ntrain_pred.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B. Evaluate the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model5.evaluate(test_images, test_labels, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## C. Performances on the Test Set"},{"metadata":{},"cell_type":"markdown","source":"This last part for plotting a confusion matrix is a failure for now since the output of the CNN isn't the same as the labels. I need to do some research about that."},{"metadata":{},"cell_type":"markdown","source":"# Predict\ntest_pred = model5.predict(test_images)\ntest_pred = test_pred.reshape(-1)\n\n# Confusion Matrix\ncm_test = confusion_matrix(test_labels, test_pred, normalize='true')\n\n# Graph\nplt.figure(figsize=(12, 7))\nplt.title('Accuracy Score on the Test Set: ' + str(accuracy_score(test_labels, test_pred).round(4)), size=25)\nsns.heatmap(cm_test, annot=True, fmt='.2%', cmap='Blues')\nplt.xlabel('Predicted Values', size=20)\nplt.ylabel('True Values', size=20)\nplt.show()\n\n# To be Continued..."},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}