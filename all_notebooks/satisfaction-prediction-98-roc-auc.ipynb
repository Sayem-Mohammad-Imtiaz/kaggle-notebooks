{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import the relevant libraries and create a random state variable to be used all across the notebook:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline\nrstate = 42 #establish a fixed random state","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.107226Z","iopub.execute_input":"2021-07-09T13:41:02.107695Z","iopub.status.idle":"2021-07-09T13:41:02.112878Z","shell.execute_reply.started":"2021-07-09T13:41:02.107664Z","shell.execute_reply":"2021-07-09T13:41:02.112127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring the data","metadata":{}},{"cell_type":"markdown","source":"Load the data from csv to dataframes and check out some basic properties:","metadata":{}},{"cell_type":"code","source":"directory = \"/kaggle/input/airline-passenger-satisfaction/\"\nfeature_tables = ['train.csv', 'test.csv']\ndf_train_str = directory + feature_tables[0]\ndf_test_str = directory + feature_tables[1]\ndf_train = pd.read_csv(df_train_str)\ndf_test = pd.read_csv(df_test_str)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.171117Z","iopub.execute_input":"2021-07-09T13:41:02.171441Z","iopub.status.idle":"2021-07-09T13:41:02.684195Z","shell.execute_reply.started":"2021-07-09T13:41:02.171414Z","shell.execute_reply":"2021-07-09T13:41:02.683242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sample(5, random_state=rstate)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.685714Z","iopub.execute_input":"2021-07-09T13:41:02.686011Z","iopub.status.idle":"2021-07-09T13:41:02.721734Z","shell.execute_reply.started":"2021-07-09T13:41:02.685983Z","shell.execute_reply":"2021-07-09T13:41:02.721115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape #amount of records is rather high, sampling will probably be required to avoid overloadign processor","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.72292Z","iopub.execute_input":"2021-07-09T13:41:02.723282Z","iopub.status.idle":"2021-07-09T13:41:02.727594Z","shell.execute_reply.started":"2021-07-09T13:41:02.723257Z","shell.execute_reply":"2021-07-09T13:41:02.726907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.728838Z","iopub.execute_input":"2021-07-09T13:41:02.729076Z","iopub.status.idle":"2021-07-09T13:41:02.806046Z","shell.execute_reply.started":"2021-07-09T13:41:02.729055Z","shell.execute_reply":"2021-07-09T13:41:02.805078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data is almost perfect in terms of readiness for analysis. The only nulls are in \"Arrival Delay in Minutes\" and since the amount is insignificant (300/100K), these records can be easily removed. Also we can see that there are some dtypes that are objects due to the fact these are categorical variables. They will need to be converted later on to dummies \\ indicators.","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.807252Z","iopub.execute_input":"2021-07-09T13:41:02.807518Z","iopub.status.idle":"2021-07-09T13:41:02.869478Z","shell.execute_reply.started":"2021-07-09T13:41:02.80749Z","shell.execute_reply":"2021-07-09T13:41:02.868511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same applies for the test data:","metadata":{}},{"cell_type":"code","source":"df_test.info()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.870578Z","iopub.execute_input":"2021-07-09T13:41:02.870856Z","iopub.status.idle":"2021-07-09T13:41:02.900211Z","shell.execute_reply.started":"2021-07-09T13:41:02.870824Z","shell.execute_reply":"2021-07-09T13:41:02.899268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the mean in all survey categories is between 2-3.5 and std is 1-1.5. Hence, the categories are rather balanced. The flight distance has a very high variance so it should be treated carefully. Age mean\\median are around 40 so no exceptions here as well. \"id\" & \"Unnamed...\" have little value as information.","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:02.901235Z","iopub.execute_input":"2021-07-09T13:41:02.901492Z","iopub.status.idle":"2021-07-09T13:41:03.038837Z","shell.execute_reply.started":"2021-07-09T13:41:02.901466Z","shell.execute_reply":"2021-07-09T13:41:03.037902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we plot the initial correlation matrix (before data is cleaned), we immediately notice a pattern of 3 correlated chunks with 4 categories each:\n1. Comfort related categories: seat comfort, food and drink etc\n2. Flight order related categories: wifi, ease of inline booking etc\n3. Service related categories: leg room, on board etc\n\nAlso, seems like departure \\ arrival delay are heavily correlated.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nsns.heatmap(df_train.corr(),annot=True,cmap='YlGnBu')\nplt.tight_layout","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:03.04125Z","iopub.execute_input":"2021-07-09T13:41:03.041544Z","iopub.status.idle":"2021-07-09T13:41:05.184187Z","shell.execute_reply.started":"2021-07-09T13:41:03.041515Z","shell.execute_reply":"2021-07-09T13:41:05.183285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are about to use the data for supervised learning tasks eventually, it is important to see if the labels are balance. If not, an adjustment in the training process is required such that bias is minimized as a result of uneven labeling. As can be seen, the labeled train set is balanced so no need for further actions.","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='satisfaction',data=df_train)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.185829Z","iopub.execute_input":"2021-07-09T13:41:05.186137Z","iopub.status.idle":"2021-07-09T13:41:05.411919Z","shell.execute_reply.started":"2021-07-09T13:41:05.18611Z","shell.execute_reply":"2021-07-09T13:41:05.411255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order for the labels to be used in the training, we need to convert them to binary. Satisfied will be assigned with 1 and all others will take zeros:","metadata":{}},{"cell_type":"code","source":"d = {'satisfied': True, 'neutral or dissatisfied': False} #create a dictionary to use map on\ndf_train['sat_label'] = df_train['satisfaction'].map(d) #map the values according to the dictionary to a new column\ndf_train.drop('satisfaction',inplace=True,axis=1) #erase old column\ndf_train[\"sat_label\"] = df_train[\"sat_label\"].astype(int) #convert to int to use later in models","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.412809Z","iopub.execute_input":"2021-07-09T13:41:05.413144Z","iopub.status.idle":"2021-07-09T13:41:05.445104Z","shell.execute_reply.started":"2021-07-09T13:41:05.413119Z","shell.execute_reply":"2021-07-09T13:41:05.444146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert the same way for the testset\ndf_test['sat_label'] = df_test['satisfaction'].map(d)\ndf_test.drop('satisfaction',inplace=True,axis=1)\ndf_test[\"sat_label\"] = df_test[\"sat_label\"].astype(int)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.446148Z","iopub.execute_input":"2021-07-09T13:41:05.446465Z","iopub.status.idle":"2021-07-09T13:41:05.460523Z","shell.execute_reply.started":"2021-07-09T13:41:05.446414Z","shell.execute_reply":"2021-07-09T13:41:05.459513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make sure the change was applied:","metadata":{}},{"cell_type":"code","source":"df_train['sat_label'].value_counts()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.462069Z","iopub.execute_input":"2021-07-09T13:41:05.462549Z","iopub.status.idle":"2021-07-09T13:41:05.471838Z","shell.execute_reply.started":"2021-07-09T13:41:05.462513Z","shell.execute_reply":"2021-07-09T13:41:05.470799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['sat_label'].value_counts()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.473058Z","iopub.execute_input":"2021-07-09T13:41:05.473347Z","iopub.status.idle":"2021-07-09T13:41:05.480556Z","shell.execute_reply.started":"2021-07-09T13:41:05.473318Z","shell.execute_reply":"2021-07-09T13:41:05.479675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Display the correlation differently, highlighting top correlated features with the satisfaction label. Easy to see that on-line boarding takes the lead with 50% while there's another ~10 features that are 20-40% correlated. This is not a bad start. If a single features has that much measured correlation, prediction will probably be very solid.","metadata":{}},{"cell_type":"code","source":"df_train.corr()['sat_label'].sort_values().drop('sat_label').plot(kind='bar')","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.48178Z","iopub.execute_input":"2021-07-09T13:41:05.482072Z","iopub.status.idle":"2021-07-09T13:41:05.898218Z","shell.execute_reply.started":"2021-07-09T13:41:05.482035Z","shell.execute_reply":"2021-07-09T13:41:05.897587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Further inspecting the strongest predictor we find out nothing out of the ordinary. 4 is the leading score which is quite high and this means that the average satisfied customer ranked \"online boarding\" close to 5.","metadata":{}},{"cell_type":"code","source":"df_train['Online boarding'].sort_values().value_counts()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.899294Z","iopub.execute_input":"2021-07-09T13:41:05.899542Z","iopub.status.idle":"2021-07-09T13:41:05.912627Z","shell.execute_reply.started":"2021-07-09T13:41:05.899519Z","shell.execute_reply":"2021-07-09T13:41:05.911896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here it is seen very clearly in a box plot. The satisfied customers (sat_label = 1) score this parameter between 4-5 (q1-3) while the other are settled in the range of 2 to 3.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='sat_label',y = 'Online boarding',data=df_train)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:05.913702Z","iopub.execute_input":"2021-07-09T13:41:05.914006Z","iopub.status.idle":"2021-07-09T13:41:06.073011Z","shell.execute_reply.started":"2021-07-09T13:41:05.91398Z","shell.execute_reply":"2021-07-09T13:41:06.072182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre processing","metadata":{}},{"cell_type":"markdown","source":"First, all the categorical variables need to be transformed to indicator variables. Most of them have only 2 types but class has 3 and therefore 2 columns are created in order to encode it. This is essential for the correlation to be calculated at first and for the trainning to be done later (in case the algorithms that will be used cannot use categorical features).","metadata":{}},{"cell_type":"code","source":"Gender_cat = pd.get_dummies(df_train['Gender'],drop_first=True)\nCustomer_cat = pd.get_dummies(df_train['Customer Type'],drop_first=True)\nTravel_cat = pd.get_dummies(df_train['Type of Travel'],drop_first=True)\nClass_cat = pd.get_dummies(df_train['Class'],drop_first=True)\ndf_train = pd.concat([df_train,Gender_cat,Customer_cat,Travel_cat,Class_cat],axis =1) # add all the newly created columns to the existing dataframe\ndf_train.drop(['Gender','Customer Type','Type of Travel','Class'],inplace =True,axis = 1) #erase the old categorical columns","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:06.074702Z","iopub.execute_input":"2021-07-09T13:41:06.075118Z","iopub.status.idle":"2021-07-09T13:41:06.166032Z","shell.execute_reply.started":"2021-07-09T13:41:06.075077Z","shell.execute_reply":"2021-07-09T13:41:06.16499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same thing is done for the testset:","metadata":{}},{"cell_type":"code","source":"Gender_cat = pd.get_dummies(df_test['Gender'],drop_first=True)\nCustomer_cat = pd.get_dummies(df_test['Customer Type'],drop_first=True)\nTravel_cat = pd.get_dummies(df_test['Type of Travel'],drop_first=True)\nClass_cat = pd.get_dummies(df_test['Class'],drop_first=True)\ndf_test = pd.concat([df_test,Gender_cat,Customer_cat,Travel_cat,Class_cat],axis =1)\ndf_test.drop(['Gender','Customer Type','Type of Travel','Class'],inplace =True,axis = 1)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:06.167367Z","iopub.execute_input":"2021-07-09T13:41:06.167626Z","iopub.status.idle":"2021-07-09T13:41:06.197678Z","shell.execute_reply.started":"2021-07-09T13:41:06.167602Z","shell.execute_reply":"2021-07-09T13:41:06.196811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sample the data to make sure it is OK:","metadata":{}},{"cell_type":"code","source":"df_train.sample(random_state=rstate)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:06.19893Z","iopub.execute_input":"2021-07-09T13:41:06.199208Z","iopub.status.idle":"2021-07-09T13:41:06.221672Z","shell.execute_reply.started":"2021-07-09T13:41:06.199181Z","shell.execute_reply":"2021-07-09T13:41:06.220998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.sample(random_state=rstate)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:06.222792Z","iopub.execute_input":"2021-07-09T13:41:06.223222Z","iopub.status.idle":"2021-07-09T13:41:06.246272Z","shell.execute_reply.started":"2021-07-09T13:41:06.223176Z","shell.execute_reply":"2021-07-09T13:41:06.245273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can plot a more comprehensive matrix that will take into consideration additional variables. Neverthelss, the strongest patterns was already seen before - 3 subgroups of survey categories.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(26,20))\nsns.heatmap(df_train.corr(),annot = True,cmap='YlGnBu')","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:06.247638Z","iopub.execute_input":"2021-07-09T13:41:06.247984Z","iopub.status.idle":"2021-07-09T13:41:09.82553Z","shell.execute_reply.started":"2021-07-09T13:41:06.247953Z","shell.execute_reply":"2021-07-09T13:41:09.824626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we check correlation against the label once more we see more relationships (especially negative ones) with type of travel and class:","metadata":{}},{"cell_type":"code","source":"df_train.corr()['sat_label'].sort_values().drop('sat_label').plot(kind='bar', color='maroon')","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:09.826796Z","iopub.execute_input":"2021-07-09T13:41:09.827089Z","iopub.status.idle":"2021-07-09T13:41:10.303225Z","shell.execute_reply.started":"2021-07-09T13:41:09.827059Z","shell.execute_reply":"2021-07-09T13:41:10.302284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since flight distance was marked before as a high variance feature, let's plot a histogram to visualize the exact distribution:","metadata":{}},{"cell_type":"code","source":"plt.hist(df_train['Flight Distance'])","metadata":{"pycharm":{"is_executing":true},"scrolled":true,"execution":{"iopub.status.busy":"2021-07-09T13:41:10.307144Z","iopub.execute_input":"2021-07-09T13:41:10.307447Z","iopub.status.idle":"2021-07-09T13:41:10.461339Z","shell.execute_reply.started":"2021-07-09T13:41:10.30742Z","shell.execute_reply":"2021-07-09T13:41:10.460699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a very long thick tail indeed. Though this feature is correlated relatively strongly with the label, it might be difficult to process for some of the algorithms. Also, we noticed before that arrival and departure delay is very heavly correlated which means there's little sense in using them both as feature. It can be seen how strong is the relationship between them (not surprising - once there's a delay in departure, assuming flight time is approx the same, the delay in arrival will be super close).","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x='Departure Delay in Minutes',y='Arrival Delay in Minutes',data=df_train)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:10.462866Z","iopub.execute_input":"2021-07-09T13:41:10.463253Z","iopub.status.idle":"2021-07-09T13:41:21.945472Z","shell.execute_reply.started":"2021-07-09T13:41:10.463209Z","shell.execute_reply":"2021-07-09T13:41:21.944544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop irrelevant columns (as discussed above) and NAN records:","metadata":{}},{"cell_type":"code","source":"df_train.drop(['Unnamed: 0','id', 'Arrival Delay in Minutes'],axis=1,inplace=True)\ndf_train.dropna(axis=0,inplace=True)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:21.946639Z","iopub.execute_input":"2021-07-09T13:41:21.946934Z","iopub.status.idle":"2021-07-09T13:41:21.971371Z","shell.execute_reply.started":"2021-07-09T13:41:21.946907Z","shell.execute_reply":"2021-07-09T13:41:21.970606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same for the testset:","metadata":{}},{"cell_type":"code","source":"df_test.drop(['Unnamed: 0','id', 'Arrival Delay in Minutes'],axis=1,inplace=True)\ndf_test.dropna(axis=0,inplace=True)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:21.972397Z","iopub.execute_input":"2021-07-09T13:41:21.972651Z","iopub.status.idle":"2021-07-09T13:41:21.984208Z","shell.execute_reply.started":"2021-07-09T13:41:21.972626Z","shell.execute_reply":"2021-07-09T13:41:21.983185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After trying out different aggregations (groupings) of age (I've erased them since the notebook is long enough without them), we see that correlation is not increased so we leave it as is. Now the data is ready for analysis and contains no nulls:","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:21.985448Z","iopub.execute_input":"2021-07-09T13:41:21.985769Z","iopub.status.idle":"2021-07-09T13:41:21.998035Z","shell.execute_reply.started":"2021-07-09T13:41:21.985743Z","shell.execute_reply":"2021-07-09T13:41:21.997001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After recognizing the 3 subgroups as detailed above, what we will do is aggregate the relevant features into 3 groups (with a minor overlap) and take the value of the subgroup as the mean of its components. This should reduce the noise within each group of correlated features and allow a better fit using the different algorithms. Age, flight distance and arrival delay have also been removed to narrow dimensions since they are not expected to generate much value.","metadata":{}},{"cell_type":"code","source":"df_train_grouped = df_train.copy() #copy to aviod deletion on original memory\ndf_train_grouped['Order'] = df_train[['Inflight wifi service','Departure/Arrival time convenient','Ease of Online booking', 'Gate location']].mean(axis=1)\ndf_train_grouped['Comfort'] = df_train[['Food and drink','Online boarding','Seat comfort', 'Inflight entertainment']].mean(axis=1)\ndf_train_grouped['Service'] = df_train[['Inflight entertainment','On-board service','Leg room service', 'Baggage handling']].mean(axis=1)\ndf_train_grouped.drop(['Inflight wifi service','Departure/Arrival time convenient','Ease of Online booking', 'Gate location', 'Food and drink','Online boarding','Seat comfort', 'Inflight entertainment', 'On-board service','Leg room service', 'Baggage handling', 'Age', 'Flight Distance', 'Departure Delay in Minutes'],axis=1,inplace=True)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:21.999245Z","iopub.execute_input":"2021-07-09T13:41:21.999511Z","iopub.status.idle":"2021-07-09T13:41:22.023753Z","shell.execute_reply.started":"2021-07-09T13:41:21.999485Z","shell.execute_reply":"2021-07-09T13:41:22.022875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same for testset:","metadata":{}},{"cell_type":"code","source":"df_test_grouped = df_test.copy()\ndf_test_grouped['Order'] = df_test[['Inflight wifi service','Departure/Arrival time convenient','Ease of Online booking', 'Gate location']].mean(axis=1)\ndf_test_grouped['Comfort'] = df_test[['Food and drink','Online boarding','Seat comfort', 'Inflight entertainment']].mean(axis=1)\ndf_test_grouped['Service'] = df_test[['Inflight entertainment','On-board service','Leg room service', 'Baggage handling']].mean(axis=1)\ndf_test_grouped.drop(['Inflight wifi service','Departure/Arrival time convenient','Ease of Online booking', 'Gate location', 'Food and drink','Online boarding','Seat comfort', 'Inflight entertainment', 'On-board service','Leg room service', 'Baggage handling', 'Age', 'Flight Distance', 'Departure Delay in Minutes'],axis=1,inplace=True)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:22.025048Z","iopub.execute_input":"2021-07-09T13:41:22.025314Z","iopub.status.idle":"2021-07-09T13:41:22.042458Z","shell.execute_reply.started":"2021-07-09T13:41:22.025287Z","shell.execute_reply":"2021-07-09T13:41:22.041729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_grouped.sample(random_state=rstate)","metadata":{"pycharm":{"is_executing":true},"scrolled":true,"execution":{"iopub.status.busy":"2021-07-09T13:41:22.043735Z","iopub.execute_input":"2021-07-09T13:41:22.044122Z","iopub.status.idle":"2021-07-09T13:41:22.061945Z","shell.execute_reply.started":"2021-07-09T13:41:22.044081Z","shell.execute_reply":"2021-07-09T13:41:22.061004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_grouped.sample(random_state=rstate)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:22.06312Z","iopub.execute_input":"2021-07-09T13:41:22.063375Z","iopub.status.idle":"2021-07-09T13:41:22.082981Z","shell.execute_reply.started":"2021-07-09T13:41:22.063351Z","shell.execute_reply":"2021-07-09T13:41:22.082005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance analysis","metadata":{}},{"cell_type":"code","source":"#Import the relevant libraries\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import MeanShift","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:22.084225Z","iopub.execute_input":"2021-07-09T13:41:22.084615Z","iopub.status.idle":"2021-07-09T13:41:22.168007Z","shell.execute_reply.started":"2021-07-09T13:41:22.084583Z","shell.execute_reply":"2021-07-09T13:41:22.167152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to the heavy processing, sometimes a 1/10 random sample we be used in the following questions. Otherwise, the available machine will not be able to process the task in a reasonable amount of time. Data is also normalized and converted to numpy for performance improvement.","metadata":{}},{"cell_type":"code","source":"sample_size = 10000\ndf_train_grouped_sample = df_train_grouped.sample(sample_size,random_state=rstate) #save the sample\ndf_train_grouped_sample_std = StandardScaler().fit_transform(df_train_grouped_sample) #scaled data\ndf_train_grouped_std = StandardScaler().fit_transform(df_train_grouped)#save a version of the entire scaled trainset\ndf_test_grouped_std = StandardScaler().fit_transform(df_test_grouped)#test as well","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:22.169178Z","iopub.execute_input":"2021-07-09T13:41:22.169461Z","iopub.status.idle":"2021-07-09T13:41:22.222653Z","shell.execute_reply.started":"2021-07-09T13:41:22.169434Z","shell.execute_reply":"2021-07-09T13:41:22.221821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We begin with the clustering task and Kmeans as its basic pioneer. Using the silhouette score to determine optimal amount of clusters:","metadata":{}},{"cell_type":"code","source":"range_n_clusters = list (range(3,10)) \nprint (\"Number of clusters from 3 to 10: \\n\", range_n_clusters)\nfor n_clusters in range_n_clusters:\n    clusterer = KMeans (n_clusters=n_clusters)\n    preds = clusterer.fit_predict(df_train_grouped_sample_std)\n    centers = clusterer.cluster_centers_\n    score = silhouette_score (df_train_grouped_sample_std, preds) \n    print (\"For %d clusters, average Silhouette score is %.2f\" % (n_clusters, score))","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:22.223814Z","iopub.execute_input":"2021-07-09T13:41:22.2241Z","iopub.status.idle":"2021-07-09T13:41:52.010379Z","shell.execute_reply.started":"2021-07-09T13:41:22.224073Z","shell.execute_reply":"2021-07-09T13:41:52.009494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best score is obtained with 5 clusters which turn out rather balanced:","metadata":{}},{"cell_type":"code","source":"kclusters = 5\nkmeans = KMeans(n_clusters=kclusters, random_state=rstate).fit(df_train_grouped)\ncluster_results_kmeans = kmeans.labels_\nnp.bincount(cluster_results_kmeans)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:52.011459Z","iopub.execute_input":"2021-07-09T13:41:52.011704Z","iopub.status.idle":"2021-07-09T13:41:54.525381Z","shell.execute_reply.started":"2021-07-09T13:41:52.011679Z","shell.execute_reply":"2021-07-09T13:41:54.524575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add the cluster id as an additional column and display the mean values for every cluster once grouped:","metadata":{}},{"cell_type":"code","source":"Summary_kmeans = df_train_grouped.copy()\nSummary_kmeans.insert(0, 'K Cluster Label', kmeans.labels_) #input the column that contains the labels of each record to the table\nSummary_kmeans_full = Summary_kmeans \nSummary_kmeans = Summary_kmeans.groupby(['K Cluster Label']).mean() #group by cluster id\nSummary_kmeans","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:54.526676Z","iopub.execute_input":"2021-07-09T13:41:54.527071Z","iopub.status.idle":"2021-07-09T13:41:54.558604Z","shell.execute_reply.started":"2021-07-09T13:41:54.527034Z","shell.execute_reply":"2021-07-09T13:41:54.557731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Easy to see cluster 0,2 have significantly higher satisfaction ratios. If we look at the means of those clusters we can see that compared to the others:\n1. Cleanliness is higher\n2. Inflight service is higher\n3. Gender is meaningless\n4. Less disloyal customers\n5. Less eco\\eco-plus classes\n6. Higher scores for the 3 systetic subgroups (a very good sign!)","metadata":{}},{"cell_type":"markdown","source":"Now we'll try agglomerative clustering since it is well built for our requirement because we can set the amount of cluster to 2 and the merge will continue all the way there and find the commonalities:","metadata":{}},{"cell_type":"code","source":"agglom = AgglomerativeClustering(n_clusters = 2, linkage = 'complete') #two clusters are set, aiming for a large difference in satisfaction labels\nAC_labels = agglom.fit_predict(df_train_grouped_sample)\ncluster_results_AC = agglom.labels_\nnp.bincount(cluster_results_AC)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:54.560013Z","iopub.execute_input":"2021-07-09T13:41:54.560408Z","iopub.status.idle":"2021-07-09T13:41:58.316113Z","shell.execute_reply.started":"2021-07-09T13:41:54.560366Z","shell.execute_reply":"2021-07-09T13:41:58.315225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Analyzing the results of agglomerative clustering, the picture is rather clear and the same trends can be seen as listed for Kmeans. Now the same groupby is performed again:","metadata":{}},{"cell_type":"code","source":"Summary_AC = df_train_grouped_sample.copy()\nSummary_AC.insert(0, 'AC Cluster Labels', AC_labels)\nSummary_AC = Summary_AC.groupby(['AC Cluster Labels']).mean()\nSummary_AC","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:58.317104Z","iopub.execute_input":"2021-07-09T13:41:58.317393Z","iopub.status.idle":"2021-07-09T13:41:58.339719Z","shell.execute_reply.started":"2021-07-09T13:41:58.317367Z","shell.execute_reply":"2021-07-09T13:41:58.338912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another algorithm we'll try is spectral and we can see the same conclusions can be derived from it. Spectral is also showing the best seperation ratio (75% vs 18% satisfaction) and it fits our task well because it decompresses the features, reducing the information to 2 clusters - similar to agglomerative:","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import SpectralClustering\nSpec = SpectralClustering(n_clusters=2, assign_labels='discretize', random_state=rstate).fit(df_train_grouped_sample)\ncluster_results_SC = Spec.labels_\nnp.bincount(cluster_results_SC)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:41:58.340778Z","iopub.execute_input":"2021-07-09T13:41:58.341049Z","iopub.status.idle":"2021-07-09T13:42:19.839992Z","shell.execute_reply.started":"2021-07-09T13:41:58.341023Z","shell.execute_reply":"2021-07-09T13:42:19.839062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Same process of addition of cluster id to table\nSummary_SC = df_train_grouped_sample.copy()\nSummary_SC.insert(0, 'SC Cluster Labels', cluster_results_SC)\nSummary_SC = Summary_SC.groupby(['SC Cluster Labels']).mean()\nSummary_SC","metadata":{"pycharm":{"is_executing":true},"scrolled":true,"execution":{"iopub.status.busy":"2021-07-09T13:42:19.844565Z","iopub.execute_input":"2021-07-09T13:42:19.846859Z","iopub.status.idle":"2021-07-09T13:42:19.882802Z","shell.execute_reply.started":"2021-07-09T13:42:19.846797Z","shell.execute_reply":"2021-07-09T13:42:19.881805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And the results show the same trends again (for the third time) so I think the conclusions are strong.","metadata":{}},{"cell_type":"markdown","source":"### Prediction of overall satisfaction label","metadata":{}},{"cell_type":"markdown","source":"Import the relevant libraries:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:42:19.887616Z","iopub.execute_input":"2021-07-09T13:42:19.890023Z","iopub.status.idle":"2021-07-09T13:42:19.898571Z","shell.execute_reply.started":"2021-07-09T13:42:19.88997Z","shell.execute_reply":"2021-07-09T13:42:19.897392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing the train & test vectors for use:","metadata":{}},{"cell_type":"code","source":"x_train = np.delete(df_train_grouped_std, 3, 1) #remove label from input vector\ny_train = df_train_grouped_std[:,3]\nx_test = np.delete(df_train_grouped_std, 3, 1) #remove label from input vector\ny_test = df_train_grouped_std[:,3]","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:42:19.906463Z","iopub.execute_input":"2021-07-09T13:42:19.909671Z","iopub.status.idle":"2021-07-09T13:42:19.924129Z","shell.execute_reply.started":"2021-07-09T13:42:19.909619Z","shell.execute_reply":"2021-07-09T13:42:19.922837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Needs to be set to int (binary) to allow classification algorithms to perform fit\ny_train = y_train.astype(int)\ny_test = y_test.astype(int)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:42:19.929219Z","iopub.execute_input":"2021-07-09T13:42:19.931396Z","iopub.status.idle":"2021-07-09T13:42:19.938843Z","shell.execute_reply.started":"2021-07-09T13:42:19.931352Z","shell.execute_reply":"2021-07-09T13:42:19.937814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create a generalized function to deal with fitting, predicting satisfaction and measuring results. For each run we print out the ROC area under curve score, classification metrics and a confusion matrix:","metadata":{}},{"cell_type":"code","source":"def run_model(model, x_train, y_train, x_test, y_test, verbose=True):\n    if verbose == False:\n        model.fit(x_train,y_train, verbose=0)\n    else:\n        model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    print(\"ROC_AUC = {}\".format(roc_auc))\n    print(classification_report(y_test,y_pred,digits=5))\n    plot_confusion_matrix(model, x_test, y_test,cmap=plt.cm.Blues, normalize = 'all')\n    \n    return model, roc_auc #function returns model object and ROC_AUC","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:42:19.941832Z","iopub.execute_input":"2021-07-09T13:42:19.942208Z","iopub.status.idle":"2021-07-09T13:42:19.949377Z","shell.execute_reply.started":"2021-07-09T13:42:19.942169Z","shell.execute_reply":"2021-07-09T13:42:19.94856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First up is random forest which is a decision-tree based classic algorithm. Since some of the optimizations are taking an extremely long amount of time, what I will do is run it offline (many hours), display the code with a comment FYI, while the actual run will be done with determinstic parameters. Other algorithms that run quickly we be applied by initializing a Grid object:","metadata":{}},{"cell_type":"code","source":"#Since runtime is extremely long for a full grid search, we will use its best parameters for setup:\nparams_rf = {'max_depth': 25, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1200,'random_state': rstate}\n\n#hparam_rf = {'criterion': ['gini', 'entropy'], 'n_estimators': [50,100,200,500,1200],'max_depth':[5,10,25,35], 'min_samples_split':[1,2,3]}\n\n#model_rf = GridSearchCV(RandomForestClassifier(), param_grid=hparam_rf,scoring = 'roc_auc', n_jobs=-1)\nmodel_rf = RandomForestClassifier(**params_rf) #get an unspecified number of parameters to the function\nmodel_rf, roc_auc_rf = run_model(model_rf, x_train, y_train, x_test, y_test) #pass the model together with the vectors for training and prediction","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:42:19.950819Z","iopub.execute_input":"2021-07-09T13:42:19.951437Z","iopub.status.idle":"2021-07-09T13:44:49.635766Z","shell.execute_reply.started":"2021-07-09T13:42:19.951397Z","shell.execute_reply":"2021-07-09T13:44:49.634854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest generates outstanding results and as can be seen from the feature importance analysis, our synthetic aggregated features do a great job helping out (50% accumulated feature importance)! the results are better than the top rated kaggle notebooks I've seen:","metadata":{}},{"cell_type":"code","source":"model_rf.feature_importances_","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:44:49.637123Z","iopub.execute_input":"2021-07-09T13:44:49.637409Z","iopub.status.idle":"2021-07-09T13:44:50.127979Z","shell.execute_reply.started":"2021-07-09T13:44:49.63738Z","shell.execute_reply":"2021-07-09T13:44:50.126962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since random forest performed very well, we will continue with another  decision tree based algorithm - lightGBM:","metadata":{}},{"cell_type":"code","source":"#listing all parameters for grid search:\nhparam_lgb = {'n_estimators': [50, 100, 200],'max_depth':[5,10,15,20],'num_leaves': [25, 50, 100], 'random_state': [rstate]}","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:44:50.129103Z","iopub.execute_input":"2021-07-09T13:44:50.129351Z","iopub.status.idle":"2021-07-09T13:44:50.133647Z","shell.execute_reply.started":"2021-07-09T13:44:50.129327Z","shell.execute_reply":"2021-07-09T13:44:50.132844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgb = GridSearchCV(lgb.LGBMClassifier(), param_grid=hparam_lgb,scoring = 'roc_auc', n_jobs=-1)\nmodel_lgb, roc_auc_lgb = run_model(model_lgb, x_train, y_train, x_test, y_test)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:44:50.134937Z","iopub.execute_input":"2021-07-09T13:44:50.135199Z","iopub.status.idle":"2021-07-09T13:46:28.399759Z","shell.execute_reply.started":"2021-07-09T13:44:50.135174Z","shell.execute_reply":"2021-07-09T13:46:28.398826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same principal as in random forest - the 3 sub groups have a major impact on prediction (last 3 features in list):","metadata":{}},{"cell_type":"code","source":"model_lgb.best_estimator_.feature_importances_","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:46:28.40129Z","iopub.execute_input":"2021-07-09T13:46:28.401682Z","iopub.status.idle":"2021-07-09T13:46:28.408357Z","shell.execute_reply.started":"2021-07-09T13:46:28.40164Z","shell.execute_reply":"2021-07-09T13:46:28.407473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will try to apply a different algorithmic basis - support vectors:","metadata":{}},{"cell_type":"code","source":"#Same principal as in random forest - we'll use the optimal hparameters obtained using gridsearch as the finite input.\nparams_svc ={'C': 1, \n         'kernel': 'linear', \n         'degree': 3, \n         'gamma': 'scale',\n          'random_state':rstate}\n\n#hparam_svc = {'C': [1,2,3],'kernel':['rbf', 'linear'],'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:46:28.409373Z","iopub.execute_input":"2021-07-09T13:46:28.409625Z","iopub.status.idle":"2021-07-09T13:46:28.422171Z","shell.execute_reply.started":"2021-07-09T13:46:28.4096Z","shell.execute_reply":"2021-07-09T13:46:28.421399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_svc = SVC(**params_svc)\n#model_svc = GridSearchCV(SVC(), param_grid=hparam_svc,scoring = 'roc_auc', n_jobs=-1)\nmodel_svc, roc_auc_svc = run_model(model_svc, x_train, y_train, x_test, y_test)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:46:28.423474Z","iopub.execute_input":"2021-07-09T13:46:28.423788Z","iopub.status.idle":"2021-07-09T13:55:35.819239Z","shell.execute_reply.started":"2021-07-09T13:46:28.423755Z","shell.execute_reply":"2021-07-09T13:55:35.818182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results are not that good as the previous, decision tree-based, algorithms. For the final run we'll use Adaboost, expecting a performance similar to light GBM & random forest. In practice, the results are worse. ","metadata":{}},{"cell_type":"code","source":"hparam_ada = {'learning_rate':[0.8, 1.0, 1.1],'n_estimators':[50,100,150,200,500,1000], 'random_state':[rstate]}\nmodel_ada = GridSearchCV(AdaBoostClassifier(), param_grid=hparam_ada,scoring = 'roc_auc', n_jobs=-1)\nmodel_ada, roc_auc_ada = run_model(model_ada, x_train, y_train, x_test, y_test)","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T13:55:35.820468Z","iopub.execute_input":"2021-07-09T13:55:35.820755Z","iopub.status.idle":"2021-07-09T14:07:20.283825Z","shell.execute_reply.started":"2021-07-09T13:55:35.820726Z","shell.execute_reply":"2021-07-09T14:07:20.282551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, we see the same effect in feature importance once more:","metadata":{}},{"cell_type":"code","source":"model_ada.best_estimator_.feature_importances_","metadata":{"pycharm":{"is_executing":true},"execution":{"iopub.status.busy":"2021-07-09T14:07:20.286933Z","iopub.execute_input":"2021-07-09T14:07:20.287349Z","iopub.status.idle":"2021-07-09T14:07:20.335797Z","shell.execute_reply.started":"2021-07-09T14:07:20.287305Z","shell.execute_reply":"2021-07-09T14:07:20.334804Z"},"trusted":true},"execution_count":null,"outputs":[]}]}