{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['ionosphere_data_kaggle.csv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"file = \"../input/ionosphere_data_kaggle.csv\"\ndf = pd.read_csv(file)\ndataset = df.values\ndataset.shape\nX = dataset[:,0:34]\nY = dataset[:,34]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nencoder.fit(Y)\ny = encoder.transform(Y)","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time Based Learning Rate Schedule"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(34, input_dim=34, init= 'normal' , activation= 'relu' ))\nmodel.add(Dense(1, init= 'normal' , activation= 'sigmoid' ))\n# Compile model\nepochs = 50\nlearning_rate = 0.1\ndecay_rate = learning_rate / epochs\nmomentum = 0.8\nsgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\nmodel.compile(loss= 'binary_crossentropy' , optimizer='sgd', metrics=[ 'accuracy' ])\n# Fit the model\nmodel.fit(X, y, validation_split=0.33, nb_epoch=epochs, batch_size=28, verbose=2)","execution_count":14,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(34, input_dim=34, activation=\"relu\", kernel_initializer=\"normal\")`\n  \n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n  import sys\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  app.launch_new_instance()\n","name":"stderr"},{"output_type":"stream","text":"Train on 235 samples, validate on 116 samples\nEpoch 1/50\n - 0s - loss: 0.6984 - acc: 0.3915 - val_loss: 0.7031 - val_acc: 0.2500\nEpoch 2/50\n - 0s - loss: 0.6972 - acc: 0.3915 - val_loss: 0.6997 - val_acc: 0.2672\nEpoch 3/50\n - 0s - loss: 0.6960 - acc: 0.3702 - val_loss: 0.6971 - val_acc: 0.3103\nEpoch 4/50\n - 0s - loss: 0.6949 - acc: 0.3830 - val_loss: 0.6948 - val_acc: 0.3362\nEpoch 5/50\n - 0s - loss: 0.6938 - acc: 0.4085 - val_loss: 0.6921 - val_acc: 0.4569\nEpoch 6/50\n - 0s - loss: 0.6927 - acc: 0.4681 - val_loss: 0.6890 - val_acc: 0.8190\nEpoch 7/50\n - 0s - loss: 0.6915 - acc: 0.5404 - val_loss: 0.6874 - val_acc: 0.9224\nEpoch 8/50\n - 0s - loss: 0.6906 - acc: 0.5830 - val_loss: 0.6858 - val_acc: 0.9138\nEpoch 9/50\n - 0s - loss: 0.6895 - acc: 0.6085 - val_loss: 0.6838 - val_acc: 0.9224\nEpoch 10/50\n - 0s - loss: 0.6886 - acc: 0.6426 - val_loss: 0.6815 - val_acc: 0.9397\nEpoch 11/50\n - 0s - loss: 0.6876 - acc: 0.6596 - val_loss: 0.6799 - val_acc: 0.9224\nEpoch 12/50\n - 0s - loss: 0.6867 - acc: 0.6723 - val_loss: 0.6783 - val_acc: 0.9138\nEpoch 13/50\n - 0s - loss: 0.6857 - acc: 0.6809 - val_loss: 0.6769 - val_acc: 0.9138\nEpoch 14/50\n - 0s - loss: 0.6848 - acc: 0.6766 - val_loss: 0.6756 - val_acc: 0.8966\nEpoch 15/50\n - 0s - loss: 0.6838 - acc: 0.6766 - val_loss: 0.6744 - val_acc: 0.8966\nEpoch 16/50\n - 0s - loss: 0.6828 - acc: 0.6766 - val_loss: 0.6728 - val_acc: 0.8879\nEpoch 17/50\n - 0s - loss: 0.6818 - acc: 0.6723 - val_loss: 0.6705 - val_acc: 0.8793\nEpoch 18/50\n - 0s - loss: 0.6807 - acc: 0.6681 - val_loss: 0.6696 - val_acc: 0.8793\nEpoch 19/50\n - 0s - loss: 0.6797 - acc: 0.7234 - val_loss: 0.6671 - val_acc: 0.8793\nEpoch 20/50\n - 0s - loss: 0.6787 - acc: 0.7106 - val_loss: 0.6652 - val_acc: 0.8707\nEpoch 21/50\n - 0s - loss: 0.6775 - acc: 0.6979 - val_loss: 0.6643 - val_acc: 0.8793\nEpoch 22/50\n - 0s - loss: 0.6765 - acc: 0.7021 - val_loss: 0.6620 - val_acc: 0.8707\nEpoch 23/50\n - 0s - loss: 0.6753 - acc: 0.7021 - val_loss: 0.6618 - val_acc: 0.8534\nEpoch 24/50\n - 0s - loss: 0.6743 - acc: 0.7106 - val_loss: 0.6591 - val_acc: 0.8448\nEpoch 25/50\n - 0s - loss: 0.6731 - acc: 0.7064 - val_loss: 0.6574 - val_acc: 0.8276\nEpoch 26/50\n - 0s - loss: 0.6719 - acc: 0.7064 - val_loss: 0.6550 - val_acc: 0.8276\nEpoch 27/50\n - 0s - loss: 0.6706 - acc: 0.7106 - val_loss: 0.6529 - val_acc: 0.8190\nEpoch 28/50\n - 0s - loss: 0.6693 - acc: 0.7106 - val_loss: 0.6508 - val_acc: 0.8017\nEpoch 29/50\n - 0s - loss: 0.6681 - acc: 0.7149 - val_loss: 0.6489 - val_acc: 0.7931\nEpoch 30/50\n - 0s - loss: 0.6668 - acc: 0.7149 - val_loss: 0.6478 - val_acc: 0.7586\nEpoch 31/50\n - 0s - loss: 0.6655 - acc: 0.7191 - val_loss: 0.6452 - val_acc: 0.7500\nEpoch 32/50\n - 0s - loss: 0.6642 - acc: 0.7149 - val_loss: 0.6417 - val_acc: 0.7586\nEpoch 33/50\n - 0s - loss: 0.6626 - acc: 0.7149 - val_loss: 0.6390 - val_acc: 0.7414\nEpoch 34/50\n - 0s - loss: 0.6611 - acc: 0.7064 - val_loss: 0.6366 - val_acc: 0.7414\nEpoch 35/50\n - 0s - loss: 0.6597 - acc: 0.7149 - val_loss: 0.6349 - val_acc: 0.7328\nEpoch 36/50\n - 0s - loss: 0.6581 - acc: 0.7191 - val_loss: 0.6329 - val_acc: 0.7328\nEpoch 37/50\n - 0s - loss: 0.6566 - acc: 0.7191 - val_loss: 0.6307 - val_acc: 0.7241\nEpoch 38/50\n - 0s - loss: 0.6551 - acc: 0.7191 - val_loss: 0.6286 - val_acc: 0.7069\nEpoch 39/50\n - 0s - loss: 0.6535 - acc: 0.7277 - val_loss: 0.6264 - val_acc: 0.6983\nEpoch 40/50\n - 0s - loss: 0.6519 - acc: 0.7319 - val_loss: 0.6231 - val_acc: 0.6983\nEpoch 41/50\n - 0s - loss: 0.6501 - acc: 0.7362 - val_loss: 0.6206 - val_acc: 0.6983\nEpoch 42/50\n - 0s - loss: 0.6484 - acc: 0.7362 - val_loss: 0.6182 - val_acc: 0.6897\nEpoch 43/50\n - 0s - loss: 0.6466 - acc: 0.7319 - val_loss: 0.6155 - val_acc: 0.6897\nEpoch 44/50\n - 0s - loss: 0.6448 - acc: 0.7319 - val_loss: 0.6119 - val_acc: 0.6897\nEpoch 45/50\n - 0s - loss: 0.6431 - acc: 0.7277 - val_loss: 0.6092 - val_acc: 0.6897\nEpoch 46/50\n - 0s - loss: 0.6412 - acc: 0.7319 - val_loss: 0.6064 - val_acc: 0.6897\nEpoch 47/50\n - 0s - loss: 0.6394 - acc: 0.7319 - val_loss: 0.6042 - val_acc: 0.6897\nEpoch 48/50\n - 0s - loss: 0.6376 - acc: 0.7362 - val_loss: 0.6018 - val_acc: 0.6897\nEpoch 49/50\n - 0s - loss: 0.6358 - acc: 0.7404 - val_loss: 0.5985 - val_acc: 0.6897\nEpoch 50/50\n - 0s - loss: 0.6339 - acc: 0.7362 - val_loss: 0.5972 - val_acc: 0.6810\n","name":"stdout"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<keras.callbacks.History at 0x7ff0d32834a8>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Drop based Learning Rate Schedule"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\nimport pandas\nimport numpy\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.callbacks import LearningRateScheduler\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n    return lrate\n\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nY = encoder.transform(Y)\n# create model\nmodel = Sequential()\nmodel.add(Dense(34, input_dim=34, init= 'normal' , activation= 'relu' ))\nmodel.add(Dense(1, init= 'normal' , activation= 'sigmoid' ))\n# Compile model\nsgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\nmodel.compile(loss= 'binary_crossentropy' , optimizer='sgd', metrics=[ 'accuracy' ])\n# learning schedule callback\nlrate = LearningRateScheduler(step_decay)\ncallbacks_list = [lrate]\n# Fit the model\nmodel.fit(X, y, validation_split=0.33, nb_epoch=50, batch_size=28,callbacks=callbacks_list, verbose=2)","execution_count":17,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(34, input_dim=34, activation=\"relu\", kernel_initializer=\"normal\")`\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")`\n/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","name":"stderr"},{"output_type":"stream","text":"Train on 235 samples, validate on 116 samples\nEpoch 1/50\n - 0s - loss: 0.6914 - acc: 0.5489 - val_loss: 0.6767 - val_acc: 0.8966\nEpoch 2/50\n - 0s - loss: 0.6823 - acc: 0.6681 - val_loss: 0.6565 - val_acc: 0.9397\nEpoch 3/50\n - 0s - loss: 0.6713 - acc: 0.6681 - val_loss: 0.6413 - val_acc: 0.8966\nEpoch 4/50\n - 0s - loss: 0.6569 - acc: 0.7319 - val_loss: 0.6113 - val_acc: 0.9052\nEpoch 5/50\n - 0s - loss: 0.6395 - acc: 0.7617 - val_loss: 0.5797 - val_acc: 0.9052\nEpoch 6/50\n - 0s - loss: 0.6197 - acc: 0.7660 - val_loss: 0.5364 - val_acc: 0.9052\nEpoch 7/50\n - 0s - loss: 0.5998 - acc: 0.7787 - val_loss: 0.5494 - val_acc: 0.9052\nEpoch 8/50\n - 0s - loss: 0.5794 - acc: 0.7915 - val_loss: 0.5031 - val_acc: 0.9224\nEpoch 9/50\n - 0s - loss: 0.5585 - acc: 0.8085 - val_loss: 0.5066 - val_acc: 0.9310\nEpoch 10/50\n - 0s - loss: 0.5393 - acc: 0.8213 - val_loss: 0.4800 - val_acc: 0.9310\nEpoch 11/50\n - 0s - loss: 0.5282 - acc: 0.8340 - val_loss: 0.4651 - val_acc: 0.9397\nEpoch 12/50\n - 0s - loss: 0.5174 - acc: 0.8298 - val_loss: 0.4537 - val_acc: 0.9397\nEpoch 13/50\n - 0s - loss: 0.5049 - acc: 0.8383 - val_loss: 0.4480 - val_acc: 0.9397\nEpoch 14/50\n - 0s - loss: 0.4933 - acc: 0.8426 - val_loss: 0.4514 - val_acc: 0.9397\nEpoch 15/50\n - 0s - loss: 0.4810 - acc: 0.8383 - val_loss: 0.4390 - val_acc: 0.9397\nEpoch 16/50\n - 0s - loss: 0.4696 - acc: 0.8468 - val_loss: 0.4367 - val_acc: 0.9397\nEpoch 17/50\n - 0s - loss: 0.4581 - acc: 0.8468 - val_loss: 0.4246 - val_acc: 0.9483\nEpoch 18/50\n - 0s - loss: 0.4489 - acc: 0.8511 - val_loss: 0.4213 - val_acc: 0.9483\nEpoch 19/50\n - 0s - loss: 0.4370 - acc: 0.8553 - val_loss: 0.3829 - val_acc: 0.9483\nEpoch 20/50\n - 0s - loss: 0.4280 - acc: 0.8553 - val_loss: 0.3813 - val_acc: 0.9483\nEpoch 21/50\n - 0s - loss: 0.4218 - acc: 0.8553 - val_loss: 0.3770 - val_acc: 0.9483\nEpoch 22/50\n - 0s - loss: 0.4169 - acc: 0.8553 - val_loss: 0.3830 - val_acc: 0.9483\nEpoch 23/50\n - 0s - loss: 0.4116 - acc: 0.8553 - val_loss: 0.3908 - val_acc: 0.9397\nEpoch 24/50\n - 0s - loss: 0.4068 - acc: 0.8596 - val_loss: 0.3745 - val_acc: 0.9483\nEpoch 25/50\n - 0s - loss: 0.4020 - acc: 0.8553 - val_loss: 0.3719 - val_acc: 0.9483\nEpoch 26/50\n - 0s - loss: 0.3974 - acc: 0.8596 - val_loss: 0.3661 - val_acc: 0.9483\nEpoch 27/50\n - 0s - loss: 0.3921 - acc: 0.8638 - val_loss: 0.3626 - val_acc: 0.9397\nEpoch 28/50\n - 0s - loss: 0.3875 - acc: 0.8681 - val_loss: 0.3580 - val_acc: 0.9397\nEpoch 29/50\n - 0s - loss: 0.3831 - acc: 0.8681 - val_loss: 0.3534 - val_acc: 0.9397\nEpoch 30/50\n - 0s - loss: 0.3793 - acc: 0.8766 - val_loss: 0.3476 - val_acc: 0.9397\nEpoch 31/50\n - 0s - loss: 0.3765 - acc: 0.8681 - val_loss: 0.3466 - val_acc: 0.9397\nEpoch 32/50\n - 0s - loss: 0.3743 - acc: 0.8766 - val_loss: 0.3490 - val_acc: 0.9397\nEpoch 33/50\n - 0s - loss: 0.3721 - acc: 0.8809 - val_loss: 0.3445 - val_acc: 0.9397\nEpoch 34/50\n - 0s - loss: 0.3705 - acc: 0.8723 - val_loss: 0.3419 - val_acc: 0.9397\nEpoch 35/50\n - 0s - loss: 0.3678 - acc: 0.8766 - val_loss: 0.3421 - val_acc: 0.9397\nEpoch 36/50\n - 0s - loss: 0.3656 - acc: 0.8809 - val_loss: 0.3443 - val_acc: 0.9397\nEpoch 37/50\n - 0s - loss: 0.3637 - acc: 0.8936 - val_loss: 0.3399 - val_acc: 0.9397\nEpoch 38/50\n - 0s - loss: 0.3616 - acc: 0.8936 - val_loss: 0.3370 - val_acc: 0.9397\nEpoch 39/50\n - 0s - loss: 0.3593 - acc: 0.8894 - val_loss: 0.3315 - val_acc: 0.9397\nEpoch 40/50\n - 0s - loss: 0.3574 - acc: 0.8894 - val_loss: 0.3313 - val_acc: 0.9397\nEpoch 41/50\n - 0s - loss: 0.3568 - acc: 0.8979 - val_loss: 0.3302 - val_acc: 0.9397\nEpoch 42/50\n - 0s - loss: 0.3555 - acc: 0.8894 - val_loss: 0.3292 - val_acc: 0.9397\nEpoch 43/50\n - 0s - loss: 0.3544 - acc: 0.8936 - val_loss: 0.3288 - val_acc: 0.9483\nEpoch 44/50\n - 0s - loss: 0.3533 - acc: 0.8979 - val_loss: 0.3267 - val_acc: 0.9483\nEpoch 45/50\n - 0s - loss: 0.3526 - acc: 0.8979 - val_loss: 0.3264 - val_acc: 0.9483\nEpoch 46/50\n - 0s - loss: 0.3516 - acc: 0.8979 - val_loss: 0.3247 - val_acc: 0.9483\nEpoch 47/50\n - 0s - loss: 0.3506 - acc: 0.8979 - val_loss: 0.3227 - val_acc: 0.9483\nEpoch 48/50\n - 0s - loss: 0.3495 - acc: 0.8979 - val_loss: 0.3217 - val_acc: 0.9483\nEpoch 49/50\n - 0s - loss: 0.3485 - acc: 0.8979 - val_loss: 0.3218 - val_acc: 0.9483\nEpoch 50/50\n - 0s - loss: 0.3477 - acc: 0.8979 - val_loss: 0.3212 - val_acc: 0.9483\n","name":"stdout"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"<keras.callbacks.History at 0x7ff0ac1bf2b0>"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}