{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis notebook is based on a project which used the CIC Darknet Traffic dataset.  One of the goals of the project was to explore the various models (classifiers) that could be trained to predict the purpose of the darknet traffic based on a set of features and subsets of those features.  The dataset was modified from what was available on the UNB/CIC website ( link: https://www.unb.ca/cic/datasets/darknet2020.html ).  The changes are discussed in the \"Changes to the Original Dataset\" section below.\n\n## Goals of this Notebook\n1. Show some feature selection methods that can be used to identify the best features to use when attempting to determine the *Purpose* of the internet traffic\n1. Show how various types of models/classifiers can be used to see which is the most accurate at predicting the *Purpose* of the internet traffic\n\n## Changes to the Original Dataset\n\n### Column Names\nThe orginal name of the target column was *Label*, which was the same as a feature column.  Both columns were renamed as follows:\n* The feature column was renamed to *Conn Type* since it defines what type of connection was used to transfer the data (e.g. VPN, non VPN, etc.)\n* The target column was renamed to *Purpose* because it defines what the purpose of the internet traffic was (e.g. email, audio streaming, etc.)\n\n### Column Values\n* \"Timestamp\" - all values in the \"Timestamp\" column have been convery to seconds since epoch\n* \"Src IP\" and \"Dst IP\" - all IP addresses have been converted to integers representing a country code for the country associated with that IP.  If that IP was not designated for any one country, then the value was 0.\n* \"Conn Type\" - all values were converted to integers\n* \"Purpose\" - all names were replaced with integers\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\n\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, mean_squared_error, plot_confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\nfrom sklearn.multiclass import OneVsRestClassifier, OutputCodeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n### CONSTANTS\nSEED = 123\nNUM_FEATURES = 5\nTRAIN_PCT = 0.75\n\nMAX_DEPTH = 4\nMAX_ITER = 300\nN_NEIGHBORS = 5\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"traffic = pd.read_csv(\"/kaggle/input/cicdarknet2020-numeric/DarkNetTraffic.csv\")\nprint(f'Number of Rows: {traffic.shape[0]}')\nprint(f'Number of Columns: {traffic.shape[1]}')\ntraffic.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove duplicate entries\ntraffic.drop_duplicates(subset=None, keep='first', inplace=True, ignore_index=False)\n\n# Remove constant columns\ntraffic = traffic.loc[:, traffic.apply(pd.Series.nunique) != 1]\n\n# Look at the dataset again\nprint(f'Number of Rows: {traffic.shape[0]}')\nprint(f'Number of Columns: {traffic.shape[1]}')\ntraffic.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\nTo choose which features are the best to use, we will select the top N features from a group of classifiers and aggregate the results to get 1 set of features.  One of the function parameters is the train percentage split used when training the models\n\n### Why were those model settings chosen?\nThese are just a few of the many available classifiers available to be used.  Furthermore, there are many settings for each classifier, and some parameters are similar, sometimes even the same.  Adding/changing the classifier settings *could* affect which features are selected."},{"metadata":{"trusted":true},"cell_type":"code","source":"def selectFeatures(x, y, train_size_pct=0.75):\n    \"\"\"\n    selectFeatures\n        x : The features of the dataset to be used for predictions\n        y : The target class for each row in \"x\"\n        train_size_pct : (default = 0.75) In the tange (0.0, 1.0), the ratio by which to split the data for training and testing\n        @return (list) The names of the selected features\n    \"\"\"\n\n    # Create classifiers\n    rf = RandomForestClassifier(max_depth=MAX_DEPTH, criterion='entropy', random_state=SEED)\n    et = ExtraTreesClassifier(max_depth=MAX_DEPTH, criterion='entropy', random_state=SEED)\n    dectree = DecisionTreeClassifier(max_depth=MAX_DEPTH, random_state=SEED)\n\n    classifier_mapping = {\n        \"RandomForest\" : rf,\n        \"ExtraTrees\" : et,\n        \"DecisionTree\" : dectree\n    }\n\n    ### Split the dataset\n    X_train_fs, X_test_fs, Y_train_fs, Y_test_fs = train_test_split(x, y, train_size=train_size_pct)\n\n    model_features = {}\n\n    for model_name, model in classifier_mapping.items():\n        print(f'[Training] {model_name}')\n        start_train = datetime.now()\n        model.fit(X_train_fs, Y_train_fs)\n        print(\">>> Training Time: {}\".format(datetime.now() - start_train))\n        model_features[model_name] = model.feature_importances_\n        model_score = model.score(X_test_fs, Y_test_fs)\n        print(f'>>> Training Accuracy : {model_score*100.0}')\n        print(\"\")\n\n    cols = X_train_fs.columns.values\n    feature_df = pd.DataFrame({'features': cols})\n    for model_name, model in classifier_mapping.items():\n        feature_df[model_name] = model_features[model_name]\n\n    ### Grab the nlargest features (by score) from each ensemble group\n    all_f = []\n    for model_name, model in classifier_mapping.items():\n        try:\n            all_f.append(feature_df.nlargest(NUM_FEATURES, model_name))\n        except KeyError as e:\n            print(f'*** Failed to add features for {model_name} : {e}')\n\n    result = []\n    for i in range(len(all_f)):\n        result.extend(all_f[i]['features'].to_list())\t\t# Concat the top nlargest scores from all groups into one list\n\n    # selected_features contains the ensemble results for best features\n    selected_features = list(set(result))\t\t\t\t\t# Drop duplicate fields from the list\n\n    return selected_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Test with Models\nNow that we have the dataset versions based on various feature sets, we can start evaluating the models.  However, we should first choose a metric by which to compare the models.  For this project, the 3 primary metrics were *accuracy*, *precision*, and *recall*.  There were also to additional metrics, *mean-squared error* and *F1 score*.\n\nSeveral functions were defined to help ease the training and testing of the models."},{"metadata":{},"cell_type":"markdown","source":"## [FUNCTION] calculateMetrics\nThe **calculateMetrics** function is a helper function that will print out all the metrics we are interested in and some extra ones too."},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculateMetrics(y_test, y_pred):\n    acc = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred, average=\"macro\")\n    precision = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n    mse = mean_squared_error(y_test, y_pred)\n    f1score = f1_score(y_pred, y_test, average='weighted')\n    print(\">>> Metrics\")\n    print(f'- Accuracy  : {acc}')\n    print(f'- Recall    : {recall}')\n    print(f'- Precision : {precision}')\n    print(f'- MSE       : {mse}')\n    print(f'- F1 Score  : {f1score}')\n\n    return [round(acc, 6), round(recall, 6), round(precision, 6), round(mse, 6), round(f1score, 6)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [FUNCTION] train_test_model\nThe process for evaluating every classifier shares the same general flow: train, test, analyze.  The **train_test_model** function does these three steps for every model and dataset given to it.  It will split the dataset accoridng to the specified train percentage, train the model, score the mode's training accuracy, make predictions, then grade the model's predicitons."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_model(model_name, model, x, y, train_size_pct):\n\n    # Split the data\n    X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=train_size_pct)\n\n    # Training\n    print(f'\\n[Training] {model_name}')\n    start_train = datetime.now()\n    model.fit(X_train, Y_train)\n    print(f'>>> Training time: {datetime.now() - start_train}')\n\n    ### Analyze Training\n    train_acc = model.score(X_train, Y_train)\n    print(f'>>> Training accuracy: {train_acc}')\n\n    ### Testing\n    start_predict = datetime.now()\n    y_pred = model.predict(X_test)\n    print(f'>>> Testing time: {datetime.now() - start_predict}')\n\n    ### Analyze Testing\n    calculateMetrics(Y_test, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [FUNCTION] **evaluateIndividualClassifiers**\nThe **evaluateIndividualClassifiers** is responsible for evaluating the dataset using several individual classifiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluateIndividualClassifiers(x, y, train_size_pct):\n    \"\"\"\n    evaluateIndividualClassifiers\n        x : The features of the dataset to be used for predictions\n        y : The target class for each row in \"x\"\n        train_size_pct : {float in the range(0.0, 1.0)} the percentage of the dataset that should be used for training\n    \"\"\"\n\n    max_depth_x2 = MAX_DEPTH * 2\n    max_iter_x2 = MAX_ITER * 2\n    n_neighbors_x2 = N_NEIGHBORS * 2\n    n_neighbors_d2 = N_NEIGHBORS // 2\n\n    rf = RandomForestClassifier(max_depth=MAX_DEPTH, criterion='entropy', random_state=SEED)\n    rf_x2 = RandomForestClassifier(max_depth=max_depth_x2, criterion='entropy', random_state=SEED)\n    et = ExtraTreesClassifier(max_depth=MAX_DEPTH, criterion='entropy', random_state=SEED)\n    dectree = DecisionTreeClassifier(max_depth=MAX_DEPTH, random_state=SEED)\n    knn = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n    knn_x2 = KNeighborsClassifier(n_neighbors=n_neighbors_x2)\n    knn_d2 = KNeighborsClassifier(n_neighbors=n_neighbors_d2)\n    mlpnn = MLPClassifier(max_iter=MAX_ITER)\n    mlpnnE = MLPClassifier(max_iter=MAX_ITER, early_stopping=True)\n    mlpnn_x2 = MLPClassifier(max_iter=max_iter_x2)\n    mlpnnE_x2 = MLPClassifier(max_iter=max_iter_x2, early_stopping=True)\n\n    classifier_mapping = {\n        f'RandomForest-{MAX_DEPTH}' : rf,\n        f'RandomForest-{max_depth_x2}' : rf_x2,\n        f'ExtraTrees-{MAX_DEPTH}' : et,\n        f'DecisionTree-{MAX_DEPTH}' : dectree,\n        f'KNeighbors-{N_NEIGHBORS}' : knn,\n        f'KNeighbors-{n_neighbors_x2}' : knn_x2,\n        f'KNeighbors-{n_neighbors_d2}' : knn_d2,\n        f'MLP-{MAX_ITER}' : mlpnn,\n        f'MLP-{MAX_ITER}-early' : mlpnnE,\n        f'MLP-{max_iter_x2}' : mlpnn_x2,\n        f'MLP-{max_iter_x2}-early' : mlpnnE_x2,\n    }\n\n    for model_name, model in classifier_mapping.items():\n\n        train_test_model(model_name, model, x, y, train_size_pct)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection & Evaluation\nWe will examine four different festure sets\n1. All Features\n1. Selected Features (from \"All Features\")\n1. All Features, except *Src Port* and *Dst Port*\n1. Selected Features (from \"All Features, except *Src Port* and *Dst Port*\")\n\nWe use \"all features\" and \"selected features\" to compare the time and accuracy between both the two approaches.  We also remove the ports because some literature indicates that the ports often becomes important features.  It will be interesting to see which features are selected when the ports are and are not available and compare the performance of the models.\n\nAfter we select which features will be used, we will train and test several individual models (as opposed to ensemble models) and compare the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: TRAIN_PCT is defind as a constant, but we could also create a list and have it loop through various percentages\n\n# (1) SELECT | ALL features\nX = traffic.iloc[:, 0:(traffic.shape[1]-1)]\nY = traffic.iloc[:, -1]\n\nprint(f'[*] Beginning evaluations: All Features')\nevaluateIndividualClassifiers(X, Y, TRAIN_PCT)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: TRAIN_PCT is defind as a constant, but we could also create a list and have it loop through various percentages\n\n# (2) SELECT | choose from ALL features\nselected_features = selectFeatures(X, Y)\nprint(f'Selected Features \"from All\": {selected_features}')\nXse_all = X[selected_features]\n\nprint(f'[*] Beginning evaluations: Selected Features (from \"All Features\")\"')\nevaluateIndividualClassifiers(Xse_all, Y, TRAIN_PCT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: TRAIN_PCT is defind as a constant, but we could also create a list and have it loop through various percentages\n\n# (3) SELECT | ALL features except 'Src Port' and 'Dst Port'\nX_noPorts = X.drop('Src Port', axis=1, inplace=False)\nX_noPorts = X_noPorts.drop('Dst Port', axis=1, inplace=False)\n\nprint(f'[*] Beginning evaluations: All Features, except Src Port and Dst Port')\nevaluateIndividualClassifiers(X_noPorts, Y, TRAIN_PCT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: TRAIN_PCT is defind as a constant, but we could also create a list and have it loop through various percentages\n\n# (4) SELECT | choose from ALL features except 'Src Port' and 'Dst Port'\nselected_features = selectFeatures(X_noPorts, Y)\nprint(f'Selected Features \"from All w/o Ports\": {selected_features}')\nXse_noPorts = X_noPorts[selected_features]\n\nprint(f'[*] Beginning evaluations: Selected Features (from \"All Features, except Src Port and Dst Port\")')\nevaluateIndividualClassifiers(Xse_noPorts, Y, TRAIN_PCT)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}