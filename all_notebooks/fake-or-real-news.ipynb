{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"+4\" color=teal><u><center>Fake News Classifier </center></u></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of contents</h3>\n\n* [Introduction](#intro)\n* [Data cleaning and Feature extraction](#data)\n* [1.  Title - Word Clouds ](#1)\n* [2.  Length - Title/Text ](#2)\n* [3.  Ngrams - Title words](#3)\n* [4.  Removal of stopwords](#4)\n* [5.  Count Vectorizer](#5)\n* [6.  Passive Aggressive Classifier Classifier for CountVectorizer](#6)\n* [7.  Hyper Paramterization with Multinomial NB for CountVectorizer](#7)\n* [8.  TfidfVectorizer](#8)\t\n* [9.  Hyperparameterization (with MultinomialNB) for TfidfVectorizer](#11)\n* [11. PassiveAggressiveClassifier for TfidfVectorizer](#12)\n* [12. Hashing Vectorizer](#13)\n* [13. Comparison Table](#14)\n* [14. LSTM](#15)\n    \n"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"intro\"></a>\n<font size=\"+2\" color=\"blue\"><b>Introduction and Imports</b></font><br>\n\n<font size=\"+1\" color=\"magenta\">\nThere are 2 files one which has true news and the other fake news.\n</font>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport re\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom plotly.subplots import make_subplots\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dropout\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\ntf.__version__\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"fake = pd.read_csv(\"/kaggle/input/fake-news-detection/Fake.csv\", parse_dates=['date'])\ntrue = pd.read_csv(\"/kaggle/input/fake-news-detection/True.csv\", parse_dates=['date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data\"></a>\n<font size=\"+2\" color=\"blue\"><b>Cleaning data</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fake.info())\nprint(fake.head())\nprint(fake['subject'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake[fake['date']==\"https://100percentfedup.com/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier/\"]\nfake.loc[9358]['date'] = 'December 31, 2017'\n\n\nfake[fake['date']==\"https://100percentfedup.com/video-hillary-asked-about-trump-i-just-want-to-eat-some-pie/\"]\nfake.loc[15507]['date'] = 'December 29, 2017'\n\nfake[fake['date']==\"https://100percentfedup.com/12-yr-old-black-conservative-whose-video-to-obama-went-viral-do-you-really-love-america-receives-death-threats-from-left/\"]\nfake.loc[15508]['date'] = 'December 30, 2017'\n\n \nfake[fake['date']==\"https://fedup.wpengine.com/wp-content/uploads/2015/04/hillarystreetart.jpg\"]\nfake.loc[15839]['date'] = 'December 30, 2017'\nfake.loc[17432]['date'] = 'December 26, 2017'\nfake.loc[21869]['date'] = 'December 25, 2017'\n\n \nfake[fake['date']==\"https://fedup.wpengine.com/wp-content/uploads/2015/04/entitled.jpg\"]\nfake.loc[15840]['date'] = 'December 29, 2017'\nfake.loc[17433]['date'] = 'December 28, 2017'\nfake.loc[21870]['date'] = 'December 27, 2017'\n\nfake[fake['date']==\"MSNBC HOST Rudely Assumes Steel Worker Would Never Let His Son Follow in His Footsteps…He Couldn’t Be More Wrong [Video]\"]\nfake.loc[18933]['date'] = 'December 24, 2017'\nfake['date'] = pd.to_datetime(fake['date'], dayfirst = True)\nprint(\"Fake News dates: \",fake['date'].min(), fake['date'].max())\nprint(\"True News dates: \",true['date'].min(), true['date'].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(true.info())\nprint(true.head())\nprint(true['subject'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\n# Data cleaning\ndef remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    #string=\"\".join(line)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake['refine_text']=fake['text'].str.lower()\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_tag(str(x)))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_mention(str(x)))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_hash(str(x)))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_newline(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_url(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_number(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_punct(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:text_strip(x))\nfake['text_length']=fake['refine_text'].str.split().map(lambda x: len(x))\n\ntrue['refine_text']=true['text'].str.lower()\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_tag(str(x)))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_mention(str(x)))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_hash(str(x)))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_newline(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_url(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_number(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_punct(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:text_strip(x))\ntrue['text_length']=true['refine_text'].str.split().map(lambda x: len(x))\n\nfake['refine_title']=fake['title'].str.lower()\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_tag(str(x)))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_mention(str(x)))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_hash(str(x)))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_newline(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_url(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_number(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_punct(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:text_strip(x))\nfake['title_length']=fake['refine_title'].str.split().map(lambda x: len(x))\n\ntrue['refine_title']=true['title'].str.lower()\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_tag(str(x)))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_mention(str(x)))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_hash(str(x)))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_newline(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_url(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_number(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_punct(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:text_strip(x))\ntrue['title_length']=true['refine_title'].str.split().map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font size=\"+2\" color=\"blue\"><b>Title Word Clouds</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax2) = plt.subplots(1,1,figsize=[17, 10])\nwordcloud2 = WordCloud(background_color='black',colormap=\"terrain_r\",width=800,height=400).generate(\" \".join(fake['title']))\n\nax2.imshow(wordcloud2,interpolation='bilinear')\nax2.axis('off')\nax2.set_title('Fake News - Most Used Words in Title',fontsize=35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax2) = plt.subplots(1,1,figsize=[17, 10])\nwordcloud2 = WordCloud(background_color='white',colormap=\"spring\", width=800,height=400).generate(\" \".join(fake['title']))\n\nax2.imshow(wordcloud2,interpolation='bilinear')\nax2.axis('off')\nax2.set_title('True News - Most Used Words in Title',fontsize=35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Average length of True News  : {}\".format(round(true['text_length'].mean(),2)))\nprint(\"Average length of Fake News  : {}\".format(round(fake['text_length'].mean(),2)))\nprint(\"Average title length of True News  : {}\".format(round(true['title_length'].mean(),2)))\nprint(\"Average title length of Fake News  : {}\".format(round(fake['title_length'].mean(),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font size=\"+2\" color=\"blue\"><b>Title/Text Length of True/Fake news</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=true['title_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='magenta', opacity=0.6,name=\"True\", x0='True News'))\nfig.add_trace(go.Violin(y=fake['title_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='skyblue', opacity=0.6,name=\"Fake\", x0='Fake News') )\n\nfig.update_traces(box_visible=False, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - News Title Length\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=true['text_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='green', opacity=0.6,name=\"True\", x0='True News'))\nfig.add_trace(go.Violin(y=fake['text_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='red', opacity=0.6,name=\"Fake\", x0='Fake News') )\n\nfig.update_traces(box_visible=True, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - News Length\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font size=\"+2\" color=\"blue\"><b>Ngrams of True/Fake news titles</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(true['refine_title'],(1,1),20)\nbigram_df=ngram_df(true['refine_title'],(2,2),20)\ntrigram_df=ngram_df(true['refine_title'],(3,3),20)\n\nunigram_fake_df=ngram_df(fake['refine_title'],(1,1),20)\nbigram_fake_df=ngram_df(fake['refine_title'],(2,2),20)\ntrigram_fake_df=ngram_df(fake['refine_title'],(3,3),20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top True News N Grams',xaxis_title=\" \",yaxis_title=\" \", showlegend=False,title_x=0.5,height=1200,template=\"plotly_white\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_fake_df['text'][::-1],\n    x=unigram_fake_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_fake_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_fake_df['text'][::-1],\n    x=bigram_fake_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_fake_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_fake_df['text'][::-1],\n    x=trigram_fake_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_fake_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top Fake titles N Grams',xaxis_title=\" \",yaxis_title=\" \", showlegend=False,title_x=0.5,height=1200,template=\"seaborn\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true['label'] = 1\nfake['label'] = 0\nnews = pd.concat([true,fake],ignore_index=True)\ny = news['label']\nnews = news.drop(['label'],axis = 1)\nnews\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font size=\"+2\" color=\"blue\"><b>Removal of Stopwords</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    ps = PorterStemmer()    \n    #review = [ps.stem(word) for word in text.split() if not word in stopwords.words('english')]    \n    review = [word for word in text.split() if not word in stopwords.words('english')]    \n    review = \" \".join(review)\n    return review\n\n\ncorpus = news['refine_text'].apply(lambda x:remove_stopwords(x))\n#corpus = news['refine_text'].values\ncorpus[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font size=\"+2\" color=\"blue\"><b>Count Vectorizer</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport itertools\n\ncv = CountVectorizer(max_features = 500, ngram_range=(1,3))\nX = cv.fit_transform(corpus).toarray()\nfeature_names = cv.get_feature_names()\nprint(\"Feature Names: \",feature_names[:20])\nprint(\"X shape: \",X.shape)\nprint(\"Get Params: \",cv.get_params())\n\nX_train, X_test, y_train,y_test = train_test_split(X,y, test_size=0.33, random_state=10)\nprint(\"X_train.shape, X_test.shape, y_train.shape, y_test.shape: \",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\nclassifier = MultinomialNB()\nclassifier.fit(X_train,y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test,pred)\nprint(\"Score: \",score)\n\ncm = metrics.confusion_matrix(y_test,pred)\nplot_confusion_matrix(cm, classes = ['FAKE', 'TRUE'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n<font size=\"+2\" color=\"blue\"><b>Hyper Parameterization (MultinomialNB) for CountVectorizer </b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\npt = PrettyTable()\npt.field_names = [\"S No.\", \"Alpha Value\", \"Score\"]\n\nprevious_score = 0\ni=1\n\n# Hyperparameters with MultinomialNB\nfor alpha in np.arange(0,1,0.1):\n    sclf = MultinomialNB(alpha = alpha)\n    sclf.fit(X_train,y_train)\n    pred = sclf.predict(X_test)\n    score = metrics.accuracy_score(pred,y_test)\n    if score > previous_score:\n        clf = sclf\n    #print(\"Alpha: {}, Score: {} \".format(alpha, score))\n    pt.add_row([i,round(alpha,1),round(score,3)])\n    i = i+1\n    \nprint(pt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most True\nsorted(zip(clf.coef_[0], feature_names),reverse=True)[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most Fake\nsorted(zip(clf.coef_[0], feature_names),reverse=False)[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<font size=\"+2\" color=\"blue\"><b>Passive Aggressive Classifier for CountVectorizer</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_clf = PassiveAggressiveClassifier()\n\nlinear_clf.fit(X_train,y_train)\npred = linear_clf.predict(X_test)\nscore = metrics.accuracy_score(y_test,pred)\nprint(\"Score: \",score)\ncm = metrics.confusion_matrix(y_test,pred)\nplot_confusion_matrix(cm, classes = ['FAKE data', 'TRUE data'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<font size=\"+2\" color=\"blue\"><b>TfidfVectorizer</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# create the transform\ntfidf = TfidfVectorizer(max_features=500,ngram_range=(1,3))\n\n# encode document\nX = tfidf.fit_transform(corpus)\n\n## Divide the dataset into Train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf.get_feature_names()[:20])\ntfidf.get_params()\n\nclf_tf = MultinomialNB()\nclf_tf.fit(X_train, y_train)\npred_tf = clf_tf.predict(X_test)\nscore_tf = metrics.accuracy_score(y_test, pred_tf)\nprint(\"accuracy:   %0.3f\" % score_tf)\ncm = metrics.confusion_matrix(y_test, pred_tf)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most True\nfeature_names_tf = tfidf.get_feature_names()\nsorted(zip(clf_tf.coef_[0], feature_names_tf),reverse=True)[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most Fake\nfeature_names_tf = tfidf.get_feature_names()\nsorted(zip(clf_tf.coef_[0], feature_names_tf),reverse=False)[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<font size=\"+2\" color=\"blue\"><b>Hyperparameterization (with MultinomialNB) for TfidfVectorizer</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"pta = PrettyTable()\npta.field_names = [\"S No.\", \"Alpha Value\", \"Score\"]\n\n# Hyperparameters with MultinomialNB (fitted with TF-IDF)\n\nprevious_score = 0\nfor alpha in np.arange(0,1,0.1):\n    sclf = MultinomialNB(alpha = alpha)\n    sclf.fit(X_train,y_train)\n    pred = sclf.predict(X_test)\n    score = metrics.accuracy_score(pred,y_test)\n    if score > previous_score:\n        clf = sclf\n    pta.add_row([i,round(alpha,3),round(score,3)])\n    \nprint(pta)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n<font size=\"+2\" color=\"blue\"><b>PassiveAggressiveClassifier for TfidfVectorizer</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_clf = PassiveAggressiveClassifier()\n\nlinear_clf.fit(X_train, y_train)\npred = linear_clf.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n<font size=\"+2\" color=\"blue\"><b>Hashing Vectorizer</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\n\n\n# create the transform\nhashVec = HashingVectorizer(n_features=1000, alternate_sign=False)\n\n# encode document\nX = hashVec.fit_transform(corpus.values)\n\n## Divide the dataset into Train and Test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n\nclf_hash = MultinomialNB()\nclf_hash.fit(X_train, y_train)\npred_hash = clf_hash.predict(X_test)\nscore_hash = metrics.accuracy_score(y_test, pred_hash)\nprint(\"accuracy:   %0.3f\" % score_hash)\ncm = metrics.confusion_matrix(y_test, pred_hash)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n<font size=\"+2\" color=\"blue\"><b>Comparison of various BOW methods </b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\nx = PrettyTable()\n\nx.field_names = [\"S No.\", \"Vectorizer\", \"Accuracy\"]\n\nx.add_row([\"1\",\"CountVectorizer\", 0.956])\nx.add_row([\"2\",\"PassiveAggressiveClassifier - CountVectorizer\", 0.994])\nx.add_row([\"3\",\"TfidfVectorizer\", 0.946])\nx.add_row([\"4\",\"PassiveAggressiveClassifier - TfidfVectorizer\", 0.982])\nx.add_row([\"5\",\"HashingVectorizer\", 0.94])\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a>\n<font size=\"+2\" color=\"blue\"><b>LSTM</b> </font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Vocabulary size\nvoc_size=5000\n\nonehot_repr=[one_hot(words,voc_size)for words in corpus] \nprint(onehot_repr[0])\nsent_length=20\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs[0])\nlen(embedded_docs),y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating model\nembedding_vector_features = 40\nmodel = Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_final=np.array(embedded_docs)\ny_final=np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n\n### Finally Training\nhistory = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\ny_pred = model.predict_classes(X_test)\nprint(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score: \",accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating model - Bidirectional, no dropout\nembedding_vector_features=40\nmodel2 = Sequential()\nmodel2.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel2.add(Bidirectional(LSTM(100)))\nmodel2.add(Dense(1,activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(embedded_docs),y.shape)\nX_final=np.array(embedded_docs)\ny_final=np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n\n# Training\nhistory2 = model2.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\ny_pred2=model2.predict_classes(X_test)\n\nprint(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred2))\nprint(\"Accuracy Score: \",accuracy_score(y_test,y_pred2))\nprint(classification_report(y_test,y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating model\nembedding_vector_features=40\nmodel1=Sequential()\nmodel1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel1.add(Bidirectional(LSTM(100)))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(1,activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(embedded_docs),y.shape)\nX_final=np.array(embedded_docs)\ny_final=np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n\n# Training\nmodel1.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\ny_pred1=model1.predict_classes(X_test)\n\nprint(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred1))\nprint(\"Accuracy Score: \",accuracy_score(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from prettytable import PrettyTable\nx = PrettyTable()\n\nx.field_names = [\"S No.\", \"Deep Learning\", \"Accuracy\"]\n\nx.add_row([\"1\",\"LSTM\", 0.9433])\nx.add_row([\"2\",\"BiDirectional LSTM\", 0.9431])\nx.add_row([\"3\",\"BiDirectional LSTM + Dropout\", 0.9367])\n\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Friends, if you like my notebook. Please upvote this notebook.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}