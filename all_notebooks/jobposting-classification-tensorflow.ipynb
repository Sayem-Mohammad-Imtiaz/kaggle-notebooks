{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nHello people, welcome to this kernel. In this kernel I am going to classify jobpostings whether they are real or not. This dataset is small and you can handle that using traditional approachs (BoW,TF-IDF) but in this kernel I'll use word embeddings and RNNs.\n\n# Table of Content\n1. Data Preprocessing\n1. Building Model\n1. Training Model\n1. Testing Model\n1. Conclusion"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\n\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndata = pd.read_csv('../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nIn this section I am going to prepare dataset in order to use in our neural network. Before starting, let's check the dataframe and class distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see there are too many features in the dataset, we'll use company profile, description and requirements. And also fraudulent."},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nsns.countplot(data[\"fraudulent\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see most of the dataset is 0 (non-fraudulent) so we can consider this mission hard."},{"metadata":{},"cell_type":"markdown","source":"### Part 1: Concatenating Text Parts\nFirst, we'll start with dropping redundant features and concatenate others."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.loc[:,[\"company_profile\",\"description\",\"requirements\",\"benefits\"]]\ny = data[\"fraudulent\"]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As we've seen from information table, there are NaN values in the set. We'll fill them with spaces."},{"metadata":{"trusted":true},"cell_type":"code","source":"x.fillna(\" \",inplace=True)\nx.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's concatenate texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_data = []\nfor i in range(len(x)):\n    txt = x[\"company_profile\"][i] + \" \"\n    txt = txt + x[\"description\"][i] + \" \"\n    txt = txt + x[\"requirements\"][i] + \" \"\n    txt = txt + x[\"benefits\"][i]\n    concat_data.append(txt.strip())\n\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's check our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_data[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are too many information here, let's move on to the next step."},{"metadata":{},"cell_type":"markdown","source":"### Part 2: Cleaning Texts Using Regular Expressions\nAs you can see in the texts there are too many redundant characters such as punctuation steps. In this part we'll clear texts using regular expressions."},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = \"[^a-zA-Z]\"\ncleanedTexts = []\nfor text in concat_data:\n    text = re.sub(pattern,\" \",text)\n    cleanedTexts.append(text.lower())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleanedTexts[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part 3: Tokenizing and Padding\nYou know, in natural languages words are the representation of everything, such as we say *hi* when we see someone, h and i letters don't have any special meaning but hi has a special meaning. If 1 means hi, we can use it instead of hi.\n\nIn this part we'll convert words into integers and texts into sequences. \nIn deep learning we generally use dataset that has predefined shape. But in text dataset shapes might be different, such as one jobposting can have 100 words other can have 102 words. In order to solve this problem we can use different approaches, but in this kernel we'll use **padding**\n\nIn padding we will add some spaces to the texts and make all texts with same shape."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(cleanedTexts)\n\nx_tokens = tokenizer.texts_to_sequences(cleanedTexts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_tokens[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now let's create a sequence that includes length of arrays and find Q3 value."},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_lens = [len(seq) for seq in x_tokens]\nq3 = np.quantile(seq_lens,.75)\nprint(q3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All texts will have shape 502"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tokens_pad = np.asarray(pad_sequences(x_tokens,maxlen=int(q3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tokens_pad.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4: Train Test Splitting\nIn this section we'll split the dataset into train and test, to test dataset truly."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(x_tokens_pad,y,test_size=0.2,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Model\nIn this section I am going to build the model using keras API of Tensorflow. I'll use the developed version of RNNs, GRU, Gated Recurrent Unit. We don't use SimpleRNN, because it has a problem called *vanishing gradient* because of backpropagation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = 10000 + 1\nVEC_SIZE = 100\nTOKEN_SIZE = int(q3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.compat.v1.keras.layers import CuDNNGRU\nmodel = keras.Sequential()\nmodel.add(layers.Embedding(input_dim=VOCAB_SIZE,\n                           output_dim=VEC_SIZE,\n                           input_length=TOKEN_SIZE\n                          ))\n\n\n\nmodel.add(CuDNNGRU(512,return_sequences=True))\nmodel.add(CuDNNGRU(1024,return_sequences=True))\nmodel.add(CuDNNGRU(2048))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Model\nIn this section I am going to train model using prepared dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(x_train,y_train,validation_split=0.2,epochs=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing Model\nIn this section we'll test model using unused test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = np.asarray(y_test)\ny_pred = model.predict_classes(x_test)\n\nprint(\"Accuracy score of model is {}%\".format(accuracy_score(y_pred=y_pred,y_true=y_test)*100))\n\nplt.subplots(figsize=(4,4))\nconf_matrix = confusion_matrix(y_pred=y_pred,y_true=y_test)\nsns.heatmap(conf_matrix,annot=True,fmt=\".1f\",linewidths=1.5)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nThanks for your attention, if you have questions in your mind, feel free to ask in comment section. Also if you liked the kernel and upvote, I would be glad :)\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}