{"metadata":{"language_info":{"version":"3.6.2","mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"cells":[{"metadata":{},"cell_type":"markdown","source":"### Read the data."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"import numpy as np\nimport pandas as pd","execution_count":1},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"data = pd.read_csv('../input/Dataset_spine.csv')","execution_count":2},{"metadata":{},"cell_type":"code","outputs":[],"source":"data.head()","execution_count":3},{"metadata":{},"cell_type":"markdown","source":"### Drop the unnecessary field and map the 'Class attr' to categorical values."},{"metadata":{},"cell_type":"code","outputs":[],"source":"data.drop(['Unnamed: 13'], axis=1, inplace=True)\n\ndata.head()","execution_count":4},{"metadata":{},"cell_type":"code","outputs":[],"source":"data['Class_att'] = data['Class_att'].map({'Abnormal': 1, 'Normal': 0})\n\ndata.head()","execution_count":5},{"metadata":{},"cell_type":"markdown","source":"### Renaming the column."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"data = data.rename(columns={'Col1': 'pelvic_incidence', \n                            'Col2': 'pelvic_tilt', \n                            'Col3': 'lumbar_lordosis_angle', \n                            'Col4': 'sacral_slope', \n                            'Col5': 'pelvic_radius', \n                            'Col6': 'degree_spondylolisthesis', \n                            'Col7': 'pelvic_slope', \n                            'Col8': 'direct_tilt', \n                            'Col9': 'thoracic_slope', \n                            'Col10': 'cervical_tilt', \n                            'Col11': 'sacrum_angle', \n                            'Col12': 'scoliosis_slope', \n                            'Class_att': 'class'})","execution_count":6},{"metadata":{},"cell_type":"code","outputs":[],"source":"data.head()","execution_count":7},{"metadata":{},"cell_type":"code","outputs":[],"source":"data.info()","execution_count":8},{"metadata":{},"cell_type":"code","outputs":[],"source":"data.describe()","execution_count":9},{"metadata":{},"cell_type":"markdown","source":"### Basic exploratory data analysis to find correlation between different features"},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":10},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nsns.set_style('whitegrid')","execution_count":11},{"metadata":{},"cell_type":"code","outputs":[],"source":"plt.figure(figsize=(12,9))\nsns.heatmap(data.corr(), annot=True)","execution_count":12},{"metadata":{},"cell_type":"code","outputs":[],"source":"sns.pairplot(data, hue='class', palette='Set1')","execution_count":13},{"metadata":{},"cell_type":"markdown","source":"### Data seems to be distributed quite randomly. No noticeable relationship between different features. Additional exploratory analysis might be useful."},{"metadata":{},"cell_type":"code","outputs":[],"source":"sns.countplot(x='class', data=data, palette='Set2')","execution_count":14},{"metadata":{},"cell_type":"markdown","source":"### 'Abnormal' class is almost double of 'Normal' class. Over-sampling or under-sampling techniques to handle imbalanced data can be used."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","execution_count":15},{"metadata":{},"cell_type":"markdown","source":"### Normalising the data."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"scaler = StandardScaler()\n\ny = data['class'].values\nX = scaler.fit_transform(data[data.columns[:-1]])","execution_count":16},{"metadata":{},"cell_type":"markdown","source":"### Using PCA to reduce dimensionality of the data."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"var = []\nfor n in range(1, 12):\n    pca = PCA(n_components=n)\n    pca.fit(X)\n    var.append(np.sum(pca.explained_variance_ratio_))","execution_count":17},{"metadata":{},"cell_type":"code","outputs":[],"source":"plt.figure(figsize=(10,6))\nplt.plot(range(1,12), var, color='red', linestyle='dashed', marker='o', markerfacecolor='black', markersize=10)\nplt.title('Variance vs. Components')\nplt.xlabel('Components')\nplt.ylabel('Variance')","execution_count":18},{"metadata":{},"cell_type":"markdown","source":"### After plotting retained variance against number of components, we can see that 85% and above variance is retained only if we keep principal components greater than 8."},{"metadata":{},"cell_type":"markdown","source":"### So, we won't apply PCA to the data."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"from sklearn.model_selection import train_test_split","execution_count":19},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)","execution_count":20},{"metadata":{},"cell_type":"markdown","source":"### Split the data and feed it to the TPOT classifer, which select the best pipeline for the data"},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"from tpot import TPOTClassifier","execution_count":21},{"metadata":{},"cell_type":"code","outputs":[],"source":"pipeline = TPOTClassifier(generations=20, population_size=100, cv=5, n_jobs=-1, random_state=101, verbosity=2)","execution_count":22},{"metadata":{},"cell_type":"code","outputs":[],"source":"pipeline.fit(X_train, y_train)","execution_count":23},{"metadata":{},"cell_type":"code","outputs":[],"source":"pipeline.score(X_test, y_test)","execution_count":24},{"metadata":{},"cell_type":"markdown","source":"### ~87% accuracy using Logistic Regression."},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"from sklearn.metrics import classification_report, confusion_matrix","execution_count":25},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"y_pred = pipeline.predict(X_test)","execution_count":26},{"metadata":{},"cell_type":"code","outputs":[],"source":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":27},{"metadata":{},"cell_type":"markdown","source":"### Training a deep neural network ."},{"metadata":{},"cell_type":"code","outputs":[],"source":"import keras\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Sequential","execution_count":28},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"model = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(12,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":29},{"metadata":{},"cell_type":"markdown","source":"### Add the layers to network. Also add dropout layer to avoid over-fitting."},{"metadata":{},"cell_type":"code","outputs":[],"source":"model.summary()","execution_count":30},{"metadata":{},"cell_type":"code","outputs":[],"source":"model.fit(X_train, y_train, batch_size=32, epochs=1000, verbose=2, validation_split=0.2)","execution_count":31},{"metadata":{},"cell_type":"code","outputs":[],"source":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":32},{"metadata":{},"cell_type":"markdown","source":"### ~88% accuracy. Maybe using more units, different optimizer can give us better accuracy."},{"metadata":{},"cell_type":"markdown","source":""}],"nbformat_minor":1}