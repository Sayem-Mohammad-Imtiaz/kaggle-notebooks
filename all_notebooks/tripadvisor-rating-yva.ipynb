{"cells":[{"metadata":{},"cell_type":"markdown","source":"##### Образовательная платформа: SkillFactory\n##### Специализация: Data Science\n##### Группа: DST-17\n### Юнит 3. Проект: \"О вкусной и здоровой пище\"\n#### Выполнил: Владимир Юшманов\n\n![title](https://raw.githubusercontent.com/vyushmanov/skillfactory_rds/master/module_3/rating_pic.jpg)\n\n![title](https://raw.githubusercontent.com/vyushmanov/skillfactory_rds/master/module_3/TA_logo.JPG)\n\n\n# 1. Импорт и объединение данных\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport re \nimport plotly\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom scipy.stats import ttest_ind\nfrom itertools import combinations\nfrom collections import Counter\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import MinMaxScaler\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Загружаем набор собственных функций\nimport myfunction as mf\n\n# Сервисные функции\npd.set_option('display.max_rows', 50) # выведем больше строк\npd.set_option('display.max_columns', 100) # выведем больше колонок\nimport warnings; warnings.simplefilter('ignore') #  отключение вывода предупреждающих сообщений\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# фиксируйте RANDOM_SEED и версию пакетов, чтобы эксперименты были воспроизводимы:\nRANDOM_SEED = 42\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Читаем датасеты\nDATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR + '/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR + 'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR + '/sample_submission.csv')\n\n# ВАЖНО! для корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n\n# Выводим сводку о содержании датасета\nmf.brief_summary(df_train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Состав признаков. Описание:\n* ____ - дата запроса, временная переменная, требует обработки.\nобразование - уровень образования, категориальная переменная, требует обработки и исправления пропущенных значений.\nпол - двоичная переменная, требует обработки.\nвозраст - непрерывная переменная, требует обработки.\ncar - наличие машины, двоичная переменная, требует обработки.\ncar_type - наличие иномарки, переменная двоичная, требует обработки.\nDec_app_cnt - количество отклоненных запросов, непрерывная переменная.\ngood_work - признак хорошо оплачиваемой работы, двоичная переменная.\nscore_bki - внутренний рейтинг BKI (агентство кредитной информации), непрерывная переменная.\nbki_request_cnt - количество запросов в БКИ (агентство кредитной информации), непрерывная переменная.\nregion_rating - рейтинг региона, категориальная переменная.\nhome_address - классификатор домашнего адреса, категориальная переменная.\nwork_address - классификатор рабочих адресов, категориальная переменная.\nдоход - уровень дохода клиента, непрерывная переменная.\nsna - уровень связи с другими клиентами, категориальная переменная.\nfirst_time - сколько времени клиент находится в базе данных, категориальная переменная.\nforeign_passport - наличие паспорта, двоичная переменная, требует обработки.\ndefault - по умолчанию в прошлом, двоичная целевая переменная.\n\nКак видим, большинство признаков у нас требует очистки и предварительной обработки.\n\nПризнак __ID_TA__ имеет количество уникальных значений меньшее, чем число строк датасета. Удалим дубликаты."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data = mf.drop_dublle(data, ['ID_TA', 'sample'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Разведывательный анализ данных (EDA)\n## 2.1. Restaurant_id \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# вывод структуры уникальных и сетевых ресторанов\n# Добавление столбца с количеством ресторанов в сети\n    \nmf.view_count_in_chain(data)\ndata = mf.calc_count_in_chain(data) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. City\n\n>### 2.2.1. Основной признак"},{"metadata":{"trusted":true},"cell_type":"code","source":"# строка преобразована в список\ndata = mf.string_to_list_distribution(data, 'City')\n# выведем распределение ресторанов по городам, сохраним общую численность ресторанов в городе в признаке restorans_in_city \ndata = mf.view_horiz_bar_n_table(data, 'City', 'restaurants_in_city') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">###  2.2.1. Дополнительные признаки, связанные с локализацией ресторанов"},{"metadata":{"trusted":true},"cell_type":"code","source":"# выделены часто встречающиеся и редкие значения City\n\n# из внешних источников датасет дополнен сведениями city_is_the_capital, population_city, country\ndata = mf.city_expansion_features(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3. Cuisine Style"},{"metadata":{"trusted":true},"cell_type":"code","source":"# заполним пропуски значением 'Empty'\n# создадим признак empty_cuisine_style, в котором '1' обозначим пустые значения Cuisine Style, для остальных применим '0'\n\n# закодируем значения в переменной до их преобразования - признак code_cuisine_style\n\n# преобразуем строку с перечислением в список - признак list_cuisine_style\n\n# выделим редко встречающиеся кухни, обозначим unique_style. Считаем, что редкие кухни - последние 23 (встречаются реже 20 раз)\n\n# посчитаем количество заявленных кухонь - признак count_cuisine_style. В случае отсутствия информации\n# о количестве кухонь используем медианное значение\n# выведем гистограммы и боксплоты для линейных и логарифмических значений признака\n\n# добавим признак, рассчитанный как натуральный логарифм количества кухонь\n    \n# выделим редко встречающиеся кухни. Последнее количество unique_border - уникальные кухни.\ndata = mf.string_to_list_distribution(data, 'Cuisine Style')\ndata = mf.cuisine_distribution(data, 'list_cuisine_style')\ndata = mf.rife_rare_distribution(data, 'list_cuisine_style',.2,.03)\ndata = mf.localisation_cuisine_country(data) # идею позаимствовал у (с)Rezinko Mikhail\ndata = mf.view_histogrm_n_boxplot(data, 'count_cuisine_style')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# определение наиболее частых и наиболее редких вариантов признака\ndata = mf.rife_rare_distribution(data, 'list_cuisine_style', .25, .02)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4. Ranking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на распределение признака в 10 крупнейших городах:\nmf.view_attribute_based_distribution(data, 'Ranking', 'City', 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# произведем сквозное ранжирование равномерно распределив ранг ресторанов в рамках города\n# значение нового признака сохраним в total_ranking\n\n# в качестве альтернативного способа преобразования признака используем стандартизацию и сохраняем результат в standard_ranking\n\ndata = mf.ranking_distribution(data)\ndata = mf.add_ranking_distribution(data)\n\nmf.view_attribute_based_distribution(data, 'total_ranking', 'City', 10)\nmf.view_attribute_based_distribution(data, 'standard_ranking', 'City', 10)\nmf.view_attribute_based_distribution(data, 'norm_ranking_on_population', 'City', 10)\nmf.view_attribute_based_distribution(data, 'norm_ranking_on_tourists', 'City', 10)\nmf.view_attribute_based_distribution(data, 'norm_ranking_on_max_rank', 'City', 10)\nmf.view_attribute_based_distribution(data, 'norm_ranking_on_restaurant', 'City', 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Полученные признаки проверим на коррекляцию с ключевой переменной, после чего примем решение о их использовании в модели. Предполагаю, что лучший результат покажет признак standard_ranking\n## 2.5. Rating"},{"metadata":{},"cell_type":"markdown","source":"\n## 2.6. Price Range"},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим признак empty_price_range, в котором '1' обозначим пустые значения Price Range, для остальных применим '0'\n\n# перекодируем признак по словарю {'$':1, '$$ - $$$':2, '$$$$':3}\n\n# заполним пропущенные значения модой, т.е. 2\n\ndata = mf.price_distribution(data, 2)\nmf.view_price_info(data)\ndata = mf.mean_price_in_city(data)\nmf.view_mean_price(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.7. Number of Reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"# приведем наименование столбца к стандартному виду\ndata.rename(columns={'Number of Reviews': 'number_of_reviews'}, inplace=True)\n\n# зафиксируем строки с пустыми значениями\ndata['empty_number_of_reviews'] = pd.isna(data['number_of_reviews']).astype('float64')\n\n# вывод гистограмм и таблицы\ndata = mf.view_histogrm_n_boxplot(data, 'number_of_reviews')\n# добавлен признак, рассчитанный как натуральный логарифм номера ревю","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# исследуем вляние различных признаков на распределение log_number_of_reviews:\nmf.view_attribute_based_distribution(data, 'log_number_of_reviews', 'City', 5)\nmf.view_attribute_based_distribution(data, 'log_number_of_reviews', 'Price Range', 4)\nmf.view_attribute_based_distribution(data, 'log_number_of_reviews', 'count_cuisine_style', 8)\n\n# в целях детального изучения распределения вновь созданного признака log_number_of_reviews выведем подробную гистограмму и расчет выбросов \nmf.view_histogram_n_outliers(data, 'number_of_reviews', 'log', 160)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# выбросы зафиксированы в 10 наблюдениях, удалим их, предварительно сохранив информацию о них\ndata['outliers_number_of_reviews'] = pd.DataFrame(data['log_number_of_reviews'] > 8.56).astype('float64')\ndata.loc[data['log_number_of_reviews'] > 8.56, 'number_of_reviews']=None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Report from Number of Reviews\n> При исследовании влияния дополнительных признаков на распределение логарифма номера ревю установлено, что в более ранних выпусках ревю гораздо чаще встречается отсутствие упоминания о ценовой категории ресторана (Price Range).\nОстальные исследованные признаки не имеют выраженного влияния на распределение логарифмического признака номера ревю.\n\n> Выявлены и удалены 10 выбросов\n## 2.8. Reviews\n> ### 2.8.1. Обработка содержания отзывов"},{"metadata":{"trusted":true},"cell_type":"code","source":"# строковая переменная преобразована в словари используемых в отзывах слов, которые сохранены в признак review_words_list\n# строки с пустыми отзывами отмечены 1 в признаке empty_review\n\n# произведен подсчет слов, имеющих позитивную и негативную окраску. Количество таких слов сохранено в признаки count_pos_words\n# и count_neg_words соответственно. В случае присутствия в отзывах слова 'not' из количества негативных слов вычитается 1.\n\ndata = mf.review_text_distribution(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Работа с текстовой информацией признака Reviews завершена. Позднее будет произведено преобразование признака review_words_list, содержащего списки используемых в отзывах слов в dummy-переменную.\n> ### 2.8.2 Обработка информации о датах размещения отзывов"},{"metadata":{"trusted":true},"cell_type":"code","source":"# выделим из Reviews информацию о датах размещения отзывов\n# рассчитаем количество дней, прошедших между публикациями отзывов\n# определим для каждого ресторана, положение самого свежего отзыва на временном луче, нулевая отметка которого соответствует \n# дню выхода самой первой публикации \ndata = mf.data_review_distribution(data)\n\n# произведем сравнение распределений количества дней после новейшей публикации и периодом между публикациями\nfig = px.scatter(data[data['review_date_count'] == 2], x=\"review_date_min\", y=\"review_date_delta\",\n                marginal_x='histogram', marginal_y='histogram',\n                trendline='ols', trendline_color_override='darkblue')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = mf.view_histogrm_n_boxplot(data, 'review_date_min')\ndata = mf.view_histogrm_n_boxplot(data, 'review_date_delta')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# исследуем распределение признаков review_date_olded и review_date_delta    \nmf.view_histogram_n_outliers(data, 'review_date_min', 'all', 200)\nmf.view_histogram_n_outliers(data, 'review_date_delta', 'lin', 200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.9. Корреляционный анализ признаков\n> ### 2.9.1. Построение матрицы корреляций"},{"metadata":{"trusted":true},"cell_type":"code","source":"# бесконечности и пропуски заменены\ndata.replace(np.inf, 1, inplace=True)\ndata.replace(-np.inf, 0, inplace=True)\ndata = data.fillna(0)\n\nmf.show_heatmap(data[data['sample'] == 1].drop(['sample'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 2.9.2. Оптимизация признаков скоррелированных между собой и признаков не имеющих корреляции с целевой переменной"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Из признаков, полученных вокруг Ranking оставляем standard_ranking как имеющий наибольшую корреляцию\ndata.drop(['total_ranking'], axis=1, inplace=True, errors='ignore')\n\n# Из пар линейного значения и логарифма выбираем имеющие наибольшую корреляцию, остальные удаляем\ndata.drop(['log_count_cuisine_style', 'log_number_of_reviews', 'log_review_date_min', 'log_review_date_delta'], axis=1, inplace=True, errors='ignore')\n\n# Признак outliers_number_of_reviews не имеет корреляции с целевой переменной - удаляем\ndata.drop(['outliers_number_of_reviews'], axis=1, inplace=True, errors='ignore')\n\n# произведение модулей признаков empty_review и review_date_count, первичные признаки удалены\n#data['empty_review_date_count'] = data['empty_review'] + data['review_date_count']\n#data.drop(['empty_review'], axis=1, inplace=True, errors='ignore')\n\nmf.show_heatmap(data[data['sample'] == 1].drop(['sample'], axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 2.9.3. Применение метода главных компонент (PCA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#list_for_pca = ['empty_review', 'review_date_count', 'empty_number_of_reviews', 'count_review_words', 'code_review_words', 'number_of_reviews']\n#data = mf.pca_distribution(data, list_for_pca, 'pca_review')\n\n#list_for_pca = ['code_city', 'restaurants_in_city', 'population_city', 'code_country', 'count_city_tourists', 'count_in_chain', 'city_is_the_capital']\n#data = mf.pca_distribution(data, list_for_pca, 'pca_city', [0,1,1,0,0,1,0])\n\n#list_for_pca = ['code_cuisine_style', 'empty_cuisine_style', 'rare_cuisine_style', 'local_cuisine']\n#data = mf.pca_distribution(data, list_for_pca, 'pca_cuisine')\n\nlist_for_pca = ['Ranking', 'count_in_chain', 'standard_ranking', 'restaurants_in_city', 'population_city']\ndata = mf.pca_distribution(data, list_for_pca, 'pca_ranking', [1,0,1,0,0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.10 Создание дополнительных признаков (генерация фичей)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# количество ресторанов на 1000 человек населения города, первичные признаки удалены\n#data['restaurant_on_population'] = data['restaurants_in_city'] / data['population_city'] / 1000\n\n# число туристов на одного жителя города\n#data['tourists_on_population'] = data['count_city_tourists'] / data['population_city']\n\n# частное от деления Ranking на количество ресторанов в городе (restaurants_in_city)\n#data['ranking_on_count_restaurant'] = data['Ranking'] / data['restaurants_in_city']\n\n# количество отзывов на 10 000 жителей\n#data['review_on_population'] = data['number_of_reviews'] / data['population_city'] / 10000\n\n# количество отзывов на 100 000 туристов\n#data['review_on_tourists'] = data['number_of_reviews'] / data['population_city'] / 100000\n\n# отношение количества туристов к жителям\n#data['tourists_on_population'] = data['count_city_tourists'] / data['population_city']\n\n#data.drop(['city_is_the_capital', 'code_cuisine_style', 'local_cuisine', 'empty_number_of_reviews',\n#          'review_date_count', 'empty_review_date_count', 'count_neg_words'], axis=1, inplace=True, errors='ignore')\n\n#mf.show_heatmap(data[data['sample'] == 1].drop(['sample'], axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.11. Преобразование признаков в dummy-переменные"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = mf.prep_dummies(data, 'name_chain', 1, 'ch_')#\n#data = mf.prep_dummies(data, 'list_city')\n#data = mf.prep_dummies(data, 'list_country')\n#data = mf.prep_dummies(data, 'list_cuisine_style', .85, 'cs_')\n#positive_words, negative_words = mf.read_positive_words()\n#data = mf.prep_dummies(data, 'list_review_words', .3, 'w_', positive_words) # обработаны только слова из \"позитивного списка\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### Report Разведывательный анализ данных (EDA)\n> Испольование полного набора преобразованных и дополненных данных приводит к катастрофическому переобучению. В целях выявления оптимального набора признаков, обеспечивающих наилучшие условия для адекватного обучения модели, произведено поэлементное добавление признаков с оценкой влияния каждого признака на результат (сравнение MAE) и проверкой на переобучение.\n\n# 3. Предподготовка данных (Data Preprocessing)\n## 3.1. Процедуры преобразования данных\nДля удобства маневрирования методами преобразования данных, весь код, производящий предподготовку, локализован в этой ячейке. При работе с данными в настоящей ячеке не отображаются диаграммы и таблицы, что сокращает время обработки."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = mf.read_dataframes() # чтение файлов и формирование исходного датасета\ndata = mf.drop_dublle(data, ['ID_TA', 'sample']) # удаление дублей\ndata.rename(columns={'Number of Reviews': 'number_of_reviews'}, inplace=True)\n#data['empty_number_of_reviews'] = pd.isna(data['number_of_reviews']).astype('float64')\ndata = mf.string_to_list_distribution(data, 'City') # строка преобразована в список 'list_city'\n\ndata = mf.ranking_distribution(data)\ndata = mf.city_expansion_features(data)\ndata = mf.add_ranking_distribution(data)\n#list_for_pca = ['norm_ranking_on_max_rank', 'norm_ranking_on_restaurant', 'norm_ranking_on_population', 'norm_ranking_on_tourists']\n#data = mf.pca_distribution(data, list_for_pca, 'pca_norm_ranking', [1,1,0,0])\n\ndata = mf.calc_count_in_chain(data)\n#data = mf.prep_dummies(data, 'name_chain', 1, 'ch_') # преобразование признаков в dummy-переменные\ndata = mf.prep_dummies(data, 'list_city') # преобразование признаков в dummy-переменные\ndata = mf.review_text_distribution(data)\n#positive_words, negative_words = mf.read_positive_words()\n#data = mf.prep_dummies(data, 'list_review_words', .3, 'w_', positive_words) # обработаны только слова из \"позитивного списка\"\n\n#data = mf.price_distribution(data, 2)\n#data = mf.mean_price_in_city(data)\n\n#data = mf.string_to_list_distribution(data, 'Cuisine Style')\n#data = mf.cuisine_distribution(data, 'list_cuisine_style')\n#data = mf.rife_rare_distribution(data, 'list_cuisine_style',.3,.01)\n#data = mf.city_expansion_features(data)\n#data = mf.localisation_cuisine_country(data)\n#data = mf.prep_dummies(data, 'list_cuisine_style', .85, 'cs_')\n\n#data = mf.data_review_distribution(data)\n\n#data['outliers_date_min'] = pd.DataFrame(data['review_date_min'] > 1122.5).astype('float64')\n#data.loc[data['review_date_min'] > 1122.5, 'number_of_reviews']=None\n#data['outliers_date_delta'] = pd.DataFrame(data['review_date_delta'] > 355.5).astype('float64')\n#data.loc[data['review_date_delta'] > 355.5, 'number_of_reviews']=None\n\n#data['empty_number_of_reviews'] = pd.isna(data['number_of_reviews']).astype('float64')\n\n#data = mf.ranking_distribution(data)\n#list_for_pca = ['Ranking', 'standard_ranking', 'total_ranking']\n#data = mf.pca_distribution(data, list_for_pca, 'pca_ranking',[1,1,1])\n#data['ranking_power'] = data['Ranking']* data['Ranking']\n#data['ranking_copy'] = data['Ranking']\n\ndata['log_number_of_reviews'] = np.log1p(data['number_of_reviews'])\n#data['log_review_date_min'] = np.log1p(data['review_date_min'])\n#data['log_review_date_delta'] = np.log1p(data['review_date_delta'])\n\n#data['outliers_number_of_reviews'] = pd.DataFrame(data['log_number_of_reviews'] > 8.56).astype('float64')\n#data.loc[data['log_number_of_reviews'] > 8.56, 'number_of_reviews']=None\n\ndata.drop(['count_in_chain', 'code_review_words',\n           'count_pos_words', 'count_neg_words'], axis=1, inplace=True, errors='ignore')\ndata.drop(['norm_ranking_on_population', 'norm_ranking_on_tourists', 'total_ranking', 'standard_ranking', 'count_city_tourists'], axis=1, inplace=True, errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Стандартизация и удаление нечисловых признаков"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.fillna(0)\n# произведена мин-макс стандартизация (за исключением списка столбцов)\ndata = mf.normalisation(data, MinMaxScaler(), ['Rating', 'sample'])\n\ndf_preproc = mf.delete_string_sign(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Поэлементный контроль"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_preproc.sample(2))\ndisplay(data.describe().head(1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4. Выделение тестовой части датасета"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\n\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data[['Rating']]           # наш таргет\nX = train_data.drop(['Rating'], axis=1)\n\n# Воспользуемся специальной функцией train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Model \n## 4.1. Тестовое обучение"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\n\n# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)\ny_pred = np.array([5.0 if x>5 else x for x in list(np.round(y_pred * 2) / 2)])\n\n# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nMAE = metrics.mean_absolute_error(y_test, y_pred)\ntry: title = 'MAE: '+str(MAE)+' <-- '+str(old_MAE)\nexcept: title = 'MAE: '+str(MAE)\nold_MAE = MAE\n\nlayout =go.Layout(\n              autosize=False,\n              width=1000,\n              height=500)\nfig = go.Figure(layout = layout)\nfig.add_trace(go.Bar(x = model.feature_importances_, y = X.columns, orientation='h')), \nfig.update_layout(title = title, title_x = 0.5,\n                  yaxis={'categoryorder':'total descending'},\n                  margin = dict(l=200, r=100, t=50, b=0), showlegend=False)\nfig.update_yaxes(range=(-.5, 20.5))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Выбор оптимального набора признаков\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n# блок тестирования оптимального набора признаков\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nlist_importance_sign = list(feat_importances.nlargest(len(train_data.columns)-1).index)\nmin_MAE = round(MAE,3)\nprint(f\"min_MAE = {min_MAE}\")\nremove_list = []\nlog = []\ndelta =0.001\nfor i in range(0,len(list_importance_sign),1):\n    col = list_importance_sign[i]\n    print(f\"{i}.{col}\")\n#     ###\n    train_data = data.query('sample == 1').drop(['sample'], axis=1)\n    test_data = data.query('sample == 0').drop(['sample'], axis=1)\n\n    y = train_data.Rating.values            # наш таргет\n    X = train_data.drop(['Rating']+[col], axis=1)\n\n    # Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n    # выделим 20% данных на валидацию (параметр test_size)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n    print(test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape)\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    y_pred = np.array([5.0 if x>5 else x for x in list(np.round(y_pred * 2) / 2)])\n    temp_MAE = metrics.mean_absolute_error(y_test, y_pred)\n#     ###\n    print(temp_MAE)\n    log.append([col, temp_MAE])\n    if round(temp_MAE,3) <= min_MAE-delta:\n        remove_list.append(col)\n        print(f\"удаляем:= {col}\")\n    else:\n        print(f\"не удаляем:= {col}\")\nprint(f\"i={i}\")\nprint(f\"remove_list: {remove_list}\")\nprint(f\"log_list: {log}\")\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3. Обучение на оптимальном наборе признаков"},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраивает - готовим Submission на kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(['Rating'], axis=1)\npredict_submission = model.predict(test_data)\npredict_submission = np.array([5.0 if x>5 else x for x in list(np.round(predict_submission * 2) / 2)])\n\nsample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # What's next?\nИли что делать, чтоб улучшить результат:\n* Обработать оставшиеся признаки в понятный для машины формат\n* Посмотреть, что еще можно извлечь из признаков\n* Сгенерировать новые признаки\n* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n* Подобрать состав признаков\n\nВ общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}