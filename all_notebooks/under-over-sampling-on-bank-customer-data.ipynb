{"cells":[{"metadata":{},"cell_type":"markdown","source":"## What is the meaning of churn prediction?\nIt the technique using which we can predict or detect the customers who  are likely to cancel their subscription in the near future. \n\nThere are 3 ways using which businesses can generate revenue:\n1. Upsell to existing customers.\n2. Acquire new customers. \n3. Increase a customer retention.\n\nChurn prediction helps in focusing in the third point i.e. increase a customer retention in which we can predict the churn rate or those customers who are going to cancel the subscription and then take pro-active action before they leave so that they stay in business.\n\n### why we are doing this project?\nIn this project we will be creating a model using the bank past churn data of the cutomers and predict the future churn rate of customers so that we can know which cateogry of customer has has high churn rate and take a pro-active action inorder to make them stay in business.\n\n### What is churn rate?\nAccoring to wikipedia, The churn rate is the rate at which customers stop doing business with an entity. It is most commonly expressed as the percentage of service subscribers who discontinue their subscriptions within a given time period."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.offline as po\nimport plotly.graph_objs as go\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/bank-customer-churn-modeling/Churn_Modelling.csv\")\ndf.drop(['RowNumber','CustomerId'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA\n\n#### Exited"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Exited'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Exited'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Exited'].value_counts().keys().tolist(), df['Exited'].value_counts().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visulaize the Exited data using plotly\nplot_by_exited_labels = df['Exited'].value_counts().keys().tolist()\nplot_by_exited_values = df['Exited'].value_counts().values.tolist()\n\ndata = [\n    go.Pie(labels=plot_by_exited_labels,\n          values=plot_by_exited_values,\n          hole=.6)\n]\n\nplot_layout = go.Layout(dict(title=\"Customer Churn\"))\n\nfig = go.Figure(data=data, layout=plot_layout)\npo.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that our data is imbalanced."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"Exited\").mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also see from the above means of different feature that is they are having significant impact in the churn data."},{"metadata":{},"cell_type":"markdown","source":"### Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_feat = ['Geography','Tenure','NumOfProducts',\"HasCrCard\",\"HasCrCard\",\"IsActiveMember\"]\nfor cat in categorical_feat:\n    print(cat,\" : \")\n    print(df[cat].value_counts())\n    sns.countplot(df[cat])\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Gender'].value_counts()/df.shape[0])\nsns.countplot(df['Gender'])\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"out of total, almost 55% of them are male and 45% are female"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender vs Exited\n# total percentage exited in by each gender\nplot_by_gender = df.groupby('Gender')['Exited'].mean().reset_index()\nprint(plot_by_gender)\n\nplot_data = [\n    go.Bar(\n    x = plot_by_gender['Gender'],\n    y = plot_by_gender['Exited'])\n]\nplot_layout = go.Layout(dict(title=\"% of Exited customers in each gender\"))\nfig = go.Figure(data = plot_data, layout=plot_layout)\npo.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Geography"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Geography'].value_counts())\nsns.countplot(df['Geography'])\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"out of 3 countries provided, France has highest churn rate "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Geography vs Exited in percentage\n# total percentage of Exited customer percentage in each Geography\nplot_by_geo = df.groupby('Geography')['Exited'].mean().reset_index()\nprint(plot_by_geo)\n\nplot_data = [\n    go.Bar(\n    x = plot_by_geo['Geography'],\n    y = plot_by_geo[\"Exited\"])\n]\nplot_layout = go.Layout(dict(title=\"Percentage of CUstomer Exited in each Geography\"))\nfig = go.Figure(data=plot_data, layout=plot_layout)\npo.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NumOfProducts"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['NumOfProducts'].value_counts())\nsns.countplot(df['NumOfProducts'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see most of the customers has used only one product, and very less customer has used 4 of the bank product."},{"metadata":{"trusted":true},"cell_type":"code","source":"df['NumOfProducts'].value_counts()/df.shape[0]*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that less then 1 % of customer has used all 4 products."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['NumOfProducts'],hue=df['Exited'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=df['Exited'], columns=df['NumOfProducts'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets draw % of Exited Customer along with NumOfProducts used\nplot_by_numOfProducts = df.groupby(\"NumOfProducts\")[\"Exited\"].mean().reset_index()\nprint(plot_by_numOfProducts)\n\nplot_data = [\n    go.Bar(\n    x = plot_by_numOfProducts[\"NumOfProducts\"],\n    y = plot_by_numOfProducts[\"Exited\"])\n]\nplot_layout = go.Layout(dict(title = \"% Exited customers with NumOfProducts used\"))\nfig = go.Figure(data=plot_data,\n               layout=plot_layout)\npo.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can conclude that all the customers that have used all the 4 products have left which is 1 i.e. 100%."},{"metadata":{},"cell_type":"markdown","source":"### HasCrCard"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['HasCrCard'].value_counts())\nsns.countplot(df['HasCrCard'], hue=df['Exited'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from above, customers with no credit card are more on leaving the business."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets plot % of Exited customer with HasCrCard\nplot_with_hasCrCard = df.groupby(\"HasCrCard\")[\"Exited\"].mean().reset_index()\nprint(plot_with_hasCrCard)\n\nplot_data = [\n    go.Bar(\n    x=plot_with_hasCrCard['HasCrCard'],\n    y= plot_with_hasCrCard['Exited'])\n]\nplot_layout = go.Layout(dict(title=\"% Exited customer with HasCrCard\"))\nfig = go.Figure(data=plot_data, layout=plot_layout)\npo.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't see much of the difference between the Exited customer between both category as both have almost equal number of exited customer so we won't get  much of the impact from this feature to our model."},{"metadata":{},"cell_type":"markdown","source":"### Balance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balance\nfor i in [0,1]:\n    sns.distplot(df[df['Exited']==i]['Balance'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see both the exited and non exited customer are having show how same distribution of data."},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\nsns.heatmap(df.corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Surname', axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(data=df, columns=['Geography','Gender','HasCrCard','IsActiveMember'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.drop('Exited', axis=1)\ny = df['Exited']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx[['CreditScore','Age','Tenure','Balance','EstimatedSalary','NumOfProducts']] = scaler.fit_transform(x[['CreditScore','Age','Tenure','Balance','EstimatedSalary','NumOfProducts']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### splitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nx_train, x_test,y_train, y_test = train_test_split(x, y, test_size=0.2)\nx_train.shape, x_test.shape, Counter(y_train), Counter(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see our data is imbalanced."},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generated_report(y_actual, y_pred):\n    print(\"Accuracy : \", accuracy_score(y_actual, y_pred))\n    print(classification_report(y_actual, y_pred))\n    \ndef generated_roc_auc_curve(model, x_test):\n    y_pred_proba = model.predict_proba(x_test)[:, 1]\n    fpr, tpr, thresh = roc_curve(y_test, y_pred_proba)\n    auc = roc_auc_score(y_test,  y_pred_proba)\n    plt.plot(fpr, tpr, label='AUC: '+str(auc))\n    plt.legend()\n    plt.show()\n    \ndef Log_Reg_Model():\n    lr = LogisticRegression()\n    lr.fit(x_train, y_train)\n    y_pred = lr.predict(x_test)\n    generated_report(y_test, y_pred)\n    generated_roc_auc_curve(lr, x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Without using sampling methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"Log_Reg_Model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the scores without doing any of the sampling methods. We cannot just rely on accuracy score as it can be misleading sometimes so we need to look for the values of other socres like f1-score, precision, etc."},{"metadata":{},"cell_type":"markdown","source":"## RandomUnderSampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampler = RandomUnderSampler(sampling_strategy=1, replacement=False)\nx_new, y_new = sampler.fit_resample(x, y)\n\nCounter(y_new), x_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)\nCounter(y_train), Counter(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Log_Reg_Model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see its score is overall better then the previous one  without balancing our data but in case of undersampling there will be alot of data still not used for making the model that causes info loss and our model cannot generalize the result well in case of future prediction"},{"metadata":{},"cell_type":"markdown","source":"## RandomOverSampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\n\nx_new, y_new = RandomOverSampler(sampling_strategy=0.8).fit_resample(x, y)\nCounter(y_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Log_Reg_Model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OUr accuracy has little decreased but some score have definitely increased cause we are not just looking for accuracy score."},{"metadata":{},"cell_type":"markdown","source":"## SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\n\nsmote = SMOTE(sampling_strategy=0.87)\nx_new, y_new = smote.fit_resample(x, y)\n\nprint(Counter(y))\nprint(Counter(y_new))\n\nx_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)\nCounter(y_train), Counter(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Log_Reg_Model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our score has increased, but using SMOTE can be misleading as we are generating systhetic data."},{"metadata":{},"cell_type":"markdown","source":"## UnderSampling and OverSampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom collections import Counter\nfrom sklearn.linear_model import LogisticRegression\n\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\npipeline = Pipeline(steps=[\n    (\"over\",SMOTE(sampling_strategy=0.6)),\n    (\"under\", RandomUnderSampler(sampling_strategy=0.9))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before: \", Counter(y))\n\nx_new, y_new = pipeline.fit_resample(x, y)\nprint(\"After: \",Counter(y_new))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_new.shape, y_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# again\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\npipeline = Pipeline(steps=[\n    (\"over\",SMOTE(sampling_strategy=0.6)),\n    (\"under\", RandomUnderSampler(sampling_strategy=0.9)), \n    (\"model\", LogisticRegression())\n])\n\n#evaluate pipeline\nscores = cross_val_score(pipeline, x, y, scoring='roc_auc', cv=cv)\nprint(scores)\nprint(\"mean roc auc: \", np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline(steps=[\n    (\"over\",SMOTE(sampling_strategy=0.6)),\n    (\"under\", RandomUnderSampler(sampling_strategy=0.9))\n])\n\nx_new, y_new = pipeline.fit_resample(x, y)\nx_new.shape, y_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_new, y_new, test_size=0.2, stratify=y_new)\nCounter(y_train), Counter(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Log_Reg_Model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see using the over and under sampling combined has given pretty good result of accuracy then all other sampling technique that we have used before.\n\nThe significane of using this combined method is that using over sampling will help the generation of synthetic data to the less cateogry data and under sampling will under sample the category data that is more in number according to the sampling_strategy paramter speficied due to which helps to balance the data from being bias to one category due to lack of data and also add some more synthetic data to the sample when may have biasness if we have used RandomOverSampler."},{"metadata":{},"cell_type":"markdown","source":"### Thank you\n#### The is my first notebook in kaggle.\n### Upvote is appreciated"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}