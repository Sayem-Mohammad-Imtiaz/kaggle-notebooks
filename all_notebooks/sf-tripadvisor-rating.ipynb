{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.pata.org/wp-content/uploads/2014/09/TripAdvisor_Logo-300x119.png)\n# Predict TripAdvisor Rating\n## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n**По ходу задачи:**\n* Прокачаем работу с pandas\n* Научимся работать с Kaggle Notebooks\n* Поймем как делать предобработку различных данных\n* Научимся работать с пропущенными данными (Nan)\n* Познакомимся с различными видами кодирования признаков\n* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n* И совсем немного затронем ML\n* И многое другое...   \n\n\n\n### И самое важное, все это вы сможете сделать самостоятельно!\n\n*Этот Ноутбук являетсся Примером/Шаблоном к этому соревнованию (Baseline) и не служит готовым решением!*   \nВы можете использовать его как основу для построения своего решения.\n\n> что такое baseline решение, зачем оно нужно и почему предоставлять baseline к соревнованию стало важным стандартом на kaggle и других площадках.   \n**baseline** создается больше как шаблон, где можно посмотреть как происходит обращение с входящими данными и что нужно получить на выходе. При этом МЛ начинка может быть достаточно простой, просто для примера. Это помогает быстрее приступить к самому МЛ, а не тратить ценное время на чисто инженерные задачи. \nТакже baseline являеться хорошей опорной точкой по метрике. Если твое решение хуже baseline - ты явно делаешь что-то не то и стоит попробовать другой путь) \n\nВ контексте нашего соревнования baseline идет с небольшими примерами того, что можно делать с данными, и с инструкцией, что делать дальше, чтобы улучшить результат.  Вообще готовым решением это сложно назвать, так как используются всего 2 самых простых признака (а остальные исключаются)."},{"metadata":{},"cell_type":"markdown","source":"# import"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nfrom datetime import datetime, timedelta\nimport seaborn as sns\nfrom itertools import combinations\nfrom scipy.stats import ttest_ind\n\nfrom pandas import Series, DataFrame\n%matplotlib inline\ndef get_rest_count(value):\n    return rest_count[value]\n\n\n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train= pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создаю словарь со странами и столицами\ncity_country = {'London': 'UK', 'Paris': 'France', 'Madrid': 'Spain', 'Barcelona': 'Spain',\n                'Berlin': 'Germany', 'Milan': 'Italy', 'Rome': 'Italy', 'Prague': 'Czech',\n                'Lisbon': 'Portugalia', 'Vienna': 'Austria', 'Amsterdam': 'Nederlands', 'Brussels': 'Belgium',\n                'Hamburg': 'Germany', 'Munich': 'Germany', 'Lyon': 'France', 'Stockholm': 'Sweden',\n                'Budapest': 'Hungary', 'Warsaw': 'Poland', 'Dublin': 'Ireland', 'Copenhagen': 'Denmark',\n                'Athens': 'Greece', 'Edinburgh': 'Schotland', 'Zurich': 'Switzerland', 'Oporto': 'Portugalia',\n                'Geneva': 'Switzerland', 'Krakow': 'Poland', 'Oslo': 'Norway', 'Helsinki': 'Finland',\n                'Bratislava': 'Slovakia', 'Luxembourg': 'Luxembourg', 'Ljubljana': 'Slovenija'}\n\n# создаю словарь, является ли город столицей\ncity_capital = {'London': 1, 'Paris': 1, 'Madrid': 1, 'Barcelona': 0, 'Berlin': 1,\n                'Milan': 0, 'Rome': 1, 'Prague': 1, 'Lisbon': 1, 'Vienna': 1,\n                'Amsterdam': 1, 'Brussels': 1, 'Hamburg': 0, 'Munich': 0, 'Lyon': 0,\n                'Stockholm': 1, 'Budapest': 1, 'Warsaw': 1, 'Dublin': 1, 'Copenhagen': 1,\n                'Athens': 1, 'Edinburgh': 1, 'Zurich': 1, 'Oporto': 0, 'Geneva': 1,\n                'Krakow': 1, 'Oslo': 1, 'Helsinki': 1, 'Bratislava': 1, 'Luxembourg': 1, 'Ljubljana': 1}\n\n# создаю словарь с информацией о населении города\ncity_population = {'London': 8173900, 'Paris': 2240621, 'Madrid': 3155360, 'Barcelona': 1593075,\n                   'Berlin': 3326002, 'Milan': 1331586, 'Rome': 2870493, 'Prague': 1272690,\n                   'Lisbon': 547733, 'Vienna': 1765649, 'Amsterdam': 825080, 'Brussels': 144784,\n                   'Hamburg': 1718187, 'Munich': 1364920, 'Lyon': 496343, 'Stockholm': 1981263,\n                   'Budapest': 1744665, 'Warsaw': 1720398, 'Dublin': 506211, 'Copenhagen': 1246611,\n                   'Athens': 3168846, 'Edinburgh': 476100, 'Zurich': 402275, 'Oporto': 221800,\n                   'Geneva': 196150, 'Krakow': 756183, 'Oslo': 673469, 'Helsinki': 574579,\n                   'Bratislava': 413192, 'Luxembourg': 576249, 'Ljubljana': 277554}\n\n# создаю словарь с указанием количества ресторанов для каждого города\nres_count = {'Paris': 17593, 'Stockholm': 3131, 'London': 22366, 'Berlin': 8110,\n             'Munich': 3367, 'Oporto': 2060, 'Milan': 7940, 'Bratislava': 1331,\n             'Vienna': 4387, 'Rome': 12086, 'Barcelona': 10086, 'Madrid': 11562,\n             'Dublin': 2706, 'Brussels': 3703, 'Zurich': 1901, 'Warsaw': 3210,\n             'Budapest': 3445, 'Copenhagen': 2637, 'Amsterdam': 4189, 'Lyon': 2833,\n             'Hamburg': 3501, 'Lisbon': 4985, 'Prague': 5850, 'Oslo': 1441,\n             'Helsinki': 1661, 'Edinburgh': 2248, 'Geneva': 1753, 'Ljubljana': 647,\n             'Athens': 2814, 'Luxembourg': 759, 'Krakow': 1832}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Reviews[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки."},{"metadata":{},"cell_type":"markdown","source":"# Cleaning and Prepping Data\nОбычно данные содержат в себе кучу мусора, который необходимо почистить, для того чтобы привести их в приемлемый формат. Чистка данных — это необходимый этап решения почти любой реальной задачи.   \n![](https://analyticsindiamag.com/wp-content/uploads/2018/01/data-cleaning.png)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Обработка NAN \nУ наличия пропусков могут быть разные причины, но пропуски нужно либо заполнить, либо исключить из набора полностью. Но с пропусками нужно быть внимательным, **даже отсутствие информации может быть важным признаком!**   \nПо этому перед обработкой NAN лучше вынести информацию о наличии пропуска как отдельный признак "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Для примера я возьму столбец Number of Reviews\ndata['Number_of_Reviews_isNAN'] = pd.isna(data['Number of Reviews']).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number_of_Reviews_isNAN']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Далее заполняем пропуски 0, вы можете попробовать заполнением средним или средним по городу и тд...\ndata['Number of Reviews'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Обработка признаков\nДля начала посмотрим какие признаки у нас могут быть категориальными."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Какие признаки можно считать категориальными?"},{"metadata":{},"cell_type":"markdown","source":"Для кодирования категориальных признаков есть множество подходов:\n* Label Encoding\n* One-Hot Encoding\n* Target Encoding\n* Hashing\n\nВыбор кодирования зависит от признака и выбраной модели.\nНе будем сейчас сильно погружаться в эту тематику, давайте посмотрим лучше пример с One-Hot Encoding:\n![](https://i.imgur.com/mtimFxh.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\ndata = pd.get_dummies(data, columns=[ 'City',], dummy_na=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Возьмем следующий признак \"Price Range\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"По описанию 'Price Range' это - Цены в ресторане.  \nИх можно поставить по возрастанию (значит это не категориальный признак). А это значит, что их можно заменить последовательными числами, например 1,2,3  \n*Попробуйте сделать обработку этого признака уже самостоятельно!*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ваша обработка 'Price Range'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Для некоторых алгоритмов МЛ даже для не категориальных признаков можно применить One-Hot Encoding, и это может улучшить качество модели. Пробуйте разные подходы к кодированию признака - никто не знает заранее, что может взлететь."},{"metadata":{},"cell_type":"markdown","source":"### Обработать другие признаки вы должны самостоятельно!\nДля обработки других признаков вам возможно придется даже написать свою функцию, а может даже и не одну, но в этом и есть ваша практика в этом модуле!     \nСледуя подсказкам в модуле вы сможете более подробно узнать, как сделать эти приобразования."},{"metadata":{"trusted":true},"cell_type":"code","source":"# тут ваш код на обработку других признаков\n# .....","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://cs10.pikabu.ru/post_img/2018/09/06/11/1536261023140110012.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# EDA \n[Exploratory Data Analysis](https://ru.wikipedia.org/wiki/Разведочный_анализ_данных) - Анализ данных\nНа этом этапе мы строим графики, ищем закономерности, аномалии, выбросы или связи между признаками.\nВ общем цель этого этапа понять, что эти данные могут нам дать и как признаки могут быть взаимосвязаны между собой.\nПонимание изначальных признаков позволит сгенерировать новые, более сильные и, тем самым, сделать нашу модель лучше.\n![](https://miro.medium.com/max/2598/1*RXdMb7Uk6mGqWqPguHULaQ.png)"},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение признака"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,7)\ndf_test['Ranking'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['City'].value_counts(ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"А кто-то говорил, что французы любят поесть=) Посмотрим, как изменится распределение в большом городе:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['City'] =='London'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на топ 10 городов\nfor x in (df_train['City'].value_counts())[0:10].index:\n    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n\n>Подумайте как из этого можно сделать признак для вашей модели. Я покажу вам пример, как визуализация помогает находить взаимосвязи. А далее действуйте без подсказок =) \n"},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Rating'].value_counts(ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной относительно признака"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### И один из моих любимых - [корреляция признаков](https://ru.wikipedia.org/wiki/Корреляция)\nНа этом графике уже сейчас вы сможете заметить, как признаки связаны между собой и с целевой переменной."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.drop(['sample'], axis=1).corr(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Вообще благодаря визуализации в этом датасете можно узнать много интересных фактов, например:\n* где больше Пицерий в Мадриде или Лондоне?\n* в каком городе кухня ресторанов более разнообразна?\n\nпридумайте свои вопрос и найдите на него ответ в данных)"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"# на всякий случай, заново подгружаем данные\ndf_train= pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproc_data(df_input):\n    '''includes several functions to pre-process the predictor data.'''\n    \n    df_output = df_input.copy()\n    \n    # ################### 1. Предобработка ############################################################## \n    # убираем не нужные для модели признаки\n    df_output.drop(['Restaurant_id','ID_TA',], axis = 1, inplace=True)\n    \n    \n    # ################### 2. NAN ############################################################## \n    # Далее заполняем пропуски, вы можете попробовать заполнением средним или средним по городу и тд...\n    df_output['Number of Reviews'].fillna(0, inplace=True)\n    # тут ваш код по обработке NAN\n    # ....\n    \n    \n    # ################### 3. Encoding ############################################################## \n    # для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n    df_output = pd.get_dummies(df_output, columns=[ 'City',], dummy_na=True)\n    # тут ваш код не Encoding фитчей\n    # ....\n    \n    \n    # ################### 4. Feature Engineering ####################################################\n    # тут ваш код не генерацию новых фитчей\n    # ....\n    \n    \n    # ################### 5. Clean #################################################### \n    # убираем признаки которые еще не успели обработать, \n    # модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n    df_output.drop(object_columns, axis = 1, inplace=True)\n    \n    return df_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">По хорошему, можно было бы перевести эту большую функцию в класс и разбить на подфункции (согласно ООП). "},{"metadata":{},"cell_type":"markdown","source":"#### Запускаем и проверяем что получилось"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc = preproc_data(data)\ndf_preproc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 \ndf=df_test\ndata = df.append(df_train, sort=False).reset_index(drop=True) # объединяем\n\n# в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\ndf=df_test\ndf.columns = map(lambda name: name.lower(), df.columns)\ndf.columns = df.columns.str.replace(' ', '_')\n\n# Относительный ранг, чем больше к единице, тем выше ранг ресторана\nrest_count = df.city.value_counts()\n\n\ndef get_rest_count(value):\n    return rest_count[value]\n\n\ndf['relative_ranking'] = 1 - \\\n    (df['ranking'] / df['city'].map(df.groupby(['city'])['ranking'].max()))\n\n\n# Добавляю столбец со страной\ndf['country'] = df['city'].map(city_country)\n\n# Создаю признак, отражающий количество ресторанов в городе, в котором расположен данный ресторан\ndf['restaurants_count'] = df['city'].map(res_count)\n\n\n# Создаю признак с населением города\ndf['population'] = df['city'].map(city_population)\n\n# признак пропорция кол-ва ресторанов и населения\ndf['per_capital'] = df['population']/df['restaurants_count']\n\n# Состоит ли ресторан в сети\nrestaurant_chain = set()\nfor chain in df['restaurant_id']:\n    restaurant_chain.update(chain)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns = map(lambda name: name.lower(), df_train.columns)\ndf_train.columns = df_train.columns.str.replace(' ', '_')\n\n# Относительный ранг, чем больше к единице, тем выше ранг ресторана\nrest_count = df_train.city.value_counts()\n\n\ndef get_rest_count(value):\n    return rest_count[value]\n\n\ndf_train['relative_ranking'] = 1 - \\\n    (df_train['ranking'] / df_train['city'].map(df_train.groupby(['city'])['ranking'].max()))\n\n\n# Добавляю столбец со страной\ndf_train['country'] = df_train['city'].map(city_country)\n\n# Создаю признак, отражающий количество ресторанов в городе, в котором расположен данный ресторан\ndf_train['restaurants_count'] = df_train['city'].map(res_count)\n\n\n# Создаю признак с населением города\ndf_train['population'] = df_train['city'].map(city_population)\n\n# признак пропорция кол-ва ресторанов и населения\ndf_train['per_capital'] = df_train['population']/df_train['restaurants_count']\n\n# Состоит ли ресторан в сети\nrestaurant_chain = set()\nfor chain in df_train['restaurant_id']:\n    restaurant_chain.update(chain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_item(cell):\n    if item in cell:\n        return 1\n    return 0\n\n\nfor item in restaurant_chain:\n    df['restaurant_chain'] = df['restaurant_id'].apply(find_item)\n\n# Сколько городов представлено в наборе данных?\nlen(df['city'].unique())\n\n# CUISINES\n\n# столбец с кухнями разделяем на отдельные виды\ndf['cuisine_style'] = df.cuisine_style.str.replace(\n    ']', '').str.replace('[', '')\no = df[['id_ta', 'cuisine_style']]\no.cuisine_style.fillna(\"'Europian'\", inplace=True)\ns = o['cuisine_style'].str.split(',').apply(Series, 1).stack()\ns.index = s.index.droplevel(-1)\ns.name = 'cuisine_style'\ndel o['cuisine_style']\nk = o.join(s)\n\n# Создаем признак \"Кол-во кухонь в каждом ресторане\"\ntable_with_num = k.copy()\ntable_with_num['num_of_cuis'] = (table_with_num.groupby(\n    ['id_ta'])['cuisine_style']).transform('nunique')\ntable_with_num.drop(['cuisine_style'], axis=1, inplace=True)\ntable_with_num = table_with_num.drop_duplicates()\ndf = df.merge(table_with_num, how='left')\n\n# среднее количество кухонь на ресторан в городе\ndf['cuisine_count_mean'] = df['city'].map(\n    df.groupby('city')['num_of_cuis'].mean())\n\n# Добавляю признак о наличии веганской кухни в ресторане\n\ndf['veg'] = df['cuisine_style'].str.contains(r'Veg', na=True)\ndf['veg'] = df['veg'].astype(int)\nCuisinesDict = {}\n# Функция заполнения словаря CuisinesDict\n\n\ndef CuisinesDictFill(cuisines):\n    global CuisinesDict\n    # из входящей строки вида \"['German', 'Central European', 'Vegetarian Friendly']\"\n    cuisines = str(cuisines).replace(\"['\", \"\").replace(\n        \"']\", \"\").replace(\"', '\", \",\").split(',')\n    # получили список отдельных значений вида ['German', 'Central European', 'Vegetarian Friendly']\n    if cuisines[0] != 'nan':  # если список кухонь не пустой\n        for i in cuisines:  # перебор полученного списка\n            if CuisinesDict.get(i) == None:  # нет такой кухни в словаре CuisinesDict\n                # добавить кухню (ключ) в CuisinesDict со значением 1\n                CuisinesDict[i] = 1\n            else:\n                # увеличить значение для этой кухни (ключа) в словаре CuisinesDict\n                CuisinesDict[i] += 1\n\n\ndf['cuisine_style'].apply(CuisinesDictFill)\n\n# 4.3.3 среднее кол-во кухонь в одном ресторане\nk.groupby('id_ta')['cuisine_style'].nunique().sort_values(\n    ascending=False).replace(0, 1).mean()\n\n# 4.3.2 какая кухня представлена в наибольшем кол-ве ресторанов\nk.groupby(['cuisine_style'])['id_ta'].nunique(\n).sort_values(ascending=False).head(1)\n\n# 2.REVIEWS\n\n\n# Разделим столбец с отзывами на текст и дату\n\ndf['reviews'] = df.reviews.str.replace(']', '').str.replace('[', '')\n# 4.4.1. Когда был оставлен самый свежий отзыв? Введите ответ в формате yyyy-mm-dd.\ndf['reviews'][1]\nreviews_table = df['reviews'].str.split(', ', expand=True)\nreviews_table[3].dropna(inplace=True)\ndf['rew_date'] = pd.to_datetime(reviews_table[3], errors='coerce')\ndf['rew_date1'] = pd.to_datetime(reviews_table[2], errors='coerce')\ndf['diff'] = df['rew_date1']-df['rew_date']\ndf['rew_date'] = pd.to_datetime(df['rew_date'], format='%Y-%m-%d')\ndf['rew_date1'] = pd.to_datetime(df['rew_date1'], format='%Y-%m-%d')\ndf['diff'] = (df.rew_date1 - df.rew_date).dt.days\n\n# создаю столбец с разницей в отзывах, но поддаю сомнению слишком большую разницу в днях,\n# и решаю взять наиболее свежие отзывы\n# убираю выбросы\n\ndaty = pd.DataFrame({'DATE': [pd.Timestamp(x) for x in df['rew_date']]})\n# print(daty)\nqa = daty['DATE'].quantile(0.1)  # lower 10%\nqb = daty['DATE'].quantile(0.9)  # higher 10%\n# remove outliers\ndf['new_date'] = daty[(daty['DATE'] >= qa) & (daty['DATE'] <= qb)]\n\n\nda = pd.DataFrame({'DATES': [pd.Timestamp(x) for x in df['rew_date1']]})\n# print(daty)\nqa = da['DATES'].quantile(0.1)  # lower 10%\nqb = da['DATES'].quantile(0.9)  # higher 10%\n\n# Получаю новый признака разницы дат отзывов\ndf['new_date1'] = da[(da['DATES'] >= qa) & (da['DATES'] <= qb)]\n\ndf['new_date'] = pd.to_datetime(df['new_date'], format='%Y-%m-%d')\ndf['new_date1'] = pd.to_datetime(df['new_date1'], format='%Y-%m-%d')\ndf['new_diff'] = (df.new_date1 - df.new_date).dt.days\n\n# Заменяю \"новизну\" отзывов на ранги\ndf['last_review'] = df.new_date1.dt.year\ndf['last_review'] = df['last_review'].replace(2017.0, 2)\ndf['last_review'] = df['last_review'].replace(2016.0, 1)\ndf['last_review'] = df['last_review'].replace(2018.0, 3)\n\n# Создаю новый признак с разницей между актуальным днем и последним отзывом\ndf['fix'] = '2020-12-03 00:00'\ndf['fix'] = pd.DataFrame({'DATES': [pd.Timestamp(x) for x in df['fix']]})\ndf['last_'] = (df.fix - df.new_date1).dt.days\n\n# Признаки на основании текста отзывов\n\ndf['good_rev'] = df['reviews'].str.contains(r'good', na=True)  # ,\ndf['good_rev'] = df['good_rev'].astype(int)\n\ndf['best_rev'] = df['reviews'].str.contains(r'best', na=True)  # ,\ndf['best_rev'] = df['best_rev'].astype(int)\n\n\ndf['great_rev'] = df['reviews'].str.contains(r'great', na=True)  # ,\ndf['great_rev'] = df['great_rev'].astype(int)\n\ndf['bad_rev'] = df['reviews'].str.contains(r'bad', na=True)  # ,\ndf['bad_rev'] = df['bad_rev'].astype(int)\n\n\n# Кол-во на отзывов на душу населения\ndf['reviews_per_people100'] = df.apply(lambda x:\n                                       round(x['number_of_reviews']*10000/x['population'], 4), axis=1)\n\n# 4.4.1. Когда был оставлен самый свежий отзыв? Введите ответ в формате yyyy-mm-dd.\ndf['reviews'][1]\nreviews_table = df['reviews'].str.split(', ', expand=True)\nreviews_table[3].dropna(inplace=True)\ndf['rew_date'] = pd.to_datetime(reviews_table[3], errors='coerce')\ndf['rew_date1'] = pd.to_datetime(reviews_table[2], errors='coerce')\ndf['diff'] = df['rew_date1']-df['rew_date']\n\n\n# Заменю ценовую категорию на числовые значения\nprice_range_dict = {'$': 10, '$$ - $$$': 100, '$$$$': 1000}\ndf['price_range'] = df['price_range'].map(price_range_dict)\n\n# Загружаю новый датасет о наличии мишленовской звезды у ресторанов по странам\n\nmi = pd.read_csv('../input/ta-rest/michelin_stars.csv')\nmi = mi.drop(['the_geom', 'cartodb_id',\n              'cartodb_georef_status', 'guide_year'], axis=1)\ndf = df.merge(mi, how='left')\n\n# Загружаю новый датасет о 50 лучших ресторанах и доп информации о ресторанах\nbest = pd.read_csv('../input/world-best-50-restaurants-2019/50 best restaurants database.csv')\nbest['name'] = best['Restaurant']\nbest.columns = map(lambda name: name.lower(), best.columns)\nbest.columns\nb = best.drop(['ranking', 'restaurant', 'lat', 'lon',\n               'stars', 'website', 'description'], axis=1)\ndf = df.merge(b, how='left')\n\n# Загружаю новый датасет, чтобы взять название и id ресторана\nta = pd.read_csv('../input/ta-rest/TA_restaurants_curated.csv')\nta.columns = map(lambda name: name.lower(), ta.columns)\nta.columns\nta = ta.drop(['unnamed: 0', 'city', 'cuisine style', 'ranking', 'rating',\n              'price range', 'number of reviews', 'name','reviews', 'url_ta'], axis=1)\ndf = df.merge(ta, how='left')\n\n# Создаю новый признак о средней цене дегустационного меню в ресторанах,\n# заранее переведя все в USD, курс указан на начало декабря\ndf.currency.value_counts()\ncurrency_dict = {'EUR': 1.21, 'GBP': 1.3481, 'DKK': 0.1629, 'SEK': 0.12}\n\ndf['currency_for_usd'] = df['currency'].map(currency_dict)\ndf['price_menu_in_usd'] = df['currency_for_usd']*df['menu']\ndf.currency_for_usd.value_counts()\n\n# проведем корреляционный анализ числовых признаков\ndf_num = df.copy()\ndf_num = df_num[['ranking', 'rating',\n                 'price_range', 'number_of_reviews',\n                 'relative_ranking', 'restaurants_count', 'population',\n                 'per_capital', 'restaurant_chain', 'num_of_cuis', 'cuisine_count_mean',\n                 'veg','new_diff', 'last_review', 'fix', 'last_',\n                 'restaurants_with_three_stars', 'restaurants_with_two_stars',\n                 'restaurants_with_one_star',  'price_menu_in_usd', 'good_rev', 'best_rev',\n                 'great_rev', 'bad_rev', 'reviews_per_people100']]\n#df_num.corr()\n# и далее рассмотрим по отдельности признаки, наиболее сильно влияющие на оценку по тесту\n\n# наиболее сильная корреляция наблюдается в restaurant_chain, num_of_cuis, ranking \n\n#Ranking сильно коррелирует с Rating, возможно потому что признание ресторана в городе\n#безусловно будет влиять на рейтинг\n\n#Также можно предположить что количетсво типов кухонь в ресторане хорошо влияет на рейтинг, т.к.\n#иногда удобно наряду с сытными грузинскими блюдами заказать Healthy food из Veg меню\n\n#Наличие ресторана в сети ресторанов, мне кажется, может сказываться на рейтинге положительно,\n#в случае, если четко соблюдается регламент сети. Те же рестораны Fast food, в незнакомом городе\n#увидев вывеску KFC, например, сразу вспоминаешь аппетитные куриные ножки и \"мысленный\" рейтинг ресторана\n#увеличивается\n\n# Разбиваем датафрейм на части, необходимые для обучения и тестирования модели\n\n# Х - данные с информацией о ресторанах, у - целевая переменная (рейтинги ресторанов)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_item(cell):\n    if item in cell:\n        return 1\n    return 0\n\n\nfor item in restaurant_chain:\n    df_train['restaurant_chain'] = df_train['restaurant_id'].apply(find_item)\n\n# Сколько городов представлено в наборе данных?\nlen(df['city'].unique())\n\n# CUISINES\n\n# столбец с кухнями разделяем на отдельные виды\ndf_train['cuisine_style'] = df_train.cuisine_style.str.replace(\n    ']', '').str.replace('[', '')\no = df_train[['id_ta', 'cuisine_style']]\no.cuisine_style.fillna(\"'Europian'\", inplace=True)\ns = o['cuisine_style'].str.split(',').apply(Series, 1).stack()\ns.index = s.index.droplevel(-1)\ns.name = 'cuisine_style'\ndel o['cuisine_style']\nk = o.join(s)\n\n# Создаем признак \"Кол-во кухонь в каждом ресторане\"\ntable_with_num = k.copy()\ntable_with_num['num_of_cuis'] = (table_with_num.groupby(\n    ['id_ta'])['cuisine_style']).transform('nunique')\ntable_with_num.drop(['cuisine_style'], axis=1, inplace=True)\ntable_with_num = table_with_num.drop_duplicates()\ndf_train = df_train.merge(table_with_num, how='left')\n\n# среднее количество кухонь на ресторан в городе\ndf_train['cuisine_count_mean'] = df_train['city'].map(\n    df_train.groupby('city')['num_of_cuis'].mean())\n\n# Добавляю признак о наличии веганской кухни в ресторане\n\ndf_train['veg'] = df_train['cuisine_style'].str.contains(r'Veg', na=True)\ndf_train['veg'] = df_train['veg'].astype(int)\nCuisinesDict = {}\n# Функция заполнения словаря CuisinesDict\n\n\ndef CuisinesDictFill(cuisines):\n    global CuisinesDict\n    # из входящей строки вида \"['German', 'Central European', 'Vegetarian Friendly']\"\n    cuisines = str(cuisines).replace(\"['\", \"\").replace(\n        \"']\", \"\").replace(\"', '\", \",\").split(',')\n    # получили список отдельных значений вида ['German', 'Central European', 'Vegetarian Friendly']\n    if cuisines[0] != 'nan':  # если список кухонь не пустой\n        for i in cuisines:  # перебор полученного списка\n            if CuisinesDict.get(i) == None:  # нет такой кухни в словаре CuisinesDict\n                # добавить кухню (ключ) в CuisinesDict со значением 1\n                CuisinesDict[i] = 1\n            else:\n                # увеличить значение для этой кухни (ключа) в словаре CuisinesDict\n                CuisinesDict[i] += 1\n\n\ndf_train['cuisine_style'].apply(CuisinesDictFill)\n\n# 4.3.3 среднее кол-во кухонь в одном ресторане\nk.groupby('id_ta')['cuisine_style'].nunique().sort_values(\n    ascending=False).replace(0, 1).mean()\n\n# 4.3.2 какая кухня представлена в наибольшем кол-ве ресторанов\nk.groupby(['cuisine_style'])['id_ta'].nunique(\n).sort_values(ascending=False).head(1)\n\n# 2.REVIEWS\n\n\n# Разделим столбец с отзывами на текст и дату\n\ndf_train['reviews'] = df_train.reviews.str.replace(']', '').str.replace('[', '')\n# 4.4.1. Когда был оставлен самый свежий отзыв? Введите ответ в формате yyyy-mm-dd.\ndf_train['reviews'][1]\nreviews_table = df_train['reviews'].str.split(', ', expand=True)\nreviews_table[3].dropna(inplace=True)\ndf_train['rew_date'] = pd.to_datetime(reviews_table[3], errors='coerce')\ndf_train['rew_date1'] = pd.to_datetime(reviews_table[2], errors='coerce')\ndf_train['diff'] = df_train['rew_date1']-df_train['rew_date']\ndf_train['rew_date'] = pd.to_datetime(df_train['rew_date'], format='%Y-%m-%d')\ndf_train['rew_date1'] = pd.to_datetime(df_train['rew_date1'], format='%Y-%m-%d')\ndf_train['diff'] = (df_train.rew_date1 - df_train.rew_date).dt.days\n\n# создаю столбец с разницей в отзывах, но поддаю сомнению слишком большую разницу в днях,\n# и решаю взять наиболее свежие отзывы\n# убираю выбросы\n\ndaty = pd.DataFrame({'DATE': [pd.Timestamp(x) for x in df['rew_date']]})\n# print(daty)\nqa = daty['DATE'].quantile(0.1)  # lower 10%\nqb = daty['DATE'].quantile(0.9)  # higher 10%\n# remove outliers\ndf_train['new_date'] = daty[(daty['DATE'] >= qa) & (daty['DATE'] <= qb)]\n\n\nda = pd.DataFrame({'DATES': [pd.Timestamp(x) for x in df_train['rew_date1']]})\n# print(daty)\nqa = da['DATES'].quantile(0.1)  # lower 10%\nqb = da['DATES'].quantile(0.9)  # higher 10%\n\n# Получаю новый признака разницы дат отзывов\ndf_train['new_date1'] = da[(da['DATES'] >= qa) & (da['DATES'] <= qb)]\n\ndf_train['new_date'] = pd.to_datetime(df_train['new_date'], format='%Y-%m-%d')\ndf_train['new_date1'] = pd.to_datetime(df_train['new_date1'], format='%Y-%m-%d')\ndf_train['new_diff'] = (df_train.new_date1 - df_train.new_date).dt.days\n\n# Заменяю \"новизну\" отзывов на ранги\ndf_train['last_review'] = df_train.new_date1.dt.year\ndf_train['last_review'] = df_train['last_review'].replace(2017.0, 2)\ndf_train['last_review'] = df_train['last_review'].replace(2016.0, 1)\ndf_train['last_review'] = df_train['last_review'].replace(2018.0, 3)\n\n# Создаю новый признак с разницей между актуальным днем и последним отзывом\ndf_train['fix'] = '2020-12-03 00:00'\ndf_train['fix'] = pd.DataFrame({'DATES': [pd.Timestamp(x) for x in df_train['fix']]})\ndf_train['last_'] = (df_train.fix - df_train.new_date1).dt.days\n\n# Признаки на основании текста отзывов\n\ndf_train['good_rev'] = df_train['reviews'].str.contains(r'good', na=True)  # ,\ndf_train['good_rev'] = df_train['good_rev'].astype(int)\n\ndf_train['best_rev'] = df_train['reviews'].str.contains(r'best', na=True)  # ,\ndf_train['best_rev'] = df_train['best_rev'].astype(int)\n\n\ndf_train['great_rev'] = df_train['reviews'].str.contains(r'great', na=True)  # ,\ndf_train['great_rev'] = df_train['great_rev'].astype(int)\n\ndf_train['bad_rev'] = df_train['reviews'].str.contains(r'bad', na=True)  # ,\ndf_train['bad_rev'] = df_train['bad_rev'].astype(int)\n\n\n# Кол-во на отзывов на душу населения\ndf_train['reviews_per_people100'] = df_train.apply(lambda x:\n                                       round(x['number_of_reviews']*10000/x['population'], 4), axis=1)\n\n# 4.4.1. Когда был оставлен самый свежий отзыв? Введите ответ в формате yyyy-mm-dd.\ndf_train['reviews'][1]\nreviews_table = df_train['reviews'].str.split(', ', expand=True)\nreviews_table[3].dropna(inplace=True)\ndf_train['rew_date'] = pd.to_datetime(reviews_table[3], errors='coerce')\ndf_train['rew_date1'] = pd.to_datetime(reviews_table[2], errors='coerce')\ndf_train['diff'] = df_train['rew_date1']-df_train['rew_date']\n\n\n# Заменю ценовую категорию на числовые значения\nprice_range_dict = {'$': 10, '$$ - $$$': 100, '$$$$': 1000}\ndf_train['price_range'] = df_train['price_range'].map(price_range_dict)\n\n# Загружаю новый датасет о наличии мишленовской звезды у ресторанов по странам\n\nmi = pd.read_csv('../input/ta-rest/michelin_stars.csv')\nmi = mi.drop(['the_geom', 'cartodb_id',\n              'cartodb_georef_status', 'guide_year'], axis=1)\ndf_train = df_train.merge(mi, how='left')\n\n# Загружаю новый датасет о 50 лучших ресторанах и доп информации о ресторанах\nbest = pd.read_csv('../input/world-best-50-restaurants-2019/50 best restaurants database.csv')\nbest['name'] = best['Restaurant']\nbest.columns = map(lambda name: name.lower(), best.columns)\nbest.columns\nb = best.drop(['ranking', 'restaurant', 'lat', 'lon',\n               'stars', 'website', 'description'], axis=1)\ndf_train = df_train.merge(b, how='left')\n\n# Загружаю новый датасет, чтобы взять название и id ресторана\nta = pd.read_csv('../input/ta-rest/TA_restaurants_curated.csv')\nta.columns = map(lambda name: name.lower(), ta.columns)\nta.columns\nta = ta.drop(['unnamed: 0', 'city', 'cuisine style', 'ranking', 'rating',\n              'price range', 'number of reviews', 'name','reviews', 'url_ta'], axis=1)\ndf_train = df_train.merge(ta, how='left')\n\n# Создаю новый признак о средней цене дегустационного меню в ресторанах,\n# заранее переведя все в USD, курс указан на начало декабря\ndf_train.currency.value_counts()\ncurrency_dict = {'EUR': 1.21, 'GBP': 1.3481, 'DKK': 0.1629, 'SEK': 0.12}\n\ndf_train['currency_for_usd'] = df_train['currency'].map(currency_dict)\ndf_train['price_menu_in_usd'] = df_train['currency_for_usd']*df_train['menu']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.truncate(before=1, after=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = pd.get_dummies(df, columns=['city'], dummy_na=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train=pd.get_dummies(df_train, columns=['city'], dummy_na=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['price_range'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['price_range'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train['number_of_reviews'].fillna(df_train['number_of_reviews'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf['number_of_reviews'].fillna(df['number_of_reviews'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.new_diff.fillna(df['new_diff'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.new_diff.fillna(df_train['new_diff'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.last_review .fillna(df['last_review'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.last_review .fillna(df_train['last_review'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.last_.fillna(df['last_'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.last_.fillna(df_train['last_'].mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['restaurant_id','city', 'cuisine_style',\n             'reviews', 'rew_date', 'rew_date1', 'fix', 'diff', 'bad_rev','new_date', 'new_date1', 'url_ta', 'id_ta', 'chef', 'name', 'menu', 'currency'\n             , 'currency_for_usd','total','country'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(['restaurant_id', 'city','cuisine_style',\n             'reviews', 'rew_date', 'rew_date1', 'fix', 'diff', 'bad_rev','new_date', 'new_date1', 'url_ta', 'id_ta', 'chef', 'name', 'menu', 'currency'\n             , 'currency_for_usd','total','country'], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#y = train_data.Rating.values\n#X['number_of_reviews'].fillna(X['number_of_reviews'].mean(), inplace=True)\n#X['new_diff'].fillna(X['new_diff'].min(), inplace=True)\n#\n#X['last_review'].fillna(1, inplace=True)\n#X.last_.fillna(X['last_'].mean(), inplace=True)\n#X.reviews_per_people100.fillna(X['reviews_per_people100'].mean(), inplace=True)\n#X['restaurants_with_three_stars'].fillna(0, inplace=True)\n#X['restaurants_with_two_stars'].fillna(0, inplace=True)\n#X['restaurants_with_one_star'].fillna(0, inplace=True)\n#X['price_menu_in_usd'].fillna(X['price_menu_in_usd'].mean(), inplace=True)\n#X.dropna(axis='index', how='any', subset=['price_range'])\n#X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = data.query('sample == 1').drop(['sample'], axis=1)\n\ntest_data = data.query('sample == 0').drop(['sample'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= train_data.rating.values            # наш таргет\nX = train_data.drop(['rating'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \nСам ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(['rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.columns= map(lambda name: name.lower(), sample_submission.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What's next?\nИли что делать, чтоб улучшить результат:\n* Обработать оставшиеся признаки в понятный для машины формат\n* Посмотреть, что еще можно извлечь из признаков\n* Сгенерировать новые признаки\n* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n* Подобрать состав признаков\n\nВ общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}