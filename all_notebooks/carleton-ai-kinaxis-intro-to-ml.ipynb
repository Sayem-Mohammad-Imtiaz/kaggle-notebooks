{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Welcome to this Carleton AI Society workshop!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%capture\nfrom IPython.display import Image\n\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import collections as matcoll\nimport seaborn as sns\nimport lightgbm\n\nimport sklearn\nfrom sklearn import ensemble\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import *\nfrom sklearn.metrics import *\n\nfrom sklearn.metrics import roc_auc_score\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\nmatplotlib.rcParams['figure.figsize'] = [15, 7.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L6_100nt = pd.read_csv('../input/L6_100nt.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Finding columns that contain data about the participant's microbiota\")\nL6_pattern = re.compile(\"k__(\\w*);p__(\\w*);c__(\\w*);o__(\\w*);f__(\\w*);g__(\\w*)$\")\nL3_pattern = re.compile(\"k__(\\w*);p__(\\w*);c__(\\w*);o__(\\w*)$\")\nL2_pattern = re.compile(\"k__(\\w*);p__(\\w*)$\")\nL6_columns = [col for col in L6_100nt.columns if L6_pattern.match(col)]\nL3_columns = [col for col in L6_100nt.columns if L3_pattern.match(col)]\nL2_columns = [col for col in L6_100nt.columns if L2_pattern.match(col)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_data(data, column, title, xAxis):\n    \"\"\" Just a quick function to plot data easily \"\"\"\n    data[column] = pd.to_numeric(data[column], errors='coerce')\n    fig, axes = plt.subplots(1, 2)\n    female = data[data['SEX'] == 'female']\n    male = data[data['SEX'] == 'male']\n    fig.suptitle(title, fontsize=16)\n    sns.distplot(male[column], bins=40, kde=False, ax=axes[0]);\n    axes[0].set_ylabel('Number of Individuals (male)', fontsize=14)\n    axes[0].set_xlabel(xAxis, fontsize=14)\n    sns.distplot(female[column], bins=40, kde=False, ax=axes[1], color='r');\n    axes[1].set_ylabel('Number of Individuals (female)', fontsize=14)\n    axes[1].set_xlabel(xAxis, fontsize=14)\n    return fig\n\ndef filter_data(study):\n    \"\"\" Removes unwanted rows or modify them to limit the space of the task \"\"\"\n    study = L6_100nt[L6_100nt['STUDY'] == study]\n\n    study['BMI_CORRECTED'] = study['BMI_CORRECTED'].replace(\"no_data\",np.nan).replace(\"Unspecified\",np.nan).replace(\"Unknown\",np.nan).astype(float)\n    study['AGE_CORRECTED'] = study['AGE_CORRECTED'].replace(\"Unspecified\",np.nan).replace(\"Unknown\",np.nan).astype(float)    \n    study = study[(study['AGE_CORRECTED'].isnull()) | (study['AGE_CORRECTED'] >= 18)]\n    \n    subset_underweight = study[(study['BMI_CORRECTED'] < 18.5)]\n    subset_healthyweight = study[(study['BMI_CORRECTED'] >= 18.5) & (study['BMI_CORRECTED'] < 25)]\n    subset_overweight = study[(study['BMI_CORRECTED'] >= 25) & (study['BMI_CORRECTED'] < 30)]\n    subset_obese = study[(study['BMI_CORRECTED'] >= 30) & (study['BMI_CORRECTED'])]\n    \n    study = pd.concat([subset_underweight, subset_healthyweight, subset_overweight, subset_obese])\n    \n    #Label Smoothing\n    study['SUBSET_UNDERWEIGHT'] = (study['BMI_CORRECTED'] < 18.5).astype(float) * 0.8\n    study['SUBSET_HEALTHYWEIGHT'] = ((study['BMI_CORRECTED'] >= 18.5) & (study['BMI_CORRECTED'] < 25)).astype(float) * 0.8\n    study['SUBSET_OVERWEIGHT'] = ((study['BMI_CORRECTED'] >= 25) & (study['BMI_CORRECTED'] < 30)).astype(float) * 0.8\n    study['SUBSET_OBESE'] = (study['BMI_CORRECTED'] >= 30).astype(float) * 0.8\n    return study","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering based on the study, as many scientific studies were involved\nmeta_study = pd.concat([filter_data(study) for study in L6_100nt['STUDY'].unique()])\nmeta_study = meta_study[~meta_study['#SampleID'].duplicated()]\nmeta = meta_study.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = meta[L6_columns].var().sort_values(ascending=False).index[:600].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering data to only consider one source of microbiota in the participant's body \n# Filtering further to ignore participants who recently used antibiotic\ndata = meta[meta['BODY_SITE'] == 'UBERON:feces']\ndata = data[data['SUBSET_ANTIBIOTIC_HISTORY'] | (data['ANTIBIOTIC_HISTORY'] == 'Year') | (data['ANTIBIOTIC_HISTORY'] == '6 months')]\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at some of the data with Seaborn or Matplotlib!\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up a section of the data as an experiment\ndata = data.groupby([\"HOST_SUBJECT_ID\"]).first()\n\nunder = data[data[\"SUBSET_UNDERWEIGHT\"] == 0.8]\nover = data[data[\"SUBSET_OBESE\"] == 0.8]\n\nobesity = pd.concat([under, over])\n\nobesity[\"obesity_target\"] = (obesity[\"SUBSET_OBESE\"] == 0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data\nx_train, x_test, y_train, y_test = train_test_split(obesity[features], obesity[\"obesity_target\"], test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a model: initializing it, training it, and predicting classes\nmodel = sklearn.neighbors.KNeighborsClassifier()\n\ntrained_model = model.fit(x_train, y_train)\npredictions = model.predict(x_test)\ncm = confusion_matrix(y_test, predictions)\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nprint(roc_auc_score(y_test, predictions))\npd.DataFrame(data=cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hm, how do we solve what we're seeing in that confusion matrix?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's explore the problem further\ndimensions = 300\nn_points = 1000\n\ncursed_data = np.random.normal(0, 1, size=(n_points, dimensions))\ncursed_label = cursed_data[:,0] > 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(cursed_data[:,0], y=[0]*(n_points), c=cursed_label, cmap=\"Accent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(cursed_data[:,0], cursed_data[:,1], c=cursed_label, cmap=\"Accent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(cursed_data[:,0], cursed_data[:,2], cursed_data[:,1], c=cursed_label, cmap=\"Accent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cursed_features = range(0,1)\ncursed_df = pd.DataFrame(cursed_data)\ncursed_df[\"target\"] = cursed_label\nx_train, x_test, y_train, y_test = train_test_split(cursed_df[cursed_features], cursed_df[\"target\"], test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sklearn.neighbors.KNeighborsClassifier()\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How can we solve that issue?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(obesity[features], obesity[\"obesity_target\"], test_size=0.30)\n# What model should we use here?\ntrained_model = model.fit(x_train, y_train)\npredictions = model.predict(x_test)\ncm = confusion_matrix(y_test, predictions)\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nprint(roc_auc_score(y_test, predictions))\npd.DataFrame(data=cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What's even better than one model?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What's even better than one pair of training and testing sets? ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"h = plt.hist(trials_1, bins=10, alpha=0.5, label=f\"Model 1: {np.mean(trials_1)}\")\nh = plt.hist(trials_2, bins=10, alpha=0.5, label=f\"Model 2: {np.mean(trials_2)}\")\nh = plt.hist(trials_3, bins=10, alpha=0.5, label=f\"Model 3: {np.mean(trials_3)}\")\nplt.legend(loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y=\"TYPES_OF_PLANTS\", hue=\"obesity_target\", data=obesity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(obesity.columns)[:250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://msystems.asm.org/content/3/3/e00031-18","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}