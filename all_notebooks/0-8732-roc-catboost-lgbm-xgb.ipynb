{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\npd.set_option('display.max_columns', 100)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom imblearn.over_sampling import SMOTE\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:20.269304Z","iopub.execute_input":"2021-05-31T07:00:20.269814Z","iopub.status.idle":"2021-05-31T07:00:20.27585Z","shell.execute_reply.started":"2021-05-31T07:00:20.269781Z","shell.execute_reply":"2021-05-31T07:00:20.274974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:21.430136Z","iopub.execute_input":"2021-05-31T07:00:21.430686Z","iopub.status.idle":"2021-05-31T07:00:21.448215Z","shell.execute_reply.started":"2021-05-31T07:00:21.430653Z","shell.execute_reply":"2021-05-31T07:00:21.447086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Pre-processing","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_csv('../input/jobathon-may-2021-credit-card-lead-prediction/train.csv')\ndf1.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:22.619925Z","iopub.execute_input":"2021-05-31T07:00:22.620339Z","iopub.status.idle":"2021-05-31T07:00:23.229961Z","shell.execute_reply.started":"2021-05-31T07:00:22.620302Z","shell.execute_reply":"2021-05-31T07:00:23.228945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so that we can separate out the category and numeric features\nfor i in df1.columns:\n    print(\"Number of unique {} are : {}\".format(i,len(df1[i].unique())))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:23.231682Z","iopub.execute_input":"2021-05-31T07:00:23.232006Z","iopub.status.idle":"2021-05-31T07:00:23.444272Z","shell.execute_reply.started":"2021-05-31T07:00:23.231974Z","shell.execute_reply":"2021-05-31T07:00:23.443253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking null values\ndf1.isnull().sum()\n\n#Imputing giving bad roc so we will just let NaN be another category","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:23.486297Z","iopub.execute_input":"2021-05-31T07:00:23.486656Z","iopub.status.idle":"2021-05-31T07:00:23.648833Z","shell.execute_reply.started":"2021-05-31T07:00:23.486626Z","shell.execute_reply":"2021-05-31T07:00:23.647728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking outliers in numeric features\nfig,axs = plt.subplots(1,2,figsize=(18,7))\nfig.suptitle('Searching For Outliers..')\n\n\nax1 = sns.boxplot(ax=axs[0],y = df1[\"Age\"])\nax2 = sns.boxplot(ax=axs[1],y = df1['Avg_Account_Balance'])\n\n#age seems fine, but account balance too many rich people\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:23.734288Z","iopub.execute_input":"2021-05-31T07:00:23.734651Z","iopub.status.idle":"2021-05-31T07:00:24.028303Z","shell.execute_reply.started":"2021-05-31T07:00:23.734622Z","shell.execute_reply":"2021-05-31T07:00:24.027159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let`s see how much data we have to sacrifice to remove outliers\n\nQ1 = df1['Avg_Account_Balance'].quantile(0.25)\nQ3 = df1['Avg_Account_Balance'].quantile(0.75)\nIQR = Q3 - Q1\n\nfilter = (df1['Avg_Account_Balance'] >= Q1 - 1.5 * IQR) & (df1['Avg_Account_Balance'] <= Q3 + 1.5 *IQR)\ndf2 = df1.loc[filter]  \nprint(\"data loss percentage {}%\".format(((len(df1) - len(df2))/len(df1))*100))","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:24.029802Z","iopub.execute_input":"2021-05-31T07:00:24.030105Z","iopub.status.idle":"2021-05-31T07:00:24.085204Z","shell.execute_reply.started":"2021-05-31T07:00:24.030074Z","shell.execute_reply":"2021-05-31T07:00:24.084167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time to check target variable is imbalance or not\nsns.countplot(x='Is_Lead',data=df1)\n\n#Imbalanced","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:24.14835Z","iopub.execute_input":"2021-05-31T07:00:24.1487Z","iopub.status.idle":"2021-05-31T07:00:24.481181Z","shell.execute_reply.started":"2021-05-31T07:00:24.14867Z","shell.execute_reply":"2021-05-31T07:00:24.479546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(df1['Credit_Product'],df1.Is_Lead,normalize='index').sort_values(by=[1],ascending=False).head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:24.483622Z","iopub.execute_input":"2021-05-31T07:00:24.484148Z","iopub.status.idle":"2021-05-31T07:00:24.559599Z","shell.execute_reply.started":"2021-05-31T07:00:24.4841Z","shell.execute_reply":"2021-05-31T07:00:24.558521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the effects of all cat features on target\ncolumn = ['Gender', 'Region_Code', 'Occupation', 'Channel_Code', 'Credit_Product', 'Is_Active','Vintage']\nfor i in column:\n    print(pd.crosstab(df1[i],df1.Is_Lead,normalize='index').sort_values(by=[1],ascending=False).head(5))\n    print('--------------------------------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:24.561458Z","iopub.execute_input":"2021-05-31T07:00:24.561762Z","iopub.status.idle":"2021-05-31T07:00:24.919728Z","shell.execute_reply.started":"2021-05-31T07:00:24.561732Z","shell.execute_reply":"2021-05-31T07:00:24.918648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let`s group features and check out their influences\n\ncomb = combinations(column, 2) \nfor i in comb:\n    \n    df1[f'{i[0]}_{i[1]}']=df1[i[0]].astype(str)+'_'+df1[i[1]].astype(str)\n    \n    print(pd.crosstab(df1[i[0]],df1.Is_Lead,normalize='index').sort_values(by=[1],ascending=False).head(5))\n    print('**'*30)\n    print(pd.crosstab(df1[f'{i[0]}_{i[1]}'],df1.Is_Lead,normalize='index').sort_values(by=[1],ascending=False).head(5))\n    print('--'*50)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:24.92245Z","iopub.execute_input":"2021-05-31T07:00:24.922914Z","iopub.status.idle":"2021-05-31T07:00:30.771772Z","shell.execute_reply.started":"2021-05-31T07:00:24.922857Z","shell.execute_reply":"2021-05-31T07:00:30.770731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# effect of Age and Avg_Account_Balance on Target\nsns.scatterplot(data=df1, x=\"Avg_Account_Balance\", y=\"Age\",hue='Is_Lead')","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:30.773393Z","iopub.execute_input":"2021-05-31T07:00:30.773761Z","iopub.status.idle":"2021-05-31T07:00:46.457972Z","shell.execute_reply.started":"2021-05-31T07:00:30.773728Z","shell.execute_reply":"2021-05-31T07:00:46.456927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data():\n    \n    train = pd.read_csv(\"../input/jobathon-may-2021-credit-card-lead-prediction/train.csv\")\n    test = pd.read_csv(\"../input/jobathon-may-2021-credit-card-lead-prediction/test.csv\")\n    \n    #Removes train rows which has Region_Code not present in test set\n    test_region_list=test['Region_Code'].tolist()\n    train=train[train['Region_Code'].isin(test_region_list)]\n    \n    \n    #Removing outliers\n    Q1 = train['Avg_Account_Balance'].quantile(0.25)\n    Q3 = train['Avg_Account_Balance'].quantile(0.75)\n    IQR = Q3 - Q1\n    filter = (train['Avg_Account_Balance'] >= Q1 - 1.5 * IQR) & (train['Avg_Account_Balance'] <= Q3 + 1.5 *IQR)\n    train = train.loc[filter]  \n    \n    train['train_or_test']='train'\n    test['train_or_test']='test'\n    df=pd.concat([train,test])\n    \n    \n    \n    le = LabelEncoder()\n    for col in ['Gender', 'Region_Code', 'Occupation', 'Channel_Code', 'Credit_Product', 'Is_Active','Vintage']:\n        df[col]=  df[col].astype('str')\n        df[col]= le.fit_transform(df[col])\n        \n\n    \n    return train,test,df","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:46.459694Z","iopub.execute_input":"2021-05-31T07:00:46.460121Z","iopub.status.idle":"2021-05-31T07:00:46.469459Z","shell.execute_reply.started":"2021-05-31T07:00:46.460085Z","shell.execute_reply":"2021-05-31T07:00:46.468473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/jobathon-may-2021-credit-card-lead-prediction/train.csv\")\ntest = pd.read_csv(\"../input/jobathon-may-2021-credit-card-lead-prediction/test.csv\")\n\nprint(len (train))\n#Removes train rows which has Region_Code not present in test set\ntest_region_list=test['Region_Code'].tolist()\ntrain1 = train[train['Region_Code'].isin(test_region_list)]\nprint(len(train1))\n((len(train) - len(train1))/len(train))*100","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:46.471076Z","iopub.execute_input":"2021-05-31T07:00:46.471601Z","iopub.status.idle":"2021-05-31T07:00:47.141703Z","shell.execute_reply.started":"2021-05-31T07:00:46.471557Z","shell.execute_reply":"2021-05-31T07:00:47.140569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frequency_encoding(column_name,output_column_name,df):\n    fe_pol = (df.groupby(column_name).size()) / len(df)\n    df[output_column_name] = df[column_name].apply(lambda x : fe_pol[x])","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:47.143057Z","iopub.execute_input":"2021-05-31T07:00:47.143392Z","iopub.status.idle":"2021-05-31T07:00:47.148459Z","shell.execute_reply.started":"2021-05-31T07:00:47.143359Z","shell.execute_reply":"2021-05-31T07:00:47.14753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef feature_engineering(df):\n    le = LabelEncoder()\n    \n     #Interaction Feature (Combining 2 categorical features and performing frequency encoding)\n        \n    cat_features=[]\n    le_features=[]\n    columns=['Gender', 'Region_Code', 'Occupation', 'Channel_Code', 'Credit_Product', 'Is_Active','Vintage']\n\n    comb = combinations(columns, 2) \n\n    for i in list(comb):  \n        df[f'{i[0]}_{i[1]}']=df[i[0]].astype(str)+'_'+df[i[1]].astype(str)\n        df[f'{i[0]}_{i[1]}_le']=le.fit_transform(df[f'{i[0]}_{i[1]}'])\n        le_features.append(f'{i[0]}_{i[1]}_le')\n        frequency_encoding(f'{i[0]}_{i[1]}',f'{i[0]}_{i[1]}',df)\n        cat_features.append(f'{i[0]}_{i[1]}')   \n        \n    #Frequency Encoding\n    \n    frequency_encoding('Region_Code','Region_Code_fe',df)\n    \n    #Deriving characteristics of each region by creating aggregate features\n    \n    region_aggregate_features = df.groupby(['Region_Code']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Gender': ['nunique','count'],             \n                                                     })\n\n    region_aggregate_features.columns = ['region_aggregate_features' + '_'.join(c).strip('_') for c in region_aggregate_features.columns]\n    df = pd.merge(df, region_aggregate_features, on = ['Region_Code'], how='left')\n\n \n    region_vintage_aggregate_features = df.groupby(['Region_Code','Vintage']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Gender': ['nunique','count'],             \n                                                     })\n    region_vintage_aggregate_features.columns = ['region_vintage_aggregate_features' + '_'.join(c).strip('_') for c in region_vintage_aggregate_features.columns]\n    df = pd.merge(df, region_vintage_aggregate_features, on = ['Region_Code','Vintage'], how='left')\n\n   \n    for i in cat_features:\n        df[f'region_{i}_max']=df.groupby('Region_Code')[i].transform('max')\n        df[f'region_{i}_min']=df.groupby('Region_Code')[i].transform('min')\n        df[f'region_{i}_mean']=df.groupby('Region_Code')[i].transform('mean')\n        df[f'region_{i}_std']=df.groupby('Region_Code')[i].transform('std')\n\n    \n        df[f'region_vinatge_{i}_max']=df.groupby(['Region_Code','Vintage'])[i].transform('max')\n        df[f'region_vinatge_{i}_min']=df.groupby(['Region_Code','Vintage'])[i].transform('min')\n        df[f'region_vinatge_{i}_mean']=df.groupby(['Region_Code','Vintage'])[i].transform('mean')\n        df[f'region_vinatge_{i}_std']=df.groupby(['Region_Code','Vintage'])[i].transform('std')\n\n\n        \n        \n        \n\n    #Deriving characteristics of Occupation by creating aggregate features\n    \n    Occupation_aggregate_features = df.groupby(['Occupation']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Region_Code': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Gender': ['nunique','count'],             \n                                                     })\n\n    Occupation_aggregate_features.columns = ['Occupation_aggregate_features' + '_'.join(c).strip('_') for c in Occupation_aggregate_features.columns]\n    df = pd.merge(df, Occupation_aggregate_features, on = ['Occupation'], how='left')\n    \n    #Deriving characteristics of Channel_Code by creating aggregate features\n    \n    Channel_Code_aggregate_features = df.groupby(['Channel_Code']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Region_Code': ['nunique','count'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Gender': ['nunique','count'],             \n                                                     })\n\n    Channel_Code_aggregate_features.columns = ['Channel_Code_aggregate_features' + '_'.join(c).strip('_') for c in Channel_Code_aggregate_features.columns]\n    df = pd.merge(df, Channel_Code_aggregate_features, on = ['Channel_Code'], how='left')\n    \n    \n    #Deriving characteristics of Is_Active by creating aggregate features\n    \n    Is_Active_aggregate_features = df.groupby(['Is_Active']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Region_Code': ['nunique','count'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Gender': ['nunique','count'],             \n                                                     })\n\n    Is_Active_aggregate_features.columns = ['Is_Active_aggregate_features' + '_'.join(c).strip('_') for c in Is_Active_aggregate_features.columns]\n    df = pd.merge(df, Is_Active_aggregate_features, on = ['Is_Active'], how='left')\n    \n     #Deriving characteristics of Credit_Product by creating aggregate features\n    \n    Credit_Product_aggregate_features = df.groupby(['Credit_Product']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Region_Code': ['nunique','count'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Gender': ['nunique','count'],             \n                                                     })\n\n    Credit_Product_aggregate_features.columns = ['Credit_Product_aggregate_features' + '_'.join(c).strip('_') for c in Credit_Product_aggregate_features.columns]\n    df = pd.merge(df, Credit_Product_aggregate_features, on = ['Credit_Product'], how='left')\n    \n    \n    #Deriving characteristics of Gender by creating aggregate features\n    \n    Gender_aggregate_features = df.groupby(['Gender']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Region_Code': ['nunique','count'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],             \n                                                     })\n\n    Gender_aggregate_features.columns = ['Gender_aggregate_features' + '_'.join(c).strip('_') for c in Gender_aggregate_features.columns]\n    df = pd.merge(df, Gender_aggregate_features, on = ['Gender'], how='left')\n    \n    #Deriving characteristics of Interaction_features by creating aggregate features (These interaction feature are selected for aggregating based on its feature importance)\n    \n    Region_Code_Occupation_grpd = df.groupby(['Region_Code_Occupation']).agg({ 'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Gender': ['nunique','count']\n                                                     })                                                              \n                                                     \n    Region_Code_Occupation_grpd.columns = ['grpd_by_Region_Code_Occupation_' + '_'.join(c).strip('_') for c in Region_Code_Occupation_grpd.columns]\n    df = pd.merge(df, Region_Code_Occupation_grpd, on = ['Region_Code_Occupation'], how='left')\n\n\n    Region_Code_Credit_Product_grpd = df.groupby(['Region_Code_Credit_Product']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Gender': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],             \n                                                     })                                                              \n                                                     \n    Region_Code_Credit_Product_grpd.columns = ['grpd_by_Region_Code_Credit_Product_' + '_'.join(c).strip('_') for c in Region_Code_Credit_Product_grpd.columns]\n    df = pd.merge(df, Region_Code_Credit_Product_grpd, on = ['Region_Code_Credit_Product'], how='left')\n    \n    # Occupation_Credit_Product_grpd = df.groupby(['Occupation_Credit_Product']).agg({'Age': ['mean', 'max', 'min','std'],\n    #                                                  'Vintage': ['nunique','count'],\n    #                                                  'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n    #                                                  'Gender': ['nunique','count'],\n    #                                                  'Channel_Code': ['nunique','count'],\n    #                                                  'Is_Active': ['nunique','count'],\n    #                                                  'Region_Code': ['nunique','count'],             \n    #                                                  })                                                              \n                                                     \n    # Occupation_Credit_Product_grpd.columns = ['grpd_by_Occupation_Credit_Product_' + '_'.join(c).strip('_') for c in Occupation_Credit_Product_grpd.columns]\n    # df = pd.merge(df, Occupation_Credit_Product_grpd, on = ['Occupation_Credit_Product'], how='left')\n    \n    Gender_Vintage_grpd = df.groupby(['Gender_Vintage']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Region_Code': ['nunique','count'],             \n                                                     })                                                              \n                                                     \n    Gender_Vintage_grpd.columns = ['grpd_by_Gender_Vintage_' + '_'.join(c).strip('_') for c in Gender_Vintage_grpd.columns]\n    df = pd.merge(df, Gender_Vintage_grpd, on = ['Gender_Vintage'], how='left')\n    \n    Credit_Product_Is_Active_grpd = df.groupby(['Credit_Product_Is_Active']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Gender': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Region_Code': ['nunique','count'],             \n                                                     })                                                              \n                                                     \n    Credit_Product_Is_Active_grpd.columns = ['grpd_by_Credit_Product_Is_Active_' + '_'.join(c).strip('_') for c in Credit_Product_Is_Active_grpd.columns]\n    df = pd.merge(df, Credit_Product_Is_Active_grpd, on = ['Credit_Product_Is_Active'], how='left')\n    \n    Gender_Credit_Product_grpd = df.groupby(['Gender_Credit_Product']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Occupation': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                      })                                                              \n                                                     \n    Gender_Credit_Product_grpd.columns = ['grpd_by_Gender_Credit_Product_' + '_'.join(c).strip('_') for c in Gender_Credit_Product_grpd.columns]\n    df = pd.merge(df, Gender_Credit_Product_grpd, on = ['Gender_Credit_Product'], how='left')\n    \n    #Creating Age Bins and deriving characteristics of each age group by creating aggregate features\n    \n    Age_Bins = KBinsDiscretizer(n_bins=14, encode='ordinal', strategy='quantile')\n    df['Age_Bins'] = Age_Bins.fit_transform(df['Age'].values.reshape(-1,1)).astype(int)\n    \n    age_aggregate_features = df.groupby(['Age_Bins']).agg({'Age': ['mean', 'max', 'min','std'],\n                                                     'Vintage': ['nunique','count'],\n                                                     'Avg_Account_Balance': ['mean', 'max', 'min','std'],\n                                                     'Region_Code': ['nunique','count'],\n                                                     'Channel_Code': ['nunique','count'],\n                                                     'Is_Active': ['nunique','count'],\n                                                     'Credit_Product': ['nunique','count'],\n                                                     'Gender': ['nunique','count'],\n                                                      'Occupation': ['nunique','count'],      \n                                                     })\n    age_aggregate_features.columns = ['age_aggregate_features' + '_'.join(c).strip('_') for c in age_aggregate_features.columns]\n    df = pd.merge(df, age_aggregate_features, on = ['Age_Bins'], how='left')\n\n    \n    return df,le_features\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:47.150396Z","iopub.execute_input":"2021-05-31T07:00:47.150839Z","iopub.status.idle":"2021-05-31T07:00:47.202532Z","shell.execute_reply.started":"2021-05-31T07:00:47.150806Z","shell.execute_reply":"2021-05-31T07:00:47.201707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation for Machine Learning","metadata":{}},{"cell_type":"code","source":"def preparedatafortraining(df,train,test):\n    \n    train=df.loc[df.train_or_test.isin(['train'])]\n    test=df.loc[df.train_or_test.isin(['test'])]\n    \n    drop_columns={'ID','Is_Lead','train_or_test'}\n    \n    target=['Is_Lead']\n    \n    x=train.drop(columns=drop_columns,axis=1)\n    y=train[target]\n    x_test=test.drop(columns=drop_columns,axis=1)\n    train_features = [_f for _f in x.columns]\n    \n    print(x.shape)\n    \n    return x,y,x_test,train_features","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:47.204319Z","iopub.execute_input":"2021-05-31T07:00:47.20463Z","iopub.status.idle":"2021-05-31T07:00:47.218275Z","shell.execute_reply.started":"2021-05-31T07:00:47.2046Z","shell.execute_reply":"2021-05-31T07:00:47.217347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def savedata():\n    \n    train,test,df=process_data()\n    df,cat_features=feature_engineering(df)\n    x_train,y_train,x_test,train_features=preparedatafortraining(df,train,test)\n    \n    #x_train.to_pickle(\"x_train_lgbm.pkl\")\n    #y_train.to_pickle(\"y_train_lgbm.pkl\")\n    #x_test.to_pickle(\"x_test_lgbm.pkl\")\n    \n    return x_train,y_train,x_test,cat_features,train_features","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:47.219363Z","iopub.execute_input":"2021-05-31T07:00:47.219794Z","iopub.status.idle":"2021-05-31T07:00:47.234911Z","shell.execute_reply.started":"2021-05-31T07:00:47.219761Z","shell.execute_reply":"2021-05-31T07:00:47.233962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"def catboost_model():\n    \n    x,y,x_test,cat_features,train_features=savedata()\n     \n    err = [] \n\n    oofs = np.zeros(shape=(len(x)))\n    preds = np.zeros(shape=(len(x_test)))\n\n    Folds=8\n\n    fold = StratifiedKFold(n_splits=Folds, shuffle=True, random_state=2021)\n    i = 1\n\n    for train_index, test_index in fold.split(x, y):\n        x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n        m =  CatBoostClassifier(n_estimators=10000,random_state=2020,eval_metric='AUC')\n    \n        m.fit(x_train, y_train,eval_set=[(x_val, y_val)], early_stopping_rounds=30,verbose=100,cat_features=cat_features)\n    \n        pred_y = m.predict_proba(x_val)[:,1]\n        oofs[test_index] = pred_y\n        print(i, \" err_cat: \", roc_auc_score(y_val,pred_y))\n        err.append(roc_auc_score(y_val,pred_y))\n        preds+= m.predict_proba(x_test)[:,1]\n        i = i + 1\n    preds=preds/Folds\n    \n    print(f\"Average StratifiedKFold Score : {sum(err)/Folds} \")\n    oof_score = roc_auc_score(y, oofs)\n    print(f'\\nOOF Auc is : {oof_score}')\n    \n    oofs=pd.DataFrame(oofs,columns=['catboostoof'])\n    preds=pd.DataFrame(preds,columns=['catboostpred'])\n    \n    oofs.to_csv('catboostoof.csv',index=False)\n    preds.to_csv('catboostpred.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:47.236131Z","iopub.execute_input":"2021-05-31T07:00:47.236573Z","iopub.status.idle":"2021-05-31T07:00:47.256686Z","shell.execute_reply.started":"2021-05-31T07:00:47.236529Z","shell.execute_reply":"2021-05-31T07:00:47.255511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"catboost_model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"\ndef lgbm_model():\n    \n    x,y,x_test,cat_features,train_features=savedata()\n    \n\n    params={'lambda': 2.8849054495567423, \n        'alpha': 0.001054193185317787, \n        'colsample_bytree': 0.5, \n        'subsample': 0.4, \n        'learning_rate': 0.014, \n        'max_depth': 13, \n        'random_state': 24,\n        'min_child_weight': 5}\n    \n    err = [] \n\n    oofs = np.zeros(shape=(len(x)))\n    preds = np.zeros(shape=(len(x_test)))\n\n    Folds=8\n\n    fold = StratifiedKFold(n_splits=Folds, shuffle=True, random_state=2020)\n    i = 1\n\n    for train_index, test_index in fold.split(x, y):\n        x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n        m = LGBMClassifier(n_estimators=10000,**params,verbose= -1)\n    \n        m.fit(x_train, y_train,eval_set=[(x_val, y_val)], early_stopping_rounds=30,verbose=False,eval_metric='auc')\n    \n        pred_y = m.predict_proba(x_val)[:,1]\n        oofs[test_index] = pred_y\n        print(i, \" err_lgm: \", roc_auc_score(y_val,pred_y))\n        err.append(roc_auc_score(y_val,pred_y))\n        preds+= m.predict_proba(x_test)[:,1]\n        i = i + 1\n    preds=preds/Folds\n\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['importance'] = m.booster_.feature_importance(importance_type='gain')\n    \n    print(f\"Average StratifiedKFold Score : {sum(err)/Folds} \")\n    oof_score = roc_auc_score(y, oofs)\n    print(f'\\nOOF Auc is : {oof_score}')\n    \n    oofs=pd.DataFrame(oofs,columns=['lgbmoof'])\n    preds=pd.DataFrame(preds,columns=['lgbmpred'])\n    \n    oofs.to_csv('lgbmoof.csv',index=False)\n    preds.to_csv('lgbmpred.csv',index=False)\n\n    return imp_df","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:47.258587Z","iopub.execute_input":"2021-05-31T07:00:47.259124Z","iopub.status.idle":"2021-05-31T07:00:47.280627Z","shell.execute_reply.started":"2021-05-31T07:00:47.259073Z","shell.execute_reply":"2021-05-31T07:00:47.279625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imp = lgbm_model()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:00:47.28183Z","iopub.execute_input":"2021-05-31T07:00:47.282293Z","iopub.status.idle":"2021-05-31T07:09:05.725903Z","shell.execute_reply.started":"2021-05-31T07:00:47.282246Z","shell.execute_reply":"2021-05-31T07:09:05.724922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to display feature importance...\ndef display_importances(feature_importance_df_,model):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:30].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title(model+\" Features (avg over folds)\")\n    plt.tight_layout()\n    plt.savefig(model +\"_importances-01.png\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:09:06.877905Z","iopub.execute_input":"2021-05-31T07:09:06.878335Z","iopub.status.idle":"2021-05-31T07:09:06.886475Z","shell.execute_reply.started":"2021-05-31T07:09:06.878298Z","shell.execute_reply":"2021-05-31T07:09:06.885263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importance based on gain...\n\ndisplay_importances(imp,\"LGBM\")","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:09:08.069671Z","iopub.execute_input":"2021-05-31T07:09:08.070069Z","iopub.status.idle":"2021-05-31T07:09:08.824997Z","shell.execute_reply.started":"2021-05-31T07:09:08.070014Z","shell.execute_reply":"2021-05-31T07:09:08.823992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBOOST","metadata":{}},{"cell_type":"code","source":"def xgb_model():\n    \n    x,y,x_test,cat_features,train_features=savedata()\n    \n    params={'lambda': 1.417495651744778, \n        'alpha': 0.4281901245971981, \n        'colsample_bytree': 0.7, \n        'subsample': 0.8, \n        'learning_rate': 0.016,\n        'max_depth': 9, \n        'random_state': 2020, \n        'min_child_weight': 30}\n    \n    err = [] \n\n    oofs = np.zeros(shape=(len(x)))\n    preds = np.zeros(shape=(len(x_test)))\n\n    Folds=8\n\n    fold = StratifiedKFold(n_splits=Folds, shuffle=True, random_state=2020)\n    i = 1\n\n    for train_index, test_index in fold.split(x, y):\n        x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n        m = XGBClassifier(n_estimators=10000,**params)\n    \n        m.fit(x_train, y_train,eval_set=[(x_val, y_val)], early_stopping_rounds=30,verbose=False,eval_metric='auc')\n    \n        pred_y = m.predict_proba(x_val)[:,1]\n        oofs[test_index] = pred_y\n        print(i, \" err_xgb: \", roc_auc_score(y_val,pred_y))\n        err.append(roc_auc_score(y_val,pred_y))\n        preds+= m.predict_proba(x_test)[:,1]\n        i = i + 1\n    preds=preds/Folds\n    \n    print(f\"Average StratifiedKFold Score : {sum(err)/Folds} \")\n    oof_score = roc_auc_score(y, oofs)\n    print(f'\\nOOF Auc is : {oof_score}')\n    \n    oofs=pd.DataFrame(oofs,columns=['xgboof'])\n    preds=pd.DataFrame(preds,columns=['xgbpred'])\n    \n    oofs.to_csv(Data_dir+'xgbmoof.csv',index=False)\n    preds.to_csv(Data_dir+'xgbmpred.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T06:24:09.771901Z","iopub.execute_input":"2021-05-31T06:24:09.77232Z","iopub.status.idle":"2021-05-31T06:24:09.784967Z","shell.execute_reply.started":"2021-05-31T06:24:09.772284Z","shell.execute_reply":"2021-05-31T06:24:09.783829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Blend","metadata":{}},{"cell_type":"code","source":"def final_process_data():\n    \n    train = pd.read_csv(\"../input/jobathon-may-2021-credit-card-lead-prediction/train.csv\")\n    test = pd.read_csv(\"../input/jobathon-may-2021-credit-card-lead-prediction/test.csv\")\n    sub= pd.read_csv(\"../input/jobathon-may-2021-credit-card-lead-prediction/sample_submission.csv\")\n    \n    test_region_list=test['Region_Code'].tolist()\n    train=train[train['Region_Code'].isin(test_region_list)]\n    \n    target=train[['Is_Lead']]\n    \n    lgbmpred = pd.read_csv('../input/mayjobathon-model/lgbmpred.csv')\n    xgbpred = pd.read_csv('../input/mayjobathon-model/xgbmpred.csv')\n    catboostpred = pd.read_csv('../input/mayjobathon-model/catboostpred.csv')\n    \n    total_pred = pd.concat([lgbmpred,xgbpred,catboostpred], axis=1)\n    \n    lgbmoof = pd.read_csv('../input/mayjobathon-model/lgbmoof.csv')\n    xgboof = pd.read_csv('../input/mayjobathon-model/xgbmoof.csv')\n    catboostoof = pd.read_csv('../input/mayjobathon-model/catboostoof.csv')\n    \n    total_oof = pd.concat([lgbmoof,xgboof,catboostoof], axis=1)\n    \n    return train,target,sub,test,total_pred,total_oof","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:09:24.940023Z","iopub.execute_input":"2021-05-31T07:09:24.940597Z","iopub.status.idle":"2021-05-31T07:09:24.949815Z","shell.execute_reply.started":"2021-05-31T07:09:24.940543Z","shell.execute_reply":"2021-05-31T07:09:24.94874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def findbestweight(df1,df2,target):\n    max_roc = -1\n    max_weight = 0\n    max_ensemble_oof  = 0\n    weights_list = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]\n    for weight in weights_list:\n        ensemble_oof = weight*df1 + (1-weight)*df2\n        roc_score = roc_auc_score(target,ensemble_oof)\n        if roc_score > max_roc:\n            max_ensemble_oof = ensemble_oof\n            max_roc = roc_score\n            max_weight = weight\n    print(\"The best weights for blending is {0} with AUC {1}\".format(max_weight, max_roc))\n    return max_weight","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:09:25.455137Z","iopub.execute_input":"2021-05-31T07:09:25.455497Z","iopub.status.idle":"2021-05-31T07:09:25.462835Z","shell.execute_reply.started":"2021-05-31T07:09:25.455463Z","shell.execute_reply":"2021-05-31T07:09:25.461633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def blend():\n    train,target,sub,test,total_pred,total_oof=final_process_data()\n    weight1=findbestweight(total_oof['lgbmoof'],total_oof['xgboof'],target)\n    lgb_xgb=weight1*total_oof['lgbmoof'] +(1-weight1)*total_oof['xgboof']\n    \n    weight2=findbestweight(lgb_xgb,total_oof['catboostoof'],target)\n    lgb_xgb_cat=weight2*lgb_xgb +(1-weight2)*total_oof['catboostoof']\n    \n    lgb_xgb_cat_pred=(weight1*total_pred['lgbmpred']+(1-weight1)*total_pred['xgbpred'])*weight2+total_pred['catboostpred']*(1-weight2)\n    \n    sub['Is_Lead']=lgb_xgb_cat_pred\n    sub.to_csv('./blend.csv',index=False)\n    print(sub)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:09:25.929139Z","iopub.execute_input":"2021-05-31T07:09:25.929525Z","iopub.status.idle":"2021-05-31T07:09:25.937315Z","shell.execute_reply.started":"2021-05-31T07:09:25.929488Z","shell.execute_reply":"2021-05-31T07:09:25.936128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blend()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T07:09:28.56112Z","iopub.execute_input":"2021-05-31T07:09:28.561465Z","iopub.status.idle":"2021-05-31T07:09:33.33502Z","shell.execute_reply.started":"2021-05-31T07:09:28.561435Z","shell.execute_reply":"2021-05-31T07:09:33.333547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Score : 0.8732 AUC_ROC Score in private leaderboard","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}