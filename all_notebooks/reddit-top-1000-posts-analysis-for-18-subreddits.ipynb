{"cells":[{"metadata":{"_uuid":"1abd7cda5c06e567e2edb0763c0721d0de993ad9"},"cell_type":"markdown","source":"# Reddit Top 1000 Posts Analysis (for 18 Subreddits)\n\n<p>&nbsp;</p>\n<img src=\"https://s3.eu-west-2.amazonaws.com/ammar-blog-post-images/2018/Nov/ytanalysisp.png\" width=600>\n<p>&nbsp;</p>\n\n[Reddit](https://reddit.com) is an American social news aggregation, web content rating, and discussion website. Registered members submit content to the site such as links, text posts, and images, which are then voted up or down by other members. Posts are organized by subject into user-created boards called \"subreddits\", which cover a variety of topics including news, science, movies, video games, music, books, fitness, food, and image-sharing. Submissions with more up-votes appear towards the top of their subreddit. As of February 2018, Reddit had 542 million monthly visitors (234 million unique users), ranking as the #3 most visited website in U.S. and #6 in the world, according to Alexa Internet. [\\[Wikipedia\\]](https://en.wikipedia.org/wiki/Reddit)\n\nIn this notebook, we will analyze the **top 1000 posts of 18** of the most popular subreddits on reddit. These subreddits are:\n* AskReddit\n* aww\n* books\n* explainlikeimfive\n* food\n* funny\n* GetMotivated\n* gifs\n* IAmA\n* Jokes\n* LifeProTips\n* movies\n* pics\n* Showerthoughts\n* todayilearned\n* videos\n* woahdude\n* worldnews\n\nThis analysis uses a [dataset](https://www.kaggle.com/ammar111/reddit-top-1000/home) which is a part of a [wider dataset](https://github.com/umbrae/reddit-top-2.5-million). Ths data of these datasets was pulled between August 15-20 of August **2013**.\n\n## Table of Contents\n* [Importing some packages](#p1)\n* [Reading the data](#p2)\n* [Getting a feel of the datasets](#p3)\n* [Preprocessing and data cleaning](#p4)\n* [Adding some features](#p5)\n* [Most common words in top-posts titles](#p6)\n    * [Most common words for all subreddits together](#p6-1)\n    * [Word cloud of the most common words](#p6-2)\n* [Most common words in body texts of some subreddits](#p7)\n* [Most common 2-grams in top-posts titles](#p8)\n* [Most common 2-grams for all subreddits together](#p8-1)\n* [2-grams word cloud](#p8-2)\n* [Most common 3-grams in top-posts titles](#p9)\n* [Most common 4-grams in top-posts titles](#p10)\n* [Data analysis and visulaization](#p11)\n    * [Distribution of title length](#p11-1)\n    * [Distribution of the number of comments](#p11-2)\n    * [Distribution of the number of upvotes and downvotes](#p11-3)\n    * [Distribution of score](#p11-4)\n    * [Correlation between variables](#p11-5)\n\n\n## <a name=\"p1\"></a>Importing some packages\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom textblob import TextBlob\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nimport wordcloud\nfrom collections import Counter\nfrom pprint import pprint\nimport random\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n# import re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"958034ac8b0509e977ffe9017c93d326f20a5249"},"cell_type":"markdown","source":"We make some configurations to enhance visualizations and appearance in general"},{"metadata":{"trusted":true,"_uuid":"e2539d55b99c24affd3a7869b6fa1816139c09fe"},"cell_type":"code","source":"pd.options.display.float_format = '{:.2f}'.format\nsns.set(style=\"ticks\")\nplt.rc('figure', figsize=(8, 5), dpi=100)\nplt.rc('axes', facecolor=\"#ffffff\", linewidth=0.4, grid=True, labelpad=8, labelcolor='#616161')\nplt.rc('patch', linewidth=0)\nplt.rc('xtick.major', width=0.2)\nplt.rc('ytick.major', width=0.2)\nplt.rc('grid', color='#9E9E9E', linewidth=0.4)\nplt.rc('text', color='#282828')\nplt.rc('savefig', pad_inches=0.3, dpi=300)\n\n# Hiding warnings for cleaner display\nwarnings.filterwarnings('ignore')\n\n# Configuring some notebook options\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n# If you want interactive plots, uncomment the next line\n# %matplotlib notebook","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3af16499d06065216d3b7cfe562851ba5c725194"},"cell_type":"markdown","source":"## <a name=\"p2\"></a>Reading the data\nWe read thr `csv` files containing the data of the subreddits we want to analyze. Each subreddit has its own dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"askReddit_df = pd.read_csv('../input/AskReddit.csv')\naww_df = pd.read_csv('../input/aww.csv')\nbooks_df = pd.read_csv('../input/books.csv')\nexplainlikeimfive_df = pd.read_csv('../input/explainlikeimfive.csv')\nfood_df = pd.read_csv('../input/food.csv')\nfunny_df = pd.read_csv('../input/funny.csv')\ngetMotivated_df = pd.read_csv('../input/GetMotivated.csv')\ngifs_df = pd.read_csv('../input/gifs.csv')\niAmA_df = pd.read_csv('../input/IAmA.csv')\njokes_df = pd.read_csv('../input/Jokes.csv')\nlifeProTips_df = pd.read_csv('../input/LifeProTips.csv')\nmovies_df = pd.read_csv('../input/movies.csv')\npics_df = pd.read_csv('../input/pics.csv')\nshowerthoughts_df = pd.read_csv('../input/Showerthoughts.csv')\ntodayilearned_df = pd.read_csv('../input/todayilearned.csv')\nvideos_df = pd.read_csv('../input/videos.csv')\nwoahdude_df = pd.read_csv('../input/woahdude.csv')\nworldnews_df = pd.read_csv('../input/worldnews.csv')\n\n# We create these two lists for easier interaction with the datasets later\nsubreddits = [askReddit_df, aww_df, books_df, explainlikeimfive_df, food_df, funny_df,\n              getMotivated_df, gifs_df, iAmA_df, jokes_df, lifeProTips_df, movies_df,\n              pics_df, showerthoughts_df, todayilearned_df, videos_df, \n              woahdude_df, worldnews_df]\n\nsubreddit_names = ['AskReddit', 'aww', 'books', 'explainlikeimfive', 'food', 'funny',\n                   'GetMotivated', 'gifs', 'IAmA', 'Jokes', 'LifeProTips', 'movies',\n                   'pics', 'Showerthoughts', 'todayilearned', 'videos', 'woahdude', 'worldnews']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a766a55f81409f382d261bf71a509f837f069ada"},"cell_type":"markdown","source":"## <a name=\"p3\"></a>Getting a feel of the datasets\nLet's see how the first rows of AskReddit dataset look like. All datasets have the same structure, the same columns."},{"metadata":{"trusted":true,"_uuid":"9f9e38f222275aee65a0b351565f221ce15c272f"},"cell_type":"code","source":"askReddit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06bc6bf91b939a3a9c9e3fc6d506357c693116cd"},"cell_type":"markdown","source":"Then let's see some information about AskReddit dataset also"},{"metadata":{"trusted":true,"_uuid":"dd628b61eadb7cdf9824b0eb0bd508fd39c7b67f","scrolled":false},"cell_type":"code","source":"askReddit_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c160149ffd0c0309ac9507cbb6269c1d31b87162"},"cell_type":"markdown","source":"We can see that there are 1,000 entries in the dataset. We can see also that there are missing values in some columns. For example, `link_flair_text` column has only 5 non-null values, which means that it has `1000 - 5 = ` 995 missing values. \n\n## <a name=\"p4\"></a>Preprocessing and data cleaning\nBefore we deal with the missing-values issue, let's remove some columns that are not useful in our analysis"},{"metadata":{"trusted":true,"_uuid":"1096a7b0151ee11fd0ef30f5bd5783382346da2a"},"cell_type":"code","source":"# We loop through all our subreddit datasets and remove some columns\n# from each of them\nfor df in subreddits:\n    df.drop(['link_flair_text', 'thumbnail', 'subreddit_id', 'link_flair_css_class', \n                       'author_flair_css_class', 'name', 'url', 'distinguished'],\n                      axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7fe630518782f6ce1983a2608fb9de751e84e5b"},"cell_type":"markdown","source":"Now, let's see which columns in all of our datasets have missing values"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b43213fdf820b5c1b1e44fbfd3418fd2e43f1005"},"cell_type":"code","source":"for df, name in zip(subreddits, subreddit_names):\n    # get the number of null values in each column of the dataset\n    null_sum = df.isna().sum()\n    # keep only the columns that have missing values\n    null_sum = null_sum[null_sum > 0]\n    print(name, 'dataset')\n    for k,v in zip(null_sum.index, null_sum.values):\n        print(k, ': ', v)\n    print('-------------')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d45a9211b204a8250cc51313f24378f2d45b5bcc"},"cell_type":"markdown","source":"We can see now that only `selftext` column has missing values in all of the datasets. This is probably because not all posts on reddit have body text (i.e. they just have titles and links). Now, let's replace each null value with an empty string to get rid of the missing-values problem"},{"metadata":{"trusted":true,"_uuid":"e31bba402b2816033549dd01e106606eab37c24e"},"cell_type":"code","source":"for df in subreddits:\n    df['selftext'].fillna(value=\"\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fcc67f0754ae82529357c2b082b52a36eb633a5"},"cell_type":"markdown","source":"## <a name=\"p5\"></a>Adding some features\nLet's add more features that might be useful in analyzing our datasets. First let's add a column that represents the title length for each post"},{"metadata":{"trusted":true,"_uuid":"d02fda757b4088e6bea6ebf02f645c4d9e86d0f7"},"cell_type":"code","source":"for df in subreddits:\n    df['title_length'] = df['title'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89a6047a911eba97ab45def8c6a1e1f82bfc9b06"},"cell_type":"markdown","source":"Then, let's add a column that represents the number of fully capitalized words in the title of each post"},{"metadata":{"trusted":true,"_uuid":"9fcc0bd5d68cec01abc74d45b8ffdbaeab3465d4"},"cell_type":"code","source":"def num_capitalized_word(s):\n    c = 0\n    for w in s.split():\n        if w.isupper():\n            c += 1\n    return c\n\nfor df in subreddits:\n    df['num_capitalized'] = df['title'].apply(num_capitalized_word)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3608c8935ed195b3408e089b701d8301fa63d614"},"cell_type":"markdown","source":"## <a name=\"p6\"></a>Most common words in top-posts titles\nLet's find out what are the most frequent words in top-posts titles. Are there some words that are common between the top posts of our subreddits?\n\nThere are two we better take care of in finding the most common words: first, we need to take care of contractions such as 'haven't' and 'you're'. To achieve consistency, we will convert them to their expanded form. This means that we will convert 'haven't' to 'have not',  'you're' to 'you are', etc. We will use a dictionary that maps the contraction to its expanded form. This dictionary is based on [this Wikipedia page](https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions) and I copied it from [this post on Stack Overflow](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python).\n\nYou can see from the dictionary of contractions below that some contractions have more than one possible expansion (e.g. 'it'd' can be expanded as 'it would' or 'it had'). In this case, we will choose a random one of them.\n\nAnother thing we want to handle is [stop words](https://en.wikipedia.org/wiki/Stop_words) which are words very common words like 'the', 'have', 'we', 'and 'which'. We will remove these words before we extract the most common words from the datasets. We will use a list of stop words provided with `nltk` Python library for that."},{"metadata":{"trusted":true,"_uuid":"96606e8208ede91a92a74cf41fb4dd1316a4bd52"},"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\n# if a contraction has more than one possible expanded forms, we replace it \n# with a list of these possible forms\ntmp = {}\nfor k,v in contractions.items():\n    if \"/\" in v:\n        tmp[k] = [x.strip() for x in v.split(sep=\"/\")]\n    else:\n        tmp[k] = v\ncontractions = tmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff5aa9fa4ccf519c25da6bfef3bdc2944ac4b716"},"cell_type":"markdown","source":"Let's see the most common words for each subreddit. For every common word, the number of times it occured in top-posts titles is shown also."},{"metadata":{"trusted":true,"_uuid":"a49e688204193eb2eda16136c7ea70a22a59ab05","scrolled":true},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r\"[\\w']+\")\nsubreddit_words = []\nfor df, name in zip(subreddits, subreddit_names):\n    all_titles = ' '.join([x.lower() for x in df['title']])\n    for k,v in contractions.items():\n        if isinstance(v, list):\n            v = random.choice(v)\n        all_titles = all_titles.replace(k.lower(), v.lower())\n    words = list(tokenizer.tokenize(all_titles))\n    words = [x for x in words if x not in stopwords.words('english')]\n    subreddit_words.append(words)\n    print('Most common words in ' + name, '*****************', sep='\\n')\n    pprint(Counter(words).most_common(35), compact=True)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f34c60ce1a2b0a505abe1769716254c28a95b5b"},"cell_type":"markdown","source":"So, as we can see,\n* 'ever', 'know', 'best', etc. are common in AskReddit\n* 'dog', 'cat', 'little', 'baby', etc. are common in aww\n* 'made', 'cake', 'cheese', etc. are common in food\n* 'life', 'today', etc. are common in GetMotivated\n* 'us', 'korea', 'north', etc are common in worldnews\n* etc.\n\n### <a name=\"p6-1\"></a>Most common words for all subreddits together\nNow let's see what are the most common words for the top posts of all the 16 subreddits we have"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"cfc1707d18702e7083e35a6598dfccde0069f7e6"},"cell_type":"code","source":"# flattening the list\nsubreddit_words_f = [x for y in subreddit_words for x in y]\nprint('Most common words in all subreddits', '*****************', sep='\\n')\npprint(Counter(subreddit_words_f).most_common(35), compact=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da509160a792d20db7fa27c670d4ad491fdafeee"},"cell_type":"markdown","source":"We can see that 'one', 'like', 'people', 'new', 'time', 'made', 'years', 'year', etc. are common in the titles of the top posts of our 16 subreddits together.\n\n### <a name=\"p6-2\"></a>Word cloud of the most common words\nNow let's create a word cloud to visualize the most common words of our subreddits. The word size represents its frequency: the bigger the word, the more common it is. "},{"metadata":{"trusted":true,"_uuid":"c2317454c84c0dd7cd364e076a04472d3ce5b9d2","scrolled":false},"cell_type":"code","source":"# a function to get custom colors for the word cloud\ndef col_func(word, font_size, position, orientation, font_path, random_state):\n    colors = ['#b58900', '#cb4b16', '#dc322f', '#d33682', '#6c71c4', \n              '#268bd2', '#2aa198', '#859900']\n    return random.choice(colors)\n\nfd = {\n    'fontsize': '32',\n    'fontweight' : 'normal',\n    'verticalalignment': 'baseline',\n    'horizontalalignment': 'center',\n}\n\nfor df, name, words in zip(subreddits, subreddit_names, subreddit_words):\n    wc = wordcloud.WordCloud(width=1000, height=500, collocations=False, \n                             background_color=\"#fdf6e3\", color_func=col_func, \n                             max_words=200,random_state=np.random.randint(1,8)\n                            ).generate_from_frequencies(dict(Counter(words)))\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.imshow(wc, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.set_title(name, pad=24, fontdict=fd)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d3247684ded8cdf85495c3591b53ebce6e7eae5"},"cell_type":"markdown","source":"## <a name=\"p7\"></a>Most common words in body texts of some subreddits\nNow let's choose some subreddits that have relatively few missing values in their `selftext` column, and see the most common words in these texts followed by a word cloud as above"},{"metadata":{"trusted":true,"_uuid":"e19b273bedff18d225ae58fd46668e8df4fbd87e"},"cell_type":"code","source":"selftext_subreddits = [askReddit_df, explainlikeimfive_df, iAmA_df, jokes_df]\nselftext_subreddit_names = ['AskReddit', 'explainlikeimfive', 'IAmA', 'Jokes']\nselftext_subreddit_words = []\n\nfor df, name in zip(selftext_subreddits, selftext_subreddit_names):\n    selftexts = ' '.join([x.lower() for x in df['selftext']])\n    for k,v in contractions.items():\n        if isinstance(v, list):\n            v = random.choice(v)\n        selftexts = selftexts.replace(k.lower(), v.lower())\n    words = list(tokenizer.tokenize(selftexts))\n    words = [x for x in words if x not in stopwords.words('english')]\n    selftext_subreddit_words.append(words)\n    print(name, '*****************', sep='\\n')\n    pprint(Counter(words).most_common(25), compact=True)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06142612c6f1a7e708866f68d9a3db5d8c4fb1d2"},"cell_type":"code","source":"for df, name, words in zip(selftext_subreddits, selftext_subreddit_names, selftext_subreddit_words):\n    wc = wordcloud.WordCloud(width=1000, height=500, collocations=False, \n                             background_color=\"#002b36\", color_func=col_func, \n                             max_words=200, random_state=np.random.randint(1,8)\n                            ).generate_from_frequencies(dict(Counter(words)))\n    plt.figure(figsize=(20,10))\n    plt.imshow(wc, interpolation='bilinear')\n    _ = plt.axis(\"off\")\n    _ = plt.title(name, fontdict=fd, pad=24)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a010ee3fceef4016f86228d6da87e4a7c4bb7f87"},"cell_type":"markdown","source":"## <a name=\"p8\"></a>Most common 2-grams in top-posts titles\nWhat is 2-gram? 2-gram is a contiguous sequence of 2 words from a given text. For example, if our text is 'lorem ipsum dolor sit amet', then the 2-grams are: 'lorem ipsum', 'ipsum dolor', 'dolor sit', and 'sit amet'. So let's find the most common 2-grams in the titles of our subreddit top posts to see if they have common combinations of words. Note that here we keep contractions and stop words untouched."},{"metadata":{"trusted":true,"_uuid":"57b0f81de0be4ca6089bb1585ff20a1d73afb4cc","scrolled":true},"cell_type":"code","source":"from nltk.util import ngrams\nsubreddit_2ngrams = []\nfor df, name in zip(subreddits, subreddit_names):\n    ng = [ngrams(tokenizer.tokenize(tw.lower()), \n                 n=2) for tw in df['title']]\n    # flattening the list\n    ng = [x for y in ng for x in y]\n    subreddit_2ngrams.append(ng)\n    print(name, '*****************', sep='\\n')\n    pprint(Counter(ng).most_common(25), compact=False)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7ca98b559028a38e5387afb5407e4e2530018f8"},"cell_type":"markdown","source":"### <a name=\"p8-1\"></a>Most common 2-grams for all subreddits together\nNow let's get the most common 2-grams for all subreddits together, but in this case, we will remove 2-grams whose one of their words is a stop word"},{"metadata":{"trusted":true,"_uuid":"3685a766da8b55244e596de05cc69edc7a8b3904"},"cell_type":"code","source":"# flattening the list\nsubreddit_2ngrams_f = [x for y in subreddit_2ngrams for x in y]\n# removing 2-grams that contain stop words\ntmp = []\nfor n in subreddit_2ngrams_f:\n    f = 0\n    for w in n:\n        if w in stopwords.words('english'):\n            f = 1\n    if f == 0:\n        tmp.append(n)\nsubreddit_2ngrams_f = tmp\npprint(Counter(subreddit_2ngrams_f).most_common(50), compact=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"314e77528a6647c8cfc5e109523e7921299738ec"},"cell_type":"markdown","source":"### <a name=\"p8-2\"></a>2-grams word cloud\nNow let's see the word cloud of the most common n-grams for each subreddit"},{"metadata":{"trusted":true,"_uuid":"f3354eaf9e4e95933b7c7ccf5dbae6155e8afeca","scrolled":false},"cell_type":"code","source":"for df, name, ngrams_2 in zip(subreddits, subreddit_names, subreddit_2ngrams):\n    wc = wordcloud.WordCloud(width=2000, height=1000, \n                             collocations=False, background_color=\"black\", \n                             colormap=\"Set3\", max_words=66,\n                             normalize_plurals=False,\n                             regexp=r\".+\", \n                             random_state=7).generate_from_frequencies(dict(Counter([x + ' ' + y for x,y in ngrams_2])))\n    plt.figure(figsize=(20,15))\n    plt.imshow(wc, interpolation='bilinear')\n    _ = plt.axis(\"off\")\n    _ = plt.title(name, fontdict=fd, pad=24)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49451f50db899715c2661c5d199dfcdd6123b7a9"},"cell_type":"markdown","source":"## <a name=\"p9\"></a>Most common 3-grams in top-posts titles\nSimilar to the previous part, we will find now the most common 3-grams in each of our 16 subreddits"},{"metadata":{"trusted":true,"_uuid":"6880fce1266fad1fabb0e2e9b0d5d84e322a9b85"},"cell_type":"code","source":"subreddit_3ngrams = []\nfor df, name in zip(subreddits, subreddit_names):\n    ng = [ngrams(tokenizer.tokenize(tw.lower()), \n                 n=3) for tw in df['title']]\n    # flattening the list\n    ng = [x for y in ng for x in y]\n    subreddit_3ngrams.append(ng)\n    print(name, '*****************', sep='\\n')\n    pprint(Counter(ng).most_common(25), compact=False)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"020ff0a24e7f6a9929fa4bf115fd1975bb0e2103"},"cell_type":"markdown","source":"## <a name=\"p10\"></a>Most common 4-grams in top-posts titles\nNow will find now the most common 4-grams in each of our 16 subreddits"},{"metadata":{"trusted":true,"_uuid":"81e423b8d6de480082bad3fba95344897289971d"},"cell_type":"code","source":"subreddit_4ngrams = []\nfor df, name in zip(subreddits, subreddit_names):\n    ng = [ngrams(tokenizer.tokenize(tw.lower()), \n                 n=4) for tw in df['title']]\n    # flattening the list\n    ng = [x for y in ng for x in y]\n    subreddit_4ngrams.append(ng)\n    print(name, '*****************', sep='\\n')\n    pprint(Counter(ng).most_common(25), compact=False)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23b25d4a94ac9c7bac19f20bd6a048058c7ba19f"},"cell_type":"markdown","source":"## <a name=\"p11\"></a>Data analysis and visulaization\nLet's explore our data by finding the distribution of some variables and looking at the relationships between them.\n\n### <a name=\"p11-1\"></a>Distribution of title length\nFirst, let's examine the distibution of title length for each subreddit by using histogram plots. This allows us to see for example how many posts have title length between 30 and 40 characters, how many posts have title length between 40 and 50 characters, etc."},{"metadata":{"trusted":true,"_uuid":"a15eb99df9e0e058d55f4334b6cf0b893eac7f25","scrolled":false},"cell_type":"code","source":"fig, axes = plt.subplots(6, 3, figsize=(20,30), sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.5, wspace=0.4)\n# fig.text(0.5, 0.04, 'Title Length', ha='center', fontdict={'size':'18'})\nfor ax, df, name in zip(axes.flat, subreddits, subreddit_names):\n    sns.distplot(df[\"title_length\"], kde=False, hist_kws={'alpha': 1}, color=\"#0747A6\", ax=ax)\n    ax.set_title(name, fontdict={'size': 16}, pad=14)\n    ax.set(xlabel=\"Title length\", ylabel=\"Number of posts\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ff452e6fba5048eaafafd90a6092cb2f85eb2c0"},"cell_type":"markdown","source":"Note how the histogram of 'todayilearned' subreddit shows us that the top-posts titles of this subreddit are generally longer than other subreddits. We can see also that 'GetMotivated', 'gifs',  'Jokes', and 'woahdude' subreddits have generally shorter titles than other subreddits.\n\n### <a name=\"p11-2\"></a>Distribution of the number of comments\nSimilar to what we did for the distribution of title length, we now explore the distribution of the number of comments for each subreddit"},{"metadata":{"trusted":true,"_uuid":"f585bdc08a56b1ec05b206b75f3d31c0dd6728b8","scrolled":false},"cell_type":"code","source":"fig, axes = plt.subplots(6, 3, figsize=(20,30))\nfig.subplots_adjust(hspace=0.5, wspace=0.4)\n# fig.text(0.5, 0.04, 'Number of Comments', ha='center', fontdict={'size':'18'})\n# fig.text(0.04, 0.5, 'Number of Posts', va='center', rotation='vertical', fontdict={'size':'18'})\nfor ax, df, name in zip(axes.flat, subreddits, subreddit_names):\n    sns.distplot(df[\"num_comments\"], kde=False, hist_kws={'alpha': 1}, color=\"#0747A6\", ax=ax)\n    ax.set_title(name, fontdict={'size': 16}, pad=14)\n    ax.set(xlabel=\"Number of comments\", ylabel=\"Number of posts\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b1a19599edc975fb9df7d2a48a1678776e7b9e5"},"cell_type":"markdown","source":"Note that this time each plot has its own axes (i.e. the axes are not shared between all plots).\n\nWe can see that posts of 'AskReddit' subreddit have more comments than other subreddits. We can see also that posts of 'funny', 'IAmA', 'movies', 'pics', 'todayilearned', 'videos' and 'worldnews' have a large  number of comments relatively. Moreover, we notice that 'food', 'GetMotivated', 'Jokes', 'Showerthoughts' and 'woahdude' have the less comments than other subreddits.\n\nFor a clearer comparison, let's compare the medians of the number of comments for our subreddits"},{"metadata":{"trusted":true,"_uuid":"d5c4914d0f86a9b9f032835d0ccf92c6fdc60a0f"},"cell_type":"code","source":"medians = []\nfor df in subreddits:\n    medians.append(df['num_comments'].median())\n\nplt.rc('axes', labelpad=16)\nfig, ax = plt.subplots(figsize=(14,8))\nd = pd.DataFrame({'subreddit': subreddit_names, 'num_comments_median': medians})\nsns.barplot(x=\"subreddit\", y=\"num_comments_median\", data=d, palette=sns.cubehelix_palette(n_colors=24, reverse=True), ax=ax);\nax.set(xlabel=\"Subreddit\", ylabel=\"Median\");\nax.set_xticklabels(ax.get_xticklabels(), rotation=90);\nplt.rc('axes', labelpad=8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3bfd1bd0626221f1fb7a7b48cfe6faa436b13e2"},"cell_type":"markdown","source":"### <a name=\"p11-3\"></a>Distribution of the number of upvotes and downvotes\nNow, we will explore the distribution of the number of upvotes and the distribution of the number of downvotes for each subreddit. We will plot both distributions together where the number of upvotes has the blue color and the number of downvotes has the orange color"},{"metadata":{"trusted":true,"_uuid":"61761c8ea81b952a575d7f2779fc82800aa068f3","scrolled":false},"cell_type":"code","source":"fig, axes = plt.subplots(6, 3, figsize=(20,30))\nfig.subplots_adjust(hspace=0.5, wspace=0.4)\nfor ax, df, name in zip(axes.flat, subreddits, subreddit_names):\n    sns.distplot(df[\"ups\"], kde=False, hist_kws={'alpha': 0.5}, color=\"#0747A6\", ax=ax)\n    sns.distplot(df[\"downs\"], kde=False, hist_kws={'alpha': 0.5}, color=\"#FF5630\", ax=ax)\n    ax.set_title(name, fontdict={'size': 16}, pad=14)\n    ax.set(xlabel=\"Number of upvotes/downvotes\", ylabel=\"Number of posts\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acdcb81e58c90a67c42cb3df9176e5c8c7cdaf85"},"cell_type":"markdown","source":"We can see that for some subreddits like 'books', 'food', 'explainlikeimfive', 'GetMotivated', 'Jokes', and 'Showerthoughts', posts have noticeably more upvote than downvotes, as you can see the blue shape is more to the right than the orange shape in this case. We can notice also that for subreddits like 'funny' and 'pics', posts have roughly similar number of upvotes and downvotes.\n\n### <a name=\"p11-4\"></a>Distribution of score\nNow, let's examine the score variable which represents `number of upvotes - number of downvotes`. We will use violin plots for doing so. For a violin plot, the width of a violin represents the frequency. This means that if a violin is the widest between 300 and 400, then the area between 300 and 400 contains more data than other areas. Moreover, inside the violin, you can see statistical measures such as the median, as illustrated by the image below\n\n![](https://s3.eu-west-2.amazonaws.com/ammar-blog-post-images/2018/Nov/violin_plot.svg)"},{"metadata":{"trusted":true,"_uuid":"912fe3a29782adcec29a1f693a5c72caa0bd9b89"},"cell_type":"code","source":"comments = pd.DataFrame(columns=['subreddit', 'score'])\nn = []\nl = []\nfor df, name in zip(subreddits, subreddit_names):\n    cl = list(df['score'])\n    l.extend(cl)\n    n.extend([name] * len(cl))\ncomments['subreddit'] = pd.Series(n)\ncomments['score'] = pd.Series(l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"280fe67544c418b99f6806f0128f907baf95eb76"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 20))\nsns.violinplot(x='score', y='subreddit', data=comments, scale='width', inner='box', ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f27058336af9a540e270b205fbe8befdc56b0038"},"cell_type":"markdown","source":"### <a name=\"p11-5\"></a>Correlation between variables\nNow, we will see how some of our dataset variables are correlated with each other: for example, we would like to see how the number of comments and the score are correlated, meaning do they increase and decrease together (positive correlation)? Does one of them increase when the other decreases and vice versa (negative correlation)? Or are they not correlated?\n\nCorrelation is represented as a value between -1 and +1 where +1 denotes the highest positive correlation, -1 denotes the highest negative correlation, and 0 denotes that there is no correlation. We will use heatmaps to visualize correlation using colors"},{"metadata":{"trusted":true,"_uuid":"2de4f78dd81f472929bd5e0ce098f83c36d6cf0c","scrolled":false},"cell_type":"code","source":"fig, axes = plt.subplots(8, 2, figsize=(20,60))\nfig.subplots_adjust(hspace=0.7, wspace=0.3)\nfor ax, df, name in zip(axes.flat, subreddits, subreddit_names):\n    sns.heatmap(df[['score', 'ups', 'downs', 'num_comments', 'title_length', 'num_capitalized']].corr(), annot=True, cmap=sns.cubehelix_palette(as_cmap=True), ax=ax)\n    ax.set_title(name, fontdict={'size':'18'}, pad=14)\n    ax.set(xlabel=\"\", ylabel=\"\")\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18d05c4edef6da4b08f7495c6d8edfc5114680ec"},"cell_type":"markdown","source":"For example, we can see that upvotes, downvotes, and score are highly correlated for 'GetMotivated' subreddit. Also, we can see that the number of comments is highly correlated with both upvotes and downvotes for 'IAmA' subreddit. Other things also can be concluded from these heatmaps.\n\n## End\nIf you like this analysis, please consider to upvote it at the top of this page.\nFollow me on [Twitter](https://twitter.com/ammar_cel) to know when I publish something new, or visit [my website and blog](http://ammar-alyousfi.com?ref=redditAnalysis)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}