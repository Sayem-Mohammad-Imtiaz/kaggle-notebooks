{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport random\nimport seaborn\n\nseaborn.set(style='whitegrid'); seaborn.set_context('talk')\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom sklearn.datasets import load_iris\niris_data = load_iris()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas\nfrom pandas.plotting import scatter_matrix\n\n\ndataset = pandas.read_csv('/kaggle/input/uci-ionosphere/ionosphere_data_kaggle.csv')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Manually separating our dataset\n\nIt is here that we will select our samples to train and test the algorithms: **80% Training Samples and 20% Test**\n<div class=\"container-fluid\">\n  <div class=\"row\">\n      <div class=\"col-md-2\" align='center'>\n      </div>\n      <div class='col-md-8' align='center'>\n      </div>\n      <div class=\"col-md-2\" align='center'></div>\n  </div>\n</div>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nrandom.seed(123)\n\ndef separate_data():\n    A = dataset[0:30]\n    tA = dataset[30:100]\n    B = dataset[100:130]\n    tB = dataset[130:200]\n    C = dataset[200:230]\n    tC = dataset[230:350]\n    train = np.concatenate((A,B,C))\n    test =  np.concatenate((tA,tB,tC))\n    return train,test\n\ntrain_porcent = 30 # Porcent Training \ntest_porcent = 70 # Porcent Test\n\ndataset = list(dataset)\n\nrandom.shuffle(dataset)\n\nFiletrain, Filetest = separate_data()\ntrain_X = np.array([i[:4] for i in Filetrain])\ntrain_y = np.array([i[4] for i in Filetrain])\ntest_X = np.array([i[:4] for i in Filetest])\ntest_y = np.array([i[4] for i in Filetest])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = 0 #example value\nativation = {(lambda x: 1/(1 + np.exp(-x)))}\nderiv = {(lambda x: x*(1-x))}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Implementation the Multilayer Perceptron in Python\n","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nimport random\n\nclass MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n    def __init__(self, params=None):     \n        if (params == None):\n            self.inputLayer = 4                        # Input Layer\n            self.hiddenLayer = 5                       # Hidden Layer\n            self.outputLayer = 3                       # Outpuy Layer\n            self.learningRate = 0.005                  # Learning rate\n            self.max_epochs = 20                      # Epochs\n            self.iasHiddenValue = -1                   # Bias HiddenLayer\n            self.BiasOutputValue = -1                  # Bias OutputLayer\n            self.activation = self.ativacao['sigmoid'] # Activation function\n            self.deriv = self.derivada['sigmoid']\n        else:\n            self.inputLayer = params['InputLayer']\n            self.hiddenLayer = params['HiddenLayer']\n            self.OutputLayer = params['OutputLayer']\n            self.learningRate = params['LearningRate']\n            self.max_epochs = params['Epocas']\n            self.BiasHiddenValue = params['BiasHiddenValue']\n            self.BiasOutputValue = params['BiasOutputValue']\n            self.activation = self.ativacao[params['ActivationFunction']]\n            self.deriv = self.derivada[params['ActivationFunction']]\n        \n        'Starting Bias and Weights'\n        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n        self.classes_number = 3 \n        \n    pass\n    \n    def starting_weights(self, x, y):\n        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n\n    ativacao = {\n         'sigmoid': (lambda x: 1/(1 + np.exp(-x))),\n               }\n    derivada = {\n         'sigmoid': (lambda x: x*(1-x)),\n               }\n \n    def Backpropagation_Algorithm(self, x):\n        DELTA_output = []\n        'Stage 1 - Error: OutputLayer'\n        ERROR_output = self.output - self.OUTPUT_L2\n        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n        \n        arrayStore = []\n        'Stage 2 - Update weights OutputLayer and HiddenLayer'\n        for i in range(self.hiddenLayer):\n            for j in range(self.OutputLayer):\n                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n      \n        'Stage 3 - Error: HiddenLayer'\n        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n \n        'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\n        for i in range(self.OutputLayer):\n            for j in range(self.hiddenLayer):\n                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))\n                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n                \n    def show_err_graphic(self,v_erro,v_epoca):\n        plt.figure(figsize=(9,4))\n        plt.plot(v_epoca, v_erro, \"m-\",color=\"b\", marker=11)\n        plt.xlabel(\"Number of Epochs\")\n        plt.ylabel(\"Squared error (MSE) \");\n        plt.title(\"Error Minimization\")\n        plt.show()\n\n    def predict(self, X, y):\n        'Returns the predictions for every element of X'\n        my_predictions = []\n        'Forward Propagation'\n        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden\n        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output\n                                 \n        for i in forward:\n            my_predictions.append(max(enumerate(i), key=lambda x:x[1])[0])\n            \n                \n        return my_predictions\n        pass\n\n    def fit(self, X, y):  \n        count_epoch = 1\n        total_error = 0\n        n = len(X); \n        epoch_array = []\n        error_array = []\n        W0 = []\n        W1 = []\n        while(count_epoch <= 20):\n            for idx,inputs in enumerate(X): \n                self.output = np.zeros(self.classes_number)\n                'Stage 1 - (Forward Propagation)'\n                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n                'Stage 2 - One-Hot-Encoding'\n                if(y[idx] == 0): \n                    self.output = np.array([1,0,0]) #Class1 {1,0,0}\n                elif(y[idx] == 1):\n                    self.output = np.array([0,1,0]) #Class2 {0,1,0}\n                elif(y[idx] == 2):\n                    self.output = np.array([0,0,1]) #Class3 {0,0,1}\n                \n                square_error = 0\n                for i in range(self.OutputLayer):\n                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n                    square_error = (square_error + (0.05 * erro))\n                    total_error = total_error + square_error\n         \n                'Backpropagation : Update Weights'\n                self.Backpropagation_Algorithm(inputs)\n                \n            total_error = (total_error / n)\n            print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n            error_array.append(total_error)\n            epoch_array.append(count_epoch)\n                \n            W0.append(self.WEIGHT_hidden)\n            W1.append(self.WEIGHT_output)\n             \n                \n            count_epoch += 1\n        self.show_err_graphic(error_array,epoch_array)\n        \n        plt.plot(W0[0])\n        plt.title('Weight Hidden update during training')\n        plt.legend(['neuron1', 'neuron2', 'neuron3', 'neuron4', 'neuron5'])\n        plt.ylabel('Value Weight')\n        plt.show()\n        \n        plt.plot(W1[0])\n        plt.title('Weight Output update during training')\n        plt.legend(['neuron1', 'neuron2', 'neuron3'])\n        plt.ylabel('Value Weight')\n        plt.show()\n\n        return self","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_test():\n    ep1 = [0,1,2,3,4,5,6,7,8,9,10,15,20]\n    h_5 = [0,60,70,70,83.3,93.3,96.7,86.7,86.7,76.7,73.3,66.7,66.7]\n    h_4 = [0,40,70,63.3,66.7,70,70,70,70,66.7,66.7,43.3,33.3]\n    h_3 = [0,46.7,76.7,80,76.7,76.7,76.6,73.3,73.3,73.3,73.3,76.7,76.7]\n    plt.figure(figsize=(10,4))\n    l1, = plt.plot(ep1, h_3, \"m-\",color='b',label=\"node-3\", marker=11)\n    l2, = plt.plot(ep1, h_4, \"m-\",color='g',label=\"node-4\", marker=8)\n    l3, = plt.plot(ep1, h_5, \"m-\",color='r',label=\"node-5\", marker=5)\n    plt.legend(handles=[l1,l2,l3], loc=1)\n    plt.xlabel(\"number of Epochs\");plt.ylabel(\"% Hits\");\n    plt.title(\"Number of Hidden Layers - Performance\")\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_test()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Artificial Neural Network(MLP)\n\n## Step 1: training our MultiLayer Perceptron","metadata":{}},{"cell_type":"code","source":"dictionary = {'InputLayer':4, 'HiddenLayer':5, 'OutputLayer':3,\n              'Epocas':20, 'LearningRate':0.005,'BiasHiddenValue':-1, \n              'BiasOutputValue':-1, 'ActivationFunction':'sigmoid'}\n\nPerceptron = MultiLayerPerceptron(dictionary)\nPerceptron.fit(train_X,train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: testing our results ","metadata":{}},{"cell_type":"code","source":"prev = Perceptron.predict(test_X,test_y)\nhits = n_set = n_vers = n_virg = 0\nscore_set = score_vers = score_virg = 0\nfor j in range(len(test_y)):\n    if(test_y[j] == 0): n_set += 1\n    elif(test_y[j] == 1): n_vers += 1\n    elif(test_y[j] == 2): n_virg += 1\n        \nfor i in range(len(test_y)):\n    if test_y[i] == prev[i]: \n        hits += 1\n    if test_y[i] == prev[i] and test_y[i] == 0:\n        score_set += 1\n    elif test_y[i] == prev[i] and test_y[i] == 1:\n        score_vers += 1\n    elif test_y[i] == prev[i] and test_y[i] == 2:\n        score_virg += 1    \n         \nhits = (hits / len(test_y))*100\nfaults = 100 - hits\nprint(\"faults\",faults)\nprint(\"hits\",hits)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3. Accuracy and precision the Multilayer Perceptron","metadata":{}},{"cell_type":"code","source":"graph_hits = []\nprint(\"Porcents :\",\"%.2f\"%(hits),\"% hits\",\"and\",\"%.2f\"%(faults),\"% faults\")\nprint(\"Total samples of test\",n_samples)\nprint(\"*Iris-Setosa:\",n_set,\"samples\")\nprint(\"*Iris-Versicolour:\",n_vers,\"samples\")\n\nprint(\"*Iris-Virginica:\",n_virg,\"samples\")\n\ngraph_hits.append(hits)\ngraph_hits.append(faults)\nlabels = 'Hits', 'Faults';\nsizes = [96.5, 3.3]\nexplode = (0, 0.14)\n\nfig1, ax1 = plt.subplots();\nax1.pie(graph_hits, explode=explode,colors=['blue','red'],labels=labels, autopct='%1.1f%%',\nshadow=True, startangle=90)\nax1.axis('equal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4. Score for each one of the samples","metadata":{}},{"cell_type":"code","source":"acc_set = (score_set/n_set)*100\nacc_vers = (score_vers/n_vers)*100\nacc_virg = (score_virg/n_virg)*100\nprint(\"- Acurracy Iris-Setosa:\",\"%.2f\"%acc_set, \"%\")\nprint(\"- Acurracy Iris-Versicolour:\",\"%.2f\"%acc_vers, \"%\")\nprint(\"- Acurracy Iris-Virginica:\",\"%.2f\"%acc_virg, \"%\")\nnames = [\"Setosa\",\"Versicolour\",\"Virginica\"]\nx1 = [2.0,4.0,6.0]\nfig, ax = plt.subplots()\nr1 = plt.bar(x1[0], acc_set,color='orange',label='Iris-Setosa')\nr2 = plt.bar(x1[1], acc_vers,color='green',label='Iris-Versicolour')\nr3 = plt.bar(x1[2], acc_virg,color='purple',label='Iris-Virginica')\nplt.ylabel('Scores %')\nplt.xticks(x1, names);plt.title('Scores by iris flowers - Multilayer Perceptron')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}