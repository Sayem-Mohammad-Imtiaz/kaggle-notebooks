{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7b199cd1-1621-051e-194a-41491ad58e6b"},"source":"# Using Gaussian Mixture Models to Explore Differences in Clustering"},{"cell_type":"markdown","metadata":{"_cell_guid":"764fb3c1-dd1d-6238-52db-3d3f9cbc9020"},"source":"### I decided to approach this problem from a more unsupervised learning method.\nWhen considering K-means clustering often one of the pitfalls can be the shape of the clusters. When considering the number of dimensions that the data has it seemed intuitive that spherical clusters would be the least likely.\nThese are the following steps in my approach\n* Researched Guassian Mixture models\n* Researched comparable metrics\n* Combined several approaches to the data cleaning, modeling and metric scoring\nIt must be noted that only the combination of these analytical methods are my own.\nI must give credit to:\n   * Kajot for the data and first part of the script for the data processing and cleaning\n   * Kam Sen and Prabhath Nanisetty from their Q&A on stats.stackexchange.com http://stats.stackexchange.com/questions/90769/using-bic-to-estimate-the-number-of-k-in-kmeans\n   * Sklearns gaussian mixture model example\n  "},{"cell_type":"markdown","metadata":{"_cell_guid":"00a7c168-e6bf-03ab-a5ee-f0acb6cc8d7d"},"source":"## Use Bayesian Information Criterion to Compare K-means and Gaussian Mixture Model\n\nGuassian Mixture Models and K-means use different metrics for comparing the best clusters. I used BIC for both forms of clustering so that I could compare the approaches\n\n## Results\n\n* The optimal number of clusters using BIC score and GMM is 5 and this has a \"full\" geometry parameter\n\n* The optimal number of clusters using BIC score and K-means was 4.\n\n* The GMM provided a lower BIC score than K-mean.\n\n## Next Steps\n\nCalculate silouhette score and inertia for GMM and compare to K-means\nPlay around with different imputation methods to see if that makes a difference\n\n## Thoughts\n\nI can't access the paper behind the paywall so I cannot use their methods or compare their methods to mine on how to find the optimal number of clusters. My results seem to point to the possibility of a larger number of clusters."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77ac874e-66e5-2083-9860-7e65a7bb5421"},"outputs":[],"source":"import pandas as pd\nfrom sklearn import metrics\nfrom sklearn import mixture\nimport re\nfrom sklearn.preprocessing import Imputer\nfrom numpy import random\nimport seaborn as sb\nimport numpy as np\nimport itertools\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn import cluster\nfrom scipy.spatial import distance\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline\n \n### Set path to the data set\ndataset_path = \"77_cancer_proteomes_CPTAC_itraq.csv\"\nclinical_info = \"clinical_data_breast_cancer.csv\"\npam50_proteins = \"PAM50_proteins.csv\"\n \n## Load data\ndata = pd.read_csv(dataset_path,header=0,index_col=0)\nclinical = pd.read_csv(clinical_info,header=0,index_col=0)## holds clinical information about each patient/sample\npam50 = pd.read_csv(pam50_proteins,header=0)\n \n## Drop unused information columns\ndata.drop(['gene_symbol','gene_name'],axis=1,inplace=True)\n \n \n## Change the protein data sample names to a format matching the clinical data set\ndata.rename(columns=lambda x: \"TCGA-%s\" % (re.split('[_|-|.]',x)[0]) if bool(re.search(\"TCGA\",x)) is True else x,inplace=True)\n \n## Transpose data for the clustering algorithm since we want to divide patient samples, not proteins\ndata = data.transpose()\ndata3 = data.copy()\n \n## Drop clinical entries for samples not in our protein data set\nclinical = clinical.loc[[x for x in clinical.index.tolist() if x in data.index],:]\n \n## Add clinical meta data to our protein data set, note: all numerical features for analysis start with NP_ or XP_\nmerged = data.merge(clinical,left_index=True,right_index=True)\n \n## Change name to make it look nicer in the code!\nprocessed = merged\n \n## Numerical data for the algorithm, NP_xx/XP_xx are protein identifiers from RefSeq database\nprocessed_numerical = processed.loc[:,[x for x in processed.columns if bool(re.search(\"NP_|XP_\",x)) == True]]\n \n## Select only the PAM50 proteins - known panel of genes used for breast cancer subtype prediction\nprocessed_numerical_p50 = processed_numerical.ix[:,processed_numerical.columns.isin(pam50['RefSeqProteinID'])]\n \n## Impute missing values (maybe another method would work better?)\n## Impute missing values (maybe another method would work better?)\nimputer = Imputer(missing_values='NaN', strategy='median', axis=1)\nimputer = imputer.fit(processed_numerical_p50)\nprocessed_numerical_p50 = imputer.transform(processed_numerical_p50)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5341c07-4253-0711-158b-9bce2ae8c8fb","collapsed":true},"outputs":[],"source":"X = processed_numerical_p50"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd33aed5-716e-e974-7310-adac7e8b1522"},"outputs":[],"source":"lowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a Gaussian mixture with EM\n        gmm = mixture.GaussianMixture(n_components=n_components,\n                                      covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.bic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n\nbic = np.array(bic)\ncolor_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n                              'darkorange'])\nclf = best_gmm\nbars = []\n\n# Plot the BIC scores\nspl = plt.subplot(2, 1, 1)\nfor i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n    xpos = np.array(n_components_range) + .2 * (i - 2)\n    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n                                  (i + 1) * len(n_components_range)],\n                        width=.2, color=color))\nplt.xticks(n_components_range)\nplt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\nplt.title('BIC score per model')\nxpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n    .2 * np.floor(bic.argmin() / len(n_components_range))\nplt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\nspl.set_xlabel('Number of components')\nspl.legend([b[0] for b in bars], cv_types)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4f7d4ba-f41f-101d-8426-49cc73fe87b2"},"outputs":[],"source":"\n\ndef compute_bic(kmeans,X):\n    \"\"\"\n    Computes the BIC metric for a given clusters\n\n    Parameters:\n    -----------------------------------------\n    kmeans:  List of clustering object from scikit learn\n\n    X     :  multidimension np array of data points\n\n    Returns:\n    -----------------------------------------\n    BIC value\n    \"\"\"\n    # assign centers and labels\n    centers = [kmeans.cluster_centers_]\n    labels  = kmeans.labels_\n    #number of clusters\n    m = kmeans.n_clusters\n    # size of the clusters\n    n = np.bincount(labels)\n    #size of data set\n    N, d = X.shape\n\n    i = 0\n    X[np.where(labels == i)]\n    \n    #compute variance for all clusters beforehand\n    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], 'euclidean')**2) for i in range(m)])\n    \n    const_term = 0.5 * m * np.log(N) * (d+1)\n\n    BIC = np.sum([n[i] * np.log(n[i]) -\n               n[i] * np.log(N) -\n             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n\n    return(BIC)\n\n\nks = range(1,10)\n\n# run 9 times kmeans and save each result in the KMeans object\nKMeans = [cluster.KMeans(n_clusters = i, init=\"k-means++\").fit(X) for i in ks]\n# now run for each cluster the BIC computation\nBIC = [compute_bic(kmeansi,X) for kmeansi in KMeans]\n# additional list to match score with number of clusters\nBIC2 = [(compute_bic(kmeansi,X), kmeansi.n_clusters) for kmeansi in KMeans]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4ae13a1-3a8e-c63e-29f4-e82c1c7b6088"},"outputs":[],"source":"bic2 = [i * -1 for i in BIC]\nn_components_range = range(1,10)\nBIC \nspl = plt.subplot(2, 1, 1)\nplt.xticks(n_components_range)\nplt.ylim([min(bic2) * 1.01 - .01 * max(bic2), max(bic2)])\nplt.title('BIC score per model K-Means')\nplt.bar(n_components_range,bic2, width=.4, color = 'darkorange')\nxpos = bic2.index(min(bic2)) + 1\nplt.text(xpos, min(bic2) * 0.97 + .03 * max(bic2), '*', fontsize=14)\nplt.xlabel('Number of components')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}