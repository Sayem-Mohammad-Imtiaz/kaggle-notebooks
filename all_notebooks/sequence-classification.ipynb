{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install lime\n!pip -q install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom imblearn.over_sampling import RandomOverSampler\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport time\nfrom collections import Counter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import DistilBertTokenizerFast, DistilBertModel\n# from transformers import DistilBertForSequenceClassification, AdamW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel, AdamW ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_lower(text):\n    # return the reviews after convering then to lowercase\n    return text.lower()\n\ndef remove_punctuation(text):\n    # return the reviews after removing punctuations\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r'[^\\w\\s]',' ',text)\n    text = re.sub(' +', ' ',text)\n    return text\n\ndef remove_stopwords(text):\n    # return the reviews after removing the stopwords\n    exclude = set(['$','&','+',':',';','=','@','|','<','>','^','*','%','-','#','\\'','ред'])\n    remove_digits = str.maketrans('', '', digits)\n    text = text.translate(remove_digits)\n    return ''.join(ch for ch in text if ch not in exclude)\n    # stop_words = stopwords.words('english')\n    # stop_words.remove('not')\n    # return ' '.join([w for w in text if not w in stop_words])  \n\ndef perform_tokenization(text):\n    # return the reviews after performing tokenization\n    return word_tokenize(text)\n\ndef perform_padding(data, TEXT):\n    # return the reviews after padding the reviews to maximum length\n    # return pad_sequences(data, maxlen=max_len)\n    return TEXT.pad(data)\n\ndef preprocess_review_and_tokenize(text):\n#     out = convert_to_lower(text)\n#     out = remove_punctuation(out)\n    # out = remove_stopwords(out)\n    # out = perform_tokenization(out)\n    out = tokenizer(text, padding='max_length', truncation=True)\n    return out\n\ndef preprocess(text):\n    out = convert_to_lower(text)\n    out = remove_punctuation(out)\n    return out\n\ndef process(data):\n    review = data.apply(lambda row: preprocess(row))\n    return review\n\ndef preprocess_data(data):\n    # make all the following function calls on your data\n    # EXAMPLE:->\n\n    review = data.apply(lambda row: preprocess_review_and_tokenize(row))\n    # review, text_field = encode_data(review, L)\n    return review\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews_train = pd.read_csv('../input/nlpdatacs772/train.csv')\nreviews_val = pd.read_csv('../input/nlpdatacs772/gold_test.csv')\nreviews_test = pd.read_csv('../input/nlpdatacs772/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_texts = reviews_train.reviews\ntrain_labels = reviews_train.ratings\nval_texts = reviews_val.reviews\nval_labels = reviews_val.ratings\n# test_texts = reviews_test.reviews\n# test_labels = reviews_test.ratings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_sample_count = np.array([len(np.where(reviews_train['ratings'].values==t)[0]) for t in np.unique(reviews_train['ratings'].values)])\n# print(class_sample_count)\n# weight = 1 / class_sample_count\n# weight = torch.tensor(weight).float().to(device)\n# print(weight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_sample_count = np.array([len(np.where(reviews_train['ratings'].values==t)[0]) for t in np.unique(reviews_train['ratings'].values)])\nclass_counts = class_sample_count\nprint(class_counts)\nnum_samples = sum(class_counts)\nprint(num_samples)\n# labels = [0, 0,..., 0, 1] #corresponding labels of samples\n\nclass_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\nprint(class_weights)\nweights = [class_weights[train_labels[i]-1] for i in range(int(num_samples))]\n# print(weights)\nweight_sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# weight_sampler = WeightedRandomSampler(weight, num_samples=len(weight), replacement=True)\n# train_texts, train_labels = sampler.fit_sample(train_texts, train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NLPDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = self.encodings[idx]\n        item['label'] = torch.tensor(self.labels[idx]-1)\n        item['input_ids'] = torch.tensor(item['input_ids'])\n        item['attention_mask'] = torch.tensor(item['attention_mask'])\n        return item","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sampler = RandomOverSampler({5: 33193, 4: 33193, 3: 33193, 2: 33193, 1: 33193},random_state=0)\n\n# train_X_rs, train_Y_rs = torch.tensor(X_rs), torch.tensor(y_rs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings = preprocess_data(train_texts)\nval_encodings = preprocess_data(val_texts)\n# test_encodings = preprocess_data(test_texts)\n# train_encodings, train_labels = sampler.fit_resample(train_encodings, train_labels)\n\ntrain_dataset = NLPDataset(train_encodings, train_labels)\nval_dataset = NLPDataset(val_encodings, val_labels)\n\ntrain_length = len(train_dataset)\nval_length = len(val_dataset)\n# test_dataset = NLPDataset(test_encodings, test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomBERTModel(torch.nn.Module):\n    def __init__(self, num_classes):\n          super(CustomBERTModel, self).__init__()\n          self.bert = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n\n          ### New layers:\n          self.linear1 = torch.nn.Linear(768, 256)\n          self.linear2 = torch.nn.Linear(256, num_classes) ## as you have 4 classes in the output\n          # self.sig = torch.nn.functional.sigmoid()\n\n    def forward(self, ids, mask):\n          sequence_output = self.bert(ids,attention_mask=mask)\n#           print(sequence_output[\"last_hidden_state\"].shape)\n          sequence_output = sequence_output[\"last_hidden_state\"][:,0,:]\n\n          # sequence_output has the following shape: (batch_size, sequence_length, 768)\n          linear1_output = self.linear1(sequence_output.view(-1,768))\n          linear2_output = self.linear2(linear1_output)\n          # linear2_output = self.sig(linear2_output)\n\n          return linear2_output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CustomBERTModel(num_classes=5)\nfor param in model.bert.parameters():\n    param.requires_grad = False\nmodel.to(device)\nmodel.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n# criterion = torch.nn.CrossEntropyLoss()\n# optim = AdamW(model.parameters(), lr=5e-5)\n\n# batch = next(iter(train_loader))\n# input_ids = batch['input_ids'].to(device)\n# attention_mask = batch['attention_mask'].to(device)\n# labels = batch['label'].to(device)\n# print(input_ids.shape)\n# print(labels.shape)\n# outputs = model(input_ids, mask = attention_mask)\n# print(outputs)\n# print(outputs.shape)\n# # loss = outputs[0]\n# loss = criterion(outputs, labels)\n# print(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation_split = .2\n# shuffle_dataset = True\n# random_seed= 42\n\n# batch_size = 200\n# # train_set = torch.utils.data.TensorDataset(self.reviews, self.ratings)\n# dataset_size = len(train_dataset)\n# indices = list(range(dataset_size))\n# split = int(np.floor(validation_split * dataset_size))\n# if shuffle_dataset :\n#     np.random.seed(random_seed)\n#     np.random.shuffle(indices)\n# train_indices, val_indices = indices[split:], indices[:split]\n\n# # Creating PT data samplers and loaders:\n# train_sampler = SubsetRandomSampler(train_indices)\n# valid_sampler = SubsetRandomSampler(val_indices)\n\n# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n#                                         sampler=train_sampler, drop_last=True)\n# val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n#                                         sampler=valid_sampler, drop_last=False)\n# train_length = dataset_size - split\n# val_length = split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=250, sampler=weight_sampler)\nval_loader = DataLoader(val_dataset, batch_size=250, shuffle=True, drop_last=True)\ncriterion = torch.nn.CrossEntropyLoss()\noptim = AdamW(model.parameters(), lr=5e-5)\ntotal_epochs = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in train_loader:\n#     print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_loader))\nprint(val_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = []\n\nfor epoch in range(total_epochs):\n    trainLoss = 0.0\n    trainAcc = 0.0\n\n    validLoss = 0.0\n    validAcc = 0.0\n    \n    epochStart = time.time()\n    print(\"Epoch: {}/{}\".format(epoch+1, total_epochs))\n    \n    for i, batch in enumerate(train_loader):\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)        \n        outputs = model(input_ids, mask = attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optim.step()\n        trainLoss += loss.item() * input_ids.size(0)\n        _, predictions = torch.max(outputs.data, 1)\n        corrCounts = predictions.eq(labels.data.view_as(predictions))\n        acc = torch.mean(corrCounts.type(torch.FloatTensor))\n        trainAcc += acc.item() * input_ids.size(0)\n        if i%20==0:\n            print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n\n    with torch.no_grad():\n\n        # Set to evaluation mode\n        model.eval()\n\n        # Validation loop\n        for j, batch in enumerate(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            # Forward pass - compute outputs on input data using the model\n            outputs = model(input_ids, mask = attention_mask)\n\n            # Compute loss\n            loss = criterion(outputs, labels)\n\n            # Compute the total loss for the batch and add it to validLoss\n            validLoss += loss.item() * input_ids.size(0)\n\n            # Calculate validation accuracy\n            _, predictions = torch.max(outputs.data, 1)\n            corrCounts = predictions.eq(labels.data.view_as(predictions))\n\n            # Convert corrCounts to float and then compute the mean\n            acc = torch.mean(corrCounts.type(torch.FloatTensor))\n\n            # Compute total accuracy in the whole batch and add to validAcc\n            validAcc += acc.item() * input_ids.size(0)\n            if j%10==0:\n                print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n            \n    model_path = os.path.join('./', str(epoch)+'_model.pt')\n    torch.save(model, model_path)\n    print('Saved Model checkpoints')\n\n    # Find average training loss and training accuracy\n    trainLossAvg = trainLoss/train_length\n    trainAccAvg = trainAcc/train_length\n\n    # Find average training loss and training accuracy\n    validLossAvg = validLoss/val_length\n    validAccAvg = validAcc/val_length\n\n    history.append([trainLossAvg, validLossAvg, trainAccAvg, validAccAvg])\n\n    epochEnd = time.time()\n\n    print(\"Epoch : {:03d}, Training: Loss : {:.4f}, Accuracy: {:.4f}%\".format(epoch, trainLossAvg, trainAccAvg*100))\n    print(\"Validation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(validLossAvg, validAccAvg*100, epochEnd-epochStart))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"saved_model = torch.load('./19_model.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_acc = 0.0\ntest_loss = 0.0\ntest_preds = []\n\n# Validation - No gradient tracking needed\nwith torch.no_grad():\n\n    # Set to evaluation mode\n    saved_model.eval()\n\n    # Validation loop\n    for j, batch in enumerate(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = saved_model(input_ids, mask = attention_mask)\n\n        _, predictions = torch.max(outputs.data, 1)\n        test_preds.append(predictions[0].item()+1)\n#         print(predictions[0].item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ntest_preds = np.array(test_preds)\nprint(classification_report(val_labels, test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(val_labels, test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lime\nimport torch\nimport torch.nn.functional as F\nfrom lime.lime_text import LimeTextExplainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = []\nfor i in process(val_texts):\n    texts.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictor(texts):\n    tokens = tokenizer(texts, return_tensors='pt', padding=True)\n    output = saved_model(tokens['input_ids'].to(device), tokens['attention_mask'].to(device))\n    tensor_logits = output\n    probas = F.softmax(tensor_logits).cpu().detach().numpy()\n    return probas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predict_label(texts):\n#     tokens = tokenizer(texts, return_tensors='pt', padding=True)\n#     model.eval()\n#     output = saved_model(tokens['input_ids'].to(device), tokens['attention_mask'].to(device))\n#     tensor_logits = output\n#     probas = F.softmax(tensor_logits).cpu().detach().numpy()\n#     val_pred = np.argmax(probas, axis=1)\n#     return val_pred+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# texts = texts[:100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels = predict_label(texts)\n# labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text = 'Building more bypasses will help the environment by reducing pollution and traffic jams in towns and cities.'\n# print(tokenizer(text, return_tensors='pt', padding=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data=[\"This product is okay.\",\"The product is not fine\",\"The product is not good, just okay!.\"]\n# print(tokenizer(data, return_tensors='pt', padding=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens = tokenizer(data, return_tensors='pt', padding=True)\n# output = model(tokens['input_ids'].to(device), tokens['attention_mask'].to(device))\n# tensor_logits = output\n# probas = F.softmax(tensor_logits).cpu().detach().numpy()\n# probas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"misclassified = []\nfor i in range(len(val_labels)):\n    if val_labels[i]!=test_preds[i]:\n        misclassified.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(misclassified)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = misclassified[5]\ntext = val_texts[idx]\nlabel = val_labels[idx]\nlabel_pred = test_preds[idx]\nexplainer = LimeTextExplainer(class_names=['1','2','3','4','5'])\nexp = explainer.explain_instance(text, predictor, (label-1,label_pred-1,), num_features=10, num_samples=1000)\nprint(text)\nprint(\"True Label : \", label)\nprint(\"Predicted Label : \", label_pred)\nexp.show_in_notebook(text=text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"This camera is awesome , I will not recommend this to anybody\"\nexplainer = LimeTextExplainer(class_names=['1','2','3','4','5'])\nexp = explainer.explain_instance(text, predictor, (0,1,2,3,4,), num_features=10, num_samples=1000)\n# print(\"True Label : \", label)\nexp.show_in_notebook(text=text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}