{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PyCaret — the library for low-code ML","metadata":{}},{"cell_type":"markdown","source":"Train, visualize, evaluate, interpret, and deploy models with minimal code","metadata":{}},{"cell_type":"markdown","source":"When we approach supervised machine learning problems, it can be tempting to just see how a random forest or gradient boosting model performs and stop experimenting if we are satisfied with the results. What if you could compare many different models with just one line of code? What if you could reduce each step of the data science process from feature engineering to model deployment to just a few lines of code?\n\nThis is exactly where PyCaret comes into play. PyCaret is a high-level, low-code Python library that makes it easy to compare, train, evaluate, tune, and deploy machine learning models with only a few lines of code. At its core, PyCaret is basically just a large wrapper over many data science libraries such as Scikit-learn, Yellowbrick, SHAP, Optuna, and Spacy. Yes, you could use these libraries for the same tasks, but if you don’t want to write a lot of code, PyCaret could save you a lot of time.","metadata":{}},{"cell_type":"markdown","source":"# Installing PyCaret","metadata":{}},{"cell_type":"markdown","source":"To install the default, smaller version of PyCaret with only the required dependencies, you can run the following command.","metadata":{}},{"cell_type":"code","source":"!pip install pycaret","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-10T06:17:01.608829Z","iopub.execute_input":"2021-07-10T06:17:01.609194Z","iopub.status.idle":"2021-07-10T06:17:09.814778Z","shell.execute_reply.started":"2021-07-10T06:17:01.60916Z","shell.execute_reply":"2021-07-10T06:17:09.8131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"markdown","source":"In the code below, We simply imported Numpy and Pandas for handling the data for this demonstration.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:17:09.817784Z","iopub.execute_input":"2021-07-10T06:17:09.818315Z","iopub.status.idle":"2021-07-10T06:17:09.823812Z","shell.execute_reply.started":"2021-07-10T06:17:09.818261Z","shell.execute_reply":"2021-07-10T06:17:09.822478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read the Data","metadata":{}},{"cell_type":"markdown","source":"For this example, We used the California Housing Prices Dataset available on Kaggle. In the code below, I read this dataset into a dataframe and displayed the first five rows of the dataframe.","metadata":{}},{"cell_type":"code","source":"housing_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nhousing_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:17:09.826067Z","iopub.execute_input":"2021-07-10T06:17:09.826579Z","iopub.status.idle":"2021-07-10T06:17:09.899747Z","shell.execute_reply.started":"2021-07-10T06:17:09.826527Z","shell.execute_reply":"2021-07-10T06:17:09.898649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical = []\nfor i in housing_data.columns:\n    if (housing_data[i].dtype=='object'):\n        categorical.append(i)\nprint(\"Categorical Attribute : {}\\n \".format(len(categorical)))\ncategorical.append('MSSubClass')\nfor x in range(len(categorical)): \n    print(categorical[x])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:17:09.901242Z","iopub.execute_input":"2021-07-10T06:17:09.901581Z","iopub.status.idle":"2021-07-10T06:17:09.916593Z","shell.execute_reply.started":"2021-07-10T06:17:09.901542Z","shell.execute_reply":"2021-07-10T06:17:09.914991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(housing_data[categorical].nunique()).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:17:09.921539Z","iopub.execute_input":"2021-07-10T06:17:09.921867Z","iopub.status.idle":"2021-07-10T06:17:09.970207Z","shell.execute_reply.started":"2021-07-10T06:17:09.921838Z","shell.execute_reply":"2021-07-10T06:17:09.968763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in categorical:\n    print(i)\n    print(housing_data[i].value_counts())\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:17:09.973663Z","iopub.execute_input":"2021-07-10T06:17:09.974139Z","iopub.status.idle":"2021-07-10T06:17:10.048626Z","shell.execute_reply.started":"2021-07-10T06:17:09.97408Z","shell.execute_reply":"2021-07-10T06:17:10.046819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:17:10.049973Z","iopub.execute_input":"2021-07-10T06:17:10.050281Z","iopub.status.idle":"2021-07-10T06:17:10.061806Z","shell.execute_reply.started":"2021-07-10T06:17:10.050252Z","shell.execute_reply":"2021-07-10T06:17:10.060058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output above gives us an idea of what the data looks like. The data contains mostly numerical features with multiple categorical features. The target column that we are trying to predict is the SalePrice column. The entire dataset contains a total of 1460 observations.","metadata":{}},{"cell_type":"markdown","source":"# Initialize Experiment","metadata":{}},{"cell_type":"markdown","source":"Now that we have the data, we can initialize a PyCaret experiment, which will preprocess the data and enable logging for all of the models that we will train on this dataset.","metadata":{}},{"cell_type":"code","source":"from pycaret.regression import *\nreg_experiment = setup(housing_data, \n                       target = 'SalePrice', \n                       session_id=42, \n                       experiment_name='me_housing',\n                       ignore_features=['Id'],\n                       normalize = True, \n                  transformation = True, \n                  remove_multicollinearity = True, #rop one of the two features that are highly correlated with each other\n                  ignore_low_variance = True,#all categorical features with statistically insignificant variances are removed from the dataset.\n                  combine_rare_levels = True,# all levels in categorical features below the threshold defined in rare_level_threshold param are combined together as a single level\n                    transform_target = True,\n                       categorical_features=categorical,ordinal_features = {\n                         'Utilities' : ['AllPub', 'NoSeWa'],\n                           'LandSlope':['Gtl', 'Mod', 'Sev'],\n                           'OverallQual':['1','2','3','4','5','6','7','8','9','10'],\n                           'MoSold':['1','2','3','4','5','6','7','8','9','10','11','12'],\n                       },\n                      high_cardinality_features =['Neighborhood','Exterior2nd','MSSubClass','Exterior1st']\n                           )","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:17:10.063825Z","iopub.execute_input":"2021-07-10T06:17:10.064229Z","iopub.status.idle":"2021-07-10T06:19:23.73823Z","shell.execute_reply.started":"2021-07-10T06:17:10.064185Z","shell.execute_reply":"2021-07-10T06:19:23.737308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare Baseline Models","metadata":{}},{"cell_type":"markdown","source":"We can compare different baseline models at once to find the model that achieves the best K-fold cross-validation performance with the compare_models function as shown in the code below. ","metadata":{}},{"cell_type":"code","source":"best_model = compare_models()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:19:23.740192Z","iopub.execute_input":"2021-07-10T06:19:23.740804Z","iopub.status.idle":"2021-07-10T06:25:45.274127Z","shell.execute_reply.started":"2021-07-10T06:19:23.740766Z","shell.execute_reply":"2021-07-10T06:25:45.272839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function produces a data frame with the performance statistics for each model and highlights the metrics for the best performing model, which in this case was the CatBoost regressor.","metadata":{}},{"cell_type":"markdown","source":"# Creating a Model","metadata":{}},{"cell_type":"markdown","source":"We can also train a model in just a single line of code with PyCaret. The create_model function simply requires a string corresponding to the type of model that you want to train. ","metadata":{}},{"cell_type":"code","source":"catboost = create_model('catboost')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:25:45.278399Z","iopub.execute_input":"2021-07-10T06:25:45.278754Z","iopub.status.idle":"2021-07-10T06:26:30.916907Z","shell.execute_reply.started":"2021-07-10T06:25:45.278724Z","shell.execute_reply":"2021-07-10T06:26:30.915304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The create_model function produces the dataframe above with cross-validation metrics for the trained CatBoost model.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"Now that we have a trained model, we can optimize it even further with hyperparameter tuning. With just one line of code, we can tune the hyperparameters of this model.","metadata":{}},{"cell_type":"code","source":"tuned_catboost = tune_model(catboost, optimize = 'MSE')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:26:30.919202Z","iopub.execute_input":"2021-07-10T06:26:30.919694Z","iopub.status.idle":"2021-07-10T06:27:36.761189Z","shell.execute_reply.started":"2021-07-10T06:26:30.91965Z","shell.execute_reply":"2021-07-10T06:27:36.759758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most important results, in this case, the average metrics, are highlighted in yellow.","metadata":{}},{"cell_type":"markdown","source":"# Visualizing the Model’s Performance","metadata":{}},{"cell_type":"markdown","source":"There are many plots that we can create with PyCaret to visualize a model’s performance. PyCaret uses another high-level library called Yellowbrick for building these visualizations.","metadata":{}},{"cell_type":"markdown","source":"## Residual Plot","metadata":{}},{"cell_type":"markdown","source":"The plot_model function will produce a residual plot by default for a regression model as demonstrated below.","metadata":{}},{"cell_type":"code","source":"plot_model(tuned_catboost)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:36.762976Z","iopub.execute_input":"2021-07-10T06:27:36.763441Z","iopub.status.idle":"2021-07-10T06:27:38.117131Z","shell.execute_reply.started":"2021-07-10T06:27:36.763394Z","shell.execute_reply":"2021-07-10T06:27:38.115998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction Error","metadata":{}},{"cell_type":"markdown","source":"We can also visualize the predicted values against the actual target values by creating a prediction error plot.","metadata":{}},{"cell_type":"code","source":"plot_model(tuned_catboost, plot = 'error')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:38.119067Z","iopub.execute_input":"2021-07-10T06:27:38.119509Z","iopub.status.idle":"2021-07-10T06:27:38.692599Z","shell.execute_reply.started":"2021-07-10T06:27:38.119464Z","shell.execute_reply":"2021-07-10T06:27:38.691391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot above is particularly useful because it gives us a visual representation of the R² coefficient for the CatBoost model. In a perfect scenario (R² = 1), where the predicted values exactly matched the actual target values, this plot would simply contain points along the dashed identity line.","metadata":{}},{"cell_type":"markdown","source":"# Feature Importances","metadata":{}},{"cell_type":"markdown","source":"We can also visualize the feature importances for a model as shown below.","metadata":{}},{"cell_type":"code","source":"plot_model(tuned_catboost, plot = 'feature')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:38.693979Z","iopub.execute_input":"2021-07-10T06:27:38.694254Z","iopub.status.idle":"2021-07-10T06:27:39.04894Z","shell.execute_reply.started":"2021-07-10T06:27:38.694227Z","shell.execute_reply":"2021-07-10T06:27:39.047579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the plot above, we can see that the median_income feature is the most important feature when predicting the price of a house. Since this feature corresponds to the median income in the area in which a house was built, this evaluation makes perfect sense. Houses built in higher-income areas are likely more expensive than those in lower-income areas.","metadata":{}},{"cell_type":"markdown","source":"# Evaluating the Model Using All Plots","metadata":{}},{"cell_type":"markdown","source":"We can also create multiple plots for evaluating a model with the evaluate_model function.","metadata":{}},{"cell_type":"code","source":"print(evaluate_model(tuned_catboost))","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:39.050551Z","iopub.execute_input":"2021-07-10T06:27:39.050859Z","iopub.status.idle":"2021-07-10T06:27:39.314872Z","shell.execute_reply.started":"2021-07-10T06:27:39.050832Z","shell.execute_reply":"2021-07-10T06:27:39.313448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpreting the Model","metadata":{}},{"cell_type":"code","source":"interpret_model(tuned_catboost)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:39.316829Z","iopub.execute_input":"2021-07-10T06:27:39.317318Z","iopub.status.idle":"2021-07-10T06:27:48.417216Z","shell.execute_reply.started":"2021-07-10T06:27:39.31727Z","shell.execute_reply":"2021-07-10T06:27:48.416221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The interpret_model function is a useful tool for explaining the predictions of a model. This function uses a library for explainable machine learning called SHAP ","metadata":{}},{"cell_type":"markdown","source":"With just one line of code, we can create a SHAP beeswarm plot for the model.","metadata":{}},{"cell_type":"markdown","source":"Based on the plot above, we can see that the GrLivArea field has the greatest impact on the predicted house value.","metadata":{}},{"cell_type":"markdown","source":"# AutoML","metadata":{}},{"cell_type":"markdown","source":"PyCaret also has a function for running automated machine learning (AutoML). We can specify the loss function or metric that we want to optimize and then just let the library take over as demonstrated below.","metadata":{}},{"cell_type":"code","source":"automl_model = automl(optimize = 'MSE')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:48.418713Z","iopub.execute_input":"2021-07-10T06:27:48.419205Z","iopub.status.idle":"2021-07-10T06:27:53.648043Z","shell.execute_reply.started":"2021-07-10T06:27:48.419161Z","shell.execute_reply":"2021-07-10T06:27:53.646931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AutoML model also happens to be a CatBoost regressor, which we can confirm by printing out the model.","metadata":{}},{"cell_type":"code","source":"automl_model","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:53.649515Z","iopub.execute_input":"2021-07-10T06:27:53.649828Z","iopub.status.idle":"2021-07-10T06:27:53.657463Z","shell.execute_reply.started":"2021-07-10T06:27:53.649798Z","shell.execute_reply":"2021-07-10T06:27:53.656478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Predictions","metadata":{}},{"cell_type":"markdown","source":"The predict_model function allows us to generate predictions by either using data from the experiment or new unseen data.","metadata":{}},{"cell_type":"code","source":"pred_holdouts = predict_model(automl_model)\npred_holdouts.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:53.658677Z","iopub.execute_input":"2021-07-10T06:27:53.658959Z","iopub.status.idle":"2021-07-10T06:27:53.92173Z","shell.execute_reply.started":"2021-07-10T06:27:53.658926Z","shell.execute_reply":"2021-07-10T06:27:53.920803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predict_model function above produces predictions for the holdout datasets used for validating the model during cross-validation. The code also gives us a dataframe with performance statistics for the predictions generated by the AutoML model.","metadata":{}},{"cell_type":"markdown","source":"# Saving the Model","metadata":{}},{"cell_type":"markdown","source":"PyCaret also allows us to save trained models with the save_model function. This function saves the transformation pipeline for the model to a pickle file","metadata":{}},{"cell_type":"code","source":"save_model(automl_model, model_name='./automl-model')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:53.922819Z","iopub.execute_input":"2021-07-10T06:27:53.923224Z","iopub.status.idle":"2021-07-10T06:27:54.206061Z","shell.execute_reply.started":"2021-07-10T06:27:53.923195Z","shell.execute_reply":"2021-07-10T06:27:54.205287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also load the saved AutoML model with the load_model function.","metadata":{}},{"cell_type":"code","source":"loaded_model = load_model('./automl-model')\nprint(loaded_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T06:27:54.207101Z","iopub.execute_input":"2021-07-10T06:27:54.207563Z","iopub.status.idle":"2021-07-10T06:27:54.261002Z","shell.execute_reply.started":"2021-07-10T06:27:54.207533Z","shell.execute_reply":"2021-07-10T06:27:54.259678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing out the loaded model produces the output","metadata":{}},{"cell_type":"markdown","source":"# Pros and Cons of Using PyCaret","metadata":{}},{"cell_type":"markdown","source":"While PyCaret is a great tool, it comes with its own pros and cons that you should be aware of if you plan to use it for your data science projects.\n\n**Pros**\n- Low-code library.\n- Great for simple, standard tasks and general-purpose machine learning.\n- Provides support for regression, classification, natural language processing, clustering, anomaly detection, and association rule mining.\n- Makes it easy to create and save complex transformation pipelines for models.\n- Makes it easy to visualize the performance of your model.\n\n**Cons**\n- As of now, PyCaret is not ideal for text classification because the NLP utilities are limited to topic modeling algorithms.\n- PyCaret is not ideal for deep learning and doesn’t use Keras or PyTorch models.\n- We can’t perform more complex machine learning tasks such as image classification and text generation with PyCaret.\n- By using PyCaret, we are sacrificing a certain degree of control for simple and high-level code.","metadata":{}}]}