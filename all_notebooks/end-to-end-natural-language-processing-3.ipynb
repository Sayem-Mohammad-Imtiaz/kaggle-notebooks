{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 🚀 END-to-END-Natural Language Processing-3\n\n\n#### In this notebook, I am demonstrating how to build different 'ANN Language Models' with Semantic Embeddings on Keras-TensorFlow 2.0 framework.\n\nFirst, I am building the ConvNet Language Model without pre-trained embedding and using it as benchmark.\n\nSecondly, I'd design the embedding matrix with pre-trained word embeddings (**GloVe,GoogleNews,Fasttext**) to feed to the embedding layer of the neural networks.\n\nThe following Neural Networks are build with the appropriate word embeddings:\n\n* ConvNets\n* Recurrent Neural Networks/ Long Short Term Memory Cells\n* Bidirectional LSTMs / Gated Recurrecnt Units (GRU)\n\n> ### So, let's get started!!!\n\n**As always, I hope you find this kernel useful and your [UPVOTES](https://www.kaggle.com/rizdelhi/quora-insincere-questions-part-3) would be highly appreciated.**"},{"metadata":{},"cell_type":"markdown","source":"**Previous Kernels**\n\n> [⚡END-to-END-Natural Language Processing-1⚡- Exploratory Data Analysis & Pre-trained Word Embedding Models](https://www.kaggle.com/rizdelhi/end-to-end-natural-language-processing-1) \n\n> [⚡END-to-END-Natural Language Processing-2⚡-Statistical Models and Ensemble Technique to Improvise Performance](https://www.kaggle.com/rizdelhi/end-to-end-natural-language-processing-2) "},{"metadata":{},"cell_type":"markdown","source":"### Data loaded "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re  \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re           \nfrom bs4 import BeautifulSoup \nfrom nltk.corpus import stopwords   \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras import regularizers\nfrom keras import layers\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPool1D\nfrom gensim.models import KeyedVectors\nfrom keras.models import Sequential\nfrom tensorflow.keras import regularizers\nfrom keras import layers\nfrom tensorflow.keras.layers import Embedding\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Building Blocks of Artificial Neural Networks (ANN) Model with Keras-Tensorflow 2.0\n\nWe'll be following the below pipeline to create neural networks for text classification:\n\n1. Tokenize the input features- This implies converting the input data into tokens (by using one hot encoding/tokenizing) \n2. Tokenize the targets- This can be done with the help of Label Encoder(sklearn) or using \"values()\" from python\n3. Padd the tokenized features- To ensure that the length of the tokenized feature is same across all the entries(post padding)\n4. Create a simple model: Build a Sequential Network with Keras 'Embedding layer' as the starting point - 'Keras Sequential API' \n5. Add Layers to the model: Add either Conv/LSTM/Bi-LSTM/RNN/GRU layers with different activations ('relu' is recommended)\n6. Add the Dense layer to the model- At the end , we have to add the Dense layer by flattening the output of the previous layer\n7. Add necessary activation functions- Sigmoid for Binary Classification , Softmax for Multi-class classification\n8. Print the model architecture using 'plot_model'\n9. Plot loss/accuracy of the model with matplotlib\n\nOR\n\nLaunch Tensorboard to visualize the training parameters - loss,accuracy, etc.\n\n\nThis forms the fundamental steps to build a basic but fundamental pipeline for any language modelling task. \n\nSophistications include adding custom embeddings before the keras Embedding layer and then adding certain other layers(transformer architectures) before the LSTM."},{"metadata":{},"cell_type":"markdown","source":"## Basic CNN Model for NLP with Keras Tokenizer\n\nI am using Keras Tokenizer Class to get the embedding layer to build a simple Convolutional Neural Network model.\n\n[Read: Understanding CNN for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n\n![CNN for NLP](http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-12.05.40-PM.png)"},{"metadata":{},"cell_type":"markdown","source":"### Train data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Model,Sequential\nfrom keras.utils import to_categorical\n# Load the input features\npd.set_option('display.max_colwidth',None)\ntrain_df=pd.read_csv('../input/clean-quora-train-data/clean_lem_stemmed_train_data.csv') #cleaned data imported from previous kernel\ntrain_df=train_df.dropna()\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df['question_text'] # input\ny = train_df['target'].values # target /label\n\nsentences_train,sentences_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=11)\n\ntokenizer = Tokenizer(num_words=30000)\ntokenizer.fit_on_texts(sentences_train)\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_val = tokenizer.texts_to_sequences(sentences_val)\n\n# Adding 1 because of  reserved 0 index\nvocab_size = len(tokenizer.word_index) + 1 # (in case of pre-trained embeddings it's +2)                         \nmaxlen = 131 # sentence length\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n\n\nprint(\"Padded and Tokenized Training Sequence\".format(),X_train.shape)\nprint(\"Target Training Values Shape\".format(),y_train.shape)\nprint(\"_____________________________________________\")\nprint(\"Padded and Tokenized Validation Sequence\".format(),X_val.shape)\nprint(\"Target Validatation Values Shape\".format(),y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_tokens=len(tokenizer.word_index)+2\nprint(\"Number of Features/Tokens:\",num_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Delete unused memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generic function to plot the train/validation loss and accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nseed = 1000\n\n# generic function to plot the train Vs validation loss/accuracy:\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    plt.figure(figsize=(25,15))\n    ## Accuracy\n    plt.subplot(2,2,1)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n\n    plt.title('Training Accuracy Vs Validation Accuracy\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(2,2,2)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    \n    plt.title('Training Loss Vs Validation Loss\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function to plot confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\ndef conf_matrix(actual, prediction, model_name):\n    cm_array=metrics.confusion_matrix(actual,prediction,labels=[0,1])\n    sns.set_context(\"notebook\", font_scale=1.1)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm_array,annot=True, fmt='.0f',xticklabels=['Sincere','Insincere'],yticklabels=['Sincere','Insincere'])\n    plt.ylabel('True\\n')\n    plt.xlabel('Predicted\\n')\n    plt.title(model_name)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep ConvNet model without pre-trained embeddings\n\nI am using the 'Keras Embedding' layer and visualize the results before using the embedding models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ConvNet model\n\nembedding_dim = 100\n# number_of_tokens=len(tokenizer.word_index)+1\n\nembedding_layer = Embedding(vocab_size,embedding_dim,input_length=maxlen,trainable=True)\n\n# model = tf.keras.Sequential()\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nx = layers.Conv1D(256, 5, activation=\"relu\")(embedded_sequences)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dense(128, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(10,activation='relu')(x)\npreds = layers.Dense(1, activation='sigmoid')(x)\nmodel = keras.Model(int_sequences_input, preds)\n\nmodel.summary()\n\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train, y_train,epochs=5,validation_data=(X_val, y_val),batch_size=1024)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the accuracy & loss "},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Architecture of the ConvNet model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model\n# model.save('cnn_nlp_model.h5')\n# plotting the architecture\ndot_img_file = '/tmp/model_cnn.png'\ntf.keras.utils.plot_model(model, show_shapes=True,to_file=dot_img_file, rankdir=\"TB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model,history,\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## About RNNs\n### Recurrent Neural Network (RNN)\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling **sequence data** such as time series or natural language.\n\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n- Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n\n- Ease of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic keras.layers.RNN layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n\nSome resources for understanding the derivatives and optimization inside the RNNs:\n\n[Maths PDF](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf)\n\n[Colah's Article on RNNs](https://colah.github.io/posts/2015-09-NN-Types-FP/)\n\n[Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn)\n\n### Long Short Term Memory (LSTM)\n\nDrawbacks of RNNS: One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends. Sometimes, we only need to look at recent information to perform the present task. \n\nFor example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information. But there are also cases where we need more context. \n\nConsider trying to predict the last word in the text “I grew up in France… I speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.\n\nIn theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs don’t have this problem!\n\n\n[LSTM Video](https://www.youtube.com/watch?v=WCUNPb-5EYI)\n\n[About LSTM - blog](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/)\n\n\nThere are several Variants of LSTMs some of the most famous being Depth GRU /Gated Recurrent Units.\n\n### Gated Recurrent Unit (GRU)\n\nGRU introduced by Cho, et al. (2014).It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n[Paper: Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf)"},{"metadata":{},"cell_type":"markdown","source":"## Basic RNN Neural Networks Model without pre-trained Embeddings\n\nIn this context, I am building a preliminary deep neural model with different variants of RNNs. I am also building a simple LSTM model for validating the influence of deep models with respect to the statistical ones. \n\nI am not be using any pretrained static/dynamic embeddings but will be using a simple Neural Network model of LSTM to create the network.\n\nThere are three built-in RNN layers in Keras:\n\n- keras.layers.SimpleRNN, a fully-connected RNN where the output from previous timestep is to be fed to next timestep.\n\n- keras.layers.GRU, first proposed in Cho et al., 2014.\n\n- keras.layers.LSTM, first proposed in Hochreiter & Schmidhuber, 1997.\n\n[Read Jason's Blog - Best Practices for Text Classification](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic RNN(LSTM) model without pretrained embeddings\n\nmodel_RNN = Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n                        LSTM(64),\n                        Dense(16,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_RNN.summary()\n# save the model\n# model_RNN.save('rnn_nlp_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the architecure"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the architecure\ndot_img_file = '/tmp/model_RNN.png'\ntf.keras.utils.plot_model(model_RNN, to_file=dot_img_file, rankdir=\"LR\",show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_RNN.fit(X_train,y_train,epochs=2,validation_data=(X_val, y_val),batch_size=2056)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the loss and accuracy of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_RNN,dot_img_file,history\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bi-directional RNN model without pre-trained Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# \n\nembedding_layer = Embedding(vocab_size,embedding_dim,input_length=maxlen,trainable=True)\n\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nz=Bidirectional(LSTM(32,return_sequences='True'))(embedded_sequences)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel_biRNN= Model(inputs=int_sequences_input,outputs=z)\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_biRNN.summary()\n# save the model\n#model_biRNN.save('biRNN_nlp_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_img_file = '/tmp/model_birnn.png'\ntf.keras.utils.plot_model(model_biRNN, to_file=dot_img_file, rankdir=\"LR\",show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_biRNN.fit(X_train,y_train,epochs=1,validation_data=(X_val, y_val),batch_size=2056)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_biRNN,dot_img_file,history\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/10LKovKon8DENq/giphy.gif\" width=\"300\" height=\"100\" align=\"left\">"},{"metadata":{},"cell_type":"markdown","source":"# Neural Networks with Static Semantic Embeddings Baseline\n \nIn this context, I'd explore certain embeddings which may increase the performance of the model. Pre-trained embeddings provide a better representation of word vectors.\n\n> ## GloVe, GoogleNews and FastText embeddigns are my starting point!!"},{"metadata":{},"cell_type":"markdown","source":"> ## GloVe Embeddings"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/xT1R9M8505GD2mz2da/giphy.gif\" width=\"300\" height=\"100\" align=\"left\">"},{"metadata":{},"cell_type":"markdown","source":"### Designing the Embedding matrix with Glove Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\n\npath_to_glove_file = os.path.join('../input/pretrained/', \"glove.6B.100d.txt\")\n\n## make a dict mapping words (strings) to their NumPy vector representation:\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, dtype=float, sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embedding Layer with GloVe Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"## prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\nword_index=tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+ 2\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))\n\n\n#load the pre-trained word embeddings matrix into an Embedding layer.\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"embedding matrix shape:\",embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the ConvNet Model - GloVe embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Build the ConvNet Model - GloVe embeddings\n\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\n\nx = layers.Conv1D(256, 5, activation=\"relu\")(embedded_sequences)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dense(128, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(10, kernel_regularizer=regularizers.l1(l1=1e-4),activation='relu')(x)\npreds = layers.Dense(1, activation='sigmoid')(x)\nmodel = keras.Model(int_sequences_input, preds)\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train,y_train,epochs=2,validation_data=(X_val, y_val),batch_size=2056)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the accuracy and loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model,history\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN model on Keras Sequential API with GloVe embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the GloVe word embeddings matrix into an Embedding layer\n\nmodel_RNN = Sequential([layers.Embedding(num_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False),\n                        LSTM(60),\n                        Dense(20,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_RNN.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_RNN.fit(X_train,y_train,epochs=1,validation_data=(X_val, y_val),batch_size=2056)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BiDirectional RNN model with GloVe Embeddings "},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the pre-trained word embeddings matrix into an Embedding layer.\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False\n)\n\nembedding_layer = Embedding(vocab_size,embedding_dim,input_length=maxlen,trainable=True)\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\n\nz=Bidirectional(LSTM(32,return_sequences='True'))(embedded_sequences)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel_biRNN= Model(inputs=int_sequences_input,outputs=z)\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_biRNN.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nhistory = model_biRNN.fit(X_train,y_train,epochs=1,validation_data=(X_val, y_val),batch_size=2056)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embedding_matrix, model_RNN, model_biRNN,history\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ##  GoogleNews Embeddings"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/l1UkRZuk6FPFYn0ewa/giphy.gif\" width=\"300\" height=\"100\" align=\"left\">"},{"metadata":{},"cell_type":"markdown","source":"### Designing GoogleNews Embedding Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"google_news_embed=\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\nembeddings_index = KeyedVectors.load_word2vec_format(google_news_embed, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Found %s word vectors.\" % len(embeddings_index.vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+2\nnb_words   = min(num_tokens, len(word_index))\nembed_size = 300\n\nembedding_matrix = np.zeros((nb_words+2, embed_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## prepare a corresponding embedding matrix that used in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n\nhits = 0\nmisses = 0\n\n# embedding matrix\nfor word, i in word_index.items():\n    try:\n        embedding_vector = embeddings_index.get_vector(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    except:\n        misses += 1\n    \n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Embedding Matrix Shape:\",embedding_matrix.shape)\nprint(\"Number of Tokens      :\",num_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the ConvNet Model - GoogleNews Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Build the ConvNet Model - GoogleNews embeddings\n\nembedding_dim=300\n\nmodel = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                    layers.Conv1D(256, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.GlobalMaxPooling1D(),\n                    layers.Dense(128, activation=\"relu\"),\n                    layers.Dropout(0.5),\n                    layers.Dense(10, kernel_regularizer=regularizers.l1(l1=1e-4),activation='relu'),\n                    layers.Dense(1, activation='sigmoid')])\n\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the ConvNet Model with GoogleNews Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train,y_train,batch_size=2056,epochs=1,validation_data=(X_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model,embeddings_index,history\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN Model with GoogleNews Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_RNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                        LSTM(60),\n                        Dense(20,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_size = 2056\nhistory = model_RNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=1,validation_data=(X_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_RNN,history\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BiDirectional RNN model with GoogleNews Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_biRNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                          layers.Bidirectional(LSTM(32,return_sequences='True')),\n                          layers.GlobalMaxPool1D(),\n                          layers.Dense(16,activation='relu'),\n                          layers.Dense(1,activation='sigmoid')])\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the architecture\ndot_img_file = '/tmp/model_birnn_google.png'\ntf.keras.utils.plot_model(model_biRNN, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_size = 2056\nhistory = model_biRNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=1,validation_data=(X_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_biRNN,history,embedding_matrix\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Fasttext Embeddings"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/3og0IMVPaqrnGfBnZm/giphy.gif\" align ='left'>"},{"metadata":{},"cell_type":"markdown","source":"### Word2Vec Model with Fasttext Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Using the fasttext word embeddding from crawl\n\nfasttext_file= \"../input/pretrained/crawl-300d-2M.vec\"\nfasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\n\nword_index = tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Found %s word vectors.\" % len(fasttext_model.vocab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Embedding Matrix with Fasttext Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"## prepare a corresponding embedding matrix that used in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n\nembed_size = 300\nembedding_matrix = np.zeros((num_tokens, embed_size))\n\nhits = 0\nmisses = 0\n\n# embedding matrix\nfor word, i in word_index.items():\n    try:\n        embedding_vector = fasttext_model.get_vector(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    except:\n        misses += 1\n    \n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Embedding Matrix Shape:\",embedding_matrix.shape)\nprint(\"Number of Tokens      :\",num_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ConvNet Model with Fasttext Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Build the ConvNet Model - asttext embeddings\n\nembedding_dim=300\n\nmodel = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                    layers.Conv1D(256, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.GlobalMaxPooling1D(),\n                    layers.Dense(128, activation=\"relu\"),\n                    layers.Dropout(0.5),\n                    layers.Dense(10, kernel_regularizer=regularizers.l1(l1=1e-4),activation='relu'),\n                    layers.Dense(1, activation='sigmoid')])\n\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_size = 1024\nhistory = model.fit(X_train,y_train,batch_size=BATCH_size,epochs=2,validation_data=(X_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the accuracy and loss "},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Print the model architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the architecture\ndot_img_file = '/tmp/model_cnn_fast.png'\ntf.keras.utils.plot_model(model, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del history,model,dot_img_file\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN Model with Fasttext Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_RNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                        LSTM(64),\n                        Dense(32,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# fit the model\nBATCH_size = 1024\nhistory = model_RNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=1,validation_data=(X_val,y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the architecture\ndot_img_file = '/tmp/model_rnn_fast.png'\ntf.keras.utils.plot_model(model_RNN, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del history,model_RNN,dot_img_file\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BiDirectional RNN Model with Fasttext Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim=300\n\nmodel_biRNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                          layers.Bidirectional(LSTM(64,return_sequences='True')),\n                          layers.GlobalMaxPool1D(),\n                          layers.Dense(128,activation='relu'),\n                          layers.Dense(64,activation='relu'),\n                          layers.Dense(16,activation='relu'),\n                          layers.Dense(1,activation='sigmoid')])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Print the Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_img_file = '/tmp/model_birnn_fast.png'\ntf.keras.utils.plot_model(model_biRNN, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation F1 Score and Accuracy on the BiRNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn import metrics\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n\n## Log Directory-TensorBoard\n\n# root log directory - with logs and sub-directory of current data and time\nroot_logdir = os.path.join(\"os.curdir\",\"my_logs\")\n\ndef get_run_logdir():\n    import time\n    run_id = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n    return os.path.join(root_logdir,run_id)\n\nrun_logdir = get_run_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir,histogram_freq=1)\n\nepochs=2\nBATCH_size = 1024\n\nfor e in range(epochs):\n    model_biRNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=3,validation_data=(X_val,y_val),callbacks=[tensorboard_cb])\n    pred_fast_val_y = model_biRNN.predict([X_val], batch_size=1024, verbose=1)\n    best_thresh = 0.6\n    best_score = 0.0\n    for thresh in np.arange(0.1, 0.601, 0.01):\n        thresh = np.round(thresh, 2)\n        score = metrics.f1_score(y_val, (pred_fast_val_y>thresh).astype(int))\n        if score > best_score:\n            best_thresh = thresh\n            best_score = score\n    print(\"Val F1 Score: {:.4f}\".format(best_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix on Validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nimport seaborn as sns\n\npred_y_val = (pred_fast_val_y>best_thresh).astype(int)\n\ndef conf_matrix(actual, prediction, model_name):\n    cm_array=metrics.confusion_matrix(actual,prediction,labels=[0,1])\n    sns.set_context(\"notebook\", font_scale=1.1)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm_array,annot=True, fmt='.0f',xticklabels=['Sincere','Insincere'],yticklabels=['Sincere','Insincere'])\n    plt.ylabel('True\\n')\n    plt.xlabel('Predicted\\n')\n    plt.title(model_name)\n    plt.show()\n    \n\nconf_matrix(y_val,pred_y_val,'Bidirectional RNN Model with fasttext embeddings\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/testdataquora/test.csv\")\ntest_sentences = test_df['question_text']\n\ntokenizer = Tokenizer(num_words=30000)\ntokenizer.fit_on_texts(test_sentences)\nX_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = pad_sequences(X_test, padding='post', maxlen=126)\n\npred_test_y = model_biRNN.predict([X_test], batch_size=1024, verbose=1)\n\n#submission file\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission_quora_birnn.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Metrics\\n\")\nprint(metrics.classification_report(y_val,pred_y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dynamic Embeddings - ELMo (Embeddings from Language Models)\n\n<img src='https://images.squarespace-cdn.com/content/v1/5208f2f8e4b0f3bf53b73293/1486510537795-YWYY4NDK68CT5VBPZZR2/ke17ZwdGBToddI8pDm48kDrMjE7hBq4fQV3wYHraitJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzj2bmKhA1a89vhGCTEuFcMrGIAhTIwGn2DOXg1A8iNSPxvh_zK_LmuDa3ZMbEzfBk/Elmo_Emoji_animating_Jazz_Hands_JS_Y_v06.gif?format=2500w'>\n\n\nDeep contextual embeddings and sentence/word vectors falls under dynamic embeddings. These embeddings are current SOTA implying that there is a need for robust Neural Network models.\n\n\n[📖 READ **Attention Is All You Need**- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhine](https://arxiv.org/abs/1706.03762)\n\n[📖 READ **Deep contextualized word representations** - Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer](https://arxiv.org/abs/1802.05365)\n\nBoth these papers are essentially important for their contributions to contextual deep embeddings.\n\nI  highly recommend to read these papers!!! These are really cool explanation of how ELMo was designed.\n\nELMo deep contextualized word embeddings (developed by AllenNLP) are helpful in achieving state-of-the-art (SOTA) results in several NLP tasks. \n\n![Lena Voita's Blog](https://lena-voita.github.io/resources/lectures/transfer/elmo/training-min.png)\n\n### Under the hood:\n\nThe architecture above uses a character-level convolutional neural network (CNN) to represent words of a text string into raw word vectors\nThese raw word vectors act as inputs to the first layer of biLM\nThe forward pass contains information about a certain word and the context (other words) before that word\nThe backward pass contains information about the word and the context after it\nThis pair of information, from the forward and backward pass, forms the intermediate word vectors\nThese intermediate word vectors are fed into the next layer of biLM\nThe final representation (ELMo) is the weighted sum of the raw word vectors and the 2 intermediate word vectors\n\nAs the input to the biLM is computed from characters rather than words, it captures the inner structure of the word. For example, the biLM will be able to figure out that terms like beauty and beautiful are related at some level without even looking at the context they often appear in. Sounds incredible!\n\n\n[📖 READ Analytical Vidya](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)\n\n<img src =\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/03/output_YyJc8E.gif\">"},{"metadata":{},"cell_type":"markdown","source":"> #### Visit **[END-to-END-Natural Language Processing-4](https://www.kaggle.com/rizdelhi/end-to-end-natural-language-processing-4)** to the explore implemenation of the following:\n\n- Sequence2Sequence models without/with Attention Heads\n- Transformers\n- ELMo, DistilBERT embeddings\n- BERT\n- roBERTo\n- ALBERT\n\n\n<img src=\"https://media.giphy.com/media/10b7yI48cD31K0/giphy.gif\" width=\"300\" height=\"100\" align=\"right\">"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}