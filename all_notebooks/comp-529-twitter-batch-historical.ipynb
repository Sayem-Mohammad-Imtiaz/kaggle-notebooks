{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom iso3166 import countries\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/covid19-tweets/covid19_tweets.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missed = pd.DataFrame()\nmissed['column'] = df.columns\n\nmissed['percent'] = [round(100* df[col].isnull().sum() / len(df), 2) for col in df.columns]\nmissed = missed.sort_values('percent')\nmissed = missed[missed['percent']>0]\n\nfig = px.bar(\n    missed, \n    x='percent', \n    y=\"column\", \n    orientation='h', \n    title='Missed values percent for every column (percent > 0)', \n    height=400, \n    width=600\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = df['user_location'].value_counts().reset_index()\nds.columns = ['user_location', 'count']\nds = ds[ds['user_location']!='NA']\nds = ds.sort_values(['count'])\n\nfig = px.bar(\n    ds.tail(40), \n    x=\"count\", \n    y=\"user_location\", \n    orientation='h', title='Top 40 user locations by number of tweets', \n    width=800, \n    height=800\n)\n\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pie_count(data, field, percent_limit, title):\n    \n    data[field] = data[field].fillna('NA')\n    data = data[field].value_counts().to_frame()\n\n    total = data[field].sum()\n    data['percentage'] = 100 * data[field]/total    \n\n    percent_limit = percent_limit\n    otherdata = data[data['percentage'] < percent_limit] \n    others = otherdata['percentage'].sum()  \n    maindata = data[data['percentage'] >= percent_limit]\n\n    data = maindata\n    other_label = \"Others(<\" + str(percent_limit) + \"% each)\"\n    data.loc[other_label] = pd.Series({field:otherdata[field].sum()}) \n    \n    labels = data.index.tolist()   \n    datavals = data[field].tolist()\n    \n    trace=go.Pie(labels=labels,values=datavals)\n\n    layout = go.Layout(\n        title = title,\n        height=600,\n        width=600\n        )\n    \n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \npie_count(df, 'user_location', 0.5, 'Number of tweets per location')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['hashtags'] = df['hashtags'].fillna('[]')\ndf['hashtags_count'] = df['hashtags'].apply(lambda x: len(x.split(',')))\ndf.loc[df['hashtags'] == '[]', 'hashtags_count'] = 0\n\ndf.head(10)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['hashtags_count'].describe()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date']) \ndf = df.sort_values(['date'])\ndf['day'] = df['date'].astype(str).str.split(' ', expand=True)[0]\ndf['time'] = df['date'].astype(str).str.split(' ', expand=True)[1]\ndf.head()","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = df['day'].value_counts().reset_index()\nds.columns = ['day', 'count']\nds = ds.sort_values('count')\nds['day'] = ds['day'].astype(str) + ':00:00:00'\nfig = px.bar(\n    ds, \n    x='count', \n    y=\"day\", \n    orientation='h',\n    title='Tweets distribution over days present in dataset', \n    width=800, \n    height=800\n)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_hashtags(x): \n    return str(x).replace('[', '').replace(']', '').split(',')\n\ntweets_df = df.copy()\ntweets_df['hashtag'] = tweets_df['hashtags'].apply(lambda row : split_hashtags(row))\ntweets_df = tweets_df.explode('hashtag')\ntweets_df['hashtag'] = tweets_df['hashtag'].astype(str).str.lower().str.replace(\"'\", '').str.replace(\" \", '')\ntweets_df.loc[tweets_df['hashtag']=='', 'hashtag'] = 'NO HASHTAG'\ntweets_df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = tweets_df['hashtag'].value_counts().reset_index()\nds.columns = ['hashtag', 'count']\nds = ds.sort_values(['count'])\nfig = px.bar(\n    ds.tail(20), \n    x=\"count\", \n    y='hashtag', \n    orientation='h', \n    title='Top 20 hashtags', \n    width=800, \n    height=700\n)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['tweet_length'] = df['text'].str.len()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_wordcloud(df, title):\n    wordcloud = WordCloud(\n        background_color='gray', \n        stopwords=set(STOPWORDS), \n        max_words=50, \n        max_font_size=40, \n        random_state=666\n    ).generate(str(df))\n\n    fig = plt.figure(1, figsize=(14,14))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=16)\n    fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_wordcloud(df['text'], 'Prevalent words in tweets for all dataset')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = df[df['user_name']=='GlobalPandemic.NET']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for GlobalPandemic.NET')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = df[df['user_name']=='covidnews.ch']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for covidnews.ch')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = df[df['user_name']=='Open Letters']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Open Letters')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = df[df['user_name']=='Hindustan Times']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Hindustan Times')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = df[df['user_name']=='Blood Donors India']\nbuild_wordcloud(test_df['text'], 'Prevalent words in tweets for Blood Donors India')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"build_wordcloud(df['user_description'], 'Prevalent words in tweets for Blood Donors India')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['location'] = df['user_location'].str.split(',', expand=True)[1].str.lstrip().str.rstrip()\nres = df.groupby(['day', 'location'])['text'].count().reset_index()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country_dict = {}\nfor c in countries:\n    country_dict[c.name] = c.alpha3\n    \nres['alpha3'] = res['location']\nres = res.replace({\"alpha3\": country_dict})\n\ncountry_list = ['England', 'United States', 'United Kingdom', 'London', 'UK']\n\nres = res[\n    (res['alpha3'] == 'USA') | \n    (res['location'].isin(country_list)) | \n    (res['location'] != res['alpha3'])\n]\n\ngbr = ['England', 'UK', 'London', 'United Kingdom']\nus = ['United States', 'NY', 'CA', 'GA']\n\nres = res[res['location'].notnull()]\nres.loc[res['location'].isin(gbr), 'alpha3'] = 'GBR'\nres.loc[res['location'].isin(us), 'alpha3'] = 'USA'\nres.loc[res['alpha3'] == 'USA', 'location'] = 'USA'\nres.loc[res['alpha3'] == 'GBR', 'location'] = 'United Kingdom'\nplot = res.groupby(['day', 'location', 'alpha3'])['text'].sum().reset_index()\nplot","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.choropleth(\n    plot, \n    locations=\"alpha3\",\n    hover_name='location',\n    color=\"text\",\n    animation_frame='day',\n    projection=\"natural earth\",\n    color_continuous_scale=px.colors.sequential.Plasma,\n    title='Tweets from different countries for every day',\n    width=800, \n    height=600\n)\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}