{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Prediction of Volatility with Econometrics-Deep Learning Integrated Model   \n### : Predicting VKOSPI with GARCH-RNN Model\n\nIn the era of uncertainty, the market volatility has never been higher. VIX, also known as the 'fear index' had hit record high in the midst of COVID-19. Predicting tomorrow's volatility is key to the future investment.  \n\nMeanwhile, machine learning boom is overtaking every industry from automobile to retail. The machine learning solution is revolutionary; in other words, it has little respect to the conventional ways of doing things. This rigidness might come as a shortcoming in some cases.   \n\nThis notebook aims to predict market volatility. Instead of mere machine learning, it is integrated with a good ol' econometric(statistical) model. Data from Korean stock market during 2009 to 2019 was employed. The target data is VKOSPI which represents the volatility in Korean stock market.\n\n\n[Link to dataset](https://www.kaggle.com/ninetyninenewton/vkospi)","metadata":{}},{"cell_type":"markdown","source":"### Volatility\nThe volatility $\\sigma$, of a stock is a measure of our uncertainty about the returns provided by the stock. It can be defined as the standard deviation of the return provided by the stock. Volatility can be measured in several ways.\n1. Historical Volatility  \n> Historical volatility is calculated from historical data of a stock price.  \n\n1. Implied Volatility  \n> Implied volatility is calculated from options prices observed in the market. VKOSPI indicates implied volatility.\n\n**Therefore, we**  \n1. **Estimate historical volatility using econometric model.**  \n1. **Close gap between historical volatility and implied volatility with option-related variables using deep learning.**","metadata":{}},{"cell_type":"markdown","source":"## 0. Setup","metadata":{}},{"cell_type":"code","source":"# Import packages\nimport os\nimport pickle\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom scipy import stats\nimport scipy.optimize as optimize ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Preprocess Data","metadata":{}},{"cell_type":"markdown","source":"### 1-1. Import data","metadata":{}},{"cell_type":"code","source":"# Import data - Korean Market Data\norg_data = pd.read_csv(\"../input/vkospi/options_KR.csv\")\n\n\n# Import data - Volatility index around the world\n\nCBOE_Volatility = pd.read_csv(\"../input/volatility-index-around-the-world/CBOE Volatility Index Historical Data.csv\")\nHSI_Volatility  = pd.read_csv(\"../input/volatility-index-around-the-world/HSI Volatility Historical Data.csv\")\nNikkei_Volatility = pd.read_csv(\"../input/volatility-index-around-the-world/Nikkei Volatility Historical Data.csv\")\n\nCBOE_Volatility = CBOE_Volatility.loc[:,['Date','Price']]\nHSI_Volatility = HSI_Volatility.loc[:,['Date','Price']]\nNikkei_Volatility = Nikkei_Volatility.loc[:,['Date','Price']]","metadata":{"_cell_guid":"","_uuid":"","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"org_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CBOE_Volatility.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1-2. Convert date data\n\nConvert date data into pd.datetime\n","metadata":{}},{"cell_type":"code","source":"# Convert date data\n\norg_data['Date'] = pd.to_datetime(org_data['Date'])\nCBOE_Volatility['Date'] = pd.to_datetime(CBOE_Volatility['Date'])\nHSI_Volatility['Date'] = pd.to_datetime(HSI_Volatility['Date'])\nNikkei_Volatility['Date'] = pd.to_datetime(Nikkei_Volatility['Date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1-3. Preprocess time series data\n\nTarget data of Day n is predicted from the variables of Day n-1, except for 'Day_till_expiration' and 'Day_of_a_week' since they can be known in advance.  \n\nIt would be convenient to preprocess the dataframe so the input variables and the corresponding target variables are in the same row.","metadata":{}},{"cell_type":"code","source":"pivot = ['Date','VKOSPI','Day_till_expiration','Day_of_a_week']\n\n# Save the original data\nfull_data = org_data.copy()\n\n# Shift the pivot data\nfull_data[pivot] = full_data[pivot].shift(periods=-1)\n\n# Drop the last row\nfull_data = full_data.drop(full_data.index[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1-4. Format foreign volatility indices as input data\n\nLike most of the other markets, Korean market is largely affected by global markets. Thus, volatility indices of major foreign markets can be helpful to predict the volatility of Korean market. Volatility indices of US(VIX), China(HSI), and Japan(Nikkei) markets are used here.\n\nConsidering that Seoul's timezone is ahead or the same of US, China, or Japan, \nany volatility index of the same day cannot be known in advance.   \n\nTherefore, when predicting Day n, foreign volatility indices of Day n-1 are used.   \nIf Day n-1 is not avilable, Day n-2.     \nIf Day n-2 is not avilable, Day n-3, and so on.    \n","metadata":{}},{"cell_type":"code","source":"# Format foreign volatility indices as input data\n\ndef correspond_foreign_vol(date,data):\n    \"\"\"\n    find the 'Price' of the date that is most recent to the given 'Date' in the 'data'\n    \"\"\"\n    \n    while True:\n        date = date - pd.Timedelta('1 Day')  # go back one day \n        result_series = data['Price'].loc[data['Date']==date]  # find the 'Price' in the data that matches with the 'date'\n        \n        if not result_series.empty:  # if not empty (which means there is a row that matches with the 'date')\n            result_series = result_series.reset_index()  # reset index\n            result_value = result_series['Price'][0]  # and get the value\n            return result_value\n\n# Apply function\n\nfull_data['CBOE'] = full_data['Date'].apply(correspond_foreign_vol,data=CBOE_Volatility)\nfull_data['HSI'] = full_data['Date'].apply(correspond_foreign_vol,data=HSI_Volatility)\nfull_data['Nikkei'] = full_data['Date'].apply(correspond_foreign_vol,data=Nikkei_Volatility)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_data[['Date','CBOE','HSI','Nikkei']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Estimate historical volatility with GARCH model\nGARCH, or GARCH(1,1) to be precise, is an widely used econometric model to estimate historical volatility.\n\nGARCH estimates the historical volatility in Day n with: \n* **Rate of Return** (of an underlying asset) in Day n-1 (denoted by $ u $)\n* **Volatility** in Day n-1 (denoted by $\\sigma$)\n* Note that $\\sigma^{2}$ is often referred to **Variance**\n\nThere are three coefficients in the model:  \n* **alpha**, coefficent of $ u^{2} $\n* **beta**, coefficient of $ \\sigma^{2} $\n* **omega**, constant (It is actually not just a constant, but we will make it simple here.)\n\nThe formula for estimating (historical) volatility of Day n is:  \n  \n$$\n\\sigma^{2}_{n}= \\omega + \\alpha u^{2}_{n-1} + \\beta \\sigma^{2}_{n-1}\n$$\n  \n  \n    \nAll referenced from John C. Hull's \"Options, Futures, And Other Derivatives (8th edition)\".    \n\nFor more info,  \n[GARCH Model](https://vlab.stern.nyu.edu/docs/volatility/GARCH)","metadata":{}},{"cell_type":"markdown","source":"### 2-1. Calculate return \n  \nCalculate 'rate of return', or 'return' in short.  \n\nThe formula for return is the following, where $u$ denotes return and $S$ denotes the price of an underlying asset(e.g. stock). There is an another formula which involves logarithm, but this one will be used here.  \n  \n    \n$$\nu_{n} = \\frac{S_{n} - S_{n-1}}{S_{n-1}}\n$$\n","metadata":{}},{"cell_type":"code","source":"# Calculate return\n\nKOSPI200_yesterday = org_data['KOSPI200'].shift(periods=1)  # Series made up of shifted KOSPI200\nreturn_array = (org_data['KOSPI200']-KOSPI200_yesterday) / KOSPI200_yesterday  # Calculate return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2-2. Build GARCH model\n\nBuild a function which estimates the volatility of Day n with GARCH model.  \nFollowing is a simple reminder of the formula.  \n\n$$\n\\sigma^{2}_{n}= \\omega + \\alpha u^{2}_{n-1} + \\beta \\sigma^{2}_{n-1}\n$$","metadata":{}},{"cell_type":"code","source":"# GARCH model\ndef garch_forward(return_rate,variance,coefficients):\n    ''' data type: float, float, 1d array(length=3)'''\n    \n    # Coefficients\n    alpha,beta,omega = coefficients\n    # Calculate\n    return omega + alpha*return_rate*return_rate + beta*variance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2-3. Choose the values for coefficients in the model - Build function\n\n#### 2-3-1. Method\n\nTo estimate the volatility with the model, the values for the coefficients must be chosen. This is done with historical data by using \"maximum likelihood method\". This method involves maximizing the probability with which historical data occurs.   \n  \nIt is assumed that the return at any Day n ($ u_{n}$) follows the nomral distribution with zero mean. Variance at each day is $v_{n}$. Therefore, a probability of observing $u_{n}$ is  \n$$\\frac{1}{\\sqrt{2\\pi v_{n}}} exp(\\frac{-u_{i}^{2}}{2v_{n}})$$\n  \nTaking logarithms and ignoring constant multiplicative factors, the expression we wish to maximize in each day is the following.\n$$-ln(v_{n})-\\frac{u_{n}^{2}}{v_{n}}$$\n  \nSum of the above expression over the data is what we finally want to maximize. \n  \n  \n#### 2-3-2. Values\n* Return($u_{n}$) is calculated directly from the data. (This was done in 2-1.)  \n* Variance($v_{n}$) is estimated by GARCH. Initial variance is set from the real VKOSPI value at that date.  \n  \n  **Variance($\\sigma^{2}$)** represents 1 day.  \n**VKSOPI** is an index of **volatility($\\sigma$)** for 1 year expressed in percentage(%).  \nAssuming there are 252 trading days per year, **VKOSPI** can be converted into **variance** by the following.  \n  \n$$\nVariance = (VKOSPI)^{2} \\: / \\: (252 \\times 100)\n$$","metadata":{}},{"cell_type":"code","source":"# Set VKOSPI as an initial variance in the model\n\ninitial_vkospi = full_data['VKOSPI'][1] \ninitial_variance = initial_vkospi*initial_vkospi/2520000 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for optimization\n\ndef garch_for_optimization(array): \n    ''' data type: 1d array(length=3)'''\n    \n    # Coeffcients\n    alpha,beta,omega = array \n    \n    # Variables\n    sum_probability = 0 # to maximize\n    variance = initial_variance\n    \n    for i in range(1,return_array_train.shape[0]):  # exclude the first value because it's nan.\n        return_rate = return_array_train[i]  \n        \n        # in case something goes wrong\n        if variance<=0:\n            print(\"Negative variance\")\n            break\n        \n        # calculate probability in a single day\n        probability = -np.log(variance) - return_rate * return_rate / variance\n        \n        # add to the sum\n        sum_probability += probability\n        \n        # calculate next day's variance by GARCH\n        variance = garch_forward(return_rate,variance,array)\n   \n    return -sum_probability # note the sign(-); because scipy.optimize requires a function to be minimized","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2-4. Choose the values for coefficients in the model - Optimize\n  \nUsing scipy.optimize library, we optimize the function we built right before. \n\nNYU V-Lab's estimate (in Feb 15, 2020) was used for initial guess. The reason not simply taking V-Lab's estimate is because the ideal coefficients can differ by the time period. The bounds are set due to the underlying concept of GARCH, which will not be further explained here.\n\nSince optimization consumes quite a time, the result is already pickled and you can load it.\n","metadata":{}},{"cell_type":"code","source":"# Optimize\n# if execute is set TRUE, optimization which takes some time will be initiated\nexecute = False    \n\nif execute:\n    bounds = optimize.Bounds([0,0,0],[1,1,np.inf])\n    initial_guess = [0.14,0.76,2.97]  # V-Lab's estimate\n\n    # Trust-constr performed the best \n    optimize_res_trust = optimize.minimize(garch_for_optimization,initial_guess,method='trust-constr',bounds=bounds)\n\n    # Pickle the result since it takes too much time\n    with open('../input/optimize_res_trust.pkl','wb') as file:\n        pickle.dump(optimize_res_trust,file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pickle: optimize_res_trust\nwith open('/kaggle/input/optimize-res-trust/optimize_res_trust.pkl','rb') as file:\n    optimize_res_trust = pickle.load(file)\n\n# Optimization result\nprint('optimization result:')\nprint('alpha =',optimize_res_trust.x[0])\nprint('beta =',optimize_res_trust.x[1])\nprint('omega =',optimize_res_trust.x[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2-5. Estimate historical volatility\n\nNow the coefficients are chosen, we can estimate historical volatility of each day using GARCH model.  ","metadata":{}},{"cell_type":"code","source":"# Estimate historical volatility with estimated coefficients\n\nvariance_array = np.zeros(full_data.shape[0],)  # Create array to store variance\n\n# Values to be pre-assigned\nvariance_array[0] = np.nan  # Historical volatility cannot be estimated for the first one in full_data (06/03/2009)\nvariance_array[1] = initial_variance \n\n# Calculate historical volatilities using GARCH\nfor i in range(2,full_data.shape[0]):\n    variance_array[i]=garch_forward(return_array[i-1],variance_array[i-1],optimize_res_trust.x)\n    \n# Adjust value to compare with VKOSPI (elaborated in 2-3-2)  \nhistorical_volatility = np.sqrt(variance_array * 252) * 100\n\n# Add to the dataset\nfull_data['Historical Volatility'] = historical_volatility\nfull_data = full_data.dropna()  # Drop NA (because of nan in historical volatility)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2-6. Result\n\nThough prediction is not finished, it's worth visualizaing and calculating the error.  ","metadata":{}},{"cell_type":"code","source":"# Before visualizing, few matplotlib settings\nmpl.rcParams['axes.labelsize'] = 'x-large'\nmpl.rcParams['axes.labelpad'] = 5.5  # space between axis and label\nmpl.rcParams['axes.titlesize'] = 'large'\nmpl.rcParams['axes.titlepad'] = '20.0'\n\nmpl.rcParams['legend.fontsize'] = 'x-large'\n\n# mpl color settings\ndefault_clrs = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\n# and seaborn settings\nsns.set_style('darkgrid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare with VKOSPI (Visualization)\npd.plotting.register_matplotlib_converters()  # because of compatilbility issue with pd.Timestamp and matplotlib\n\nfig, ax = plt.figure(figsize=(20,7)), plt.axes()\nsns.lineplot(data=full_data,x='Date',y='VKOSPI',label='VKOSPI (target)',ax=ax)\nsns.lineplot(data=full_data,x='Date',y='Historical Volatility',label='Historical Volatility (prediction)',ax=ax)\nplt.ylabel('Volatility')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate error\nabs_garch = abs(full_data['VKOSPI']-full_data['Historical Volatility'])\nsquare_garch = (full_data['VKOSPI']-full_data['Historical Volatility'])**2\n\nprint('Total set')\nprint('MAE:',abs_garch.mean())\nprint('RMSE:',square_garch.mean()**0.5)\n\nprint('\\nTest set ([-516:])')\nprint('MAE:',abs_garch[-516:].mean())\nprint('RMSE:',square_garch[-516:].mean()**0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis\n  \nIn this section, we examine and visualize various market variables to see whether they qualify as an input variable of the nerual network. \n\n\n1. Day of a week \n> `'Day_of_a_week'`\n1. Foreign volatility indices  \n> `'CBOE'` `'HSI'` `'Nikkei'`\n1. Days left untill expiration date \n>`'Day_till_expiration'`\n1. Other market variables\n>`'KOSPI200','Open_interest','For_KOSPI_Netbuying_Amount','For_Future_Netbuying_Quantity',\n 'For_Call_Netbuying_Quantity','For_Put_Netbuying_Quantity','Indiv_Future_Netbuying_Quantity',\n 'Indiv_Call_Netbuying_Quantity','Indiv_Put_Netbuying_Quantity','PCRatio'`","metadata":{}},{"cell_type":"code","source":"# Split data into train data(including validation data) and test data\nsplit_ratio = 0.8\n\ndata = full_data.set_index('Date')\nlen_data = full_data.shape[0]\n\ntrain_org = full_data.iloc[:int(len_data*split_ratio),]\ntest_org = full_data.iloc[int(len_data*split_ratio):,]\n\nprint('train set',train_org.shape)\nprint('test set',test_org.shape)","metadata":{"_cell_guid":"","_uuid":"","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3-1. Day of a week ('Day_of_a_week')\n\n\nThe correlation seems to be statistically insignificant. ","metadata":{}},{"cell_type":"code","source":"# 1. Day of a week\n# confidence interval = 95%\nbarplot = sns.barplot(x='Day_of_a_week',y='VKOSPI',data=train_org,ci=95,order=['Mon','Tue','Wed','Thu','Fri']) \nbarplot.set_ylim((16,18))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3-2. Foreign volatility indices ('CBOE', 'HSI',  'Nikkei')\n1. ```VKOSPI(KOSPI200,South Korea)``` has the highest correlation with ```VIX(S&P500,US)```, just like the rest of the world.  \n(VIX is denoted by `'CBOE'` in the code, in which VIX is calculated and disseminated.) \n2. ```VHSI(HSI,Hong Kong)``` follows, as Korean economy is highly dependent on Chinese economy.  \n3. ```JNVI(Nikkei,Japan)``` then follows.  \n\nConsidering the high correlations with each other, we're only including `'CBOE'(VIX)`.  \n\n","metadata":{}},{"cell_type":"code","source":"# 2. Foreign volatility indices\ncorr = train_org.loc[:,['VKOSPI','CBOE','HSI','Nikkei']].corr()\n\n# heatmap\nmask = np.zeros_like(corr)\nfor i in range(mask.shape[0]):\n    mask[i,i]=True\nsns.heatmap(corr,annot=True,mask=mask,cmap='coolwarm',vmin=-1,vmax=1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3-3. Days left untill expiration date ('Day_till_expiration')\n\nWe will include `Day_till_expiration`.","metadata":{}},{"cell_type":"code","source":"# start with simple scatterplot\nsns.scatterplot(x='Day_till_expiration',y='VKOSPI',data=train_org)\nplt.show()\n\n# too bizarre when VKOSPI is small.\nprint('Too bizarre when VKOSPI is small.')\nprint('Instead of plotting every data, plot mean of VKOSPI within each Day_till_expiration.')\n      \nmean_by_dtexp = train_org.groupby('Day_till_expiration').mean()['VKOSPI']\nplt.figure(figsize=(15,5))\nplt.ylabel('Mean of VKOSPI')\nmean_by_dtexp_plot = mean_by_dtexp - 10  # for plotting purpose\nsns.barplot(x=mean_by_dtexp.index,y=mean_by_dtexp_plot.values,bottom=10)\nplt.show()\n\n# reasonable to suspect that there aren't much data with very high day_till_expiration\nprint('It is reasonable to suspect that there aren\\'t much data when day_till_expiration is high')\nlen_by_dtexp = train_org['Day_till_expiration'].value_counts()\nplt.figure(figsize=(15,5))\nplt.xlabel('Day_till_expiration')\nplt.ylabel('Frequency of Data')\nsns.barplot(x=len_by_dtexp.index,y=len_by_dtexp.values)\nplt.show()\n\nprint('The amount of data seems to be insufficient when Day_till_expiration is high')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exclude the cases with insufficient amount of data\n\ndef slice_and_anaylze(slice_at):\n    # slice dataframe till the given parameter(slice_at)\n    \n    print(f'Plot from 0 to {slice_at}.')\n    mean_by_dtexp = full_data.groupby('Day_till_expiration').mean()['VKOSPI']\n    mean_by_dtexp = mean_by_dtexp[:slice_at]  # slice\n    mean_by_dtexp = pd.DataFrame({'Day_till_expiration':mean_by_dtexp.index,\n                                  'Mean_of_VKOSPI':mean_by_dtexp.values})  # Convert series to dataframe (for sns plot)\n    \n    # lmplot\n    sns.lmplot(x='Day_till_expiration',y='Mean_of_VKOSPI',data=mean_by_dtexp)\n    plt.show()\n\n    # correlation test (pearson)\n    corr_coef = stats.pearsonr(mean_by_dtexp['Day_till_expiration'],mean_by_dtexp['Mean_of_VKOSPI'])\n    print('Pearson correlation test')\n    print('='*50)\n    print('correlation coefficient:',corr_coef[0])\n    print('2-tailed p-value:', corr_coef[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It isn't clear how much data is sufficient enough. Hereby we test a couple of cases.  \n\n1. 0~24  \n> This case excludes only 25 and 26, in which there is almost no data at all. \n\n1. 0~19  \n> This case only includes ones with more than 50 days, which is the half of maximum.","metadata":{}},{"cell_type":"code","source":"#~24\nslice_and_anaylze(24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#~19\nslice_and_anaylze(19)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> It's hard to determine how much data is needed to consider mean of VKOSPI valid.     \n  \n> However, the first analysis(\\~24) definitely shows the correlation between **the days left until expiration date** and **VKOSPI**.   \nEven the second analysis(\\~19) wasn't so bad. \n\nTherefore, it's reasonable to think that correlation is statistically significant, or at least **this variable can be fed into a neural network later on**.  ","metadata":{}},{"cell_type":"markdown","source":"### 3-4. Other market variables\n`'KOSPI200','Open_interest','For_KOSPI_Netbuying_Amount','For_Future_Netbuying_Quantity',\n 'For_Call_Netbuying_Quantity','For_Put_Netbuying_Quantity','Indiv_Future_Netbuying_Quantity',\n 'Indiv_Call_Netbuying_Quantity','Indiv_Put_Netbuying_Quantity','PCRatio'`\n \n \n> Only `KOSPI200` seems to have linear correlation with `VKOSPI`\n  \n> Within the variables, only `Indiv_Future_Netbuying_Quantity` and `For_Future_Netbuying_Quantity` seems to have correlation with each other. However, it seems unnecessary to exclude one of the variables from the poteintial neural net input variables.\n\nSince visualization doesn't reveal everything, we will not entirely exclude these variables.","metadata":{}},{"cell_type":"code","source":"# Plot each market variable with VKOSPI\n\n# Columns to plot \nplot_columns = ['KOSPI200','Open_interest','For_KOSPI_Netbuying_Amount','For_Future_Netbuying_Quantity',\n                'For_Call_Netbuying_Quantity','For_Put_Netbuying_Quantity','Indiv_Future_Netbuying_Quantity',\n                'Indiv_Call_Netbuying_Quantity','Indiv_Put_Netbuying_Quantity','PCRatio']\n\n# plot\nfig, axes = plt.subplots(10,2,figsize=(25,75))\nfor i,col in enumerate(plot_columns):\n    sns.regplot(col,'VKOSPI',data=train_org, ax=axes[i,0]) # regression plot\n    sns.kdeplot(train_org[col],train_org['VKOSPI'],shade=True, ax=axes[i,1]) # kernel density estimate plot\n  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ncorr = train_org[plot_columns].corr()\n\nmask = np.zeros_like(corr)\nfor i in range(mask.shape[0]):\n    mask[i,i]=True\nplt.figure(figsize=(12,5))\nsns.heatmap(corr,annot=True,mask=mask,cmap='coolwarm',vmin=-1,vmax=1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot variables with high correlation coefficient\n\ndef cor_reg_kde(x,y,data):\n    # Correlation coefficient\n    print('correlation coefficient:',corr.loc[x,y])\n    \n    # Subplot adjust\n    plt.subplots_adjust(wspace=0.5)\n    \n    # Plot (regression and kernel density)\n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n    sns.regplot(x,y,data=data,ax=ax1)\n    sns.kdeplot(data[x],data[y],shade=True,ax=ax2)\n    \ncor_reg_kde('Indiv_Future_Netbuying_Quantity','For_Future_Netbuying_Quantity',train_org)\ncor_reg_kde('Indiv_Put_Netbuying_Quantity','Open_interest',train_org)\ncor_reg_kde('Indiv_Put_Netbuying_Quantity','Indiv_Call_Netbuying_Quantity',train_org)\ncor_reg_kde('Indiv_Call_Netbuying_Quantity','Open_interest',train_org)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Data Preprocessing for Neural Net\nIn this section, we preprocess data before feeding into neural network.","metadata":{}},{"cell_type":"markdown","source":"### 4-1. Normalize data","metadata":{}},{"cell_type":"code","source":"# Normalize data\n\n# Keep the original\ntrain = train_org.copy()\ntest = test_org.copy()\n\n# Variables that has to be normalized\nto_normalize = ['VKOSPI','KOSPI200', 'Open_interest',\n                  'For_KOSPI_Netbuying_Amount', 'For_Future_Netbuying_Quantity',\n                  'For_Call_Netbuying_Quantity', 'For_Put_Netbuying_Quantity',\n                  'Indiv_Future_Netbuying_Quantity', 'Indiv_Call_Netbuying_Quantity',\n                  'Indiv_Put_Netbuying_Quantity', 'PCRatio', 'Day_till_expiration',\n                  'CBOE','Historical Volatility'\n                 ]\n# Normalize\nmean_train = train[to_normalize].mean()\nstd_train = train[to_normalize].std()\n\ntrain[to_normalize] = (train[to_normalize]-mean_train)/std_train\ntest[to_normalize] = (test[to_normalize]-mean_train)/std_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-2. Select input variables","metadata":{}},{"cell_type":"code","source":"# Select input variables\ninput_var = ['KOSPI200', 'Open_interest','Day_till_expiration', 'CBOE', 'Historical Volatility',\n             'For_KOSPI_Netbuying_Amount','For_Future_Netbuying_Quantity',\n             'For_Call_Netbuying_Quantity','For_Put_Netbuying_Quantity',\n             'Indiv_Future_Netbuying_Quantity', 'Indiv_Call_Netbuying_Quantity',\n            'Indiv_Put_Netbuying_Quantity'\n            ] \nprint(f'total {len(input_var)} variables')\n\n# Subsample data\nx_train_org = train[input_var]\nx_test_org = test[input_var]\ny_train_org = train['VKOSPI']\ny_test_org = test['VKOSPI']\n\nprint('x_train_org', x_train_org.shape)\nprint('y_train_org', y_train_org.shape)\nprint('x_test_org', x_test_org.shape)\nprint('y_test_org', y_test_org.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-3. Reshape data\n* We set our **time steps to be 4**.   \n* This means that we predict the VKOSPI with previous data from **last 5 days**.   \n* It's 5, not 4, because we already shifted 1 day before EDA (in 1-3). \n* 5 days usually represent one week. (excluding weekend)\n  \nShape of reshpaed data is ***(batch_size, time_steps, number_of_feautures)***.   \n  \n  \n`number of features` will remain the same.    \n`time steps` will be set to 4.","metadata":{}},{"cell_type":"code","source":"TIME_STEPS = 4  #predict with 5 days (+1 because we already shifted 1 day in 1-3)\n\n# function\ndef reshape(data, time_steps = TIME_STEPS):\n    stack = [data.iloc[i:i+time_steps].to_numpy() for i in range(len(data)-time_steps)]\n    reshaped = np.stack(stack, axis=0) \n    return reshaped\n\n# reshape\nx_train_val = reshape(x_train_org) # train data and validation data\ny_train_val = reshape(y_train_org)\nx_test = reshape(x_test_org)\ny_test = reshape(y_test_org)\n\nprint('x_train + x_val', x_train_val.shape)\nprint('y_train + y_val', y_train_val.shape)\nprint('x_test', x_test.shape)\nprint('y_test', y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-4. Seperate validation data from train data","metadata":{}},{"cell_type":"code","source":"# Seperate validation data from train data\ni = int(len(x_train_val) * 0.8)\n\nx_train = x_train_val[:i]\nx_val = x_train_val[i:]\ny_train = y_train_val[:i]\ny_val = y_train_val[i:]\n\nprint('train data:', x_train.shape[0])\nprint('validation data:', x_val.shape[0])\nprint('test data:', x_test.shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Neural Network\nIn this section, we build a neural network which includes historical volatility as an input variable.  \nOther option-related variables are also included in order to predict implied volatility.\n  \n  \nWe will use a couple of different neural network.\n1. Vanilla NN\n2. LSTM\n3. GRU","metadata":{}},{"cell_type":"markdown","source":"### 5-1. Hyperparameter tuning\nBayesian optimization will be applied using `kerastuner`.","metadata":{}},{"cell_type":"code","source":"from kerastuner import BayesianOptimization\n\n# Function 'model_builder' builds and compiles neural network with given hyperparameters\n\n# vanilla nn\ndef nn_builder(hp):\n    # hp\n    UNITS = hp.Int('UNITS', min_value = 16, max_value = 128, step = 16)\n    ACTIVATION_1 = hp.Choice('ACTIVATION',values = ['relu','linear','tanh'])\n    ACTIVATION_2 = hp.Choice('ACTIVATION',values = ['relu','linear','tanh'])\n    \n    # model instance\n    model = keras.Sequential([\n        keras.layers.Flatten(),\n        keras.layers.Dense(units = UNITS, activation = ACTIVATION_1),\n        keras.layers.Dropout(rate = 0.2),\n        keras.layers.Dense(1, activation = ACTIVATION_2)\n    ])\n    \n    # compile\n    model.compile(optimizer = keras.optimizers.Adam(),\n                  loss = 'mse', metrics = ['mae'])    \n    \n    return model\n\n# lstm\ndef lstm_builder(hp):\n    # hp\n    UNITS_1 = hp.Int('UNITS_1', min_value = 16, max_value = 128, step = 16)\n    UNITS_2 = hp.Int('UNITS_2', min_value = 8, max_value = 32, step = 8)\n    ACTIVATION = hp.Choice('ACTIVATION',values = ['relu','linear','tanh'])\n    \n    # model instance\n    model = keras.Sequential([\n        # input_shape = (time_steps, # of features)\n        keras.layers.LSTM(units = UNITS_1, input_shape = (None, x_train.shape[2]), return_sequences = False), \n        keras.layers.Dense(UNITS_2, activation = ACTIVATION),\n        keras.layers.Dropout(rate = 0.2),\n        keras.layers.Dense(1, activation = 'linear')\n    ])\n    \n    # compile\n    model.compile(optimizer = keras.optimizers.Adam(),\n                  loss = 'mse', metrics = ['mae'])\n    \n    return model\n\n# gru\ndef gru_builder(hp):\n    # hp\n    UNITS_1 = hp.Int('UNITS_1', min_value = 16, max_value = 128, step = 16)\n    UNITS_2 = hp.Int('UNITS_2', min_value = 8, max_value = 32, step = 8)\n    ACTIVATION = hp.Choice('ACTIVATION',values = ['relu','softmax','linear'])\n    \n    # model instance\n    model = keras.Sequential([\n        # input_shape = (time_steps, # of features)\n        keras.layers.GRU(units = UNITS_1, input_shape = (None, x_train.shape[2]), return_sequences = False),\n        keras.layers.Dense(UNITS_2, activation = ACTIVATION),\n        keras.layers.Dropout(rate = 0.2),\n        keras.layers.Dense(1, activation = 'linear')\n    ])\n    \n    # compile\n    model.compile(optimizer = keras.optimizers.Adam(),\n                  loss = 'mse', metrics = ['mae'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implement earlystopping with keras callback \n\nPATIENCE = 5 # number of epochs with no improvement after which training will be stopped.\n\nEarlystopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n                                              min_delta = 0.001,\n                                              patience=PATIENCE, \n                                              mode='min', \n                                              restore_best_weights=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Search hyperparameters\nSEED = 121\n\n# NN\ntuner_nn = BayesianOptimization(nn_builder,\n                                objective = 'val_loss',\n                                max_trials = 20,\n                                seed = SEED,\n                                directory = 'kerastuner',\n                                overwrite = True\n                                )\n\ntuner_nn.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val), verbose=0, callbacks=[Earlystopping])\n\n## Build model based on the optimized hyperparameters\nbesthp_nn = tuner_nn.get_best_hyperparameters()[0]\nmodel_nn = tuner_nn.hypermodel.build(besthp_nn)\n\n\n# lstm\ntuner_lstm = BayesianOptimization(lstm_builder,\n                            objective = 'val_loss',\n                            max_trials = 20,\n                            seed = SEED,\n                            directory = 'kerastuner')\n\ntuner_lstm.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val), verbose=0, callbacks=[Earlystopping])\n\n## Build model based on the optimized hyperparameters\nbesthp_lstm = tuner_lstm.get_best_hyperparameters()[0]\nmodel_lstm = tuner_lstm.hypermodel.build(besthp_lstm)\n\n\n# gru\ntuner_gru = BayesianOptimization(gru_builder,\n                            objective = 'val_loss',\n                            max_trials = 20,\n                            seed = SEED,\n                            directory = 'kerastuner')\n\ntuner_gru.search(x_train, y_train, epochs=50, validation_data=(x_val, y_val), verbose=0, callbacks=[Earlystopping])\n\n## Build model based on the optimized hyperparameters\nbesthp_gru = tuner_gru.get_best_hyperparameters()[0]\nmodel_gru = tuner_gru.hypermodel.build(besthp_gru)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"besthp_nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5-2. Train model","metadata":{}},{"cell_type":"code","source":"# Colors designated to each model\npalette = default_clrs[:4]\npalette.append(default_clrs[6])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PALETTE**\n* 0: target data (VKOSPI)\n* 1: GARCH (benchmark)\n* 2: GARCH-NN\n* 3: GARCH-LSTM\n* 4: GARCH-GRU","metadata":{}},{"cell_type":"code","source":"# Ensure reproducible result\nprint(SEED)\ntf.random.set_seed(SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model - NN\nhistory = model_nn.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), verbose=0, callbacks=[Earlystopping])\n\n# Plot loss\nplt.plot(history.history['loss'], label = 'loss')\nplt.plot(history.history['val_loss'], label = 'val_loss', color=palette[2])\nplt.legend()\n\n# Loss\nprint('final loss:',history.history['loss'][-1])\nprint('final val_loss:',history.history['val_loss'][-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_nn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model - LSTM\nhistory = model_lstm.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), verbose=0, callbacks=[Earlystopping])\n\n# Plot loss\nplt.plot(history.history['loss'], label = 'loss')\nplt.plot(history.history['val_loss'], label = 'val_loss',color=palette[3])\nplt.legend()\n\n# Loss\nprint('final loss:',history.history['loss'][-1])\nprint('final val_loss:',history.history['val_loss'][-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lstm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model - GRU\nhistory = model_gru.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), verbose=0, callbacks=[Earlystopping])\n\n# Plot loss\nplt.plot(history.history['loss'], label = 'loss')\nplt.plot(history.history['val_loss'], label = 'val_loss',color=palette[4])\nplt.legend()\n\n# Loss\nprint('final loss:',history.history['loss'][-1])\nprint('final val_loss:',history.history['val_loss'][-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_gru.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Result","metadata":{}},{"cell_type":"code","source":"# Data for comparison\ntarget_date = test_org['Date'].iloc[TIME_STEPS:] \ntarget_vkospi = test_org['VKOSPI'].iloc[TIME_STEPS:]\npredictions_hv = test_org['Historical Volatility'].iloc[TIME_STEPS:]\n\n# Function to scale back predictions\ndef scale_back(predictions):\n    predictions = [prediction[0] for prediction in predictions] # unpack array\n\n    # scale back\n    mean_vol = train_org['VKOSPI'].mean()\n    std_vol = train_org['VKOSPI'].std() \n    predictions = [prediction * std_vol + mean_vol for prediction in predictions]\n    \n    return predictions\n\n# Predict\npredictions_nn = scale_back(model_nn.predict(x_test))\npredictions_lstm = scale_back(model_lstm.predict(x_test))\npredictions_gru = scale_back(model_gru.predict(x_test))\n\n# pd.Series-ization\npredictions_nn = pd.Series(predictions_nn, index=target_vkospi.index, name='predictions_nn')\npredictions_lstm = pd.Series(predictions_lstm, index=target_vkospi.index, name='predictions_lstm')\npredictions_gru = pd.Series(predictions_gru, index=target_vkospi.index, name='predictions_gru')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\nfig1, ax1 = plt.figure(figsize=(20,7)), plt.axes()\nfig2, ax2 = plt.figure(figsize=(20,7)), plt.axes()\nfig3, ax3 = plt.figure(figsize=(20,7)), plt.axes()\nfig4, ax4 = plt.figure(figsize=(20,7)), plt.axes()\n\n# target data\nax1.plot(target_date, target_vkospi, label='VKOSPI (target data)')\nax2.plot(target_date, target_vkospi, label='VKOSPI (target data)')\nax3.plot(target_date, target_vkospi, label='VKOSPI (target data)')\nax4.plot(target_date, target_vkospi, label='VKOSPI (target data)')\n\n# predictions\nax1.plot(target_date, predictions_hv, label='GARCH',color=palette[1])\nax2.plot(target_date, predictions_nn, label='GARCH-NN',color=palette[2])\nax3.plot(target_date, predictions_lstm, label='GARCH-LSTM',color=palette[3])\nax4.plot(target_date, predictions_gru, label='GARCH-GRU',color=palette[4])\n\n# show\nax1.legend()\nax2.legend()\nax3.legend()\nax4.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MSE and MAE","metadata":{}},{"cell_type":"code","source":"# Calculate error\ndef cal_MSE(predictions):\n    errors = (predictions - target_vkospi)**2\n    return errors.mean()\n\ndef cal_MAE(predictions):\n    errors = abs(predictions - target_vkospi)\n    return errors.mean()\n\n\n# Print error\nprint('GARCH')\nprint('MSE:',cal_MSE(predictions_hv))\nprint('MAE:',cal_MAE(predictions_hv))\n\nprint('GARCH-NN')\nprint('MSE:',cal_MSE(predictions_nn))\nprint('MAE:',cal_MAE(predictions_nn))\n\nprint('GARCH-LSTM')\nprint('MSE:',cal_MSE(predictions_lstm))\nprint('MAE:',cal_MAE(predictions_lstm))\n\nprint('GARCH-GRU')\nprint('MSE:',cal_MSE(predictions_gru))\nprint('MAE:',cal_MAE(predictions_gru))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of best predictions","metadata":{}},{"cell_type":"code","source":"# Compare day by day\n\n# error list\nerrors_hv = abs(target_vkospi - predictions_hv)\nerrors_nn = abs(target_vkospi - predictions_nn)\nerrors_lstm = abs(target_vkospi - predictions_lstm)\nerrors_gru = abs(target_vkospi - predictions_gru)\n\n# count the best models\nbest_models = []\nfor i in range(len(errors_hv)):\n    each_predictions = [errors_hv.iloc[i], errors_nn.iloc[i], errors_lstm.iloc[i], errors_gru.iloc[i]]\n    best_models.append(each_predictions.index(min(each_predictions)))\n\nbest_counts = [best_models.count(i) for i in range(4)]\n\n    \n# plot\nfig, ax = plt.figure(), plt.axes()\nax.pie(best_counts, labels=['GARCH','GARCH-NN','GARCH-LSTM','GARCH-GRU'], autopct='%.0f%%', startangle=90, colors=palette[1:])\nplt.title('Number of days in which it performed the best')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of predicting the right direction (up or down)","metadata":{}},{"cell_type":"code","source":"# direction arrays\ndirections_vkospi = target_vkospi - target_vkospi.shift()\ndirections_hv = predictions_hv - predictions_hv.shift()\ndirections_nn = predictions_nn - predictions_nn.shift()\ndirections_lstm = predictions_lstm - predictions_lstm.shift()\ndirections_gru = predictions_gru - predictions_gru.shift()\n\n# multiplicate element-wise, and count the positivie elements\ndef count_pos(series):\n    count = 0\n    for x in series:\n        if x>0:\n            count += 1\n    return count\n\ndirection_counts = [511,0,0,0,0]\ndirection_counts[1] = count_pos(directions_hv * directions_vkospi)\ndirection_counts[2] = count_pos(directions_nn * directions_vkospi)\ndirection_counts[3] = count_pos(directions_lstm * directions_vkospi)\ndirection_counts[4] = count_pos(directions_gru * directions_vkospi)\n\n# plot\nfig, ax = plt.figure(), plt.axes()\nax.bar(['Total Days','GARCH','GARCH-NN','GARCH-LSTM','GARCH-GRU'],direction_counts, color=palette)\nplt.title('Number of days in which it predicted the right direction(up or down)')\nplt.show()\n\n# direction prediction accuracy\nprint('GARCH')\nprint(direction_counts[1] / direction_counts[0]*100,'%')\nprint('GARCH-NN')\nprint(direction_counts[2] / direction_counts[0]*100,'%')\nprint('GARCH-LSTM')\nprint(direction_counts[3] / direction_counts[0]*100,'%')\nprint('GARCH-GRU')\nprint(direction_counts[4] / direction_counts[0]*100,'%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}