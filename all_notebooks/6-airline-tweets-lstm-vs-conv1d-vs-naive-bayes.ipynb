{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(1) Load Libraries","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport keras\nfrom keras.utils import to_categorical\n\nimport re\nfrom nltk.corpus import stopwords\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(2) Load tweets dataset","metadata":{}},{"cell_type":"code","source":"tweets = pd.read_csv(\"../input/twitter-airline-sentiment/Tweets.csv\")\ntweets.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweets.columns)\ndf = pd.DataFrame({\n    \"sentiment\":tweets.airline_sentiment,\n    \"text\":tweets.text\n})\nprint(f\"Shape of our dataset >> {df.shape}\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sentiment.replace(['negative','neutral','positive'],[-1,0,1],inplace=True)\ndf.sample(5)\n\nsns.set_style(\"whitegrid\")\nsns.countplot(data=df,x='sentiment')\nindex = [0,1,2]\nplt.xticks(index,['negative','neutral','positive'])\nplt.title(\"Distribution of sentiment labels\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(3) Clean text data: \n* remove urls\n* remove shortwords ( words of which length is 1 or 2)\n* remove @\n* remove #\n* remove stopwords from nltk.corpus module","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\nshortword = re.compile(r\"\\b\\w{1,2}\\b\")\nurl = re.compile(r\"https?:*/+[a-zA-Z0-9./]*\")\n\ndef clean(text):\n    text = re.sub(url,'',text)\n    text = re.sub(shortword,'',text)\n    text = text.replace('@','')\n    text = text.replace('#','')\n    \n    text = text.split()\n    text = [word for word in text if word not in stop_words]\n    text = \" \".join(text)\n    \n    return text\n\ndf.text = df.text.apply(clean)\ndf.text[:15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(4) Tokenize (words to integers)","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.text)\nprint(f\"{len(tokenizer.word_index)} words are used\\n\")\n\ncounts = tokenizer.word_counts\nprint(len(counts))\n\ntotal_freq = 0\nrare_freq = 0\nrare_counts = 0\nthread=2\n\nfor key,value in counts.items():\n    total_freq += value\n    if value<thread:\n        rare_freq += value\n        rare_counts += value\n\nprint(f\"{rare_counts} are used less than {thread} times\")\nprint(f\"And these words accounts for {np.round(rare_freq/total_freq*100,2)}% of whole texts\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Tokenize only 7000 words.\\nOther words are considered OOV\")\nword_size=7000\nvocab_size = word_size+1\ntokenizer = Tokenizer(num_words=word_size)\n\ntokenizer.fit_on_texts(df.text)\ntokenized = tokenizer.texts_to_sequences(df.text)\n\nprint(\"\\nSamples\\n\")\nprint(tokenized[0])\nprint(tokenized[1])\nprint(len(tokenized))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(5) Pad & truncate sequences (post)","metadata":{}},{"cell_type":"code","source":"lengths = [len(s) for s in tokenized]\nprint(f\"Average length of each row >> {np.mean(lengths)}\")\nprint(f\"Maximum length of each row >> {np.max(lengths)}\")\n\nplt.hist(lengths,bins=50)\nplt.show()\n\nsequence_size = 20\nprint(f\"Pad all sequences into size of {sequence_size}\")\n\npadded = pad_sequences(tokenized,maxlen=sequence_size,padding='post',truncating='post')\nprint(padded.shape)\nprint(\"Padded samples\")\nprint(padded[0])\nprint(padded[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(6) Transform label (setiment data) into one-hot vectors","metadata":{}},{"cell_type":"code","source":"data = padded\nlabel = to_categorical(df.sentiment,num_classes=3)\n\nprint(\"shape of data >>\",data.shape)\nprint(\"shape of label >>\",label.shape)\n\nprint(\"\\nSamples of label data\")\nprint(label[0])\nprint(label[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(7) Train/Test split","metadata":{}},{"cell_type":"code","source":"train_data,test_data,train_label,test_label = train_test_split(data,label,test_size=0.3,stratify=label,random_state=42)\n\nprint(\"shape of train data >>\",train_data.shape)\nprint(\"shape of test data >>\",test_data.shape)\n\nfig = plt.figure(figsize=(12,6))\nax1 = fig.add_subplot(1,2,1)\nsns.countplot(x=np.argmax(train_label,axis=1))\nplt.title(\"Distribution of train label\")\n\nax2 = fig.add_subplot(1,2,2)\nsns.countplot(x=np.argmax(test_label,axis=1))\nplt.title(\"Distribution of test label\")\nplt.show()\n\nindex_to_sentiment = {\n    0:'neutral',\n    1:'positive',\n    -1:'negative'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(8) Train and Test with LSTM model ( using pre-trained embedding vectors)","metadata":{}},{"cell_type":"markdown","source":"(8-1) Load pre-trained Embedding vectors and make an embedding matrix  customized for words we will use","metadata":{}},{"cell_type":"code","source":"import os\n\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict=dict()\n\nf = open(os.path.join('glove.6B.100d.txt'),encoding='utf-8')\nfor line in f:\n    tokens = line.split()\n    word = tokens[0]\n    word_vector = np.asarray(tokens[1:],dtype='float32')\n    embedding_dict[word] = word_vector\n\nf.close()\n\nprint(f\"There are {len(embedding_dict)} embedding vectors in total\")\nprint(f\"Dimension of each vector >> {len(embedding_dict['read'])}\")\nembedding_size = len(embedding_dict['read'])\n\n\nembedding_matrix = np.zeros((vocab_size,embedding_size))\n\nfor word,idx in tokenizer.word_index.items():\n    if idx <= 7000:\n        vector = embedding_dict.get(word)\n        if vector is not None:\n            embedding_matrix[idx] = np.asarray(vector,dtype='float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(8-2) Build Stacked LSTM model (+ bidirectional, many-to-many)","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input,Embedding,TimeDistributed,Bidirectional,LSTM,BatchNormalization,Dense,GlobalMaxPool1D,GlobalAveragePooling1D,Dropout,Masking\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.utils import plot_model\n\nword_vec_size=100\nhidden_size=128\n\ndef create_lstm():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False)(X)\n    H = Masking(mask_value=0.0)(H)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Bidirectional(LSTM(int(hidden_size/2),return_sequences=True))(H)\n    H = Bidirectional(LSTM(int(hidden_size/2),return_sequences=True))(H)\n    \n    H = GlobalMaxPool1D()(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model\n\nes = EarlyStopping(monitor='val_accuracy',mode='min',patience=4,verbose=1)\nrl = ReduceLROnPlateau(monitor='val_loss',mode='min',patience=3,verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model = create_lstm()\nplot_model(lstm_model)\nlstm_history = lstm_model.fit(train_data,train_label,epochs=10,batch_size=64,validation_split=0.2,callbacks=[rl])\nlstm_model.evaluate(test_data,test_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(8-3) LSTM model without using pre-trained embedding vectors","metadata":{}},{"cell_type":"code","source":"word_vec_size=100\nhidden_size=256\n\ndef create_lstm_no_emb():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = Masking(mask_value=0.0)(H)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = BatchNormalization()(H)\n    H = Bidirectional(LSTM(int(hidden_size/2),return_sequences=True))(H)\n    H = BatchNormalization()(H)\n    H = Bidirectional(LSTM(int(hidden_size/2),return_sequences=True))(H)\n    \n    H = GlobalMaxPool1D()(H)\n    H = Dense(64,activation='relu')(H)\n    H = Dense(32,activation='relu')(H)\n    H = Dropout(0.2)(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_no = create_lstm_no_emb()\nplot_model(lstm_no)\nlstm_no_hist = lstm_model.fit(train_data,train_label,epochs=15,batch_size=64,validation_split=0.2,callbacks=[rl])\nlstm_no.evaluate(test_data,test_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(9)Multi Kernel Conv1D model (using pre-trained Embedding Vectors)","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv1D,Concatenate,LeakyReLU,Flatten\n\ndef create_conv1d():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False,mask_zero=True)(X)\n    H = Dropout(0.3)(H)\n    \n    num_filters=[256,256,128,128]\n    kernel_sizes=[3,4,5,6]\n    conv_blocks=[]\n    \n    for i in range(len(kernel_sizes)):\n        conv = Conv1D(filters=num_filters[i],kernel_size=kernel_sizes[i],padding='valid',activation='relu')(H)\n        conv = GlobalMaxPool1D()(conv)\n        conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    \n    H = Concatenate()(conv_blocks)\n    H = Dropout(0.2)(H)\n    \n    H = Dense(128)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    H = Dense(16)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv1d = create_conv1d()\nplot_model(conv1d)\nhist = conv1d.fit(train_data,train_label,epochs=10,validation_split=0.2,batch_size=64,callbacks=[rl])\nconv1d.evaluate(test_data,test_label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(10) Naive Bayes Models : GaussianNB, MultiNomailNB, BernoulliNB","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\nvectorizer = CountVectorizer()\ntransformer = TfidfTransformer()\n\nx = vectorizer.fit_transform(df.text)\nprint(f\"shape >> {x.toarray().shape}\")\nprint(\"samples\\n\")\nprint(x.toarray()[0])\n\nx = transformer.fit_transform(x)\nprint(f\"\\n\\nshape >> {x.toarray().shape}\")\nprint(\"samples\\n\")\nprint(x.toarray()[0])\n\nx = x.toarray()\ny = df.sentiment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.3,random_state=42,stratify=y)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n\nmodels_NB = []\nmodels_NB.append(GaussianNB())\nmodels_NB.append(MultinomialNB())\nmodels_NB.append(BernoulliNB())\n\nfor model in models_NB:\n    model.fit(train_x,train_y)\n    pred = model.predict(test_x)\n    acc = accuracy_score(test_y,pred)\n    print(f\"Model {model.__class__.__name__} accuracy on test dataset >> {acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(11) RandomForestClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n","metadata":{},"execution_count":null,"outputs":[]}]}