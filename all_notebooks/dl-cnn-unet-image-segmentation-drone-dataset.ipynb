{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple Deep Learning model for semantic segmentation \n## Aerial Semantic Segmentation Drone Dataset","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\">This dataset is provided by Graz University of Technology ==> [Aerial Semantic Segmentation Drone Dataset](http://dronedataset.icg.tugraz.at). It is really useful to gain wider knowledge about **Computer Vision** and **Deep Learning** techniques. In this notebook several concepts will be explained:\n    <font>\n- <font size=\"4\">Tensorflow basics-intermediate commands<font>\n- <font size=\"4\">Dealing with **large Datasets**<font>\n- <font size=\"4\">Image processing with OpenCV<font>\n- <font size=\"4\">Creating **Pipelines** using data generators to avoid memory overrunning<font>\n- <font size=\"4\">Training, validation and testing sets<font>\n- <font size=\"4\">Building your own **Convolutional Neural Network (CNN)**<font>\n- <font size=\"4\">Familiarization with simple callbacks:<font>\n    - <font size=\"4\">EarlyStopping<font>\n- <font size=\"4\">Performance analysis - metrics: <font>\n    - <font size=\"4\">Confusion matrix<font>\n    - <font size=\"4\">**MIoU**...<font>\n- <font size=\"4\">Predictions visualization<font>\n\n<font size='4'>The aim of this notebook is to perform semantic segmentation on drone images like the ones shown below.<font><font>","metadata":{}},{"cell_type":"code","source":"import os, cv2\n\npath = '../input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/'\n\nfiles = []\n\nfor i in os.listdir(path)[:9]:\n    files.append(os.path.join(path, i))\n\nimg=[cv2.resize(cv2.imread(files[i], 3), (1200, 800)) for i in range(len(files))]\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(3, 3, figsize=(20,15))\nfor i in range(3):\n    for j in range(3):\n        ax[i, j].imshow(img[3*i+j])\n        ax[i, j].get_xaxis().set_visible(False)\n        ax[i, j].get_yaxis().set_visible(False)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-09T21:09:51.877322Z","iopub.execute_input":"2021-08-09T21:09:51.87784Z","iopub.status.idle":"2021-08-09T21:09:58.758796Z","shell.execute_reply.started":"2021-08-09T21:09:51.87773Z","shell.execute_reply":"2021-08-09T21:09:58.757779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# INDEX\n\n\n- [<font size=\"4\">Setup<font>](#section-one)\n- [<font size=\"4\">Analysing the dataset<font>](#section-two)\n- [<font size=\"4\">Dataset Pipeline for preprocessing<font>](#section-three)\n- [<font size=\"4\">Convolutional Neural Network Model and Training<font>](#section-four)\n- [<font size=\"4\">Model performance<font>](#section-five)\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# 1. Setup \n\n <font size=\"4\">As always, it is necessary to import **required libraries**, just basic setting up.<font>","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence\nimport random\nimport itertools\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-08-09T21:09:58.760385Z","iopub.execute_input":"2021-08-09T21:09:58.760889Z","iopub.status.idle":"2021-08-09T21:10:04.003972Z","shell.execute_reply.started":"2021-08-09T21:09:58.760842Z","shell.execute_reply":"2021-08-09T21:10:04.002993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# 2. Analysing the dataset\n\n<font size=\"4\">Firstly, we should take a look to our dataset, analyse the folders structure as well as the content of each of the dataset files.\nIt can be seen that label images (or masks) store information from the label of each pixel as a number corresponding to each class from 0 to 23 and this is the reason why the images seems to be too dark (0-255).","metadata":{}},{"cell_type":"code","source":"img = cv2.imread('../input/semantic-drone-dataset/dataset/semantic_drone_dataset/original_images/002.jpg', 3)\nlabel = cv2.imread('../input/semantic-drone-dataset/dataset/semantic_drone_dataset/label_images_semantic/002.png', 3)\nrgb = cv2.imread('../input/semantic-drone-dataset/RGB_color_image_masks/RGB_color_image_masks/002.png', 3)\nfig, ax = plt.subplots(1, 3, figsize=(20,6))\nax[0].imshow(img)\nax[1].imshow(label)\nax[2].imshow(rgb)\nax[0].set_title('Input image')\nax[1].set_title('Mask image')\nax[2].set_title('RGB Mask - Expected output')\nax[0].get_xaxis().set_visible(False)\nax[0].get_yaxis().set_visible(False)\nax[1].get_xaxis().set_visible(False)\nax[1].get_yaxis().set_visible(False)\nax[2].get_xaxis().set_visible(False)\nax[2].get_yaxis().set_visible(False)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-09T21:10:04.006869Z","iopub.execute_input":"2021-08-09T21:10:04.007261Z","iopub.status.idle":"2021-08-09T21:10:10.407057Z","shell.execute_reply.started":"2021-08-09T21:10:04.007218Z","shell.execute_reply":"2021-08-09T21:10:10.406251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\">To ease the segmentation visualization, a dictionary has been incorporated to the dataset mapping each class to a specific color<font>. Below, you can see the **RGB tuple** corresponding to each class as well as the class corresponding to each number in the mask image.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/semantic-drone-dataset/class_dict_seg.csv')\ndf","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-09T21:10:10.408707Z","iopub.execute_input":"2021-08-09T21:10:10.409086Z","iopub.status.idle":"2021-08-09T21:10:10.448441Z","shell.execute_reply.started":"2021-08-09T21:10:10.409043Z","shell.execute_reply":"2021-08-09T21:10:10.447669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# 3. Dataset Pipeline for preprocessing\n\n <font size=\"4\"> Once we know better our dataset, we will create a **data generator** using a `tf.data.Dataset` object. Which will allow us not to run out of memory while loading all the dataset at once. What we want to do is just load the original dataset in **batches** in order to pass a lower amount of data to our Neural Network setting **free RAM memory**. You can try to load all the dataset at once to realise what the problem is.<font>\n ","metadata":{}},{"cell_type":"code","source":"num_classes=23\nH=800\nW=1200\n\ndef read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (W, H))\n    x = x/255.0\n    x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (W, H))\n    x = x.astype(np.int32)\n    return x\n\n\ndef tf_dataset(x,y, batch=4):\n    dataset = tf.data.Dataset.from_tensor_slices((x,y)) # Dataset object from Tensorflow\n    dataset = dataset.shuffle(buffer_size=100) \n    dataset = dataset.map(preprocess) # Applying preprocessing to every batch in the Dataset object\n    dataset = dataset.batch(batch) # Determine atch-size\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(2) # Optimization\n    return dataset\n        \n\ndef preprocess(x,y):\n    def f(x,y):\n        x = x.decode()\n        y = y.decode()\n        image = read_image(x)\n        mask = read_mask(y)\n        return image, mask\n    \n    image, mask = tf.numpy_function(f,[x,y],[tf.float32, tf.int32])\n    mask = tf.one_hot(mask, num_classes, dtype=tf.int32)\n    image.set_shape([H, W, 3])    # In the Images, number of channels = 3. \n    mask.set_shape([H, W, num_classes])    # In the Masks, number of channels = number of classes. \n    return image, mask\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:10.449744Z","iopub.execute_input":"2021-08-09T21:10:10.450105Z","iopub.status.idle":"2021-08-09T21:10:10.460078Z","shell.execute_reply.started":"2021-08-09T21:10:10.450067Z","shell.execute_reply":"2021-08-09T21:10:10.459224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'> The structure of the **Pipeline** requires the filenames and the ubication of every of them to create the **Dataset**. So, before calling the data generator we will get the **files paths**. <font>","metadata":{}},{"cell_type":"code","source":"root_dir = '../input/semantic-drone-dataset/dataset/semantic_drone_dataset'\nimg_path = root_dir + '/original_images/'\nmask_path = root_dir + '/label_images_semantic/'\n\nnames = list(map(lambda x: x.replace('.jpg', ''), os.listdir(img_path)))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:10.461371Z","iopub.execute_input":"2021-08-09T21:10:10.461865Z","iopub.status.idle":"2021-08-09T21:10:10.478598Z","shell.execute_reply.started":"2021-08-09T21:10:10.461828Z","shell.execute_reply":"2021-08-09T21:10:10.477776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'> To make things easier we will split our data into the **training, validation and testing** set spliting the file paths array in order to waste less memory.","metadata":{}},{"cell_type":"code","source":"X_trainval, X_test = train_test_split(names, test_size=0.1, random_state=19)\nX_train, X_val = train_test_split(X_trainval, test_size=0.2, random_state=19)\n\nprint(f\"Train Size : {len(X_train)} images\")\nprint(f\"Val Size   :  {len(X_val)} images\")\nprint(f\"Test Size  :  {len(X_test)} images\")\n\ny_train = X_train #the same values for images (X) and labels (y)\ny_test = X_test\ny_val = X_val\n\nimg_train = [os.path.join(img_path, f\"{name}.jpg\") for name in X_train]\nmask_train = [os.path.join(mask_path, f\"{name}.png\") for name in y_train]\nimg_val = [os.path.join(img_path, f\"{name}.jpg\") for name in X_val]\nmask_val = [os.path.join(mask_path, f\"{name}.png\") for name in y_val]\nimg_test = [os.path.join(img_path, f\"{name}.jpg\") for name in X_test]\nmask_test = [os.path.join(mask_path, f\"{name}.png\") for name in y_test]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:10.479897Z","iopub.execute_input":"2021-08-09T21:10:10.480328Z","iopub.status.idle":"2021-08-09T21:10:10.495606Z","shell.execute_reply.started":"2021-08-09T21:10:10.480283Z","shell.execute_reply":"2021-08-09T21:10:10.494578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'> **Testing set preprocessing is quite different!!!** So we will create another Pipeline. <font>\n    \n","metadata":{}},{"cell_type":"code","source":"## Dataset Pipeline used for testing the model\nnum_classes=23\nH=800\nW=1200\n\n\ndef test_dataset(x, batch=4):\n    dataset = tf.data.Dataset.from_tensor_slices(x)\n    dataset = dataset.map(preprocess_test)\n    dataset = dataset.batch(batch)\n    dataset = dataset.prefetch(2)\n    return dataset\n        \n\ndef preprocess_test(x):\n    def f(x):\n        x = x.decode()\n        image = read_image(x)\n        return image\n    \n    image = tf.convert_to_tensor(tf.numpy_function(f, [x] , [tf.float32]))\n    image = tf.reshape(image, (H, W, 3))    # In the Images, number of channels = 3.  \n    return image","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:10.498864Z","iopub.execute_input":"2021-08-09T21:10:10.499129Z","iopub.status.idle":"2021-08-09T21:10:10.506269Z","shell.execute_reply.started":"2021-08-09T21:10:10.499105Z","shell.execute_reply":"2021-08-09T21:10:10.505334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>To end up with preprocessing we will set the batch size (just try the maximum value that prevents your session from crashing). The intention is to put the **bigger batch size possible**: the one that will <u>optimize the training time without running out of memory</u>.<font>","metadata":{}},{"cell_type":"code","source":"batch_size=3\n\ntrain_dataset = tf_dataset(img_train, mask_train, batch = batch_size)\nvalid_dataset = tf_dataset(img_val, mask_val, batch = batch_size)\n\ntrain_steps = len(img_train)//batch_size\nvalid_steps = len(img_val)//batch_size","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:10.508425Z","iopub.execute_input":"2021-08-09T21:10:10.508974Z","iopub.status.idle":"2021-08-09T21:10:12.450938Z","shell.execute_reply.started":"2021-08-09T21:10:10.508938Z","shell.execute_reply":"2021-08-09T21:10:12.449742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# 4. Convolutional Neural Network Model and Training\n\n<font size='4'>The model I have chosen for training is a simple U-Net. Take a look at its architecture [here](https://towardsdatascience.com/u-net-b229b32b4a71).<font>","metadata":{}},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n\n\ndef multi_unet_model(n_classes=23, IMG_HEIGHT=800, IMG_WIDTH=1200, IMG_CHANNELS=3):\n#Build the model\n    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n    s = inputs\n\n    #Contraction path\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n    c1 = Dropout(0.1)(c1)  # Original 0.1\n    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    p1 = MaxPooling2D((2, 2))(c1)\n    \n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n    c2 = Dropout(0.1)(c2)  # Original 0.1\n    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n    p2 = MaxPooling2D((2, 2))(c2)\n     \n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n    c3 = Dropout(0.1)(c3)\n    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n    p3 = MaxPooling2D((2, 2))(c3)\n     \n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n    c4 = Dropout(0.1)(c4)\n    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n     \n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = Dropout(0.3)(c5)\n    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n    \n    #Expansive path \n    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n    c6 = Dropout(0.1)(c6)\n    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n     \n    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n    c7 = Dropout(0.2)(c7)\n    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n     \n    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n    c8 = Dropout(0.1)(c8)  # Original 0.1\n    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n     \n    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n    c9 = Dropout(0.1)(c9)  # Original 0.1\n    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n     \n    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n     \n    model = Model(inputs=[inputs], outputs=[outputs])\n    \n    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n    model.compile(optimizer='adam', loss=['categorical_crossentropy'], metrics=['accuracy'])\n    \n    model.summary()\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:12.456193Z","iopub.execute_input":"2021-08-09T21:10:12.458749Z","iopub.status.idle":"2021-08-09T21:10:12.552297Z","shell.execute_reply.started":"2021-08-09T21:10:12.458702Z","shell.execute_reply":"2021-08-09T21:10:12.551378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>Let's take a glance at the `model.summary()` which will help us to analyse the **layers dimensions**. It is specially important to take care of the **Input and Output layers** as they must match our input and expected output dimensions.","metadata":{}},{"cell_type":"code","source":"model = multi_unet_model()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:12.556875Z","iopub.execute_input":"2021-08-09T21:10:12.55985Z","iopub.status.idle":"2021-08-09T21:10:13.150286Z","shell.execute_reply.started":"2021-08-09T21:10:12.559776Z","shell.execute_reply":"2021-08-09T21:10:13.149513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>To avoid losing time training, it is always useful to set up a `EarlyStopping` callback. There are other types of callbacks which can help you to perform different tasks while training such as saving checkpoints.<font>","metadata":{}},{"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(min_delta=0.001, patience=10)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:13.152352Z","iopub.execute_input":"2021-08-09T21:10:13.152738Z","iopub.status.idle":"2021-08-09T21:10:13.156664Z","shell.execute_reply.started":"2021-08-09T21:10:13.152699Z","shell.execute_reply":"2021-08-09T21:10:13.155665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>Given that this model takes a lot of time to train (6 hours more less). I have pre-trained it on my computer and I will load the pretrain model in order not to fully train it on Kaggle. Here, I will just run 1 epoch to show that it works properly.<font>","metadata":{}},{"cell_type":"code","source":"model =  tf.keras.models.load_model('../input/pretrained-model/model.h5') ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:13.157779Z","iopub.execute_input":"2021-08-09T21:10:13.158524Z","iopub.status.idle":"2021-08-09T21:10:14.273914Z","shell.execute_reply.started":"2021-08-09T21:10:13.158488Z","shell.execute_reply":"2021-08-09T21:10:14.272986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_dataset,\n          steps_per_epoch=train_steps,\n          validation_data=valid_dataset,\n          validation_steps=valid_steps,\n          epochs=1 #just to check it works properly\n         )","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:10:14.275335Z","iopub.execute_input":"2021-08-09T21:10:14.275704Z","iopub.status.idle":"2021-08-09T21:14:33.350081Z","shell.execute_reply.started":"2021-08-09T21:10:14.275664Z","shell.execute_reply":"2021-08-09T21:14:33.349106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# 5. Model performance\n<font size='4'>To conclude, we will load our test Dataset and **evaluate our model on the test set**.<font>","metadata":{}},{"cell_type":"code","source":"test_ds = tf_dataset(img_test, mask_test, batch = batch_size)\nmodel.evaluate(test_ds, steps=14)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:14:33.351952Z","iopub.execute_input":"2021-08-09T21:14:33.352257Z","iopub.status.idle":"2021-08-09T21:15:12.552965Z","shell.execute_reply.started":"2021-08-09T21:14:33.352215Z","shell.execute_reply":"2021-08-09T21:15:12.552031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>Accuracy on the testing set is around **77%**. However, accuracy is not the better metric while doing semantic segmentation. It is commonly used the **Jaccard coefficient** also known as **MIoU (Mean Intersection over Union)**. More information about it [here](https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2).<font>","metadata":{}},{"cell_type":"code","source":"# To visualize loss and accuracy evolution\n\n# df = pd.DataFrame(history.history)\n# df[['loss', 'acc']].plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:15:12.557323Z","iopub.execute_input":"2021-08-09T21:15:12.559589Z","iopub.status.idle":"2021-08-09T21:15:12.565597Z","shell.execute_reply.started":"2021-08-09T21:15:12.559519Z","shell.execute_reply":"2021-08-09T21:15:12.56438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>**Let's predict the outputs!!**<font>","metadata":{}},{"cell_type":"code","source":"pred = model.predict(test_dataset(img_test, batch = 1), steps=40)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:15:12.56966Z","iopub.execute_input":"2021-08-09T21:15:12.570668Z","iopub.status.idle":"2021-08-09T21:15:52.274342Z","shell.execute_reply.started":"2021-08-09T21:15:12.570627Z","shell.execute_reply":"2021-08-09T21:15:52.273263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>Now some post-processing is required to get the **MIoU**. We will define a function to calculate the IoU knowing the values of the confusion matrix and then it is straight-forward to calculate it.<font>","metadata":{}},{"cell_type":"code","source":"pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:15:52.276126Z","iopub.execute_input":"2021-08-09T21:15:52.27647Z","iopub.status.idle":"2021-08-09T21:15:52.282692Z","shell.execute_reply.started":"2021-08-09T21:15:52.276431Z","shell.execute_reply":"2021-08-09T21:15:52.281814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = np.argmax(pred, axis=3)\nlabel = np.array([cv2.resize(cv2.imread(mask_path+img_test[i][-7:-4]+'.png')[:, :, 0], (1200, 800)) for i in range(predictions.shape[0])])\nlabel = label.flatten()\npredictions = predictions.flatten()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:15:52.284082Z","iopub.execute_input":"2021-08-09T21:15:52.284682Z","iopub.status.idle":"2021-08-09T21:16:02.619375Z","shell.execute_reply.started":"2021-08-09T21:15:52.284639Z","shell.execute_reply":"2021-08-09T21:16:02.618276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncolor_dict = pd.read_csv('../input/semantic-drone-dataset/class_dict_seg.csv')\ncm = confusion_matrix(label, predictions, labels=range(23))\ndf_cm = pd.DataFrame(cm,  columns=list(color_dict['name'])[:23])\ndf_cm","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:18:10.556187Z","iopub.execute_input":"2021-08-09T21:18:10.556602Z","iopub.status.idle":"2021-08-09T21:18:59.400538Z","shell.execute_reply.started":"2021-08-09T21:18:10.556565Z","shell.execute_reply":"2021-08-09T21:18:59.39817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iou(cm, i):\n    return cm[i,i]/(sum(cm[i])+sum(cm[:,i])-cm[i,i])\n\nprint('MIoU: {0}%'.format(round(100*np.mean(np.nan_to_num(np.array([iou(cm, i) for i in range(23)]))), 4)))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:21:46.537282Z","iopub.execute_input":"2021-08-09T21:21:46.53769Z","iopub.status.idle":"2021-08-09T21:21:46.550187Z","shell.execute_reply.started":"2021-08-09T21:21:46.537654Z","shell.execute_reply":"2021-08-09T21:21:46.544917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size='4'>**28% MIoU**... Definitely, our model can improve quite a lot but it is a good and simple approach to a **multi-class semantic segmentation problem**. <font>","metadata":{}},{"cell_type":"markdown","source":"# 6. Visualize the model predictions\n\n<font size='4'>Finally, let's see some predictions. Some post-processing is required<font>","metadata":{}},{"cell_type":"code","source":"cmap = np.array(list(color_dict[[' r', ' g', ' b']].transpose().to_dict('list').values()))\npredictions = predictions.reshape(-1, 800, 1200)\nlabel = label.reshape(-1, 800, 1200)\n\ni = 18\nfig, ax = plt.subplots(3, 2, figsize=(15, 15))\nfor j in range(3):\n    ax[j, 0].imshow(cmap[predictions[i+j]])\n    ax[j, 1].imshow(cmap[label[i+j]])\n    ax[j, 0].set_title('Prediction')\n    ax[j, 1].set_title('Ground truth')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T21:22:37.412813Z","iopub.execute_input":"2021-08-09T21:22:37.413168Z","iopub.status.idle":"2021-08-09T21:22:38.715391Z","shell.execute_reply.started":"2021-08-09T21:22:37.413134Z","shell.execute_reply":"2021-08-09T21:22:38.714422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It performs quite well!!","metadata":{}}]}