{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:11.159018Z","iopub.execute_input":"2021-08-03T08:46:11.159718Z","iopub.status.idle":"2021-08-03T08:46:11.180867Z","shell.execute_reply.started":"2021-08-03T08:46:11.159625Z","shell.execute_reply":"2021-08-03T08:46:11.180123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Importing all the important libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\n\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:11.2699Z","iopub.execute_input":"2021-08-03T08:46:11.271966Z","iopub.status.idle":"2021-08-03T08:46:13.010322Z","shell.execute_reply.started":"2021-08-03T08:46:11.271927Z","shell.execute_reply":"2021-08-03T08:46:13.00935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As of now only analysing Twitter data, will do for Reddit data later.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/twitter-and-reddit-sentimental-analysis-dataset/Twitter_Data.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:24.776955Z","iopub.execute_input":"2021-08-03T08:46:24.777452Z","iopub.status.idle":"2021-08-03T08:46:25.599508Z","shell.execute_reply.started":"2021-08-03T08:46:24.77742Z","shell.execute_reply":"2021-08-03T08:46:25.598631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:27.942182Z","iopub.execute_input":"2021-08-03T08:46:27.942606Z","iopub.status.idle":"2021-08-03T08:46:27.950799Z","shell.execute_reply.started":"2021-08-03T08:46:27.942565Z","shell.execute_reply":"2021-08-03T08:46:27.949794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:28.516999Z","iopub.execute_input":"2021-08-03T08:46:28.5174Z","iopub.status.idle":"2021-08-03T08:46:28.542223Z","shell.execute_reply.started":"2021-08-03T08:46:28.517371Z","shell.execute_reply":"2021-08-03T08:46:28.541275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.category.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:29.647451Z","iopub.execute_input":"2021-08-03T08:46:29.647807Z","iopub.status.idle":"2021-08-03T08:46:29.661736Z","shell.execute_reply.started":"2021-08-03T08:46:29.647776Z","shell.execute_reply":"2021-08-03T08:46:29.660857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, there are 3 different values for category.\nLets encode these numeric value to categorical as folllows\n\n* -1 to Negative,\n* 0 to Neutral,\n* 1 to Positive.","metadata":{}},{"cell_type":"code","source":"df['category']=df['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:51.926953Z","iopub.execute_input":"2021-08-03T08:46:51.927516Z","iopub.status.idle":"2021-08-03T08:46:51.946966Z","shell.execute_reply.started":"2021-08-03T08:46:51.927468Z","shell.execute_reply":"2021-08-03T08:46:51.945974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename the description column name\ndf['Tweet'] = df['clean_text']","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:52.876986Z","iopub.execute_input":"2021-08-03T08:46:52.877483Z","iopub.status.idle":"2021-08-03T08:46:52.885514Z","shell.execute_reply.started":"2021-08-03T08:46:52.877441Z","shell.execute_reply":"2021-08-03T08:46:52.884288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('clean_text', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:54.896952Z","iopub.execute_input":"2021-08-03T08:46:54.897347Z","iopub.status.idle":"2021-08-03T08:46:54.933062Z","shell.execute_reply.started":"2021-08-03T08:46:54.897311Z","shell.execute_reply":"2021-08-03T08:46:54.932047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:56.401973Z","iopub.execute_input":"2021-08-03T08:46:56.402408Z","iopub.status.idle":"2021-08-03T08:46:56.412582Z","shell.execute_reply.started":"2021-08-03T08:46:56.402373Z","shell.execute_reply":"2021-08-03T08:46:56.411903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, Lets start the cleaning process.\n\nThe usual cleaning process in NLP involves:- \n\n* Remove missing value if any.\n* Remove unwanted character like punctuations.\n* Replace all the Uppercase to lowercase as machine treat them differently but we knw   meaning of 'cat' and 'CAT' is same.\n* Remove type of words that follow a certain pattern like link, email, or username, these words does not contribute much in analysis and can be removed from description with he help of regular expression.\n* Remove all the stopwords like pronoun, articles etc. these words occur in very huge number in any sentence but does not contribute much in NLP analysis and thus can be removed.\n* At last Changing the verb form to its root form.\nexample :- root word for 'Playing' and 'Played' will be 'Play'\n\n","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:46:59.657152Z","iopub.execute_input":"2021-08-03T08:46:59.657542Z","iopub.status.idle":"2021-08-03T08:46:59.700931Z","shell.execute_reply.started":"2021-08-03T08:46:59.65751Z","shell.execute_reply":"2021-08-03T08:46:59.700101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 7 records missing in category and 4 records missing in Tweet.\nLets remove these missing records from the dataset.","metadata":{}},{"cell_type":"code","source":"df = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:01.862303Z","iopub.execute_input":"2021-08-03T08:47:01.862681Z","iopub.status.idle":"2021-08-03T08:47:01.930296Z","shell.execute_reply.started":"2021-08-03T08:47:01.862648Z","shell.execute_reply":"2021-08-03T08:47:01.929375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:02.947397Z","iopub.execute_input":"2021-08-03T08:47:02.947743Z","iopub.status.idle":"2021-08-03T08:47:02.989749Z","shell.execute_reply.started":"2021-08-03T08:47:02.947712Z","shell.execute_reply":"2021-08-03T08:47:02.988952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punct = string.punctuation\npunct","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:04.32194Z","iopub.execute_input":"2021-08-03T08:47:04.32231Z","iopub.status.idle":"2021-08-03T08:47:04.327179Z","shell.execute_reply.started":"2021-08-03T08:47:04.32228Z","shell.execute_reply":"2021-08-03T08:47:04.326524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#alpha = [' ','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:05.661958Z","iopub.execute_input":"2021-08-03T08:47:05.664026Z","iopub.status.idle":"2021-08-03T08:47:05.667135Z","shell.execute_reply.started":"2021-08-03T08:47:05.663988Z","shell.execute_reply":"2021-08-03T08:47:05.666407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopWords = stopwords.words('english')\nstopWords","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:06.466877Z","iopub.execute_input":"2021-08-03T08:47:06.468872Z","iopub.status.idle":"2021-08-03T08:47:06.59306Z","shell.execute_reply.started":"2021-08-03T08:47:06.468836Z","shell.execute_reply":"2021-08-03T08:47:06.592205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  There are 2 ways to find the root word. \n1. Stemming  :- This is hardcoded alogirithm to remove suffix like 'ing', 's', 'es'..etc the resulting word may not be correct english word. This is a computationally faster than Lemmatizing.\n\n2. Lemmatizing :- This alogorithm look for synonyms for the word and find appropriate root word for the given word. This is bit slower than Stemming.","metadata":{}},{"cell_type":"code","source":"ps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:47:07.927093Z","iopub.execute_input":"2021-08-03T08:47:07.927705Z","iopub.status.idle":"2021-08-03T08:47:07.931158Z","shell.execute_reply.started":"2021-08-03T08:47:07.927653Z","shell.execute_reply":"2021-08-03T08:47:07.930425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know 'goose' and 'geese' denote samething one word is singular and other pularal. but stem and lemmatize treat them differently. See below example","metadata":{}},{"cell_type":"code","source":"print(ps.stem('geese'))\nprint(ps.stem('goose'))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:02:27.647869Z","iopub.execute_input":"2021-08-03T09:02:27.648212Z","iopub.status.idle":"2021-08-03T09:02:27.653372Z","shell.execute_reply.started":"2021-08-03T09:02:27.648183Z","shell.execute_reply":"2021-08-03T09:02:27.652407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(wn.lemmatize('geese'))\nprint(wn.lemmatize('goose'))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:01:50.799288Z","iopub.execute_input":"2021-08-03T09:01:50.79978Z","iopub.status.idle":"2021-08-03T09:01:50.804796Z","shell.execute_reply.started":"2021-08-03T09:01:50.799751Z","shell.execute_reply":"2021-08-03T09:01:50.804048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lemmatize method can be use, when we have smaller dataset, as it will not take much time. but if we have very large dataset the using Lemmatization could be time expensive, in that case prefer to use Stem method.","metadata":{}},{"cell_type":"markdown","source":"Now, lets write a function to clean the data. ","metadata":{}},{"cell_type":"code","source":"def cleanData(text):\n    \n    # To convert the all uppercase to lowercase\n    text = text.lower()\n    \n    # This is a reguglar expression to replace anything char that is not alphabet or numeric.\n    text = re.sub(r\"[^A-Za-z0-9]\",' ', text)\n    \n    # The above regular expression itself will take care of punctuation, below is an alternative to remove only punctuation.\n    text = ''.join([char for char in text if char not in punct])\n    \n    # This will remove the stopwords and lemmatize the remaining word to its root word.\n    text = [wn.lemmatize(word) for word in text.split(' ') if ((word not in stopWords) & len(word)!=0)]\n    \n    return ' '.join(text)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:07:01.279527Z","iopub.execute_input":"2021-08-03T09:07:01.279868Z","iopub.status.idle":"2021-08-03T09:07:01.28601Z","shell.execute_reply.started":"2021-08-03T09:07:01.27984Z","shell.execute_reply":"2021-08-03T09:07:01.285105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Tweet'] = df['Tweet'].apply(cleanData) ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:07:04.031725Z","iopub.execute_input":"2021-08-03T09:07:04.03206Z","iopub.status.idle":"2021-08-03T09:07:22.620556Z","shell.execute_reply.started":"2021-08-03T09:07:04.032031Z","shell.execute_reply":"2021-08-03T09:07:22.619745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:07:22.622045Z","iopub.execute_input":"2021-08-03T09:07:22.622471Z","iopub.status.idle":"2021-08-03T09:07:22.632846Z","shell.execute_reply.started":"2021-08-03T09:07:22.62243Z","shell.execute_reply":"2021-08-03T09:07:22.63188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Tweet'][0]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:07:22.634635Z","iopub.execute_input":"2021-08-03T09:07:22.635011Z","iopub.status.idle":"2021-08-03T09:07:22.65008Z","shell.execute_reply.started":"2021-08-03T09:07:22.634974Z","shell.execute_reply":"2021-08-03T09:07:22.649109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See, The Tweet is cleared.","metadata":{}},{"cell_type":"markdown","source":"Lets create a column with the word length of tweet and then analize it.","metadata":{}},{"cell_type":"code","source":"def find_len(txt):\n    return len(txt.split())","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:09:01.467831Z","iopub.execute_input":"2021-08-03T09:09:01.468186Z","iopub.status.idle":"2021-08-03T09:09:01.472302Z","shell.execute_reply.started":"2021-08-03T09:09:01.468135Z","shell.execute_reply":"2021-08-03T09:09:01.471317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Txt_len'] = [find_len(txt) for txt in df['Tweet']]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:09:02.546875Z","iopub.execute_input":"2021-08-03T09:09:02.547347Z","iopub.status.idle":"2021-08-03T09:09:02.734716Z","shell.execute_reply.started":"2021-08-03T09:09:02.547317Z","shell.execute_reply":"2021-08-03T09:09:02.734006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:09:04.847091Z","iopub.execute_input":"2021-08-03T09:09:04.847567Z","iopub.status.idle":"2021-08-03T09:09:04.856667Z","shell.execute_reply.started":"2021-08-03T09:09:04.847539Z","shell.execute_reply":"2021-08-03T09:09:04.85588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('category').count()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:09:15.88697Z","iopub.execute_input":"2021-08-03T09:09:15.887367Z","iopub.status.idle":"2021-08-03T09:09:15.938316Z","shell.execute_reply.started":"2021-08-03T09:09:15.887331Z","shell.execute_reply":"2021-08-03T09:09:15.937352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(x='category', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:09:49.757687Z","iopub.execute_input":"2021-08-03T09:09:49.758016Z","iopub.status.idle":"2021-08-03T09:09:50.09539Z","shell.execute_reply.started":"2021-08-03T09:09:49.75799Z","shell.execute_reply":"2021-08-03T09:09:50.094407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.boxplot(y='Txt_len', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:10:04.278017Z","iopub.execute_input":"2021-08-03T09:10:04.278514Z","iopub.status.idle":"2021-08-03T09:10:04.405134Z","shell.execute_reply.started":"2021-08-03T09:10:04.278474Z","shell.execute_reply":"2021-08-03T09:10:04.404319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can most of the tweets are of length between 0 and 15 words","metadata":{}},{"cell_type":"code","source":"ax = sns.histplot(x = 'Txt_len', data=df)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:30:01.324818Z","iopub.execute_input":"2021-08-03T09:30:01.325202Z","iopub.status.idle":"2021-08-03T09:30:01.791327Z","shell.execute_reply.started":"2021-08-03T09:30:01.325171Z","shell.execute_reply":"2021-08-03T09:30:01.790445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['Txt_len']>20].count()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T09:31:38.724429Z","iopub.execute_input":"2021-08-03T09:31:38.724751Z","iopub.status.idle":"2021-08-03T09:31:38.73581Z","shell.execute_reply.started":"2021-08-03T09:31:38.724722Z","shell.execute_reply":"2021-08-03T09:31:38.735043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are only 227 Tweets of length more than 20 words.","metadata":{}},{"cell_type":"code","source":"majority_tweet = df[df['Txt_len']<10]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:07:24.898612Z","iopub.execute_input":"2021-08-03T10:07:24.899111Z","iopub.status.idle":"2021-08-03T10:07:24.911711Z","shell.execute_reply.started":"2021-08-03T10:07:24.899066Z","shell.execute_reply":"2021-08-03T10:07:24.910881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(x = 'category', data = majority_tweet)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:07:25.318272Z","iopub.execute_input":"2021-08-03T10:07:25.318757Z","iopub.status.idle":"2021-08-03T10:07:25.589828Z","shell.execute_reply.started":"2021-08-03T10:07:25.318722Z","shell.execute_reply":"2021-08-03T10:07:25.5891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe Majority of te tweets are positive or neutral.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## These are basically the tweets which were in other language and  does not had english char in original tweet, and got cleared up while pre-proscessing\n\nZero_len_tweet = df[df['Txt_len']==0]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:10:28.33988Z","iopub.execute_input":"2021-08-03T10:10:28.340437Z","iopub.status.idle":"2021-08-03T10:10:28.347983Z","shell.execute_reply.started":"2021-08-03T10:10:28.34038Z","shell.execute_reply":"2021-08-03T10:10:28.347202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(x='category', data = Zero_len_tweet)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:10:29.018307Z","iopub.execute_input":"2021-08-03T10:10:29.018668Z","iopub.status.idle":"2021-08-03T10:10:29.160331Z","shell.execute_reply.started":"2021-08-03T10:10:29.018638Z","shell.execute_reply":"2021-08-03T10:10:29.159304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['category']=='Positive']['Tweet']","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:10:40.798413Z","iopub.execute_input":"2021-08-03T10:10:40.798761Z","iopub.status.idle":"2021-08-03T10:10:40.835864Z","shell.execute_reply.started":"2021-08-03T10:10:40.798731Z","shell.execute_reply":"2021-08-03T10:10:40.834899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets plot cloud plot for each category.","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:10:45.418303Z","iopub.execute_input":"2021-08-03T10:10:45.418612Z","iopub.status.idle":"2021-08-03T10:10:45.421795Z","shell.execute_reply.started":"2021-08-03T10:10:45.418586Z","shell.execute_reply":"2021-08-03T10:10:45.421195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pos_tweet = ' '.join([word for word in df[df['category']=='Positive']['Tweet']])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def printWordCloud(x,cat):\n    \n    '''\n    x : df\n    cat: Category\n    '''\n    \n    cat_tweet = ' '.join([word for word in x[x['category']==cat]['Tweet']])\n    \n    # Initialize wordcloud object\n    wc = WordCloud(background_color='white', max_words=50, stopwords = STOPWORDS)\n\n    # Generate and plot wordcloud\n    plt.figure(figsize=(20,10))\n    plt.imshow(wc.generate(cat_tweet))\n    plt.title('{} Sentiment Words'.format(cat), fontsize=20)\n    plt.axis('off')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:11:21.258457Z","iopub.execute_input":"2021-08-03T10:11:21.259007Z","iopub.status.idle":"2021-08-03T10:11:21.265313Z","shell.execute_reply.started":"2021-08-03T10:11:21.258974Z","shell.execute_reply":"2021-08-03T10:11:21.264465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now with the help of word cloud lets see which words are used more number of time in each categories.","metadata":{}},{"cell_type":"code","source":"printWordCloud(df,'Positive')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:12:56.228537Z","iopub.execute_input":"2021-08-03T10:12:56.228836Z","iopub.status.idle":"2021-08-03T10:13:01.298152Z","shell.execute_reply.started":"2021-08-03T10:12:56.228812Z","shell.execute_reply":"2021-08-03T10:13:01.297485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printWordCloud(df,'Negative')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:13:09.728295Z","iopub.execute_input":"2021-08-03T10:13:09.728832Z","iopub.status.idle":"2021-08-03T10:13:12.862031Z","shell.execute_reply.started":"2021-08-03T10:13:09.728788Z","shell.execute_reply":"2021-08-03T10:13:12.86115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printWordCloud(df,'Neutral')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:13:12.863411Z","iopub.execute_input":"2021-08-03T10:13:12.86371Z","iopub.status.idle":"2021-08-03T10:13:15.75633Z","shell.execute_reply.started":"2021-08-03T10:13:12.86368Z","shell.execute_reply":"2021-08-03T10:13:15.755113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:13:25.338243Z","iopub.execute_input":"2021-08-03T10:13:25.33855Z","iopub.status.idle":"2021-08-03T10:13:25.343894Z","shell.execute_reply.started":"2021-08-03T10:13:25.338524Z","shell.execute_reply":"2021-08-03T10:13:25.343023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, now we are almost done with Data cleaning and analysing, Now we need to convert the data into the format where Machine can read. i.e to convert the Tweet columns in numeric form. This is also called as Vectorization.\n\n# There are 3 ways to do that.\n1. Count vectorization\n2. N-gram\n3. Tfidf Vectorization. \n\nHere in this Notebook i will be using Tfidf (term frequency–inverse document frequency).","metadata":{}},{"cell_type":"markdown","source":"So, What this method does is, it create the columns for each word. and provide the wieght of each word used in a particular tweet (record.)","metadata":{}},{"cell_type":"code","source":"vector = TfidfVectorizer(sublinear_tf=True)\nX = vector.fit_transform(df['Tweet'].values)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:31:45.409057Z","iopub.execute_input":"2021-08-03T10:31:45.409425Z","iopub.status.idle":"2021-08-03T10:31:47.294258Z","shell.execute_reply.started":"2021-08-03T10:31:45.409397Z","shell.execute_reply":"2021-08-03T10:31:47.293292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vector.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:31:51.198187Z","iopub.execute_input":"2021-08-03T10:31:51.198508Z","iopub.status.idle":"2021-08-03T10:31:51.244507Z","shell.execute_reply.started":"2021-08-03T10:31:51.198481Z","shell.execute_reply":"2021-08-03T10:31:51.243623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, We can say there are total 53309 unique words are available in combining all tweets. and this number of columns have been created. and for each of these words the weight will be assigned for each tweet.","metadata":{}},{"cell_type":"code","source":"X.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:34:06.554449Z","iopub.execute_input":"2021-08-03T10:34:06.554807Z","iopub.status.idle":"2021-08-03T10:34:06.560457Z","shell.execute_reply.started":"2021-08-03T10:34:06.554775Z","shell.execute_reply":"2021-08-03T10:34:06.559491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_col = vector.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:34:08.033349Z","iopub.execute_input":"2021-08-03T10:34:08.033695Z","iopub.status.idle":"2021-08-03T10:34:08.080017Z","shell.execute_reply.started":"2021-08-03T10:34:08.033666Z","shell.execute_reply":"2021-08-03T10:34:08.078951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_col[:20]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:38:57.753709Z","iopub.execute_input":"2021-08-03T10:38:57.754084Z","iopub.status.idle":"2021-08-03T10:38:57.759797Z","shell.execute_reply.started":"2021-08-03T10:38:57.754051Z","shell.execute_reply":"2021-08-03T10:38:57.758833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:34:12.07336Z","iopub.execute_input":"2021-08-03T10:34:12.073734Z","iopub.status.idle":"2021-08-03T10:34:12.080193Z","shell.execute_reply.started":"2021-08-03T10:34:12.073701Z","shell.execute_reply":"2021-08-03T10:34:12.079331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import csr_matrix","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:34:19.394085Z","iopub.execute_input":"2021-08-03T10:34:19.394469Z","iopub.status.idle":"2021-08-03T10:34:19.398455Z","shell.execute_reply.started":"2021-08-03T10:34:19.394436Z","shell.execute_reply":"2021-08-03T10:34:19.397541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.DataFrame.sparse.from_spmatrix(X, columns = X_col)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:34:21.533251Z","iopub.execute_input":"2021-08-03T10:34:21.533613Z","iopub.status.idle":"2021-08-03T10:34:23.106154Z","shell.execute_reply.started":"2021-08-03T10:34:21.533581Z","shell.execute_reply":"2021-08-03T10:34:23.10519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:36:07.814694Z","iopub.execute_input":"2021-08-03T10:36:07.815176Z","iopub.status.idle":"2021-08-03T10:36:20.48611Z","shell.execute_reply.started":"2021-08-03T10:36:07.815132Z","shell.execute_reply":"2021-08-03T10:36:20.485078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Txt_len'] = df['Txt_len']","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:36:20.493361Z","iopub.execute_input":"2021-08-03T10:36:20.49368Z","iopub.status.idle":"2021-08-03T10:36:20.680788Z","shell.execute_reply.started":"2021-08-03T10:36:20.493652Z","shell.execute_reply":"2021-08-03T10:36:20.679737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:36:20.682236Z","iopub.execute_input":"2021-08-03T10:36:20.682532Z","iopub.status.idle":"2021-08-03T10:36:33.422989Z","shell.execute_reply.started":"2021-08-03T10:36:20.682503Z","shell.execute_reply":"2021-08-03T10:36:33.422097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:36:33.424389Z","iopub.execute_input":"2021-08-03T10:36:33.424764Z","iopub.status.idle":"2021-08-03T10:36:33.432374Z","shell.execute_reply.started":"2021-08-03T10:36:33.424724Z","shell.execute_reply":"2021-08-03T10:36:33.431511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yeah.....! That's it for now.","metadata":{}},{"cell_type":"markdown","source":"Any Feedback commnent will be very very appriciable...!","metadata":{}},{"cell_type":"markdown","source":"# Thank you..!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}