{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Assignment 12 - Use huggingface BERT and XLNet transformers on Tensorflow to carry out text classification\n\n## This notebook is laid out as follows:\n\n### **1)Loading and PreProcessing**\n\n### **2)Text Classification Using BERT**\n\n### **3)Text Classifcation Using XLNet**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Dependencies","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import DistilBertTokenizer, RobertaTokenizer\nfrom tensorflow.keras.optimizers import Adam, SGD\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/yelp-review-polarity/yelp_review_polarity_csv/train.csv')\ntest = pd.read_csv('../input/yelp-review-polarity/yelp_review_polarity_csv/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns = ['class','text']\ntest.columns = ['class','text']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting target values to (0,1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(train)):\n    if train['class'][i] == 2:\n        train['class'][i] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,len(test)):\n    if test['class'][i] == 2:\n        test['class'][i] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Classifcation Using BERT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(data,maximum_length) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data.text)):\n        encoded = tokenizer.encode_plus(\n        \n        data.text[i],\n        add_special_tokens=True,\n        max_length=maximum_length,\n        pad_to_max_length=True,\n        \n        return_attention_mask=True,\n        \n      )\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input_ids,train_attention_masks = bert_encode(train[:50000],60)\ntest_input_ids,test_attention_masks = bert_encode(test[:20000],60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BERT uses 2 inputs:\n\n1) input_ids : List of token ids to be fed to the model\n\n2) attention_masks: List of indices specifying which tokens should be attended to by the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(bert_model):\n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n    \n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    \n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(bert_model)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model.png', expand_nested=True, show_shapes=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy = train[:50000]\ntargets = dummy['class'].values\n\ndummy2 = test[:20000]\ntargets_y = dummy2['class'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([train_input_ids,train_attention_masks],targets,validation_data=([test_input_ids,test_attention_masks],targets_y), epochs=4,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing BERT Performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy Curves')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss Curves')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Classification XLNET","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFXLNetModel, XLNetTokenizer\nxlnet_model = 'xlnet-base-cased'\nxlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_xlnet(xlnet_model):\n    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')\n\n    \n    xlnet = TFXLNetModel.from_pretrained(xlnet_model)\n    xlnet_encodings = xlnet(word_inputs)[0]\n\n    # Collect last step from last hidden state (CLS)\n    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n    \n    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n     \n    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n\n    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlnet = create_model_xlnet(xlnet_model)\nxlnet.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(xlnet, to_file='model_xl.png', expand_nested=True, show_shapes=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_xl = xlnet.fit(train_input_ids,targets,validation_data=(test_input_ids,targets_y), epochs=4,batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing XLNet performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_xl.history['accuracy'])\nplt.plot(history_xl.history['val_accuracy'])\nplt.title('Accuracy Curves')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_xl.history['loss'])\nplt.plot(history_xl.history['val_loss'])\nplt.title('Loss Curves')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore the significance of self attention,positional encoding along with the encoders,decoders in the transformer architecture.\n\nThe goal of self-attention is to understand the\nrelationship and causality between the words in a sentence and find the\ninternal dependencies of the sentence using that information. It is\ncapable of learning distant dependencies within the\nsentence. \n\nHowever, the self attention layer does not distinguish the word order in a sequence hence we use a positional encoding layer to add sequential information into each sequence item.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}