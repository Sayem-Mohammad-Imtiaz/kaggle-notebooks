{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Word2Vec 알고리즘\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 경로의 경우 각자의 환경에 맞게 설정해주면 됩니다. \npath = '../input/t-academy-recommendation2/movies/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie = pd.read_csv(path + 'ratings.csv', low_memory=False)\nmovie.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie = movie.sort_values(by='timestamp', ascending=True).reset_index(drop=True)\nmovie.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 영화의 Metadata를 불러와서 movieID에 맞는 TITLE을 구해줍니다. \nmeta = pd.read_csv(path + 'movies_metadata.csv', low_memory=False)\nmeta.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = meta.rename(columns={'id':'movieId'})\nmovie['movieId'] = movie['movieId'].astype(str)\nmeta['movieId'] = meta['movieId'].astype(str)\n\nmovie = pd.merge(movie, meta[['movieId', 'original_title']], how='left', on='movieId')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"# UserId 별 MovidID 구매 목록을 생성 \nitem_agg = movie.groupby(['title'])['userId'].agg({'nunique'}).reset_index()\nitem_agg = item_agg[item_agg['nunique'] >= 5]['title'].values\n\nuser_agg = movie.groupby(['userId'])['title'].agg({'nunique'}).reset_index()\nuser_agg = user_agg[user_agg['nunique'] >= 5]['userId'].values"},{"metadata":{},"cell_type":"raw","source":"# 전처리 수행 \n# 성능향상을 위해 5회 미만 구매된 상품, 5회 미만 구매한 고객은 제외하고 분석진행\nmovie['check1'] = 0\nmovie.loc[movie['title'].isin(item_agg), 'check1'] = 1\n\nmovie['check2'] = 0\nmovie.loc[movie['userId'].isin(user_agg), 'check2'] = 1\n\nmovie = movie.loc[(movie['check1'] == 1) & (movie['check2'] == 1)].reset_index(drop=True)"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie = movie[movie['original_title'].notnull()].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agg = movie.groupby(['userId'])['original_title'].agg({'unique'})\nagg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie['original_title'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word2vec 적용"},{"metadata":{"trusted":true},"cell_type":"code","source":"# int형식은 Word2vec에서 학습이 안되어서 String으로 변경해줍니다. \nsentence = []\nfor user_sentence in agg['unique'].values:\n    sentence.append(list(map(str, user_sentence)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word2vec의 학습을 진행해줍니다. \nfrom gensim.models import Word2Vec\nembedding_model = Word2Vec(sentence, size=20, window = 5, \n                           min_count=1, workers=4, iter=200, sg=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_model.wv.most_similar(positive=['Spider-Man 2'], topn=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Doc2Vec 적용"},{"metadata":{},"cell_type":"markdown","source":"![](https://drive.google.com/uc?export=view&id=1g2ausKfoaAT0jMwSatRUG3fiGWfDuysV\n)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import doc2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = pd.read_csv(path + 'movies_metadata.csv', low_memory=False)\nmeta = meta[meta['original_title'].notnull()].reset_index(drop=True)\nmeta = meta[meta['overview'].notnull()].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport re \nstop_words = set(stopwords.words('english')) \n\noverview = []\nfor words in tqdm(meta['overview']):\n    word_tokens = word_tokenize(words)\n    sentence = re.sub('[^A-Za-z0-9]+', ' ', str(word_tokens))\n    sentence = sentence.strip()\n    \n    sentence_tokens = word_tokenize(sentence)\n    result = ''\n    for token in sentence_tokens: \n        if token not in stop_words:\n            result += ' ' + token \n    result = result.strip().lower()\n    overview.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta['pre_overview'] = overview","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_vectorizer = doc2vec.Doc2Vec(\n    dm=0,            # PV-DBOW / default 1\n    dbow_words=1,    # w2v simultaneous with DBOW d2v / default 0\n    window=10,        # distance between the predicted word and context words\n    size=100,        # vector size\n    alpha=0.025,     # learning-rate\n    seed=1234,\n    min_count=5,    # ignore with freq lower\n    min_alpha=0.025, # min learning-rate\n    workers=4,   # multi cpu\n    hs = 1,          # hierar chical softmax / default 0\n    negative = 10   # negative sampling / default 5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import namedtuple\n\nagg = meta[['id', 'original_title', 'pre_overview']]\nTaggedDocument = namedtuple('TaggedDocument', 'words tags')\ntagged_train_docs = [TaggedDocument((c), [d]) for d, c in agg[['original_title', 'pre_overview']].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_vectorizer.build_vocab(tagged_train_docs)\nprint(str(doc_vectorizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 벡터 문서 학습\nfrom time import time\n\nstart = time()\n\nfor epoch in tqdm(range(5)):\n    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\n    doc_vectorizer.alpha -= 0.002 # decrease the learning rate\n    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\n\n#doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\nend = time()\nprint(\"During Time: {}\".format(end-start))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_vectorizer.docvecs.most_similar('Toy Story', topn=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc_vectorizer.docvecs.most_similar('Harry Potter and the Deathly Hallows: Part 1', topn=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}