{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BANK CUSTOMER CHURN PREDICTION\n"},{"metadata":{},"cell_type":"markdown","source":"<img align=\"left\" width=\"500\" height=\"400\" src=\"https://drive.google.com/uc?export=view&id=1cndfDAb6JDdtMtxxSl6bIyZfDztJeTkS\">"},{"metadata":{},"cell_type":"markdown","source":"## Introduction."},{"metadata":{},"cell_type":"markdown","source":"### Customer churn refers to the phenomenon when a customer leaves a company or an organization,in our case a bank. Some studies shows that accquiring new coustomers can cost 5 times than that of satisfying and retaining existing customers. Thus tracking of bank customer churn rate through prediction will help in reducing marketing costs, lead to increase in capital ,expanding total customers and a lot more."},{"metadata":{},"cell_type":"markdown","source":"### In this project, we will be doing an Exploratory Data Analysis(EDA) and churn prediction through machine learning and deep learning techniques on the bank customers dataset which is taken from Kaggle."},{"metadata":{},"cell_type":"markdown","source":"## Overview of Notebook\n\n### 1. Load and Manipulate Data\n### 2. Exploratory Data Analysis¶\n### 3. Feature Engineering for the baseline model\n### 4. Data Preparation for the Model fitting\n### 5. Model fitting and selection\n### 6.Handling the problem of Imbalanced dataset\n### 7. Conclusion.\n\n### Check my github repo for more info- https://github.com/tanish265/Bank-Customers-Churn-Prediction"},{"metadata":{"id":"PF_ZOlQZyXIt","trusted":true},"cell_type":"code","source":"# importing libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\n# tf.test.gpu_device_name()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load and Manipulate Data"},{"metadata":{"id":"aQoIaSqpyXJD","outputId":"1828a856-3e30-4f39-a32d-0e692d8b4425","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"gxc7jabpyXJI","outputId":"a580a38a-802a-4d20-9651-2052e9533108","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"WmqGtwJZyXJK","outputId":"d2fbc3ee-a06d-4f91-f2d4-d9c27353e410","trusted":true},"cell_type":"code","source":"#  Checking missing values in dataset\ndf.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"N_4kZ8kwyXJM","outputId":"0732b139-d486-4281-dce7-726bd70844e7","trusted":true},"cell_type":"code","source":"# Checking unique values in a column to categorize into continuous and categorical columns.\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{"id":"XiGYEEpQyXJO","trusted":true},"cell_type":"code","source":"# Dropping columns which are not necessary for prediction\ndf = df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"9qYAevdyyXJP","outputId":"0b764b57-2dd4-4434-edaf-fd7fa608895b","trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"fLP8jPV1yXJR","outputId":"102478d8-12a9-42db-a761-d7461e097302","trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Exploratory Data Analysis¶"},{"metadata":{"id":"kKslNthhyXJS","outputId":"76e41743-f0b8-4052-f10e-38ecd170b204","trusted":true},"cell_type":"code","source":"labels = 'Exited', 'Retained'\nsizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots(figsize=(9, 7))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')\nplt.title(\"Proportion of customer churned and retained\", size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above pie chart,we can see that around 20% of customers had churned i.e exited and 80% retained.This shows that our dataset is a little imbalanced so we have to predict customer churn with a good accuracy as this 20% customers are of more interest to the bank. "},{"metadata":{},"cell_type":"markdown","source":"### Now visualizing countplots for categorical columns."},{"metadata":{"id":"zCA_idbnyXJU","outputId":"2ddb9b9e-fd7f-493a-cdd3-87f13eb1b30a","scrolled":true,"trusted":true},"cell_type":"code","source":"sns.countplot(x='Geography', hue = 'Exited',data = df).set_title('Countplot-Geography Column')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"9x6j33f8yXJV","outputId":"860da537-b725-4b9b-c85c-46672640f748","trusted":true},"cell_type":"code","source":"sns.countplot(x='Gender', hue = 'Exited',data = df).set_title('Countplot-Gender Column')","execution_count":null,"outputs":[]},{"metadata":{"id":"RZbzeVtlyXJW","outputId":"9fa76f34-73d9-4002-a7ba-40186d1ac1d4","trusted":true},"cell_type":"code","source":"sns.countplot(x='HasCrCard', hue = 'Exited',data = df).set_title('Countplot-HasCreditCard Column')","execution_count":null,"outputs":[]},{"metadata":{"id":"KlSFo7mRyXJX","outputId":"2d787ba1-bf57-4460-c755-2f81cb504eac","trusted":true},"cell_type":"code","source":"sns.countplot(x='IsActiveMember', hue = 'Exited',data = df).set_title('Countplot-IsActiveMember Column')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above countplots we can infer that-\n\n#### 1.Total umber of customers who retained is highest from France and those who exited are highest from Germany,which means the bank needs to focus more on customers from Germany followed by France so that they don't churn.\n#### 2. The proportion of female customers churning is greater than that of male customers.\n#### 3. Suprisingly,coustomers who had credit card churned more which can be a coincidence.\n#### 4. As usual,the inactive members churned more. "},{"metadata":{"id":"ReifB7siyXJX","outputId":"72913bcd-1dab-4c48-a204-1527cf3e32d2","trusted":true},"cell_type":"code","source":" # Relations based on the continuous data attributes\nfig, axarr = plt.subplots(3, 2, figsize=(20, 12))\nsns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = df, ax=axarr[0][0]).set_title('Boxplot- Credit Score Column')\nsns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = df , ax=axarr[0][1]).set_title('Boxplot- Age Column')\nsns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][0])\nsns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = df, ax=axarr[1][1])\nsns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][0])\nsns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = df, ax=axarr[2][1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above boxplots we can infer that-\n\n#### -- There is no significant difference in Credit score,estimated salary and number of products they possess  between customers who churned and who don't.\n#### -- The older customers are churning more than the young ones which indicates that the bank need to focus on older customers more.\n#### -- Customers with tenure period with bank either too less or too more tends to churn more.\n#### -- Customers who churned generally have more bank balance which is a bad indications as it will lead to capital deficiency in the bank."},{"metadata":{"id":"hSoF4g_EyXJY"},"cell_type":"markdown","source":"## 3. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### We would like to add features that are likely to have an impact on the probability of churning."},{"metadata":{"id":"Tesoz5L0yXJZ","outputId":"78e713a5-ea18-4e50-b486-0e676f683a41","trusted":true},"cell_type":"code","source":"# 1st Attribute - Balance Salary Ratio\ndf['BalanceSalaryRatio'] = df.Balance/df.EstimatedSalary\nsns.boxplot(y='BalanceSalaryRatio',x = 'Exited', hue = 'Exited',data = df)\nplt.ylim(-1, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clearly we can see that customers with high BalanceSalaryRatio is churning more,which balance or salary feature didn't showed up."},{"metadata":{"id":"wcy0krxayXJa","outputId":"4b297d6a-dc6f-4fad-dd20-526939f37b98","trusted":true},"cell_type":"code","source":"#  2nd Attribute-Tenure By Age\ndf['TenureByAge'] = df.Tenure/(df.Age)\nsns.boxplot(y='TenureByAge',x = 'Exited', hue = 'Exited',data = df)\nplt.ylim(-0.2, 0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"wr9EAwnbyXJc","outputId":"3052f2f6-963f-4685-846e-25753d38af0c","trusted":true},"cell_type":"code","source":"# 3rd Attribute- Credit Score Given Age\ndf['CreditScoreGivenAge'] = df.CreditScore/(df.Age)\nsns.boxplot(y='CreditScoreGivenAge',x = 'Exited', hue = 'Exited',data = df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"5ayTT4nYyXJd","outputId":"ad6bd2a1-a08f-462e-e0e8-847227fd363d","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Preparation for the Model fitting"},{"metadata":{"id":"qJ7n9-ofyXJe","outputId":"29bbc6f4-07e8-417b-e002-919f7e5d1916","trusted":true},"cell_type":"code","source":"# Arranging columns by data type for easier manipulation\n\ncontinuous_vars = ['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n                   'TenureByAge','CreditScoreGivenAge']\ncategorical_vars = ['HasCrCard', 'IsActiveMember','Geography', 'Gender']\ndf = df[['Exited'] + continuous_vars + categorical_vars]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correlation Matrix for continuous attributes"},{"metadata":{"id":"t7BfY4R5E0_s","outputId":"6f51385a-7bbe-4de4-cf4b-21931acd9eaa","trusted":true},"cell_type":"code","source":"sns.set()\nsns.set(font_scale = 1.25)\nsns.heatmap(df[continuous_vars].corr(), annot = True,fmt = \".1f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see from the correlation matrix that only the columns which we have created have some significant correlation with columns they are made from."},{"metadata":{"id":"_9O4yI1jyXJf","outputId":"8996d9e1-c82a-4c5a-bbc2-9c0885d1be96","trusted":true},"cell_type":"code","source":"# Changing values of column HasCrCard and IsActiveMember from 0 to -1 so that they will influence negatively to the model instead of no effect.\ndf.loc[df.HasCrCard == 0, 'HasCrCard'] = -1\ndf.loc[df.IsActiveMember == 0, 'IsActiveMember'] = -1\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One-hot encoding categorical columns"},{"metadata":{"id":"y0b46vgGyXJh","outputId":"07dbf99f-8cfa-407e-d8b8-3fbbc131ae73","trusted":true},"cell_type":"code","source":"df['Gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"QQvaJMxDyXJi","outputId":"f3878bcc-1c30-4b62-9148-f6aa71298c8c","trusted":true},"cell_type":"code","source":"df['Geography'].unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"VwyVsE2dyXJj","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \n  \nle = LabelEncoder() \n  \ndf['Gender']= le.fit_transform(df['Gender']) \ndf['Geography']= le.fit_transform(df['Geography']) \n\n# Gender 0-Female,1-Male\n# Geography 0-France,1-Germany,2-Spain","execution_count":null,"outputs":[]},{"metadata":{"id":"5q998xRbyXJk","outputId":"8aee78cb-4b92-40f6-e68a-bde8125fa477","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"3MDFINSHyXJl","outputId":"bec38dcb-1416-4a09-862a-761d65d44068","trusted":true},"cell_type":"code","source":"df1 = pd.get_dummies(data=df, columns=['Gender','Geography'])\ndf1.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"wUZL19jFyXJm","outputId":"7625222d-34fc-4b33-e107-8f7f65969d02","trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"4JxrW_1uyXJm","outputId":"bab4662e-ba12-47a9-f302-2f7e177d1554","trusted":true},"cell_type":"code","source":"continuous_vars","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling the continuous attributes using MinMaxScaler"},{"metadata":{"id":"rRDxnFZ_yXJn","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf1[continuous_vars] = scaler.fit_transform(df1[continuous_vars])","execution_count":null,"outputs":[]},{"metadata":{"id":"FKgIsJxvyXJo","outputId":"56cdb5e3-862f-411d-90b3-c707941aeb54","trusted":true},"cell_type":"code","source":"for col in df1:\n    print(f'{col}: {df1[col].unique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Model fitting and selection\n"},{"metadata":{},"cell_type":"markdown","source":"### For Model fitting, we will try a couple of different machine learning algorithms in order to get an idea about which machine learning algorithm performs better.Since this is a classification problem,we will try the following algorithms :\n### 1. Logistic Regression\n### 2. Logistic Regression with degree 2 polynomial kernel\n### 3.SVM with Rbf kernel and poly kernel\n### 4. Random Forest Classifier\n### 5. Extreme Gradient Boosting Classifier\n"},{"metadata":{},"cell_type":"markdown","source":"## We will also use deep learning  after these techniques."},{"metadata":{"id":"FLVv_RS3yXJp","trusted":true},"cell_type":"code","source":"# Support functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform\n\n# Fit models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# Scoring functions\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"kIWmotRAyXJz","outputId":"842ebe8b-d9d4-4961-c03c-26d6db8f5a9d","trusted":true},"cell_type":"code","source":"df1.head()\ndf1.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"mp2MfH32yXJ1","trusted":true},"cell_type":"code","source":"X = df1.drop('Exited',axis='columns')\ny = df1['Exited']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"id":"kww0BhpuyXJ1","outputId":"0eee12e2-02af-4c20-aca2-d9e1fea1ec23","trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Figuring out the importance of features in our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform training on the Random Forest model and generate the importance of the features\n\nfeatures_label = X_train.columns\nforest = RandomForestClassifier (n_estimators = 1000, random_state = 0, n_jobs = -1)\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X_train.shape[1]), importances[indices], color = \"green\", align = \"center\")\nplt.xticks(range(X_train.shape[1]), features_label, rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"FOMtvlA8zQ0m","trusted":true},"cell_type":"code","source":"# Function to give best model score and parameters\ndef best_model(model):\n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitted different models to GridSearchCV to find out the best parameters."},{"metadata":{"id":"zkBUS8-t3qxB"},"cell_type":"markdown","source":"### Fitting our training dataset with the model with best parameters got from GridSearchCV for each of the machine learning techniques."},{"metadata":{"id":"GD64UYAp3q-k","outputId":"1a57c58c-9e05-48b5-a807-724daa2fb7bc","trusted":true},"cell_type":"code","source":"# Fit primal logistic regression\nlog_primal = LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=250, multi_class='auto',n_jobs=None, \n                                penalty='l2', random_state=None, solver='lbfgs',tol=1e-05, verbose=0, warm_start=False)\nlog_primal.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"DnF0zCAN67kn","outputId":"8b85cc1e-9aff-40dc-f278-3d2eb2e00fdd","trusted":true},"cell_type":"code","source":"# Fit logistic regression with pol 2 kernel\npoly2 = PolynomialFeatures(degree=2)\ndf_train_pol2 = poly2.fit_transform(X_train)\nlog_pol2 = LogisticRegression(C=50, class_weight=None, dual=False, fit_intercept=True,intercept_scaling=1, max_iter=300, multi_class='auto', n_jobs=None, \n                              penalty='l2', random_state=None, solver='liblinear',tol=0.0001, verbose=0, warm_start=False)\nlog_pol2.fit(df_train_pol2,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"NmBo9rNQ7UMT","outputId":"8704c191-fd12-4153-8a4a-ed9ca800596d","trusted":true},"cell_type":"code","source":"# Fit SVM with RBF Kernel\nSVM_RBF = SVC(C=150, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf', max_iter=-1, probability=True, \n              random_state=None, shrinking=True,tol=0.001, verbose=False)\nSVM_RBF.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"K13435hu7av0","outputId":"71bf1c10-6bc6-4c5a-8dd2-aa9c9a24bbfd","trusted":true},"cell_type":"code","source":"# Fit SVM with Pol Kernel\nSVM_POL = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,  decision_function_shape='ovr', degree=2, gamma=0.1, kernel='poly',  max_iter=-1,\n              probability=True, random_state=None, shrinking=True, tol=0.001, verbose=False)\nSVM_POL.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"Vn_k5CB_78nH","outputId":"f6746dc0-b275-48c4-ea9e-0e8578cd8f3c","trusted":true},"cell_type":"code","source":"# Fit Random Forest classifier\nRF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',max_depth=8, max_features=7, max_leaf_nodes=None,min_impurity_decrease=0.0,\n                            min_impurity_split=None,min_samples_leaf=1, min_samples_split=3,min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n                            oob_score=False, random_state=None, verbose=0,warm_start=False)\nRF.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"br58tZ848R1n","outputId":"0282f6d1-4f9c-4e40-a61c-627288baaff3","trusted":true},"cell_type":"code","source":"# Fit Extreme Gradient Boost Classifier\nXGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=5,\n                    min_child_weight=1, missing=None, n_estimators=100,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None,  subsample=1)\nXGB.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"-Rm6yZ7l8gWH"},"cell_type":"markdown","source":"### Reviewing best model fit accuracy. Our keen interest is on the performance in predicting 1's (Customers who churn)"},{"metadata":{"id":"rYAQ41f98ghs","outputId":"e62fa586-3594-4a2d-b0e5-3ac9abf7e46a","trusted":true},"cell_type":"code","source":"# Normal logistic regression\nprint(classification_report(y_train, log_primal.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"id":"8WP7Mux78nCw","outputId":"dd311bda-d8f3-45bd-a790-fafcc1a0dce9","trusted":true},"cell_type":"code","source":"# Logistic Regression with degree 2 polynomial kernel\nprint(classification_report(y_train,  log_pol2.predict(df_train_pol2)))","execution_count":null,"outputs":[]},{"metadata":{"id":"izzp2gki_133","outputId":"a02829a1-ad30-4be0-dcf8-6a05c50d6a49","trusted":true},"cell_type":"code","source":"# SVM with RBF kernel\nprint(classification_report(y_train,  SVM_RBF.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"id":"fFGUe0Gh_-lP","outputId":"c04d5ae6-3b3b-494d-d1a1-c26ed9bf1441","trusted":true},"cell_type":"code","source":"# SVM with polynomial kernel\nprint(classification_report(y_train,  SVM_POL.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classifier\nprint(classification_report(y_train,  RF.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Xtreme Gradient Boosting\nprint(classification_report(y_train,  XGB.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clearly XG Boost is giving the best training data acuracy of 89% for our dataset."},{"metadata":{},"cell_type":"markdown","source":"### Checking accuracy for test data with XG Boost Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,  XGB.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final accuracy for the test data is coming to be 86 % which is quite good but as we have seen that our dataset is a little imbalanced thatswhy our accuracy for customers who had exited is coming low.\n"},{"metadata":{},"cell_type":"markdown","source":"## Using Artificial Neural Network technique"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting model with 2 hidden layers along with appling dropout regularization.Final accuracy for training data is coming to be 85.28 %"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the model\nmodel = tf.keras.Sequential()\n\nfrom keras.layers import Dropout\n\n# first hidden layer\nmodel.add(Dense(8,activation = 'relu', input_dim = 16))\nmodel.add(Dropout(0.1))\n\n# second hidden layer\nmodel.add(Dense( 8, activation = 'relu'))\nmodel.add(Dropout(0.1))\n\n# output layer\nmodel.add(Dense( 1,activation = 'sigmoid'))\n\n# Compiling the NN\n# binary_crossentropy loss function used when a binary output is expected\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n\nmodel.fit(X_train, y_train, batch_size = 10, epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting model with 2 hidden layers along without appling dropout regularization.Final accuracy for training data is coming to be 86.21 %,better than the previous one."},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating the model\nmodel = Sequential()\n\nfrom keras.layers import Dropout\n\n# first hidden layer\nmodel.add(Dense(8,activation = 'relu', input_dim = 16))\n\n# second hidden layer\nmodel.add(Dense( 8, activation = 'relu'))\n\n# output layer\nmodel.add(Dense( 1,activation = 'sigmoid'))\n\n# Compiling the NN\n# binary_crossentropy loss function used when a binary output is expected\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) \n\nmodel.fit(X_train, y_train, batch_size = 10, epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluating test data with this model and accuracy is coming to be 85.85 % which is almost similar to our Random Forest Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manually verifying some predictions\nyp = model.predict(X_test)\nyp[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = []\nfor element in yp:\n    if element > 0.5:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)\ny_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Report for this model is almost same as that of Random Forest Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , classification_report\n\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sn\ncm = tf.math.confusion_matrix(labels=y_test,predictions=y_pred)\n\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True, fmt='d')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.Handling the problem of Imbalanced dataset"},{"metadata":{},"cell_type":"markdown","source":"### Removing the imbalance of our dataset by SMOTE oversampling technique  "},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(sampling_strategy='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\ny_sm.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we have equal number of churned and retaining customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=15, stratify=y_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting with the XGB model generated using GridSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.2, max_delta_step=0,max_depth=7,\n                    min_child_weight=1, missing=None, n_estimators=100,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None,  subsample=1)\nXGB2.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training set accuracy is coming to be 97 % which is great in itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"a=XGB2\nprint(classification_report(y_train,  a.predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing set accuracy is coming to be 91 % which has increased from 86% which we got in from our previous XGB model ."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,  a.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.2, max_delta_step=0,max_depth=7,\n                    min_child_weight=1, missing=None, n_estimators=100,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, \n                    reg_lambda=1, scale_pos_weight=1, seed=None,  subsample=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib \n  \n# Save the model as a pickle in a file \njoblib.dump(XGB2, 'churnXGB.pkl') \n  \n# Load the model from the file \n# XGB_from_joblib = joblib.load('churnXGB.pkl')  \n  \n# Use the loaded model to make predictions \n# XGB_from_joblib.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7.Conclusion"},{"metadata":{},"cell_type":"markdown","source":"### We can see that by balancing the dataset has increased our overall testing data accuracy to 91% , also it has invidually increased the accuracy for the customers who had churned (57% previously to 91% now) from the bank which matters to us more than the customers who retained."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}