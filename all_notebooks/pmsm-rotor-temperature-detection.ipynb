{"cells":[{"metadata":{},"cell_type":"markdown","source":"##### Please upvote if you like the work!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Cells with requirement of high computational power (kfold cv) have been commented but the results have been displayed in the following cells.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Permanent Magnet Synchronous Motor\n\n![alt text](https://alliedmarketresearch.files.wordpress.com/2017/02/permanent-magnet-synchronous-motor-pmsm.png?w=705)\n\nThe permanent-magnet synchronous machine (PMSM) drive is one of best choices for a full range of motion control applications. For example, the PMSM is widely used in robotics, machine tools, actuators, and it is being considered in high-power applications such as industrial drives and vehicular propulsion. It is also used for residential/commercial applications. The PMSM is known for having low torque ripple, superior dynamic performance, high efficiency and high power density.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/electric-motor-temperature/pmsm_temperature_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = df[(df['profile_id'] == 65) | (df['profile_id'] == 72)]\ndf = df[(df['profile_id'] != 65) & (df['profile_id'] != 72)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the dataset.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,6))\ndf['profile_id'].value_counts().sort_values().plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, session ids 66, 6 and 20 have the most number of measurements recorded.","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"for i in df.columns:\n    sns.distplot(df[i],color='g')\n    sns.boxplot(df[i],color = 'y')\n    plt.vlines(df[i].mean(),ymin = -1,ymax = 1,color = 'r')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the the above plots, the mean and median for most of the plots are very close to each other. So the data seems to have low skewness for almost all variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking skewness and kurtosis numerically","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"import scipy.stats as stats\nfor i in df.columns:\n    print(i,' :\\nSkew : ',df[i].skew(),' : \\nKurtosis : ',df[i].kurt())\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it is not highly skewed data and looking at the values of the dataset it seems there already has been some normalization done.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heatmap above, we can see that torque and q component of current are almost perfectly correlated. Also there seems to be a very high correlation between temperature measurements of stator yoke, stator tooth and stator windings.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For a random measurement, we can try to compare the temperatures of the 3 stator components.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\ndf[df['profile_id'] == 20]['stator_yoke'].plot(label = 'stator yoke')\ndf[df['profile_id'] == 20]['stator_tooth'].plot(label = 'stator tooth')\ndf[df['profile_id'] == 20]['stator_winding'].plot(label = 'stator winding')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the plot, all three stator components follow a similar measurment variance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As the dataset author mentioned, the records in the same profile id have been sorted by time, we can assume that these recordings have been arranged in series of time.\n\nDue to this we can infer that there has not been much time given for the motor to cool down in between recording the sensor data as we can see that initially the stator yoke temperature is low as compared to temperature of stator winding but as we progress in time, the stator yoke temperature goes above the temperature of stator winding.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As profile_id is an id for each measurement session, we can remove it from any furthur analysis and model building.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.drop('profile_id',axis = 1,inplace=True)\ndf_test.drop('profile_id',axis = 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Statistical Analysis of Variables\nWe'll see which particular variables contribute to the rotor temperature individually by checking their statistical significance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Ambient Temperature","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['ambient'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(df['ambient'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shapiro(df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0 : variance_ambient = variance_pm\n\nH1 : variance_ambient != variance_pm","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import bartlett\nbartlett(df['ambient'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for ambient temperature is not equal to the variance of rotor temperature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Coolant Temperature","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['coolant'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(df['coolant'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shapiro(df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0 : variance_coolant = variance_pm\n\nH1 : variance_coolant != variance_pm","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import bartlett\nbartlett(df['coolant'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for coolant temperature is not equal to the variance of rotor temperature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Voltage d-component","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['u_d'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(df['u_d'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shapiro(df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0 : variance_u_d = variance_pm\n\nH1 : variance_u_d != variance_pm","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import bartlett\nbartlett(df['u_d'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for voltage d-component is not equal to the variance of rotor temperature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Voltage q-component","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['u_q'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(df['u_q'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shapiro(df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0 : variance_u_q = variance_pm\n\nH1 : variance_u_q != variance_pm","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import bartlett\nbartlett(df['u_q'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for voltage q-component is not equal to the variance of rotor temperature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Motor speed","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['motor_speed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(df['motor_speed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shapiro(df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0 : variance_motor_speed = variance_pm\n\nH1 : variance_motor_speed != variance_pm","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import bartlett\nbartlett(df['motor_speed'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance of motor speed is not equal to the variance of rotor temperature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Current d-component","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['i_d'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(df['i_d'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shapiro(df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0 : variance_i_d = variance_pm\n\nH1 : variance_i_d != variance_pm","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import bartlett\nbartlett(df['i_d'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue is higher than 0.05. So we fail to reject the null hypothesis and can say that we do not have enough evidence to reject the null hypothesis. So we do not have enough evidence to prove that variance of d component of current is not equal to the variance of motor temperature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Current q-component","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.distplot(df['i_q'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\nshapiro(df['i_q'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"shapiro(df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"H0 : variance_i_q = variance_pm\n\nH1 : variance_i_q != variance_pm","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from scipy.stats import bartlett\nbartlett(df['i_q'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pvalue is higher than 0.05. So we fail to reject the null hypothesis and can say that we do not have enough evidence to reject the null hypothesis. So we do not have enough evidence to prove that variance of q component of current is not equal to the variance of motor temperature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Shuffling the data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.sample(frac=1,random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data description did not provide us with any information on the units of measure. So its difficult to interpret the values measured.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(df['ambient'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(df['coolant'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(df['motor_speed'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(df['u_q'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(df['u_d'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(df['i_q'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(df['i_d'],df['pm'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic multivariate regression (Base Model)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As we want to predict the temperatures of stator components and rotor(pm), we will drop these values from our dataset for regression. Also, torque is a quantity, which is not reliably measurable in field applications, so this feature shall be omitted in this modelling.","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nX = df.drop(['pm','stator_yoke','stator_tooth','stator_winding','torque'],axis = 1)\nX_df_test = df_test.drop(['pm','stator_yoke','stator_tooth','stator_winding','torque'],axis = 1)\nmm = MinMaxScaler()\nX = mm.fit_transform(X)\nX_df_test = mm.fit_transform(X_df_test)\ny = df['pm']\ny_df_test = df_test['pm']\nX = pd.DataFrame(X,columns = ['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d','i_q'])\nX_df_test = pd.DataFrame(X_df_test,columns = ['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d','i_q'])\ny.reset_index(drop = True,inplace = True)\ny_df_test.reset_index(drop = True,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in X.columns:\n    print(X[i].skew())\n    sns.distplot(X[i],color='g')\n    sns.boxplot(X[i],color = 'y')\n    plt.vlines(X[i].mean(),ymin = -1,ymax = 1,color = 'r')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.stats.diagnostic import linear_rainbow\nlinear_rainbow(lin_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.stats.api import het_goldfeldquandt\nhet_goldfeldquandt(lin_reg.resid,lin_reg.model.exog)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(X_train_const.values,i) for i in range(X_train_const.shape[1])]\npd.DataFrame(vif,index=X_train_const.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations :\n1. Looking at the pvalues of the each feature, all the them seems to be significant is predicting the stator winding temperature as pvalues are very low.\n2. The Durbin watson test score also is very close to 2, so we can say there seems to be very low autocorrelation in the dataset.\n3. The pvalue for Jarque-Bera test is less that 0.05, so we reject the null hypothesis that the residuals are normally distributed. We will also check for distribution of residuals as well as QQ-plot to check visually.\n4. The pvalue for rainbow test is greater than 0.05, so we fail to reject the null hypothesis and can say that the data follows linearity.\n5. The pvalue for Goldfeld Quantile distribution test is greater than 0.05, so we fail to reject the null hypothesis and can say that the data is homoskedastic in nature.\n6. But we can also see that there are high vif value for motor_speed. So we can say that there seems to be some multicollinearity in our model.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"lin_reg.resid.plot(kind = 'density')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import scipy.stats as stats\nimport pylab\nst_residual = lin_reg.get_influence().resid_studentized_internal\nstats.probplot(st_residual, dist=\"norm\", plot = pylab)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the from the QQ plot as well as kde plot that the residuals are quiet well normally distributed around the centre but deviate from normal distribution towards the extremes which might be the factor influencing JB test to fail the normality test.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))/len(y_train))\ntrain_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))/len(y_test))\ntest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lin_reg.rsquared_adj","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transforming skewed data and capping outliers","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X_trans = X\nX_trans['coolant'] = np.power(X_trans['coolant'],1/3)\nX_trans['ambient'] = np.power(X_trans['ambient'],3)\nX_trans['i_d'] = np.power(X_trans['i_d'],3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for i in X_trans.columns:\n    print(X_trans[i].skew())\n    sns.distplot(X_trans[i],color='g')\n    sns.boxplot(X_trans[i],color = 'y')\n    plt.vlines(X_trans[i].mean(),ymin = -1,ymax = 1,color = 'r')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"z = np.abs(stats.zscore(X_trans))\nprint(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_trans = X_trans.drop(np.where(z > 3)[0][0:])\nX_trans.reset_index(drop=True,inplace = True)\ny = y.drop(np.where(z > 3)[0][0:])\ny.reset_index(drop = True,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(X_trans.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.3, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))/len(y_train))\ntrain_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))/len(y_test))\ntest_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no imporvement in our rmse by transforming the data. So we will not go ahead with the transformation.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X = X_trans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking care of multicollinearity using PCA","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca  = PCA()\npca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.cumsum(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, 96 percent of the variance in data is explained by the first 5 principal components. So we'll choose these 5 components and see if there is any improvement in the Linear model.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"pca5 = PCA(n_components=5)\nX_pca = pca5.fit_transform(X)\nX_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_pca_train_const = sm.add_constant(X_pca_train)\nlin_reg = sm.OLS(y_train,X_pca_train_const).fit()\nlin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred = lin_reg.predict(X_pca_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))/len(y_train))\ntrain_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_pca_test_const = sm.add_constant(X_pca_test)\ny_test_pred = lin_reg.predict(X_pca_test_const)\ny_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))/len(y_test))\ntest_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no imporvement in our rmse by using PCA. So we will not go ahead with the PCA transformation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dropping the d and q components of current(i) looking at the statistical analysis","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"X_wo_dqi = X.drop(['i_d','i_q'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_wo_dqi, y, test_size=0.3, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))/len(y_train))\ntrain_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))/len(y_test))\ntest_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no imporvement in our rmse by using elimination d and q components of current. So we will not go ahead with the elimination.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Dropping the motor speed looking at the vif values","execution_count":null},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"X_wo_ms = X.drop(['motor_speed'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_wo_ms, y, test_size=0.3, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))/len(y_train))\ntrain_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))/len(y_test))\ntest_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no imporvement in our rmse by using elimination motor speed feature. So we will not go ahead with the elimination.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Giving a range estimate rather than giving a point estimate is always a more believable strategy. This can be achieved by using k-fold cross validation.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"y = pd.DataFrame(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n# lr = LinearRegression()\n# ridge = Ridge(alpha = 20000)\nlasso = Lasso(alpha = 0.012)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# from sklearn.model_selection import KFold\n# from sklearn import metrics\n# kf = KFold(n_splits=5,shuffle=True,random_state=0)\n# for model,name in zip([lr,ridge,lasso],['LR','Ridge','Lasso']):\n#     mse_li = []\n#     for train_idx,test_idx in kf.split(X,y):\n#         X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n#         y_train,y_test = y.iloc[train_idx,:],y.iloc[test_idx,:]\n#         model.fit(X_train,y_train)\n#         y_pred = model.predict(X_test)\n#         mse = metrics.mean_squared_error(y_test,y_pred)\n#         mse_li.append(mse)\n#     print('RMSE scores : %0.03f (+/- %0.08f) [%s]'%(np.mean(mse_li), np.var(mse_li,ddof = 1), name))\n#     print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE scores : 0.536 (+/- 0.00000489) [LR]\n\nRMSE scores : 0.631 (+/- 0.00000388) [Ridge]\n\nRMSE scores : 0.564 (+/- 0.00000385) [Lasso]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Non Parametric Models","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import randint\n# dt = DecisionTreeRegressor(random_state=0)\n# rf = RandomForestRegressor(random_state=0,n_jobs = -1)\n# param_dt = {\n#         'criterion' : ['mse','mae'],\n#         'max_depth' : randint(1,11)\n# }\n# param_rf = {\n#         'n_estimators' : randint(1,70),\n#         'max_depth' : randint(1,11)\n# }\n# rscv_dt = RandomizedSearchCV(dt,param_dt,scoring='neg_mean_squared_error',cv = 5,n_jobs=1,n_iter = 2,verbose = 1000,random_state = 0)\n# rscv_rf = RandomizedSearchCV(rf,param_rf,scoring='neg_mean_squared_error',cv = 5,n_jobs=-1,n_iter = 2,verbose = 1000,random_state = 0)\n# rscv_dt.fit(X,y)\n# rscv_rf.fit(X,y)\n# print(rscv_dt.best_params_)\n# print(rscv_rf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DT : criterion=mse, max_depth=6","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"RF : max_depth=6, n_estimators=41","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.neighbors import KNeighborsRegressor\n# dt = DecisionTreeRegressor(criterion='mse',max_depth=6,random_state=0)\n# rf = RandomForestRegressor(n_estimators=41,max_depth=6,random_state=0,n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# from sklearn.model_selection import KFold\n# from sklearn import metrics\n# kf = KFold(n_splits=5,shuffle=True,random_state=0)\n# for model,name in zip([dt,rf],['DT','RF']):\n#     mse_li = []\n#     for train_idx,test_idx in kf.split(X,y):\n#         X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n#         y_train,y_test = y.iloc[train_idx,:],y.iloc[test_idx,:]\n#         model.fit(X_train,y_train)\n#         y_pred = model.predict(X_test)\n#         mse = metrics.mean_squared_error(y_test,y_pred)\n#         mse_li.append(mse)\n#     print('RMSE scores : %0.03f (+/- %0.08f) [%s]'%(np.mean(mse_li), np.var(mse_li,ddof = 1), name))\n#     print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSE scores : 0.380 (+/- 0.00000606) [DT]\n\nRMSE scores : 0.374 (+/- 0.00000853) [RF]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Bagging Models\nFinding best number of estimators","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\n# from sklearn.model_selection import KFold, cross_val_score\n# models = []\n# models.append((\"LinearRegression\",lr))\n# models.append((\"Lasso\",lasso))\n# models.append((\"Ridge\",ridge))\n# models.append((\"DT\",dt))\n# for name,model in models:\n#     mse_var = []\n#     for val in np.arange(1,21):\n#         bg_model = BaggingRegressor(base_estimator=model,n_estimators=val,n_jobs=-1,verbose = 1000, random_state = 0)\n#         kfold = KFold(n_splits=5,shuffle=True,random_state=0)\n#         results = cross_val_score(bg_model,X,y,cv=kfold,n_jobs=-1,scoring='neg_mean_squared_error',verbose = 1000)\n#         mse_var.append(np.var(results,ddof = 1))\n#     print(name,np.argmin(mse_var)+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LinearRegression 12\n\nLasso 2\n\nRidge 2\n\nDT 3","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Boosting Models\nFinding best number of estimators","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.model_selection import KFold, cross_val_score\n# models = []\n# models.append((\"LinearRegression\",lr))\n# models.append((\"Lasso\",lasso))\n# models.append((\"Ridge\",ridge))\n# models.append((\"DT\",dt))\n# models.append((\"RF\",rf))\n# for name,model in models:\n#     mse_mean = []\n#     for val in np.arange(1,21):\n#         bg_model = AdaBoostRegressor(base_estimator=model,n_estimators=val, random_state = 0)\n#         kfold = KFold(n_splits=5,shuffle=True,random_state=0)\n#         results = cross_val_score(bg_model,X,y,cv=kfold,n_jobs=-1,scoring='neg_mean_squared_error',verbose = 1000)\n#         mse_mean.append(np.mean(results))\n#     print(name,np.argmax(mse_mean)+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LinearRegression 1\n\nLasso 10\n\nRidge 3\n\nDT 15\n\nRF 8","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# #Bagging Models\n# LR_bag = BaggingRegressor(base_estimator = lr,n_estimators = 12,random_state = 0,n_jobs = -1)\nlasso_bag = BaggingRegressor(base_estimator = lasso,n_estimators = 2,random_state = 0,n_jobs = -1)\n# DT_bag = BaggingRegressor(base_estimator = dt,n_estimators = 3,random_state = 0,n_jobs = -1,verbose = 1000)\n# ridge_bag = BaggingRegressor(base_estimator = ridge,n_estimators = 2,random_state = 0,n_jobs = -1) \n# # #Boosting models\n# lasso_boost = AdaBoostRegressor(base_estimator = lasso,n_estimators = 10,random_state = 0)\n# ridge_boost = AdaBoostRegressor(base_estimator = ridge,n_estimators = 3,random_state = 0)\n# DT_boost = AdaBoostRegressor(base_estimator = dt,n_estimators = 15,random_state = 0)\n# RF_boost = AdaBoostRegressor(base_estimator = rf,n_estimators = 8,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingRegressor\n# GBC = GradientBoostingRegressor(n_estimators = 100,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# models = []\n# models.append(('LR Bagged',LR_bag))\n# models.append(('Lasso Bagged',lasso_bag))\n# models.append(('Lasso Boosted',lasso_boost))\n# models.append(('Ridge Bagged',ridge_bag))\n# models.append(('Ridge Boosted',ridge_boost))\n# models.append(('DTree Bagged',DT_bag))\n# models.append(('DTree Boosted',DT_boost))\n# models.append(('Gradient Boost',GBC))\n# models.append(('RF Boosted',RF_boost))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# results = []\n# names = []\n# for name, model in models:\n#     kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n#     cv_results = cross_val_score(model,X,y,cv = kfold,scoring='neg_mean_squared_error',n_jobs = -1)\n#     results.append(cv_results)\n#     names.append(name)\n#     print(name,' : ',np.mean(cv_results),' -- ',np.var(cv_results,ddof = 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model : Bias Error -- Variance Error\n\nRMSE scores : 0.536 (+/- 0.00000489) [LR]\n\nRMSE scores : 0.631 (+/- 0.00000388) [Ridge]\n\nRMSE scores : 0.564 (+/- 0.00000385) [Lasso]\n\nRMSE scores : 0.380 (+/- 0.00000606) [DT]\n\nRMSE scores : 0.374 (+/- 0.00000853) [RF]\n\nLR Bagged  :  -0.5363121272813037  --  4.881295834654977e-06\n\nLasso Bagged  :  -0.5645736874272845  --  3.2947376969099687e-06\n\nLasso Boosted  :  -0.571080275441007  --  5.1543103985968674e-06\n\nRidge Bagged  :  -0.6309426367758058  --  3.2555007823844387e-06\n\nRidge Boosted  :  -0.6172788016072153  --  3.302273899220944e-06\n\nDTree Bagged  :  -0.37582901559820736  --  5.251632737569686e-06\n\nDTree Boosted  :  -0.3232454863288907  --  2.7692299178526762e-05\n\nGradient Boost  :  -0.3165118622627031  --  8.654229505655393e-06\n\nRF Boosted  :  -0.315700682698399  --  4.0892902046352836e-05\n\nAs we can see from the result, ridge bagged seems to gives the best result as far as the handling of variance error is concerned, but on the other hand, Gradient boost and RF boosted gives the best result as far as the handling of bias error is concerned. Overall if we see, Lasso bagged gives quite a reasonable and acceptable result as far as handling both bias and variance error is concerned. So, we will select Lasso Bagged as our final model.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import r2_score,mean_squared_error\nlasso_bag.fit(X,y)\ntest_pred = lasso_bag.predict(X_df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Please upvote if you like the work!!!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}