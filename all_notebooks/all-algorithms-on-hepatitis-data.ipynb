{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Import basic libraries \nimport os\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nfrom string import ascii_uppercase\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt        \n%matplotlib inline\n\nimport sklearn.preprocessing as skp\nimport sklearn.model_selection as skm\nimport os\n#import classification modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n# Selection\nfrom sklearn.model_selection import GridSearchCV as gs\nfrom sklearn.model_selection import RandomizedSearchCV as rs\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n#import decision tree plotting libraries\n# Metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix,precision_score, recall_score, roc_auc_score,roc_curve, auc, f1_score ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading Dataset\nmissing=[\"na\",\"--\",\".\",\"..\"]\ntd= pd.read_csv(\"/kaggle/input/hepatitis/hepatitis.csv\",na_values=missing)\ntd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.isnull().sum() # Checking for nulls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td[\"class\"].replace((1,2),(0,1),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td[\"class\"]=td[\"class\"].astype(\"bool\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Discretization of Age Column\ntd[\"age\"]=np.where((td[\"age\"]>10) & (td[\"age\"]<20),\"Teenagers\",\n                   np.where((td[\"age\"]>=20) & (td[\"age\"]<=30),\"Adults\",\n                   np.where((td[\"age\"]>30) & (td[\"age\"]<=40),\"Middle Aged\",np.where((td[\"age\"]<=10),\"Children\",\n                            \"Old\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td[\"age\"]=pd.Categorical(td.age,[\"Children\",'Teenagers','Adults', 'Middle Aged', 'Old'],ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td[\"age\"].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#draw a bar plot of Age vs. survival\nsns.barplot(x=\"age\", y=\"class\", data=td)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td[\"sex\"].replace((1,2),(\"Male\",\"Female\"),inplace=True)\ntd[\"sex\"]=pd.Categorical(td.sex,[\"Male\",'Female'],ordered=False)\ntd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.dropna(inplace=True) # Now dropping all nulls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have categorical variables .getdummies seperates the different categories of categorical variables as separate \n#binary columns\ntd1 = pd.get_dummies(td,drop_first=True)\n#List of new columns\nprint(td1.columns)\ntd1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td1[\"bilirubin\"]=np.abs((td1[\"bilirubin\"]-td1[\"bilirubin\"].mean())/(td1[\"bilirubin\"].std()))\ntd1[\"albumin\"]=np.abs((td1[\"albumin\"]-td1[\"albumin\"].mean())/(td1[\"albumin\"].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=td1[\"class\"].copy()\nX=td1.drop(columns=[\"class\"])\nprint(y.shape)\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering Using Random Forest Algorithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest method for feature selection\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()    \n#thit is how we get the feature importance with simple steps:\nX_features=X.columns\nmodel.fit(X, y)\n# display the relative importance of each attribute\nimportances = np.around(model.feature_importances_,decimals=4)\nimp_features= model.feature_importances_\nfeature_array=np.array(X_features)\nsorted_features=pd.DataFrame(list(zip(feature_array,imp_features))).sort_values(by=1,ascending=False)\n#print(sorted_features)\ndata_top=sorted_features[:X.shape[1]]\nfeature_to_rem=sorted_features[X.shape[1]:]\nprint(\"Unimportant Columms after simple Random Forrest\\n\",feature_to_rem)\nrem_index=list(feature_to_rem.index)\nprint(rem_index)\nprint(\"Important Columms after simple Random Forrest\\n\",data_top)\ndata_top_index=list(data_top.index)\nprint(\"Important Columms after simple Random Forrest\\n\",data_top_index )\nprint(importances)\n#0.0250 is a  selected threshold looking at the importance values this can be changed to any other value too\n#cols_randfor_removed=[index for index,value in enumerate(importances) if value <= 0.0250]\n#print(cols_randfor_removed)\nX_randfor_sel = X.drop(X.columns[rem_index],axis=1)\n#X_randfor_sel = X.drop(X.columns[cols_randfor_removed],axis=1)\nfeatures_randfor_select=X_randfor_sel.columns\nprint(features_randfor_select)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Train Test Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#creat train-test split parts for manual split\n\ntrainX, testX, trainy, testy= skm.train_test_split(X,y, test_size=0.25, random_state=99) #explain random state\nprint(\"\\n shape of train split: \")\nprint(trainX.shape, trainy.shape)\nprint(\"\\n shape of train split: \")\nprint(testX.shape, testy.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Making X Scalar for ML algorithms\nX = skp.StandardScaler().fit(X).transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# All Machine Learning Algorithms with Default Parameters"},{"metadata":{},"cell_type":"markdown","source":"## K Nearest Neighbor Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(trainX,trainy)\npredictions = knn.predict(testX)\naccknn=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of KNN (%): \\n\", accknn)  \n#get FPR\nfprknn, tprknn, _ = roc_curve(testy, predictions)\naucknn=auc(fprknn, tprknn)*100\nprint(\"AUC OF KNN (%): \\n\", aucknn)\nrecallknn=recall_score(testy,predictions)*100\nprint(\"Recall of KNN is: \\n\",recallknn)\nprecknn=precision_score(testy,predictions)*100\nprint(\"Precision of KNN is: \\n\",precknn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gaussian Naive Bayes Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb=GaussianNB()\ngnb.fit(trainX,trainy)\npredictions = gnb.predict(testX)\naccgnb=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Gaussian Naive Bayes (%): \\n\",accgnb)  \n#get FPR\nfprgnb, tprgnb, _ = roc_curve(testy, predictions)\naucgnb=auc(fprgnb, tprgnb)*100\nprint(\"AUC OF Gaussian Naive Bayes (%): \\n\", aucgnb)\nrecallgnb=recall_score(testy,predictions)*100\nprint(\"Recall of Gaussian Naive Bayes is: \\n\",recallgnb)\nprecgnb=precision_score(testy,predictions)*100\nprint(\"Precision of Gaussian Naive Bayes is: \\n\",precgnb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"lrg=LogisticRegression(solver='lbfgs')\nlrg.fit(trainX,trainy)\npredictions = lrg.predict(testX)\nacclrg=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Logistic regression (%): \\n\",acclrg)  \n#get FPR\nfprlrg, tprlrg, _ = roc_curve(testy, predictions)\nauclrg=auc(fprlrg, tprlrg)*100\nprint(\"AUC OF Logistic regression (%): \\n\", auclrg)\nrecalllrg=recall_score(testy,predictions)*100\nprint(\"Recall of Logistic regression is: \\n\",recalllrg)\npreclrg=precision_score(testy,predictions)*100\nprint(\"Precision of Logistic regression is: \\n\",preclrg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Networks Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"nn=MLPClassifier(solver='lbfgs',hidden_layer_sizes=20,batch_size=150,max_iter=100, random_state=1)\nnn.fit(trainX,trainy)\npredictions = nn.predict(testX)\naccnn=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Neural Networks (%): \\n\",accnn)  \n#get FPR\nfprnn, tprnn, _ = roc_curve(testy, predictions)\naucnn=auc(fprnn, tprnn)*100\nprint(\"AUC OF Neural Networks (%): \\n\", aucnn)\nrecallnn=recall_score(testy,predictions)*100\nprint(\"Recall of Neural Networks is: \\n\",recallnn)\nprecnn=precision_score(testy,predictions)*100\nprint(\"Precision of Neural Networks is: \\n\",precnn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm=clf = SVC(gamma=\"auto\",kernel='poly',degree=3)\nsvm.fit(trainX,trainy)\npredictions = svm.predict(testX)\naccsvm=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Support Vector Machine (%): \\n\",accsvm)  \n#get FPR\nfprsvm, tprsvm, _ = roc_curve(testy, predictions)\naucsvm=auc(fprsvm, tprsvm)*100\nprint(\"AUC OF Support Vector Machine (%): \\n\", aucsvm)\nrecallsvm=recall_score(testy,predictions)*100\nprint(\"Recall of Support Vector Machine is: \\n\",recallsvm)\nprecsvm=precision_score(testy,predictions)*100\nprint(\"Precision of Support Vector Machine is: \\n\",precsvm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=DecisionTreeClassifier(max_depth=10,criterion=\"gini\")\ndt.fit(trainX,trainy)\npredictions = dt.predict(testX)\naccdt=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Decision Tree (%): \\n\",accdt)  \n#get FPR\nfprdt, tprdt, _ = roc_curve(testy, predictions)\naucdt=auc(fprdt, tprdt)*100\nprint(\"AUC OF Decision Tree (%): \\n\",aucdt)\nrecalldt=recall_score(testy,predictions)*100\nprint(\"Recall of Decision Tree is: \\n\",recalldt)\nprecdt=precision_score(testy,predictions)*100\nprint(\"Precision of Decision Tree is: \\n\",precdt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random forest Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier()\nrf.fit(trainX,trainy)\npredictions = rf.predict(testX)\naccrf=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Random Forest (%): \\n\",accrf)  \n#get FPR\nfprrf, tprrf, _ = roc_curve(testy, predictions)\naucrf=auc(fprrf, tprrf)*100\nprint(\"AUC OF Random Forest (%): \\n\", aucrf)\nrecallrf=recall_score(testy,predictions)*100\nprint(\"Recall of Random Forest is: \\n\",recallrf)\nprecrf=precision_score(testy,predictions)*100\nprint(\"Precision of Random Forest is: \\n\",precrf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ada Boost Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"ab=AdaBoostClassifier()\nab.fit(trainX,trainy)\npredictions = ab.predict(testX)\naccab=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of AdaBoost (%): \\n\",accab)  \n#get FPR\nfprab, tprab, _ = roc_curve(testy, predictions)\naucab=auc(fprab, tprab)*100\nprint(\"AUC OF AdaBoost (%): \\n\",aucab)\nrecallab=recall_score(testy,predictions)*100\nprint(\"Recall of AdaBoost is: \\n\",recallab)\nprecab=precision_score(testy,predictions)*100\nprint(\"Precision of AdaBoost is: \\n\",precab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Descent Boosting Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb=GradientBoostingClassifier()\ngb.fit(trainX,trainy)\npredictions = gb.predict(testX)\naccgb=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Gradient Descent Boosting (%): \\n\",accgb)  \n#get FPR\nfprgb, tprgb, _ = roc_curve(testy, predictions)\naucgb=auc(fprgb, tprgb)*100\nprint(\"AUC OF Gradient Descent Boosting (%): \\n\", aucgb)\nrecallgb=recall_score(testy,predictions)*100\nprint(\"Recall of Gradient Descent Boosting is: \\n\",recallgb)\nprecgb=precision_score(testy,predictions)*100\nprint(\"Precision of Gradient Descent Boosting is: \\n\",precgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparison of all the Machine Learning Algorithms by Comparing some Evaluation Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"algos=[\"K Nearest Neighbor\",\"Guassian Naive Bayes\",\"Logistic Regression\",\"Neural Networks\",\"Support Vector Machine\",\"Decision Tree\",\"Random Forrest\",\"AdaBoost\",\"Gradient Descent Boosting\"]\nacc=[accknn,accgnb,acclrg,accnn,accsvm,accdt,accrf,accab,accgb]\nauc=[aucknn,aucgnb,auclrg,aucnn,aucsvm,aucdt,aucrf,aucab,aucgb]\nrecall=[recallknn,recallgnb,recalllrg,recallnn,recallsvm,recalldt,recallrf,recallab,recallgb]\nprec=[precknn,precgnb,preclrg,precnn,precsvm,precdt,precrf,precab,precgb]\ncomp={\"Algorithms\":algos,\"Accuracies\":acc,\"Area Under the Curve\":auc,\"Recall\":recall,\"Precision\":prec}\ncompdf=pd.DataFrame(comp)\ndisplay(compdf.sort_values(by=[\"Accuracies\",\"Area Under the Curve\",\"Recall\",\"Precision\"], ascending=False))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC of all the Machine Learning Algorithms on default parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\nroc_auc1=metrics.auc(fprknn,tprknn)\nroc_auc2=metrics.auc(fprgnb,tprgnb)\nroc_auc3=metrics.auc(fprlrg,tprlrg)\nroc_auc4=metrics.auc(fprnn,tprnn)\nroc_auc5=metrics.auc(fprsvm,tprsvm)\nroc_auc6=metrics.auc(fprdt,tprdt)\nroc_auc7=metrics.auc(fprrf,tprrf)\nroc_auc8=metrics.auc(fprab,tprab)\nroc_auc9=metrics.auc(fprgb,tprgb)\n\n# Method-I: PLot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.title(\"Receiver Operating Curve\")\nplt.plot(fprknn,tprknn,\"b\",label=\"ROC of KNN = %0.2f\" % roc_auc1)\nplt.plot(fprgnb,tprgnb,\"r\",label=\"ROC of Guassian Naive Bayes = %0.2f\" % roc_auc2)\nplt.plot(fprlrg,tprlrg,\"y\",label=\"ROC of Logistic Regression = %0.2f\" % roc_auc3)\nplt.plot(fprnn,tprnn,\"c\",label=\"ROC of Neural Networks = %0.2f\" % roc_auc4)\nplt.plot(fprsvm,tprsvm,\"k\",label=\"ROC of SVM = %0.2f\" % roc_auc5)\nplt.plot(fprdt,tprdt,\"m\",label=\"ROC of Descision Tree= %0.2f\" % roc_auc6)\nplt.plot(fprrf,tprrf,\"y--\",label=\"ROC of Random Forrest= %0.2f\" % roc_auc7)\nplt.plot(fprab,tprab,\"g--\",label=\"ROC of Ada Boost= %0.2f\" % roc_auc8)\nplt.plot(fprgb,tprgb,\"b--\",label=\"ROC of Gradient Boost= %0.2f\" % roc_auc9)\nplt.rcParams.update({'font.size': 16})\nplt.legend(loc=\"lower right\")\nplt.plot([0, 1],[0, 1],\"r--\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\n\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning using Random Search on any 4 Algorithms"},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning on K Nearest Neighbor using Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV as rs\n# K Nearest Neighbor with random search\nparameters={\"algorithm\":['auto','ball_tree','kd_tree','brute'],\"n_neighbors\":range(1,10,1),\"p\":[1,2],\"weights\":[\"uniform\",\"distance\"]}\nclf_knn=KNeighborsClassifier()\nclfknnrs=rs(clf_knn,parameters,cv=5,scoring=\"precision\")\nclfknnrs.fit(trainX,trainy)\npredictions = clfknnrs.predict(testX)\naccknnrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of KNN after Hyperparameter Tuning (%): \\n\",accknnrs)  \n#get FPR\nfprknnrs, tprknnrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecallknnrs=recall_score(testy,predictions)*100\nprint(\"Recall of KNN after Hyperparameter Tuning is: \\n\",recallknnrs)\nprecknnrs=precision_score(testy,predictions)*100\nprint(\"Precision of KNN after Hyperparameter Tuning is: \\n\",precknnrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clfknnrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clfknnrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clfknnrs.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning on Logistic Regression using Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV as rs\n# Logistic Regression with random search\nparameters={\"solver\":['lbfgs','newton-cg','liblinear','sag','saga'],\"max_iter\":range(100,500,100)}\nclf_lrg=LogisticRegression()\nclflrgrs=rs(clf_lrg,parameters,cv=5,scoring=\"precision\")\nclflrgrs.fit(trainX,trainy)\npredictions = clflrgrs.predict(testX)\nacclrgrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Logistic Regression after Hyperparameter Tuning (%): \\n\",acclrgrs)  \n#get FPR\nfprlrgrs, tprlrgrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecalllrgrs=recall_score(testy,predictions)*100\nprint(\"Recall of Logistic Regression after Hyperparameter Tuning is: \\n\",recalllrgrs)\npreclrgrs=precision_score(testy,predictions)*100\nprint(\"Precision of Logistic Regression after Hyperparameter Tuning is: \\n\",preclrgrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clflrgrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clflrgrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clflrgrs.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning on Decision Tree using Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV as rs\n# Decision Tree with random search\nparameters={\"min_samples_split\":range(10,200,10),\"max_depth\":range(1,20,1)}\nclf_treers=DecisionTreeClassifier()\nclfrs=rs(clf_treers,parameters,cv=5,scoring=\"precision\")\nclfrs.fit(trainX,trainy)\npredictions = clfrs.predict(testX)\naccdtrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Decision Tree after Hyperparameter Tuning (%): \\n\",accdtrs)  \n#get FPR\nfprdtrs, tprdtrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecalldtrs=recall_score(testy,predictions)*100\nprint(\"Recall of Decision Tree after Hyperparameter Tuning is: \\n\",recalldtrs)\nprecdtrs=precision_score(testy,predictions)*100\nprint(\"Precision of Decision Tree after Hyperparameter Tuning is: \\n\",precdtrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clfrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clfrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clfrs.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter Tuning on Neural Networks using Random Search\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV as rs\n# Neural Networks with random search\nparameters={\"solver\":['lbfgs','sgd','adam'],\"hidden_layer_sizes\":range(1,100,1),\"batch_size\":range(50,250,10),\"max_iter\":range(100,500,50),\"learning_rate\":['constant', 'invscaling', 'adaptive'],\"activation\":['identity', 'logistic', 'tanh', 'relu']}\nclf_nn=MLPClassifier()\nclfnnrs=rs(clf_nn,parameters,cv=5,scoring=\"precision\")\nclfnnrs.fit(trainX,trainy)\npredictions = clfnnrs.predict(testX)\naccnnrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Neural Networks after Hyperparameter Tuning (%): \\n\",accnnrs)  \n#get FPR\nfprnnrs, tprnnrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecallnnrs=recall_score(testy,predictions)*100\nprint(\"Recall of Neural Networks after Hyperparameter Tuning is: \\n\",recallnnrs)\nprecnnrs=precision_score(testy,predictions)*100\nprint(\"Precision of Neural Networks after Hyperparameter Tuning is: \\n\",precnnrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clfnnrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clfnnrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clfnnrs.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC Graph after Hyperparameter Tuning using Random Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.metrics as metrics\nroc_auc1=metrics.auc(fprknnrs,tprknnrs)\nroc_auc2=metrics.auc(fprdtrs,tprdtrs)\nroc_auc3=metrics.auc(fprnnrs,tprnnrs)\nroc_auc4=metrics.auc(fprlrgrs,tprlrgrs)\n\n# Method-I: PLot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(15,10))\nplt.title(\"Receiver Operating Curve\")\nplt.plot(fprknnrs,tprknnrs,\"b\",label=\"ROC of KNN after RS= %0.2f\" % roc_auc1)\nplt.plot(fprdtrs,tprdtrs,\"r\",label=\"ROC of Decision Tree after RS= %0.2f\" % roc_auc2)\nplt.plot(fprnnrs,tprnnrs,\"g\",label=\"ROC of Neural Networks after RS= %0.2f\" % roc_auc3)\nplt.plot(fprlrgrs,tprlrgrs,\"k\",label=\"ROC of Logistic Regression after RS= %0.2f\" % roc_auc4)\nplt.rcParams.update({'font.size': 20})\n\nplt.legend(loc=\"lower right\")\nplt.plot([0, 1],[0, 1],\"r--\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\n\n\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparision of 4 algorithms before and after hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"algos1=[\"K Nearest Neighbor\",\"Neural Networks\",\"Decision Tree\",\"Logistic Regression\"]\nacc1=[accknn,accnn,accdt,acclrg]\nrecall1=[recallknn,recallnn,recalldt,recalllrg]\nprec1=[precknn,precnn,precdt,preclrg]\ncomp1={\"Algorithms\":algos1,\"Accuracies before RS\":acc1,\"Recall before RS\":recall1,\"Precision before RS\":prec1}\ncompdf1=pd.DataFrame(comp1)\ndisplay(compdf1.sort_values(by=[\"Accuracies before RS\",\"Recall before RS\",\"Precision before RS\"], ascending=False))\nacc2=[accknnrs,accnnrs,accdtrs,acclrgrs]\nrecall2=[recallknnrs,recallnnrs,recalldtrs,recalllrgrs]\nprec2=[precknnrs,precnnrs,precdtrs,preclrgrs]\ncomp2={\"Algorithms\":algos1,\"Accuracies after RS\":acc2,\"Recall after RS\":recall2,\"Precision after RS\":prec2}\ncompdf2=pd.DataFrame(comp2)\ndisplay(compdf2.sort_values(by=[\"Accuracies after RS\",\"Recall after RS\",\"Precision after RS\"], ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}