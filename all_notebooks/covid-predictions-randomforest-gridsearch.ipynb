{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID predictions using RandomForest and GridSearch\n\n**THIS NOTEBOOK IS UNDER CONSTRUCTION**\n\nThis is my second attempt to build a Time Series model of COVID new cases and deaths. With a different approach of my first attempt (check [here](https://www.kaggle.com/lssilveira11/covid-cases-prediction-limeira-sp-brazil)), here I will try to predict new cases and deaths at the same time."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/covid19-in-limeiraspbrazil/covid-limeira-daily.csv')\n\n# convert Date columns to 'datetime' type\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# drop columns that will not be predicted\n#data.drop(columns=['UnderInvestigation', 'UnderInvestigationDeaths', 'Negative', 'NewInvestigation', 'NewNegative', 'NegativeRate', 'Active'], inplace=True)\n\n# keep sequential dates only\ndata = data[(data['Date'] - data['Date'].shift(5)) == pd.Timedelta(5, 'D')]\n\n# fillna at URCOccypancy with zeros\n#data['URCOccupancy'] = data['URCOccupancy'].fillna(0)\n\n# reset index\ndata.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['NewNotifications'] < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cols = ['NewCases', 'NewDeaths', 'Notifications']\ny_val = data[y_cols].to_numpy()[1:]\ny_val = np.append(y_val, np.full([1,len(y_cols)], fill_value=np.NaN), axis=0)\ndata['y'] = y_val.tolist()\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sep_feature_target(data):\n    y = data['y']\n    y = np.stack(y.to_numpy())\n    x = data.drop('y', axis=1)\n    return x, y\n\ndef train_test_split(data, split_size=15):\n    train = data.iloc[:-split_size,:]\n    train.reset_index(drop=True, inplace=True)\n    test = data.tail(split_size).copy()\n    test.reset_index(drop=True, inplace=True)    \n    return train, test\n\ntrain, test = train_test_split(data)\ntrain, valid = train_test_split(train,60)\n\nx_train, y_train = sep_feature_target(train)\nx_valid, y_valid = sep_feature_target(valid)\nx_test, y_test   = sep_feature_target(test)\n\n# drop last row of test_set because there is no target for it\nx_test.drop(x_test.tail(1).index,inplace=True)\ny_test = y_test[:-1]\n\nprint('Checking sizes of train/dev/test sets:')\nprint(len(x_train), len(y_train))\nprint(len(x_valid), len(y_valid))\nprint(len(x_test), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline\n\nfrom sklearn.metrics import mean_squared_log_error\n\ndef calc_error(valid, predict):\n    return np.sqrt(mean_squared_log_error( valid, predict ))\n\nbaseline_valid = np.append([x_train.tail(1)[y_cols].values[0]],\n                           x_valid[y_cols].shift(1).values[1:], \n                           axis=0)\nprint(\"baseline shift1 =\", calc_error( y_valid, baseline_valid ))\n\n#baseline_valid = x_valid['NewCases'].shift(1).rolling(5).mean()\n#print(\"baseline rollingavg5 =\", calc_error( y_valid[y_valid.notnull()][5:], baseline_valid[baseline_valid.notnull()] ) * 100.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\n\ndef ra_gen(data_features, colName):\n    data_features[colName+'diff_1'] = data_features[colName].diff()\n    data_features[colName+'diff_2'] = data_features[colName].shift(1).diff()\n    data_features[colName+'diff_3'] = data_features[colName].shift(2).diff()\n    data_features[colName+'diff_4'] = data_features[colName].shift(3).diff()\n    data_features[colName+'diff_5'] = data_features[colName].shift(4).diff()\n    data_features[colName+'diff_6'] = data_features[colName].shift(5).diff()\n    data_features[colName+'diff_7'] = data_features[colName].shift(6).diff()\n    data_features[colName+'diff_8'] = data_features[colName].shift(7).diff()\n    data_features[colName+'diff_9'] = data_features[colName].shift(8).diff()\n    data_features[colName+'diff_10'] = data_features[colName].shift(9).diff()\n    data_features[colName+'ra3'] = data_features[colName].rolling(3).mean()#.reset_index(level=0, drop=True)\n    data_features[colName+'ra7'] = data_features[colName].rolling(7).mean()#.reset_index(level=0, drop=True)\n    data_features[colName+'ra10'] = data_features[colName].rolling(10).mean()#.reset_index(level=0, drop=True)\n    data_features[colName+'ra14'] = data_features[colName].rolling(14).mean()#.reset_index(level=0, drop=True)\n    data_features[colName+'ra20'] = data_features[colName].rolling(20).mean()#.reset_index(level=0, drop=True)\n    data_features[colName+'ra30'] = data_features[colName].rolling(30).mean()#.reset_index(level=0, drop=True)\n    return data_features\n\ndef feature_generation(data_features):\n    data_features['weekday'] = data_features['Date'].dt.weekday\n    data_features['weekofyear'] = data_features['Date'].dt.isocalendar().week\n    data_features['weekofyear'] = data_features['weekofyear'].astype('int32')\n    data_features['dayofyear'] = data_features['Date'].dt.dayofyear\n    data_features = ra_gen(data_features, 'NewCases')\n    data_features = ra_gen(data_features, 'NewDeaths')\n    data_features = ra_gen(data_features, 'NewNotifications')\n    data_features = data_features.fillna(0)\n    #data_features['Date'] = data_features['Date'].map(dt.datetime.toordinal)\n    data_features['Date'] = data_features['Date'].values.astype(float)\n    return data_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_results(target,predict,setName=''):\n    target_df = pd.DataFrame(target)\n    target_df.columns = y_cols\n    #target_df.columns = data.columns.values[1:6]\n    #target_df['NewCases'] = target_df['Confirmed'].diff()\n    #target_df['NewDeaths'] = target_df['Deaths'].diff()\n    #target_df['NewNotifications'] = target_df['Notifications'].diff()\n    \n    predict_df = pd.DataFrame(predict)\n    predict_df.columns = y_cols\n    #predict_df.columns = data.columns.values[1:6]\n    #predict_df['NewCases'] = predict_df['Confirmed'].diff()\n    #predict_df['NewDeaths'] = predict_df['Deaths'].diff()\n    #predict_df['NewNotifications'] = predict_df['Notifications'].diff() \n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(21,5))\n    \n    target_df.plot(y='NewCases', ax=ax[0])\n    predict_df.plot(y='NewCases', ax=ax[0])\n    ax[0].legend(['Target', 'Predicted'])\n    ax[0].set_title('New cases ('+setName+')')\n    \n    target_df.plot(y='NewDeaths', ax=ax[1])\n    predict_df.plot(y='NewDeaths', ax=ax[1])\n    ax[1].legend(['Target', 'Predicted'])\n    ax[1].set_title('New deaths ('+setName+')')\n    \n    target_df.plot(y='Notifications', ax=ax[2])\n    predict_df.plot(y='Notifications', ax=ax[2])\n    ax[2].legend(['Target', 'Predicted'])\n    ax[2].set_title('Notifications ('+setName+')')\n    \n    return target_df, predict_df\n#    results = pd.DataFrame()\n#    results['target'] = valid[:,0]\n#    results['predict'] = predict[:,0]\n#    results.plot(figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmaxfeat=int(x_train.shape[1]*0.6)\n\nmodel1 = RandomForestRegressor(n_jobs=-1, random_state=93, n_estimators=200, max_features=41, max_depth=6)\nmodel1.fit(feature_generation(x_train), y_train)\n\nprint(model1)\nprint('Checking the model1 errors:')\n\nt = model1.predict(feature_generation(x_train))\nprint(\"model1 (train set error)=\", calc_error(y_train, t))\n\nv = model1.predict(feature_generation(x_valid))\nprint(\"model1 (dev set error)=\", calc_error(y_valid, v))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=93)\n    Checking the model1 errors:\n    model1 (train set error)= 0.3647075764880938\n    model1 (dev set error)= 1.068082654287613\n    \n    RandomForestRegressor(max_depth=5, max_features=26, n_estimators=1000,\n                      n_jobs=-1, random_state=93)\n    Checking the model1 errors:\n    model1 (train set error)= 0.5469734071346023\n    model1 (dev set error)= 1.0342799360032215"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results(y_train, t, 'Train set')\nplt.show()\n\nplot_results(y_valid, v, 'Dev set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams = {\n          'n_estimators': [100,200], \n          'min_samples_leaf': [2,3],\n          'min_samples_split': [2,3],\n          'max_depth': [*range(1, 6, 1)],\n          'max_features': [*range(1, x_train.shape[1], 20)],\n         }\n\ncv = GridSearchCV(model1, param_grid=params, scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=1)\ncv.fit(feature_generation(x_train), y_train)\n\nprint(cv)\nprint('Checking the cv errors:')\n\nt = cv.predict(feature_generation(x_train))\nprint(\"cv (train set error)=\", calc_error(y_train, t))\n\nv = cv.predict(feature_generation(x_valid))\nprint(\"cv (dev set error)=\", calc_error(y_valid, v))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best parameters found after GridSearch\nprint(cv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results(y_train, t)\nplt.show()\n\nplot_results(y_valid, v)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}