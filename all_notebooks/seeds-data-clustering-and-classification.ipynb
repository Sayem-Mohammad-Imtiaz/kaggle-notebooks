{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SEEDS DATA SET:"},{"metadata":{},"cell_type":"markdown","source":"Measurements of geometrical properties of kernels belonging to three different varieties of wheat-  Kama,Rosa and Canadian. \n\nA soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes."},{"metadata":{},"cell_type":"markdown","source":"**Importing necessary Libraries:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading the seeds dataset:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/seedsdata/seeds.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking the data type of each attribute and if any missing value in each feature:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Every attribute median and max values of all features:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df,hue='seedType')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heat Map:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\ncor=df.corr()\nsns.heatmap(cor,annot=True,cmap='coolwarm')\nplt.ylim(8,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting independent and dependent variables:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop('ID',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=df.groupby('seedType').count()\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols=['area','perimeter','compactness','lengthOfKernel','widthOfKernel','asymmetryCoefficient','lengthOfKernelGroove']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=feature_cols\ny=df['seedType']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.area.astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Standardization the data:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\ndf_new=ss.fit_transform(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop('seedType',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df['seedType']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train - Test Split:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=3,test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LINEAR REGRESSION:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(X, y)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\nprint(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LINEAR REGRESSION - OLS:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n%matplotlib inline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\n\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\n\npredictions = model.predict(X_constant)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's deal the problem with Classification first."},{"metadata":{},"cell_type":"markdown","source":"# LOGISTIC REGRESSION:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(fit_intercept=True,solver='liblinear',multi_class='ovr')\nmodel.fit(X_train,y_train)\ny_test_pred=model.predict(X_test)\ny_test_prob=model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=model.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train,predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('AUC Value of the model:',roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE CLASSIFIER:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=DecisionTreeClassifier()\n\ndt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Decision Tree - HYPER PARAMETER TUNING USING GRID SEARCH:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndtc=DecisionTreeClassifier()\n\n\n#from sklearn import preprocessing\n#y = preprocessing.label_binarize(y, classes=[0, 1, 2])\n\nparams={'max_depth':[2,3,4,5,6],\n        'min_samples_leaf':[1,2,3,4,5,6,7],\n        'min_samples_split':[2,3,4,5,6,7,8,9,10],\n        'criterion':['gini','entrophy']}\ngsearch=GridSearchCV(dtc,param_grid=params,cv=3,scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs=pd.DataFrame(gsearch.cv_results_)\ngs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=DecisionTreeClassifier(**gsearch.best_params_)\n\ndt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree - HYPER PARAMETER TUNING USING RANDOMIZED SEARCH:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\ndtc=DecisionTreeClassifier()\nparams={'max_depth':sp_randint(2,20),\n        'min_samples_leaf':sp_randint(1,20),\n        'min_samples_split':sp_randint(2,40),\n        'criterion':['gini','entrophy']}\n\n\nrsearch=RandomizedSearchCV(dtc,param_distributions=params,\n                           cv=3,n_iter=200,scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsearch.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs=pd.DataFrame(rsearch.cv_results_)\nrs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt=DecisionTreeClassifier(**rsearch.best_params_)\n\ndt.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test) \n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Desicion Tree classifier and hyperparameter tuning using grid search and randomised search all 3 give an accuracy of 0.95 and AUC is about 0.96  for the test data."},{"metadata":{},"cell_type":"markdown","source":"# RANDOM FOREST CLASSIFIER:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=3,test_size=0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc=RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred=rfc.predict(X_test)\ny_test_prob=rfc.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest -  HYPER PARAMETER TUNING USING RANDOMIZED SEARCH:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nrfc=RandomForestClassifier()\n\nparams={'n_estimators':sp_randint(100,200),\n        'max_features':sp_randint(1,24),\n        'max_depth':sp_randint(2,10),\n        'min_samples_split':sp_randint(2,20),\n        'min_samples_leaf':sp_randint(1,20),\n        'criterion':['gini','entropy']}\n\nrsearch=RandomizedSearchCV(rfc,param_distributions=params,n_iter=50,cv=3,scoring='accuracy',\n                           random_state=3,return_train_score=True)\nrsearch.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(rsearch.cv_results_).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier(**rsearch.best_params_,random_state=3)\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred=rfc.predict(X_test)\ny_test_prob=rfc.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest - HYPER PARAMETER TUNING USING GRID SEARCH:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nrfc=RandomForestClassifier()\n\nparams={'max_depth':[2,3,4,5,6],\n        'min_samples_leaf':[1,2,3,4,5,6,7],\n        'min_samples_split':[2,3,4,5,6,7,8,9,10],\n        'criterion':['gini','entrophy']}\ngsearch=GridSearchCV(dtc,param_grid=params,cv=3,scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs=pd.DataFrame(gsearch.cv_results_)\ngs.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier(**gsearch.best_params_)\n\nrfc.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest classifier and the hyper parameters have been tuned using grid search and randomised search. Out of which rsearch has the highest accuracy in test data."},{"metadata":{},"cell_type":"markdown","source":"# MULTINOMINAL NAIVE BAYES:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmnb=MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb.fit(X_train,y_train)\n\ny_test_pred=mnb.predict(X_test)\ny_test_prob=mnb.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob, multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ADA BOOST CLASSIFIER:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(random_state=3)\n\nada.fit(X_train,y_train)\n\n\ny_test_pred=ada.predict(X_test)\ny_test_prob = ada.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LIGHTGBM CLASSIFIER:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = lgb.LGBMClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = { 'n_estimators' : sp_randint(50,200),\n        'max_depth' : sp_randint(2,15),\n         'learning_rate' : sp_uniform(0.201,0.5),\n         'num_leaves' : sp_randint(20,50)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsearch = RandomizedSearchCV(lgbm, param_distributions=params, cv=3, n_iter=200, n_jobs=-1, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsearch.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = lgb.LGBMClassifier(**rsearch.best_params_)\nlgbm.fit(X_train,y_train)\n\ny_test_pred=lgbm.predict(X_test)\ny_test_prob = lgbm.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"out of the all the boosting techniques used, LightGBM works the best."},{"metadata":{},"cell_type":"markdown","source":"# STACKING the results of 3 learners (Decision Tree, K-NN , Logistic Regression)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver='liblinear')\n\nlr.fit(X_train,y_train)\n\ny_test_pred=lr.predict(X_test)\ny_test_prob = lr.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-NN ALGORITHM:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\nknn = KNeighborsClassifier()\n\nparams={'n_neighbors' : sp_randint(1,15),'p' : sp_randint(1,5)}\n\nrsearch_knn = RandomizedSearchCV(knn, param_distributions=params, cv =3,n_iter=50,n_jobs=-1,return_train_score=True, random_state=3)\n\nrsearch_knn.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rsearch_knn.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(**rsearch_knn.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"knn.fit(X_train,y_train)\ny_train_pred=knn.predict(X_train)\n\ny_test_pred=knn.predict(X_test)\ny_test_prob = knn.predict_proba(X_test)\n\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nlr=LogisticRegression(solver='liblinear')\nknn=KNeighborsClassifier(**rsearch_knn.best_params_)\ndt=DecisionTreeClassifier(**gsearch.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hard Voting\n\nclf = VotingClassifier(estimators=[('lr',lr),('knn',knn),('dt',dt)], voting='hard')\n\nclf.fit(X_train, y_train)\n\ny_train_pred = clf.predict(X_train)\ny_test_pred = clf.predict(X_test)\n\nprint(\"Accuracy score Train : \",accuracy_score(y_train,y_train_pred))\nprint(\"Accuracy score Test : \",accuracy_score(y_test,y_test_pred))\nprint(\"\\n\")\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Soft Voting -- equal weightages\n\nclf =VotingClassifier(estimators=[('lr',lr),('knn',knn),('dt',dt)],voting='soft')\n\nclf.fit(X_train,y_train)\n\ny_test_pred=clf.predict(X_test)\ny_test_prob = clf.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Soft Voting -- Different weightages\n\nclf =VotingClassifier(estimators=[('lr',lr),('knn',knn),('dt',dt)],weights=[1,2,3],voting='soft')\nclf.fit(X_train,y_train)\n\ny_test_pred=clf.predict(X_test)\ny_test_prob = clf.predict_proba(X_test)\n\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stacking 3 learners for better results, with the scaled data - we can find the overall accuracy and Auc results of the test data has been better in terms of Soft voting either using equal weightages or different weightages than Hard voting technique."},{"metadata":{},"cell_type":"markdown","source":"___________________________________________________________________________________________________________________________"},{"metadata":{},"cell_type":"markdown","source":"**Having dealt the problem wrt classification, lets check on how the data performs when the problem is processed with clustering algorithms and techniques.**"},{"metadata":{},"cell_type":"markdown","source":"# CLUSTERING:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2=df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import zscore\ndf_scaled = df2.apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K - MEANS:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_range = range( 1, 15 )\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 10 )\n  clusters.fit(df_scaled)\n # labels = clusters.labels_\n # centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elbow plot\n\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3, n_init = 15, random_state=2345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.fit(df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids = kmeans.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroid_df = pd.DataFrame(centroids, columns = list(df_scaled) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroid_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\n\ndf_labels['labels'] = df_labels['labels'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snail_df_labeled = df.join(df_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_analysis = (snail_df_labeled.groupby(['labels'] , axis=0)).head() \ndf_analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snail_df_labeled['labels'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8, 6))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=100)\nkmeans.fit(df_scaled)\nlabels = kmeans.labels_\nax.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], df_scaled.iloc[:, 3],c=labels.astype(np.float), edgecolor='k')\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Length')\nax.set_ylabel('Height')\nax.set_zlabel('Weight')\nax.set_title('3D plot of KMeans Clustering')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we know our best k value is 3, I am creating a new kmeans model:\nkmeans2 = KMeans(n_clusters=3)\n\n# Training the model:\nclusters = kmeans2.fit_predict(df)\n\n# Adding a label feature with the predicted class values:\ndf_k = df.copy(deep=True)\ndf_k['label'] = clusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comparing Original Classes and K-Means Algorithm Classes:**\n\nFor visualization I will use only two features (area and perimeter) for the original and predicted datasets. Different classes will have seperate color and styles."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nax1 = plt.subplot(1,2,1)\nplt.title('Original Classes')\nsns.scatterplot(x='area', y='perimeter', hue='seedType', style='seedType', palette='plasma',data=df, ax=ax1)\n\nax2 = plt.subplot(1,2,2)\nplt.title('Predicted Classes')\nsns.scatterplot(x='area', y='perimeter', hue='label', style='label', palette='plasma',data=df_k, ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original Data Classes:')\nprint(df.seedType.value_counts())\nprint('-' * 30)\nprint('Predicted Data Classes:')\nprint(df_k.label.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**HIERARCHICAL CLUSTERING ALGORITHM:**\n\n**Creating the Dendrogram:**\n\nWe use dendrogram to find how many classes we have in our data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram\nplt.figure(figsize=[10,10])\nmerg = linkage(df, method='ward')\ndendrogram(merg, leaf_rotation=90)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Euclidean Distances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the dendrogram we can read there are 3 classes in our data set.**"},{"metadata":{},"cell_type":"markdown","source":"**Hierarchical Clustering Algorithm:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nhie_clus = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\ncluster2 = hie_clus.fit_predict(df)\n\ndf_h = df.copy(deep=True)\ndf_h['label'] = cluster2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comparing Original, K-Means and Hierarchical Clustered Classes:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Original Classes')\nsns.scatterplot(x='area', y='perimeter', hue='seedType', style='seedType', data=df,palette='viridis')\nplt.show()\nplt.title('K-Means Classes')\nsns.scatterplot(x='area', y='perimeter', hue='label', style='label', data=df_k,palette='viridis')\nplt.show()\nplt.title('Hierarchical Classes')\nsns.scatterplot(x='area', y='perimeter', hue='label', style='label', data=df_h,palette='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original Data Classes:')\nprint(df.seedType.value_counts())\nprint('-' * 30)\nprint('K-Means Predicted Data Classes:')\nprint(df_k.label.value_counts())\nprint('-' * 30)\nprint('Hierarchical Predicted Data Classes:')\nprint(df_h.label.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Silhouette analysis for K-Means clustering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\n%matplotlib inline\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX, y = make_blobs(n_samples=500,\n                  n_features=2,\n                  centers=4,\n                  cluster_std=1,\n                  center_box=(-10.0, 10.0),\n                  shuffle=True,\n                  random_state=1)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.Spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the analysis, we can see the highest accuracy is for the clusters with  n=4 and the silhouette score is about 0.65."},{"metadata":{},"cell_type":"markdown","source":"# Build An Classification model with Hierarchical clustering:"},{"metadata":{},"cell_type":"markdown","source":"# K-Means: "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_k.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df_k.drop('label',axis=1)\ny= df_k['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=test_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nindependent_scalar = StandardScaler()\nx_train = independent_scalar.fit_transform (x_train) #fit and transform\nx_validate = independent_scalar.transform (x_validate) # only transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \n#DecisionTreeClassifier is the corresponding Classifier\nDtree = DecisionTreeClassifier(max_depth=3)\nDtree.fit (x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_train = Dtree.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\npredictValues_validate = Dtree.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM FOREST:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nRFclassifier = RandomForestClassifier(n_estimators = 100, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFclassifier = RandomForestClassifier(n_estimators = 11, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-NN :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df_k.drop('label',axis=1)\ny= df_k['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_standardize = x.apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN only takes array as input hence it is importanct to convert dataframe to array\nx1 = np.array(x_standardize)\ny1 = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x1, y1, test_size=test_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN = KNeighborsClassifier(n_neighbors= 8 , weights = 'uniform', metric='euclidean')\nKNN.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_train = KNN.predict(x_train)\nprint(predictValues_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\nprint(\"Train Accuracy  :: \",accuracy_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_validate = KNN.predict(x_validate)\nprint(predictValues_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building a model using K-means algorithms of clustering, Random forest classifier has the highest accuracy in f1 score without underfitting or overfitting values."},{"metadata":{},"cell_type":"markdown","source":"# Build An Classification model with Non - Hierarchical clustering:"},{"metadata":{},"cell_type":"markdown","source":"# Agglomerative Clustering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_h.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df_k.drop('label',axis=1)\ny= df_k['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=test_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nindependent_scalar = StandardScaler()\nx_train = independent_scalar.fit_transform (x_train) #fit and transform\nx_validate = independent_scalar.transform (x_validate) # only transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DECISION TREE CLASSIFIER:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \n#DecisionTreeClassifier is the corresponding Classifier\nDtree = DecisionTreeClassifier(max_depth=3)\nDtree.fit (x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_train = Dtree.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\npredictValues_validate = Dtree.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RANDOM FOREST:"},{"metadata":{"trusted":true},"cell_type":"code","source":"RFclassifier = RandomForestClassifier(n_estimators = 100, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RFclassifier = RandomForestClassifier(n_estimators = 11, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= df_k.drop('label',axis=1)\ny= df_k['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_standardize = x.apply(zscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN only takes array as input hence it is importanct to convert dataframe to array\nx1 = np.array(x_standardize)\ny1 = np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x1, y1, test_size=test_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN = KNeighborsClassifier(n_neighbors= 8 , weights = 'uniform', metric='euclidean')\nKNN.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_train = KNN.predict(x_train)\nprint(predictValues_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\nprint(\"Train Accuracy  :: \",accuracy_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictValues_validate = KNN.predict(x_validate)\nprint(predictValues_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In agglomerative clustering, Random forest is the best generalized model with accuracy of 0.98%."},{"metadata":{},"cell_type":"markdown","source":"___________________________________________________________________________________________________________________________"},{"metadata":{},"cell_type":"markdown","source":"**NOTE:**"},{"metadata":{},"cell_type":"markdown","source":"The analysis for the given dataset is done in both the perspectives of classification and clustering algorithms.\n\nThe maximum accuracy and the best models are evaluated using different techniques and test scores. \n\nAs per the necessity and requirement the obtained results can be modified further to proceed."},{"metadata":{},"cell_type":"markdown","source":"# END"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}