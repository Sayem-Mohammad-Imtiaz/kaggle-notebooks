{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data loading\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart = pd.read_csv(\"/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\nheart.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart[\"DEATH_EVENT\"].value_counts()\n\n# deaths = 13\n# alive = 299","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart[\"DEATH_EVENT\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = \"DEATH_EVENT\" , y = \"age\" , data = heart)\nsns.set(style = \"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = \"DEATH_EVENT\" , y = \"serum_creatinine\" , data = heart)\nsns.set(style = \"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = \"DEATH_EVENT\" , y = \"creatinine_phosphokinase\" , data = heart)\nsns.set(style = \"whitegrid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"anaemia\")\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"platelets\")\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"high_blood_pressure\")\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(heart,hue = \"DEATH_EVENT\",height = 5)\ng.map(sns.distplot,\"serum_sodium\")\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''by above analysis we can say that serum_creatinine,platelets,high_blood_pressure,creatinine_phosphokinase,anaemia\ncause changes to death event'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.corr()  # correlation between any two features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize = (10,10))\nsns.heatmap(heart.corr() , annot = True , linewidths = 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = heart.corr()\ncorrelation_target = abs(correlation)\ncorrelation_target['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here we can say that age,ejection_fraction,serum_creatinine,serum_sodium,time has impacted death event more\nbut after comparing with above seaborn analysis i analysed below features as important"},{"metadata":{"trusted":true},"cell_type":"code","source":"heart.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making new datasets with important features\n\nx = heart.loc[:,{\"high_blood_pressure\",\"anaemia\",\"age\",\"ejection_fraction\",\"serum_creatinine\",\"time\"}]\ny = np.array(heart[\"DEATH_EVENT\"])\nheart['DEATH_EVENT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test\n# train = 70% of total data\n# test = 30% of total data\n\nx_train,x_test,y_train,y_test = model_selection.train_test_split(x,y,test_size = 0.3,random_state = 0)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = []\nf1_score = []\naccuracy = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.a) K-NN WITH SIMPLE CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now for cross validation we will take data from train data and split it equally\n\n# x_tr = 70% of total x_train\n# x_cv = 30% of total x_train\n# y_tr = 70% of total y_train\n# y_cv = 30% of total y_train\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nx_tr,x_cv,y_tr,y_cv = model_selection.train_test_split(x_train,y_train,test_size = 0.3,random_state = 0)\n\nprint(x_tr.shape)\nprint(x_cv.shape)\nprint(y_tr.shape)\nprint(y_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now main thing i.e, fitting and predicting\n\nfor i in range(1,30,2):\n    knn = KNeighborsClassifier(n_neighbors = i )\n    knn.fit(x_tr,y_tr)\n    pred = knn.predict(x_cv)\n    acc = accuracy_score(y_cv , pred ,normalize = True)*float(100)\n    print(' cv accuracy for k = {0} is {1}' .format (i,acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_tr,y_tr)\npredict = knn.predict(x_test)\nprint('test accuracy',accuracy_score(y_test , predict ,normalize = True)*float(100))\n\nknn_normal_accuracy = accuracy_score(y_test , predict ,normalize = True)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,predict)\nsns.heatmap(cm , annot = True )\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\nknn_normal_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(knn_normal_f1_score))\n\n\n# putting datas in list\n\nmodel.append('knn_normal')\nf1_score.append(knn_normal_f1_score)\naccuracy.append(knn_normal_accuracy)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.b) KNN WITH GRID SEARCH CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors': np.arange(1, 15)}\n\n\n\nknn_gcv = GridSearchCV(knn, param_grid, cv=4)\n\nknn_gcv.fit(x_train, y_train)\n\nprint(\"Best K Value is \",knn_gcv.best_params_)\n\nprint(\"test accuracy \",(knn_gcv.score(x_test,y_test))*float(100))\n\nknn_grid_accuracy = knn_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,knn_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\nknn_grid_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(knn_grid_f1_score))\n\n# putting datas in list\n\nmodel.append('knn_grid')\nf1_score.append(knn_grid_f1_score)\naccuracy.append(knn_grid_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. LOGISTIC REGRESSION WITH GRID SEARCH CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlor = LogisticRegression(max_iter=1000)\n\nparams_lor = {'C':[0.00001,0.0001,0.001,0.1,1,10,100]}\n\nlor_gcv = GridSearchCV(lor , param_grid = params_lor)\n\nlor_gcv.fit(x_train, y_train)\n\nprint(\"Best C Value is \",lor_gcv.best_params_)\n\nprint(\"test accuracy \",(lor_gcv.score(x_test,y_test))*float(100))\n\nlogistic_regression_accuracy = lor_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,lor_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\nlogistic_regression_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(logistic_regression_f1_score))\n\n# putting datas in list\n\nmodel.append('logistic_regression')\nf1_score.append(logistic_regression_f1_score)\naccuracy.append(logistic_regression_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. NAIVE BAYES WITH GRID SEARCH CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnaive = GaussianNB()\n\nparams_naive = {'var_smoothing':[0.00001,0.0001,0.001,0.1,1,10,100]}\n\nnaive_gcv = GridSearchCV(naive , param_grid = params_naive )\n\nnaive_gcv.fit(x_train, y_train)\n\nprint(\"Best var_smoothing Value is \",naive_gcv.best_params_)\n\nprint(\"test accuracy \",(naive_gcv.score(x_test,y_test))*float(100))\n\nnaive_bayes_accuracy = naive_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,naive_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\nnaive_bayes_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(naive_bayes_f1_score))\n\n\n\n# putting datas in list\n\nmodel.append('naive_bayes')\nf1_score.append(naive_bayes_f1_score)\naccuracy.append(naive_bayes_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. RBF SVM WITH GRID SEARCH CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm = SVC()\n\nparams_svm = {'C':[0.00001,0.0001,0.001,0.1,1,10,100]}\n\nsvm_gcv = GridSearchCV(svm , param_grid = params_svm )\n\nsvm_gcv.fit(x_train,y_train)\n\nprint(\"Best C Value is \",svm_gcv.best_params_)\n\nprint(\"test accuracy \",(svm_gcv.score(x_test,y_test))*float(100))\n\nsvm_accuracy = svm_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,svm_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\nsvm_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(svm_f1_score))\n\n\n# putting datas in list\n\nmodel.append('svm')\nf1_score.append(svm_f1_score)\naccuracy.append(svm_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Decision Tree WITH GRID SEARCH CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\n\nparams_dt = {'max_depth':np.arange(1,10)}\n\ndt_gcv = GridSearchCV(dt , param_grid = params_dt)\n\ndt_gcv.fit(x_train , y_train)\n\nprint(\"Best C Value is \",dt_gcv.best_params_)\n\nprint(\"test accuracy \",(dt_gcv.score(x_test,y_test))*float(100))\n\ndecision_tree_accuracy = dt_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,dt_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\ndecision_tree_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(decision_tree_f1_score))\n\n\n# putting datas in list\n\nmodel.append('decision_tree')\nf1_score.append(decision_tree_f1_score)\naccuracy.append(decision_tree_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. RANDOM FOREST WITH RANDOMIZED SEARCH CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf = RandomForestClassifier()\n\nparams_rf = {'n_estimators' : np.arange(1,100,10) }\n\nrf_gcv = RandomizedSearchCV(rf , param_distributions = params_rf)\n\nrf_gcv.fit(x_train,y_train)\n\nprint(\"Best n_estimators Value is \",rf_gcv.best_params_)\n\n\nprint(\"test accuracy \",(rf_gcv.score(x_test,y_test))*float(100))\n\nrandom_forest_accuracy = rf_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,rf_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\nrandom_forest_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(random_forest_f1_score))\n\n\n# putting datas in list\n\nmodel.append('random_forest')\nf1_score.append(random_forest_f1_score)\naccuracy.append(random_forest_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. X-GRADIENT BOOSTING WITH GRID SEARCH CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n\nparams_xgb = {'learning_rate':[0.00001,0.0001,0.001,0.01,0.1,1],'n_estimators':np.arange(1,50,10),'max_depth':np.arange(1,10)}\n\n\nxgb_gcv = RandomizedSearchCV(xgb , param_distributions = params_xgb)\n\nxgb_gcv.fit(x_train,y_train)\n\nprint(\"Best parameters values are \",xgb_gcv.best_params_)\n\n\nprint(\"test accuracy \",(xgb_gcv.score(x_test,y_test))*float(100))\n\nxg_boost_accuracy = xgb_gcv.score(x_test,y_test)*float(100)\n\n# to plot confusion matrix\n\ncm = confusion_matrix(y_test,xgb_gcv.predict(x_test))\nsns.heatmap(cm , annot = True )\n\n\n# calculation of F1 score\n\nTN = cm[0,0]\nTP = cm[1,1]\nFN = cm[0,1]\nFP = cm[1,0]\n\nRecall = TP/(TP+FN)\nPrecision = TP/(TP+FP)\n\nxg_boost_f1_score = ((2 * Recall * Precision)/(Recall + Precision))\n\nprint('f1_score of the model is {}'.format(xg_boost_f1_score))\n\n\n# putting datas in list\n\nmodel.append('xg_boost')\nf1_score.append(xg_boost_f1_score)\naccuracy.append(xg_boost_accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# table for heart_failure_models\n\ndict = {'model': model, 'accuracy': accuracy, 'f1_score': f1_score}   \n\nheart_failure_prediction = pd.DataFrame(dict) \n\nheart_failure_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting to compare each models on accuracy\n\nfig_dims = (13, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(x = model, y = accuracy, ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting to compare each models on f1_score\n\nfig_dims = (13, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(x = model, y = f1_score, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. All codes are easy to understand\n2. XG boost,knn,naive have maximum accuracy and maximum f1_score for the given data.\n3. upvote if you found this notebook usefull\n4. Most important thing i observed while making these models is choosing correct features."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}