{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%bash\npip install -q transformers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\nfrom transformers import AlbertConfig, AlbertModel, AlbertTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nIMDB_dataset = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ntrain,test = train_test_split(IMDB_dataset, test_size = 0.5)\n\nprint(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig),\n    'albert': (AlbertModel, AlbertTokenizer, AlbertConfig)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nseed = 42\nuse_fp16 = False\nbs = 10\n\n\nmodel_type = 'albert'\npretrained_model_name = 'albert-base-v2'\n\n# model_type = 'roberta'\n# pretrained_model_name = 'roberta-large'\n\n# model_type = 'bert'\n# pretrained_model_name='bert-base-uncased'\n\n# model_type = 'distilbert'\n# pretrained_model_name = 'distilbert-base-uncased'\n\n#model_type = 'xlm'\n#pretrained_model_name = 'xlm-clm-enfr-1024'\n\n# model_type = 'xlnet'\n# pretrained_model_name = 'xlnet-base-cased'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_all(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transformer_tokenizer = tokenizer_class.from_pretrained(\"/kaggle/input/large-roberta\")\ntransformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id\ntokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\nprint(tokens)\nids = transformer_tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\ntransformer_tokenizer.convert_ids_to_tokens(ids)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"databunch = (TextList.from_df(IMDB_dataset, cols='review', processor=transformer_processor)\n             .split_by_rand_pct(0.5,seed=seed)\n             .label_from_df(cols= 'sentiment')\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.tensor as Tensor\ndef dropout_mask(x:Tensor, sz, p:float):\n    \"Return a dropout mask of the same type as x, size sz, with probability p to cancel an element.\"\n    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n\nclass RNNDropout(nn.Module):\n    \"Dropout with probability p that is consistent on the seq_len dimension.\"\n\n    def __init__(self, p:float=0.5): \n        super(RNNDropout, self).__init__()\n        self.p=p\n\n    def forward(self, x:Tensor)->Tensor:\n        if not self.training or self.p == 0.: return x\n        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n        return x * m\n\nclass WeightDropout(nn.Module):\n    \"A module that warps another layer in which some weights will be replaced by 0 during training.\"\n\n    def __init__(self, module:nn.Module, weight_p:float, layer_names=['weight_hh_l0']):\n        super(WeightDropout, self).__init__()\n        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n        for layer in self.layer_names:\n            #Makes a copy of the weights of the selected layers.\n            w = getattr(self.module, layer)\n            # Registered in list of parameters\n            self.register_parameter(str(layer)+'_raw', nn.Parameter(w.data))\n            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n\n    def _setweights(self):\n        \"Apply dropout to the raw weights.\"\n        for layer in self.layer_names:\n            raw_w = getattr(self, str(layer) +'_raw')\n            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n\n    def forward(self, *args):\n        self._setweights()\n        with warnings.catch_warnings():\n            #To avoid the warning that comes because the weights aren't flattened.\n            warnings.simplefilter(\"ignore\")\n            return self.module.forward(*args)\n\n    def reset(self):\n        for layer in self.layer_names:\n            raw_w = getattr(self, str(layer) + '_raw')\n            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=False)\n        if hasattr(self.module, 'reset'): self.module.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        self.transformer = self.transformer\n        self.rnns = [nn.LSTM(768 if l == 0 else 1152,\n                            (1152 if l != 2-1 else 768)//2,\n                            1, bidirectional = True, batch_first = True) for l in range(2)]\n        \n        \n        self.rnns = [WeightDropout(rnn, 0.4) for rnn in self.rnns]\n        self.rnns = nn.ModuleList(self.rnns)\n        self.hidden_dps = nn.ModuleList([RNNDropout(0.5) for l in range(2)])\n        self.out = nn.Linear(768,2)\n        self.dropout = nn.Dropout(0.3)\n        self.bs = bs\n        \n    def forward(self, input_ids, attention_mask=None):\n        self.bs =input_ids.shape[0]\n        self.reset()\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        raw_output = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]\n#         print(raw_output.shape)\n# #         hidden, _ = self.rnns(logits)\n        for l, (rnn,hid_dp) in enumerate(zip(self.rnns,self.hidden_dps)):\n            raw_output,_ = rnn(raw_output, self.hidden[l])\n            if l != 2-1:raw_output = hid_dp(raw_output)\n        output = self.out(self.dropout(raw_output))\n        return output.mean(1).squeeze()\n    def _one_hidden(self, l:int)->Tensor:\n        \"Return one hidden state.\"\n        nh = (1152 if l != 1 else 768) //2\n        return torch.zeros(2,self.bs,nh).cuda()\n    def reset(self):\n        \"Reset the hidden states.\"\n        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from transformers import RobertaTokenizer, RobertaModel\ntransformer_model = model_class.from_pretrained(\"albert-base-v2\")\n# transformer_model = RobertaModel.from_pretrained(\"roberta-base\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)\ncustom_transformer_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\n# if use_fp16: learner = learner.to_fp16()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For DistilBERT\n# list_layers = [learner.model.transformer.embeddings,\n#                learner.model.transformer.transformer.layer[0],\n#                learner.model.transformer.transformer.layer[1],\n#                learner.model.transformer.transformer.layer[2],\n#                learner.model.transformer.transformer.layer[3],\n#                learner.model.transformer.transformer.layer[4],\n#                learner.model.transformer.transformer.layer[5]]\n\n# For xlnet-base-cased\n# list_layers = [learner.model.transformer.word_embedding,\n#               learner.model.transformer.layer[0],\n#               learner.model.transformer.layer[1],\n#               learner.model.transformer.layer[2],\n#               learner.model.transformer.layer[3],\n#               learner.model.transformer.layer[4],\n#               learner.model.transformer.layer[5],\n#               learner.model.transformer.layer[6],\n#               learner.model.transformer.layer[7],\n#               learner.model.transformer.layer[8],\n#               learner.model.transformer.layer[9],\n#               learner.model.transformer.layer[10],\n#               learner.model.transformer.layer[11]]\n\n# For roberta-base\n# list_layers = [learner.model.transformer.embeddings,\n#               learner.model.transformer.encoder.layer[0],\n#               learner.model.transformer.encoder.layer[1],\n#               learner.model.transformer.encoder.layer[2],\n#               learner.model.transformer.encoder.layer[3],\n#               learner.model.transformer.encoder.layer[4],\n#               learner.model.transformer.encoder.layer[5],\n#               learner.model.transformer.encoder.layer[6],\n#               learner.model.transformer.encoder.layer[7],\n#               learner.model.transformer.encoder.layer[8],\n#               learner.model.transformer.encoder.layer[9],\n#               learner.model.transformer.encoder.layer[10],\n#               learner.model.transformer.encoder.layer[11],\n#               learner.model.transformer.encoder.layer[12],\n#               learner.model.transformer.encoder.layer[13],\n#               learner.model.transformer.encoder.layer[14],\n#               learner.model.transformer.encoder.layer[15],\n#               learner.model.transformer.encoder.layer[16],\n#               learner.model.transformer.encoder.layer[17],\n#               learner.model.transformer.encoder.layer[18],\n#               learner.model.transformer.encoder.layer[19],\n#               learner.model.transformer.encoder.layer[20],\n#               learner.model.transformer.encoder.layer[21],\n#               learner.model.transformer.encoder.layer[22],\n#               learner.model.transformer.encoder.layer[23],\n#               learner.model.transformer.pooler,\n#               learner.model.rnns]\n\n\nlist_layers = [\n    learner.model.transformer.embeddings,\n    learner.model.transformer.encoder,\n    learner.model.transformer.pooler,\n    learner.model.rnns,\n    \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.save('untrain')\nseed_all(seed)\nlearner.load('untrain');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.model.reset()\nlearner.freeze_to(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.lr_find()\nlearner.recorder.plot(skip_end=10,suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(1,max_lr=3e-4,moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.freeze_to(-2)\nlearner.lr_find()\nlearner.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-4\nlearner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.freeze_to(-3)\nlearner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.unfreeze()\n# learner.lr_find()\n# learner.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 3e-5\n# learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learner.fit_one_cycle(5, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.85, 0.95))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}