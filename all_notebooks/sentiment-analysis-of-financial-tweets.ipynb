{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This is an attempt to sentiment analysis on the financial tweets.**\n\nOur goal is :\n1. Clean the text from the file stockerbot-export1.csv \n2. Find the **polarity** for the cleaned text (i.e. **Positive(1)**, **Neutral(0)**, **Negative(-1)**)\n3. Create **Word Cloud** \n4. Creating a **sparse matrix** of all the unique words\n5. Using **Naive Bayes** for classification and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting the dataset\ndataset = pd.read_csv('../input/sentiment-analysis-on-financial-tweets/stockerbot-export1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.drop('id',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rather than removing the null url values, I just replace them with http://www.NULL.com\n\ndataset['url'] = dataset['url'].fillna('http://www.NULL.com')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the top 10 sources with the most financial tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\ndataset['source'].value_counts()[:10].plot(kind='barh',color=sns.color_palette('summer',30))\nplt.title('Source with most number of tweets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the top 10 url with the most tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\ndataset['url'].value_counts()[:10].plot(kind='barh',color=sns.color_palette('summer',30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the top 30 talked about companies in the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\ndataset['company_names'].value_counts()[:30].plot(kind='bar',color=sns.color_palette('summer',30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Cleaning the Tweets****\n\nWe will use the NLTK and re libraries to clean up the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"pat1 = r'@[A-Za-z0-9]+' # this is to remove any text with @....\npat2 = r'https?://[A-Za-z0-9./]+'  # this is to remove the urls\ncombined_pat = r'|'.join((pat1, pat2)) \npat3 = r'[^a-zA-Z]' # to remove every other character except a-z & A-Z\ncombined_pat2 = r'|'.join((combined_pat,pat3)) # we combine pat1, pat2 and pat3 to pass it in the cleaning steps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dataset['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"re.sub() will clean up the text\n\ntweets.lower() - converting text to lowercase\n\ntweets.split() - splits the sentence by each word\n\nps.stem() - converts the words to lowest degree and we also remove all the stopwords from the text (for example: this, that, etc)\n\n' '.join(tweets) - joins back the words to a sentence and separates them with a space"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncleaned_tweets = []\n\nfor i in range(0, len(dataset['text'])) :\n    tweets = re.sub(combined_pat2,' ',dataset['text'][i])\n    tweets = tweets.lower()\n    tweets = tweets.split()\n    tweets = [ps.stem(word) for word in tweets if not word in set(stopwords.words('english'))]\n    tweets = ' '.join(tweets)\n    cleaned_tweets.append(tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_tweets[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['cleaned_tweets'] = cleaned_tweets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Finding Polarity**\n\nTo find the polarity we use the **SentimentIntensityAnalyzer** from **nltk.sentiment.vader**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#nltk.download('vader_lexicon')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code will give us the polarity scores for each of the cleaned tweet \n\nFor example:\n\ncompound: 0.0, \n    neg: 0.0, \n    neu: 1.0, \n    pos: 0.0, "},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\nfor tweet in cleaned_tweets[:10]:\n    print(tweet)\n    s = sia.polarity_scores(tweet)\n    for k in sorted(s):\n        print('{0}: {1}, '.format(k, s[k]), end='')\n        print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, based on the 'compound' polarity score and the knowledge of the data, we can choose which tweet falls in the categories of Positive, Negative and Neutral"},{"metadata":{"trusted":true},"cell_type":"code","source":"def findpolarity(data):\n    sid = SentimentIntensityAnalyzer()\n    polarity = sid.polarity_scores(data)\n    if(polarity['compound'] >= 0.2):  \n        sentiment = 1\n    if(polarity['compound'] <= -0.2):\n        sentiment = -1 \n    if(polarity['compound'] < 0.2 and polarity['compound'] >-0.2):\n        sentiment = 0     \n    return(sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"findpolarity(cleaned_tweets[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment = []\nfor i in range(0, len(cleaned_tweets)):\n    s = findpolarity(cleaned_tweets[i])\n    sentiment.append(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cleaned_tweets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create a new dataframe to store the cleaned tweets and their respective polarities and save them to a .csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_sentiment = pd.DataFrame()\ntweet_sentiment['cleaned_tweets'] = cleaned_tweets\ntweet_sentiment['sentiment'] = sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_sentiment.to_csv('tweet_sentiment.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_sentiment.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word Cloud**\n\nTo create word clouds of different sentiments, I created three different lists each for positive, negative and neutral tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_tweet = []\nnegative_tweet = []\nneutral_tweet = []\n\nfor i in range(0, tweet_sentiment.shape[0]):\n    if tweet_sentiment['sentiment'][i] == 0:\n        neutral_tweet.append(tweet_sentiment['cleaned_tweets'][i])\n    elif tweet_sentiment['sentiment'][i] == 1:\n        positive_tweet.append(tweet_sentiment['cleaned_tweets'][i])\n    elif tweet_sentiment['sentiment'][i] == -1:\n        negative_tweet.append(tweet_sentiment['cleaned_tweets'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_tweet[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To create word clouds, we will first have to install word cloud (If using Jupyter Notebook)\n\n**pip install wordcloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(positive_tweet)\nshow_wordcloud(neutral_tweet)\nshow_wordcloud(negative_tweet)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use **CountVectorizer** to create a sparse matrix from the cleaned tweets and define the DV and IV for the classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(tweet_sentiment['cleaned_tweets']).toarray()\ny = tweet_sentiment['sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the classification we will use the **Naive Bayes** classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nscore = accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is the first version of the analysis and there are a lot of possibilities for improvement. I will try to do the best I can in updating this notebook as frequently as possible. **\n\n**Any feedback, support and comments will be highly appreciated. **"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}