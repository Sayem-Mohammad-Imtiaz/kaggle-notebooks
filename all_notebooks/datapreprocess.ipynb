{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-20T13:42:00.18096Z","iopub.execute_input":"2021-08-20T13:42:00.181512Z","iopub.status.idle":"2021-08-20T13:42:00.203878Z","shell.execute_reply.started":"2021-08-20T13:42:00.181388Z","shell.execute_reply":"2021-08-20T13:42:00.202941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\n\nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import stopwords, names\nfrom nltk.stem import WordNetLemmatizer\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:43:37.197353Z","iopub.execute_input":"2021-08-20T13:43:37.19778Z","iopub.status.idle":"2021-08-20T13:43:37.204636Z","shell.execute_reply.started":"2021-08-20T13:43:37.197748Z","shell.execute_reply":"2021-08-20T13:43:37.203401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataroot = os.path.join(os.path.abspath(os.path.sep), \"kaggle\", \"input\", \"librispeechtext\", \"data\")\noutroot = os.path.join(os.path.abspath(os.path.sep), \"kaggle\", \"working\", \"librispeechtext\", \"data\")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:46:42.175316Z","iopub.execute_input":"2021-08-20T13:46:42.175827Z","iopub.status.idle":"2021-08-20T13:46:42.180065Z","shell.execute_reply.started":"2021-08-20T13:46:42.175795Z","shell.execute_reply":"2021-08-20T13:46:42.179435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input and output filepaths\ntrain_clean_100_path = os.path.join(dataroot, \"train-clean-100.csv\")\ndev_clean_path = os.path.join(dataroot, \"dev-clean.csv\")\ntest_clean_path = os.path.join(dataroot, \"test-clean.csv\")\n\ntrain_clean_100_outpath = os.path.join(outroot, \"preprocessed-train-clean-100.csv\")\ndev_clean_outpath = os.path.join(outroot, \"preprocessed-dev-clean.csv\")\ntest_clean_outpath = os.path.join(outroot, \"preprocessed-test-clean.csv\")\n\nos.makedirs(outroot, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:46:45.169779Z","iopub.execute_input":"2021-08-20T13:46:45.170376Z","iopub.status.idle":"2021-08-20T13:46:45.175973Z","shell.execute_reply.started":"2021-08-20T13:46:45.170331Z","shell.execute_reply":"2021-08-20T13:46:45.1752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the csv files\ntrain_df = pd.read_csv(train_clean_100_path, index_col=0)\ndev_df = pd.read_csv(dev_clean_path, index_col=0)\ntest_df = pd.read_csv(test_clean_path, index_col=0)\n\n# https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ntrain_df =  pd.DataFrame({\"TEXT\": train_df[\"REAL TEXT\"].apply(str.lower).apply(decontracted), \"BOOK\": train_df[\"BOOK TITLE\"]})\ndev_df = pd.DataFrame({\"TEXT\": dev_df[\"TEXT\"].apply(str.lower).apply(decontracted), \"BOOK\": dev_df[\"BOOK TITLE\"]})\ntest_df = pd.DataFrame({\"TEXT\": test_df[\"TEXT\"].apply(str.lower).apply(decontracted), \"BOOK\": test_df[\"BOOK TITLE\"]})\n\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:42:32.914546Z","iopub.execute_input":"2021-08-20T13:42:32.9149Z","iopub.status.idle":"2021-08-20T13:42:33.232841Z","shell.execute_reply.started":"2021-08-20T13:42:32.91487Z","shell.execute_reply":"2021-08-20T13:42:33.231625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the books into lists of words\ntok_train_df = pd.DataFrame(train_df, copy=True)\ntok_dev_df = pd.DataFrame(dev_df, copy=True)\ntok_test_df = pd.DataFrame(test_df, copy=True)\n\ntok_train_df[\"TEXT\"] = tok_train_df[\"TEXT\"].apply(word_tokenize)\ntok_train_df[\"N_WORDS\"] = tok_train_df[\"TEXT\"].apply(len)\ntok_train_df[\"U_WORDS\"] = tok_train_df[\"TEXT\"].apply(set).apply(len)\n\ntok_dev_df[\"TEXT\"] = tok_dev_df[\"TEXT\"].apply(word_tokenize)\ntok_dev_df[\"N_WORDS\"] = tok_dev_df[\"TEXT\"].apply(len)\ntok_dev_df[\"U_WORDS\"] = tok_dev_df[\"TEXT\"].apply(set).apply(len)\n\ntok_test_df[\"TEXT\"] = tok_test_df[\"TEXT\"].apply(word_tokenize)\ntok_test_df[\"N_WORDS\"] = tok_test_df[\"TEXT\"].apply(len)\ntok_test_df[\"U_WORDS\"] = tok_test_df[\"TEXT\"].apply(set).apply(len)\n\ntok_train_df","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:42:45.170537Z","iopub.execute_input":"2021-08-20T13:42:45.170893Z","iopub.status.idle":"2021-08-20T13:42:53.693016Z","shell.execute_reply.started":"2021-08-20T13:42:45.170863Z","shell.execute_reply":"2021-08-20T13:42:53.691892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove stop words from books\nstop_train_df = pd.DataFrame(tok_train_df, copy=True)\nstop_dev_df = pd.DataFrame(tok_dev_df, copy=True)\nstop_test_df = pd.DataFrame(tok_test_df, copy=True)\n\n_more_stopwords = set([\n    # interjections\n    \"oh\", \"ah\",\n    # useless\n    \"yes\", \"no\",\n    # archaic terms: they, you, triplet, to do, you\n    \"thy\", \"thou\", \"thrin\", \"didst\", \"thee\",\n    # names\n    *map(str.lower, names.words())\n])\n_stopwords = set(stopwords.words('english')) | _more_stopwords\nstopwords_filter = partial(filter, lambda w: w not in _stopwords)\n\nstop_train_df[\"TEXT\"] = stop_train_df[\"TEXT\"].apply(stopwords_filter).apply(list)\nstop_train_df[\"N_WORDS\"] = stop_train_df[\"TEXT\"].apply(len)\nstop_train_df[\"U_WORDS\"] = stop_train_df[\"TEXT\"].apply(set).apply(len)\n\nstop_dev_df[\"TEXT\"] = stop_dev_df[\"TEXT\"].apply(stopwords_filter).apply(list)\nstop_dev_df[\"N_WORDS\"] = stop_dev_df[\"TEXT\"].apply(len)\nstop_dev_df[\"U_WORDS\"] = stop_dev_df[\"TEXT\"].apply(set).apply(len)\n\nstop_test_df[\"TEXT\"] = stop_test_df[\"TEXT\"].apply(stopwords_filter).apply(list)\nstop_test_df[\"N_WORDS\"] = stop_test_df[\"TEXT\"].apply(len)\nstop_test_df[\"U_WORDS\"] = stop_test_df[\"TEXT\"].apply(set).apply(len)\n\nstop_train_df","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:43:41.190948Z","iopub.execute_input":"2021-08-20T13:43:41.191486Z","iopub.status.idle":"2021-08-20T13:43:41.617685Z","shell.execute_reply.started":"2021-08-20T13:43:41.191438Z","shell.execute_reply":"2021-08-20T13:43:41.616597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stem the words in each book to a simpler form using a lemmatizer\nlemm_train_df = pd.DataFrame(stop_train_df, copy=True)\nlemm_dev_df = pd.DataFrame(stop_dev_df, copy=True)\nlemm_test_df = pd.DataFrame(stop_test_df, copy=True)\n\n_lemmatizer = WordNetLemmatizer()\nlemmatize = lambda tokens: [_lemmatizer.lemmatize(token, pos=pos[0].lower()) \n                            if pos[0].lower() in [\"a\", \"n\", \"v\"] else token\n                            for token, pos in pos_tag(tokens)]\n\n\nlemm_train_df[\"TEXT\"] = lemm_train_df[\"TEXT\"].apply(lemmatize).apply(list)\nlemm_train_df[\"N_WORDS\"] = lemm_train_df[\"TEXT\"].apply(len)\nlemm_train_df[\"U_WORDS\"] = lemm_train_df[\"TEXT\"].apply(set).apply(len)\n\nlemm_dev_df[\"TEXT\"] = lemm_dev_df[\"TEXT\"].apply(lemmatize).apply(list)\nlemm_dev_df[\"N_WORDS\"] = lemm_dev_df[\"TEXT\"].apply(len)\nlemm_dev_df[\"U_WORDS\"] = lemm_dev_df[\"TEXT\"].apply(set).apply(len)\n\nlemm_test_df[\"TEXT\"] = lemm_test_df[\"TEXT\"].apply(lemmatize).apply(list)\nlemm_test_df[\"N_WORDS\"] = lemm_test_df[\"TEXT\"].apply(len)\nlemm_test_df[\"U_WORDS\"] = lemm_test_df[\"TEXT\"].apply(set).apply(len)\n\nlemm_train_df","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:44:39.577715Z","iopub.execute_input":"2021-08-20T13:44:39.578062Z","iopub.status.idle":"2021-08-20T13:45:20.844678Z","shell.execute_reply.started":"2021-08-20T13:44:39.578033Z","shell.execute_reply":"2021-08-20T13:45:20.843585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_train_df = pd.DataFrame(lemm_train_df, copy=True)\nout_dev_df = pd.DataFrame(lemm_dev_df, copy=True)\nout_test_df = pd.DataFrame(lemm_test_df, copy=True)\n\nout_train_df.to_csv(train_clean_100_outpath, columns=[\"TEXT\", \"BOOK\"])\nout_dev_df.to_csv(dev_clean_outpath, columns=[\"TEXT\", \"BOOK\"])\nout_test_df.to_csv(test_clean_outpath, columns=[\"TEXT\", \"BOOK\"])","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:46:53.312941Z","iopub.execute_input":"2021-08-20T13:46:53.31356Z","iopub.status.idle":"2021-08-20T13:46:53.563387Z","shell.execute_reply.started":"2021-08-20T13:46:53.313527Z","shell.execute_reply":"2021-08-20T13:46:53.562606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'X': tok_train_df[\"BOOK\"],\n    'Y': tok_train_df[\"U_WORDS\"],\n    'Z': stop_train_df[\"U_WORDS\"],\n    'W': lemm_train_df[\"U_WORDS\"]\n})\n\n# creating subplots\nax = plt.subplots(figsize=(15, 10))\n  \n# plotting columns\nax = sns.barplot(x=df[\"X\"], y=df[\"Y\"], color='r', label=\"tok\")\nax = sns.barplot(x=df[\"X\"], y=df[\"Z\"], color='g', label=\"stop\")\nax = sns.barplot(x=df[\"X\"], y=df[\"W\"], color='b', label=\"lemm\")\n\n# renaming the axes\nax.set(xlabel=\"book\", ylabel=\"unique words\")\nax.set(xticklabels=[])\nax.set_title(\"Change in the number of unique words\")\nax.legend()\n  \n# visulaizing illustration\nplt.show()\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:45:26.151386Z","iopub.execute_input":"2021-08-20T13:45:26.151971Z","iopub.status.idle":"2021-08-20T13:45:32.690344Z","shell.execute_reply.started":"2021-08-20T13:45:26.151919Z","shell.execute_reply":"2021-08-20T13:45:32.689263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unique words after each step: tokenization, stopwords removal and lemmatization\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:45:39.051009Z","iopub.execute_input":"2021-08-20T13:45:39.051586Z","iopub.status.idle":"2021-08-20T13:45:39.078102Z","shell.execute_reply.started":"2021-08-20T13:45:39.051537Z","shell.execute_reply":"2021-08-20T13:45:39.077088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = len(set(tok_train_df[\"TEXT\"].sum()))\ny = len(set(tok_dev_df[\"TEXT\"].sum()))\nz = len(set(tok_test_df[\"TEXT\"].sum()))\n\nprint(f\"Number of unique words in the train set {x}\")\nprint(f\"Number of unique words in the dev set {y}\")\nprint(f\"Number of unique words in the test set {z}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:45:45.786651Z","iopub.execute_input":"2021-08-20T13:45:45.787232Z","iopub.status.idle":"2021-08-20T13:45:48.405053Z","shell.execute_reply.started":"2021-08-20T13:45:45.787182Z","shell.execute_reply":"2021-08-20T13:45:48.403787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = len(set(lemm_train_df[\"TEXT\"].sum()))\ny = len(set(lemm_dev_df[\"TEXT\"].sum()))\nz = len(set(lemm_test_df[\"TEXT\"].sum()))\n\nprint(f\"Number of unique words in the train set after preprocessing {x}\")\nprint(f\"Number of unique words in the dev set after preprocessing {y}\")\nprint(f\"Number of unique words in the test set after preprocessing {z}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T13:45:49.368618Z","iopub.execute_input":"2021-08-20T13:45:49.369011Z","iopub.status.idle":"2021-08-20T13:45:50.559439Z","shell.execute_reply.started":"2021-08-20T13:45:49.368977Z","shell.execute_reply":"2021-08-20T13:45:50.558699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}