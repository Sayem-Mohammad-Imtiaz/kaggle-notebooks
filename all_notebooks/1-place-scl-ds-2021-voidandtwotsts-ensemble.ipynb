{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training\n- Make inference using our finetuned models\n- Our 1st place solution used **ensembling of many models** by taking the **average of the prediction probabilities for each word** and **the entire dataset was used for training with no validation**\n\n## Steps:\n1. [Preprocessing](https://www.kaggle.com/nguyncaoduy/1-place-scl-ds-2021-voidandtwotsts-preprocess)\n2. [Training](https://www.kaggle.com/nguyncaoduy/1-place-scl-ds-2021-voidandtwotsts-train)\n3. [Ensembling](https://www.kaggle.com/nguyncaoduy/1-place-scl-ds-2021-voidandtwotsts-ensemble) - This Notebook","metadata":{}},{"cell_type":"code","source":"!pip install ohmeow-blurr==0.0.22 datasets==1.3.0 fsspec==0.8.5 -qq","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-08-22T05:06:06.220028Z","iopub.execute_input":"2021-08-22T05:06:06.220481Z","iopub.status.idle":"2021-08-22T05:06:22.76633Z","shell.execute_reply.started":"2021-08-22T05:06:06.22038Z","shell.execute_reply":"2021-08-22T05:06:22.765367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# turn off multithreading to avoid deadlock\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:22.767834Z","iopub.execute_input":"2021-08-22T05:06:22.768106Z","iopub.status.idle":"2021-08-22T05:06:22.772709Z","shell.execute_reply.started":"2021-08-22T05:06:22.768077Z","shell.execute_reply":"2021-08-22T05:06:22.77164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import *\nfrom fastai.text.all import *\n\nfrom blurr.data.all import *\nfrom blurr.modeling.all import *\n\nSEED = 42\nset_seed(SEED, True)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:22.774644Z","iopub.execute_input":"2021-08-22T05:06:22.775017Z","iopub.status.idle":"2021-08-22T05:06:34.111029Z","shell.execute_reply.started":"2021-08-22T05:06:22.774978Z","shell.execute_reply":"2021-08-22T05:06:34.110175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('../input/sclds2021preprocess/wordlist.json', 'r') as f:\n    wordlist = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.112294Z","iopub.execute_input":"2021-08-22T05:06:34.112581Z","iopub.status.idle":"2021-08-22T05:06:34.138233Z","shell.execute_reply.started":"2021-08-22T05:06:34.112554Z","shell.execute_reply":"2021-08-22T05:06:34.137352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ndf_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval}\n\nvalid_df = pd.read_csv('../input/sclds2021preprocess/valid.csv', converters=df_converters)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.141119Z","iopub.execute_input":"2021-08-22T05:06:34.141469Z","iopub.status.idle":"2021-08-22T05:06:34.839831Z","shell.execute_reply.started":"2021-08-22T05:06:34.141438Z","shell.execute_reply":"2021-08-22T05:06:34.838863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(valid_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.841382Z","iopub.execute_input":"2021-08-22T05:06:34.841671Z","iopub.status.idle":"2021-08-22T05:06:34.850205Z","shell.execute_reply.started":"2021-08-22T05:06:34.841645Z","shell.execute_reply":"2021-08-22T05:06:34.849133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = Path('../input/sclds2021train')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.851685Z","iopub.execute_input":"2021-08-22T05:06:34.851986Z","iopub.status.idle":"2021-08-22T05:06:34.861404Z","shell.execute_reply.started":"2021-08-22T05:06:34.851957Z","shell.execute_reply":"2021-08-22T05:06:34.860152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-define certain things for 'load_learner' to work\n\n@delegates()\nclass TokenCrossEntropyLossFlat(BaseLoss):\n    \"Same as `CrossEntropyLossFlat`, but for mutiple tokens output\"\n    y_int = True\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n    def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n    def decodes(self, x):    return L([ i.argmax(dim=self.axis) for i in x ])\n    def activation(self, x): return L([ F.softmax(i, dim=self.axis) for i in x ])\n\ndef get_y(inp): return [(label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels)]","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.863078Z","iopub.execute_input":"2021-08-22T05:06:34.86351Z","iopub.status.idle":"2021-08-22T05:06:34.873469Z","shell.execute_reply.started":"2021-08-22T05:06:34.863464Z","shell.execute_reply":"2021-08-22T05:06:34.872483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation\n- This is only relevant during model selection and testing\n- For the final training, full dataset is used so the accuracy below doesn't really reflect the power of the model.","metadata":{}},{"cell_type":"code","source":"@patch\ndef blurr_predict(self:Learner, items, rm_type_tfms=None):\n    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n    is_split_str = hf_before_batch_tfm.is_split_into_words and isinstance(items[0], str)\n    is_df = isinstance(items, pd.DataFrame)\n    if (not is_df and (is_split_str or not is_listy(items))): items = [items]\n    dl = self.dls.test_dl(items, rm_type_tfms=rm_type_tfms, num_workers=0)\n    with self.no_bar(): probs, _, decoded_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    trg_tfms = self.dls.tfms[self.dls.n_inp:]\n    outs = []\n    probs, decoded_preds = L(probs), L(decoded_preds)\n    for i in range(len(items)):\n        item_probs = [probs[i]]\n        item_dec_preds = [decoded_preds[i]]\n        item_dec_labels = tuplify([tfm.decode(item_dec_preds[tfm_idx]) for tfm_idx, tfm in enumerate(trg_tfms)])\n        outs.append((item_dec_labels, item_dec_preds, item_probs))\n    return outs","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.874366Z","iopub.execute_input":"2021-08-22T05:06:34.874651Z","iopub.status.idle":"2021-08-22T05:06:34.885563Z","shell.execute_reply.started":"2021-08-22T05:06:34.874624Z","shell.execute_reply":"2021-08-22T05:06:34.884711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from string import punctuation\n\ndef reconstruct(num, pred, raw_tokens, raw_address):\n    def complete_word(x):\n        y = x.strip().strip(punctuation)\n        if y != '' and y in wordlist:\n            x = x.replace(y, wordlist[y])\n        return x\n    \n    def normalize_bracket(x):\n        if '(' in x and ')' not in x:\n            x = x + ')'\n        elif ')' in x and '(' not in x:\n            x = '(' + x\n        return x\n    \n    ans = ['/'] * num\n    for idx in range(num):\n        res = pred[idx]\n        start_poi, end_poi = -1, -1\n        start_str, end_str = -1, -1\n        for i in range(len(res[0])):\n            if 'POI' in res[1][i]:\n                if start_poi == -1: start_poi = i\n                end_poi = i\n            if 'STR' in res[1][i]:\n                if start_str == -1: start_str = i\n                end_str = i\n        \n        if start_poi != -1:\n            txt1 = raw_address[idx]\n            for i in range(start_poi):\n                txt1 = txt1[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_poi, -1):\n                txt1 = txt1[:-len(raw_tokens[idx][i])].strip()\n            \n            txt1_check = ''.join(raw_tokens[idx][start_poi:end_poi + 1]).replace(' ', '')\n            assert txt1.replace(' ', '') == txt1_check\n            \n            last = len(txt1)\n            for i in range(end_poi, start_poi - 1, -1):\n                while last > 0 and txt1[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt1 = txt1[:last] + complete_word(raw_tokens[idx][i]) + txt1[last + len(raw_tokens[idx][i]):]\n        else:\n            txt1 = ''\n        \n        if start_str != -1:\n            txt2 = raw_address[idx]\n            for i in range(start_str):\n                txt2 = txt2[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_str, -1):\n                txt2 = txt2[:-len(raw_tokens[idx][i])].strip()\n            \n            txt2_check = ''.join(raw_tokens[idx][start_str:end_str + 1]).replace(' ', '')\n            assert txt2.replace(' ', '') == txt2_check\n            \n            last = len(txt2)\n            for i in range(end_str, start_str - 1, -1):\n                while last > 0 and txt2[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt2 = txt2[:last] + complete_word(raw_tokens[idx][i]) + txt2[last + len(raw_tokens[idx][i]):]\n        else:\n            txt2 = ''\n        \n        txt1 = txt1.strip(punctuation)\n        txt2 = txt2.strip(punctuation)\n        txt1 = normalize_bracket(txt1)\n        txt2 = normalize_bracket(txt2)\n        \n        ans[idx] = (txt1 + '/' + txt2)\n    \n    return ans\n\ndef reconstruct_ensemble(num, pred, raw_tokens, raw_address):\n    def complete_word(x):\n        y = x.strip().strip(punctuation)\n        if y != '' and y in wordlist:\n            x = x.replace(y, wordlist[y])\n        return x\n    \n    def normalize_bracket(x):\n        if '(' in x and ')' not in x:\n            x = x + ')'\n        elif ')' in x and '(' not in x:\n            x = '(' + x\n        return x\n    \n    ans = ['/'] * num\n    for idx in range(num):\n        res = pred[idx]\n        start_poi, end_poi = -1, -1\n        start_str, end_str = -1, -1\n        for i in range(len(res)):\n            if 'POI' in res[i]:\n                if start_poi == -1: start_poi = i\n                end_poi = i\n            if 'STR' in res[i]:\n                if start_str == -1: start_str = i\n                end_str = i\n        \n        if start_poi != -1:\n            txt1 = raw_address[idx]\n            for i in range(start_poi):\n                txt1 = txt1[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_poi, -1):\n                txt1 = txt1[:-len(raw_tokens[idx][i])].strip()\n            \n            txt1_check = ''.join(raw_tokens[idx][start_poi:end_poi + 1]).replace(' ', '')\n            assert txt1.replace(' ', '') == txt1_check\n            \n            last = len(txt1)\n            for i in range(end_poi, start_poi - 1, -1):\n                while last > 0 and txt1[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[i]:\n                    txt1 = txt1[:last] + complete_word(raw_tokens[idx][i]) + txt1[last + len(raw_tokens[idx][i]):]\n        else:\n            txt1 = ''\n        \n        if start_str != -1:\n            txt2 = raw_address[idx]\n            for i in range(start_str):\n                txt2 = txt2[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_str, -1):\n                txt2 = txt2[:-len(raw_tokens[idx][i])].strip()\n            \n            txt2_check = ''.join(raw_tokens[idx][start_str:end_str + 1]).replace(' ', '')\n            assert txt2.replace(' ', '') == txt2_check\n            \n            last = len(txt2)\n            for i in range(end_str, start_str - 1, -1):\n                while last > 0 and txt2[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[i]:\n                    txt2 = txt2[:last] + complete_word(raw_tokens[idx][i]) + txt2[last + len(raw_tokens[idx][i]):]\n        else:\n            txt2 = ''\n        \n        txt1 = txt1.strip(punctuation)\n        txt2 = txt2.strip(punctuation)\n        txt1 = normalize_bracket(txt1)\n        txt2 = normalize_bracket(txt2)\n        \n        ans[idx] = (txt1 + '/' + txt2)\n    \n    return ans","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.886935Z","iopub.execute_input":"2021-08-22T05:06:34.887438Z","iopub.status.idle":"2021-08-22T05:06:34.929513Z","shell.execute_reply.started":"2021-08-22T05:06:34.887407Z","shell.execute_reply":"2021-08-22T05:06:34.928587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_acc(df):\n    return df.loc[valid_df['pred'] == df['POI/street'], 'id'].count() / len(df)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.930959Z","iopub.execute_input":"2021-08-22T05:06:34.931423Z","iopub.status.idle":"2021-08-22T05:06:34.946313Z","shell.execute_reply.started":"2021-08-22T05:06:34.931362Z","shell.execute_reply":"2021-08-22T05:06:34.945294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_tokens = list(valid_df['tokens'])\nraw_address = list(valid_df['raw_address'])","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.948007Z","iopub.execute_input":"2021-08-22T05:06:34.948449Z","iopub.status.idle":"2021-08-22T05:06:34.95849Z","shell.execute_reply.started":"2021-08-22T05:06:34.948403Z","shell.execute_reply":"2021-08-22T05:06:34.957638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_avg_pred = []\n\nfor model in model_path.ls():\n    learn = load_learner(model)\n    raw_pred = learn.blurr_predict_tokens(raw_tokens)\n    raw_avg_pred.append([raw[3] for raw in raw_pred])\n    pred = reconstruct(len(valid_df), raw_pred, raw_tokens, raw_address)\n    valid_df['pred'] = pred\n    score = calc_acc(valid_df)\n    print(f'{model.name} - {score:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:06:34.959633Z","iopub.execute_input":"2021-08-22T05:06:34.959945Z","iopub.status.idle":"2021-08-22T05:13:54.077937Z","shell.execute_reply.started":"2021-08-22T05:06:34.959917Z","shell.execute_reply":"2021-08-22T05:13:54.076748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_ensemble_pred = [(sum(col))/len(col) for col in zip(*raw_avg_pred)]\nraw_ensemble_pred = [pred.argmax(-1) for pred in raw_ensemble_pred]\nraw_ensemble_pred = learn.dls.vocab.map_ids(raw_ensemble_pred)\npred = reconstruct_ensemble(len(valid_df), raw_ensemble_pred, raw_tokens, raw_address)\nvalid_df['pred'] = pred\nscore = calc_acc(valid_df)\nprint(f'Ensemble - {score:.5f}')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T05:13:54.079508Z","iopub.execute_input":"2021-08-22T05:13:54.079821Z","iopub.status.idle":"2021-08-22T05:13:54.220337Z","shell.execute_reply.started":"2021-08-22T05:13:54.07979Z","shell.execute_reply":"2021-08-22T05:13:54.219517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean(s):\n    res = re.sub(r'(\\w)(\\()(\\w)', '\\g<1> \\g<2>\\g<3>', s)\n    res = re.sub(r'(\\w)([),.:;]+)(\\w)', '\\g<1>\\g<2> \\g<3>', res)\n    res = re.sub(r'(\\w)(\\.\\()(\\w)', '\\g<1>. (\\g<3>', res)\n    res = re.sub(r'\\s+', ' ', res)\n    res = res.strip()\n    return res","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:48:35.507588Z","iopub.execute_input":"2021-08-21T12:48:35.508085Z","iopub.status.idle":"2021-08-21T12:48:35.514555Z","shell.execute_reply.started":"2021-08-21T12:48:35.508046Z","shell.execute_reply":"2021-08-21T12:48:35.513036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/scl-2021-ds/test.csv')\ntest_df['raw_address'] = test_df['raw_address'].apply(lambda x: x.strip())\ntest_df['tokens'] = test_df['raw_address'].apply(clean).str.split()\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:48:35.516263Z","iopub.execute_input":"2021-08-21T12:48:35.517316Z","iopub.status.idle":"2021-08-21T12:48:35.655668Z","shell.execute_reply.started":"2021-08-21T12:48:35.517256Z","shell.execute_reply":"2021-08-21T12:48:35.654444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_tokens = list(test_df['tokens'])\nraw_address = list(test_df['raw_address'])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:48:35.657229Z","iopub.execute_input":"2021-08-21T12:48:35.65774Z","iopub.status.idle":"2021-08-21T12:48:35.662601Z","shell.execute_reply.started":"2021-08-21T12:48:35.657705Z","shell.execute_reply":"2021-08-21T12:48:35.661439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_avg_pred = []\n\nfor model in model_path.ls():\n    learn = load_learner(model)\n    raw_pred = learn.blurr_predict_tokens(raw_tokens)\n    raw_avg_pred.append([raw[3] for raw in raw_pred])","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:48:35.663892Z","iopub.execute_input":"2021-08-21T12:48:35.664342Z","iopub.status.idle":"2021-08-21T12:49:18.317513Z","shell.execute_reply.started":"2021-08-21T12:48:35.664307Z","shell.execute_reply":"2021-08-21T12:49:18.315973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_ensemble_pred = [(sum(col))/len(col) for col in zip(*raw_avg_pred)]\nraw_ensemble_pred = [pred.argmax(-1) for pred in raw_ensemble_pred]\nraw_ensemble_pred = learn.dls.vocab.map_ids(raw_ensemble_pred)\npred = reconstruct_ensemble(len(test_df), raw_ensemble_pred, raw_tokens, raw_address)\ntest_df['POI/street'] = pred","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:49:18.319321Z","iopub.execute_input":"2021-08-21T12:49:18.319662Z","iopub.status.idle":"2021-08-21T12:49:18.344154Z","shell.execute_reply.started":"2021-08-21T12:49:18.319631Z","shell.execute_reply":"2021-08-21T12:49:18.342761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop(columns=['raw_address', 'tokens'], inplace=True)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:49:18.345912Z","iopub.execute_input":"2021-08-21T12:49:18.346321Z","iopub.status.idle":"2021-08-21T12:49:18.360499Z","shell.execute_reply.started":"2021-08-21T12:49:18.346288Z","shell.execute_reply":"2021-08-21T12:49:18.359109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-21T12:49:18.361808Z","iopub.execute_input":"2021-08-21T12:49:18.362139Z","iopub.status.idle":"2021-08-21T12:49:18.370895Z","shell.execute_reply.started":"2021-08-21T12:49:18.362107Z","shell.execute_reply":"2021-08-21T12:49:18.369674Z"},"trusted":true},"execution_count":null,"outputs":[]}]}