{"cells":[{"metadata":{},"cell_type":"markdown","source":"Note: This Red-Wine Analysis using K-Means is just one part of a group project which I had done together with my team-mates. For full analysis using different machine learning models - please refer to my another notebook \"Red-Wine Analysis (Full)\".","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import libraries \n\n#structures\nimport numpy as np\nimport pandas as pd\n\n#visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#get model duration\nimport time\nfrom datetime import date\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Description of data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load dataset\ndata = '../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv'\ndataset = pd.read_csv(data)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The red wine data consists of 1599 rows and 12 columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing data\ndataset.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for unreasonable data\ndataset.applymap(np.isreal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns_plot = sns.pairplot(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns_plot = sns.distplot(dataset['quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#set x and y\nfrom sklearn.preprocessing import StandardScaler\n\nX = dataset.iloc[:,0:11]\ny = dataset['quality']\n\n#stadardize data\nX_scaled = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Feature extraction: Principal component analysis\n2. Feature selection: Pearson's correlation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Principal component analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=6)\npc_X = pca.fit_transform(X_scaled)\npc_columns = ['pc1','pc2','pc3','pc4','pc5','pc6']\nprint(pca.explained_variance_ratio_.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Pearson's Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlation map\ncorr_mat=dataset.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualise data\nplt.figure(figsize=(13,5))\nsns_plot=sns.heatmap(data=corr_mat, annot=True, cmap='GnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a correlation of 0.6 to -0.5 as benchmark, a correlation matrix has been created to sieve out features that are highly correlated to the quality of red wine. Our results show that all features are within the acceptable range of 0.6 to -0.5.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the heatmap, it can be seen that most features are weakly correlated to the quality of wine the exception of alcohol (0.48) which is a moderate correlation.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Direction of relationship** <br>\nAcidity (-0.39), chlorides (-0.13), free sulfur dioxide (-0.051), total sulfur dioxide (-0.19), density (-0.17) and PH (-0.058) are negatively correlated to the quality of wine; as these variables decrease, the quality of wine will increase vice versa. <br> <br>\n\nConversely, fixed acidity (0.12), citric acid, residual sugar (0.014), sulphates (0.25) and alcohol (0.48) are positively correlated to the quality of wine; as these variables increase, the quality of wine improves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for highly correlated values to be removed\ntarget = 'quality'\ncandidates = corr_mat.index[\n    (corr_mat[target] > 0.5) | (corr_mat[target] < -0.5)\n].values\ncandidates = candidates[candidates != target]\nprint('Correlated to', target, ': ', candidates)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-Means (Without PCA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import libraries\nfrom sklearn.metrics import f1_score\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this model, the entire dataset has been used as a training data. <br>\nThen an elbow method will be used to find out an optimal number of “K” clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#try to find optimal k using the elbow method\nwcss = []\nfor i in range(1,11):\n    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300, n_init=12, random_state=0)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\nf3, ax = plt.subplots(figsize=(8, 6))\nplt.plot(range(1,11),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"“K” value of 2 will be used as a dip can be seen around 2 which is our elbow in a graph above. <br> <br>\n\nFirst, clustering will be performed with K-Means on dataset without applying principle component analysis (PCA).\nNote that the total dimension of dataset is 11.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(X_scaled)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training Time – 0.072 seconds","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization of Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(X_scaled[clusters==i,0],\n               X_scaled[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that clusters are not well separated. Some members of Cluster 2 can be seen in Cluster 1 and vice versa.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization of Clustering in 3D Plot (Fixed Acidity, Residual Sugar, Alcohol)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualise the clusterds considerig fixed acidity, residual sugar, and alcohol\nfig = plt.figure(figsize=(20, 15))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=15, azim=40)\n\nax.scatter(X_scaled[:,0], X_scaled[:,3], X_scaled[:,10],c=y, edgecolor='k')\nax.set_xlabel('Acidity')\nax.set_ylabel('Sugar')\nax.set_zlabel('Alcohol')\nax.set_title('K=2: Acidity, Sugar, Alcohol', size=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the silhouette score of the model will be measured. The silhouette score ranges from -1 to +1. <br>\nThe high silhouette score indicates that the objects are well matched to its own cluster and not to its neighbouring clusters. <br>\n(The higher the silhouette score – the better the clustering)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nfrom sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nmetrics.silhouette_score(X_scaled, labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The silhouette score obtained is considered low. It means clusters are neither dense nor well separated. <br>\nNext, let’s measure the inertia value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An extremely high inertia value of 14330.119 was obtained. It is an indicative of the “curse of dimensionality”. <br>\nWe are using 11 dimensions of data in this model. <br>\nIn this case, we will explore the model again using PCA (principle component analysis).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## K-means with PCA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our purpose of applying principal component analysis is to reduce dimension. <br>\nIn this dataset, we reduced the 11-dimensional data to 6-dimensional data during PCA.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying kmeans to the dataset, set k=2\nkmeans = KMeans(n_clusters = 2)\nstart_time = time.time()\nclusters = kmeans.fit_predict(pc_X)\ntoday = date.today()\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training time – 0.062 seconds Training time is observed to have reduced slightly.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization of Clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#2D plot\ncolors = 'rgbkcmy'\nfor i in np.unique(clusters):\n    plt.scatter(pc_X[clusters==i,0],\n               pc_X[clusters==i,1],\n               color=colors[i], label='Cluster' + str(i+1))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After implementing PCA, it can be seen that clustering is improved. So it is expected to see a higher silhouette score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluate model\nmetrics.silhouette_score(pc_X, labels, metric='euclidean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, we can see an improvement in the silhouette score. But it is still considered low which means there are still some overlapping of clusters or incorrect grouping. <br>\n\nAlthough the silhouette score increased with PCA, it still low; clusters are overlapping or incorrectly grouped.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.inertia_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The inertia value is also decreased but still extremely high.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"K-means clustering has poor clustering result for high dimensional data. Even with the implementation of PCA, the silhouette score can only be improved to some extent but is considered low. Also the inertia value is observed to be extremely high. In an ideal situation, the inertia value should be as low as possible. Hence, we can conclude that this is not a good model fit to the data.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}