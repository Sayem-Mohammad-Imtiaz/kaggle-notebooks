{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ec01a474-346f-79e3-8d0e-cfdfd2b8ab22"},"source":"I am trying to find some hidden causality patterns in the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8436920-ea95-b057-bcb5-35b82ed215dd"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ca7412a-7ef6-3aac-9efd-e6a301110cf1"},"outputs":[],"source":"# Read the data\ndata=pd.read_csv(\"../input/diabetes.csv\")\n\ndata.shape\n#768,9\n\ndata.columns.values\n#Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome\n\n# First let us find out if there are any unknowns etc. We will have to impute them\ndata.isnull().values.any()\n\ncolumnNames=list(data.columns.values)\ncolumnNames.remove('Outcome')\n\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=5)\npca.fit(data[columnNames])\npca.explained_variance_ratio_\n\nfrom mpl_toolkits.mplot3d import Axes3D\nplot3D = plt.figure().gca(projection='3d')\nplt1=plot3D.scatter(data[data['Outcome']==0]['Pregnancies'],data[data['Outcome']==0]['Glucose'],\n                    data[data['Outcome']==0]['BloodPressure'],c='r',\n                    label='0')\nplt2=plot3D.scatter(data[data['Outcome']==1]['Pregnancies'],data[data['Outcome']==1]['Glucose'],\n                    data[data['Outcome']==1]['BloodPressure'],c='b',\n                    label='1')\nplot3D.set_xlabel('Pregnancies')\nplot3D.set_ylabel('Glucose')\nplot3D.set_zlabel('BloodPressure')\nplt.legend()\nplt.show()\n\n# Let us now plot the same using the Principal components\nplt.figure()\ndataPCA=data\nratios=pca.components_\ndataPCA['PCA1']=(ratios[0][0] * dataPCA['Pregnancies']) + (ratios[0][1] * dataPCA['Glucose']) + (ratios[0][2] * dataPCA['BloodPressure'])\ndataPCA['PCA2']=(ratios[1][0] * dataPCA['Pregnancies']) + (ratios[1][1] * dataPCA['Glucose']) + (ratios[1][2] * dataPCA['BloodPressure'])\ndataPCA['PCA3']=(ratios[2][0] * dataPCA['Pregnancies']) + (ratios[2][1] * dataPCA['Glucose']) + (ratios[2][2] * dataPCA['BloodPressure'])\n\nplot3D = plt.figure().gca(projection='3d')\nplt1=plot3D.scatter(dataPCA[dataPCA['Outcome']==0]['PCA1'],dataPCA[dataPCA['Outcome']==0]['PCA2'],\n                    dataPCA[dataPCA['Outcome']==0]['PCA3'],c='r',\n                    label='0')\nplt2=plot3D.scatter(dataPCA[dataPCA['Outcome']==1]['PCA1'],dataPCA[dataPCA['Outcome']==1]['PCA2'],\n                    dataPCA[dataPCA['Outcome']==1]['PCA3'],c='b',\n                    label='1')\nplot3D.set_xlabel('PCA1')\nplot3D.set_ylabel('PCA2')\nplot3D.set_zlabel('PCA3')\nplt.legend()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07d88dfa-3312-e17e-ba48-c885ebf8c80b"},"outputs":[],"source":"# SCATTER PLOT OF ALL THE FEATURES SPLIT BY OUTCOME. A GOOD WAY OF IDENITFYING A STRONG PATTERN IF ANY\ndata=pd.read_csv(\"../input/diabetes.csv\")\ncolnames=list(data.columns.values)\n# Remove the target variable\ncolnames.remove('Outcome')\narr1=np.linspace(1,len(colnames),len(colnames))\n\nfig=plt.figure(figsize=(30,20))\nfig.subplots_adjust(hspace=1,wspace=1)\ncounter=1\nimport itertools\nfor p in itertools.permutations(arr1,2):\n    plt.subplot(len(colnames),len(colnames),counter)\n    plt.scatter(data.ix[:,int(p[0])-1].values,data.ix[:,int(p[1])-1].values,c=data['Outcome'],alpha=0.5)\n    plt.xlabel(colnames[int(p[0])-1])\n    plt.ylabel(colnames[int(p[1])-1])\n    counter=counter + 1\n    \nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c79a2c6d-0805-298c-c76f-6e1bdd853b39"},"outputs":[],"source":"# CORRELATION PLOT OF THE FEATURES\nfrom matplotlib.collections import EllipseCollection\n\ndef plot_corr_ellipses(data, ax=None, **kwargs):\n    M = np.array(data)\n    if not M.ndim == 2:\n        raise ValueError('data must be a 2D array')\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, subplot_kw={'aspect':'equal'})\n        ax.set_xlim(-0.5, M.shape[1] - 0.5)\n        ax.set_ylim(-0.5, M.shape[0] - 0.5)\n\n    # xy locations of each ellipse center\n    xy = np.indices(M.shape)[::-1].reshape(2, -1).T\n\n    # set the relative sizes of the major/minor axes according to the strength of\n    # the positive/negative correlation\n    w = np.ones_like(M).ravel()\n    h = 1 - np.abs(M).ravel()\n    a = 45 * np.sign(M).ravel()\n\n    ec = EllipseCollection(widths=w, heights=h, angles=a, units='x', offsets=xy,\n                           transOffset=ax.transData, array=M.ravel(), **kwargs)\n    ax.add_collection(ec)\n\n    # if data is a DataFrame, use the row/column names as tick labels\n    if isinstance(data, pd.DataFrame):\n        ax.set_xticks(np.arange(M.shape[1]))\n        ax.set_xticklabels(data.columns, rotation=90)\n        ax.set_yticks(np.arange(M.shape[0]))\n        ax.set_yticklabels(data.index)\n\n    return ec\n\ndata=pd.read_csv(\"../input/diabetes.csv\")\ndata = data.corr()\nfig, ax = plt.subplots(1, 1)\nm = plot_corr_ellipses(data, ax=ax, cmap='Reds')\ncb = fig.colorbar(m)\ncb.set_label('Correlation coefficient')\nax.margins(0.1)\n\n# There are some good inferences from this correlation chart\n# AGE and PREGNANCIES are very much correlated\n# GLUCOSE has a very high explanatory power on OUTCOME\n# SKINTHICKNESS and AGE have no correlation in this data set which was weird\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84269dc6-75c6-c1f8-dd4e-18addda58414"},"outputs":[],"source":"# Let us now do a SVC on different feature selection techniques\n# ICA ( Independent Component Analysis)\ndata=pd.read_csv(\"../input/diabetes.csv\")\ncolnames=list(data.columns.values)\n# Remove the target variable\ncolnames.remove('Outcome')\n\n# Independent Component Analysis\nfrom sklearn.decomposition import FastICA\nfastICA=FastICA(n_components=3)\nfastICA.fit(data[colnames])\n\n# So if we go by ICA, then the 3 new features will be\nmat2=np.array(fastICA.components_)\nmat2=mat2.T\nnewData=np.dot(np.array(data[colnames]),mat2)\nnewData=pd.DataFrame(newData)\nnewData['Outcome']=data['Outcome']\n\n# Let us now split it into test train and check the prediction power. We will use \nfrom sklearn import linear_model\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.svm import LinearSVC\nsvc=LinearSVC(random_state=0)\n# Train the model using the training sets\nnewData.columns=['ICA1','ICA2','ICA3','Outcome']\nX_train,X_test,Y_train,Y_test=train_test_split(newData[['ICA1','ICA2','ICA3']],newData['Outcome'],train_size=0.4,random_state=42)\nsvc.fit(X_train,Y_train)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(Y_test,svc.predict(X_test))\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}