{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n        \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd                 #for working with data in Python\nimport numpy as np\nimport matplotlib.pyplot as plt     #for visualization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use Pandas to read in csv files. The pd.read_csv() method creates a DataFrame from a csv file\n#train = pd.read_csv('../input/house-prices-dataset/train.csv')\n#test = pd.read_csv('../input/house-prices-dataset/test.csv')\n\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\nprint(\"1 \\n\")\n\n#check out the size of the data\nprint (\"Train data shape:\", train.shape)\nprint (\"Test data shape:\", test.shape)\n\nprint(\"2 \\n\")\n\n#look at a few rows using the DataFrame.head() method\n#train.head()\nprint(train.head())\n\n#to do some plotting\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################################################\n#  2. Explore the data and engineer Features          ###\n#######################################################\n\nprint(\"3. \\n\")\n\n#to get more information like count, mean, std, min, max etc\n#train.SalePrice.describe()\nprint (train.SalePrice.describe())\n\n#to plot a histogram of SalePrice\nprint (\"\\n4. Skew is:\", train.SalePrice.skew())\nplt.hist(train.SalePrice, color='blue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use np.log() to transform train.SalePric and calculate the skewness a second time, as well as re-plot the data\ntarget = np.log(train.SalePrice)\nprint (\"\\n5. Skew is:\", target.skew())\nplt.hist(target, color='blue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################################################\n#   Working with Numeric Features                   ###\n#######################################################\n\nprint(\"6 \\n\")\n\n#return a subset of columns matching the specified data types\nnumeric_features = train.select_dtypes(include=[np.number])\n#numeric_features.dtypes\nprint(numeric_features.dtypes)\n\nprint(\"7 \\n\")\n\n#displays the correlation between the columns and examine the correlations between the features and the target.\ncorr = numeric_features.corr()\n\n#The first five features are the most positively correlated with SalePrice, while the next five are the most negatively correlated.\nprint (corr['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"8 \\n\")\n\n#to get the unique values that a particular column has.\n#train.OverallQual.unique()\nprint(train.OverallQual.unique())\n\nprint(\"9 \\n\")\n\n#investigate the relationship between OverallQual and SalePrice.\n#We set index='OverallQual' and values='SalePrice'. We chose to look at the median here.\nquality_pivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\nprint(quality_pivot)\n\n\n#visualize this pivot table more easily, we can create a bar plot\n#Notice that the median sales price strictly increases as Overall Quality increases.\nquality_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to generate some scatter plots and visualize the relationship between the Ground Living Area(GrLivArea) and SalePrice\nplt.scatter(x=train['GrLivArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Above grade (ground) living area square feet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(x=train['GarageArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#######################################################\n# create a new dataframe with some outliers removed ###\n#######################################################\n\n#create a new dataframe with some outliers removed\ntrain = train[train['GarageArea'] < 1200]\n\n#display the previous graph again without outliers\nplt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))\nplt.xlim(-200,1600) # This forces the same scale as before\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################\n#   Handling Null Values                            ##\n######################################################\n\n#create a DataFrame to view the top null columns and return the counts of the null values in each column\nnulls = pd.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])\nnulls.columns = ['Null Count']\nnulls.index.name = 'Feature'\n#nulls\nprint(nulls)\n\nprint(\"15 \\n\")\n\n#to return a list of the unique values\nprint (\"Unique values are:\", train.MiscFeature.unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################\n#   Wrangling the non-numeric Features              ##\n######################################################\n\n#consider the non-numeric features and display details of columns\ncategoricals = train.select_dtypes(exclude=[np.number])\n#categoricals.describe()\nprint(categoricals.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################\n#   Transforming and engineering features           ##\n######################################################\n\n#When transforming features, it's important to remember that any transformations that you've applied to the training data before\n# fitting the model must be applied to the test data.\n\n#Eg:\nprint (\"Original: \\n\")\nprint (train.Street.value_counts(), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#our model needs numerical data, so we will use one-hot encoding to transform the data into a Boolean column.\n#create a new column called enc_street. The pd.get_dummies() method will handle this for us\ntrain['enc_street'] = pd.get_dummies(train.Street, drop_first=True)\ntest['enc_street'] = pd.get_dummies(train.Street, drop_first=True)\n\nprint ('Encoded: \\n')\nprint (train.enc_street.value_counts())  #Pave and Grvl values converted into 1 and 0\n\n#look at SaleCondition by constructing and plotting a pivot table, as we did above for OverallQual\ncondition_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()\n\n#encode this SaleCondition as a new feature by using a similar method that we used for Street above\ndef encode(x): return 1 if x == 'Partial' else 0\ntrain['enc_condition'] = train.SaleCondition.apply(encode)\ntest['enc_condition'] = test.SaleCondition.apply(encode)\n\n#explore this newly modified feature as a plot.\ncondition_pivot = train.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Encoded Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################################################################\n#   Dealing with missing values                                                                      #\n#   We'll fill the missing values with an average value and then assign the results to data          #\n#   This is a method of interpolation                                                                #\n######################################################################################################\ndata = train.select_dtypes(include=[np.number]).interpolate().dropna()\n\n#Check if the all of the columns have 0 null values.\n#sum(data.isnull().sum() != 0)\nprint(sum(data.isnull().sum() != 0))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################\n#  3. Build a linear model                             ##\n######################################################\n\n#separate the features and the target variable for modeling.\n# We will assign the features to X and the target variable(Sales Price)to y.\n\ny = np.log(train.SalePrice)\nX = data.drop(['SalePrice', 'Id'], axis=1)\n#exclude ID from features since Id is just an index with no relationship to SalePrice.\n\n#======= partition the data ===================================================================================================#\n#   Partitioning the data in this way allows us to evaluate how our model might perform on data that it has never seen before.\n#   If we train the model on all of the test data, it will be difficult to tell if overfitting has taken place.\n#==============================================================================================================================#\n#also state how many percentage from train data set, we want to take as test data set\n#In this example, about 33% of the data is devoted to the hold-out set.\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n\n\n#========= Begin modelling =========================#\n#    Linear Regression Model                        #\n#===================================================#\n\n#----    first create a Linear Regression model.\n# First, we instantiate the model.\nlr = linear_model.LinearRegression()\n\n#----   fit the model / Model fitting\n#lr.fit() method will fit the linear regression on the features and target variable that we pass.\nmodel = lr.fit(X_train, y_train)\n\n#----   Evaluate the performance and visualize results\n#r-squared value is a measure of how close the data are to the fitted regression line\n# a higher r-squared value means a better fit(very close to value 1)\nprint (\"R^2 is: \\n\", model.score(X_test, y_test))\n\n#use the model we have built to make predictions on the test data set.\npredictions = model.predict(X_test)\n\n#calculates the rmse\nprint ('RMSE is: \\n', mean_squared_error(y_test, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view this relationship between predictions and actual_values graphically with a scatter plot.\nactual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.75,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#====== improve the model ================================================================#\n#  try using Ridge Regularization to decrease the influence of less important features    #\n#=========================================================================================#\n\n#experiment by looping through a few different values of alpha, and see how this changes our results.\n\nfor i in range (-2, 3):\n    alpha = 10**i\n    rm = linear_model.Ridge(alpha=alpha)\n    ridge_model = rm.fit(X_train, y_train)\n    preds_ridge = ridge_model.predict(X_test)\n\n    plt.scatter(preds_ridge, actual_values, alpha=.75, color='b')\n    plt.xlabel('Predicted Price')\n    plt.ylabel('Actual Price')\n    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n                    ridge_model.score(X_test, y_test),\n                    mean_squared_error(y_test, preds_ridge))\n    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')\n    plt.show()\n\n# if you examined the plots you can see these models perform almost identically to the first model.\n# In our case, adjusting the alpha did not substantially improve our model.\n\nprint (\"R^2 is: \\n\", model.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######################################################\n#    4.  Make a submission                          ##\n######################################################\n\n#create a csv that contains the predicted SalePrice for each observation in the test.csv dataset.\nsubmission = pd.DataFrame()\n#The first column must the contain the ID from the test data.\nsubmission['Id'] = test.Id\n\n#select the features from the test data for the model as we did above.\nfeats = test.select_dtypes(\n    include=[np.number]).drop(['Id'], axis=1).interpolate()\n\n#generate our predictions.\npredictions = model.predict(feats)\n\n#transform the predictions to the correct form.\n#apply np.exp() to our predictions becasuse we have taken the logarithm(np.log()) previously.\nfinal_predictions = np.exp(predictions)\n\n#check the difference\nprint(\"Original predictions are: \\n\", predictions[:1549], \"\\n\")\nprint(\"Final predictions are: \\n\", final_predictions[:1549])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assign these predictions\nsubmission['SalePrice'] = final_predictions\n#submission.head()\nprint(submission.head(1459))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#export to a .csv file as Kaggle expects.\n# pass index=False because Pandas otherwise would create a new index for us.\nsubmission.to_csv('submission1.csv', index=False)\n\n\nprint(\"\\n Finish\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}