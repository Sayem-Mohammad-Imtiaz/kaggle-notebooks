{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-18T12:14:47.558438Z","iopub.execute_input":"2021-06-18T12:14:47.558719Z","iopub.status.idle":"2021-06-18T12:14:47.566214Z","shell.execute_reply.started":"2021-06-18T12:14:47.558692Z","shell.execute_reply":"2021-06-18T12:14:47.565183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Neural Langauge Model \nIn this notebook I learned how to create language model from scratch. I followed UMASS Lec of Mohit Iyyer 365. [Lecture](https://www.youtube.com/watch?v=pZt2TLSl7L4&list=PLWnsVgP6CzadmQX6qevbar3_vDBioWHJL&index=6)\nIn this note book I followed below steps:\n\n1. Created a sentences \n2. Created vocabulary from it with idx \n3. Build a simple neural Network with help of pytorch \n4. Did predictions like predicting next word (3rd since I am using 3 words sentence) \n","metadata":{}},{"cell_type":"code","source":"# cross entropy loss : it is used in NLMS as well as other classification tasks \n# what is a loss function \n# intuitively : telss us how bad a model is doing at predicting the training data \n# in NLM how bad is the model at predicting the next word \n# assume we have a training ex \n# \"Stuedents opened their\"=> \"books\"\n# p(\"books\"| \"students open their\") we want to maximise this probability in our training data \n# we want to minimize the negative log probabilities \n# L = -log(p(books | students opened their))\n\n# why this called cross entroy loss ? \n# cross entory of two distributoins let say p & q \n# quantifies distance between the distributions \n# H(p, q) = -(summation(p(w)logq(w))) this is we can treat as ground truth distribiton \n# p(w) is 1 for books and 0 for every other type \n# q(w) is predicted conditiional probs \n#\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:47.569944Z","iopub.execute_input":"2021-06-18T12:14:47.570179Z","iopub.status.idle":"2021-06-18T12:14:47.575023Z","shell.execute_reply.started":"2021-06-18T12:14:47.570157Z","shell.execute_reply":"2021-06-18T12:14:47.574178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = ['mohit likes icecream', 'Starks were cool', 'life is fool', 'batman is ironman']\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:47.798819Z","iopub.execute_input":"2021-06-18T12:14:47.799144Z","iopub.status.idle":"2021-06-18T12:14:47.803875Z","shell.execute_reply.started":"2021-06-18T12:14:47.799116Z","shell.execute_reply":"2021-06-18T12:14:47.802894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will try to predict next word using NLM \n\n# step one : Tokenization \n\nvocab = {} # map from word type to index \ninputs = [] # stores an indexified version of each sentences \n\n\nfor sent in sentences:\n    sent_idxes = []\n    sent = sent.split() #tokenize w/ whitespace \n    \n    for w in sent: \n        if w not in vocab: \n            vocab[w] = len(vocab) # add new type to the vocab \n            \n        sent_idxes.append(vocab[w])\n    \n    inputs.append(sent_idxes)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:48.278974Z","iopub.execute_input":"2021-06-18T12:14:48.279299Z","iopub.status.idle":"2021-06-18T12:14:48.28462Z","shell.execute_reply.started":"2021-06-18T12:14:48.279264Z","shell.execute_reply":"2021-06-18T12:14:48.283447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vocab)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:48.518603Z","iopub.execute_input":"2021-06-18T12:14:48.518986Z","iopub.status.idle":"2021-06-18T12:14:48.524512Z","shell.execute_reply.started":"2021-06-18T12:14:48.518953Z","shell.execute_reply":"2021-06-18T12:14:48.523377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(inputs)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:48.964024Z","iopub.execute_input":"2021-06-18T12:14:48.964331Z","iopub.status.idle":"2021-06-18T12:14:48.968825Z","shell.execute_reply.started":"2021-06-18T12:14:48.964303Z","shell.execute_reply":"2021-06-18T12:14:48.967762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \n\n# 1 convert to long tensors \n# 2 define inpts and outpts\nprefixes = torch.LongTensor([sent[:-1] for sent in inputs])\n#print(prefixes)\nlabels = torch.LongTensor([sent[-1] for sent in inputs])\nprint('Prefix',prefixes,'Labels', labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:49.214699Z","iopub.execute_input":"2021-06-18T12:14:49.215068Z","iopub.status.idle":"2021-06-18T12:14:50.331772Z","shell.execute_reply.started":"2021-06-18T12:14:49.215036Z","shell.execute_reply":"2021-06-18T12:14:50.330805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# onto defining the network \nimport torch.nn as nn\n\nclass NLM(nn.Module):\n    # important things \n    # 1. write init functions (initializes all teh paarameters of the network )\n    # 2. forward functions (defines the forward propagation computations)\n    \n    \n    def __init__(self, d_embedding, d_hidden, window_size, len_vocab):\n        super(NLM, self).__init__() # init the base module class \n        self.d_emb = d_embedding\n        self.embeddings = nn.Embedding(len_vocab, d_embedding)\n        \n        # concatenated embeddings > hidden\n        self.W_hid = nn.Linear(d_embedding * window_size, d_hidden)\n        \n        # hidden > output probability distribution over vocab \n        self.W_out = nn.Linear(d_hidden, len_vocab)\n        \n        \n    def forward(self, input):\n        batch_size, window_size = input.size()\n        embs = self.embeddings(input)\n        #print('Embedding size',embs.size())\n        \n        # next we want to concatenate the prefix emveddings together\n        concat_embs = embs.view(batch_size, window_size * self.d_emb)\n        #print('concat_embs', concat_embs.size())\n        \n        #print(embs[0])\n        #print(concat_embs[0])\n\n        # now we project thsi to the hidden space \n        hiddens = self.W_hid(concat_embs)\n        \n#        print('hidden size:',hiddens.size())\n        \n        \n        # finally project hiddens to vocabulary space \n        out = self.W_out(hiddens)\n #       print(out.size())\n        \n        return out # return unnormalized probability also known as **logits** \n        \n        #probs = nn.functional.softmax(out, dim=1)\n        #print(probs)\nnetwork = NLM(d_embedding=5, d_hidden=12, window_size=2, len_vocab=len(vocab))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:50.333347Z","iopub.execute_input":"2021-06-18T12:14:50.333688Z","iopub.status.idle":"2021-06-18T12:14:50.349674Z","shell.execute_reply.started":"2021-06-18T12:14:50.33365Z","shell.execute_reply":"2021-06-18T12:14:50.348879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(network)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:50.351564Z","iopub.execute_input":"2021-06-18T12:14:50.351943Z","iopub.status.idle":"2021-06-18T12:14:50.356988Z","shell.execute_reply.started":"2021-06-18T12:14:50.351909Z","shell.execute_reply":"2021-06-18T12:14:50.355712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = network(prefixes)\nprint(logits)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:50.358638Z","iopub.execute_input":"2021-06-18T12:14:50.358992Z","iopub.status.idle":"2021-06-18T12:14:50.449088Z","shell.execute_reply.started":"2021-06-18T12:14:50.35896Z","shell.execute_reply":"2021-06-18T12:14:50.448196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of epochs : how many times we are going trhough our network \nnum_epochs = 100 \nlearning_rate = 0.1 \nloss_fn = nn.CrossEntropyLoss()# it applies log softmax function and then negative log likelihood \n\n# we will use vanilla gradient descent  you can expreiment with others like adam \noptimizer = torch.optim.SGD(params = network.parameters(), lr=learning_rate)\n\n# training loop \nfor i in range(num_epochs):\n    logits = network(prefixes)\n    loss = loss_fn(logits, labels)\n    \n    # now let;s update our params to make the loss smaaller \n   # print(loss)\n\n    #Step1 compute gradient \n    loss.backward()\n    # step 2 update params using gradient descent \n    optimizer.step()\n    \n    # zero the gradients for next epoch\n    optimizer.zero_grad()\n    print(f'Epoch {i}, loss {loss}')\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-18T12:14:50.450263Z","iopub.execute_input":"2021-06-18T12:14:50.450612Z","iopub.status.idle":"2021-06-18T12:14:50.624211Z","shell.execute_reply.started":"2021-06-18T12:14:50.450577Z","shell.execute_reply":"2021-06-18T12:14:50.623388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# is it working\n# reverse vocabulary mapping (idx> word type)\n\nrev_vocab =dict((idx, word) for (word, idx) in vocab.items()) ","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:50.625468Z","iopub.execute_input":"2021-06-18T12:14:50.625854Z","iopub.status.idle":"2021-06-18T12:14:50.629512Z","shell.execute_reply.started":"2021-06-18T12:14:50.625773Z","shell.execute_reply":"2021-06-18T12:14:50.628709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rev_vocab","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:50.630709Z","iopub.execute_input":"2021-06-18T12:14:50.631118Z","iopub.status.idle":"2021-06-18T12:14:50.646352Z","shell.execute_reply.started":"2021-06-18T12:14:50.631081Z","shell.execute_reply":"2021-06-18T12:14:50.645153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mohitlikes = prefixes[0].unsqueeze(0)\nlogits = network(mohitlikes)\n#print(logits)\nprob = nn.functional.softmax(logits, dim=1).squeeze()\n\nprint(prob)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:50.838716Z","iopub.execute_input":"2021-06-18T12:14:50.839056Z","iopub.status.idle":"2021-06-18T12:14:50.848977Z","shell.execute_reply.started":"2021-06-18T12:14:50.839026Z","shell.execute_reply":"2021-06-18T12:14:50.847682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"argmax_idx = torch.argmax(prob).item()\nprint('given mohit likes model predicts \"%s\" as the next word with %0.4f probability '%(rev_vocab[argmax_idx], prob[argmax_idx]))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:51.119222Z","iopub.execute_input":"2021-06-18T12:14:51.119545Z","iopub.status.idle":"2021-06-18T12:14:51.127491Z","shell.execute_reply.started":"2021-06-18T12:14:51.119513Z","shell.execute_reply":"2021-06-18T12:14:51.126364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lifeis = prefixes[2].unsqueeze(0)\nlogits = network(lifeis)\n#print(logits)\nprob = nn.functional.softmax(logits, dim=1).squeeze()\n\nprint(prob)\nargmax_idx = torch.argmax(prob).item()\nprint('given \"life is\" model predicts \"%s\" as the next word with %0.4f probability '%(rev_vocab[argmax_idx], prob[argmax_idx]))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:51.398635Z","iopub.execute_input":"2021-06-18T12:14:51.398967Z","iopub.status.idle":"2021-06-18T12:14:51.406903Z","shell.execute_reply.started":"2021-06-18T12:14:51.398935Z","shell.execute_reply":"2021-06-18T12:14:51.405905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's no use a dataset to Generate new text with help of GRU\n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:14:51.678726Z","iopub.execute_input":"2021-06-18T12:14:51.679064Z","iopub.status.idle":"2021-06-18T12:14:51.682551Z","shell.execute_reply.started":"2021-06-18T12:14:51.679033Z","shell.execute_reply":"2021-06-18T12:14:51.681436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TASK 2\n    let's no use a dataset to Generate new text with help of GRU\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport nltk \nimport string \nimport unidecode \nimport random \nimport torch \n","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:19.714354Z","iopub.execute_input":"2021-06-18T12:45:19.714718Z","iopub.status.idle":"2021-06-18T12:45:22.24289Z","shell.execute_reply.started":"2021-06-18T12:45:19.714641Z","shell.execute_reply":"2021-06-18T12:45:22.242003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to check the GPU is working or not \ndevice = torch.cuda.is_available()\nif device : \n    print('GPU is available')\nelse : \n    print(\"we are working on CPU ;P\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:22.874178Z","iopub.execute_input":"2021-06-18T12:45:22.87463Z","iopub.status.idle":"2021-06-18T12:45:22.962312Z","shell.execute_reply.started":"2021-06-18T12:45:22.874589Z","shell.execute_reply":"2021-06-18T12:45:22.960596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's load spooky data set\n\ntrain = pd.read_csv('../input/spooky/train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:24.753678Z","iopub.execute_input":"2021-06-18T12:45:24.753992Z","iopub.status.idle":"2021-06-18T12:45:24.853573Z","shell.execute_reply.started":"2021-06-18T12:45:24.753964Z","shell.execute_reply":"2021-06-18T12:45:24.8526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.author.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:24.993196Z","iopub.execute_input":"2021-06-18T12:45:24.993493Z","iopub.status.idle":"2021-06-18T12:45:25.005089Z","shell.execute_reply.started":"2021-06-18T12:45:24.993465Z","shell.execute_reply":"2021-06-18T12:45:25.004242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will use MWS only for this task \ntext = list(train[train['author']=='MWS']['text'][:2000])","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:25.273545Z","iopub.execute_input":"2021-06-18T12:45:25.273854Z","iopub.status.idle":"2021-06-18T12:45:25.282745Z","shell.execute_reply.started":"2021-06-18T12:45:25.273826Z","shell.execute_reply":"2021-06-18T12:45:25.281861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:25.603552Z","iopub.execute_input":"2021-06-18T12:45:25.603866Z","iopub.status.idle":"2021-06-18T12:45:25.609369Z","shell.execute_reply.started":"2021-06-18T12:45:25.603835Z","shell.execute_reply":"2021-06-18T12:45:25.608367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def joinString(text):\n    return ' '.join(s for s in text)\ntext = joinString(text)\nlen(text.split())","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:26.273706Z","iopub.execute_input":"2021-06-18T12:45:26.274034Z","iopub.status.idle":"2021-06-18T12:45:26.288869Z","shell.execute_reply.started":"2021-06-18T12:45:26.274004Z","shell.execute_reply":"2021-06-18T12:45:26.288069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = text.lower().split()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:26.633403Z","iopub.execute_input":"2021-06-18T12:45:26.633719Z","iopub.status.idle":"2021-06-18T12:45:26.641796Z","shell.execute_reply.started":"2021-06-18T12:45:26.63369Z","shell.execute_reply":"2021-06-18T12:45:26.640857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trigrams = [([text[i], text[i+1]], text[i+2]) for i in range(len(text) - 2)]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:26.953339Z","iopub.execute_input":"2021-06-18T12:45:26.953614Z","iopub.status.idle":"2021-06-18T12:45:27.085147Z","shell.execute_reply.started":"2021-06-18T12:45:26.953589Z","shell.execute_reply":"2021-06-18T12:45:27.084392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trigrams[:2]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:27.113431Z","iopub.execute_input":"2021-06-18T12:45:27.113694Z","iopub.status.idle":"2021-06-18T12:45:27.119742Z","shell.execute_reply.started":"2021-06-18T12:45:27.113669Z","shell.execute_reply":"2021-06-18T12:45:27.118955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = set(text)\nprint(len(vocab))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:27.393491Z","iopub.execute_input":"2021-06-18T12:45:27.393767Z","iopub.status.idle":"2021-06-18T12:45:27.401823Z","shell.execute_reply.started":"2021-06-18T12:45:27.39374Z","shell.execute_reply":"2021-06-18T12:45:27.401003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voc_len = len(vocab)\nword_to_idx = { word: i for i, word in enumerate(vocab)}","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:27.763367Z","iopub.execute_input":"2021-06-18T12:45:27.76369Z","iopub.status.idle":"2021-06-18T12:45:27.770537Z","shell.execute_reply.started":"2021-06-18T12:45:27.763662Z","shell.execute_reply":"2021-06-18T12:45:27.769695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = []\ntar = []\nfor context, target in trigrams : \n    context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n    inp.append(context_idxs)\n    targ = torch.tensor([word_to_idx[target]], dtype=torch.long)\n    tar.append(targ)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:28.033401Z","iopub.execute_input":"2021-06-18T12:45:28.033725Z","iopub.status.idle":"2021-06-18T12:45:28.646232Z","shell.execute_reply.started":"2021-06-18T12:45:28.033696Z","shell.execute_reply":"2021-06-18T12:45:28.645395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's build our model now \nimport torch\nimport torch.nn as nn \nfrom torch.autograd import Variable \n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size \n        self.n_layers = n_layers\n        \n        self.encoder = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers, batch_first=True, bidirectional=False)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n        \n    def forward(self, input, hidden):\n        input = self.encoder(input.view(1, -1))\n        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n        output = self.decoder(output.view(1, -1))\n        return output, hidden\n    \n    def init_hidden(self):\n        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:28.647761Z","iopub.execute_input":"2021-06-18T12:45:28.648098Z","iopub.status.idle":"2021-06-18T12:45:28.656688Z","shell.execute_reply.started":"2021-06-18T12:45:28.64806Z","shell.execute_reply":"2021-06-18T12:45:28.655808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn = RNN(2, 10, 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:28.658216Z","iopub.execute_input":"2021-06-18T12:45:28.658778Z","iopub.status.idle":"2021-06-18T12:45:28.681336Z","shell.execute_reply.started":"2021-06-18T12:45:28.65874Z","shell.execute_reply":"2021-06-18T12:45:28.680601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rnn)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:28.713327Z","iopub.execute_input":"2021-06-18T12:45:28.713573Z","iopub.status.idle":"2021-06-18T12:45:28.717378Z","shell.execute_reply.started":"2021-06-18T12:45:28.713551Z","shell.execute_reply":"2021-06-18T12:45:28.716552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training \nchunklen = len(trigrams)\ndef train(inp , target):\n    hidden = decoder.init_hidden().cuda()\n    decoder.zero_grad()\n    loss = 0\n    \n    for c in range(chunklen):\n        output, hidden = decoder(inp[c].cuda(), hidden)\n        loss += criterion(output, target[c].cuda())\n        \n    loss.backward()\n\n    decoder_optimizer.step()\n    \n    return loss.data.item()/chunklen\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:28.973372Z","iopub.execute_input":"2021-06-18T12:45:28.973634Z","iopub.status.idle":"2021-06-18T12:45:28.980222Z","shell.execute_reply.started":"2021-06-18T12:45:28.97361Z","shell.execute_reply":"2021-06-18T12:45:28.979327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time, math \ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s/60)\n    s -= m * 60\n    \n    return '%dm %ds' %(m, s)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:29.433525Z","iopub.execute_input":"2021-06-18T12:45:29.433833Z","iopub.status.idle":"2021-06-18T12:45:29.439281Z","shell.execute_reply.started":"2021-06-18T12:45:29.433803Z","shell.execute_reply":"2021-06-18T12:45:29.438242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs =40\nprint_every = 5\nplot_every = 1\nhidden_size = 100\nn_layers = 1 \nlr = 0.015 \n\ndecoder = RNN(voc_len, hidden_size, voc_len, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\nstart = time.time()\nall_losses = []\nloss_avg = 0\nif(device):\n    decoder.cuda()\nfor epoch in range(1, n_epochs+1): \n    loss = train(inp, tar)\n    loss_avg  +=loss\n    \n    if epoch % print_every ==0:\n        print(\"[%s (%d %d%%) %.4f]\" %(time_since(start), epoch, epoch/n_epochs * 50, loss))\n    \n    if epoch % plot_every == 0: \n        all_losses.append(loss_avg/plot_every)\n        loss_avg = 0","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:45:29.843718Z","iopub.execute_input":"2021-06-18T12:45:29.844024Z","iopub.status.idle":"2021-06-18T13:21:59.049951Z","shell.execute_reply.started":"2021-06-18T12:45:29.843995Z","shell.execute_reply":"2021-06-18T13:21:59.049033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(prime_str='ok so', predict_len=100, temp = 0.8):\n    hidden = decoder.init_hidden().cuda()\n    \n    for p in range(predict_len):\n        prime_input = torch.tensor([word_to_idx[w] for w in prime_str.split()], dtype=torch.long).cuda()\n        inp = prime_input[-2:]\n        output, hidden = decoder(inp, hidden)\n        \n        output_dist = output.data.view(-1).div(temp).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        \n        # add predicted word to string and use as next input\n        predicted_word = list(word_to_idx.keys())[list(word_to_idx.values()).index(top_i)]\n        prime_str +=\" \"+predicted_word\n        \n    return prime_str","metadata":{"execution":{"iopub.status.busy":"2021-06-18T13:21:59.05943Z","iopub.execute_input":"2021-06-18T13:21:59.059801Z","iopub.status.idle":"2021-06-18T13:21:59.067161Z","shell.execute_reply.started":"2021-06-18T13:21:59.059761Z","shell.execute_reply":"2021-06-18T13:21:59.06603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(inference(\"i am\", 40, temp=1))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T13:21:59.068834Z","iopub.execute_input":"2021-06-18T13:21:59.069181Z","iopub.status.idle":"2021-06-18T13:22:06.515757Z","shell.execute_reply.started":"2021-06-18T13:21:59.069145Z","shell.execute_reply":"2021-06-18T13:22:06.51486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(inference(\"proud of\", 40))","metadata":{"execution":{"iopub.status.busy":"2021-06-18T13:22:06.517275Z","iopub.execute_input":"2021-06-18T13:22:06.517778Z","iopub.status.idle":"2021-06-18T13:22:13.254449Z","shell.execute_reply.started":"2021-06-18T13:22:06.517737Z","shell.execute_reply":"2021-06-18T13:22:13.253462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}