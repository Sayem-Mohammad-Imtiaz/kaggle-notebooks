{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport emoji\nimport string\nimport re\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.sentiment_analyzer import SentimentAnalyzer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_full=pd.read_csv('/kaggle/input/amazon-music-reviews/Musical_instruments_reviews.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_full.head(1).T)\nprint(\n    \"\"\"\n    reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n    asin - ID of the product, e.g. 0000013714\n    reviewerName - name of the reviewer\n    helpful - helpfulness rating of the review, e.g. 2/3\n    reviewText - text of the review\n    overall - rating of the product\n    summary - summary of the review\n    unixReviewTime - time of the review (unix time)\n    reviewTime - time of the review (raw)\"\"\"\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I`ll assume, that there can be some reviewers with many bad/good reviews and such products, which get mostly one type review. And i will check this hypotesis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full['sentiment']=df_full.overall.apply(lambda x:1 if x>3 else 0 if x==3 else -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full=df_full.merge(df_full.groupby(by='reviewerID')[['overall']].mean(),on='reviewerID',\n              suffixes=('', '_reviewer')).merge(\ndf_full.groupby(by='asin')[['overall']].mean(),on='asin',\n              suffixes=('', '_product_mean')).merge(\ndf_full.groupby(by='asin')[['overall']].std(),on='asin',\n              suffixes=('', '_product_std')).merge(\ndf_full.groupby(by='asin')[['asin']].count().rename(columns={'asin':'asin_counts'}),on='asin',\n              suffixes=('', '_counts'))\ndf_full['upper_interval_limit']=df_full['overall_product_mean']+2.58*df_full['overall_product_std']\ndf_full['lower_interval_limit']=df_full['overall_product_mean']-2.58*df_full['overall_product_std']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_full[~((df_full['overall_reviewer']>df_full['lower_interval_limit'])&\n        (df_full['overall_reviewer']<df_full['upper_interval_limit']))\n        &(df_full.asin_counts>3)&(df_full.overall_product_std!=0.0)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_norm=df_full[((df_full['overall_reviewer']>df_full['lower_interval_limit'])&\n        (df_full['overall_reviewer']<df_full['upper_interval_limit']))\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_emoji(line):\n    emoji_=''.join(emoji.UNICODE_EMOJI.keys())\n    emoji_flag=sum([i in emoji_ for i in line])>0\n    return emoji_flag\n\ndef check_capslock(line):\n    capslock_flag=len(re.findall(r'[A-Z][A-Z][A-Z]+',line))>1\n    return capslock_flag\n\ndef preprocess(line):\n    ps=PorterStemmer()\n    remove_list=string.punctuation\n    remove_list+=''.join(emoji.UNICODE_EMOJI.keys())\n    translator = str.maketrans(remove_list, ' '*len(remove_list), '')\n    line=line.translate(translator)\n    line=re.sub(r'http(s)?:\\/\\/\\S*? ', \" \", line)\n    this_stopwords=set(stopwords.words('english'))\n    line = ' '.join(filter(lambda l: l not in this_stopwords, line.split(' ')))\n    line=line.replace('  ','').lower()\n    tokens=[]\n    for word in line.split(' '):\n        tokens.append(ps.stem(word))\n    #line=' '.join([i if i not in stopwords.words() else '' for i in line.split(' ') ])\n    \n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_norm['summary_capslock']=df_norm.summary.apply(lambda x:check_capslock(x))\ndf_norm['summary_emoji']=df_norm.summary.apply(lambda x:check_emoji(x))\ndf_norm['summary_prep']=df_norm.summary.apply(lambda x:preprocess(x))\ndf_norm['reviewText_prep']=df_norm.reviewText.astype(str).apply(lambda x:preprocess(x))\ndf_norm['reviewText_capslock']=df_norm.reviewText.astype(str).apply(lambda x:check_capslock(x))\ndf_norm['reviewText_emoji']=df_norm.reviewText.astype(str).apply(lambda x:check_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There will be few models for sentiment analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"NLTK VADER","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()\ndef dict_max(scores):\n    if scores['pos']==max(scores.values()):\n        return 1\n    elif scores['neg']==max(scores.values()):\n        return -1\n    else:\n        return 0\n    \ndf_norm['sentiment_vader']=df_norm['summary'].astype(str).apply(lambda x:dict_max(sid.polarity_scores(x)))\nprint(\"NLTK VADER:\",np.round(metrics.accuracy_score(df_norm.sentiment, df_norm.sentiment_vader),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Naive Bayes\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(analyzer=\"word\",ngram_range = (1,1))\ntext_counts= cv.fit_transform(df_norm['reviewText_prep'].apply(lambda x:' '.join(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test=text_counts[:-3000],text_counts[-3000:]\ny_train, y_test=df_norm.sentiment[:-3000],df_norm.sentiment[-3000:]\nclf = MultinomialNB().fit(x_train, y_train)\npredicted= clf.predict(x_test)\nprint(\"MultinomialNB Accuracy:\",np.round(metrics.accuracy_score(y_test, predicted),4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LinearSVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb=LinearSVC(penalty='l1',dual=False)\ngnb.fit(x_train.toarray(),y_train)\npredicted_svc= gnb.predict(x_test.toarray())\nprint(\"LinearSVC Accuracy:\",np.round(metrics.accuracy_score(y_test, predicted_svc),4))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}