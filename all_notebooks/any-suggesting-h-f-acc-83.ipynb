{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Heart failure dataset\n\nThis notebook is based on the heart failure dataset, and is my second notebook after Titanic.\n\nMy hope with this notebook is to increase my machine learning knowledge and craftmanship, so In the spirit of Kaggle i invite you to comment if you find anything that could be improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nfilepath = (\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndf = pd.read_csv(filepath)\n\nsns.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The entere dataframe\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The column headings\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data types\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistics\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time ranges from 4 to 285 I guess it is counted in days."},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing values?\npd.isna(df).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nope, no missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many?\n\nprint(f'Total number of patients: {len(df)}')\nprint(f'Number of patients with anaemia: {df.anaemia.sum()}')\nprint(f'Number of patients with diabetes: {df.diabetes.sum()}')\nprint(f'Number of patients with high blod pressure: {df.high_blood_pressure.sum()}')\nprint(f'Number of female patients: {df.sex.sum()}')\nprint(f'Number of smokers: {df.smoking.sum()}')\nprint(f'Number of deaths or exits: {df.DEATH_EVENT.sum()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Displaying the data in graphs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first displaying the numerical data.\n\nf, axs = plt.subplots(ncols=4, figsize=(15, 6))\n\nsns.histplot(data=df, x=\"age\", ax=axs[0])\nsns.histplot(data=df, x='creatinine_phosphokinase',ax=axs[1])\nsns.histplot(data=df, x='ejection_fraction', ax=axs[2])\nsns.scatterplot(data=df, x=\"age\", y=\"ejection_fraction\", ax=axs[3])\nf.subplots_adjust(wspace=0.4)\n\nf, axs = plt.subplots(3,2, figsize=(15, 12))\n\nsns.kdeplot(data=df, x='creatinine_phosphokinase', hue=\"DEATH_EVENT\", ax=axs[0,0])\nsns.kdeplot(data=df, x='ejection_fraction', hue=\"DEATH_EVENT\", ax=axs[0,1])\nsns.kdeplot(data=df, x='platelets', hue='DEATH_EVENT', ax=axs[1,0])\nsns.kdeplot(data=df, x='serum_creatinine', hue='DEATH_EVENT', ax=axs[1,1])\nsns.kdeplot(data=df, x='serum_sodium', hue='DEATH_EVENT', ax=axs[2,0])\nsns.kdeplot(data=df, x='age', hue='DEATH_EVENT', ax=axs[2,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age, ejection fraction, serum creatinine and serum sodium seems to be different for those patinents with death events compared other patients. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting as box plots\n\nf, axs = plt.subplots(3,2, figsize=(17, 12))\n\nsns.boxplot(data=df, x='DEATH_EVENT', y='serum_creatinine', ax=axs[0,0])\nsns.boxplot(data=df, x='DEATH_EVENT', y='creatinine_phosphokinase',ax=axs[0,1])\nsns.boxplot(data=df, x='DEATH_EVENT', y='platelets', ax=axs[1,1])\nsns.boxplot(data=df, x='DEATH_EVENT', y='serum_sodium',ax=axs[1,0])\nsns.boxplot(data=df, x='DEATH_EVENT', y='age',ax=axs[2,0])\nsns.boxplot(data=df, x='DEATH_EVENT', y='ejection_fraction',ax=axs[2,1])\nf.subplots_adjust(wspace=0.2, hspace=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like age and ejection fraction are different for patient experiencing an event.\nAlso I notice outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Then lets look at the nominal data\n# Barplot and interpretation\nsns.barplot(x='high_blood_pressure', y='DEATH_EVENT', data=df)\nprint('Percentage of patients with low blood pressure who had an event:', round(df.loc[df['high_blood_pressure']==0].DEATH_EVENT.sum()/len(df.loc[df['high_blood_pressure']==0])*100))\nprint('Percentage of patients with high blood pressure who had an event:', round(df.loc[df['high_blood_pressure']==1].DEATH_EVENT.sum()/len(df.loc[df['high_blood_pressure']==1])*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"High blood pressure seems to correlate with risk of event."},{"metadata":{"trusted":true},"cell_type":"code","source":"# More plots!\n\nf, axs = plt.subplots(2,2, figsize=(15, 9))\n\nsns.barplot(x='smoking', y='DEATH_EVENT', data=df, ax=axs[0,0])\nsns.barplot(x='sex', y='DEATH_EVENT', data=df, ax=axs[1,0])\nsns.barplot(x='anaemia', y='DEATH_EVENT', data=df, ax=axs[1,1])\nsns.barplot(x='diabetes', y='DEATH_EVENT', data=df, ax=axs[0,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Anamia might also be related to events"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way of creating plot.\n# In these plots age is used as a reference. \n\nbinary_columns =['anaemia','high_blood_pressure','smoking','diabetes']\n\nfor columns in binary_columns:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df, x=columns, y='age', hue='DEATH_EVENT')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is interesting that in the above barplot smoking does not seem to be an important feature, but plottet in relation to age it is.\nAlso one might think that smoking is bad for your heart, but it seems to be the opposite in this dataset.\n\nAlso interesting that diabetes cancels out the effect of age on death events\nComplex data, maybe there is a need for combined features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see if scatter plots can reveal anything.\n\nf, axs = plt.subplots(2,2, figsize=(15, 9))\n\nsns.scatterplot(data=df, x='serum_creatinine', y='age',hue='DEATH_EVENT',ax=axs[0,0])\nsns.scatterplot(data=df, x='serum_sodium', y='age', hue='DEATH_EVENT',ax=axs[0,1])\nsns.scatterplot(data=df, x='creatinine_phosphokinase', y='age', hue='DEATH_EVENT', ax=axs[1,0])\nsns.scatterplot(data=df, x='platelets', y='age', hue='DEATH_EVENT', ax=axs[1,1])\nsns.lmplot(data=df, x='serum_sodium', y='age', hue='DEATH_EVENT')\nsns.lmplot(data=df, x='time', y='age', hue='DEATH_EVENT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the last lmplot one can see that patients at high age are more likely to have an early event.\nCan not find anything too interesting on the other plots. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# One last scatter plot, just because is looks nice in 3D.\n\nfig = plt.figure(figsize=(17, 12))\nax = fig.add_subplot(111, projection = '3d')\n\nx = df['age']\ny = df['serum_creatinine']\nz = df['ejection_fraction']\n\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"Serum creatinine\")\nax.set_zlabel(\"Ejektion fraction\")\n\nscatter = ax.scatter(x, y, z, c=df['DEATH_EVENT'], marker='o')\n\nlegend = ax.legend(*scatter.legend_elements(),\n                    loc=\"upper right\", title=\"Death events\")\nax.add_artist(legend)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One might be able to see that patients at high age are more likely to have an event, similar for patients with low ejection fractions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now lets see how the features relate to death events.\n\nv = pd.DataFrame(df.corr()['DEATH_EVENT'])\nplt.figure(figsize=(20,12))\nsns.barplot(x='DEATH_EVENT', y=v.index, data=v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like age, ejektion fraction, serum creatinine, serum sodium and time are the features that correlates the most to death events."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Based on the above figure I am\n# setting a negative treshold of -0.2 and a positive of 0.2.\n\ndf_corr = df.corr()\ndf_corr_true = [(df_corr['DEATH_EVENT'] >0.2) | (df_corr['DEATH_EVENT'] <-0.2)]\nprint(df_corr_true)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age, ejection fraction, serum creatinine and time are the true correlators..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way of displaying this is a heatmap.\n\nplt.figure(figsize=(13,9))\nsns.heatmap(df.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on finding in the previous barplots i would like to \n# create a new column that combines smoking, diabetes and sex.\n# Hopefully this could make some of the nominal data usefull to the model.\n\ndef sds(row):\n    return 1 + row['smoking']-row['diabetes']+row['sex']\n  \nsds = df.apply(sds, axis=1)\ndf.insert(0, 'sds', sds)\ndf['sds'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,6))\nax=sns.boxplot(data=df, x='sds', y='age', hue='DEATH_EVENT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combined with age i looks like the new feature sds is usefull. People with a high sds score are more likely to survive (die later)."},{"metadata":{},"cell_type":"markdown","source":"# Feature importance\nLets see which features a random forest consideres important, and plot that information."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nX = df.drop(columns='DEATH_EVENT')\ny = df['DEATH_EVENT']\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(X, y)\n\nprint(f'\\n Feature importance: {randomforest.feature_importances_}')\nfeat_importances = pd.Series(randomforest.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()\n\nfrom sklearn.inspection import plot_partial_dependence\n\nfig, ax = plt.subplots(figsize=(17, 10))\nax.set_title(\"Random forest\")\ntree_disp = plot_partial_dependence(randomforest, X, ['sds','age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time'], ax=ax)\nfig.subplots_adjust(wspace=0.2, hspace=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The new feature?\n\nOnce again age, ejection fraction, serum cratinine and time wins!!\n\nIt does not look my newly created feature sds is not considered important, too bad. At least it did better then the single binary features.\n\nMaybe i was confused in input line 11 because i plottet the columns with binary values with age on the y-axis\nlet try it with time on the y-axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"for columns in binary_columns:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=df, x=columns, y='time', hue='DEATH_EVENT')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Truly on these plots the 4 features does not seem important."},{"metadata":{},"cell_type":"markdown","source":"# Now lets make some models\n\nIn the next steps i copied a lot from this notebook on Titanic:\nhttps://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner\n\nAnd this notebook:\nhttps://www.kaggle.com/codeblogger/step-by-step-decision-tree-classifier-98-34"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfeatures = ['age','ejection_fraction','serum_creatinine','time']\npredictors = df[features]\n\ntarget = df[\"DEATH_EVENT\"]\nX_train, X_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)\n\n# Standardization of data for tree based models\n\nsc=StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_val = sc.fit_transform(X_val)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model_creator\n\ndef model_creator (model, best_parameter_grid):\n    if not best_parameter_grid:\n        f=model()\n    \n    else:\n        f=model(**best_parameter_grid)\n    \n    f.fit(X_train, y_train)\n    y_pred = f.predict(X_val)\n    \n    \n    print('Accuracy:', round(accuracy_score(y_pred, y_val)*100, 2))\n    print('F1 score:', round(f1_score(y_val , y_pred), 2))\n    return f","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have chosen to use 4 models:\n    Random Forest Classifier,\n    Gradient Boosting Classifier,\n    DecisionTreeClassifier and\n    Ada Boost Classifier.\n\nThis choice is mainly based on what I have seen other people do."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score\n\n\nrfc=model_creator(RandomForestClassifier, best_parameter_grid={})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making grid search on Random Forest Classifier\n\nfrom sklearn.model_selection import GridSearchCV\n\n# reduced search\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,6,8],\n    'criterion' :['gini', 'entropy']\n}\n\nCV_randomforest = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 4)\nCV_randomforest.fit(X_train, y_train)\n\nprint('Best parameters:', CV_randomforest.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_2=model_creator(RandomForestClassifier, best_parameter_grid=CV_randomforest.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_2.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc=model_creator(GradientBoostingClassifier, best_parameter_grid=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making grid search for GradientboostingClassifier\n\nparam_grid = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 3),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 3),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.8, 0.95, 1.0],\n    \"n_estimators\":[10]\n    }\n\nCV_gbc = GridSearchCV(estimator=gbc, param_grid=param_grid, cv=4, n_jobs=-1)\nCV_gbc.fit(X_train, y_train)\n\nprint('Best parameters:',CV_gbc.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc_2=model_creator(GradientBoostingClassifier, CV_gbc.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc=model_creator(DecisionTreeClassifier, best_parameter_grid=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n    \"max_depth\": [1,5,10],\n    \"min_samples_split\": [0.001, 0.01, 0.1, 0.2, 0.02, 0.002],\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"max_leaf_nodes\": [2,5,10],\n    \"class_weight\": [\"balanced\", None]\n}\n\nCV_decisiontree = GridSearchCV(estimator=dtc, param_grid=param_grid, cv= 4)\nCV_decisiontree.fit(X_train, y_train)\n\nprint('Best parameters:', CV_decisiontree.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc_2 = model_creator(DecisionTreeClassifier, CV_decisiontree.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nabc=model_creator(AdaBoostClassifier, best_parameter_grid=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute ROC curve and ROC area for each class and making it a function\n# This part is from: https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n\ndef ROC_Curve(model):\n    probs = model.predict_proba(X_val)\n    preds = probs[:,1]\n\n    fpr, tpr, threshold = roc_curve(y_val, preds)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure(figsize=(10,10))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([-0.001, 1])\n    plt.ylim([0, 1.001])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \n#Compute Confusion Matrix and making it a function\n\ndef Confusion_matrix(model):\n    cm = confusion_matrix(y_val, model.predict(X_val))\n    plt.figure(figsize=(6,6))\n    sns.heatmap(cm, annot=True, cmap=\"Blues\",\n                   xticklabels=[\"FALSE\",\"TRUE\"],\n                   yticklabels=[\"FALSE\",\"TRUE\"],\n                   cbar=False)\n    plt.title(\"Stacking Classifier Confusion Matrix (Number)\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking Classifier\n\nLet's see if a stacking classifier performs better by combining the four models. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n\nestimators = [\n    ('rfc_2', rfc_2),\n    ('gbc_2', gbc_2),\n    ('dtc_2', dtc_2),\n    ('abc', abc)\n]\n\nsc = StackingClassifier(\nestimators=estimators, final_estimator=LogisticRegression()\n)\n\nsc.fit(X_train, y_train)\nsc_y_pred = sc.predict(X_val)\n\nprint('Accuracy:', round(accuracy_score(sc_y_pred, y_val) * 100, 2))\nprint('F1 score:', round(f1_score(sc_y_pred, y_val),2)) \nprint('Roc auc score',round(roc_auc_score(sc_y_pred, y_val),2))\nprint(\"\\n\",classification_report(sc_y_pred, y_val))\n\nConfusion_matrix(sc)\nROC_Curve(sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# End\n\nSo this (above) is the final model, although this stacking classifier it is not much better then the previous models.\n\nPlease leave a comment if you find anything that could be improved.\n\nThanks for reading through :)."},{"metadata":{},"cell_type":"markdown","source":"# Appendix\n\nFrom here it does not count, but to me it was interesting to see that if i looped through a model my accuracy could increse significantly. I know that i does not make my model better, it just had more luck on this validation set, but it is funny when this it is the way Kaggle judges models."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [\n    ('rfc_2', rfc_2),\n    ('gbc_2', gbc_2),\n    ('dtc_2', dtc_2),\n    ('abc', abc)\n]\n\nsc_acc_list=[]\n\nfor i in range(40):\n\n    sc = StackingClassifier(\n         estimators=estimators, final_estimator=LogisticRegression()\n    )\n\n    sc.fit(X_train, y_train)\n    sc_y_pred = sc.predict(X_val)\n    sc_acc_list.append(round(accuracy_score(sc_y_pred, y_val) * 100, 2))\n    \nprint('Max accuracy:', max(sc_acc_list))\nprint('Min accuracy:', min(sc_acc_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_loop = 0\n\nfor i in range(500):\n    rfc.fit(X_train, y_train)\n    rfc_y_pred = rfc.predict(X_val)\n    acc_rfc = round(accuracy_score(rfc_y_pred, y_val) * 100, 2)\n     \n    if acc_rfc >= 89:\n        rfc_loop += 1\n        print('Model:',i,' Accuracy',acc_rfc)\n        print('Model:',i,' F1 score:',round(f1_score(y_val , rfc_y_pred),2))\n        print(\"\\n\",classification_report(rfc_y_pred, y_val))\n        Confusion_matrix(rfc)\n        ROC_Curve(rfc)\n        break\n\nif rfc_loop == 0:          \n    for i in range(500):\n        rfc.fit(X_train, y_train)\n        rfc_y_pred = rfc.predict(X_val)\n        acc_rfc = round(accuracy_score(rfc_y_pred, y_val) * 100, 2)\n\n        if acc_rfc >=87:\n            rfc_loop += 2\n            print('Model:',i,' Accuracy',acc_rfc)\n            print('Model:',i,' F1 score:',round(f1_score(y_val , rfc_y_pred),2))\n            print(\"\\n\",classification_report(rfc_y_pred, y_val))\n            Confusion_matrix(rfc)\n            ROC_Curve(rfc)\n            break\n            \nif rfc_loop == 0:\n    print('No rfc model is that good')\nelse: \n    print('Loop number: ', rfc_loop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc_loop = 0\n\nfor i in range(500):\n    gbc.fit(X_train, y_train)\n    gbc_y_pred = gbc.predict(X_val)\n    gbc_acc = accuracy_score(gbc_y_pred, y_val)\n    \n    if gbc_acc > 0.86:\n        gbc_loop += 1\n        print(f'Model:{i}, Accuracy: {round(gbc_acc*100, 2)}')\n        print(f'Model:{i}, F1 score: {round(f1_score(y_val , gbc_y_pred), 2)}')\n        print(\"\\n\",classification_report(gbc_y_pred, y_val))\n        Confusion_matrix(gbc)\n        ROC_Curve(gbc)\n        break\n        \nif gbc_loop == 0:\n    for i in range(500):\n        gbc.fit(X_train, y_train)\n        gbc_y_pred = gbc.predict(X_val)\n        gbc_acc = accuracy_score(gbc_y_pred, y_val)\n    \n        if gbc_acc > 0.84:\n            gbc_loop += 2\n            print(f'Model:{i}, Accuracy: {round(gbc_acc*100, 2)}')\n            print(f'Model:{i}, F1 score: {round(f1_score(y_val , gbc_y_pred), 2)}')\n            print(\"\\n\",classification_report(gbc_y_pred, y_val))\n            Confusion_matrix(gbc)\n            ROC_Curve(gbc)\n            break\n\nif gbc_loop == 0:\n    print('No gbc model is that good')\nelse: \n    print('Loop number: ', gbc_loop)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}