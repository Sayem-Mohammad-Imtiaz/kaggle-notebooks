{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pokemon"},{"metadata":{},"cell_type":"markdown","source":"## Data content\n\nThis database includes 21 variables per each of the 721 Pokémon of the first six generations, plus the Pokémon ID and its name. These variables are briefly described next:\n\n    Number. Pokémon ID in the Pokédex.\n    Name. Name of the Pokémon.\n    Type_1. Primary type.\n    Type_2. Second type, in case the Pokémon has it.\n    Total. Sum of all the base stats (Health Points, Attack, Defense, Special Attack, Special Defense, and Speed).\n    HP. Base Health Points.\n    Attack. Base Attack.\n    Defense. Base Defense.\n    Sp_Atk. Base Special Attack.\n    Sp_Def. Base Special Defense.\n    Speed. Base Speed.\n    Generation. Number of the generation when the Pokémon was introduced.\n    isLegendary. Boolean that indicates whether the Pokémon is Legendary or not.\n    Color. Colour of the Pokémon according to the Pokédex.\n    hasGender. Boolean that indicates if the Pokémon can be classified as female or male.\n    Pr_male. In case the Pokémon has Gender, the probability of its being male. The probability of being female is, of course, 1 minus this value.\n    EggGroup1. Egg Group of the Pokémon.\n    EggGroup2. Second Egg Group of the Pokémon, in case it has two.\n    hasMegaEvolution. Boolean that indicates whether the Pokémon is able to Mega-evolve or not.\n    Height_m. Height of the Pokémon, in meters.\n    Weight_kg. Weight of the Pokémon, in kilograms.\n    Catch_Rate. Catch Rate.\n    Body_Style. Body Style of the Pokémon according to the Pokédex."},{"metadata":{},"cell_type":"markdown","source":"## Prepare data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/pokemon-datasets-for-ml/train_pokemon.csv')\ntest = pd.read_csv('../input/pokemon-datasets-for-ml/test_pokemon.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# describe(include = ['O']) will show the descriptive statistics of object data types.\ntrain.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fill the rows in `Type_2` column that are currently null with `None`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_type_2(cols):\n    type_2 = cols[0]\n    if pd.isnull(type_2):\n        return \"None\"\n    else:\n        return type_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Type_2'] = train[['Type_2']].apply(fill_type_2,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because the majority of the `Egg_Group_2` is `NaN`, we will drop from the dataset as it will not be of any help. Then, we will be in a position to start investigating our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(columns=['Egg_Group_2'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Relationship between Features and Legendary\n\nIn this section, we will analyse the relationship between different features with respect to `isLegendary`."},{"metadata":{"trusted":true},"cell_type":"code","source":"legendary = train[train['isLegendary'] == 1]\nnot_legendary = train[train['isLegendary'] == 0]\n\nprint(\"Legendary: %i (%.1f%%)\"%(len(legendary), float(len(legendary))/len(train)*100.0))\nprint(\"Not Legendary: %i (%.1f%%)\"%(len(not_legendary), float(len(not_legendary))/len(train)*100.0))\nprint(\"Total: %i\"%len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlating Features\n\nHeatmap of Correlation between different features:\n   - Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.\n   - Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the Survived feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,10))\ntrain2 = train.drop(['Number','Name','hasGender','shuffle'], axis=1)\nsns.heatmap(train2.corr(), vmin= -1, vmax=1, square=True, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently, some feature have no correlation with legendary. These features are `hasMegaEvolution`,`Generation`."},{"metadata":{"trusted":true},"cell_type":"code","source":"#boxplot of Attack vs. Legendary\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='isLegendary',y='Attack',data=train, palette='rainbow')\n\n#stripplot of Attack vs. Legendary\nplt.figure(figsize=(15, 4))\nsns.stripplot(x='Type_1',y='Total',data=train, jitter=True,hue='isLegendary',palette=['r','b'],dodge=False).set_title('Type_1 Distribution on Legendary')\n\n#stripplot of Attack vs. Legendary\nplt.figure(figsize=(15, 4))\nsns.stripplot(x='Type_2',y='Total',data=train, jitter=True,hue='isLegendary',palette=['r','b'],dodge=False).set_title('Type_2 Distribution on Legendary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Type_1 vs Lengendary"},{"metadata":{"trusted":true},"cell_type":"code","source":"type_1 = train[['Type_1','isLegendary']].groupby(['Type_1'], as_index=False).mean().set_index('Type_1')\ntype_1.sort_values(by='isLegendary',ascending=False).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that most legendary Pokemons are also a Flying type, followed by the Dragon type. There are no legendary Poison, Fighting or Bug types. Still, `Type_1` feature can be useful to predict legendary Pokemons."},{"metadata":{"trusted":true},"cell_type":"code","source":"type_2 = train[['Type_2','isLegendary']].groupby(['Type_2'], as_index=False).mean().set_index('Type_2')\ntype_2.sort_values(by='isLegendary',ascending=False).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Like Type_1, `Type_2` can be useful to predict legendary Pokemons."},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction\n\nIn this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_data = [train, test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    dataset['isLegendary'] = dataset['isLegendary'].map({True: 1, False: 0}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type_1.sort_values(by='isLegendary',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After that, we convert the categorical Title values into numeric form."},{"metadata":{"trusted":true},"cell_type":"code","source":"type_1_mapping = {\"Fire\": 1, \"Dragon\": 2, \"Electric\": 3, \"Fighting\": 4, \"Ice\": 5, \"Flying\": 6, \"Water\": 7, \"Ghost\": 8, \"Steel\": 9, \"None\": 10, \"Fairy\": 11, \"Psychic\": 12, \"Ground\": 13, \"Rock\": 14, \"Bug\": 15, \"Poison\": 16, \"Normal\": 17, \"Dark\": 18, \"Grass\": 19}\nfor dataset in train_test_data:\n    dataset['Type_1'] = dataset['Type_1'].map(type_1_mapping)\n    dataset['Type_1'] = dataset['Type_1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's do the same thing for `Type_2`. Luckily, both `Type_1` and `Type_2` have the same Pokemon types, so we can just copy & paste and replace `Type_1` for `Type_2`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"type_2_mapping = {\"Fire\": 1, \"Dragon\": 2, \"Electric\": 3, \"Fighting\": 4, \"Ice\": 5, \"Flying\": 6, \"Water\": 7, \"Ghost\": 8, \"Steel\": 9, \"None\": 10, \"Fairy\": 11, \"Psychic\": 12, \"Ground\": 13, \"Rock\": 14, \"Bug\": 15, \"Poison\": 16, \"Normal\": 17, \"Dark\": 18, \"Grass\": 19}\nfor dataset in train_test_data:\n    dataset['Type_2'] = dataset['Type_2'].map(type_2_mapping)\n    dataset['Type_2'] = dataset['Type_2'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pr_Male\n\nWe first fill the NULL values of `Pr_Male` with a random number between (mean_Pr_Male - std_Pr_Male) and (mean_Pr_Male + std_Pr_Male). Then, we create a new column named Pr_Male_Band. This categorises Pr_Male into different ranges."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    pr_male_avg = dataset['Pr_Male'].mean()\n    pr_male_std = dataset['Pr_Male'].std()\n    pr_male_null_count = dataset['Pr_Male'].isnull().sum()\n    \n    pr_male_null_random_list = np.random.uniform(pr_male_avg - pr_male_std, pr_male_avg + pr_male_std, pr_male_null_count)\n    dataset['Pr_Male'][np.isnan(dataset['Pr_Male'])] = pr_male_null_random_list\n    dataset['Pr_Male'] = dataset['Pr_Male'].astype(int)\n    \ntrain['Pr_Male_Band'] = pd.cut(train['Pr_Male'], 5)\n\nprint(train[['Pr_Male_Band', 'isLegendary']].groupby(['Pr_Male_Band'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we map `Pr_Male` according to `Pr_Male_Band`."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    dataset.loc[ dataset['Pr_Male'] <= 0.2, 'Pr_Male'] = 0\n    dataset.loc[(dataset['Pr_Male'] > 0.2) & (dataset['Pr_Male'] <= 0.4), 'Pr_Male'] = 1\n    dataset.loc[(dataset['Pr_Male'] > 0.4) & (dataset['Pr_Male'] <= 0.6), 'Pr_Male'] = 2\n    dataset.loc[(dataset['Pr_Male'] > 0.6) & (dataset['Pr_Male'] <= 0.8), 'Pr_Male'] = 3\n    dataset.loc[ dataset['Pr_Male'] >= 1, 'Pr_Male'] = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attack & Defense"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    attack_avg = dataset['Attack'].mean()\n    attack_std = dataset['Attack'].std()\n    attack_null_count = dataset['Attack'].isnull().sum()\n    \n    attack_null_random_list = np.random.randint(attack_avg - attack_std, attack_avg + attack_std, attack_null_count)\n    dataset['Attack'][np.isnan(dataset['Attack'])] = attack_null_random_list\n    dataset['Attack'] = dataset['Attack'].astype(int)\n    \ntrain['Attack_Band'] = pd.cut(train['Attack'], 5)\n\nprint(train[['Attack_Band', 'isLegendary']].groupby(['Attack_Band'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    dataset.loc[ dataset['Attack'] <= 36, 'Attack'] = 0\n    dataset.loc[(dataset['Attack'] > 36) & (dataset['Attack'] <= 67), 'Attack'] = 1\n    dataset.loc[(dataset['Attack'] > 67) & (dataset['Attack'] <= 98), 'Attack'] = 2\n    dataset.loc[(dataset['Attack'] > 98) & (dataset['Attack'] <= 129), 'Attack'] = 3\n    dataset.loc[ dataset['Attack'] >= 129, 'Attack'] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    defense_avg = dataset['Defense'].mean()\n    defense_std = dataset['Defense'].std()\n    defense_null_count = dataset['Defense'].isnull().sum()\n    \n    defense_null_random_list = np.random.randint(defense_avg - defense_std, defense_avg + defense_std, defense_null_count)\n    dataset['Defense'][np.isnan(dataset['Defense'])] = defense_null_random_list\n    dataset['Defense'] = dataset['Defense'].astype(int)\n    \ntrain['Defense_Band'] = pd.cut(train['Defense'], 5)\n\nprint(train[['Defense_Band', 'isLegendary']].groupby(['Defense_Band'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    dataset.loc[ dataset['Defense'] <= 50, 'Defense'] = 0\n    dataset.loc[(dataset['Defense'] > 50) & (dataset['Defense'] <= 95), 'Defense'] = 1\n    dataset.loc[(dataset['Defense'] > 95) & (dataset['Defense'] <= 140), 'Defense'] = 2\n    dataset.loc[(dataset['Defense'] > 140) & (dataset['Defense'] <= 230), 'Defense'] = 3\n    dataset.loc[ dataset['Defense'] >= 230, 'Defense'] = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Catch Rate %"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    cr_avg = dataset['Catch_Rate'].mean()\n    cr_std = dataset['Catch_Rate'].std()\n    cr_null_count = dataset['Catch_Rate'].isnull().sum()\n    \n    cr_null_random_list = np.random.randint(cr_avg - cr_std, cr_avg + cr_std, cr_null_count)\n    dataset['Catch_Rate'][np.isnan(dataset['Catch_Rate'])] = cr_null_random_list\n    dataset['Catch_Rate'] = dataset['Catch_Rate'].astype(int)\n    \ntrain['Catch_Rate_Band'] = pd.cut(train['Catch_Rate'], 5)\n\nprint(train[['Catch_Rate_Band', 'isLegendary']].groupby(['Catch_Rate_Band'], as_index=False).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in train_test_data:\n    dataset.loc[ dataset['Catch_Rate'] <= 53, 'Catch_Rate'] = 0\n    dataset.loc[(dataset['Catch_Rate'] > 53) & (dataset['Catch_Rate'] <= 104), 'Catch_Rate'] = 1\n    dataset.loc[(dataset['Catch_Rate'] > 104) & (dataset['Catch_Rate'] <= 154), 'Catch_Rate'] = 2\n    dataset.loc[(dataset['Catch_Rate'] > 154) & (dataset['Catch_Rate'] <= 204), 'Catch_Rate'] = 3\n    dataset.loc[ dataset['Catch_Rate'] >= 255, 'Catch_Rate'] = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection\n\nWe drop unnecessary columns/features and keep only the useful ones for our experiment."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_drop = ['Number', 'Name', 'Total', 'HP', 'Sp_Atk', 'Sp_Def', 'Speed', 'Generation','Color', 'hasGender', 'Egg_Group_1', 'hasMegaEvolution','Height_m', 'Weight_kg', 'Body_Style', 'shuffle','Pr_Male_Band', 'Attack_Band', 'Defense_Band', 'Catch_Rate_Band']\ntrain = train.drop(train_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_drop = ['Number', 'Name', 'Total', 'HP', 'Sp_Atk', 'Sp_Def', 'Speed', 'Generation',\n       'Color', 'hasGender', 'Egg_Group_1', 'Egg_Group_2', 'isLegendary',\n       'hasMegaEvolution', 'Height_m', 'Weight_kg', 'Body_Style',\n       'shuffle']\ntest = test.drop(test_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done with Feature Selection/Engineering. Now, we are ready to train a classifier with our feature set."},{"metadata":{},"cell_type":"markdown","source":"## Classification & Accuracy\nDefine training and testing set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop('isLegendary', axis=1)\ny_train = train['isLegendary']\nX_test = test.copy()\n\nX_train.shape, y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many classifying algorithms. Among them, we will apply the following Classification algorithms to predict a legendary Pokémon:\n\n   - Logistic Regression\n   - Support Vector Machines (SVC)\n   - *k*-Nearest Neighbor (KNN)\n   - Decision Tree\n   - Random Forest\n   - Naive Bayes (GaussianNB)\n   - Perceptron\n   - Stochastic Gradient Descent (SGD)\n\nHere is the training and testing procedure:\n\n   - First, we train these classifiers with our training data.\n   - After that, using the trained classifier, we predict the Survival outcome of test data.\n   - Finally, we calculate the accuracy score (in percentange) of the trained classifier.\n\n**Please note**: that the accuracy score is generated based on our training dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train, y_train) * 100, 2)\nprint(str(acc_log_reg) + ' percent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine (SVM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SVC()\nclf.fit(X_train, y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_svc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### *k*-Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors = 3)\nclf.fit(X_train, y_train)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_knn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_decision_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest = clf.predict(X_test)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_random_forest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GaussianNB()\nclf.fit(X_train, y_train)\ny_pred_gnb = clf.predict(X_test)\nacc_gnb = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_gnb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = Perceptron(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_perceptron = clf.predict(X_test)\nacc_perceptron = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_perceptron)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stochastic Gradient Descent (SGD)"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_sgd = clf.predict(X_test)\nacc_sgd = round(clf.score(X_train, y_train) * 100, 2)\nprint (acc_sgd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix\n\nA confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class and *vice versa*. The name stems from the fact that it makes it easy to see if the system is confusing two classes (*i.e.* commonly mislabelling one as another).\n\nIn predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.\n\nHere's another guide explaining Confusion Matrix with example.\n\nIn our Pokémon case:\n\n   - **True Positive**: The classifier predicted legendary and the Pokémon was actually a legendary.\n\n   - **True Negative**: The classifier predicted not legendary and the Pokémon was not a legendary.\n\n   - **False Positive**: The classifier predicted legendary but the Pokémon was not a legendary.\n\n   - **False Negative**: The classifier predicted not legendary the Pokémon was actually a legendary.\n    \nIn the example code below, we plot a confusion matrix for the prediction of Random Forest Classifier on our training dataset. This shows how many entries are correctly and incorrectly predicted by our classifer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest_training_set = clf.predict(X_train)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy: %i %% \\n\"%acc_random_forest)\n\nclass_names = ['Legendary', 'Not Legendary']\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_pred_random_forest_training_set)\nnp.set_printoptions(precision=2)\n\nprint ('Confusion Matrix in Numbers')\nprint (cnf_matrix)\nprint ('')\n\ncnf_matrix_percent = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n\nprint ('Confusion Matrix in Percentage')\nprint (cnf_matrix_percent)\nprint ('')\n\ntrue_class_names = ['True Legendary', 'True Not Legendary']\npredicted_class_names = ['Predicted Legendary', 'Predicted Not Legendary']\n\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, \n                             index = true_class_names,\n                             columns = predicted_class_names)\n\ndf_cnf_matrix_percent = pd.DataFrame(cnf_matrix_percent, \n                                     index = true_class_names,\n                                     columns = predicted_class_names)\n\nplt.figure(figsize = (15,5))\n\nplt.subplot(121)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n\nplt.subplot(122)\nsns.heatmap(df_cnf_matrix_percent, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing Models\n\nLet's compare the accuracy score of all the classifier models used above."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', \n              'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', \n              'Perceptron', 'Stochastic Gradient Decent'],\n    \n    'Score': [acc_log_reg, acc_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest, acc_gnb, \n              acc_perceptron, acc_sgd]\n    })\n\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above table, we can see that Decision Tree and Random Forest classfiers have the highest accuracy score.\n\nBetween the two, we choose **Random Forest Classifier** as it has the ability to limit overfitting as compared to Decision Tree classifier."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}