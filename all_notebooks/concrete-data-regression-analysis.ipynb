{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Thanks for visiting this notebook!!!\n"},{"metadata":{},"cell_type":"markdown","source":"### please **UPVOTE** if you find this notebook useful...!"},{"metadata":{},"cell_type":"markdown","source":"### What you will learn!!!\n- Regression\n- Exploratory Data Analysis\n- Modeling\n- Hyper parameter tuning"},{"metadata":{},"cell_type":"markdown","source":"## Concrete Compressive Strength Prediction\n\n\nConcrete is one of the most important materials in Civil Engineering. Knowing the compressive strength of concrete is very important when constructing a building or a bridge. The Compressive Strength of Concrete is a highly nonlinear function of ingredients used in making it and their characteristics. Thus, using Machine Learning to predict the Strength could be useful in generating a combination of ingredients which result in high Strength.\n\n\n### Problem Statement\nPredicting Compressive Strength of Concrete given its age and quantitative measurements of ingredients.\n\n### Data Description\n\n* Number of instances - 1030\n* Number of Attributes - 9\n  * Attribute breakdown - 8 quantitative inputs, 1 quantitative output\n\n#### Attribute information\n##### Inputs\n* Cement\n* Blast Furnace Slag\n* Fly Ash\n* Water\n* Superplasticizer\n* Coarse Aggregate\n* Fine Aggregate\n\nAll above features measured in kg/$m^3$\n\n* Age (in days)\n\n##### Output\n* Concrete Compressive Strength (Mpa)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nplt.rc('font',size=14)\nsns.set(style='white')\nsns.set(style='darkgrid',color_codes=True)\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split, cross_val_score\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Loading the Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/concrete.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simplifying Column names, since they appear to be too lengthy."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data quality report"},{"metadata":{},"cell_type":"markdown","source":"### 1.a Univariate analysis"},{"metadata":{},"cell_type":"markdown","source":"##### checking datatypes"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Data types information"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### There are 1030 rows and 9 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# column names\ndata.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset distribution\ndata.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- cement,slag,ash are left skewed."},{"metadata":{},"cell_type":"markdown","source":"###### 1.a -> Checking for 'null' values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [col for col in data.columns.tolist() if col not in ['strength']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def univariate_analysis(data):\n    for col in features:\n        print(\"*\"*50)\n        print(\"Column name: \", col)\n        print('Range of values: ', data[col].max() - data[col].min())\n        print(\"<- Pivote values -> \")\n        print('Minimum value: ', data[col].min())\n        print('Maximum value: ',data[col].max())\n        print('Mean value: ', data[col].mean())\n        print('Median value: ',data[col].median())\n        print('Standard deviation: ', data[col].std())\n        print('Null values: ',data[col].isnull().any())\n        print(\"<- Outlier Detection -> \")\n        Q1=data[col].quantile(q=0.25)\n        Q3=data[col].quantile(q=0.75)\n        print('1st Quartile (Q1) is: ', Q1)\n        print('3st Quartile (Q3) is: ', Q3)\n        print('Interquartile range (IQR) is ', stats.iqr(data[col]))\n        L_outliers=Q1-1.5*(Q3-Q1)\n        U_outliers=Q3+1.5*(Q3-Q1)\n        print(f'Lower outliers in {col}: {L_outliers}')\n        print(f'Upper outliers in {col}: {U_outliers}' )\n        print(f'Number of outliers in {col} upper : ', data[data[col] > U_outliers][col].count())\n        print(f'Number of outliers in {col} lower : ', data[data[col]<L_outliers][col].count())\n        print(f'% of Outlier in {col} upper: {round(data[data[col] > U_outliers][col].count()*100/len(data))} %')\n        print(f'% of Outlier in {col} lower: {round(data[data[col]<L_outliers][col].count()*100/len(data))} %')\n        fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n        #boxplot\n        sns.boxplot(x=col,data=data,orient='v',ax=ax1)\n        ax1.set_ylabel(col, fontsize=15)\n        ax1.set_title(f'Distribution of {col}', fontsize=15)\n        ax1.tick_params(labelsize=15)\n\n        #distplot\n        sns.distplot(data[col],ax=ax2)\n        ax2.set_xlabel(col, fontsize=15)\n        ax2.set_ylabel(col, fontsize=15)\n        ax2.set_title(f'{col} vs Strength', fontsize=15)\n        ax2.tick_params(labelsize=15)\n\n        #histogram\n        ax3.hist(data[col])\n        ax3.set_xlabel(col, fontsize=15)\n        ax3.set_ylabel(col, fontsize=15)\n        ax3.set_title(f'{col} vs Strength', fontsize=15)\n        ax3.tick_params(labelsize=15)\n\n        plt.subplots_adjust(wspace=0.5)\n        plt.tight_layout() \n        plt.show()\n        print(\"#\"*50)\nunivariate_analysis(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multivariate Analysis\n"},{"metadata":{},"cell_type":"markdown","source":"##### 1.b Checking the pairwise relations of Features."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be no high correlation between independant variables (features). This can be further confirmed by plotting the **Pearson Correlation coefficients** between the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(data['cement'],ax=ax2[0][0])\nsns.distplot(data['slag'],ax=ax2[0][1])\nsns.distplot(data['ash'],ax=ax2[0][2])\nsns.distplot(data['water'],ax=ax2[1][0])\nsns.distplot(data['superplastic'],ax=ax2[1][1])\nsns.distplot(data['coarseagg'],ax=ax2[1][2])\nsns.distplot(data['fineagg'],ax=ax2[2][0])\nsns.distplot(data['age'],ax=ax2[2][1])\nsns.distplot(data['strength'],ax=ax2[2][2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Observation :\n- cement is almost normal. \n- slag has  three gausssians and rightly skewed.\n- ash has two gaussians and rightly skewed.\n- water has three guassians and slighly left skewed.\n- superplastic has two gaussians and rightly skewed.\n- coarseagg has three guassians and almost normal.\n- fineagg has almost two guassians and looks like normal.\n- age has multiple guassians and rightly skewed."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\n\nplt.figure(figsize=(14,10))\nsns.heatmap(corr, annot=True, cmap='Blues')\nb, t = plt.ylim()\nplt.ylim(b+0.5, t-0.5)\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations\n* There are'nt any **high** correlations between **Compressive strength** and other features except for **Cement**, which should be the case for more strength.\n* **Age** and **Super plasticizer** are the other two features which are strongly correlated with **Compressive Strength**.\n* **Super Plasticizer** seems to have a negative high correlation with **Water**, positive correlations with **Fly ash** and **Fine aggregate**.\n\nWe can further analyze these correlations visually by plotting these relations."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nax = sns.distplot(data.strength)\nax.set_title(\"Compressive Strength Distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  2.c"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,10))\nsns.scatterplot(y=\"strength\", x=\"cement\", hue=\"water\", size=\"age\", data=data, ax=ax, sizes=(50, 300))\nax.set_title(\"Strength vs (Cement, Age, Water)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations from Strength vs (Cement, Age, Water)\n* Compressive **strength increases with amount of cement**\n* Compressive **strength increases with age**\n* Cement with **low age** requires **more cement** for **higher strength**\n* The **older the cement** is the **more water** it requires\n* Concrete **strength increases** when **less water** is used in preparing it  "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,10))\nsns.scatterplot(y=\"strength\", x=\"fineagg\", hue=\"ash\", size=\"superplastic\", \n                data=data, ax=ax, sizes=(50, 300))\nax.set_title(\"Strength vs (Fine aggregate, Super Plasticizer, FlyAsh)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations from CC Strength vs (Fine aggregate, Super Plasticizer, FlyAsh)\n* As **Flyash increases** the **strength decreases**\n* **Strength increases** with **Super plasticizer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,10))\nsns.scatterplot(y=\"strength\", x=\"fineagg\", hue=\"water\", size=\"superplastic\", \n                data=data, ax=ax, sizes=(50, 300))\nax.set_title(\"Strength vs (Fine aggregate, Super Plasticizer, Water)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observations from CC Strength vs (Fine aggregate, Super Plasticizer, Water)\n* **Strength decreases** with **increase in water**, **strength increases** with **increase in Super plasticizer** (already from above plots)\n* **More Fine aggregate** is used when **less water**, **more Super plasticizer** is used.\n"},{"metadata":{},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_df1 = data.copy()\nconcrete_df1.boxplot(figsize=(35,15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of outliers in cement: ',concrete_df1[((concrete_df1.cement - concrete_df1.cement.mean()) / concrete_df1.cement.std()).abs() >3]['cement'].count())\nprint('Number of outliers in slag: ',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) / concrete_df1.slag.std()).abs() >3]['slag'].count())\nprint('Number of outliers in ash: ',concrete_df1[((concrete_df1.ash - concrete_df1.ash.mean()) / concrete_df1.ash.std()).abs() >3]['ash'].count())\nprint('Number of outliers in water: ',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) / concrete_df1.water.std()).abs() >3]['water'].count())\nprint('Number of outliers in superplastic: ',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) / concrete_df1.superplastic.std()).abs() >3]['superplastic'].count())\nprint('Number of outliers in coarseagg: ',concrete_df1[((concrete_df1.coarseagg - concrete_df1.coarseagg.mean()) / concrete_df1.coarseagg.std()).abs() >3]['coarseagg'].count())\nprint('Number of outliers in fineagg: ',concrete_df1[((concrete_df1.fineagg - concrete_df1.fineagg.mean()) / concrete_df1.fineagg.std()).abs() >3]['fineagg'].count())\nprint('Number of outliers in age: ',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) / concrete_df1.age.std()).abs() >3]['age'].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here, we have used Standard deviation method to detect the outliers.If we have any data point that is more than 3 times the standard deviation, then those points are very likely to be outliers.\n* We can see that slag, water, superplastic and age contain outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Records containing outliers in slag: \\n',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) / concrete_df1.slag.std()).abs() >3]['slag'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Records containing outliers in water: \\n',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) / concrete_df1.water.std()).abs() >3]['water'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Records containing outliers in superplastic: \\n',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) / concrete_df1.superplastic.std()).abs() >3]['superplastic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Records containing outliers in age: \\n',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) / concrete_df1.age.std()).abs() >3]['age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing wit outliers"},{"metadata":{},"cell_type":"markdown","source":"we will replace the outliers with the median"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col_name in concrete_df1.columns[:-1]:\n    q1 = concrete_df1[col_name].quantile(0.25)\n    q3 = concrete_df1[col_name].quantile(0.75)\n    iqr = q3 - q1\n    \n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    concrete_df1.loc[(concrete_df1[col_name] < low) | (concrete_df1[col_name] > high), col_name] = concrete_df1[col_name].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_df1.boxplot(figsize=(35,15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"##### Scaling \nStandardizing the data i.e. to rescale the features to have a mean of zero and standard deviation of 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_df_z = concrete_df1.apply(zscore)\nconcrete_df_z = pd.DataFrame(concrete_df_z,columns=data.columns) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### seperate feature and targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = concrete_df_z.iloc[:,:-1]         # Features - All columns but last\ny = concrete_df_z.iloc[:,-1]          # Target - Last Column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Splitting data into Training and Test. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.a Model Building\n"},{"metadata":{},"cell_type":"markdown","source":"#### Decision Trees\n\nWe can use Decision Trees, since we have a lot of zeros in some of the input features as seen from their distributions in the pair plot above. This would help the decision trees build trees based on some conditions on features which can further improve performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)\n\n#printing the feature importance\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns).sort_values('Imp', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- cement is the most important feature\n- Here, ash, coarseagg, fineagg, superplastic and slag are the less significant variable.These will impact less to the strength column. This we have seen in pairplot also."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using Decision Tree:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using Decision Tree:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is a overfitting in the model as the dataset is performing 99% accurately in trainnig data. However, the accuracy on test data drops."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT},index={'1'})\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Decision Tree k fold'], 'accuracy': [accuracy]},index={'2'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Drop least significant features"},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_df2=concrete_df_z.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#independent and dependent variable\nX = concrete_df2.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df2['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'], index=X_train.columns).sort_values('Imp', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The acuracy on testing dataset is not improved, still it is an overfit model."},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Decision Tree2'], 'accuracy': [acc_DT]},index={'3'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regularising/Pruning of Decision Tree\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Feature importances: \\n',pd.DataFrame(reg_dt_model.feature_importances_,columns=['Imp'], index=X_train.columns).sort_values('Imp', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the Regularized Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydotplus --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nfrom io import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\nimport graphviz\nbank_df=concrete_df_z\nxvar = bank_df.drop('strength', axis=1)\nfeature_cols = xvar.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dot_data = StringIO()\nexport_graphviz(reg_dt_model, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('concrete_pruned.png')\nImage(graph.create_png())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree'], 'accuracy': [acc_RDT]},index={'4'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(reg_dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree k fold'], 'accuracy': [accuracy]},index={'5'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_df3=concrete_df_z.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = concrete_df3.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df3['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree2'], 'accuracy': [acc_RDT]},index={'6'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-Mean Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_range = range( 1, 15 )  \ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(concrete_df1)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster = KMeans( n_clusters = 6, random_state = 2354 )\ncluster.fit(concrete_df_z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction=cluster.predict(concrete_df_z)\nconcrete_df_z[\"GROUP\"] = prediction     \n# Creating a mirror copy for later re-use instead of building repeatedly\nconcrete_df_z_copy = concrete_df_z.copy(deep = True)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroids = cluster.cluster_centers_\ncentroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"centroid_df = pd.DataFrame(centroids, columns = list(concrete_df1) )\ncentroid_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot centroids and the data in the cluster into box plots\nconcrete_df_z.boxplot(by = 'GROUP',  layout=(3,3), figsize=(15, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here, None of the dimensions are good predictor of target variable.\n* For all the dimensions (variables) every cluster have a similar range of values except in one case.\n* We can see that the body of the cluster are overlapping.\n* So in k means, though, there are clusters in datasets on different dimensions. But we can not see any distinct characteristics of these clusters which tell us to break data into different clusters and build separate models for them."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=RandomForestRegressor()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using RFR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using RFR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RFR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RFR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This model is also overfit."},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor'], 'accuracy': [acc_RFR]},index={'7'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor k fold'], 'accuracy': [accuracy]},index={'8'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GradientBoostingRegressor()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_GBR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_GBR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor'], 'accuracy': [acc_GBR]},index={'9'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor k fold'], 'accuracy': [accuracy]},index={'10'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bagging regressor  "},{"metadata":{"trusted":true},"cell_type":"code","source":"model=BaggingRegressor()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_BR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_BR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Bagging Regressor'], 'accuracy': [acc_BR]},index={'13'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Fold Cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Bagging Regressor k fold'], 'accuracy': [accuracy]},index={'14'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error=[]\nfor i in range(1,30):\n    knn = KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i!=y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.plot(range(1,30),error,color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsRegressor(n_neighbors=3)\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using KNNR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using KNNR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_K=metrics.r2_score(y_test, y_pred)\nprint('Accuracy KNNR: ',acc_K)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['KNN Regressor'], 'accuracy': [acc_K]},index={'15'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['KNN Regressor k fold'], 'accuracy': [accuracy]},index={'16'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVR(kernel='linear')\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using SVR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using SVR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_S=metrics.r2_score(y_test, y_pred)\nprint('Accuracy SVR: ',acc_S)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Support Vector Regressor'], 'accuracy': [acc_S]},index={'17'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['SVR k fold'], 'accuracy': [accuracy]},index={'18'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemeble KNN Regressor, SVR, LR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\n\nLR=LinearRegression()\nKN=KNeighborsRegressor(n_neighbors=3)\nSVM=SVR(kernel='linear') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evc=VotingRegressor(estimators=[('LR',LR),('KN',KN),('SVM',SVM)])\nevc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = evc.predict(X_test)\n# performance on train data\nprint('Performance on training data using ensemble:',evc.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using ensemble:',evc.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_E=metrics.r2_score(y_test, y_pred)\nprint('Accuracy ensemble: ',acc_E)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Ensemble'], 'accuracy': [acc_E]},index={'19'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K fold cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(evc,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tempResultsDf = pd.DataFrame({'Method':['Ensemble k fold'], 'accuracy': [accuracy]},index={'20'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* After applying all the models we can see that Random Forest Regressor, Random Forest Regressor k fold, Gradient Boost Regressor, Gradient Boost Regressor k fold, Bagging Regressor are giving better results as compared to other models.\n* Now as the dataset have different gaussians, we can apply k means clustering and then we can apply the models and compare the accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Bootstrap Sampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"concrete_XY = X.join(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.c Using Gradient Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\nvalues = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbmTree = GradientBoostingRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbmTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbmTree.score(test[:, :-1] , y_test)\n    predictions = gbmTree.predict(test[:, :-1])  \n\n    stats.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"values = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    rfTree = RandomForestRegressor(n_estimators=100)\n    # fit against independent variables and corresponding target values\n    rfTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_test)\n    predictions = rfTree.predict(test[:, :-1])  \n\n    stats.append(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bootstrap random forest  classification model performance is between 84%-90.8% which is better than other classification algorithms."},{"metadata":{},"cell_type":"markdown","source":"# Please **upvote** if you liked this notebook!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}