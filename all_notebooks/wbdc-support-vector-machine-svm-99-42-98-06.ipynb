{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import division\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, recall_score\nfrom math import log\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nimport statistics\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom imblearn.metrics import specificity_score\nfrom imblearn.metrics import sensitivity_score\nfrom imblearn.metrics import geometric_mean_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.datasets import make_blobs\n\n\n\nclf_rf_2 = RandomForestClassifier(random_state=43)   \nrfe = RFE(estimator=clf_rf_2, n_features_to_select=16, step=1)\n\n\nfrom sklearn.svm import SVC\nsvm=SVC(kernel = 'linear',C=.3, gamma=30, probability = True)\nsvmm=SVC(random_state=1, probability=True)\n#svm=SVC(probability=True)\n\n#clf = AdaBoostClassifier(SVC(probability=True,kernel='linear'),n_estimators=50, learning_rate=1.0, algorithm='SAMME')\n\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\nab = pd.read_csv(\"../input/lung-cancer-dataset/lung_cancer_examples.csv\")\nwbc = pd.read_csv(\"../input/wisconsin-breast-cancer/wbc.csv\")\nwbc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\n\nab.drop([\"Name\",\"Surname\"],axis=1,inplace=True)\n\nwbc.drop([\"Sample code number \"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M = df[df.diagnosis==\"M\"]\nB = df[df.diagnosis==\"B\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(M.radius_mean,M.texture_mean, color=\"red\",label=\"Bad\")\nplt.scatter(B.radius_mean,B.texture_mean, color=\"green\",label=\"Good\")\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.diagnosis = [1 if each == \"M\" else -1 for each in  df.diagnosis]\n#wbc.Class = [1 if each == 4 else -1 for each in  wbc.Class]\n#ab.Result = [1 if each == \"1\" else 0 for each in  ab.Result]\n# bcs = pd.DataFrame(preprocessing.scale(df.ix[:,2:32]))\n# bcs.columns = list(df.ix[:,2:32].columns)\n# bcs['diagnosis'] = df['diagnosis']\n# y=bcs.diagnosis.values\n# x_data=bcs.drop([\"diagnosis\"],axis=1)\n\ny=df.diagnosis.values\nx_data=df.drop([\"diagnosis\"],axis=1)\n\nyl=ab.Result.values\nxl=ab.drop([\"Result\"],axis=1)\n\nwy=wbc.Class.values\nwx=wbc.drop([\"Class\"],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#wbc.Class\n#df.diagnosis\nwx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclustering = AgglomerativeClustering(n_clusters=6).fit(x_data)\nx_data['clabel']=clustering.labels_\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nscatter = ax.scatter(x_data['radius_mean'],x_data['texture_mean'],\n                     c=x_data['clabel'],s=50)\nax.set_title('Agglomerative Clustering')\nax.set_xlabel('radius_mean')\nax.set_ylabel('texture_mean')\nplt.colorbar(scatter)\n\n\n\n\n\nxx_data = x_data.copy()\nx=(xx_data - np.min(xx_data))/(np.max(xx_data)-np.min(xx_data))\n\nclustering = AgglomerativeClustering(n_clusters=6).fit(x)\nx_data['clabel']=clustering.labels_\nprint(x_data)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\nscatter = ax.scatter(x_data['radius_mean'],x_data['texture_mean'],\n                     c=x_data['clabel'],s=50)\nax.set_title('Agglomerative Clustering')\nax.set_xlabel('radius_mean')\nax.set_ylabel('texture_mean')\nplt.colorbar(scatter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array=df.values\nxxx = array[:,1:31]\nyyy=array[:,0]\nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(xxx, yyy)\nprint(model.feature_importances_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feat_importances = pd.Series(model.feature_importances_, index=x_data.columns)\n# feat_importances.nlargest(30).plot(kind='barh')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res= dict(zip(list(x_data.columns)  , mutual_info_classif(xxx,yyy, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)))\nprint(sorted(res.items(), key = \n             lambda kv:(kv[1], kv[0]))) \nprint(\"\\n\")\nprint(len(res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #for i in range(model.feature_importances_.size):\n#     #if(i>29):\n#         #continue\n        \n#     #if(model.feature_importances_[i]<.02):\n#          #x_data.drop([x_data.columns[i+1]], axis ='columns', inplace=True)\n        \n drop_list = ['fractal_dimension_se','texture_se','smoothness_se','symmetry_se','fractal_dimension_mean','symmetry_mean']\n xx_data = x_data.copy()\n# #drop(drop_list,axis = 1 )\n# copy()\n\n# #       # do not modify x, we will use it later \n# xx_data.head()\n       \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalization\nx=(xx_data - np.min(xx_data))/(np.max(xx_data)-np.min(xx_data))\n# x=xx_data.copy()\n#x=(x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data))\n\nxll=(xl - np.min(xl))/(np.max(xl)-np.min(xl))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list =['smoothness_mean','compactness_mean','symmetry_mean','radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se','compactness_worst','concavity_worst','fractal_dimension_worst']\nx = x.copy()\n#copy()\n#drop(drop_list,axis = 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# kmeans = KMeans(n_clusters=5, random_state=42).fit(x)\n# x['clabel']=kmeans.labels_\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf =StratifiedKFold(n_splits=10,random_state=42)\nskf.get_n_splits(x)\n\nprint(skf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=[None]*10\ni=0\nfor train_index, test_index in skf.split(x,y):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    svm.fit(x_train,y_train)\n    scores[i]=svm.score(x_test,y_test)\n    i=i+1\n    \n    \ni=0\n\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(statistics.mean(scores))\nprint(statistics.stdev(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=15,random_state=42)\nkf.get_n_splits(x)\n\nprint(kf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=[None]*15\ni=0\nfor train_index, test_index in kf.split(x):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    svm.fit(x_train,y_train)\n    scores[i]=svm.score(x_test,y_test)\n    i=i+1\n    \n    \ni=0\n\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(statistics.mean(scores))\nprint(statistics.stdev(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a= make_blobs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'C': [0.1,0.2,0.3,1, 10, 100, 1000],  \n              'gamma': [100,30,10,1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['linear']} \n\ngrid = GridSearchCV(svmm, param_grid, refit = True, verbose = False,cv=10)\ngrid.fit(x, y) \n# print best parameter after tuning \nprint(grid.best_params_) \n  \n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x_train,x_test,y_train,y_test=train_test_split(xll,yl,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=cross_val_score(svm, x, y, cv=10)\nscores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm.fit(x_train,y_train)\nprint(\"print accuracy of svm alg: \", svm.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SelectFromModel(svm, prefit=True)\nxx_new = model.transform(x)\nxx_new\nmodel.get_support()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rfe = rfe.fit(x_train, y_train)\n# #print(\"print accuracy of svm alg: \", rfe.score(x_test,y_test))\n# accuracy = accuracy_score(y_test, rfe.predict(x_test))\n# print('Accuracy is: ', accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"print accuracy of svm alg: \", clf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores=[None]*15\nsp_scores=[None]*15\nsn_scores=[None]*15\ngm_scores=[None]*15\nre_scores=[None]*15\ntn_scores=[None]*15\n\nii=0\nfor train_index, test_index in kf.split(x,y):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    row_count=len(x_train.axes[0])\n    weight_arr1 = [None]*row_count\n\n    i=0\n    for i in range(row_count):\n        weight_arr1[i]=1/row_count\n\n\n    x_new=x_train.copy()\n    x_new['diagnosis']=y_train\n    x_new.insert(3,'weight',weight_arr1)\n\n    #print(x_new)\n\n    \n    l=list(y_train)\n    n=l.count(-1)\n    p=l.count(1)\n    k=min(n,p)\n\n    ap=[None]*p\n    an=[None]*n\n\n    j=0\n    s=[0]*(y_train.size)\n\n    alphaa=[0]*5\n    kk=0\n\n    for i in range(5):\n\n        x_new.set_index('diagnosis' , inplace=True)\n        df_plus = x_new.loc[1]\n        df_minus = x_new.loc[-1]\n\n\n        #print(x_new)\n\n        a=0\n        for a in range(p):\n            ap[a]=1\n\n        df_plus['diagnosis']=ap\n\n        b=0\n        for b in range(n):\n            an[b]=-1\n\n        df_minus['diagnosis']=an\n\n        g_plus = df_plus.nlargest(k, ['weight'])\n        #print(g_plus)\n\n        g_minus = df_minus.nlargest(k, ['weight'])\n        g_total = pd.concat([g_plus,g_minus],ignore_index=False)\n        g_total\n        \n#         print(g_total['weight'])\n        yy=g_total.diagnosis.values\n        xx=g_total.drop([\"diagnosis\",\"weight\"],axis=1)\n\n\n        svm.fit(xx,yy)\n        yy_pred = svm.predict(x_train)\n\n    #     rfe = rfe.fit(xx, yy)\n    #     yy_pred = rfe.predict(x_train)\n\n\n        sum_weight=0\n        pp=0\n        for pp in range(yy_pred.size):\n            if(y_train[pp]!=yy_pred[pp]):\n                sum_weight+=x_new['weight'].iloc[pp]\n\n\n        ##print(sum_weight)\n\n        if(sum_weight!=0):\n            alpha=(1/2)*np.log((1-sum_weight)/sum_weight)\n        else:\n            alpha=0\n\n\n        alphaa[kk]=alpha\n        kk=kk+1\n\n    \n        r=0\n        for r in range(y_train.size):\n            s[r]+=alpha*yy_pred[r]\n\n        t=0\n        for t in range(y_train.size):\n             x_new['weight'].iloc[t]=np.exp((-1)*s[t]*(y_train[t]))\n\n        x_new['weight']=x_new['weight'].div(x_new['weight'].sum())\n\n    \n       \n    #adaboost        \n\n    #          if(y_train[i]!=yy_pred[i]):\n\n    #                 x_new['weight'].iloc[i]= (x_new['weight'].iloc[i])*(np.exp(alpha))\n    #                 #(np.exp((-1)*((alpha*yy_pred[i])*y_train[i])))\n\n    #          else:\n    #             x_new['weight'].iloc[i]= (x_new['weight'].iloc[i])*(np.exp((-1)*alpha))\n\n\n\n\n            #/(np.min(x_new['weight']))\n\n        #x_new=(x_new - np.min(x_new))/(np.max(x_new)-np.min(x_new))\n\n        x_new['diagnosis']=y_train\n        #print(x_new['weight'])\n        j=j+1\n       \n\n\n\n        #yy_pred_prob = svm.predict_proba(x_train)[:,0]\n        #yy_pred_prob\n\n\n    \n   \n\n\n\n\n#     print(j)\n    y_pred=svm.predict(x_test)\n    yn_pred=svm.predict(x_train)\n    \n    scores[ii]=metrics.accuracy_score(y_test,y_pred)\n    tn_scores[ii]=metrics.accuracy_score(y_train,yn_pred)\n    \n    gm_scores[ii]=geometric_mean_score(y_test, y_pred, average='macro')\n    sp_scores[ii]=specificity_score(y_test, y_pred, average='macro')\n    sn_scores[ii]=sensitivity_score(y_test, y_pred, average='macro')\n    re_scores[ii]=recall_score(y_test, y_pred, average='macro')\n    \n    \n    \n    \n    ii=ii+1\n\n\nii=0\n\nscores\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test accuracy',statistics.mean(scores))\nprint('Standard Deviation',statistics.stdev(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train accuracy',statistics.mean(tn_scores))\nprint('Standard Deviation',statistics.stdev(tn_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(statistics.mean(re_scores))\nprint(statistics.stdev(re_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(statistics.mean(gm_scores))\nprint(statistics.stdev(gm_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(statistics.mean(sn_scores))\nprint(statistics.stdev(sn_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(statistics.mean(sp_scores))\nprint(statistics.stdev(sp_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# x,y = make_blobs(n_samples=532, centers=2, random_state=42, n_features=31)\n# plt.scatter(x[:, 0], x[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# ax = plt.gca()\n# xlim = ax.get_xlim()\n# ylim = ax.get_ylim()\n\n# aa = np.linspace(xlim[0], xlim[1], 30)\n# bb = np.linspace(ylim[0], ylim[1], 30)\n# BB, AA = np.meshgrid(bb, aa)\n# xy = np.vstack([AA.ravel(), BB.ravel()]).T\n# Z = svm.decision_function(xy).reshape(AA.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row_count=len(x_train.axes[0])\n#print(row_count)\nweight_arr1 = [None]*row_count\n \ni=0\nfor i in range(row_count):\n    weight_arr1[i]=1/row_count\n    \n\nx_new=x_train.copy()\n\nx_new['diagnosis']=y_train\nx_new.insert(3,'weight',weight_arr1)\n\n#print(x_new)\n\n    \nl=list(y_train)\nn=l.count(-1)\np=l.count(1)\nk=min(n,p)\n    \nap=[None]*p\nan=[None]*n\n\n#x_new.set_index(\"diagnosis\" , inplace=True)\n#df_plus = x_new.loc[[1]]\n#df_minus = x_new.loc[[-1]]\n\n#for i in range(p):\n    #ap[i]=1\n\n#df_plus['diagnosis']=ap\n    \n#for i in range(p):\n    #an[i]=-1\n    \n#df_minus['diagnosis']=an\n    \nj=0\ns=[0]*(y_train.size)\n\nalphaa=[0]*5\nkk=0\n\nfor i in range(5):\n    \n    x_new.set_index('diagnosis' , inplace=True)\n    df_plus = x_new.loc[1]\n    df_minus = x_new.loc[-1]\n    \n    \n    #print(x_new)\n    \n    a=0\n    for a in range(p):\n        ap[a]=1\n\n    df_plus['diagnosis']=ap\n    \n    b=0\n    for b in range(n):\n        an[b]=-1\n    \n    df_minus['diagnosis']=an\n    \n    g_plus = df_plus.nlargest(k, ['weight'])\n    #print(g_plus)\n    \n    g_minus = df_minus.nlargest(k, ['weight'])\n    g_total = pd.concat([g_plus,g_minus],ignore_index=False)\n    g_total\n    \n    yy=g_total.diagnosis.values\n    xx=g_total.drop([\"diagnosis\",\"weight\"],axis=1)\n    \n    \n    svm.fit(xx,yy)\n    yy_pred = svm.predict(x_train)\n    \n#     rfe = rfe.fit(xx, yy)\n#     yy_pred = rfe.predict(x_train)\n    \n    \n    sum_weight=0\n    pp=0\n    for pp in range(yy_pred.size):\n        if(y_train[pp]!=yy_pred[pp]):\n            sum_weight+=x_new['weight'].iloc[pp]\n        \n    \n    print(sum_weight)\n    \n    if(sum_weight!=0):\n        alpha=(1/2)*np.log((1-sum_weight)/sum_weight)\n    else:\n        alpha=0\n        \n        \n    alphaa[kk]=alpha\n    kk=kk+1\n    \n    \n    r=0\n    for r in range(y_train.size):\n        s[r]+=alpha*yy_pred[r]\n    \n    t=0\n    for t in range(y_train.size):\n         x_new['weight'].iloc[t]=np.exp((-1)*s[t]*(y_train[t]))\n            \n    x_new['weight']=x_new['weight'].div(x_new['weight'].sum())\n        \n    \n       \n#adaboost        \n        \n#          if(y_train[i]!=yy_pred[i]):\n                                              \n#                 x_new['weight'].iloc[i]= (x_new['weight'].iloc[i])*(np.exp(alpha))\n#                 #(np.exp((-1)*((alpha*yy_pred[i])*y_train[i])))\n                 \n#          else:\n#             x_new['weight'].iloc[i]= (x_new['weight'].iloc[i])*(np.exp((-1)*alpha))\n                 \n       \n        \n        \n        #/(np.min(x_new['weight']))\n    \n    #x_new=(x_new - np.min(x_new))/(np.max(x_new)-np.min(x_new))\n    \n    x_new['diagnosis']=y_train\n    print(x_new['weight'])\n    j=j+1\n    print(j)\n    \n        \n    \n    #yy_pred_prob = svm.predict_proba(x_train)[:,0]\n    #yy_pred_prob\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yy_pred\n#yy_pred_prob\n#xxx\nx_new\n\n#df_minus\n#yy\n#x_train\n#x_new['radius_mean'].iloc[1]\n#df_minus\n#df_plus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#g_plus.head()\n#g_total\n#df_plus.head()\n#xx\n#print(\"print accuracy of svm alg: \", svm.score(xx,yy))\n#y_train\n    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#svm\n#from sklearn.svm import SVC\n\n#svm=SVC(random_state=1, probability=True)\n#svm.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svm.predict(x_test)\n#rfe.predict(x_test)\n#\n\n\ny_pred\nalphaa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f=[0]*y_pred.size\nyyy_pred=[0]*y_pred.size\n\n\n\nfor i in range(5):\n    for j in range(y_pred.size):\n        f[j]=f[j]+y_pred[j]*alphaa[i]\n        \n\n        \nfor j in range(y_pred.size):\n    if(f[j]<0):\n        yyy_pred[j]=-1\n    else:\n        yyy_pred[j]=1\n        \n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred_prob = svm.predict_proba(x_test)[:,0]\n#y_pred_prob\n#ll=list(y_pred_prob)\n\n\n#prob_per_class_dictionary = dict(zip(svm.classes_, y_pred_prob))\n#results_ordered_by_probability = map(lambda x: x[0], sorted(zip(svm.classes_,y_pred_prob), key=lambda x: x[1], reverse=True))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prob_per_class_dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#results_ordered_by_probability","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores=[None]*10\n# i=0\n# for train_index, test_index in skf.split(x,y):\n#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n#     x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n#     y_train, y_test = y[train_index], y[test_index]\n#     #svm.fit(x_train,y_train)\n#     scores[i]=svm.score(x_test,y_test)\n#     i=i+1\n    \n    \n# i=0\n\n# scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(statistics.mean(scores))\n# print(statistics.stdev(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test\nfrom sklearn import metrics\nprint(\"print accuracy of svm alg: \", metrics.accuracy_score(y_test,yyy_pred))\n\nprint(\"print accuracy of svm alg: \", metrics.accuracy_score(y_test,y_pred))\n\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n\nprint(\"print accuracy of svm alg: \", svm.score(x_test,y_test))\nprint(\"print accuracy of svm alg: \", svm.score(x_train,y_train))\nprint(\"print accuracy of svm alg: \", svm.score(xx,yy))\nprint(\"\\n\",confusion_matrix(y_test, y_pred))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}