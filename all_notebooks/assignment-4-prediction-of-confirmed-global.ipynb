{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First, use population in Hubei confirmed with coronavirus to find a reasonable RNN model. Then, predict the confirmed population in Ontario with the model to verificate.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/ece657aw20asg4coronavirus/time_series_covid19_confirmed_global.csv')\nHubei = dataset[dataset['Province/State'].isin(['Hubei'])]\nprint(Hubei)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns; sns.set()\ncopy = dataset.copy()\ncorr = copy.corr(method='kendall')\nplt.figure(figsize=(18,12))\nsns.heatmap(corr, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot\nconfirmed_df = copy\ncountries=['China', 'Italy', 'Brazil', 'Canada', 'Germany']\nt = confirmed_df.loc[confirmed_df['Country/Region']=='China'].iloc[0,4:]\ns = pd.DataFrame({'China':t})\nfor c in countries:    \n    s[c] = confirmed_df.loc[confirmed_df['Country/Region']==c].iloc[0,4:]\npyplot.plot(range(t.shape[0]), s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = Hubei.drop(columns=['Province/State', 'Country/Region', 'Lat', 'Long'], axis = 1)\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value = data.columns.values\nvalue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = data.values\nfeature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date = data.columns.values.tolist()\ndata_new = {'population': feature[0]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data_new)\ndf.index = date\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0, 1))\ndata = scaler.fit_transform(df.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\n\nX, y = create_dataset(data, 10)\nX_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 42)\n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(10, 1), return_sequences = True, activation = 'relu'))\nmodel.add(LSTM(64, return_sequences = True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(64, return_sequences = True))\nmodel.add(Dropout(0.1))\nmodel.add(LSTM(128))\nmodel.add(Dense(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nlearning_rate = tf.keras.callbacks.ReduceLROnPlateau('value_loss', patience = 3, factor = 0.3, min_lr = 0.00001)\n\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mae'])\nmodel.fit(X_train, y_train, epochs = 200, batch_size = 20, validation_data = (X_test, y_test), validation_freq = 10, callbacks = [learning_rate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [df.values[-1]]\nnew = data.copy()\n\nfor i in range(20):\n    prediction = model.predict(new[-10:,:].reshape(1, 10, 1))\n    predictions.append(float(prediction * predictions[-1]))\n    new = np.append(new, prediction, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_predict = list(range(88, 88 + 20))\nplt.plot(df.values)\nplt.plot(data_predict, predictions[1:])\nplt.legend(['Origin', 'Prediction'], loc = 'lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Ontario = dataset[dataset['Province/State'].isin(['Ontario'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_o = Ontario.drop(columns=['Province/State', 'Country/Region', 'Lat', 'Long'], axis = 1)\ndata_o","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature = data_o.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_r = data_o.columns.values.tolist()\ndata_n = {'population': feature[0]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(data_n)\ndf.index = date\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sca = MinMaxScaler(feature_range = (0, 1))\ndata = sca.fit_transform(df.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = create_dataset(data, 10)\nX_train, X_test, y_train, y_test = tts(X, y, test_size = 0.2, random_state = 42)\n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(10, 1), return_sequences = True, activation = 'relu'))\nmodel.add(LSTM(64, return_sequences = True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(64, return_sequences = True))\nmodel.add(Dropout(0.1))\nmodel.add(LSTM(128))\nmodel.add(Dense(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nlearning_rate = tf.keras.callbacks.ReduceLROnPlateau('value_loss', patience = 3, factor = 0.3, min_lr = 0.00001)\n\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics = ['mae'])\nmodel.fit(X_train, y_train, epochs = 200, batch_size = 20, validation_data = (X_test, y_test), validation_freq = 10, callbacks = [learning_rate])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [df.values[-1]]\nnew = data.copy()\n\nfor i in range(15):\n    prediction = model.predict(new[-10:,:].reshape(1, 10, 1))\n    predictions.append(float(prediction * predictions[-1]))\n    new = np.append(new, prediction, axis = 0)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_predict = list(range(88, 88 + 15))\nplt.plot(df.values)\nplt.plot(data_predict, predictions[1:])\nplt.legend(['Origin', 'Prediction'], loc = 'upper left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}