{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"We will be working with US adult census data, extracted by Barry Becker from the 1994 US Census Database. The data set consists of anonymous information such as occupation, age, native country, race, capital gain, capital loss, education, work class and more. Each row is labelled as either having a salary greater than “>50K” or “<=50K”. \nThe goal here is to train a binary classifier on the training dataset to predict the column income_bracket which has two possible values “>50K” and “<=50K” and evaluate the accuracy of the classifier with the test dataset.\n\n\nIn this assignment, we implement logistic regression from scratch to get a good understanding of the key components of logistic regression:\n\n* hypothesis function\n* cost function\n* decision boundary\n* gradient descent algorithm\n\n"},{"metadata":{},"cell_type":"markdown","source":"We define de function to train and get the predictions of the model trained."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_with_file(data_file,iters):\n    import pandas as pd\n    import numpy as np\n    \"\"\"Trains a logisitc regression classifier.\n    Args:\n    data_file: a path to a csv file containing training data, without headers.\n    iters: the number of iterations to use when training the classifier\n\n    Returns:\n    weights: a column vector (1d numpy array) containing the weights learned in your classifier.\n    normalization_params: a dict mapping column names to (min, max) values from the training set\n\n    \"\"\"\n\n    COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n               \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n               \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n               \"income_bracket\"]\n\n    LABEL_COLUMN = \"Label\"\n\n    CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\",\n                           \"relationship\", \"race\", \"gender\",\"native_country\"]\n\n    CONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n                          \"hours_per_week\"]\n    \n    features = pd.read_csv(data_file,  names = COLUMNS, skipinitialspace = True)\n    features = features.dropna(how=\"any\",axis = 0)\n    \n    #one hot + categorical data\n    \n    features[LABEL_COLUMN] = (features[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n    \n    Xtrain = features.drop(['income_bracket','Label'], axis=1)\n    ytrain = features['Label']\n    \n    #Categorical tratament \n    \n    for col in CATEGORICAL_COLUMNS:\n        Xtrain = pd.concat([Xtrain, pd.get_dummies(Xtrain[col], prefix=col, prefix_sep=':')], axis=1)\n        Xtrain.drop(col, axis=1, inplace=True)\n    \n    # Normalization\n    \n    fmean = np.mean(Xtrain)\n    frange = np.amax(Xtrain) - np.amin(Xtrain)\n\n    #Vector Subtraction\n    Xtrain -= fmean\n\n    #Vector Division\n    Xtrain /= frange\n\n    normalization_params = [fmean,frange]\n    \n    # train :3\n    \n    N = len(features)\n    weights = [0] * Xtrain.shape[1]\n    lr = 0.01\n\n\n    for i in range(iters):\n    #1 - Get Predictions\n        labels = 1 / (1 + np.exp(-np.dot(Xtrain, weights) ))\n\n        gradient = np.dot(Xtrain.T,  labels - ytrain)\n\n    # Take the average cost derivative for each feature\n        gradient = (gradient/N)*lr\n\n    # - Subtract from our weights to minimize cost\n        weights -= gradient\n    \n    return weights,normalization_params\n\n\n\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def classify(data_file, weights, normalization_params):\n    import pandas as pd\n    import numpy as np\n \n    \"\"\"\nClassifies data based on the supplied logistic regression weights.\n\n  Args:\n    data_file: a path to a csv file containing test data, without headers.\n    weights: a column vectors containing the weights learned during training.\n    normalization_params: a dict mapping column names to (min, max) values from the training set\n\n  Returns:\n    a column vector containing either a 1 or a 0 for each row in data_file\n\"\"\"    \n    COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n               \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n               \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n               \"income_bracket\"]\n\n    LABEL_COLUMN = \"Label\"\n\n    CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\",\n                           \"relationship\", \"race\", \"gender\",\"native_country\"]\n\n    CONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n                          \"hours_per_week\"]\n\n    features = pd.read_csv(data_file,  names = COLUMNS, skipinitialspace = True, skiprows=1)\n    features = features.dropna(how=\"any\",axis = 0)\n    \n    #one hot + categorical data\n    \n    features[LABEL_COLUMN] = (features[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n    \n    Xtrain = features.drop(['income_bracket','Label'], axis=1)\n    ytrain = features['Label']\n    \n    for col in CATEGORICAL_COLUMNS:\n        Xtrain = pd.concat([Xtrain, pd.get_dummies(Xtrain[col], prefix=col, prefix_sep=':')], axis=1)\n        Xtrain.drop(col, axis=1, inplace=True)\n    \n    # Normalization\n    \n    fmean,frange = normalization_params \n\n    #Vector Subtraction\n    Xtrain -= fmean\n\n    #Vector Division\n    Xtrain /= frange\n    \n    labels = 1 / (1 + np.exp(-np.dot(Xtrain, weights[:Xtrain.shape[1]]) ))\n    labels = np.round(labels,0)\n    \n    \n    return labels,ytrain","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(predicted_labels, actual_labels):\n    import pandas as pd\n    import numpy as np\n    diff = predicted_labels - actual_labels\n    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))\n\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights, normalization_params = train_with_file(\"../input/adult-training.csv\",1000)\n\nlabels,ytrain = classify(\"../input/adult-test.csv\", weights, normalization_params)\n\nprint('The accuracy to a test dataset is:' , accuracy(labels,ytrain))\n\n","execution_count":4,"outputs":[{"output_type":"stream","text":"The accuracy to a test dataset is: 0.7175234936428967\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The results was satisfactory implementing Logistic Regression from scratch in Python, this algorithm work fast but the accuracy is less than a model from a library like sklearn but its still good enought to clasiffy with an accuracy of 0.71 in a test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}