{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes from Essey Abraham https://www.kaggle.com/esseytezare/image-colorization/notebook","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, UpSampling2D, Dropout\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom skimage.io import imshow\nfrom skimage.color import rgb2lab, lab2rgb, gray2rgb\nfrom skimage.transform import resize\nimport skimage.io\nfrom skimage.io import imsave\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport os\nimport glob\nimport seaborn as sns\nimport pandas as pd\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for train image pixel \ncy = []\ncx = []\nfor i in (glob.glob(\"../input/indian-dance-classification/Indian_Dance/test/*.jpg\")):\n    img = plt.imread(i)\n    a = np.shape(img)\n    c = np.reshape(img,(a[0]*a[1],a[2]))\n    cy.append(np.shape(c)[0])\n    cx.append(i)\ncolumns = ['Images','pixels']\ndt = np.array([cx,cy])\ndf = pd.DataFrame(dt.T, columns = columns)\ndf['pixels'] = df['pixels'].astype('int')\ndf = df.sort_values('pixels')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\n\nfor i in range(9):\n    img = cv2.imread(df['Images'][i])\n    ax[i // 3, i % 3].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    print(df['Images'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\n\nfor i in range(9):\n    img = cv2.imread(df['Images'][i])\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray,200,100)\n    ax[i // 3, i % 3].imshow(edges)\n    print(df['Images'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\nj = 0\nfor i in range((len(df['Images'])-1),(len(df['Images'])-10),-1):\n    img = cv2.imread(df['Images'][i])\n    ax[j // 3, j % 3].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    print(df['Images'][i])\n    j += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12, 12))\nj = 0\nfor i in range((len(df['Images'])-1),(len(df['Images'])-10),-1):\n    img = cv2.imread(df['Images'][i])\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    edges = cv2.Canny(gray,200,100)\n    ax[j // 3, j % 3].imshow(edges)\n    print(df['Images'][i])\n    j += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pixel_matrix(path):\n    image = plt.imread(path)\n    dims = np.shape(image)\n    return np.reshape(image, (dims[0] * dims[1], dims[2]))# changing shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def variance_of_laplacian(image):\n    # compute the Laplacian of the image and then return the focus\n    # measure, which is simply the variance of the Laplacian\n    return cv2.Laplacian(image, cv2.CV_64F).var()\n\ncount = 0\nfor imagePath in df['Images']:\n    # load the image, convert it to grayscale, and compute the\n    # focus measure of the image using the Variance of Laplacian\n    # method\n    image = cv2.imread(imagePath)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    fm = variance_of_laplacian(gray)\n\n    # if the focus measure is less than the supplied threshold,\n    # then the image should be considered \"blurry\"\n    \n    if fm < 110.0:\n        count += 1\n        \nprint(\"Total blur image is \",count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/indian-dance-classification/Indian_Dance'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Cleaning Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize images - divide by 255\ntrain_datagen = ImageDataGenerator(rescale=1. / 255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Resize images, if needed\ntrain = train_datagen.flow_from_directory(path, \n                                          target_size=(256, 256), \n                                          batch_size=340, \n                                          class_mode=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iterating on each image and covert the RGB to Lab.\nX =[]\nY =[]\nfor img in train[0]:\n    try:\n        lab = rgb2lab(img)\n        X.append(lab[:,:,0]) \n        Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128, \n      #so we divide the values by 128 to restrict values to between -1 and 1.\n    except:\n        print('error')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(X)\nY = np.array(Y)\nX = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\nprint(X.shape)\nprint(Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Design a Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoder\nmodel = Sequential()\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(256, 256, 1)))\nmodel.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same', strides=2))\nmodel.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(256, (3,3), activation='relu', padding='same'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decoder\n#Decoder\n#Note: For the last layer we use tanh instead of Relu. \n#This is because we are colorizing the image in this layer using 2 filters, A and B.\n#A and B values range between -1 and 1 so tanh (or hyperbolic tangent) is used\n#as it also has the range between -1 and 1. \n#Other functions go from 0 to 1.\nmodel.add(Conv2D(128, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(64, (3,3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(32, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(16, (3,3), activation='relu', padding='same'))\nmodel.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\nmodel.add(UpSampling2D((2, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Compile and Train Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])\nmodel.summary()\nhistory=model.fit(X,Y,validation_split=0.1, epochs=150, batch_size=16)\nmodel.save('other_files/colorize_autoencoder.model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That snippet above took a long time. More than 10 minutes(just to arrive in only 7/150).More than one hour to make 10 per cent of the epochs. When you think it's over. It's far from over. Next time I'll read how many epochs before and reduce it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Ploting Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title('Accuracy Model')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title('Loss Model')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Testing with black and white Image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.models.load_model(\n    'other_files/colorize_autoencoder.model',\n    custom_objects=None,\n    compile=True)\nimg1_color=[]\nimg1=img_to_array(load_img('../input/indian-dance-classification/Indian_Dance/test/114.jpg'))\nimg1 = resize(img1 ,(256,256))\nimg1_color.append(img1)\nimg1_color = np.array(img1_color, dtype=float)\nimg1_color = rgb2lab(1.0/255*img1_color)[:,:,:,0]\nimg1_color = img1_color.reshape(img1_color.shape+(1,))\noutput1 = model.predict(img1_color)\noutput1 = output1*128\nresult = np.zeros((256, 256, 3))\nresult[:,:,0] = img1_color[0][:,:,0]\nresult[:,:,1:] = output1[0]\nimshow(lab2rgb(result))\nimsave(\"result.png\", lab2rgb(result))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The VGG16 start at that part. As it takes a long time, I quit from here. \n\nDas War's Kaggle Notebook Runner: Marília Prata  @mpwolke","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}