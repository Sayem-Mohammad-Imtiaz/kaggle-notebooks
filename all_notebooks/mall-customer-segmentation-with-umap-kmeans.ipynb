{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Visualization and Segmentation of Mall Customer Data\n\n### In this Kernel, we visualize UMAP embeddings for the Mall Customer Dataset, and then find and describe Clusters in the embedded datapoints.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport itertools as it\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom umap import UMAP\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport plotly.express as px","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mall = pd.read_csv('/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv')\nmall.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since Gender is a binary categorical feature, we can transform it into a number for easier treatment, without losing information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mall['Gender Numeric'] = mall['Gender'].map({'Male': -1, 'Female': 1})\nmall","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sklearn Pipeline: MinMaxScale our features","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"numeric_features = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)', 'Gender Numeric']\n\nnumeric_transformer = Pipeline(steps=[('scaler', preprocessing.MinMaxScaler())])\n\npreprocessor = ColumnTransformer(\n    transformers=[('num', numeric_transformer, numeric_features)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"UMAP is an outstanding dimensionality reduction technique, which we will use to have a \"compact\" 2-D representation of our 4-D dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"umap = UMAP(n_components=2, random_state=2020)\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('uamp', umap)])\numap_out = pipe.fit_transform(mall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how many clusters we would need for our efforts. For that, we calculate the inertia (residuals) for the fit of several number of clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sse = []\nnclusters = list(range(1,11))\nfor k in nclusters:\n    kmeans = KMeans(n_clusters=k)\n    clusters = kmeans.fit_predict(umap_out)\n    sse.append(kmeans.inertia_)\n    \nsb.pointplot(nclusters, sse).set_title('Inertia');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to this result, setting more than 4 clusters does not improve the fit much more. But, as we will see later, setting 6 will be a better choice.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters = 6, random_state = 2020)\nclusters = kmeans.fit_predict(umap_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add clusters to a *results* dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_umap = pd.DataFrame(data = umap_out, columns = ['Embedding 1', 'Embedding 2'])\ndf_clusters = pd.DataFrame(data = clusters, columns = ['Clusters']).apply(lambda x: 'C'+x.astype(str))\n\nresults = pd.concat([mall, df_umap, df_clusters], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's see what we did!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(results, x = 'Embedding 1', y='Embedding 2',\n                    color= 'Clusters',\n                    hover_data = ['Age', 'Gender', 'Annual Income (k$)', 'Spending Score (1-100)'],\n                    width=600, height=600)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see 3 clusters, one for each gender, featuring traits such as:\n- [C4 & C5] People in their 20s with high Spending Score.\n- [C1 & C3] High income & high Spending Score, Ages 30-40.\n- [C0 & C2] Mostly older people (>40) with medium and low Spending Scores. Some outliers show young people (<25) with very low Spending Score.\n\nIf we set 4 clusters, we would end up fusing pairs of these clusters, and much of these human interpretations!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A final thought: I believe that adding Gender as a feature to our embeddings didn't add much, since there are clear parallels between the proposed clusters. It might have been better to just set 3 clusters, and dismiss that feature. Of course, in the real world this might be a bad idea as well! (take for example, I am targeting potential customers of gender-defined apparel?)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}