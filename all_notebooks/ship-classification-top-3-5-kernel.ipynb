{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/uAAVYel.png)"},{"metadata":{},"cell_type":"markdown","source":"**INTRODUCTION**"},{"metadata":{},"cell_type":"markdown","source":"Hello Everyone !\n\nThis kernel consists of my work for the **Game of Deep Learning** - Compter Vision Hackathon on Analytics Vidhya in which we were supposed to classify different images of ships into 5 classes - \n\n1. Cargo\n2. Military\n3. Carrier\n4. Cruise\n5. Tanker\n\n\nThe kernel got a highest Public Leaderboard score of **0.9813**.\n\nAnd the highest Cross-Validation Score attained was **0.985**.\n\nThe private Leaderboard score attained by it is **0.98007**, which implies a rank of 15th among the 450 odd submissions.\n\nThe kernel explains the different steps and decisions I took during the training of the model and the reason behind them too."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#from albumentations import *\nimport cv2\n\nimport os\nprint(os.listdir(\"../input\"))\n\n#!pip install pretrainedmodels\nfrom tqdm import tqdm_notebook as tqdm\nfrom torchvision.models import *\n#import pretrainedmodels\n\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\nfrom fastai.callbacks import * \n\n#from utils import *\nimport sys\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA PREPROCESSING**\n\n**Preparing the Dataset for training.**"},{"metadata":{},"cell_type":"markdown","source":"Lets have a look at the distribution of classes in the dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{'Cargo': 1, \n'Military': 2, \n'Carrier': 3, \n'Cruise': 4, \n'Tankers': 5}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wedge = [train['category'].value_counts()[1],train['category'].value_counts()[2],\n         train['category'].value_counts()[3],train['category'].value_counts()[4],\n         train['category'].value_counts()[5]]\n\nperc = [train['category'].value_counts()[1]/len(train),\n        train['category'].value_counts()[2]/len(train),\n        train['category'].value_counts()[3]/len(train),\n        train['category'].value_counts()[4]/len(train),\n        train['category'].value_counts()[5]/len(train),\n       ]\nplt.pie(wedge,labels=['Cargo - '+ format(perc[0]*100, '.2f') + '%','Military - '+ format(perc[1]*100, '.2f') + '%','Carrier - '+ format(perc[2]*100, '.2f') + '%','Cruise - '+ format(perc[3]*100, '.2f') + '%','Tankers - '+ format(perc[4]*100, '.2f') + '%'],\n        shadow=True,radius = 2.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Seeing the slightly skewed distribution of classes in the training set, I decided to first **Balance** the number of examples belonging to each of the classes so that the model is not biased towards predicting any particular class in specific."},{"metadata":{},"cell_type":"markdown","source":"\n* Now, it is a classical oversampling mistake which many people commit, and that is, to oversample the data first and then split the new Dataset into train and validation set.\n\n\n* This essentially results in the validation set not being completely \"Unseen Data\" or true \"Hold-out Set\" for that matter, because the model has already seen a slightly different form of the images in your validation set. Hence, the scores on validation set come out to be highly optimistic whereas on the Test set, such models tend to perform poorly\n\n\n* For further reading on the right way to oversample your data, refer to this link - \n\n  https://beckernick.github.io/oversampling-modeling/"},{"metadata":{},"cell_type":"markdown","source":"\n* So moving forward, I first extracted 175 random images from each class (175 times 5 = 875 images) and separated it out as my Validation set. \n\n\n* With the remaining images of each class, the classes were all oversampled too have around 2000 examples from each class, leading to a inflated training set of 10000 images and 875 validation set images for my model to train and evaluate on."},{"metadata":{},"cell_type":"markdown","source":"\n* Also, since there was a possibility of the model trained on my new Augmented Train Dataset to overfit on the training examples, I also maintained one train Dataset as it was provided on the portal, as it is.\n\n\n* The thought behind this being, if I train different models on different datasets and predict on the same test set, I can expect to get some really good results while ensembling the various models as they would be pretty different from each other owing to the randomness of the splits."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is the code for Over-Sampling the images in order to make a new dataset.\n\n# I used OpenCV2.0 for the same.\n\n#  TRANSFORMATION -1 \n\n#     scr = ShiftScaleRotate(p=1,rotate_limit=15)\n#     hor = HorizontalFlip(p=1)\n#     rbc = RandomBrightnessContrast(p=1)\n#     image1 = scr(image = img)['image']\n#     image1 = hor(image=image1)['image']\n#     image1 = rbc(image=image1)['image']\n\n#  TRANSFORMATION -2 \n\n#     hor = HorizontalFlip(p=1)\n#     rbc = RandomBrightnessContrast(p=1)\n#     cut = Cutout(num_holes = 12,max_h_size=12,max_w_size=12,p = 1)\n#     image2 = hor(image = img)['image']\n#     image2 = rbc(image = image2)['image']\n#     image2 = cut(image = image2)['image']\n    \n#  TRANSFORMATION -3 \n    \n#     rr = MotionBlur(p=1)\n#     cs = ChannelShuffle(p=1)\n#     hor = HorizontalFlip(p=1)\n#     image3 = rr(image = img)['image']\n#     image3 = cs(image = image3)['image']\n#     image3 = hor(image = image3)['image']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I used the library named \"[**Albumentations**](https://github.com/albu/albumentations)\" for the image transformation shown above. \n\n\n( Since I was unable to install the library on Kaggle, the image transformations were done locally. )\n\nHave a look at the below images to get an idea about the transformed images."},{"metadata":{},"cell_type":"markdown","source":"**ORIGINAL IMAGE**"},{"metadata":{},"cell_type":"markdown","source":"![Original Image](https://i.imgur.com/08A6bJx.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**1ST TRANSFORMATION**"},{"metadata":{},"cell_type":"markdown","source":"![Transformation 1](https://i.imgur.com/kzfHhVl.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**2ND TRANSFORMATION**"},{"metadata":{},"cell_type":"markdown","source":"![Transformation 2](https://i.imgur.com/UOvgajn.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**3RD TRANSFORMATION**"},{"metadata":{},"cell_type":"markdown","source":"![Transformation 3](https://i.imgur.com/FQ9Zuej.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**ARCHITECTURES USED**"},{"metadata":{},"cell_type":"markdown","source":"Now coming to the different model architectures used, I used the following architectures - \n* Resnet34\n* Resnet52\n* Resnet101\n* Resnet152\n* Densenet161\n* Densenet201\n* SENet154\n* ResNext101_64x4d\n\nAmong the models, Resnet 152 gave the best performance individually, with the validation F-Score reaching 0.9807 after some fine-tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"path = pathlib.Path('../input/train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TRAINING THE MODEL**"},{"metadata":{},"cell_type":"markdown","source":"**DATA AUGMENTATION**"},{"metadata":{},"cell_type":"markdown","source":"Real-Time Data Augmentations like \n* Flipping\n* Rotation\n* Lighting Changes\n* Warps\n\nwere all carried out to make the model learn better"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfms = get_transforms(do_flip=True,max_rotate=20.0,p_affine=0.75,\n                      max_lighting=0.5, max_warp=0.3, p_lighting=0.75)\n\n#np.random.seed(20)\nnp.random.seed(31)\ndata = ImageDataBunch.from_csv(path, folder='images', csv_labels='train.csv',\n                               valid_pct=0.15, test='test', ds_tfms=tfms,\n                               size=256,bs = 32)\n\ndata.show_batch(rows=3, figsize=(5,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fbetaW = FBeta(beta=1, average=\"weighted\")\n\nlearn = create_cnn(data, models.resnet152, metrics=[accuracy,fbetaW],model_dir=\"/tmp/model/\")\n\nlearn.fit_one_cycle(5, callbacks=[SaveModelCallback(learn,every='improvement',monitor='f_beta',mode='max',name='resnet-152')]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* First we trained the model with all the **ImageNet** pretrained layers of ResNet having **fixed weights**, i.e, we only **tuned the last layers** in the above training part.\n\n\n* Now, we can proceed to \"Unfreezing\" the inner layers of the architecture and fine-tuning them for our specific cause - Classifying Ships from each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = learn.load('resnet-152')\nlearn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)\nmin_grad_lr = learn.recorder.min_grad_lr\n\nlearn.fit_one_cycle(5,max_lr=slice(1e-6,min_grad_lr*10),callbacks=[SaveModelCallback(learn,monitor='f_beta',every='improvement',mode='max',name='resnet-152')])\nlearn = learn.load('resnet-152')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**THAT EXTRA PUSH TO THE SCORE...**"},{"metadata":{},"cell_type":"markdown","source":"These are the basic model training steps.\n\nThese should already give us pretty good results as we can see from our F-1 scores on our validation set.\n\nHere comes a few more additional steps which helped me push the score further and squeeze out those last decimal points - \n\n1. FastAI by default uses **Adam Optimizer** in order to train the models. \n   Also, Adam Optimizer usually **converges faster than SGD**.\n   \n   So, I used Adam First till training stagnates and then I switched out the optimizer with SGD because SGD    being the slower one, converges better, squeezing out a bit more from the model.\n   \n   Thus effectively, I used Adam to get the training parameters near the optimal values and then used SGD to get to the optimal values.\n   \n   \n2.  **Discriminative Learning rates** were used so that the inner layers of the pretrained model do not get changed much, and the outer layers get updated at a greater rate than that.\n\n    *( For a brief idea about Discriminative Learning Rate, please refer to Edit 1 at the end of the kernel )*\n    \n\n3. **Cyclical Learning rate scheduler** was used, following the **1-cycle policy** so that the we do not get stuck at an instable minima and get a stabler minima which performs over a wider range of loss functions and not just the train set specifically.\n\n   More about the 1-cycle learning policy - https://arxiv.org/pdf/1803.09820.pdf\n   \n   \n4. Finally, **ensembling** the different models trained gave a great push to the score of about 0.0147 to the F-1 Score.\n\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is the part to switch out the optimizer from Adam to SGD\n\nlearn = create_cnn(data, models.resnet152, metrics=[accuracy,fbetaW],model_dir=\"/tmp/model/\",opt_func=optim.SGD)\nlearn = learn.load('resnet-152')\n\nlearn.fit_one_cycle(5, callbacks=[SaveModelCallback(learn,every='improvement',monitor='f_beta',mode='max',name='resnet-152')])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tuning the model a bit more to squeeze out the last drop of scores..."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = learn.load('resnet-152')\n\nlearn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)\nmin_grad_lr = learn.recorder.min_grad_lr\n\nlearn.fit_one_cycle(5,max_lr=slice(1e-6,min_grad_lr*10),callbacks=[SaveModelCallback(learn,monitor='f_beta',every='improvement',mode='max',name='resnet-152')])\nlearn = learn.load('resnet-152')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets have a look at how our model is performing now..."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(9,figsize=(15,11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FINAL PREDICTION FOR SUBMISSION**"},{"metadata":{},"cell_type":"markdown","source":"Since there is some problem with making a CSV to submit using FastAI on Kaggle, I have directly uploaded the CSV file at the end.\n\nNonetheless, the code for making predictions is given below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds, y = learn.get_preds(ds_type=DatasetType.Test)\n# y = torch.argmax(preds, dim=1)\n\n# testFold = pd.DataFrame({'image':[],'category':[]})\n# yArr = y.numpy()\n# iterator = 0\n# for imgName in os.listdir('images/test'):\n#     testFold.loc[iterator,'image'] = imgName\n#     testFold.loc[iterator,'category'] = int(yArr[iterator]+1)\n#     iterator = iterator + 1\n    \n# test = pd.read_csv('../input/test_ApKoW4T.csv')\n\n# test['category'] = 0\n\n# for row in tqdm(test['image'].unique()):\n#     test.loc[test['image']==row,'category'] = int(testFold.loc[testFold['image']==row,'category'].values[0])\n\n# test['category'] = test['category'].astype('int')\n\n# test.to_csv('submission-resnet152.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the same thing was done for all the different architectures mentioned above and on both the train sets, followed by Ensembling of the models.\n\nThe ensembling was done using **Weighted Averaging** of the probabilities predicted by the different models.\n\nFor a detailed analysis on different techniques used for ensembling, here is a great learning resource - \n\nhttps://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/"},{"metadata":{},"cell_type":"markdown","source":"Bayesian Optimization was used to find the optimal weights to combine 14 different models trained on either one of the Two Datasets as mentioned above."},{"metadata":{},"cell_type":"markdown","source":"The final ensemble attained a cross-validation F-1 score of 0.985\n\nThe final CSV file generated is - \n\nhttps://drive.google.com/open?id=1ztaz0vK_8lGZUjKR2zwJWe3WqnyoGaLd"},{"metadata":{},"cell_type":"markdown","source":"**Now that we have reached the end of the kernel, I am assuming you liked the kernel, since you didnt close it mid-way.**\n\n**If you did like it, please UPVOTE the kernel. That keeps me going !**\n\n**Any suggestions and criticism are welcome.**\n\n**Cheers !**"},{"metadata":{},"cell_type":"markdown","source":"\n\n**Edit 1** - \n**Discriminative Learning Rate - **\n\nThis is a really important concept, especially when training models using **pretrained models**.\n\n* As we know, the deeper we go into the layers of a model, i.e, the initial layers learn small nuance features like **edges, corners** etc. and the more we go towards the final layers, the more train set specific features are learnt, like faces, big shapes etc.\n\n  Now, nuance features like corners, edges etc are expected to be **present in every object**, irrespective of what the object is.\n\n* So, the initial layers are still **highly relevant** irrespective of the particular type of objects we are looking at and the farther we go from initial layers, we need the model to learn more **object specific features** which fits our train set better.\n\n* Therefore, logically, in case of pre-trained models, we **need not change the weights by much in the initial layers** as much as we need to for the later layers.Thus, essentially what we are looking for here is to have **different learning rates for different layers**, instead of having it the conventional way - same unifrom learning rate throughout the layers.\n\n* Thus, we are able to **preserve the near optimal weights** in the initial layers and change them by very minute amounts and change the later layers more to suit our training set in a much better way.\n\nThis technique of having different Learning rates for different layers is known as **Discriminative Learning Rates******.\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}