{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"0bf669ae-359e-9db0-fa2a-4bf21699e39f"},"source":"I manually manipulated the Titanic competition dataset to be better suited for binary logistic regression."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94b736ec-22fe-4fdd-609e-fa52ee6180fc"},"outputs":[],"source":"# Importing the required libraries (plus no heavy use of scikit-learn):\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.utils import shuffle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ffa6327-3ca6-77e0-4bea-46abcd9b6457"},"outputs":[],"source":"# Processing the data:\n\ndef get_data():\n\ttitanic = pd.read_csv(\"../input/train_and_test2.csv\")\n\tdata = titanic.as_matrix()\n\tX = data[:,1:-1]\n\tY = data[:,-1]\n\tX[:,1] = (X[:,1]-X[:,1].mean())/X[:,1].std()\n\tX[:,0] = (X[:,0]-X[:,0].mean())/X[:,0].std()\n\tN, D = X.shape\n\tX2 = np.zeros((N,D))\n\tX2[:,0:3] = X[:,0:3]\n    \n\t# One-Hot-Encoding:\n\tfor n in range(N):\n\t\tt = int(X[n,D-3]) #Embarked\n\t\tt2 = int(X[n,D-6]) #pclass\n\t\tt3 = int(X[n,D-15]) #parch\n\t\tt4 = int(X[n,D-23]) #sibsp\n\t\tX2[n,t+D-3] = 1\n\t\tX2[n,t2+D-6] = 1\n\t\tX2[n,t3+D-15] = 1\n\t\tX2[n,t4+D-23] = 1\n\n\t\treturn X2, Y\n\ndef get_binary_data():\n\tX, Y = get_data()\n\tX2 = X[Y <= 1]\n\tY2 = Y[Y <= 1]\n\treturn X2, Y2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f9759f6-f086-9ce1-4a69-1839408e1f89"},"outputs":[],"source":"X, Y = get_binary_data()\nX, Y = shuffle(X,Y)\n\nXtrain = X[0:891,:]\nYtrain = Y[0:891]\nXtest = X[-418:]\nYtest = Y[-418:] # Which I manually put to be zero in every row!\n\nD = X.shape[1]\nW = np.random.randn(D)\nb = 0"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"daa7e2d2-2887-85f6-0b32-17f2d2f83eff"},"outputs":[],"source":"# Making some necessary functions:\n\ndef sigmoid(z):\n\treturn 1/(1+np.exp(-z))\n\ndef forward(X,W,b):\n\treturn sigmoid(X.dot(W)+b)\n\ndef classification_rate(targets,predictions):\n\treturn np.mean(targets == predictions)\n\ndef cross_entropy(T,pY):\n\treturn -np.mean(T*np.log(pY) + (1-T)*np.log(1-pY))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90cb5dbb-c2ce-f479-893b-d4c41f5e733a"},"outputs":[],"source":"# Logistic regression via gradient descent plus L2 regularization:\n\ntrain_costs = []\ntest_costs = []\nlearning_rate = 0.001\n\nfor i in range(10000):\n\tpYtrain = forward(Xtrain,W,b)\n\tpYtest = forward(Xtest,W,b)\n\n\tctrain = cross_entropy(Ytrain,pYtrain)\n\tctest = cross_entropy(Ytest,pYtest)\n\ttrain_costs.append(ctrain)\n\ttest_costs.append(ctest)\n\n\tW -= learning_rate*(Xtrain.T.dot(pYtrain-Ytrain)-0.1*W)\n\tb -= learning_rate*(pYtrain-Ytrain).sum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"144e3986-04b1-209b-f7b6-363a7cc04fd8"},"outputs":[],"source":"# Displaying my model:\n\nlegend1, = plt.plot(train_costs,label='Train Cost')\nlegend2, = plt.plot(test_costs,label='Test Cost')\nplt.legend([legend1,legend2])\nplt.show()\n\nprint(\"Final Train Classification Rate: \",classification_rate(Ytrain,np.round(pYtrain)))\nprint(\"Final Test Classification Rate: \",classification_rate(Ytest,np.round(pYtest)))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}