{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom scipy import signal\nimport soundfile as sf\nimport keras\nfrom keras.models import Sequential\nfrom keras.datasets import mnist\nfrom keras.layers import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.layers import PReLU\nimport cv2\nimport librosa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputData = np.empty((6898,110250))\ntargetData = np.empty(6898)\n\nroot = '../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\nfilenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]\ni_list = []\nrec_annotations = []\nrec_annotations_dict = {}\ninputImageData = np.empty((6898,32,216))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Extract_Annotation_Data(file_name, root):\n    tokens = file_name.split('_')\n    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    return (recording_info, recording_annotations)\n\ndef slice_data(start, end, raw_data,  sample_rate):\n    max_ind = len(raw_data)\n    new_sample_rate = 22050\n    new_raw_data = librosa.resample(raw_data,sample_rate,new_sample_rate,res_type='kaiser_fast')\n    new_max_ind = len(new_raw_data)\n    start_ind = min(int(start * new_sample_rate), new_max_ind)\n    end_ind = min(int(end * new_sample_rate), new_max_ind)\n    max_len = 110250\n    if (end_ind-start_ind)>max_len:\n        #print('1')\n        return new_raw_data[start_ind:(start_ind+max_len)]\n    elif ((end_ind-start_ind)<max_len):\n        #print('2')\n        return np.concatenate((new_raw_data[start_ind:end_ind],np.zeros(max_len+start_ind-end_ind)))\n    elif (end_ind-start_ind)==max_len:\n        #print('3')\n        return new_raw_data[start_ind:end_ind]\n    \ndef getClass(df,index):\n    if(df.at[index,'Wheezes']==0 and df.at[index,'Crackles']==0):\n        return 0\n    elif(df.at[index,'Wheezes']==1 and df.at[index,'Crackles']==0):\n        return 1\n    elif(df.at[index,'Wheezes']==0 and df.at[index,'Crackles']==1):\n        return 2\n    elif(df.at[index,'Wheezes']==1 and df.at[index,'Crackles']==1):\n        return 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for s in filenames:\n    (i,a) = Extract_Annotation_Data(s, root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s] = a\nrecording_info = pd.concat(i_list, axis = 0)\ndel i_list\ndel rec_annotations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l=0\nfor i in rec_annotations_dict:\n    j = rec_annotations_dict[i]\n    for k in range(j.shape[0]):\n        data,sampleRate = sf.read(root+i+'.wav')\n        inputData[l] = slice_data(j.at[k,'Start'],j.at[k,'End'], data, sampleRate)\n        targetData[l] = getClass(j,k)\n        l=l+1\ndel rec_annotations_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sos = signal.butter(20, 5000, btype='lowpass', analog=False, output='sos', fs=220250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(6898):\n    inputImageData[i] = librosa.feature.mfcc(inputData[i],22050,n_mfcc=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import librosa.display\n_ = 21\nprint(targetData[_])\nfig, ax = plt.subplots()\nimg = librosa.display.specshow(inputImageData[_][:][:], x_axis='time', ax=ax)\nfig.colorbar(img, ax=ax)\nax.set(title='MFCC')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model configuration\nimg_width, img_height = 32, 216\nbatch_size = 256\nno_epochs = 20\nno_classes = 4\nverbosity=1\ninput_shape = (img_width, img_height,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targetData = keras.utils.to_categorical(targetData, no_classes)\nif K.image_data_format() == 'channels_first':\n    inputImageData = inputImageData.reshape(inputImageData.shape[0], 1, img_width, img_height)\n    input_shape = (1, img_width, img_height)\nelse:\n    inputImageData = inputImageData.reshape(inputImageData.shape[0], img_width, img_height, 1)\n    input_shape = (img_width, img_height, 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(inputImageData, targetData, test_size=0.2, shuffle= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the model\nmodel = Sequential()\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size=(5,5), strides=(1, 1), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Conv2D(64, (3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(1000, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(no_classes, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n# Fit data to model\nhistory = model.fit(x_train, y_train, batch_size=batch_size, epochs=no_epochs, verbose=verbosity, validation_data=(x_valid, y_valid))\n# Visualize model history\nplt.plot(history.history['accuracy'], label='Training accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation accuracy')\nplt.title('training / validation accuracies')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc=\"upper left\")\nplt.show()\nplt.plot(history.history['loss'], label='Training loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.title('training / validation loss values')\nplt.ylabel('Loss value')\nplt.xlabel('Epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}