{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Any results you write to the current directory are saved as output.\ndf1 = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv') # importing training data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head() #checking the head of the data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing Values If Any\ndf1.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df1[df1.label == 0]), 'Non-Hatred Tweets')\nprint(len(df1[df1.label == 1]), 'Hatred Tweets')\n# Class distribution in this data seems to be imbalanced.\n# F1 score should be used fot model performance evaluation in such situation. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing different libraries for analysis, processing and classification\nimport nltk\nfrom sklearn import re #regular expression for text processing\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer #word stemmer class\nlemma = WordNetLemmatizer()\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk import FreqDist \n# vectorizer \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression #classification model\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score # performance evaluation criteria","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalizer(tweet):\n    tweets = \" \".join(filter(lambda x: x[0]!= '@' , tweet.split()))\n    tweets = re.sub('[^a-zA-Z]', ' ', tweets)\n    tweets = tweets.lower()\n    tweets = tweets.split()\n    tweets = [word for word in tweets if not word in set(stopwords.words('english'))]\n    tweets = [lemma.lemmatize(word) for word in tweets]\n    tweets = \" \".join(tweets)\n    return tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['normalized_text'] = df1.tweet.apply(normalizer)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_hashtag(tweet):\n    tweets = \" \".join(filter(lambda x: x[0]== '#', tweet.split()))\n    tweets = re.sub('[^a-zA-Z]',' ',  tweets)\n    tweets = tweets.lower()\n    tweets = [lemma.lemmatize(word) for word in tweets]\n    tweets = \"\".join(tweets)\n    return tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['hashtag'] = df1.tweet.apply(extract_hashtag)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all tweets \nall_words = \" \".join(df1.normalized_text)\n#print(all_all_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hatred tweets\nhatred_words = \" \".join(df1[df1['label']==1].normalized_text)\n#print(hatred_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(height=2000, width=2000, stopwords=STOPWORDS, background_color='white')\nwordcloud = wordcloud.generate(all_words)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(height=2000, width=2000, stopwords=STOPWORDS, background_color='white')\nwordcloud = wordcloud.generate(hatred_words)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_all_hashtag = FreqDist(list((\" \".join(df1.hashtag)).split())).most_common(10)\nfreq_all_hashtag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_hatred_hashtag = FreqDist(list((\" \".join(df1[df1['label']==1]['hashtag'])).split())).most_common(10)\nfreq_hatred_hashtag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_allhashtag = pd.DataFrame(freq_all_hashtag, columns=['words', 'frequency'])\ndf_hatredhashtag = pd.DataFrame(freq_hatred_hashtag, columns=['words', 'frequency'])\nprint(df_allhashtag.head())\nprint(df_allhashtag.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='words', y='frequency', data=df_allhashtag)\nplt.xticks(rotation = 45)\nplt.title('hashtag words frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='words', y='frequency', data=df_hatredhashtag)\nplt.xticks(rotation = 45)\nplt.title('hatred hashtag frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to create sparse matrix corpus is created to pass to vectorizer\nlen(df1)\ncorpus = []\nfor i in range(0,31962):\n    corpus.append(df1['normalized_text'][i])\n#corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(stop_words=stopwords.words('english'))\ncv.fit(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating dense matrix\nX = cv.transform(corpus).toarray()\ny = df1.iloc[:,1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier1 = LogisticRegression(C=10)\nclassifier1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier1.predict(X_test)\ny_prob = classifier1.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1,3), min_df=10, stop_words=stopwords.words('english'))\nX1 = tfidf.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.33, random_state=42)\nclassifier2 = LogisticRegression(C=10)\nclassifier2.fit(X1_train, y1_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_pred = classifier2.predict(X1_test)\ny1_prob = classifier2.predict_proba(X1_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y1_test, y1_pred))\nprint(classification_report(y1_test, y1_pred))\nprint(confusion_matrix(y1_test, y1_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = np.arange(0.1,0.9,0.1)\nscore = [f1_score(y1_test, ((y1_prob[:,1] >= x).astype(int))) for x in threshold]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(threshold, score)\nplt.xlabel('Threshold Probability')\nplt.ylabel('F1 score')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}