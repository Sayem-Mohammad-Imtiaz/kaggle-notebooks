{"cells":[{"metadata":{"_uuid":"58988bdb9bd4f2fd6997efe361f93331ebff1e1f"},"cell_type":"markdown","source":"**垃圾邮件分类 - 手写实现 Naive Bayers**\n\nhttps://www.kaggle.com/uciml/sms-spam-collection-dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\n# 读取数据\ndata_dir = \"../input/\"\ndf = pd.read_csv(data_dir + '/spam.csv', encoding='latin-1')\n\n# 把数据拆分成为训练集和测试集\ndata_train, data_test, labels_train, labels_test = train_test_split(\n    df.v2,\n    df.v1, \n    test_size=0.2, \n    random_state=0)  \n\nprint ('拆分过后的每个邮件内容')\nprint (data_train[:10])\nprint ('拆分过后每个邮件是否是垃圾邮件')\nprint (labels_train[:10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d519f62479e07e80dfc42af9fe6ce994e5ee1ec2"},"cell_type":"markdown","source":"**Build Word Dict** i.e. { word_1 : word_1_index, word_2 : word_2_index .......}"},{"metadata":{"trusted":true,"_uuid":"d82bbec4d4d03d86789379222d7a3315dac39aa1"},"cell_type":"code","source":"'''\n    用一个dictionary保存词汇，并给每个词汇赋予唯一的id\n'''\ndef GetVocabulary(data): \n    vocab_dict = {}\n    wid = 0\n    for document in data: # document represent each line in the input spam.csv file\n        words = document.split() # split each document(i.e. each line) 按空格分词 “I am a student” => [\"I\", \"am\", \"a\", \"student\"]\n        for word in words:\n            word = word.lower() #归一化\n            if word not in vocab_dict:\n                vocab_dict[word] = wid\n                wid += 1\n    return vocab_dict\n\n# 用训练集建立词汇表\nvocab_dict = GetVocabulary(data_train)\nprint ('Number of all the unique words : ' + str(len(vocab_dict.keys())))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db5cac9c92530a1d10585ba0f803964a5e525e53"},"cell_type":"markdown","source":"**把文章里每一句话（也就是输入文件的每一行）变成词向量**"},{"metadata":{"trusted":true,"_uuid":"f1eb7d4c95dc5d5697ec659fb306021496daa1ea"},"cell_type":"code","source":"'''\n    把文本变成向量的表示形式，以便进行计算\n'''\n# 把上一步骤建立的好vocab_dict传入，\ndef Document2Vector(vocab_dict, data):\n    word_vector = np.zeros(len(vocab_dict.keys()))\n    words = data.split()\n    out_of_voc = 0 # track没有在词汇表里出现过的单词， 这样后面好做smoothing\n    for word in words:\n        word = word.lower()\n        if word in vocab_dict:\n            word_vector[vocab_dict[word]] += 1\n        else:\n            out_of_voc += 1\n    return word_vector, out_of_voc\n\n# 下面是一个例子，解释向量长什么样\nexample, oov = Document2Vector(vocab_dict,\"we are good good student\")\nprint(example)\nprint(example[vocab_dict['we']], example[vocab_dict['are']], example[vocab_dict['good']], example[vocab_dict['student']])\n# 每个单词是一个维度，如果单词没有出现过，对应那一维为0，否则为出现的次数.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.values # data_train.values 不包含第一列line number","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9b304a4606b385d0878fadede7146892cb64c9"},"cell_type":"code","source":"# 把训练集的句子全部变成向量形式\ntrain_matrix = []\nfor document in data_train.values: # document represent each line in the input file\n    word_vector, _ = Document2Vector(vocab_dict, document) # _ 表示第二个返回值 用不到\n    train_matrix.append(word_vector)\n\nprint (len(train_matrix))\ntrain_matrix[:8]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af8527ee4b4d557536c6e9ddcb57ed177f49136e"},"cell_type":"markdown","source":"**做naive bayes 训练，得到训练集每个词概率** i.e. p(word|spam), p(word|ham)"},{"metadata":{"trusted":true,"_uuid":"3b9e0b7fd586e3597e483942266ef5780fa042a3"},"cell_type":"code","source":"\n'''\n    在训练集计算两种概率：\n        1. 词在每个分类下的概率，比如P('email'|Spam)\n        2. 每个分类的概率，比如P(Spam)\n        \n    这里的计算实现巧妙利用了numpy的array结构：\n        1. 在每个分类下创建一个与词汇量大小相等的vector(即 numpy array), 即spam_word_counter 和 ham_word_counter\n        2. 在遍历每一个句子的时候，直接与句子对应的vector相加，累积每个单词出现的次数\n        3. 在遍历完所有句子之后，再除以总词汇量，得到每个单词的概率\n'''\ndef NaiveBayes_train(train_matrix,labels_train):\n    # train_matrix => (10，1000)\n    num_docs = len(train_matrix) # 多少行 就有多少个doc \n    num_words = len(train_matrix[0]) #对第一个样本取一下vector的长度. 每一行的长度就是word dict的长度，也就表示有多少个word 在dict里\n    \n    spam_word_counter = np.ones(num_words) # np.ones 其实默认做了 +1 smoothing\n    ham_word_counter = np.ones(num_words)  #计算频数初始化为1，即使用拉普拉斯平滑，详单于在最开始就给每个单词加了一个平滑因子\n\n    ham_total_count = 0;\n    spam_total_count = 0;\n    \n    spam_count = 0\n    ham_count = 0\n    # 一个文件一个文件处理，也就是输入文件的每一行\n    for i in range(num_docs):\n        if i % 500 == 0:\n            print ('Train on the doc id:' + str(i))\n            \n        if labels_train[i] == 'ham':\n            ham_word_counter += train_matrix[i] # 数组对应位置相加\n            ham_total_count += sum(train_matrix[i]) # 数字相加\n            ham_count += 1\n        else:\n            spam_word_counter += train_matrix[i]\n            spam_total_count += sum(train_matrix[i])\n            spam_count += 1\n    \n    #spam_word_counter => 每个词的计数， 是array\n    #spam_total_count => Spam的总词数,是一个数字\n    #spam_count => Spam邮件计数， 是一个数字\n    \n    # 注意，这里对所有的概率都取了log\n    p_spam_vector = np.log(spam_word_counter/(spam_total_count + num_words)) #注意在分母也加上平滑部分\n    p_ham_vector = np.log(ham_word_counter/(ham_total_count + num_words))  #注意在分母也加上平滑部分\n    \n    return p_spam_vector, np.log(spam_count/num_docs), spam_total_count, p_ham_vector, np.log(ham_count/num_docs), ham_total_count\n\n# p_spam_vector/p_ham_vector 的每一维分别是一个单词在spam/ham分类下的概率\n# p_spam / p_ham 分别是两个分类的概率\np_spam_vector, p_spam, spam_total_count, p_ham_vector, p_ham, ham_total_count = NaiveBayes_train(train_matrix, labels_train.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d228df864a45da8267ef3886a8cbf43f1916dbe"},"cell_type":"code","source":"'''\n    对测试集进行预测，按照公式计算例子在两个分类下的概率，选择概率较大者作为预测结果\n'''\ndef Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham, spam_smoothing, ham_smoothing):\n    \n    # 注意: 如果单词没出现过，则test_word_vector对应的维度为0\n    # 所以: test_word_vector * p_spam_vector 不为0的维度正好是句子中每个词的概率\n    # [2, 0, 1] * [0.3, 0.2, 0.4] = sum([0.6, 0, 0.4]) \n    # 一个单词可能出现多余一次，哟与test_word_vector 里存放的即使每个单词出现的次数，所以 sum(test_word_vector * p_spam_vector) 正好达到公式的要求 i.e. \n    # 原始公式 2log(P(A|Spam))+ log(P(B|Spam))  对照Week3.Session2.Naive_ Bayers_1_V3.0.pdf 中 ”朴素贝叶斯的工程技巧 – 概率处理技巧“ 那一页 好理解\n    spam = sum(test_word_vector * p_spam_vector) + p_spam + spam_smoothing\n    ham = sum(test_word_vector * p_ham_vector) + p_ham + ham_smoothing\n    if spam > ham:\n        return 'spam'\n    else:\n        return 'ham'\n\npredictions = []\nnum_words = len(vocab_dict.keys())\ni = 0\nfor document in data_test.values: # data_test.values 是array\n    if i % 200 == 0:\n        print ('Test on the doc id:' + str(i))\n    i += 1    \n    test_word_vector, out_of_voc = Document2Vector(vocab_dict, document)\n    spam_smoothing = 0\n    ham_smoothing = 0\n    if out_of_voc != 0:\n        spam_smoothing = out_of_voc * np.log(1/(spam_total_count + num_words))\n        ham_smoothing = out_of_voc * np.log(1/(ham_total_count + num_words))\n    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham, spam_smoothing, ham_smoothing)\n    predictions.append(ans)\n\nprint (len(predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"533f0c0a6918a2d627773cf1e96543e928fdd76c"},"cell_type":"code","source":"# 检测模型\n\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n\nprint (accuracy_score(labels_test, predictions))\nprint (classification_report(labels_test, predictions))\nprint (confusion_matrix(labels_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8156047a73c41841afee9c89866cfa03f5164646"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c865ff972e3055ab78033a603fbffc60f922ef91"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}