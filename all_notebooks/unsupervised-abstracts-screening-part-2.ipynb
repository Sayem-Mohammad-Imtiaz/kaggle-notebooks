{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom scipy.stats import loguniform, uniform, randint\n\nRANDOM_STATE = 1563","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\n**<span style=\"color:red\">The notebook is based upon our previous work. They should be evaluated together!</span>** [Link](https://www.kaggle.com/quittend/unsupervised-abstracts-screening-part-1)"},{"metadata":{},"cell_type":"markdown","source":"## Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/cleaning-cord-19-metadata/cord_metadata_cleaned.csv')\nprint(f'There are {len(df)} studies.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BioWordVec + GMM"},{"metadata":{},"cell_type":"markdown","source":"### Extract features\nI couldn't extract features using BioWordVec in the kernel due to the out of memory issues, thus I did it on my personal computer using the following snippet. I decided to \"max pool features over time\" as suggested in many papers like [Rethinking Complex Neural Network Architectures forDocument Classification](https://cs.uwaterloo.ca/~jimmylin/publications/Adhikari_etal_NAACL2019.pdf).\n\n```\nfrom gensim.models.keyedvectors import KeyedVectors\n\nmodel = KeyedVectors.load_word2vec_format(\n    fname='./BioWordVec_PubMed_MIMICIII_d200.vec.bin', \n    binary=True\n)\n\nnlp = spacy.load('en_core_sci_sm')\n\ndef vectorize(text: str):\n    features = np.array([\n        model[token.text] \n        for token in nlp(text)\n        if token.text in model.vocab\n    ])\n    \n    \n    return features.max(axis=0) if features.size != 0 else np.zeros(200)\n    \ndf['text_vector'] = df['text'].apply(vectorize)\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = np.load('/kaggle/input/biowordvec-precomputed-cord19/biowordvec.npy')\nprint(f'Embedding matrix has shape: {embeddings.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define model"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = GaussianMixture(\n    n_components=10,\n    covariance_type='full', \n    max_iter=100, \n    n_init=1, \n    init_params='kmeans', \n    random_state=RANDOM_STATE, \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Hyperparameter search\nA single set of hyperparameters for 4 splits on 4 cores with one KMeans initialization for GMM takes roughly 3.5 min including refiting model on the whole dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"N_ITER = 20\nN_SPLITS = 4\n\nparam_distributions = {\n    \"n_components\": randint(2, 256),\n    \"covariance_type\": ['diag', 'full', 'spherical'],\n}\n\ncv = KFold(\n    n_splits=N_SPLITS, \n    shuffle=True, \n    random_state=RANDOM_STATE\n)\n\nhp_search = RandomizedSearchCV(\n    estimator=estimator,\n    param_distributions=param_distributions,\n    n_iter=N_ITER,\n    n_jobs=N_SPLITS,\n    cv=cv,\n    verbose=1,\n    random_state=RANDOM_STATE,\n    return_train_score=True,\n    refit=True\n)\n\nhp_search.fit(embeddings)\nbest_model = hp_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Best validation likelihood: {hp_search.best_score_}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Best params: {hp_search.best_params_}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cluster'] = best_model.predict(embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How many elements does each cluster have?"},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_count = df['cluster'].value_counts().sort_values()\n\nax = cluster_count.plot(kind='bar', figsize=(15, 5))\nax.set_xticks([])\nax.set_xlabel(\"Cluster id\")\nax.set_ylabel(\"Count\")\nax.grid(True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clusters seem to be very well balanced!"},{"metadata":{},"cell_type":"markdown","source":"## Save results\nSuch results are not easily interpretable as TF-IDF, but there are much less clusters, meaning that such approach is worth evaluation."},{"metadata":{"trusted":true},"cell_type":"code","source":"(df\n    .drop(columns=['title_lang', 'abstract_lang', 'distance'])\n    .to_csv('/kaggle/working/cord_metadata_word2vec.csv', index=False)\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}