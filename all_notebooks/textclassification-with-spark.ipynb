{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n\nspark = SparkSession.builder \\\n        .master(\"local\") \\\n        .appName(\"Natural Language Processing\") \\\n        .config(\"spark.executor.memory\", \"2gb\") \\\n        .getOrCreate()\ndf = spark.read.format('com.databricks.spark.csv')\\\n        .options(header='true', inferschema='true')\\\n        .load('../input/fraud-email-dataset/fraud_email_.csv')\ndf = df.selectExpr(\"Class as label\", \"Text\")\ndf = df.withColumn(\"label\",df[\"label\"].cast(IntegerType()))\nprint (df.printSchema())\ndf = spark.createDataFrame(df.head(2000), df.schema)\ndf.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print((df.count(), len(df.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (df.filter(df.label.isNotNull()).count())\ndf = df.na.drop(subset=[\"label\"])\nprint (df.filter(df.label.isNotNull()).count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.show(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupBy(\"label\") \\\n    .count() \\\n    .orderBy(\"count\", ascending = False) \\\n    .show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pyspark.sql.functions as F\ndf = df.withColumn('word_count',F.size(F.split(F.col('Text'),' ')))\ndf.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.filter(df[\"label\"].isin(['0','1'])).collect()\ndf.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((df.count(), len(df.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom pyspark.sql.functions import col, lower, regexp_replace, split\n\ndef clean_text(reqText):\n    reqText = lower(reqText)\n    reqText = regexp_replace(reqText, \"=2e\", \"\")\n    reqText = regexp_replace(reqText, \"=2c\", \"\")\n    reqText = regexp_replace(reqText, \"\\=\", \"\")\n    reqText = regexp_replace(reqText, \"news.website.http\\:\\/.*\\/.*502503.stm.\", \"\")\n    reqText = regexp_replace(reqText, \"http://www.forcetacticalarmy.com\",\"\")\n    reqText = regexp_replace(reqText, \"\\'s\", \" \")\n    reqText = regexp_replace(reqText, \"\\'\", \" \")\n    reqText = regexp_replace(reqText, \":\", \" \")\n    reqText = regexp_replace(reqText, \"_\", \" \")\n    reqText = regexp_replace(reqText, \"-\", \" \")\n    reqText = regexp_replace(reqText, \"\\'ve\", \" have \")\n    reqText = regexp_replace(reqText, \"can't\", \"can not \")\n    reqText = regexp_replace(reqText, \"n't\", \" not \")\n    reqText = regexp_replace(reqText, \"i'm\", \"i am \")\n    reqText = regexp_replace(reqText, \"\\'re\", \" are \")\n    reqText = regexp_replace(reqText, \"\\'d\", \" would \")\n    reqText = regexp_replace(reqText, \"\\d\", \"\")\t\n    reqText = regexp_replace(reqText, \"\\b[a-zA-Z]\\b\",\"\")\n    reqText = regexp_replace(reqText, \"[\\,|\\.|\\&|\\;|<|>]\",\"\")\n    reqText = regexp_replace(reqText, \"\\S*@\\S*\", \" \")\n    return reqText\n\nclean_text_df = df.select(clean_text(col(\"Text\")).alias(\"Text\"),col('label'))\n\nclean_text_df.printSchema()\nclean_text_df.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text_df = clean_text_df.withColumn('words',F.split(F.col('Text'),' '))\nclean_text_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = ['i','me','my','myself','we','our','ours','ourselves',\n              'you','your','yours','yourself','yourselves','he','him',\n              'his','himself','she','her','hers','herself','it','its',\n              'itself','they','them','their','theirs','themselves',\n              'what','which','who','whom','this','that','these','those',\n              'am','is','are','was','were','be','been','being','have',\n              'has','had','having','do','does','did','doing','a','an',\n              'the','and','but','if','or','because','as','until','while',\n              'of','at','by','for','with','about','against','between',\n              'into','through','during','before','after','above','below',\n              'to','from','up','down','in','out','on','off','over','under',\n              'again','further','then','once','here','there','when','where',\n              'why','how','all','any','both','each','few','more','most',\n              'other','some','such','no','nor','not','only','own','same',\n              'so','than','too','very','can','will','just','don','should','now']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom pyspark.ml.feature import StopWordsRemover\nstopwordsRemovalFeature = StopWordsRemover(inputCol=\"words\",\n                                           outputCol=\"words without stop\")\\\n                                           .setStopWords(stop_words)\n\nfrom pyspark.ml import Pipeline\nstopWordRemovalPipeline = Pipeline(stages=[stopwordsRemovalFeature])\npipelineFitRemoveStopWords = stopWordRemovalPipeline.fit(clean_text_df)\n\nclean_text_df = pipelineFitRemoveStopWords.transform(clean_text_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text_df.select('words', 'words without stop','label').show(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport pyspark.ml.feature as feat\nTF_ = feat.HashingTF(inputCol=\"words without stop\", outputCol=\"rawFeatures\", numFeatures=500)\nIDF_ = feat.IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\npipelineTFIDF = Pipeline(stages=[TF_, IDF_])\n\npipelineFit = pipelineTFIDF.fit(clean_text_df)\nclean_text_df = pipelineFit.transform(clean_text_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text_df.show(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom pyspark.ml.classification import LogisticRegression\n\n(trainingDF, testDF) = clean_text_df.randomSplit([0.75, 0.25], seed = 7)\nlogreg = LogisticRegression(regParam=0.25)\n\nlogregModel = logreg.fit(trainingDF)\npredictionDF = logregModel.transform(testDF)\npredictionDF.select('label', 'probability', 'prediction').show(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn import metrics\nactual = predictionDF.select('label').toPandas()\npredicted = predictionDF.select('prediction').toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprint('accuracy score: {}%'.format(round(metrics.accuracy_score(actual, predicted),3)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}