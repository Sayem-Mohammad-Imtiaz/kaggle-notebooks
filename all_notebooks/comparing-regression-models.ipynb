{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"19b48ac8-47fc-22db-6b87-df86c6d0294f"},"source":"**Hello everyone.This is a notebook comparing various regression models such as Ridge,Knn,Bayesian Regression,Decision Tree and SVM.**\n*It is extremely beneficial for beginners to take a close look at the notebook so as to get an insight as to how different algorithms work and also which algorithms can perform better in some cases depending upon cases*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0021976-41b2-1fd5-70ff-b9d741126f72"},"outputs":[],"source":"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d3caf393-4760-300e-d133-37500d765b0d"},"outputs":[],"source":"# Importing packages\n\nimport os\nimport pandas as pd\nfrom pandas import DataFrame,Series\nfrom sklearn import tree\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn import neighbors\nfrom sklearn import linear_model\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2635ea7a-83b6-1afc-064b-c0ad0f5f39fe"},"outputs":[],"source":"f = pd.read_csv(\"../input/movie_metadata.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"607c0c70-a448-de8e-d3be-5e3ff2f4bdf8"},"outputs":[],"source":"data=DataFrame(f)\ndata.head()[:2]"},{"cell_type":"markdown","metadata":{"_cell_guid":"255de915-854b-ea70-888a-9c830b3a96bd"},"source":"*Getting non-object elements*\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dadbc531-af3a-0d76-940f-f3b8646598a9"},"outputs":[],"source":"X_data=data.dtypes[data.dtypes!='object'].index\nX_train=data[X_data]\nX_train.head()[:2]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"545c2c75-c02d-f12a-6475-f467098c51cc"},"outputs":[],"source":"X_train.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"653fe9e5-21b4-f858-ac33-ef40bfda40b6"},"outputs":[],"source":"# Finding all the columns with NULL values\n\nnp.sum(X_train.isnull())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0932b6d5-74e9-cb27-5ea8-5cfe4d9098a4"},"outputs":[],"source":"# Filling all Null values\nX_train=X_train.fillna(0)\ncolumns=X_train.columns.tolist()\ny=X_train['imdb_score']\nX_train.drop(['imdb_score'],axis=1,inplace=True)\nX_train.head()[:2]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc70dbae-a2cb-8765-e253-683301a12cf4"},"outputs":[],"source":"# GETTING Correllation matrix\ncorr_mat=X_train.corr(method='pearson')\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28e1db03-ef4b-f4c9-7168-5385e8fd76f6"},"outputs":[],"source":"X_Train=X_train.values\nX_Train=np.asarray(X_Train)\n\n# Finding normalised array of X_Train\nX_std=StandardScaler().fit_transform(X_Train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94104116-0c07-17a7-70b6-ed60d4ccddc8"},"outputs":[],"source":"from sklearn.decomposition import PCA\npca = PCA().fit(X_std)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,7,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d60dea0-362f-1d93-4906-442644707d82"},"source":"**Since 5 components can explain more than 70% of the variance, we choose the number of the components to be 5**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a0ec24a-f1fa-48ad-24e5-6d2101ee0481"},"outputs":[],"source":"from sklearn.decomposition import PCA\nsklearn_pca=PCA(n_components=5)\nX_Train=sklearn_pca.fit_transform(X_std)\n\nsns.set(style='darkgrid')\nf, ax = plt.subplots(figsize=(8, 8))\n# ax.set_aspect('equal')\nax = sns.kdeplot(X_Train[:,0], X_Train[:,1], cmap=\"Greens\",\n          shade=True, shade_lowest=False)\nax = sns.kdeplot(X_Train[:,1], X_Train[:,2], cmap=\"Reds\",\n          shade=True, shade_lowest=False)\nax = sns.kdeplot(X_Train[:,2], X_Train[:,3], cmap=\"Blues\",\n          shade=True, shade_lowest=False)\nred = sns.color_palette(\"Reds\")[-2]\nblue = sns.color_palette(\"Blues\")[-2]\ngreen = sns.color_palette(\"Greens\")[-2]\nax.text(0.5, 0.5, \"2nd and 3rd Projection\", size=12, color=blue)\nax.text(-4, 0.0, \"1st and 3rd Projection\", size=12, color=red)\nax.text(2, 0, \"1st and 2nd Projection\", size=12, color=green)\nplt.xlim(-6,5)\nplt.ylim(-2,2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac488dbb-31be-bb36-30c1-cfa7177089bf"},"outputs":[],"source":"number_of_samples = len(y)\nnp.random.seed(0)\nrandom_indices = np.random.permutation(number_of_samples)\nnum_training_samples = int(number_of_samples*0.75)\nx_train = X_Train[random_indices[:num_training_samples]]\ny_train=y[random_indices[:num_training_samples]]\nx_test=X_Train[random_indices[num_training_samples:]]\ny_test=y[random_indices[num_training_samples:]]\ny_Train=list(y_train)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"47c5593c-81d3-bf04-40b3-ec29db3c6e32"},"source":"**Ridge Regression**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d9a67ab-5ee2-4d6a-5842-4c22c18ccbe0"},"outputs":[],"source":"model=linear_model.Ridge()\nmodel.fit(x_train,y_train)\ny_predict=model.predict(x_train)\n\nerror=0\nfor i in range(len(y_Train)):\n    error+=(abs(y_Train[i]-y_predict[i])/y_Train[i])\ntrain_error_ridge=error/len(y_Train)*100\nprint(\"Train error = \"'{}'.format(train_error_ridge)+\" percent in Ridge Regression\")\n\nY_test=model.predict(x_test)\ny_Predict=list(y_test)\n\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y_Predict[i]-Y_test[i])/y_Predict[i])\ntest_error_ridge=error/len(Y_test)*100\nprint(\"Test error = \"'{}'.format(test_error_ridge)+\" percent in Ridge Regression\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"939f14b0-4bb9-d832-a035-3b2d4d2f05de"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in Ridge Regression\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"fc466e90-9ec9-9883-1bb8-94067c18129f"},"source":"**Knn Algorithm**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5f7b91b-f2c0-b8cd-1f49-963c40806406"},"outputs":[],"source":"n_neighbors=5\nknn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')\nknn.fit(x_train,y_train)\ny1_knn=knn.predict(x_train)\ny1_knn=list(y1_knn)\n\nerror=0\nfor i in range(len(y_train)):\n    error+=(abs(y1_knn[i]-y_Train[i])/y_Train[i])\ntrain_error_knn=error/len(y_Train)*100\nprint(\"Train error = \"+'{}'.format(train_error_knn)+\" percent\"+\" in Knn algorithm\")\n\ny2_knn=knn.predict(x_test)\ny2_knn=list(y2_knn)\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y2_knn[i]-Y_test[i])/Y_test[i])\ntest_error_knn=error/len(Y_test)*100\nprint(\"Test error = \"'{}'.format(test_error_knn)+\" percent\"+\" in knn algorithm\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be40e3e0-d06d-cd52-551f-dd92f73e2781"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\npreds = pd.DataFrame({\"preds\":knn.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in Knn\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"64be3c22-d0ef-c8b6-a284-1efabbfe4e41"},"source":"**Bayesian Regression**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54a78918-e912-64f4-8022-9a4debf9ea9f"},"outputs":[],"source":"reg = linear_model.BayesianRidge()\nreg.fit(x_train,y_train)\ny1_reg=reg.predict(x_train)\ny1_reg=list(y1_reg)\ny2_reg=reg.predict(x_test)\ny2_reg=list(y2_reg)\n\nerror=0\nfor i in range(len(y_train)):\n    error+=(abs(y1_reg[i]-y_Train[i])/y_Train[i])\ntrain_error_bay=error/len(y_Train)*100\nprint(\"Train error = \"+'{}'.format(train_error_bay)+\" percent\"+\" in Bayesian Regression\")\n\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y2_reg[i]-Y_test[i])/Y_test[i])\ntest_error_bay=(error/len(Y_test))*100\nprint(\"Test error = \"+'{}'.format(test_error_bay)+\" percent\"+\" in Bayesian Regression\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"774a2c64-d64b-8c9b-6ee3-de189d6cbd5b"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\npreds = pd.DataFrame({\"preds\":reg.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in Bayesian Regression\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"2706cfd0-40d5-d1aa-4fa9-ece504d46618"},"source":"**Decision Tree Regressor**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69efc5d8-5619-0826-6dd5-d8227b34162e"},"outputs":[],"source":"dec = tree.DecisionTreeRegressor(max_depth=1)\ndec.fit(x_train,y_train)\ny1_dec=dec.predict(x_train)\ny1_dec=list(y1_dec)\ny2_dec=dec.predict(x_test)\ny2_dec=list(y2_dec)\n\nerror=0\nfor i in range(len(y_train)):\n    error+=(abs(y1_dec[i]-y_Train[i])/y_Train[i])\ntrain_error_tree=error/len(y_Train)*100\nprint(\"Train error = \"+'{}'.format(train_error_tree)+\" percent\"+\" in Decision Tree Regressor\")\n\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y1_dec[i]-Y_test[i])/Y_test[i])\ntest_error_tree=error/len(Y_test)*100\nprint(\"Test error = \"'{}'.format(test_error_tree)+\" percent in Decision Tree Regressor\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a22ba5c0-f1be-538f-e147-dd5bd252431b"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\npreds = pd.DataFrame({\"preds\":dec.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in Decision Tree\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e7c15d0-7608-3604-e191-be532253ad70"},"source":"**SVM**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa77abcc-c96a-70f7-40d0-b6c489ad66d4"},"outputs":[],"source":"svm_reg=svm.SVR()\nsvm_reg.fit(x_train,y_train)\ny1_svm=svm_reg.predict(x_train)\ny1_svm=list(y1_svm)\ny2_svm=svm_reg.predict(x_test)\ny2_svm=list(y2_svm)\n\nerror=0\nfor i in range(len(y_train)):\n    error+=(abs(y1_svm[i]-y_Train[i])/y_Train[i])\ntrain_error_svm=error/len(y_Train)*100\nprint(\"Train error = \"+'{}'.format(train_error_svm)+\" percent\"+\" in SVM Regressor\")\n\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y2_svm[i]-Y_test[i])/Y_test[i])\ntest_error_svm=error/len(Y_test)*100\nprint(\"Test error = \"'{}'.format(test_error_svm)+\" percent in SVM Regressor\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43650d8c-fbbb-26d6-e716-5196d13033a0"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\npreds = pd.DataFrame({\"preds\":knn.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in SVM\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dee3b0e3-c04e-510a-9e31-1ed88d97c4e6"},"outputs":[],"source":"train_error=[train_error_ridge,train_error_knn,train_error_bay,train_error_tree,train_error_svm]\ntest_error=[test_error_ridge,test_error_knn,test_error_bay,test_error_tree,test_error_svm]\n\ncol={'Train Error':train_error,'Test Error':test_error}\nmodels=['Ridge Regression','Knn','Bayesian Regression','Decision Tree','SVM']\ndf=DataFrame(data=col,index=models)\ndf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"756ba5aa-c431-2a5c-fa80-9f7458443011"},"outputs":[],"source":"df.plot(kind='bar')"},{"cell_type":"markdown","metadata":{"_cell_guid":"788fae01-ffcb-a4c0-6434-e44cfbbe9d55"},"source":"**Seems that KNN turned out to be the winner.Its because of the fact that there are very large number of data points and and also  features are highly continuous**\n*Moreover the dimentionality of the processed data is not too high*"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}