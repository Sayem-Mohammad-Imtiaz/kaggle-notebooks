{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction \nThis is a digits dataset, the  so-called \"Hello World\" of Machine Learning. Most of the code here is from Aurelien Geron's book *Hands-on Machine Learning*, but I've added some for my own learning. The dataset is different from the one in his book, just the sklearn load_digits() function with far few samples and features than the one Geron uses in the book.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_digits\ndigits = load_digits()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Digits is an sklearn bunch object that contains both the data and metadata. The **data** and **target** attributes are both numpy arrays. The Bunch object extends the conventional python dictionary, so it's got keys and values.","metadata":{}},{"cell_type":"code","source":"print(digits.DESCR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"digits.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data has 1797 samples with 64 features that represent pixel intensities between 1 and 16.","metadata":{}},{"cell_type":"code","source":"digits.data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(digits.data[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target consists of the 1797 labels, for whichever digit the sample represents.","metadata":{}},{"cell_type":"code","source":"digits.target.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and they're in order:\nfor n in range(14):\n    print(digits.target[n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample images\nThe bunch object also contains an image attribute that stores the images as 8x8 arrays of pixel intensities.","metadata":{}},{"cell_type":"code","source":"digits.images.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef showDigitImage(n):\n    axes=plt.subplot()\n    image=plt.imshow(digits.images[n], cmap='binary')\n    xticks = axes.set_xticks([])\n    yticks = axes.set_yticks([])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"showDigitImage(35)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"digits.target[35]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prep","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=13)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n### First: A Binary Classifier\n\nThis entire subsection is almost identical to the section in Geron's book. \n\nGoing to train a simple binary classifier to try and identify the 7's. A binary classifier is one that identifies whether some sample is or is not the target. Geron uses the [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html). First we have to split off the 7's from the target vectors.","metadata":{}},{"cell_type":"code","source":"y_train_7 = (y_train == 7)\ny_test_7 = (y_test == 7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(random_state=10) # need to specify since the SGD classifier relies on randomness\nsgd.fit(X_train, y_train_7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can check if it correctly identifies our 5 from above as not being a 7:","metadata":{}},{"cell_type":"code","source":"sgd.predict([digits.data[35]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And if correctly predicts a 7, since we know they're in order:","metadata":{}},{"cell_type":"code","source":"sgd.predict([digits.data[7]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Measuring the Binary Classifier with Cross-Validation\nCross-Validation involves randomly splitting the training set into subsets called *folds* and training and evaluating the model however many times we specify; for *n* folds it'll train on the other *n-1* folds and evaluate on that fold.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# give it 5 folds\nscores = cross_val_score(sgd, X_train, y_train_7, cv=5, scoring=\"accuracy\")\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Average over 5 folds: {100*scores.mean():.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Accuracy Standard Deviation: {100*scores.std():.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"98.61% is very high. We can design and compare it to a dumb classifier that extends from sklearn's base estimator to guess not 7 every time and see that it is still good:","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator\n\nclass Never7Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        return np.zeros((len(X),1),dtype=bool)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"never7 = Never7Classifier()\ncross_val_score(never7, X_train, y_train_7, cv=5, scoring=\"accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can get some predictions with the cross validation as well. Instead of going through and returning evaluation scores, it returns predictions made for each fold.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred = cross_val_predict(sgd, X_train, y_train_7, cv=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Confusion Matrix** for our binary classifier:.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train_7, y_train_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 1269 true negatives: correctly classified as not-a-7\n* 15 false positives: incorrectly classified as 7\n* 5 false negatives: 7's that were incorrectly classified as not-a-7\n* 148 true positives: 7's correctly classified as 7","metadata":{}},{"cell_type":"markdown","source":"### Precision, Recall and the tradeoff\nPrecision, *p*: Accuracy of the positive predictions. $ p = \\frac{TP}{TP+FP}$ \n\nRecall, *r*: ratio of positives that are correctly detected $ p = \\frac{TP}{TP+FN}$ ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\nprecision_score(y_train_7, y_train_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall_score(y_train_7, y_train_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(y_train_7, y_train_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an unavoidable trade-off between precision and recall. The SGD classifier makes it's decisions according to a *decision function*. Each instance gets a score based on this function, which declares it a 7 or not-a-7 based on whether that score is above or below some threshold. We cannot set this threshold directly, but we can call the decision function method to make predictions based on any threshold:","metadata":{}},{"cell_type":"code","source":"aDigit = digits.data[7]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_scores = sgd.decision_function([aDigit])\ny_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0\ny_predicted_aDigit = (y_scores > threshold)\ny_predicted_aDigit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 5550\ny_predicted_aDigit = (y_scores > threshold)\ny_predicted_aDigit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So raising the threshold does decrease the recall, because it'll miss more true cases. To make the decision between precision and recall, we can use cross_val_predict again, but have it return decision scores rather than predictions, and then plot them on a precision-recall curve:","metadata":{}},{"cell_type":"code","source":"y_scores = cross_val_predict(sgd, X_train, y_train_7, cv=5, method=\"decision_function\")\ny_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_7, y_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_precision_recall_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recalls\")\n    plt.legend(loc=\"center right\")\n    plt.grid(True)               \n    \nplot_precision_recall_threshold(precisions, recalls, thresholds)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The ROC Curve\nThe receiver operating characteristic curve for a binary classifier. It plots the true positive rate (i.e., the recall) against the false positive rate, or the ratio of negative instances that are incorrectly classified as positive.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_train_7, y_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal \n    plt.grid(True)                                            \n\nplt.figure(figsize=(8, 6))                                   \nplot_roc_curve(fpr, tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Other Models\nNow we're going to work on the entire dataset and see if we can train a model that can identify all of the digits. First, with a k-nearest neighbors classifier:","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn=KNeighborsClassifier()\nknn.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = knn.predict(X_test)\ntest = y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count the ones the knn classifier got wrong\nwrong = [(p,e) for (p,e) in zip(predicted, test) if p != e]\nwrong","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\nExamining the estimator's score and the confusion matrix. \n\nFor classification model's the score method will return the accuracy score.","metadata":{}},{"cell_type":"code","source":"print(f'Score: {100*knn.score(X_test, y_test):.2f} %')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrix goes through every prediction and class and show the correct and incorrect predictions for that class.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(test, predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion matrices look real nice as seaborn heatmaps\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seaborn needs the confusion matrix in a pandas DataFrame\ncm_df = pd.DataFrame(cm, index=range(10), columns=range(10))\n\naxes = sns.heatmap(cm_df, annot=True, cmap=\"nipy_spectral_r\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classification report produces a table of classification metrics.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nnames = [str(digit) for digit in digits.target_names]\n\nprint(classification_report(test, predicted, target_names=names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(knn, X_train, y_train, cv=5)\nscores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Average over 5 folds: {100*scores.mean():.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Accuracy Standard Deviation: {100*scores.std():.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HyperParameter Tuning: Finding the best *k*\n","metadata":{}},{"cell_type":"code","source":"def analyzeK(X_train, y_train):\n    # lists to plot the results\n    kvalues = []\n    av_scores = []\n    \n    # loop over the odd k values from 1-20\n    for k in range(1, 20, 2):\n        knn =  KNeighborsClassifier(n_neighbors=k)\n        scores = cross_val_score(knn, X_train, y_train, cv=5)\n        print(f'{k}: mean accuracy = {100*scores.mean():.2f}% -- standard deviation = {100*scores.std():.2f}%')\n        kvalues.append(k)\n        av_scores.append(100*scores.mean())\n        \n    plt.figure(figsize=(15,10))\n    kplot = sns.barplot(x=kvalues, y=av_scores)\n    \n    kplot.set(ylim=(96,100))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyzeK(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"to be continued....","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}