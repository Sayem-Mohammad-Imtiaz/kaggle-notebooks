{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **K-means clustering using a randomly generated dataset**\n\n*K-means clustering is an unsupervised learning algorithm where we do not need to pass output labels to the algorithm for it to be trained. The training exercise is done without supervision from the users and the end result is a set of 'K' non-overlapping partitions.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **We will create a dataset using the make_blobs function which we will use in our clustering exercise. We will also be using matplotlib to visualize our clusters.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the packages\nimport random\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Setting up a seed for repeatability of results. '42' because it is the answer to everything(google if you don't understand :P)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **We shall now create a dataset using the make_blobs function. Kindly note the following:** \n\n\n1. The centers are the ones which we have randomly selected since we are creating the dataset ourselves. We can also select 'K' points on our own which we can choose to call as cluster centroids. This is best done when we have an external dataset with us.\n2. The cluster_std is the cluster standard deviation\n3. n_samples is the number of rows(training samples) we want to create","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = make_blobs(n_samples=5000, centers= [[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std= 0.9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Let us visualize how the data looks**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nplt.title(\"The different clusters generated from make_blobs\")\nplt.scatter(x = X[:, 0], y = X[:,1], c=y[:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **We will now be using the K-means clustering algorithm for K=4**\n\nNote that the value of K is a hard problem in K-means and often multiple different values have to be selected to determine the best K. But here, we have created our own dataset using 4 centroids hence we can easily assign k=4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means = KMeans(n_clusters=4, init = 'k-means++', n_init= 12)\nk_means.fit(X)\nk_means_labels = k_means.labels_\nk_means_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* *K means gave us the following cluster centers*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k_means_cluster_centers = k_means.cluster_centers_\nk_means_cluster_centers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Visualizing the results using the below code for a beautiful representation of K means","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the plot with the specified dimensions.\nfig = plt.figure(figsize=(10, 7))\n\n# Colors uses a color map, which will produce an array of colors based on\n# the number of labels there are. We use set(k_means_labels) to get the\n# unique labels.\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))\n\n# Create a plot\nax = fig.add_subplot(1, 1, 1)\n\n# For loop that plots the data points and centroids.\n# k will range from 0-3, which will match the possible clusters that each\n# data point is in.\nfor k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1]])), colors):\n\n    # Create a list of all data points, where the data poitns that are \n    # in the cluster (ex. cluster 0) are labeled as true, else they are\n    # labeled as false.\n    my_members = (k_means_labels == k)\n    \n    # Define the centroid, or cluster center.\n    cluster_center = k_means_cluster_centers[k]\n    \n    # Plots the datapoints with color col.\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.')\n    \n    # Plots the centroids with specified color, but with a darker outline\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n\n# Title of the plot\nax.set_title('KMeans')\n\n# Remove x-axis ticks\nax.set_xticks(())\n\n# Remove y-axis ticks\nax.set_yticks(())\n\n# Show the plot\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Hope that you liked it. K-means is just one of the many types of clustering algorithms but the one you wouldn't want to miss learning. Hope that you liked the notebook and its explanation. Kindly upvote and comment what you learned*","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}