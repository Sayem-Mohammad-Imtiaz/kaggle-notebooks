{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas\nimport tensorflow as tf\nimport re\nimport keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.engine.topology import Layer\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.models import Model\nimport gensim\nfrom gensim.models import Word2Vec\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct_map = {\"ain't\": \"is not\", \"'cause\": \"because\",\"he's\": \"he is\", \"how'd\": \"how did\",\"how's\": \"how is\",\n          \"'m\": \" am\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"o'clock\": \"of the clock\",\"shan't\": \"shall not\",\n          \"so's\": \"so as\", \"this's\": \"this is\", \"that's\": \"that is\", \"there's\": \"there is\",\n          \"here's\": \"here is\", \"what's\": \"what is\", \"when's\": 'when is', \"where'd\": \"where did\",\n          \"where's\": \"where is\", \"who's\": \"who is\", \"why's\": \"why is\", \"y'all\": \"you all\", \"'d\": ' would',\n          \"'ll\": ' will', \"n't\": \" not\", \"'ve\": \" have\", \"'re\": ' are'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Contraction Mapping"},{"metadata":{"trusted":true},"cell_type":"code","source":"def CleanSentence(string):\n    for a, b in ct_map.items():\n        string = string.replace(a, b)#Contraction Recovery\n    string = string.replace('!', ' puncexc')\n    string = string.replace('?', ' puncask')\n    string = re.sub(r'[0-9]+', '0', string)\n    string = string.replace(' ', '9')#'9' here is just a special symbol\n    string = ''.join(list(filter(str.isalnum, string)))\n    return string.split('9')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TextToSequence(data, vocab):\n    return [list(filter(lambda x: x is not None, [vocab[token].index + 1 if token in vocab.keys() else None \n                                                  for token in sentence]))for sentence in data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def textpre(emb_dim):\n    train = pandas.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv','\\t')\n    unlabel_train = pandas.read_csv('../input/word2vec-nlp-tutorial/unlabeledTrainData.tsv','\\t',\n                                    error_bad_lines = False)\n    additional_data = pandas.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n    test = pandas.read_csv('../input/word2vec-nlp-tutorial/testData.tsv','\\t')\n    s = train['sentiment']\n    del train['sentiment']\n    train['sentiment'] = s #to rearrange the postion of column 'sentiment'\n    unlabel_train['sentiment'] = -2\n    additional_data['sentiment'] = np.asarray(additional_data['sentiment'] == 'positive', dtype = 'int32')\n    s = np.asarray(np.asarray(test['id'].apply(lambda x: x.split('_')[1]), dtype = 'int32') > 5,\n                   dtype = 'int32')\n    #To deal with data leakage.\n    #See https://www.kaggle.com/c/word2vec-nlp-tutorial/discussion/27022#latest-400953\n    test['sentiment'] = -1\n    del train['id'], test['id'], unlabel_train['id']\n    data = pandas.concat([train, additional_data, unlabel_train, test])\n    review = list(data['review'].apply(lambda x: CleanSentence(x)))\n    model = Word2Vec(review, size = emb_dim, workers = 4, iter = 0)\n    emb_mat = np.concatenate([np.zeros((1, emb_dim), dtype = 'float32'), model.wv.vectors])\n    print(emb_mat.shape)\n    seq = TextToSequence(review, model.wv.vocab)\n    seq = pad_sequences([doc if len(doc) <= 128 else doc[:64] + doc[-64:] for doc in seq], padding = 'post')\n    tar = np.asarray(data['sentiment'], dtype = 'int32')\n    r = data['sentiment'] >= 0\n    train = np.hstack((seq[r], np.expand_dims(tar[r], -1)))\n    r = data['sentiment'] == -1\n    test = np.hstack((seq[r], np.expand_dims(s, -1)))\n    print('text preprocessing completed')\n    return [train, test, emb_mat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Self_Attention(Layer):\n    def __init__(self, head_num, head_size, **kwargs):\n        self.head_size = head_size\n        self.sqrt_head_size = math.sqrt(head_size)\n        self.head_num = head_num\n        self.output_units = head_size * head_num\n        self.supports_masking = True\n        super(Self_Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.d_model = input_shape[-1]\n        self.kernal = self.add_weight('kernal', (self.head_num, 3, self.d_model, self.head_size),\n                                      initializer = 'glorot_uniform')\n        super(Self_Attention, self).build(input_shape)\n        self.built = True\n    def call(self, inputs, mask = None):\n        w = K.dot(inputs, self.kernal)\n        w = K.permute_dimensions(w, [3, 2, 0, 1, 4])\n        Head = K.batch_dot(w[0], K.permute_dimensions(w[1], [0, 1, 3, 2]))\n        if mask is not None:\n            new_mask = K.expand_dims(mask, -1)\n            new_mask = K.batch_dot(new_mask, K.permute_dimensions(new_mask, [0, 2, 1]))\n            new_mask = K.expand_dims(new_mask, 0)\n            Head -= 1e8 * (K.ones_like(Head) - new_mask)\n        Head = K.softmax(Head)\n        Head = K.batch_dot(Head, w[2])\n        Head = K.permute_dimensions(Head, [1, 2, 3, 0])\n        return K.reshape(Head, (-1, K.shape(Head)[1],self.output_units))\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1], self.output_units)\n    def compute_mask(self, inputs, mask = None):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Position_Embedding(Layer):\n    def __init__(self, **kwargs):\n        self.supports_masking = True\n        super(Position_Embedding, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.depth = input_shape[1]\n        self.d_model = input_shape[2]\n        super(Position_Embedding, self).build(input_shape)\n        self.built = True\n    def call(self, inputs):\n        pos_emb = np.zeros([1, self.depth, self.d_model])\n        p = np.zeros([self.d_model], dtype = 'int32')\n        for i in range(self.d_model):\n            p[i] = math.pow(10000, (i - 1) / self.d_model) if i%2 else math.pow(10000, i / self.d_model)\n        for i in range(self.depth):\n            for j in range(self.d_model):\n                pos_emb[0][i][j] = math.cos(i / p[j]) if j%2 else math.sin(i / p[j])\n        return inputs + tf.Variable(np.asarray(pos_emb, dtype = 'float32'), trainable = False)\n    def compute_mask(self, inputs, mask = None):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Attention_block(inputs, head_num, head_size, hidden_units):\n    s = Self_Attention(head_num, head_size)(inputs)\n    s = BatchNormalization()(Add()([s, inputs]))\n    output = Dense(hidden_units, activation = 'relu')(s)\n    output = Dense(int(inputs.shape[-1]))(output)\n    return BatchNormalization()(Add()([s, output]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The full layer in the original paper *Attention is all you need*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, emb_mat = textpre(256)\ntf.reset_default_graph()\nkeras.backend.clear_session()\nnp.random.seed(7)\ntf.set_random_seed(7)\nx = Input(shape = (train.shape[1] - 1,), dtype = 'int32')\nemb = Embedding(emb_mat.shape[0], emb_mat.shape[1], weights = [emb_mat], mask_zero = True,\n                trainable = False)(x)\nprob = Position_Embedding()(BatchNormalization()(emb))\nprob = Attention_block(prob, 4, 64, 1024)\nprob = GlobalMaxPool1D()(prob)\nprob = Dense(1, activation = 'sigmoid')(prob)\nmodel = Model(inputs = x, outputs = prob)\nmodel.compile('adam', 'binary_crossentropy', ['accuracy'])\nmodel.summary()\nmodel.fit(train[:, :-1], train[:, -1], 128, 12, validation_data = (test[:, :-1], test[:, -1]))#epoch 12\n#make submission\nres = np.asarray(model.predict(test[:, :-1]) > 0.5, dtype = 'int32')\np = pandas.read_csv('../input/word2vec-nlp-tutorial/sampleSubmission.csv')\np['sentiment'] = res[:, 0]\np.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}