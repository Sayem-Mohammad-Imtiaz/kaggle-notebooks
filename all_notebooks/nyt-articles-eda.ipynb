{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nimport math\nimport calendar\n\nfrom scipy.stats import boxcox, yeojohnson","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this section, I looked for trends, patterns and possible predictors of article popularity. I also split my data into a train and test dataset. My train dataset contains articles from January 1 to September 31, and my test dataset contains articles from October 1 to December 31. I did this EDA with the goal of informing my modelling efforts to predict whether an article is popular (has more than 90 comments)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load train dataset and keywords column as a list\ntrain = pd.read_csv('/kaggle/input/new-york-times-articles-comments-2020/train.csv', converters={'keywords': eval})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our train dataset has 12,792 articles\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our train dataset has 12 features which are ordinal, continous and text-based features.\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have 62 unique newsdesks, 41 sections, and 61 subsections\ntrain['newsdesk'].nunique(), train['section'].nunique(), train['subsection'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for Null Values\nAlmost 2/3 of our articles don't have subsections. It's a pretty important predictor for article popularity, so we won't drop it. There are three articles that don't have abstracts -- we'll impute fill this in with a whitespace character."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for null values\ntrain.isnull().sum()[train.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['abstract'] = train['abstract'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for Duplicates\nThere are a couple of duplicated headlines in our data. These are likely recurring weekly or monthly articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for duplicated values\ntrain['headline'].duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['headline'].duplicated()]['headline'].value_counts().head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Articles with duplicate headlines are generally pretty unpopular\ntrain[train['headline'].duplicated() == True]['is_popular'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Popularity vs Number of Comments\nThere's a large group of articles that have less than 90 comments -- this is where I chose to split the data. We can see that `n_comments` has a heavy positive skew, with the number of articles decreasing in proportion to the number of comments. You can change this into a binary classification problem by using the following code: `train['is_popular'] = train['n_comments'].apply(lambda x: 1 if x > 90 else 0)`. \n\nOf course, if you want to do this, you'll need to drop `n_comments` at some point.\n\nIt's important to note here that not all NYT articles are open for comments. The NYT moderation team chooses articles to open for public commentary. Our data only reflects articles that were opened for commentary AND recieved at least one comment."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average number of comments\ntrain['n_comments'].mean()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(16,8))\nsns.histplot(train['n_comments'].drop(train[train['n_comments'] > 3000].index), bins=35)\nmean = train['n_comments'].mean()\nplt.axvline(90, ls='-', c='red', label='Split', lw=4)\nplt.legend(fontsize=12, loc=1)\nplt.xlabel('Number of Comments')\nplt.ylabel('Number of Articles')\nplt.title(f'Number of Comments', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Class Balance"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    # Overall, our classes are pretty much evenly balanced\n    g = sns.countplot(train['is_popular'])\n    g.set_xticklabels(['Unpopular (< 90 comments)', 'Popular (> 90 comments)'])\n    plt.xlabel('')\n    plt.ylabel('Number of Articles')\n    plt.title('Class Balance', fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train['is_popular'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are a few extreme outliers in our data\nplt.figure(figsize=(16,4))\nsns.boxplot(data=train['n_comments'], orient='h')\nplt.xlabel('n_comments')\nplt.yticks([])\nplt.xlabel('Number of Comments')\nplt.title('Number of Comments', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 3 outliers\ntrain[train['n_comments'] > 4000][['headline', 'abstract', 'n_comments', 'pub_date']] \\\n.sort_values(by='n_comments', ascending=False).head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Count\n\nDealing with word count is slightly tricky. We can see that the feature is normally distributed in general with a heavy positive skew. We can also see that there are a large number of articles that have a word count of 0. These are interactive features that don't have 'words' in the conventional sense. Because this is not technically 'missing' data, I'm not going to impute it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Close to a normal distribution, with a positive skew\nplt.figure(figsize=(16,8))\nmean = train['word_count'].mean()\nplt.axvline(mean, ls='--', color='black')\nsns.histplot(train['word_count'])\nplt.xlabel('Word Count')\nplt.title(f'Word Count (Mean: {mean:.0f} words)', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general, machine learning algorithms tend to perform better when the distribution of variables is normal -- in other words, performance tends to improve for variables that have a standard distribution."},{"metadata":{},"cell_type":"markdown","source":"#### Feature Transformation\n\nHere, we can see the effectiveness of various transformation methods. The Boxcox transformation seems to work best here -- our data is still slightly skewed but much closer to a normal distribution."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def transform_var(col, df):\n    skew_dict = {} # Creating dictionary to store skew values\n    df[f'{col}_log'] = np.log1p(df[f'{col}'])\n    df[f'{col}_box'] = df[f'{col}'].replace(0, 0.001) # Replacing as Boxcox can't transform values that are 0\n    df[f'{col}_box'] = boxcox(df[f'{col}_box'])[0]\n    df[f'{col}_sqrt'] = np.sqrt(df[f'{col}'])\n    \n    skew_dict['Original'] = df[f'{col}'].skew()\n    skew_dict['Log1p'] = df[f'{col}_log'].skew()\n    skew_dict['Boxcox'] = df[f'{col}_box'].skew()\n    skew_dict['Square Root'] = df[f'{col}_sqrt'].skew()\n    return skew_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_transform(col, df):\n    fig, ax = plt.subplots(2, 2, figsize=(13,9), sharey=True)\n    ax = ax.ravel()\n    sns.histplot(df[f'{col}'], ax=ax[0])\n    ax[0].set_title(f\"Original (Skew: {skew_dict['Original']:.3f})\", fontsize=14)\n    sns.histplot(df[f'{col}_log'], ax=ax[1])\n    ax[1].set_title(f\"Log1p (Skew: {skew_dict['Log1p']:.3f})\", fontsize=14)\n    sns.histplot(df[f'{col}_box'], ax=ax[2])\n    ax[2].set_title(f\"Boxcox (Skew: {skew_dict['Boxcox']:.3f})\", fontsize=14)\n    sns.histplot(df[f'{col}_sqrt'], ax=ax[3])\n    ax[3].set_title(f\"Square Root (Skew: {skew_dict['Square Root']:.3f})\", fontsize=14)\n    for ax in ax:\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n    plt.suptitle('Transformed Word Count', fontsize=18)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew_dict = transform_var('word_count', train)\nplot_transform('word_count', train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# There are a few extreme outliers in our data\nplt.figure(figsize=(16,4))\nsns.boxplot(data=train['word_count'], orient='h')\nplt.yticks([])\nplt.xlabel('Word Count')\nplt.title('Word Count', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interactive Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stories with a word count of 0 seem to be interactive features\ntrain[train['word_count'] == 0] \\\n    [['headline', 'newsdesk', 'section', 'material', 'word_count']].head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# All articles with a word count of 0 are interactive features\ntrain[train['word_count'] == 0]['material'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# They mostly come from these particular newsdesks\ntrain[train['word_count'] == 0]['newsdesk'].value_counts().head(5)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# 'The Upshot' and 'Opinion' appear to be newsdesks specifically created for interactive features\ntrain['newsdesk'].value_counts()[train['newsdesk'].value_counts().index.str.contains('Upshot|Magazine|U.S.|Op|Clim')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# They generally seem to be more popular than the average article\ntrain[train['word_count'] == 0][['is_popular']].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of Comments versus Word Count\nDoes word count, or the length of an article affect the number of comments on each article? In the plot below, we can see that there's generally a positive relationship between these two variables, except for OpEd articles where it seems that word count doesn't affect number of comments at all. OpEd articles have an average of around 1100 words."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining different newsdesk names\nplt.figure(figsize=(12, 10))\ntrain['newsdesk'] = train['newsdesk'].apply(lambda x: 'The Upshot' if x=='Upshot' else x)\ntrain['newsdesk'] = train['newsdesk'].apply(lambda x: 'OpEd' if x=='Opinion' else x)\ntrain['newsdesk'] = train['newsdesk'].apply(lambda x: 'AtHome' if x=='At Home' else x)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# OpEd length doesn't affect number of comments -- but other desks tend to have more comments as word count increases\ntop_news_df = train['newsdesk'].value_counts().head(5).index\n\ng = sns.lmplot(data=train.loc[train['newsdesk'].isin(top_news_df)], x='word_count', y='n_comments', \n               hue='newsdesk', palette='tab10', height=8, aspect=1.10, scatter_kws={'alpha':0.3, 's':15}, legend_out=False)\n\nfor lh in g._legend.legendHandles: \n    lh.set_alpha(1)\n    lh._sizes = [20] \n\ng._legend.set_title('News Desk')\n\nplt.ylabel('Number of Comments', fontsize=11)\nplt.xlabel('Word Count', fontsize=11)\nplt.legend(fontsize=14)\nplt.title('Number of Comments versus Word Count', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There's a moderate positive correlation between these two variables\ntrain.corr()['is_popular']['word_count']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this point, I'll be shifting to looking at popularity (as denoted by `is_popular`) instead of number of comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We won't be using number of comments moving forward\ntrain = train.drop(columns=['n_comments'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Headline / Abstract Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['headline_len'] = train['headline'].apply(lambda x: len(x))\ntrain['abstract_len'] = train['abstract'].apply(lambda x: len(x))\ntrain['head_abs_len'] = train['headline_len'] + train['abstract_len']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Headline Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(data=train, x='headline_len');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.lineplot(data=train, x='headline_len', y='is_popular')\nmean = train.groupby('headline_len').mean()['is_popular'].mean()\n#plt.axhline(mean, color='black', ls='--', label=f'Avg. Popularity: {mean:.2f}')\n#plt.legend(fontsize=12)\nplt.axhline(0.5, color='black', ls='--')\nplt.xlabel('Headline Length')\nplt.title('Average Popularity vs Headline Length')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Abstract Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(data=train, x='abstract_len');","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.lineplot(data=train, x='abstract_len', y='is_popular')\nmean = train.groupby('abstract_len').mean()['is_popular'].mean()\nplt.axhline(0.5, color='black', ls='--')\n#plt.axhline(mean, color='black', ls='--', label=f'Avg. Popularity: {mean:.2f}')\n#plt.legend(fontsize=12);\nplt.xlabel('Abstract Length')\nplt.xlim(15, 250)\nplt.title('Average Popularity vs Abstract Length');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Headline & Abstract Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(data=train, x='head_abs_len');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.lineplot(data=train, x='head_abs_len', y='is_popular')\nmean = train.groupby('head_abs_len').mean()['is_popular'].mean()\nplt.axhline(0.5, color='black', ls='--')\n#plt.axhline(mean, color='black', ls='--', label=f'Avg. Popularity: {mean:.2f}')\n#plt.legend(fontsize=12);\nplt.xlabel('Headline + Abstract Length')\nplt.title('Average Popularity vs Headline & Abstract Length');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## News Desk"},{"metadata":{},"cell_type":"markdown","source":"These are the newsdesks with the most number of comments. Unsurprisingly, OpEd articles are at the top, followed by Foreign and Business. In 2017, the NYT implemented a [new commenting system](https://www.nytimes.com/2017/06/13/insider/have-a-comment-leave-a-comment.html) that opened up OpEd articles and other selected news articles for 24 hours. This is likely part of the reason why OpEd articles seem to draw a higher frequency of comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping largest 20 newsdesks and sorting by popularity\ndf = train['newsdesk'].value_counts(ascending=False).reset_index()\ndf.columns=['newsdesk', 'n_articles']\ntemp = pd.merge(df, train.groupby('newsdesk').mean()['is_popular'].reset_index()).head(10)\ntemp['n_articles'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_index = df['newsdesk'].head(20).values\ng_df = train[train['newsdesk'].isin(g_index)]\ng_data = g_df.groupby('newsdesk').mean()['is_popular'].sort_values(ascending=False)\ng_data = g_data.to_frame().reset_index()\ng_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 newsdesks\nplt.figure(figsize=(12, 12))\nsns.barplot(data=g_data, y=g_data['newsdesk'], x=g_data['is_popular'], orient='h', palette='coolwarm_r')\nplt.xlabel('Average Popularity')\nplt.ylabel('Newsdesk')\nplt.xticks(np.arange(0.0, 1.1, 0.1), fontsize=12)\nplt.yticks(fontsize=12)\nplt.title('Newsdesk Avg. Popularity', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some newsdesks have many more popular than unpopular articles such as OpEd, Politics, Games, and Washington. Other newsdesks have many more unpopular than popular articles, like Culture and Podcasts."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing top newsdesks by proportion of popularity\ng_index = df['newsdesk'].head(20).values\ng_df = train[train['newsdesk'].isin(g_index)]\norder = g_df.groupby('newsdesk').mean()['is_popular'].sort_values(ascending=False).index\nplt.figure(figsize=(12,12))\nsns.countplot(data=g_df, y='newsdesk', hue='is_popular', order=order)\nplt.legend(labels=['Not Popular', 'Popular'], fontsize=12, loc='best')\nplt.title('Largest News Desks Sorted By Avg. Popularity', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Section"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train['section'].value_counts(ascending=False).reset_index()\ndf.columns=['section', 'n_articles']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_index = df['section'].head(20).values\ng_df = train[train['section'].isin(g_index)]\ng_data = g_df.groupby('section').mean()['is_popular'].sort_values(ascending=False)\ng_data = g_data.to_frame().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 20 sections\nplt.figure(figsize=(12, 10))\nsns.barplot(data=g_data, y=g_data['section'], x=g_data['is_popular'], orient='h', palette='coolwarm_r')\nplt.xlabel('Average Popularity')\nplt.ylabel('Section')\nplt.yticks(fontsize=12)\nplt.xticks(np.arange(0.0, 1.1, 0.1), fontsize=12)\nplt.title('Section Avg. Popularity', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Opinion is the most popular section, followed by Crossword & Games and U.S. Most OpEd newsdesk articles fall into the Opinion section, except for a couple that fall under Sunday Review."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train['newsdesk'] == 'OpEd') & (train['section'] != 'Opinion')][['newsdesk', 'section', 'subsection', \n                                                                        'material', 'headline']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing top newsdesks by proportion of popularity\ng_index = df['section'].head(20).values\ng_df = train[train['section'].isin(g_index)]\norder = g_df.groupby('section').mean()['is_popular'].sort_values(ascending=False).index\nplt.figure(figsize=(12,12))\nsns.countplot(data=g_df, y='section', hue='is_popular', order=order)\nplt.legend(labels=['Not Popular', 'Popular'], fontsize=12, loc='best')\nplt.title('Largest Sections Sorted By Avg. Popularity', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Subsection"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['subsection'] = train['subsection'].fillna('N/A')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train['subsection'].value_counts(ascending=False).reset_index()\ndf.columns=['subsection', 'n_articles']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_index = df['subsection'].head(20).values\ng_df = train[train['subsection'].isin(g_index)]\ng_data = g_df.groupby('subsection').mean()['is_popular'].sort_values(ascending=False)\ng_data = g_data.to_frame().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Top 20 sections\nplt.figure(figsize=(12, 12))\nsns.barplot(data=g_data, y=g_data['subsection'], x=g_data['is_popular'], orient='h', palette='coolwarm_r')\nplt.xlabel('Average Popularity')\nplt.ylabel('Subsection')\nplt.xticks(np.arange(0.0, 1.1, 0.1), fontsize=12)\nplt.title('Subsection Avg. Popularity', fontsize=18)\nplt.yticks(fontsize=12);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Viewing top newsdesks by proportion of popularity\ng_index = df['subsection'].head(20).values\ng_df = train[train['subsection'].isin(g_index)]\norder = g_df.groupby('subsection').mean()['is_popular'].sort_values(ascending=False).index\nplt.figure(figsize=(12,16))\nsns.countplot(data=g_df, y='subsection', hue='is_popular', order=order)\nplt.legend(labels=['Not Popular', 'Popular'], fontsize=12, loc='best')\nplt.title('Largest Subsections Sorted By Avg. Popularity', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Material"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train['material'].value_counts(ascending=False).reset_index()\ndf.columns=['material', 'n_articles']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_index = df['material'].head(20).values\ng_df = train[train['material'].isin(g_index)]\ng_data = g_df.groupby('material').mean()['is_popular'].sort_values(ascending=False)\ng_data = g_data.to_frame().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Top materials\nplt.figure(figsize=(12, 6))\nsns.barplot(data=g_data, y=g_data['material'], x=g_data['is_popular'], orient='h', palette='coolwarm_r')\nplt.xlabel('Average Popularity')\nplt.ylabel('Material')\nplt.yticks(fontsize=12)\nplt.xticks(np.arange(0.0, 1.1, 0.1), fontsize=12)\nplt.title('Material Avg. Popularity', fontsize=18)\nplt.axvline(0.5, ls='--', color='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keywords']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating df that will allow us to calculate frequency of keywords\nkeyword_df = pd.DataFrame(list(train['keywords'].values), index=train.index)\nkeyword_df = keyword_df.stack().reset_index()\nkeyword_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keyword_df[0].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['n_keywords'] = train['keywords'].apply(lambda x: len(x))\nmean = train['n_keywords'].mean()\n\nplt.figure(figsize=(16,8))\nplt.axvline(mean, ls='--', color='black')\nsns.histplot(train['n_keywords'], bins=60)\nplt.xlabel('Number of Keywords')\nplt.xlim(0, 41)\nplt.title(f'Number of Keywords (Mean: {mean:.1f} keywords)', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\nsns.boxplot(data=train['n_keywords'], orient='h')\nplt.title('Number of Keywords Box Plot', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at articles with more than 40 keywords\ntrain[train['n_keywords'] > 40][['newsdesk', 'section', 'material', 'headline', 'n_keywords', 'is_popular']].sort_values(by='n_keywords', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are only 2210 articles with more than 10 keywords\ntrain['n_keywords'].value_counts()[train['n_keywords'].value_counts().index > 10].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are only 370 articles with more than 15 keywords\ntrain['n_keywords'].value_counts()[train['n_keywords'].value_counts().index > 15].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g4_df = train.groupby('n_keywords').mean().reset_index().drop(columns='word_count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.lineplot(data=g4_df, x='n_keywords', y='is_popular', color='orange')\nplt.axhline(train.groupby('n_keywords').mean()['is_popular'].mean(), color='gray', ls='--')\nplt.title('Avg Popularity vs Number of Keywords', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(data=train[train['n_keywords'] < 30], x='n_keywords', hue='is_popular', bins=np.arange(0,30))\nplt.legend(labels=['Popular', 'Unpopular'], fontsize=12)\nplt.title('Popularity by Number of Keywords', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There's a faint positive correlation between number of keywords and popularity\ntrain.corr()['is_popular']['n_keywords']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Variables\n\nIn this section, we're going to look at how time affects both the frequency and popularity of articles."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pub_date'] = pd.to_datetime(train['pub_date'])\ntrain['day_of_month'] = train['pub_date'].apply(lambda x: x.day)\ntrain['month'] = train['pub_date'].apply(lambda x: x.month)\ntrain['day_of_week'] = train['pub_date'].apply(lambda x: x.dayofweek)\ntrain['hour'] = train['pub_date'].apply(lambda x: x.hour)\ntrain['ymd'] = train['pub_date'].apply(lambda x: str(x)[:10])\ntrain['ymd'] = pd.to_datetime(train['ymd'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['is_trump'] = train['keywords'].apply(lambda x: 1 if 'Trump, Donald J' in x else 0)\ntrain['is_covid'] = train['keywords'].apply(lambda x: 1 if 'Coronavirus (2019-nCoV)' in x else 1 if 'Coronavirus Risks and Safety Concerns' in x else 0)\ntrain['is_racial'] = train['keywords'].apply(lambda x: 1 if 'Black People' in x else 1 if 'Race and Ethnicity' in x else 1 if 'Discrimination' in x \\\n                                             else 1 if 'Black Lives Matter Movement' in x else 0)\ntrain['is_re'] = train['keywords'].apply(lambda x: 1 if 'Real Estate and Housing (Residential)' in x else 0) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the news cycle here is somewhat cyclical -- within a single month, there are multiple peaks and valleys where the amount of news rapidly increases before falling. This pattern is consistent throughout the year."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\nsns.lineplot(data=train['ymd'].value_counts())\nplt.title('Daily Article Frequency', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Month\nArticles that were published in January, July, August and September have a slightly higher average popularity."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nax = sns.lineplot(data=train['month'].value_counts(), label='Number of Articles')\nax.set_ylabel('Number of Articles')\nax.legend(loc=2)\nax2 = ax.twinx()\nsns.lineplot(data=train.groupby('month').mean()['is_popular'], color='orange', ax=ax2, label='Average Popularity')\nax2.set_ylabel('Average Popularity')\nax2.legend(loc=1)\nplt.title('Avg Popularity vs Frequency (Monthly)', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Day of Month\n\nThere doesn't appear to be a particular increase or decrease in popularity according to the day of the month"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(data=train, x='day_of_month', hue='is_popular')\nplt.legend(labels=['Popular', 'Unpopular'], fontsize=12)\nplt.title('Popularity by Day of Month', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Day of Week\nThere seems to be an inverse relationship between average popularity and the number articles opened for commentary. Average article popularity tends to be lower during the week, and higher on the weekend."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nax = sns.lineplot(data=train['day_of_week'].value_counts(), label='Number of Articles')\nax.set_ylabel('Number of Articles')\nax.legend(loc=2)\nax2 = ax.twinx()\nsns.lineplot(data=train.groupby('day_of_week').mean()['is_popular'], color='orange', ax=ax2, label='Average Popularity')\nax2.set_ylabel('Average Popularity')\nax2.legend(loc=1)\nplt.title('Avg Popularity vs Frequency (Day of Week)', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(data=train, x='day_of_week', hue='is_popular', bins = np.arange(0, 8))\nplt.legend(labels=['Popular', 'Unpopular'], fontsize=12)\nplt.title('Popularity by Day of Week', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hour\nArticles published between 10pm and 2am have a much higher average popularity."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nax = sns.lineplot(data=train['hour'].value_counts(), label='Number of Articles')\nax.set_ylabel('Number of Articles')\nax.legend(loc=2)\nax2 = ax.twinx()\nsns.lineplot(data=train.groupby('hour').mean()['is_popular'], color='orange', ax=ax2, label='Average Popularity')\nax2.set_ylabel('Average Popularity')\nax2.legend(loc=1)\nplt.title('Avg Popularity vs Frequency (Hour)', fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(data=train, x='hour', hue='is_popular', bins=24)\nplt.xticks(np.arange(0, 24, 1))\nplt.legend(labels=['Popular', 'Unpopular'], fontsize=12)\nplt.title('Popularity by Hour', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I created the two graphs below to look at changes in the popularity certain keywords (a proxy for topics) over time. 80% of the articles mentioning Donald Trump are popular, while only 25% of articles mentioning real estate are popular. We can also see the number of popular COVID articles spiked in April."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trump_data = train[train['is_trump'] > 0].groupby('month').mean()\ncovid_data = train[train['is_covid'] > 0].groupby('month').mean()\nracial_data = train[train['is_racial'] > 0].groupby('month').mean()\nre_data = train[train['is_re'] > 0].groupby('month').mean()\n\ng1_data = train.groupby('month').mean().reset_index()\nplt.figure(figsize=(14,8))\nsns.lineplot(data=trump_data, x='month', y='is_popular', label='Trump')\nsns.lineplot(data=covid_data, x='month', y='is_popular', label='COVID-19')\nsns.lineplot(data=racial_data, x='month', y='is_popular', label='Race & Ethnicity')\nsns.lineplot(data=re_data, x='month', y='is_popular', label='Real Estate')\nplt.legend(fontsize=14, loc=1)\nplt.ylabel('Average Popularity', fontsize=12)\nplt.yticks(fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Average Article Popularity by Month by Topic', fontsize=22);\nplt.xticks(np.arange(1,10,1), fontsize=12);\nplt.axhline(0.5, ls='--', color='black')\nplt.ylim(0, 1.0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trump_data = train[(train['is_trump'] > 0) & (train['is_popular'] > 0)].groupby('month').sum()\ncovid_data = train[(train['is_covid'] > 0) & (train['is_popular'] > 0)].groupby('month').sum()\nracial_data = train[(train['is_racial'] > 0) & (train['is_popular'] > 0)].groupby('month').sum()\nre_data = train[(train['is_re'] > 0) & (train['is_popular'] > 0)].groupby('month').sum()\n\nplt.figure(figsize=(14, 8))\nsns.lineplot(data=trump_data, x='month', y='is_popular', label='Trump')\nsns.lineplot(data=covid_data, x='month', y='is_popular', label='COVID-19')\nsns.lineplot(data=racial_data, x='month', y='is_popular', label='Race & Ethnicity')\nsns.lineplot(data=re_data, x='month', y='is_popular', label='Real Estate')\nplt.legend(fontsize=14)\nplt.ylabel('Number of Popular Articles', fontsize=12)\nplt.yticks(fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Topic Popularity by Month', fontsize=22);\nplt.xticks(np.arange(1,10,1), fontsize=12);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}