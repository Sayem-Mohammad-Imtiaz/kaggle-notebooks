{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Implementing RNN(Recurrent Neural Network with LSTM) with Keras\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport math\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and Visualizing Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/international-airline-passengers/international-airline-passengers.csv', skipfooter = 5,engine='python')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = data.iloc[:,1].values\nplt.plot(dataset)\nplt.xlabel('time')\nplt.ylabel('# of Passenger')\nplt.title('International Airline Passenger')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* > In this part, data is preprocessed with different processes.\n* > I follow this order in preprocessing part :\n* 1. reshape\n* 2. change type\n* 3. scalling\n* 4. spliting train-test dataset\n* 5. create dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Needed Libraries**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Reshape and Change Type**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.reshape(-1,1)\ndataset = dataset.astype(\"float32\")\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Scalling and Spliting Test-Train Dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler(feature_range = (0,1))\ndataset = scaler.fit_transform(dataset)\ntrain_size = int(len(dataset) * 0.50)\ntest_size = len(dataset) - train_size\ntrain = dataset[0:train_size,:]\ntest = dataset[train_size:len(dataset),:]\nprint(\"Train size : {}\".format(len(train)))\nprint(\"Test size : {}\".format(len(test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Create Dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"time_stemp = 10\ndataX = []\ndataY = []\nfor i in range(len(train) - time_stemp - 1):\n    a = train[i:(i+time_stemp),0]\n    dataX.append(a)\n    dataY.append(train[i+time_stemp,0])\ntrainX = np.array(dataX)\ntrainY = np.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndataX = []\ndataY = []\nfor i in range(len(train) - time_stemp - 1):\n    a = test[i:(i+time_stemp),0]\n    dataX.append(a)\n    dataY.append(test[i+time_stemp,0])\ntestX = np.array(dataX)\ntestY = np.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX = np.reshape(trainX, (trainX.shape[0],1,trainX.shape[1]))\ntestX = np.reshape(testX,(testX.shape[0],1,testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing LSTM Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Needed Libraries**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(10,input_shape=(1,time_stemp))) #10LSTM neuron(block)\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error',optimizer='adam')\nmodel.fit(trainX,trainY,epochs = 50, batch_size = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions and Visualising RNN Model","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_predict = model.predict(trainX)\ntest_predict = model.predict(testX)\ntrain_predict = scaler.inverse_transform(train_predict)\ntrainY = scaler.inverse_transform([trainY])\ntest_predict = scaler.inverse_transform(test_predict)\ntestY= scaler.inverse_transform([testY])\ntrain_score = math.sqrt(mean_squared_error(trainY[0],train_predict[:,0]))\nprint('Train Score %.2f RMSE' %(train_score))\ntest_score = math.sqrt(mean_squared_error(testY[0],test_predict[:,0]))\nprint('Test Score %.2f RMSE' %(test_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_predict_plot = np.empty_like(dataset)\ntrain_predict_plot[:,:] = np.nan\ntrain_predict_plot[time_stemp:len(train_predict) +time_stemp,:]=train_predict\ntest_predict_plot = np.empty_like(dataset)\ntest_predict_plot[:,:] = np.nan\ntest_predict_plot[len(train_predict)+(time_stemp*2)+1:len(dataset) -1,:] = test_predict\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(train_predict_plot)\nplt.plot(test_predict_plot)\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}