{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/naive-bayes/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.datasets import load_iris\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" \t1) Alcohol\n \t2) Malic acid\n \t3) Ash\n\t4) Alcalinity of ash  \n \t5) Magnesium\n\t6) Total phenols\n \t7) Flavanoids\n \t8) Nonflavanoid phenols\n \t9) Proanthocyanins\n\t10)Color intensity\n \t11)Hue\n \t12)OD280/OD315 of diluted wines\n \t13)Proline     \n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/naive-bayes/wine.data.csv', names=[\"Wine\",\"Alcohol\",\"Malic acid\",\"Ash\",\"Alcalinity of ash\",\"Magnesium\",\"Total phenols\",\"Flavanoids\",\"Nonflavanoid phenols\",\"Proanthocyanins\",\"Color intensity\",\"Hue\",\"OD280_OD315\",\"Proline\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.iloc[:,0]\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,1:]\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#iris_data = load_iris()\n#X = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n#y = iris_data.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Descent Based Algorithms\nMachine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below:\n\nGradient descent formula\n\nThe presence of feature value X in the formula will affect the step size of the gradient descent. The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\n\nHaving features on a similar scale can help the gradient descent converge more quickly towards the minima.\n\nhttps://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, test_size=0.2)\nsc_X = StandardScaler()\nX_trainscaled=sc_X.fit_transform(X_train)\nX_testscaled=sc_X.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_trainscaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_testscaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation=\"relu\",random_state=1).fit(X_trainscaled, y_train)\ny_pred=clf.predict(X_testscaled)\nprint(clf.score(X_testscaled, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig=plot_confusion_matrix(clf, X_testscaled, y_test,display_labels=[\"Setosa\",\"Versicolor\",\"Virginica\"])\nfig=plot_confusion_matrix(clf, X_testscaled, y_test)\nfig.figure_.suptitle(\"Confusion Matrix for Iris Dataset\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}