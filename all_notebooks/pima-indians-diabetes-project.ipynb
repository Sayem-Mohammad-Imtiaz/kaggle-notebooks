{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# 1. Prepare Problem\n# 1.a) Load libraries\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.io as pio\npio.renderers.default = \"svg\"\npy.init_notebook_mode(connected=True)\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import skew\nfrom scipy.stats import kurtosis\n\n# Load libraries for evaluating algorithms\nfrom pandas import set_option\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import VotingClassifier\n\n# 1.b) Load dataset\ndf = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n# Getting dataframe columns names\ndf_name=df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# 2. Summarize Data\n# 2.a) Descriptive statistics\nprint('dimension of data',df.shape)\ndf.info()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check missing values\nsns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2.b) Data visualizations:\n# histogram of individual attributes\ndf.hist(bins=20,figsize=(18,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A further characterization of the data includes skewness and kurtosis.\n\nSkewness is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution. It differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n\nKurtosis is all about the tails of the distribution — not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"# density, skewness and kurtosis of each attribute\nfor i in range(len(df.columns)):\n    sns.kdeplot(df[df_name[i]], shade=True);\n    plt.show()\n    print(\"%s: mean (%f), variance (%f), skewness (%f), kurtosis (%f)\" % (df_name[i], np.mean(df[df_name[i]]), np.var(df[df_name[i]]), skew(df[df_name[i]]), kurtosis(df[df_name[i]])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# box and whisker plots\ndf.plot(kind= 'box', subplots=True, layout=(4,4), sharex=False, sharey=False,fontsize=8,figsize=(18,12))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above, it looks like Insulin, DiabetesPedigreeFunction, Age and Pregnancies are skewed Gaussian distributions, and some attributes like BloodPressure, DiabetesPedigreeFunction, Insulin, BMI, Glucose and SkinThickness depict possible outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#  checking the target distribution\nprint(df.groupby('Outcome').size())\nOutLabels = [str(df['Outcome'].unique()[i]) for i in range(df['Outcome'].nunique())]\nOutValues = [df['Outcome'].value_counts()[i] for i in range(df['Outcome'].nunique())]\npie=go.Pie(labels=OutLabels,values=OutValues)\ngo.Figure([pie])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Feature selection, if there are two highly corrolated features we have to drop the one that has more corrolation with other features. because highly corrolated attributes result in the problem of overfitting.\nLet's visualise the correlation between attributes.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizations of the interactions between variables :\nsns.pairplot(df, hue=\"Outcome\", palette='husl')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18, 12))\nax = sns.heatmap(df.corr(),vmin=-1, vmax=1, annot=True, fmt='.2f', cmap='coolwarm')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown, there is no highly corrolated features. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Evaluate Algorithms\n# 4.a) Split-out validation dataset \nX =  df[df_name[0:8]]\nY = df[df_name[8]]\nX_train, X_test, y_train, y_test =train_test_split(X,Y,test_size=0.2,random_state=1,stratify=df['Outcome']) \n# stratify is used to keep the same distribution of 'Outcome' in the train and test dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The diﬀering scales of the data may be negatively impacting the skill of the algorithms. Let’s evaluate the algorithms with a standardized copy of the dataset. \n\nThe preprocessing types used are :\n- MinMax scaler to [0,1]\n- Standard Scaler to mean = 0 and std =1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.b) Data Transforms & Spot-Check Algorithms \ndef GetScaledModel(nameOfScaler):\n    \n    if nameOfScaler == 'standard':\n        scaler = StandardScaler()\n    elif nameOfScaler =='minmax':\n        scaler = MinMaxScaler()\n\n    pipelines = []\n    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier(random_state=2))])))\n    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier(random_state=2))])  ))\n    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier(random_state=2))])  ))\n    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier(random_state=2))])  ))\n\n    return pipelines \n\n# Test options and evaluation metric\nnum_folds = 10\nseed = 7\nscoring = 'accuracy'\n\n# evaluate each model in turn\ndef EvaluateAlg(X,y,nameOfScaler):\n    results = []\n    names = []\n    models = GetScaledModel(nameOfScaler)\n    for name, model in models:\n        kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n        cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)    \n    return results, names\n\nstandard_results, standard_names = EvaluateAlg(X_train,y_train,'standard')\nminmax_results, minmax_names = EvaluateAlg(X_train,y_train,'minmax')\nscore = pd.DataFrame({'Model':standard_names, 'Score-mean':[np.mean(i) for i in standard_results], \n                   'Model_2':minmax_names, 'Score-mean_2':[np.mean(i) for i in minmax_results] })\nprint(score)\n\n# Compare Algorithms\ndef CompAlg(results, names): \n    fig = plt.figure()\n    ax = fig.add_axes([0,0,2,2])\n    ax.boxplot(results, labels=names, showmeans=True, meanline=True, meanprops = dict(linestyle='--', linewidth=2.5, color='green'))\n    ax.yaxis.grid(True)\n    ax.set_title('Algorithm Comparison')\n    fig.text(1.8, 1.9, 'mean : ---', color='green', weight='roman',size=14)\n    plt.show()\n    \nCompAlg(standard_results, standard_names)\nCompAlg(minmax_results, minmax_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown above, the preprocessing of data affects only non tree models.  \nAfter this, we will see the impact of the removal of outliers on the algorithms performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.d) Data Cleaning : outliers investigation\ndf_copy = df.copy()\n# using box and whisker plots to visualize outliers\ndef OutliersBox(df,nameOfFeature):\n    \n    trace0 = go.Box(\n        y = df[nameOfFeature],\n        name = \"All Points\",\n        jitter = 0.3,\n        pointpos = -1.8,\n        boxpoints = 'all',\n        marker = dict(\n            color = 'rgb(7,40,89)'),\n        line = dict(\n            color = 'rgb(7,40,89)')\n    )\n\n    trace1 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Only Whiskers\",\n        boxpoints = False,\n        marker = dict(\n            color = 'rgb(9,56,125)'),\n        line = dict(\n            color = 'rgb(9,56,125)')\n    )\n\n    trace2 = go.Box(\n        y = df[nameOfFeature],\n        name = \"Suspected Outliers\",\n        boxpoints = 'suspectedoutliers',\n        marker = dict(\n            color = 'rgb(8,81,156)',\n            outliercolor = 'rgba(219, 64, 82, 0.6)',\n            line = dict(\n                outliercolor = 'rgba(219, 64, 82, 0.6)',\n                outlierwidth = 2)),\n        line = dict(\n            color = 'rgb(8,81,156)')\n    )\n\n    data = [trace0,trace1,trace2]\n\n    layout = go.Layout(title = \"{} Outliers\".format(nameOfFeature))\n                       \n    go.Figure(data=data,layout=layout).show()\n\n# function to remove outliers\ndef DropOutliers(df_copy,nameOfFeature):\n\n    valueOfFeature = df_copy[nameOfFeature]\n    \n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(valueOfFeature, 25.)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(valueOfFeature, 75.)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3-Q1)*1.5\n    \n    # Index of outliers\n    outliers_idx = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n    \n    # Values of outliers\n    outliers_val = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n\n    # Remove the outliers\n    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers_idx), outliers_val))\n    good_data = df_copy.drop(df_copy.index[outliers_idx]).reset_index(drop = True)\n    print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n    return good_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we remove the outliers of attributes with high kurtozis, namely BloodPressure, DiabetesPedigreeFunction, Insuline, BMI, Glucose and SkinThickness."},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_clmn=['BloodPressure', 'DiabetesPedigreeFunction', 'Insulin', 'BMI', 'Glucose', 'SkinThickness']\ndf_clean=df_copy\nfor i in range(len(outliers_clmn)):\n    OutliersBox(df,outliers_clmn[i])\n    df_clean = DropOutliers(df_clean,outliers_clmn[i])\n    OutliersBox(df_clean,outliers_clmn[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing the accuracy of models after removing outliers\ndf_clean_name = df_clean.columns\nX_c =  df_clean[df_clean_name[0:8]]\nY_c = df_clean[df_clean_name[8]]\nX_train_c, X_test_c, y_train_c, y_test_c =train_test_split(X_c,Y_c,test_size=0.2, random_state=0, stratify=df_clean['Outcome'])  \n\nstandard_results_c, standard_names_c = EvaluateAlg(X_train_c,y_train_c,'standard')\nminmax_results_c, minmax_names_c = EvaluateAlg(X_train_c,y_train_c,'minmax')\n\nscore_c = pd.DataFrame({'Model-s_c':standard_names_c, 'Score-s_c':[np.mean(i) for i in standard_results_c],\n                        'Model-m_c':minmax_names_c, 'Score-m_c':[np.mean(i) for i in minmax_results_c]})\nscore=pd.concat([score,score_c],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CompAlg(standard_results_c, standard_names_c)\nCompAlg(minmax_results_c, minmax_names_c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Irrelevant features can negatively impact model performance. Identifying and removing uneeded and irrelevant feature will enhance the performance of the model.\nAs shown previously, the correlation between attribute didn't say too much, so estimating the importance of features using Bagged decision trees like Random Forest and Extra Trees can yield good results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.e) Feature selection:\n\nclf = ExtraTreesClassifier(n_estimators=250,random_state=2)\nclf.fit(X_train_c, y_train_c)\n\n# feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\n# indexes from min to max value\nsorted_idx = np.argsort(feature_importance)\n\nVariable_importance = pd.DataFrame({'feature':df_clean_name[sorted_idx],'Relative Importance':feature_importance[sorted_idx]})\nprint(Variable_importance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Insulin and SkinThickness are relatively less important than the rest of features, so droping these features could improve the model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_feature_imp=df_clean.drop(['Insulin','SkinThickness'], axis=1)\ndf_feature_imp_name = df_feature_imp.columns\n\nX =  df_feature_imp[df_feature_imp_name[0:6]]\nY = df_feature_imp[df_feature_imp_name[6]]\nX_train_c_imp, X_test_c_imp, y_train_c_imp, y_test_c_imp =train_test_split(X,Y,test_size=0.2,random_state=0,stratify=df_feature_imp['Outcome'])\n\nminmax_results_c_imp, minmax_names_c_imp = EvaluateAlg(X_train_c_imp,y_train_c_imp,'minmax')\n\nscore_c_imp = pd.DataFrame({'Model-m_c_imp':minmax_names_c_imp, 'Score-m_c_imp':[np.mean(i) for i in minmax_results_c_imp]})\nprint(score_c_imp)\n\nCompAlg(minmax_results_c_imp, minmax_names_c_imp)\nscore=pd.concat([score,score_c_imp],axis=1)                                     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still could improve the prediction with algorithm tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Improve Accuracy\n# 5.a) Algorithm Tuning\n\ndef GridSearch(X_train,y_train,model,hyperparameters):\n    kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\n    clf = GridSearchCV(estimator=model,param_grid=hyperparameters,scoring=scoring,cv=kfold)\n    # Fit grid search\n    best_model = clf.fit(X_train, y_train)\n    message = (best_model.best_score_, best_model.best_params_)\n    print(\"Best: %f using %s\" % (message))\n\n    return best_model,best_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression (LR) :  \n\nC : Regularization value.  \npenalty : Can be either \"L2\" or “L1”. Default is “L2”."},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nmodel = Pipeline([('MinMaxScaler', MinMaxScaler()),('LR', LogisticRegression())])\n# create regularization penalty space\npenalty = ['l1', 'l2']\n# create regularization hyperparameter distribution using uniform distribution\nC = [0.01,0.1,0.5,1,1.2,1.4,1.6]\n# Create hyperparameter options\nhyperparameters = dict(LR__C=C,LR__penalty=penalty)\n\nLR = GridSearch(X_train_c_imp,y_train_c_imp,model,hyperparameters)\n#LR_best_model,LR_best_params = GridSearch.GridSearch()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"KNeighborsClassifier (KNN) :  \n\nn_neighbors: Number of neighbors."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([('MinMaxScaler', MinMaxScaler()),('KNN', KNeighborsClassifier())])\n\nneighbors = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nhyperparameters = dict(KNN__n_neighbors=neighbors)\n\nKNN = GridSearch(X_train_c_imp,y_train_c_imp,model,hyperparameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SVC :\n\nC: The Penalty parameter C of the error term.  \nKernel: Kernel type could be linear, poly, rbf or sigmoid."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([('MinMaxScaler', MinMaxScaler()),('SVM', SVC())])\n\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = [ 'linear' , 'poly' , 'rbf' , 'sigmoid' ]\nhyperparameters = dict(SVM__C=c_values, SVM__kernel=kernel_values)\n\nSVC = GridSearch(X_train_c_imp,y_train_c_imp,model,hyperparameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DecisionTreeClassifier (CART) : \n\nmax_depth: Maximum depth of the tree (double).  \nrow_subsample: Proportion of observations to consider (double).  \nmax_features: Proportion of columns (features) to consider in each level (double).  "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([('MinMaxScaler', MinMaxScaler()),('CART', DecisionTreeClassifier(random_state=2))])\n\nmax_depth_value = [2,3,4, None]\nmax_features_value =  [1,2,3,4]\nmin_samples_leaf_value = [1,2,3,4]\ncriterion_value = [\"gini\", \"entropy\"]\n\nhyperparameters = dict(CART__max_depth = max_depth_value,\n                  CART__max_features = max_features_value,\n                  CART__min_samples_leaf = min_samples_leaf_value,\n                  CART__criterion = criterion_value)\n\nCART = GridSearch(X_train_c_imp,y_train_c_imp,model,hyperparameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AdaBoostClassifier (AB) :\n\nlearning_rate: Learning rate shrinks the contribution of each classifier by learning_rate.  \nn_estimators: Number of trees to build."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([('MinMaxScaler', MinMaxScaler()),('AB', AdaBoostClassifier())])\n\nlearning_rate_value = [.01,.05,.1,.5,1]\nn_estimators_value = [50,100,150,200,250,300]\n\nhyperparameters = dict(AB__learning_rate=learning_rate_value, AB__n_estimators=n_estimators_value)\n\nAB = GridSearch(X_train_c_imp,y_train_c_imp,model,hyperparameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GradientBoostingClassifier (GMB)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([('MinMaxScaler', MinMaxScaler()),('GMB', GradientBoostingClassifier(random_state=2))])\n\nlearning_rate_value = [.01,.05,.1,.5,1]\nn_estimators_value = [50,100,150,200,250,300]\n\nhyperparameters = dict(GMB__learning_rate=learning_rate_value, GMB__n_estimators=n_estimators_value)\n\nGMB = GridSearch(X_train_c_imp,y_train_c_imp,model,hyperparameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ExtraTreesClassifier (ET) :\n\nn_estimators: Number of trees to build."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesClassifier(random_state=2))])\n\nn_estimators_value = [50,60,80,100]\nmax_features_value =  [2,3]\nmin_samples_leaf_value = [2,4]\ncriterion_value = [\"gini\", \"entropy\"]\n\nhyperparameters = dict(ET__n_estimators = n_estimators_value,\n                  ET__max_features = max_features_value,\n                  ET__min_samples_leaf = min_samples_leaf_value,\n                  ET__criterion = criterion_value)\n\nET = GridSearch(X_train_c_imp,y_train_c_imp,model,hyperparameters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voting Ensemble :\n\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n# 5.b) Ensembles\n# Voting ensemble\nmodel1 = LogisticRegression(C=1.2, penalty='l2')\n\nmodel2 = KNeighborsClassifier(n_neighbors= 7)\n\nmodel3 = SVC(C =0.1, kernel='poly')\n\nmodel4 = DecisionTreeClassifier(criterion='entropy', max_depth= 2, max_features=3, min_samples_leaf= 1,random_state=2)\n\nmodel5 = AdaBoostClassifier(learning_rate= 0.05, n_estimators=200)\n\nmodel6 = GradientBoostingClassifier(learning_rate=0.5, n_estimators=50,random_state= 2)\n\nmodel7 = GaussianNB()\n\nmodel8 = RandomForestClassifier(random_state = 2)\n\nmodel9 = ExtraTreesClassifier(criterion='entropy', n_estimators= 60, max_features=3, min_samples_leaf= 4,random_state=2)\n\n# create the sub models\nestimators = []\nestimators.append(('MinMax '+'LR'  , Pipeline([('Scaler', MinMaxScaler()),('LR'  , model1)])))\nestimators.append(('MinMax '+'KNN' , Pipeline([('Scaler', MinMaxScaler()),('KNN' , model2)])))\nestimators.append(('MinMax '+'SVM' , Pipeline([('Scaler', MinMaxScaler()),('SVM' , model3)])))\nestimators.append(('MinMax '+'CART', Pipeline([('Scaler', MinMaxScaler()),('CART', model4)])))\nestimators.append(('MinMax '+'AB'  , Pipeline([('Scaler', MinMaxScaler()),('AB'  , model5)])  ))\nestimators.append(('MinMax '+'GBM' , Pipeline([('Scaler', MinMaxScaler()),('GMB' , model6)])  ))\nestimators.append(('MinMax '+'NB' , Pipeline([('Scaler', MinMaxScaler()),('NB' , model7)])  ))\nestimators.append(('MinMax '+'RF'  , Pipeline([('Scaler', MinMaxScaler()),('RF'  , model8)])  ))\nestimators.append(('MinMax '+'ET'  , Pipeline([('Scaler', MinMaxScaler()),('ET'  , model9)])  ))\n\n\nkfold = StratifiedKFold(n_splits=num_folds, random_state=seed, shuffle=True)\nensemble = VotingClassifier(estimators)\nresults = cross_val_score(ensemble, X_train_c_imp, y_train_c_imp, cv=kfold)\nprint('Accuracy on train: ',results.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finalize model :\n\nWe will use the Voting Ensemble to make prediction on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. Finalize Model : Predictions on test dataset\n\n# prepare the model : training the model on the entire training dataset\nsc = MinMaxScaler()\nrescaledX = sc.fit_transform(X_train_c_imp)\nmodel = VotingClassifier(estimators)\nmodel.fit(rescaledX, y_train_c_imp)\n\n# transform the test dataset\nrescaledTestX = sc.transform(X_test_c_imp)\npredictions = model.predict(rescaledTestX)\n\n# Evaluate predictions\nprint(accuracy_score(y_test_c_imp, predictions))\nprint(confusion_matrix(y_test_c_imp, predictions)) \nprint(classification_report(y_test_c_imp, predictions))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}