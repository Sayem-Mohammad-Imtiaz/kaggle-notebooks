{"cells":[{"metadata":{"_uuid":"d89cf4c551305f4374c07fd40eaae43dd9f867dd"},"cell_type":"markdown","source":"# San Francisco Crime Classification\n\n### **Description:** The main purpose of this project is to classify the category of crime based on Location and Time at which crime was commited .\n\n\nThe steps followed are as follows:\n* Step-1: Download dataset from [https://www.kaggle.com/c/sf-crime/data](https://www.kaggle.com/c/sf-crime/data), store it in dataframe. \n* Step-2: Create a new dataframe containing Latitude, Longtitude, Category, Hour, Week, Month and Year of crime.\n* Step-3: Create a new dataframe X by dropping Category. Create a variable Y containing the category of crime.\n* Step-4: Split the data(X_train, y_train and X_test, y_test data(80:20)).\n* Step-5: Check Different Machine Learning Algorithm to get best accuracy.\n* Step-6: Check Different feature Engineering techniques on a particular algorithm(I have tried on KNN) to get better accuracy.\n     * Convert to 3 Dimensional point(x, y, z) from Latitude and Longtitude and use as input.\n     * Use only Latitude and Longtitude as input data.\n     * Use Hour, Week, Month, Year, Latitude, Longtitude as input data.\n     * Use the columns of dataframe that has positive correlation with category of crime.\n     * Try on a smaller part of dataset maybe the model is overfitting.\n* Step-7: Use One Hot Encoder Technique to submit predicted value into required submission format\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d9c27e3f2e633cb204c3ed465bacf0ac08c433df"},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"776518c1796666a80c42accfc481cf38bf1ff74b"},"cell_type":"code","source":"import pandas as pd\nsfc = pd.read_csv('../input/train.csv')\n\nlat=sfc['X'].values\nlong=sfc['Y'].values\ncategory=sfc['Category'].values","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"97639b482f0c2421b5e8119fb8cd3febb537ad7e"},"cell_type":"code","source":"#Convert Weekaday from Text to Numeric\nweek=sfc['DayOfWeek'].values\nweek_map={'Monday':'1', 'Tuesday':'2', 'Wednesday':'3', 'Thursday':'4', 'Friday':'5', 'Saturday':'6', 'Sunday':'7'}\nweek1=[]\nfor i in range(len(week)):\n    week1.append(int(week_map[str(week[i])]))\nprint(week1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"face3fd9448d88b4836563a162b56d27bc88d626"},"cell_type":"code","source":"#Create Month, Year and Hour list from DateStamp\nmonth=[]\nyear=[]\nhour=[]\nfor i in range(len(sfc['Dates'])):\n    t = pd.tslib.Timestamp(sfc['Dates'][i])\n    month.append(t.month)\n    year.append(t.year)\n    hour.append(t.hour)","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5d748fe3f0cfa3f5db198a92aa2cec49beb9919d"},"cell_type":"code","source":"df=pd.DataFrame({'Latitude':lat, 'Longtitude':long, 'Category':category, 'Hour':hour, 'Week':week1, 'Month':month, 'Year':year})\nprint(df)","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2df89dd4e667def7c3712c0b156143fa247adfff"},"cell_type":"code","source":"import copy\nX=copy.deepcopy(df)\nX.drop('Category', axis=1, inplace=True)\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(df['Category'])\nY=le.transform(df['Category']) \nprint(Y)","execution_count":6,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"89991fb6216553d3730107ca05d90beac044c658"},"cell_type":"code","source":"df['Y']=Y\ncorr = df.corr()\nprint(corr)","execution_count":7,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2c9e64f09c244fb3ea381d0e02170fc1d21a7c32"},"cell_type":"code","source":"import pandas as pd\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score","execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cff2d1a1e88c6ec2306f6cdd0164c5c78f342ef3"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)","execution_count":10,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1cf434df233c0c812acad86fcb8c8a23b0538fda"},"cell_type":"code","source":"#Convert to 3 Dimensional point(x, y, z) from Latitude and Longtitude and use as input.\nimport math\nX4=[]\nY4=[]\nZ4=[]\nfor i in range(len(df['Latitude'])):\n    X4.append(math.cos(df['Latitude'][i])*math.cos(df['Longtitude'][i]))\n    Y4.append(math.cos(df['Latitude'][i])*math.sin(df['Longtitude'][i]))\n    Z4.append(math.sin(df['Latitude'][i]))","execution_count":9,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"190628d4578dfcec8943ab1594c91044f1d9449a"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nX3=pd.DataFrame({'X':X4, 'Y':Y4, 'Z':Z4})\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X3, Y, test_size=0.2)\nneigh3 = KNeighborsClassifier(n_neighbors = 50, weights='uniform', algorithm='auto')\nneigh3.fit(X_train3,y_train3) \ny_pred3 = neigh3.predict(X_test3)\nprint(\"Accuracy is \", accuracy_score(y_test3,y_pred3)*100,\"% for K-Value:\")","execution_count":40,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"18c5d92f1eb1df18ab142bdf7048e678e4b656c5"},"cell_type":"code","source":"#Use only Latitude and Longtitude as input data.\nX2=pd.DataFrame({'Latitude':df['Latitude'], 'Longitude':df['Longtitude']})\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y, test_size=0.2)\n#Implement KNN(So we take K value to be )\nneigh2 = KNeighborsClassifier(n_neighbors = 50, weights='uniform', algorithm='auto')\nneigh2.fit(X_train2,y_train2) \ny_pred2 = neigh2.predict(X_test2)\nprint(\"Accuracy is \", accuracy_score(y_test2,y_pred2)*100,\"% for K-Value:\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd6eb6b70f831dd417e365af2a23a77882b2298f"},"cell_type":"code","source":"#Use Hour, Week, Month, Year, Latitude, Longtitude as input data.\nneigh = KNeighborsClassifier(n_neighbors = 50, weights='uniform', algorithm='auto')\nneigh.fit(X_train,y_train) \ny_pred = neigh.predict(X_test)\nprint(\"Accuracy is \", accuracy_score(y_test,y_pred)*100,\"% for K-Value:\")","execution_count":44,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f62ece839de1216bb349ad0efd9fe136cea50cc5"},"cell_type":"code","source":"#Use the columns of dataframe that has positive correlation with category of crime.\nX6=pd.DataFrame({'Hour':df['Hour'], 'Month':df['Month'], 'Week':df['Week']})\nX_train6, X_test6, y_train6, y_test6 = train_test_split(X6, Y, test_size=0.2)\nneigh6 = KNeighborsClassifier(n_neighbors = 50, weights='uniform', algorithm='auto')\nneigh6.fit(X_train6,y_train6) \ny_pred6 = neigh6.predict(X_test6)\nprint(\"Accuracy is \", accuracy_score(y_test6,y_pred6)*100,\"% for K-Value:\")","execution_count":16,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"26f34be86b50d33c6ec2c201e159230ec788fb15"},"cell_type":"code","source":"#Try on a smaller part of dataset maybe the model is overfitting.\ndf1 = df.sample(frac=0.2).reset_index(drop=True)\nX1=copy.deepcopy(df1)\nX1.drop('Category', axis=1, inplace=True)\nY1=df1['Category']\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size=0.2)\n#Implement KNN(So we take K value to be )\nneigh1 = KNeighborsClassifier(n_neighbors = 50, weights='uniform', algorithm='auto')\nneigh1.fit(X_train1,y_train1) \ny_pred1 = neigh1.predict(X_test1)\nprint(\"Accuracy is \", accuracy_score(y_test1,y_pred1)*100,\"% for K-Value:\")","execution_count":45,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"05680b3b65bf7c5399f2fa24bf395a9180753e64"},"cell_type":"code","source":"#Check k for best results of KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfor K in range(100):\n    K_value = K+1\nneigh = KNeighborsClassifier(n_neighbors = 50, weights='uniform', algorithm='auto')\nneigh.fit(X_train, y_train) \ny_pred = neigh.predict(X_test)\nprint(\"Accuracy is \", accuracy_score(y_test,y_pred)*100,\"% for K-Value:\",50)\n","execution_count":38,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1a1ba7ee75e71310d436691f3153a42100acbe11"},"cell_type":"code","source":"#Implement KNN for the best features after Feature Engineering and best value of K\nneigh = KNeighborsClassifier(n_neighbors = 50, weights='uniform', algorithm='auto')\nneigh.fit(X_train,y_train) \ny_pred = neigh.predict(X_test)\nprint(\"Accuracy is \", accuracy_score(y_test,y_pred)*100,\"% for K-Value:\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"50883dbdde80790ff79ddce144ecc97d57334d41"},"cell_type":"code","source":"#Implement Grid Serch for best Gamma, C and Selection between rbf and linear kernel\nfrom sklearn import svm, datasets\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.svm import SVC\nparameter_candidates = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n]\nclf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)\nclf.fit(X_train1, y_train1)   \nprint('Best score for data1:', clf.best_score_) \nprint('Best C:',clf.best_estimator_.C) \nprint('Best Kernel:',clf.best_estimator_.kernel)\nprint('Best Gamma:',clf.best_estimator_.gamma)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"691b2aa5fb07f0ffde657f634e2be094d2f07e5d"},"cell_type":"code","source":"#OVA SVM(Grid Search Results: Kernel - linear, C -1 , Gamma - auto)\nfrom sklearn import svm\nlin_clf = svm.LinearSVC(C=1)\nlin_clf.fit(X_train2, y_train2)\ny_pred2=lin_clf.predict(X_test2)\nprint(accuracy_score(y_test2,y_pred2)*100)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"af78a4ae8387793f730cf42d5d83ac724caa9478"},"cell_type":"code","source":"#OVA SVM(Grid Search Results: Kernel - rbf, C -1 , Gamma - auto)\nfrom sklearn import svm\nlin_clf=svm.SVC(kernel='rbf')\nlin_clf.fit(X_train2, y_train2)\ny_pred2=lin_clf.predict(X_test2)\nprint(accuracy_score(y_test2,y_pred2)*100)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cf62ac9282c40961f7438a8b658bee56fa980341"},"cell_type":"code","source":"#SVM by Crammer(Grid Search Results: Gamma - Auto, C - 1)\nlin_clf = svm.LinearSVC(C=1, multi_class='crammer_singer')\nlin_clf.fit(X_train2, y_train2)\ny_pred2=lin_clf.predict(X_test2)\nprint(accuracy_score(y_test2,y_pred2)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"919e0fdd319f3334e5f7fa882f5621c0cecec629"},"cell_type":"code","source":"#Implementing OVA Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(X_train2, y_train2)\ny_pred2 = clf.predict(X_test2)\nprint(accuracy_score(y_test2,y_pred2)*100)","execution_count":12,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9bd3dbeb5305be0520c13c776bc15931b85c5f22"},"cell_type":"code","source":"#Implementing OVA Logistic Regerssion\nfrom sklearn.linear_model import LogisticRegression\nX2=pd.DataFrame({'Latitude':df['Latitude'], 'Longitude':df['Longtitude']})\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y, test_size=0.2)\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(X_train2, y_train2)\ny_pred = logisticRegr.predict(X_test2)\nprint(accuracy_score(y_test,y_pred)*100)","execution_count":40,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d1e019978ec0c0a8103ae49eb9975cc9ff977f17"},"cell_type":"code","source":"data_dict={}\ntarget = df[\"Category\"].unique()\ncount = 1\nfor data in target:\n    data_dict[data] = count\n    count+=1\nprint(data_dict)","execution_count":18,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d892f3522753ed9689fd0fa42685ebe9df86b846"},"cell_type":"code","source":"from collections import OrderedDict\ndata_dict_new = OrderedDict(sorted(data_dict.items()))\nprint(data_dict_new)","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"22eaf2b7479d60e354bdce18cd432e86b76ff8ec"},"cell_type":"code","source":"import pandas as pd\ntest_dataset = pd.read_csv('C:/Users/RAHUL/AppData/Roaming/SPB_16.6/San Francisco Data Set/test.csv')\ndf_test=pd.DataFrame({'Latitude':test_dataset['X'],'Longtitude':test_dataset['Y']})\n\npredictions = neigh2.predict(df_test)\n","execution_count":33,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"26b081ee192788465e01f97944ddb57b35545530"},"cell_type":"code","source":"#One Hot Encoding of knn as per submission format\nresult_dataframe = pd.DataFrame({\n    \"Id\": test_dataset[\"Id\"]\n})\nfor key,value in data_dict_new.items():\n    result_dataframe[key] = 0\ncount = 0\nfor item in predictions:\n    for key,value in data_dict.items():\n        if(value == item):\n            result_dataframe[key][count] = 1\n    count+=1\nresult_dataframe.to_csv(\"submission_knn.csv\", index=False) ","execution_count":35,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6be83a65ad5fb0ee70cb6cdcddc43ac7bfdf7c19"},"cell_type":"code","source":"predictions = logisticRegr.predict(df_test)","execution_count":41,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"af03c56ff7c5254f47a59e094da7fbc1f58a0475"},"cell_type":"code","source":"#One Hot Encoding of logistic regression as per submission format\nresult_dataframe = pd.DataFrame({\n    \"Id\": test_dataset[\"Id\"]\n})\nfor key,value in data_dict_new.items():\n    result_dataframe[key] = 0\ncount = 0\nfor item in predictions:\n    for key,value in data_dict.items():\n        if(value == item):\n            result_dataframe[key][count] = 1\n    count+=1\nresult_dataframe.to_csv(\"submission_logistic.csv\", index=False) ","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"bb4d3d461f1c9626f1605bfc2ab56bc8e3b8ccea"},"cell_type":"markdown","source":"# Conclusion\n* Out of the feature engineering methods suggested the one which works best on KNN is simply by taking latitude and longtitude as input data.\n* Since input data contains negative values we cannot use OVA Naive Bayes.\n* OVA SVM with linear and rbf kernel and CrammerSVM \n* The  Algorithm gives the best accuracy of  for the given dataset.\n\n|       Algorithm       |           Parameters           | Accuracy |\n|:----------------------|:------------------------------:|:--------:|\n|          KNN          |              K= 50             |  27.65%  |\n|    OVA Naive Bayes    |               -                |    -     |      \n|OVA Logistic Regression|               -                |  19.96%  |\n\nThe reason for such less accuracy is because the dataset contains around 800k rows and ML algorithm work well only till 100k. A strong proof for this can be seen with the shoot of accuracy to 83.97% by reducing the dataset to 20% after randomly shuffing it. So we need to use Neural Network approach.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}