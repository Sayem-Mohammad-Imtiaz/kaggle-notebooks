{"cells":[{"metadata":{},"cell_type":"markdown","source":"**SUPPORT VECTOR MACHINE vs K-NEAREST-NEIGHBOUR\n**\n\n  *In this notebook, my goal is to compare the SVM one vs rest algorithm, to KNN. These types of tasks, related to image recognition, usually can be solved easily with deep learning algorithms such as convolutional neural networks, but I wanted to use these two algorithms, because they can be also be efficient sometimes in these problems and require less computations, as you would not expect with CNN or neural networks in general. The dataset has 27455 training samples with pixel intensity of 28 x 28 handsign images for 24 different classes (different handsign per class) and 7172 testing samples.*\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/sign-language-mnist'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SUPPORT VECTOR MACHINE OvR**\n\n*First we start with Support Vector Machine algorithm, and since the this is a multiclass problem, I chose one vs rest decision function. The script is pretty straight forward, we use all of our training data to estimate the parameters for our SVM model and afterwards validate it with the test dataset. The more data we use the more accurate our model will become. In this case, our classifier manages to predict the test images at an 84% accuracy.*"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n\ndf = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_train.csv')\ndf_test = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_test.csv')\n\nx_train = df.iloc[0:27455, 1:785].values\ny_train = df.iloc[0:27455, 0].values\n\nx_test = df_test.iloc[0:7172, 1:785].values\ny_test = df_test.iloc[0:7172,0].values\n\nlabel_enc = LabelEncoder()\ny_train = label_enc.fit_transform(y_train)\ny_test = label_enc.fit_transform(y_test)\n\nfrom sklearn.svm import SVC\n\nclassifier = SVC(decision_function_shape='ovr')\n\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\n\nacc = accuracy_score(y_test,y_pred)\nf1 = f1_score(y_test,y_pred,average='micro')\ncm = confusion_matrix(y_test,y_pred)\n\nprint(cm)\nprint(f1)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-NEAREST-NEIGHBOUR**\n\n*KNN is considered as an instance based learning or lazy learning. That is why it does not require as much time as other classification methods to be fitted and to predict. In this case our KNN classifier has approximately a 60% accuracy. As for the k number of neighbours I chose 165, since that is the closest odd number to the squareroot of the training dataset size.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n\ndf = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_train.csv')\ndf_test = pd.read_csv('/kaggle/input/sign-language-mnist/sign_mnist_test.csv')\n\nx_train = df.iloc[0:27455, 1:785].values\ny_train = df.iloc[0:27455, 0].values\n\npixel_number = np.arange(0,784,1)\n\nx_test = df_test.iloc[0:7172, 1:785].values\ny_test = df_test.iloc[0:7172,0].values\n\nplt.scatter(x_train[0],pixel_number, s=0.4, c = 'r')\nplt.scatter(x_train[1],pixel_number, s=0.4, c = 'b')\nplt.scatter(x_train[2],pixel_number, s=0.4, c = 'g')\nplt.scatter(x_train[3],pixel_number, s=0.4, c = 'y')\nplt.scatter(x_train[4],pixel_number, s=0.4, c = 'm')\nplt.show()\n\nlabel_enc = LabelEncoder()\ny_train = label_enc.fit_transform(y_train)\ny_test = label_enc.fit_transform(y_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nKNN = KNeighborsClassifier(n_neighbors=165)\nclassifier = KNN.fit(x_train,y_train)\n\ny_pred = classifier.predict(x_test)\nacc = accuracy_score(y_test,y_pred)\nf1 = f1_score(y_test,y_pred,average='micro')\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(f1)\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CONCLUSION**\n\n*In the end we can say that SVM is much more efficient than KNN in predicting images from the given test dataset, but either way, both need to be used in these kinds of tasks, before jumping to deep learning algorithms, since they are much faster, even though most of the time less accurate.*"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}