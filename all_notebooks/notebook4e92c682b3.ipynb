{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"__author__ = 'HaoningWu'\n__StudentID__ = '518030910285'\n\n\"\"\"\nSince it may take a long time to run the whole codes, \nI also store the outputs as comments at the end of each code block.\nIf there exists any question, please contact with me.\nThank you!\n\"\"\"\n\n# DGIM counts the number of 1's in the last k bits in the window\n# According to this problem, we set k exactly equals to windowsize\nimport time\nimport sys\n\nbucket_list = [] # The list for storing buckets, its elements are directory objects\n\nmaxBucket = 10   # the maximum number of similar buckets allowed\nwindowSize = 1000    # size of the window\ncurrentTime = 2000   # current time (start with 0)\nk = windowSize    # the last k bits, in this problem, we just need to set it equal to windowSize\n\nfileName = '../input/coding2/stream_data.txt'\nf = open(fileName, 'r')\ndataset = f.read().split('\\t')    # Load data and preprocess\n\ndef DGIM(data = dataset, window_size = windowSize, current_time = currentTime):\n    start_time = time.time()\n    cnt = 0\n    \n    for i in range(current_time):\n        value = data[i]\n        if value:\n            # Determine if any buckets are due\n            # If the timestamp of the leftmost bucket is equal to the current time minus the window_size, it's due, so delete it\n            if (len(bucket_list) > 0) and (i+1-window_size == bucket_list[0]['timestamp']):\n                del bucket_list[0]\n            # Create new bucket if the new input is 1, otherwise pass it\n            if int(value) == 1:\n                bucket = {'timestamp': i+1, 'cnt': 1}    # Bucket Structure \n                bucket_list.append(bucket)\n                # If there exist maxBucket buckets of the same size, we need to merge them\n                for i in range(len(bucket_list)-1, maxBucket-1, -1):\n                    if bucket_list[i]['cnt'] == bucket_list[i-maxBucket]['cnt']: # With the same size\n                        bucket_list[i-maxBucket]['cnt'] += bucket_list[i-maxBucket+1]['cnt'] # Merge\n                        bucket_list[i-maxBucket]['timestamp'] = bucket_list[i-maxBucket+1]['timestamp']\n                        del bucket_list[i-maxBucket+1] # delete the bucket closer to the right\n    \n    for i in range(len(bucket_list)):\n        cnt += bucket_list[i]['cnt']    # Sum the sizes of all buckets but the last\n    cnt -= int(bucket_list[0]['cnt'] / 2)    # Add half the size of the last bucket\n    total_space = sys.getsizeof(bucket_list)\n    total_time = time.time() - start_time\n    \n    return cnt, total_time, total_space\n\ndgim_cnt, dgim_cnt_time, dgim_cnt_space = DGIM(dataset, windowSize, currentTime)\nprint(\"DGIM Estimated Count: %d, Running Time of DGIM: %f, Space of DGIM: %f, maxBucket: %d, windowSize: %d, currentTimestap: %d\"%(dgim_cnt, dgim_cnt_time, dgim_cnt_space, maxBucket, windowSize, currentTime))\n\n# DGIM Estimated Count: 41, Running Time of DGIM: 0.000305, Space of DGIM: 272.000000, maxBucket: 10, windowSize: 1000, currentTimestap: 100\n# DGIM Estimated Count: 375, Running Time of DGIM: 0.005183, Space of DGIM: 536.000000, maxBucket: 10, windowSize: 1000, currentTimestap: 1000\n# DGIM Estimated Count: 390, Running Time of DGIM: 0.011971, Space of DGIM: 536.000000, maxBucket: 10, windowSize: 1000, currentTimestap: 2000\n# DGIM Estimated Count: 39, Running Time of DGIM: 0.000269, Space of DGIM: 216.000000, maxBucket: 4, windowSize: 1000, currentTimestap: 100\n# DGIM Estimated Count: 359, Running Time of DGIM: 0.003294, Space of DGIM: 288.000000, maxBucket: 4, windowSize: 1000, currentTimestap: 1000\n# DGIM Estimated Count: 374, Running Time of DGIM: 0.008183, Space of DGIM: 368.000000, maxBucket: 4, windowSize: 1000, currentTimestap: 2000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"# Accurately Count the number of 1's in the window\ndef count_acc_num(data = dataset, window_size = windowSize, current_time = currentTime):\n    start_time = time.time()\n    cnt = 0\n    # If the current time is smaller than the windowSize, simply count at the beginning until the current moment\n    # If the current time is larger than the windowSize, the starting position is the current position minus the windowSize\n    start = max(current_time - window_size, 0)\n    for i in range(min(current_time, window_size)):\n        value = data[i + start]\n        if value and int(value) == 1:\n            cnt += 1\n    total_time = time.time() - start_time\n    total_space = sys.getsizeof(data[start: start + min(current_time, window_size)])\n    return cnt, total_time, total_space\n\nacc_cnt, acc_cnt_time, acc_cnt_space = count_acc_num(dataset, windowSize, currentTime)\nprint(\"Accurate Count: %d, Running Time of accurate count: %f, Space of accurate count: %f, maxBucket: %d, windowSize: %d, currentTimestap: %d\"%(acc_cnt, acc_cnt_time, acc_cnt_space, maxBucket, windowSize, currentTime))\nerror = abs(acc_cnt - dgim_cnt)\nerror_rate = error / acc_cnt\ntime_difference = abs(dgim_cnt_time - acc_cnt_time)\nspace_difference = abs(dgim_cnt_space - acc_cnt_space)\nprint(\"Error: %d, Error rate: %f, time_difference: %f, space_difference: %f\"%(error, error_rate, time_difference, space_difference))\n\n\n# Accurate Count: 43, Running Time of accurate count: 0.000038, Space of accurate count: 872.000000, maxBucket: 10, windowSize: 1000, currentTimestap: 100\n# Error: 2, Error rate: 0.046512, time_difference: 0.000267, space_difference: 600.000000\n# Accurate Count: 391, Running Time of accurate count: 0.000357, Space of accurate count: 8072.000000, maxBucket: 10, windowSize: 1000, currentTimestap: 1000\n# Error: 16, Error rate: 0.040921, time_difference: 0.004826, space_difference: 7536.000000\n# Accurate Count: 399, Running Time of accurate count: 0.000455, Space of accurate count: 8072.000000, maxBucket: 10, windowSize: 1000, currentTimestap: 2000\n# Error: 9, Error rate: 0.022556, time_difference: 0.011516, space_difference: 7536.000000\n# Accurate Count: 43, Running Time of accurate count: 0.000038, Space of accurate count: 872.000000, maxBucket: 4, windowSize: 1000, currentTimestap: 100\n# Error: 4, Error rate: 0.093023, time_difference: 0.000231, space_difference: 656.000000\n# Accurate Count: 391, Running Time of accurate count: 0.000360, Space of accurate count: 8072.000000, maxBucket: 4, windowSize: 1000, currentTimestap: 1000\n# Error: 32, Error rate: 0.081841, time_difference: 0.002934, space_difference: 7784.000000\n# Accurate Count: 399, Running Time of accurate count: 0.000377, Space of accurate count: 8072.000000, maxBucket: 4, windowSize: 1000, currentTimestap: 2000\n# Error: 25, Error rate: 0.062657, time_difference: 0.007805, space_difference: 7704.000000\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"# Dataset Preprocessing\nimport csv\nimport numpy as np\nfrom random import shuffle\n\nfileName = '../input/coding2/docs_for_lsh.csv'\ndataset = open(fileName, 'r')\nreader = csv.reader(dataset)\nrows = [row for row in reader]\ndata = []\n\n# I found that only 1~100000 rows of data are useful, so here is a trick: I just use these 100000 rows of data\n# This can be useful for accelerating, while we can also use the full dataset.\n# Besides, only 1~20 columns of data have non-zero elements, so here is another trick: I just use these 20 columns of data\n# We can also use the full dataset, however, in my opinion, it's the same.\n\n#for item in rows[1:]:\nfor item in rows[1:100001]:\n    data.append(item[1:21]) \n    #data.append(item[1:])    # Split with the first column and row (id and description) \n\ndata = np.array(data).T\n# print(data.shape)\n# data.shape = (200, 1000000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GenerateSignature using minHash\nb, r = 15, 5\n# n = b * r: Length of signature (number of distinct minhash functions)\n# b: Number of bands that divide the signature matrix\n\ndef generateSignature(input_matrix):\n    # Generate signature row value for the signature matrix\n    # row number of the original matrix\n    rowSeries = [i for i in range(input_matrix.shape[0])]    # 200 / 20\n    result = [-1 for i in range(input_matrix.shape[1])]    # 1000000 / 100000\n    columnCount = 0\n    # Reorder the row numbers\n    shuffle(rowSeries)\n    for i in range(len(rowSeries)):    # 200 / 20\n        rowIndex = rowSeries.index(i)\n        for j in range(input_matrix.shape[1]):    # 1000000 / 100000\n            if result[j] == -1 and int(input_matrix[rowIndex][j]) != 0:    # Pay attention to converting it into 'int'\n                result[j] = rowIndex\n                columnCount += 1\n        if columnCount == input_matrix.shape[1]:\n            break\n    return result\n\ndef minHash(input_matrix, b, r):\n    # minHash function, generate signature matrix\n    sigMatrix = []\n    n = b * r\n    for i in range(n):\n        sig = generateSignature(input_matrix)\n        sigMatrix.append(sig)\n    return np.array(sigMatrix)\n\nsignatureMatrix = minHash(data, b, r) # shape: (100, 1000000) / (100, 100000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"# Local Sensitive Hash\n# I just take the 30 most similar document ids, which share the most buckets with the query.\nimport hashlib\nimport sklearn\nfrom sklearn.metrics import jaccard_score\n\ndef LSH(sigMatrix, b, r):\n    # Local Sensitive Hash\n    hashBuckets = {}\n    # start and end of band row\n    start, end = 0, r\n    count = 0     # count the number of band level\n\n    while end <= sigMatrix.shape[0]:    # 100\n        count += 1\n        \n        # traverse the column of signature matrix\n        for colNum in range(sigMatrix.shape[1]):\n            hashObject = hashlib.sha256()\n            band = str(sigMatrix[start: start + r, colNum]) + str(count)\n            #band = \"\".join(str(sigMatrix[start:start+r, colNum])) + str(count)\n            hashObject.update(band.encode())    # hash function\n            tag = hashObject.hexdigest()\n            \n            # Update buckets\n            if tag not in hashBuckets:\n                hashBuckets[tag] = [colNum]\n            elif colNum not in hashBuckets[tag]:\n                hashBuckets[tag].append(colNum)\n        start += r\n        end += r\n            \n    return hashBuckets\n\nLSH_result = LSH(signatureMatrix, b, r)\n\n# Search for documents which are similar to the query document\ndef nn_search(dataSet, queryColumn = 0):\n    result = {}\n\n    for key in dataSet:\n        if queryColumn in dataSet[key]:\n            for i in dataSet[key]:\n                if i in result.keys():\n                    result[i] += 1\n                else:\n                    result[i] = 1\n\n    return result\n\nres = nn_search(LSH_result, 0)\nres = sorted(res.items(), key = lambda item:item[1], reverse = True)\n\nprint(\"LSH output length:\", len(res))\noutput = []\n\nfor i in res[1:31]:\n    output.append(i[0])\n    \nprint(\"The 30 outputs of LSH: \", sorted(output))\n\ncheck_list = {}\ncheck_data = data.T\n\nfor i in res:\n    index = i[0]\n    Jaccard_score = sklearn.metrics.jaccard_score(check_data[0], check_data[index], pos_label='1')\n    if Jaccard_score > 0.8:\n        check_list[index] = Jaccard_score\nprint(\"LSH check output length: \", len(check_list) - 1)\nprint(\"Calculate Jaccard_score with LSH output information to check: \", check_list)\nprint(\"Check output: \", sorted(check_list.keys())[1:])\nprint(\"Ground Truth: [1331, 2575, 20854, 23585, 26980, 28910, 32681, 39310, 39784, 40298, 46220, 48131, 52076, 58694, 58852, 62080, 67032, 68730, 69724, 72156, 73681, 81289, 81379, 81480, 84306, 84520, 89825, 89833, 91300, 99370]\")\n\n# output of b = 15, r = 5, n = 75: accuracy = 21 / 30 = 70% \n# [1331, 2110, 2575, 20854, 23585, 28910, 32681, 39784, 42302, 43869, 46220, 48131, 52076, 58694, 58852, 61687, 67032, 68730, 69724, 72078, 72156, 73681, 78531, 79551, 81289, 84520, 89825, 89833, 91300, 96208]\n# output of b = 20, r = 5, n = 100: accuracy = 14 / 30 = 47% \n# [1331, 2575, 14134, 14518, 15637, 20854, 20988, 23585, 28910, 32681, 34797, 40298, 44663, 48131, 58711, 61687, 62080, 66125, 67032, 69028, 70964, 73681, 73687, 78531, 81480, 83204, 89580, 89825, 89833, 99370]\n# output of b = 20, r = 10, n = 200: accuracy = 14 / 30 = 47% \n# [1331, 2575, 14134, 14518, 15637, 20854, 20988, 23585, 28910, 32681, 34797, 40298, 44663, 48131, 58711, 61687, 62080, 66125, 67032, 69028, 70964, 73681, 73687, 78531, 81480, 83204, 89580, 89825, 89833, 99370]\n# output of b = 20, r = 15, n = 300: accuracy = 12 / 30 = 40% \n# [474, 2110, 2397, 6385, 14134, 14518, 28910, 32681, 36005, 37464, 39784, 41720, 44663, 46220, 46923, 50250, 52172, 55780, 58124, 58852, 59443, 62080, 64156, 69724, 72156, 81289, 82105, 89825, 89833, 91300]\n# output of b = 30, r = 10, n = 300: accuracy = 22 / 30 = 73% \n# [474, 1331, 2575, 6429, 20854, 23585, 28910, 32681, 39310, 39784, 41720, 42302, 48131, 52076, 58694, 58852, 62080, 64156, 67032, 69028, 69724, 72078, 81289, 81480, 84520, 87793, 89825, 89833, 91300, 99370]\n# Ground Truth: \n# [1331, 2575, 20854, 23585, 26980, 28910, 32681, 39310, 39784, 40298, 46220, 48131, 52076, 58694, 58852, 62080, 67032, 68730, 69724, 72156, 73681, 81289, 81379, 81480, 84306, 84520, 89825, 89833, 91300, 99370]\n\n\n# The results of LSH are related to probability, so the accuracy may be hard to be 100%, but we can reduce the search dimension to find the most similar documents\n\n# The check results are as follows, it can run very fast because we use LSH to reduce searching dimensions\n# Length: 30\n# {0: 1.0, 89833: 0.9090909090909091, 32681: 0.9090909090909091, 91300: 0.9090909090909091, 62080: 0.8333333333333334, 89825: 0.8333333333333334, 20854: 0.8333333333333334, 58852: 0.8181818181818182, 69724: 0.8181818181818182, 84520: 0.8181818181818182, 99370: 0.8181818181818182, 2575: 0.8181818181818182, 48131: 0.8181818181818182, 67032: 0.8181818181818182, 28910: 0.8333333333333334, 23585: 0.8181818181818182, 1331: 0.8181818181818182, 39310: 0.8333333333333334, 81289: 0.8181818181818182, 39784: 0.8181818181818182, 81480: 0.8333333333333334, 52076: 0.8181818181818182, 58694: 0.8181818181818182, 84306: 0.8333333333333334, 40298: 0.8333333333333334, 72156: 0.8181818181818182, 73681: 0.8181818181818182, 68730: 0.8181818181818182, 81379: 0.8333333333333334, 46220: 0.8181818181818182, 26980: 0.8181818181818182}\n# We still take b = 15, r = 5, n = 75\n# Check outputs:\n# [1331, 2575, 20854, 23585, 26980, 28910, 32681, 39310, 39784, 40298, 46220, 48131, 52076, 58694, 58852, 62080, 67032, 68730, 69724, 72156, 73681, 81289, 81379, 81480, 84306, 84520, 89825, 89833, 91300, 99370]\n# Ground Truth: \n# [1331, 2575, 20854, 23585, 26980, 28910, 32681, 39310, 39784, 40298, 46220, 48131, 52076, 58694, 58852, 62080, 67032, 68730, 69724, 72156, 73681, 81289, 81379, 81480, 84306, 84520, 89825, 89833, 91300, 99370]\n# Accuracy is 100% now","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for answers\nimport sklearn\nfrom sklearn.metrics import jaccard_score\ntest_data = data.T\n# test_data.shape = (1000000, 200)\n\ncandidates = {}\n# for i in range(1, 1000000):\nfor i in range(1, 100000):\n    Jaccard_score = sklearn.metrics.jaccard_score(test_data[0], test_data[i], pos_label='1')    # Pay attention to the parameter 'pos_label'\n    if Jaccard_score > 0.8:\n        candidates[i] = Jaccard_score\nprint(\"Ground Truth and corresponding Jaccard_score: \", candidates)\nprint(\"Ground Truth: \", sorted(candidates.keys()))\n# {1331: 0.8181818181818182, 2575: 0.8181818181818182, 20854: 0.8333333333333334, 23585: 0.8181818181818182, 26980: 0.8181818181818182, 28910: 0.8333333333333334, 32681: 0.9090909090909091, 39310: 0.8333333333333334, 39784: 0.8181818181818182, 40298: 0.8333333333333334, 46220: 0.8181818181818182, 48131: 0.8181818181818182, 52076: 0.8181818181818182, 58694: 0.8181818181818182, 58852: 0.8181818181818182, 62080: 0.8333333333333334, 67032: 0.8181818181818182, 68730: 0.8181818181818182, 69724: 0.8181818181818182, 72156: 0.8181818181818182, 73681: 0.8181818181818182, 81289: 0.8181818181818182, 81379: 0.8333333333333334, 81480: 0.8333333333333334, 84306: 0.8333333333333334, 84520: 0.8181818181818182, 89825: 0.8333333333333334, 89833: 0.9090909090909091, 91300: 0.9090909090909091, 99370: 0.8181818181818182}","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}