{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\n\nNeural Collaborative Filtering (NCF) is one of the recommendation system frameworks, based on neural networks, proposed by He, et. al. (2017). According to them a neural network can develop a model by learning item user interactions as a key factor of a collaboritive filtering from implicit feedback.\n\nOur first notebook [(hakanerdem)](https://www.kaggle.com/hakanerdem/recommender-system-with-embedding-layers) was about recommender systems for cross selling opportunities on the domain of retail marketing. We try to implement the same issue with NCF at this time. Python code mostly developed thanks to beautiful notebooks like:\n\n[fuzzywizard](https://www.kaggle.com/fuzzywizard/rec-sys-collaborative-filtering-dl-techniques#4-Matrix-Factorization-using-Deep-Learning-(Keras))\n\n[rajmehra03](https://www.kaggle.com/rajmehra03/cf-based-recsys-by-low-rank-matrix-factorization)\n\nData preparation sections are the same as the firts notebook mentioned above. Please see for detailed information about NCF:\n\nHe, X., Liao, L., Zhang, H., Nie, L., Hu, X.,Chua, T.,  (2017), Neural Collaborative Filtering. WWW'17: Proceedings of the 26th International Conference on World Wide Web 173–182 DOI: http://dx.doi.org/10.1145/3038912.3052569\n\nDataset choosen: Online Retail II. For detailed information please visit:\n\nhttps://www.kaggle.com/mashlyn/online-retail-ii-uci, [UCI Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T11:26:35.655615Z","iopub.execute_input":"2021-07-29T11:26:35.655976Z","iopub.status.idle":"2021-07-29T11:26:42.022362Z","shell.execute_reply.started":"2021-07-29T11:26:35.655896Z","shell.execute_reply":"2021-07-29T11:26:42.02133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data & Preparation","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/online-retail-ii-uci/online_retail_II.csv\",\n                   parse_dates=[\"InvoiceDate\"],\n                   dtype={\"Customer ID\":\"object\"})","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:26:49.718582Z","iopub.execute_input":"2021-07-29T11:26:49.719019Z","iopub.status.idle":"2021-07-29T11:26:52.982755Z","shell.execute_reply.started":"2021-07-29T11:26:49.718977Z","shell.execute_reply":"2021-07-29T11:26:52.981695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:26:58.88017Z","iopub.execute_input":"2021-07-29T11:26:58.880612Z","iopub.status.idle":"2021-07-29T11:26:58.993835Z","shell.execute_reply.started":"2021-07-29T11:26:58.880577Z","shell.execute_reply":"2021-07-29T11:26:58.992847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Droping rows with missing values and irrelevant labels","metadata":{}},{"cell_type":"code","source":"df = df.dropna()\ndf = df.drop(df[df[\"Quantity\"]<0].index)\ndf = df.drop(df[df[\"StockCode\"].str.contains(\"TEST\")].index)\ndf = df.drop(df[df[\"StockCode\"]==\"POST\"].index)\n\ndf = df.sort_values(\"InvoiceDate\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:27:10.48823Z","iopub.execute_input":"2021-07-29T11:27:10.488673Z","iopub.status.idle":"2021-07-29T11:27:12.155346Z","shell.execute_reply.started":"2021-07-29T11:27:10.488637Z","shell.execute_reply":"2021-07-29T11:27:12.154396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common Functions\n\nFirst function can be used to obtain lists having unique elements. Second, for generating product purchase sequences and a target sequence having *n_target* length occuring after a sequence of product purchased. The last one for generating a negative sample.","metadata":{}},{"cell_type":"code","source":"def unique(list1):\n    list_set = set(list1)\n    unique_list = (list(list_set))\n    return unique_list\n\ndef generate_sequence(serie, n_target):\n    input_sequence = []\n    output_sequence = []\n    for x in serie:\n        x = unique(x)\n        if len(x)>n_target:\n            input_sequence.append(x[:-n_target])\n            output_sequence.append(x[-n_target:])\n    return input_sequence, output_sequence\n\ndef agg(x, corp, sample_size=1):\n    diff = np.setdiff1d(corp, list(x))\n    ind = np.random.permutation(len(diff))\n    return diff[ind[:int(sample_size*len(x))]]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:28:06.576175Z","iopub.execute_input":"2021-07-29T11:28:06.576626Z","iopub.status.idle":"2021-07-29T11:28:06.596459Z","shell.execute_reply.started":"2021-07-29T11:28:06.576587Z","shell.execute_reply":"2021-07-29T11:28:06.595317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generating customers' purchase sequences of distinct products.","metadata":{}},{"cell_type":"code","source":"by_customer = df.groupby(\"Customer ID\", as_index=False).agg(\n    {\"StockCode\": [lambda x: list(x)]}\n)\nsequential_df = by_customer[\"StockCode\"].rename(\n    columns={\"<lambda>\":\"purchase_sequence\"}\n)\nsequential_df[\"CustomerID\"] = by_customer[\"Customer ID\"]\nsequential_df[\"product_count\"] = sequential_df[\"purchase_sequence\"].apply(\n    lambda x: len(unique(list(x)))\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:28:20.747831Z","iopub.execute_input":"2021-07-29T11:28:20.748243Z","iopub.status.idle":"2021-07-29T11:28:21.382885Z","shell.execute_reply.started":"2021-07-29T11:28:20.748208Z","shell.execute_reply":"2021-07-29T11:28:21.381936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Coosing some hyperparameter values arbitrarily. We prefer to choose a frequency number to get rid of sparsity of data. That kind of sparsity means that some customers' purchasing behaviour includes only one or two different purchasing within plenty of different products. This issue also know as *cold start* problem. For detailed information please see:\n\nLü, L., Medo, M., Yeung, C. H., Zhang, Y., Zhang, Z., Zhou, T., (2012), Recommender systems, Physics Reports 519, 1-49, DOI: http://dx.doi.org/10.1016/j.physrep.2012.02.006","metadata":{}},{"cell_type":"code","source":"n_target = 1\nn_frequency = 3\ncorp = sequential_df.explode(\"purchase_sequence\")[\"purchase_sequence\"].unique()\nfrequent_df = sequential_df[(sequential_df[\"product_count\"]>n_frequency)]\n\ninput_seq, output_seq = generate_sequence(\n    frequent_df[\"purchase_sequence\"],\n    n_target\n    )\n\nfrequent_df[\"input_sequence\"] = input_seq\nfrequent_df[\"output_sequence\"] = output_seq\nfrequent_df = frequent_df[[\"CustomerID\", \"input_sequence\", \"output_sequence\"]]\nfrequent_df = frequent_df.explode(\"input_sequence\")\nfrequent_df[\"purchase\"] = 1\nfrequent_df = frequent_df.set_index(\"CustomerID\", drop=True)\nfrequent_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:28:43.713125Z","iopub.execute_input":"2021-07-29T12:28:43.713651Z","iopub.status.idle":"2021-07-29T12:28:44.519026Z","shell.execute_reply.started":"2021-07-29T12:28:43.713606Z","shell.execute_reply":"2021-07-29T12:28:44.517397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Negative Sampling\n\nSince all instances prepared so far represent positive-only feedback, we try to supply some negative information to the model. Instead of providing all non-purchased products, some negative instances are chosen randomly from products not purchased for a particular customer.\n\n> sample_size=1\n\nmeans there is 1 non-purchased product to be selected randomly.","metadata":{}},{"cell_type":"code","source":"new_df = frequent_df.reset_index().groupby(\"CustomerID\").agg({\"input_sequence\": (lambda x: list(x))})\nnew_df[\"agg\"] = new_df[\"input_sequence\"].apply(lambda y: agg(y, corp, 1))\nndf = new_df.explode(\"agg\")[[\"agg\"]]\nndf[\"purchase\"] = 0\nndf = ndf.rename(columns={\"agg\":\"input_sequence\"})\n\npdf = frequent_df[[\"input_sequence\", \"purchase\"]]\n\nsample_df = pdf.append(ndf)\nsample_df = sample_df.reset_index()\nsample_df = sample_df.sort_values(\"CustomerID\", ignore_index=True)\n\ndisplay(sample_df.info())\ndisplay(sample_df.head(50))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:38:25.656551Z","iopub.execute_input":"2021-07-29T11:38:25.657008Z","iopub.status.idle":"2021-07-29T11:40:54.241859Z","shell.execute_reply.started":"2021-07-29T11:38:25.656965Z","shell.execute_reply":"2021-07-29T11:40:54.240804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding & Splitting\n\nAs a last step we try to encode user and product features. Method taken from [keras.io](https://keras.io/examples/structured_data/collaborative_filtering_movielens/) examples. We take the data as train & validation, but the better practice is holding out some samples in advance as test data","metadata":{}},{"cell_type":"code","source":"cust_ids = sample_df[\"CustomerID\"].unique().tolist()\ncust2cust_encoded = {x: i for i, x in enumerate(cust_ids)}\ncust_encoded2cust = {i: x for i, x in enumerate(cust_ids)}\nprod_ids = corp\nprod2prod_encoded = {x: i for i, x in enumerate(prod_ids)}\nprod_encoded2prod = {i: x for i, x in enumerate(prod_ids)}\nsample_df[\"cust\"] = sample_df[\"CustomerID\"].map(cust2cust_encoded)\nsample_df[\"prod\"] = sample_df[\"input_sequence\"].map(prod2prod_encoded)\n\nnum_custs = len(cust2cust_encoded)\nnum_prods = len(prod2prod_encoded)\nsample_df[\"purchase\"] = sample_df[\"purchase\"].values.astype(np.float32)\n\nprint(\n    \"Number of Customers: {}, Number of Products: {}, Purchase: {}, Not Purchase: {}\".format(\n        num_custs, num_prods, 1, 0\n    )\n)\n\nsample_df = sample_df.sample(frac=1, random_state=52)\nX = sample_df[[\"cust\", \"prod\"]].values\ny = sample_df[\"purchase\"].values\n\ntrain_indices = int(0.8 * sample_df.shape[0])\nX_train, X_val, y_train, y_val = (X[:train_indices],\n                                  X[train_indices:],\n                                  y[:train_indices],\n                                  y[train_indices:])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:47:53.524805Z","iopub.execute_input":"2021-07-29T11:47:53.525237Z","iopub.status.idle":"2021-07-29T11:47:54.100727Z","shell.execute_reply.started":"2021-07-29T11:47:53.525192Z","shell.execute_reply":"2021-07-29T11:47:54.099522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base Model","metadata":{}},{"cell_type":"code","source":"num_custs = sample_df[\"cust\"].nunique()\nnum_prods = sample_df[\"prod\"].nunique()\nhidden_units = (128,64)\nprod_embedding_size = 8\nuser_embedding_size = 8\n\nuser_id_input = keras.Input(shape=(1,), name=\"user_id\")\nprod_id_input = keras.Input(shape=(1,), name=\"prod_id\")\nuser_embedded = layers.Embedding(num_custs,\n                                 user_embedding_size, \n                                 input_length=1,\n                                 embeddings_regularizer=keras.regularizers.l2(1e-7),\n                                 name=\"user_embedding\")(user_id_input)\nprod_embedded = layers.Embedding(num_prods,\n                                 prod_embedding_size,\n                                 input_length=1,\n                                 embeddings_regularizer=keras.regularizers.l2(1e-6),\n                                 name=\"prod_embedding\")(prod_id_input)\n\nconcatenated = layers.Concatenate(name=\"concat\")([user_embedded, prod_embedded])\nout = layers.Flatten(name=\"flat\")(concatenated)\n\nfor n_hidden in hidden_units:\n    out = layers.Dense(n_hidden, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(out)\n    out = layers.Dropout(0.4)(out)\n\nout = layers.Dense(1, activation=\"sigmoid\", name=\"prediction\")(out)\n\nneural_model = keras.Model(inputs = [user_id_input, prod_id_input],\n                           outputs = out, name=\"neural_model\")\nneural_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:48:04.322879Z","iopub.execute_input":"2021-07-29T11:48:04.32331Z","iopub.status.idle":"2021-07-29T11:48:04.430146Z","shell.execute_reply.started":"2021-07-29T11:48:04.323261Z","shell.execute_reply":"2021-07-29T11:48:04.428466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neural_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n                     optimizer=keras.optimizers.Adam(learning_rate=0.001))\n\nes = keras.callbacks.EarlyStopping(monitor='val_loss',\n                                   mode='min',\n                                   verbose=1,\n                                   patience=5)\n\nhistory = neural_model.fit(\n    [sample_df[\"cust\"].values, sample_df[\"prod\"].values],\n    sample_df.purchase.values,\n    batch_size=256,\n    epochs=50,\n    callbacks=[es],\n    verbose=1,\n    validation_split=.1\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:48:32.917524Z","iopub.execute_input":"2021-07-29T11:48:32.917964Z","iopub.status.idle":"2021-07-29T11:53:46.491252Z","shell.execute_reply.started":"2021-07-29T11:48:32.917927Z","shell.execute_reply":"2021-07-29T11:53:46.48994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"val\"], loc=\"upper right\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:57:25.337817Z","iopub.execute_input":"2021-07-29T11:57:25.338226Z","iopub.status.idle":"2021-07-29T11:57:25.559781Z","shell.execute_reply.started":"2021-07-29T11:57:25.338194Z","shell.execute_reply":"2021-07-29T11:57:25.558942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n\nWe try to measure the model performance by providing candidate products to the model and evaluating the outputs. Same protocol with [hakanerdem](https://www.kaggle.com/hakanerdem/recommender-system-with-embedding-layers) is conducted.","metadata":{}},{"cell_type":"code","source":"cust_id = sample_df[\"CustomerID\"].sample(1).iloc[0]\ncust_encoder = cust2cust_encoded.get(cust_id)\npurchased = frequent_df[(frequent_df.index==cust_id) & (frequent_df[\"purchase\"]==1)]\n\ncandidates = frequent_df[~frequent_df[\"input_sequence\"].isin(purchased[\"input_sequence\"].values)][\"input_sequence\"][:49]\ncandidates = set(candidates).intersection(set(prod2prod_encoded.keys()))\ncandidates = candidates.union(set(frequent_df[frequent_df.index==cust_id][\"output_sequence\"].values[0]))\ncandidates = [[prod2prod_encoded.get(x)] for x in list(candidates)]\n\nvals = neural_model.predict([np.array([cust_encoder] * len(candidates)), np.array(candidates)]).flatten()\ntop_ratings_indices = vals.argsort()[-20:][::-1]\nrecommended_prod_ids = [prod_encoded2prod.get(candidates[x][0]) for x in top_ratings_indices]\n\nprint(\"Showing recommendations for user: {}\".format(cust_id))\nprint(\"====\" * 12)\nprint(\"Products purchased from customer\")\nprint(\"----\" * 8)\nprint(frequent_df[frequent_df.index==cust_id])\n\nprint(\"----\" * 8)\nprint(\"Top 20 product recommendations\")\nprint(\"----\" * 8)\nprint(recommended_prod_ids)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:01:31.358865Z","iopub.execute_input":"2021-07-29T12:01:31.359385Z","iopub.status.idle":"2021-07-29T12:01:31.929401Z","shell.execute_reply.started":"2021-07-29T12:01:31.359336Z","shell.execute_reply":"2021-07-29T12:01:31.927599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = 0\nsize = 100\n\nfor s in range(size):\n\n    cust_id = sample_df[\"CustomerID\"].values[s]\n    cust_encoder = cust2cust_encoded.get(cust_id)\n    purchased = frequent_df[(frequent_df.index==cust_id) & (frequent_df[\"purchase\"]==1)]\n\n    candidates = frequent_df[~frequent_df[\"input_sequence\"].isin(purchased[\"input_sequence\"].values)][\"input_sequence\"][:49]\n    candidates = set(candidates).intersection(set(prod2prod_encoded.keys()))\n    candidates = candidates.union(set(frequent_df[frequent_df.index==cust_id][\"output_sequence\"].values[0]))\n    candidates = [[prod2prod_encoded.get(x)] for x in list(candidates)]\n\n    vals = neural_model.predict([np.array([cust_encoder] * len(candidates)), np.array(candidates)]).flatten()\n    top_ratings_indices = vals.argsort()[-20:][::-1]\n    recommended_prod_ids = [prod_encoded2prod.get(candidates[x][0]) for x in top_ratings_indices]\n    target_prod_ids = frequent_df.loc[(frequent_df.index==cust_id), \"output_sequence\"].values[0]\n\n    if len(np.setdiff1d(target_prod_ids, recommended_prod_ids)) < n_target:\n        counter = counter + 1\n\nprint(\"recall@20 for first\", size, \" input: \", counter/size)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:03:14.247881Z","iopub.execute_input":"2021-07-29T12:03:14.248528Z","iopub.status.idle":"2021-07-29T12:03:54.806007Z","shell.execute_reply.started":"2021-07-29T12:03:14.248489Z","shell.execute_reply":"2021-07-29T12:03:54.804364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Critics\n\nCritics are more important than upvotes. Of course, base model have plenty of faulties, please criticise. On the other hand, it is a better practice to compare with different notebooks implemented same dataset, like my first study with embedding layers.\n\nSome hyperparameters which are should be tuned.\n\n* Number and unit numbers of *hidden_units*\n* *prod_embedding_size* and *user_embedding_size*\n* regularizers, learning rate, activation functions, batch size, dropout rates\n* *n_frequency* number of frequent products\n* *sample_size* number of negative samples corresponding to a positive sample\n\nThanks in advance..","metadata":{}}]}