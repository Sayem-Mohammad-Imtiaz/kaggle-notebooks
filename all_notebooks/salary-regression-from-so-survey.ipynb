{"cells":[{"metadata":{"_uuid":"2aa200a1bdf8150eedb9d26eed16fb45192700bd"},"cell_type":"markdown","source":"# Estimating Salary from Data in the Stack Overflow Survey\n### Using Support Vector Regression to calculate respondents' salaries based on Survey responses"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"33a872cb0b9692da6de47a3d55e2b4e4099777bb"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport csv\nimport datetime\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c2b39dfeb679f6b0429021aac84995e377889cf"},"cell_type":"markdown","source":"We load the data and take a look at it."},{"metadata":{"trusted":true,"_uuid":"d9b6bf05144b33f518fdc0c2b4ddfebe665ea35c"},"cell_type":"code","source":"df = pd.read_csv('../input/survey_results_public.csv', low_memory=False)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"97466cc8a3c8e97ff764d8afe47f0dbecbb6a8ea"},"cell_type":"code","source":"df.index = df['Respondent']\ndel(df['Respondent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e834dfd89e5d22803afdd80b23927172418fbb47"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70f46caedce8a8dd020ea6d4d2f7d0b211a27d8f"},"cell_type":"markdown","source":"## Numerical v Categorical Data\nBeing able to quickly distinguish which columns are numerical and which are numerical is so useful it’s bound to be part of the `pandas` package soon. But it's straightforward to do by hand."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dbceed610de96e66c38acdeed0886be4d2d9d07b"},"cell_type":"code","source":"numerical = []\ntext = []\nfor c in df.columns:\n    if df[c].dtype == 'float64':\n        numerical.append(c)\n    elif df[c].dtype == 'int64':\n        numerical.append(c)\n    else:\n        text.append(c)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18096ab26f1b5aba8cb210c7175e260b34081f18"},"cell_type":"markdown","source":"## Reducing the Number of Columns\nWe don’t need all 129 columns of data to figure out what’s going with pay scales. All the multiple choice columns can go, as can the personal ones. There’s no way to say these are the only possible columns to select, but these surviving columns are the results of a combination of best guess and trial-and-error."},{"metadata":{"trusted":true,"_uuid":"72f8cab59583489c93d6d48748592048a929274e"},"cell_type":"code","source":"shorter_columns = ['ConvertedSalary',\n                    'Hobby',\n                     'OpenSource',\n                     'Country',\n                     'Employment',\n                     'FormalEducation',\n                     'UndergradMajor',\n                     'CompanySize',\n                     'DevType',\n                     'YearsCoding',\n                     'YearsCodingProf',\n                     'DatabaseWorkedWith',\n                     'PlatformWorkedWith',\n                     'FrameworkWorkedWith',\n                     'OperatingSystem',\n                     'Age']\n\ndf = df[shorter_columns]\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c6ba3e3175ba0835d6df944a50be71c1dae3ab8"},"cell_type":"markdown","source":"Data where the salary values are `np.nan` are of no use to us. Away they go."},{"metadata":{"trusted":true,"_uuid":"4ee40f11264f7c34427d84dded379b685cb326df"},"cell_type":"code","source":"df = df[df.ConvertedSalary > 0]\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10159d5151cc1bf7b660ebdf33ba1bcd3e9de1f7"},"cell_type":"markdown","source":"## Countries"},{"metadata":{"trusted":true,"_uuid":"0dbb5b3f41488432ffd622f9ffc8caf227edcd1c"},"cell_type":"code","source":"df.Country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ef1d8e73220c8fc227f9d21efb6e05e6f6f2b5a"},"cell_type":"markdown","source":"## Reducing the Number of Countries\nThe countries data is, unsurprisingly, dominated by the USA. Therefore, we’re going to create an `Others` category to mop up the lower tail of the distribution and keep things tidy.\n\nWe’ll do this by writing a function, `shorten_categories()`, that takes the series generated by calling `.value_counts()` on a categorical series, and a cut-off point. If the values of a category are above or equal to the cut-off, that category maps to itself in the dictionary that the function will return. Otherwise, the category maps to `Other`.\n\nWe then use this function to create a shorter country column, and we'll then call `.value_counts()/df.shape[0]` on it to see how it works out proportionally."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27bcb576bc1adbf6552c3e6ac94b8c140abdfb8d"},"cell_type":"code","source":"def shorten_categories(categories, cutoff):\n    categorical_map = {}\n    for i in range(len(categories)):\n        if categories.values[i] >= cutoff:\n            categorical_map[categories.index[i]] = categories.index[i]\n        else:\n            categorical_map[categories.index[i]] = 'Other'\n    return categorical_map","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"9b33a7ba571e4db4f399433a408960eeae61980b"},"cell_type":"code","source":"country_map = shorten_categories(df.Country.value_counts(), 400)\ndf['Country_Shorter'] = df.Country.map(country_map)\ndf.Country_Shorter.value_counts()/df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01c9916d3cd17a45bbe6626b871842fd539a4a8e"},"cell_type":"markdown","source":"## Shortening the Formal Education Categories\nSome of the Formal Education categories run a little long for graphing purposes. \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\" will be bigger than the graph itself. Therefore, we'll shorten these categories by mapping a dictionary."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8c11f9858c3a844a523ec6e98ee559f77b6a9d58"},"cell_type":"code","source":"education_dict = {\"Bachelor’s degree (BA, BS, B.Eng., etc.)\": \"Batchelor's\",\n                    \"Some college/university study without earning a degree\": \"Some college\",\n                    \"Master’s degree (MA, MS, M.Eng., MBA, etc.)\": \"Masters\",\n                    \"Associate degree\": \"Associate Degree\",\n                    \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\": \"High School\",\n                    \"Professional degree (JD, MD, etc.)\": \"Professional\",\n                    \"Other doctoral degree (Ph.D, Ed.D., etc.)\": \"Doctoral\",\n                    \"nan\": \"nan\",\n                    \"Primary/elementary school\": \"Elementary\",\n                    \"I never completed any formal education\": \"None\"}\ndf['Education'] = df.FormalEducation.map(education_dict)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7e5bb79c828f20a1287043d67c3c393b12fe2cd"},"cell_type":"markdown","source":"### Examining the Data\nWe can now compare individual categories against salary, again to get a better sense of them.\n\n#### Converted Salary v Formal Education"},{"metadata":{"trusted":true,"_uuid":"0a73cc925d515d2eadf86f06fa5d28087424645b"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\ndf.boxplot('ConvertedSalary', 'Education', ax=ax)\nplt.suptitle('Salary v Formal Education')\nplt.title('')\nplt.ylabel('Salary ($)')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d6f0bf530b64b4bede986b4be158839e153e07f"},"cell_type":"markdown","source":"#### Salary v Years of Professional Experience"},{"metadata":{"trusted":true,"_uuid":"efac1e6f9ea3961da54032827066b7316a8a62d0"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\ndf.boxplot('ConvertedSalary', 'YearsCodingProf', ax=ax)\nplt.suptitle('Salary (US$) v Years Coding Professionally')\nplt.title('')\nplt.ylabel('Salary')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a81b7610cec7b3628983499a1a1bc3ece149b229"},"cell_type":"markdown","source":"There are quite a few outliers that crush the main detail of our graph. We'll set a salary cutoff at $250,000 and see what things are like for regular folks, to use one of President Obama's favorite phrases. Regular folks don't generally pull down two mill pa on 0-2 years' experience."},{"metadata":{"trusted":true,"_uuid":"bedf997dc05af0d7f039353fceb4027bc12d0d8c"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\ndf[df.ConvertedSalary <=250000].boxplot('ConvertedSalary', 'YearsCodingProf', ax=ax)\nplt.suptitle('Salary (US$) v Years Coding Professionally, outliers removed')\nplt.title('')\nplt.ylabel('Salary')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b518578cf4014e193875498233dadfdfd13b6ecb"},"cell_type":"markdown","source":"#### Salary v Country"},{"metadata":{"trusted":true,"_uuid":"a1c0c94bb32bd451b0bb5200a5fc3f9e9b735559"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\ndf.boxplot('ConvertedSalary', 'Country_Shorter', ax=ax)\nplt.suptitle('Salary (US$) v Country')\nplt.title('')\nplt.ylabel('Salary')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e04d8c7df88efad17c97aa1e38863d8205f6ee1d"},"cell_type":"markdown","source":"Again, we'll impose a $250k ceiling to get a better idea of the general data."},{"metadata":{"trusted":true,"_uuid":"a708b557bc766f629b4f6cbfd1b2715b19e79b40"},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(12, 7))\ndf[df.ConvertedSalary <= 250000].boxplot('ConvertedSalary', 'Country_Shorter', ax=ax)\nplt.suptitle('Salary (US$) v Country, Outliers Removed')\nplt.title('')\nplt.ylabel('Salary')\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b26bff05c157848fb6fff08831161faacbad4d85"},"cell_type":"markdown","source":"And that's instructive. Being a developer means you've a pretty good chance of good pay in the USA, UK, Israel and Australia. Things aren't so good in Ukraine, India and Russia. There are also some pretty low-paid developer jobs in Sweden.\n\n## Can we Predict Salary?\nIt's unfortunate that we don't have better language data. The language category allows for a number of different languages and, while it's trivial to separate them, what we can't do is figure out a reliable way to weigh the languages against each other in terms of general work as it's reasonable to understand it.\n\nConsider, for instance, CSS. With the greatest of respect, you could argue that CSS isn't a language at all, but if you're a front-end developer, it's pretty important to what you do. But even so, if you can only write CSS you won't get a job. So it would have a small weighing even for someone who uses it every day, and exponentially smaller again for someone who just threw in the list, like emergency snacks into a shopping basket. We have no way of figuring out the weight of the languages relative to each other for a particular respondent and we are far better to just leave them out than to guess and deliver a misleading result.\n\nThose caveats noted, we'll proceed by loading some modules from `sklearn` and starting getting our data ready for analysis."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"926415448ec9ec9b57fe3379c9be9cca66f04ad3"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f89286e18152d3e23ba651de50ea498d8215536a"},"cell_type":"markdown","source":"#### Dropping Categories from the `Country` Field and Capping the Salary\nBecause this script could take well over an hour to run over the full data set, we have to cut the data. We can cut it at random, by sampling the data. Or we can cut it methodically, with a method to our madness.\n\nI've chosen the methodical method. Over 40% of our data is respresented by two countries, USA and Other. Our data will be more interesting if we drop these two countries, and only look at countries other than the big beast and the collection of smaller beasties - again, with all due respect to big and little beasties.\n\nSecondly, we'll also cap the salary at $250,000 to give ourselves some sort of fighting chance of getting this right. The spectacularly high salaries just aren't credible. We'll then we create `train` and `test` dataframes in the usual way, with `sklearn.model_selection.train_test_split`."},{"metadata":{"trusted":true,"_uuid":"7dd2061e62f6e367ce77bdf5ef54dbb86b0bd8b7"},"cell_type":"code","source":"df2 = df.copy()\ndf2 = df2[(df2.ConvertedSalary <= 250000) & (df2.Country_Shorter != 'Other') & (df2.Country_Shorter != 'United States')]\ndel(df2['Country_Shorter'])\ndf2.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a77d504cfa78ee6738e86b7ea463da1ae1a030fe"},"cell_type":"markdown","source":"## Solely Categorical Data\nWe find an unusual case here in that our data is entirely categorial – there are no numerical data. This is a chance occurrence, but that’s OK. If anything, it makes our job easier.\n\nNormally, we create another data frame that one-hot encodes all the categorical data and merge that back with the original. However, in this case, no merging is necessary as the dummy data covers the entirety of the data.\n\nEven though the dummy data doesn’t have to be merged with anything, we’ll change its name to `features` nonetheless. We don’t need the original `features` data frame, and it’s easier to keep up with these operations when the names are consistent."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"23cb78aa8022754d289a4613eaf2494a243b1d9b"},"cell_type":"code","source":"labels = df2['ConvertedSalary']\nfeatures = df2.drop('ConvertedSalary', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84e4e773bf4874411061a9a425d144607f1a2e0f"},"cell_type":"code","source":"dummies = pd.get_dummies(features)\nfeatures = dummies\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d079937abbc0fe92635ce34129200b27d4b8aaea"},"cell_type":"code","source":"train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53ed86d6c234e8d123bfaf3419728a1fe60f289d"},"cell_type":"markdown","source":"A sanity check is helpful every now and again - let's make sure we're getting what we ought to be getting."},{"metadata":{"trusted":true,"_uuid":"72b7006ca428f7c17f814cd533a42183efe11aeb"},"cell_type":"code","source":"for i in [train_features, test_features, train_labels, test_labels]:\n    print(len(i), type(i))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26d353ba21c148813fec77af2a34d5795e369e81"},"cell_type":"markdown","source":"## Finding the Best Setting for our Support Vector Regressor\nThe support-vector regression model in `sklearn` has adjustable parameters. Different kernels can be used, there are different degrees against which we can balance bias against variance, and there are other parameters too. Rather than work these out piece-by-piece, we can use the `GridSearchCV` module in `sklearn` to automate the process for us. It creates a `GridSearchCV` object which has a `best_estimator_` attribute, and it is this that we shall use as our regressor.\n\nAs it happens, `sklearn` seems to be phasing out its `GridSearchCV` module in favor of CV (cross-validation) extensions of the different models, like Linear Regression, Random Trees, and the rest. However, the SVRCV module doesn’t exist yet, so we’re going with the tried and the tested."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5e6b05b042c591773d0eeac0f3d89c5e3bb36825"},"cell_type":"code","source":"param_grid = [{'kernel':('linear', 'rbf'), 'C':[1, 10]}]\nregressor = SVR()\ngridsearch = GridSearchCV(regressor, param_grid, scoring='neg_mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f6059f18dfa5e25235506b952a4d70f0752ab18"},"cell_type":"markdown","source":"### Sparse Matrix\nAgain, our good luck in having one-hot encoded data means we can convert them to a sparse matrix. Using a sparse matrix makes the data processing exponentially shorter in time."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c670136040d0b99541d2c39887ae1e0a78f4577"},"cell_type":"code","source":"from scipy.sparse import csr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"67fe00e34be93e8f293b52bcdf9a50c2c1d5ccfa"},"cell_type":"code","source":"train_regressor_ready_data = csr_matrix(train_features.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"757106d30aa39f4dc0a29ccad3a12be3bb0a0316","scrolled":true},"cell_type":"code","source":"gridsearch.fit(train_regressor_ready_data, train_labels.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a53ea1498804120d16cd625b434c3609fc8b5b09"},"cell_type":"code","source":"regressor = gridsearch.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eea3a4ce6bbff1051104168685f3b82f4ffaf3bb"},"cell_type":"markdown","source":"## Checking for Accuracy\nWe’ll use RMSE, the root-mean-squared error, to see how accurate our regressor is. We’ll then create a database that contains the correct salary data, the predicted data, the country data and the formal education data. We’ll group the data by country and formal education, and see how we’re doing in each category with scatter plots, as we map observed salaries against predicted salaries."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"05ce4ea4726100aa4e2ef53ec87628fb0cd80126"},"cell_type":"code","source":"train_predictions = regressor.predict(train_regressor_ready_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"985097eb2929bc262a3575fbf66a447b9ec6d0e0"},"cell_type":"code","source":"rootMeanSquaredError_train = np.sqrt(mean_squared_error(train_labels, train_predictions))\nprint(\"${:,.02f}\".format(rootMeanSquaredError_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b9283fb8ee4623c38c5a8e48f358d692d9c6b39"},"cell_type":"markdown","source":"## Further Testing Our Model\nWe'll test our model further by running it against the test data. Our aim is for it to have about the same RMSE value, as per this comment on, of all places, Stack Exchange: https://stats.stackexchange.com/a/288809/190839. So we're going to prepare our `test_features` data as we did the `train_features`, predict some values, and measure the root-mean-squared error between the predicted data and the `test_labels` data, hoping to arrive in or around the RMSE value we got with the training data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f0d101a3b4e7a38945017505bd62365fe13f7d2e"},"cell_type":"code","source":"test_regressor_ready_data = csr_matrix(test_features.values)\ntest_predictions = regressor.predict(test_regressor_ready_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a507d26757ecdd45c32c23c874718f2b5d47662c"},"cell_type":"code","source":"rootMeanSquaredError_test = np.sqrt(mean_squared_error(test_labels, test_predictions))\nprint(\"${:,.02f}\".format(rootMeanSquaredError_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d99732a31d17b0d69ffa1f812bd111de0c5cbf80"},"cell_type":"markdown","source":"## Level of Fit"},{"metadata":{"trusted":true,"_uuid":"cccee547831d9fe6d889ad1a546e63311e20a5af"},"cell_type":"code","source":"levelOfFit = abs(rootMeanSquaredError_train-rootMeanSquaredError_test)/rootMeanSquaredError_train*100.0\nprint(\"There is a {:.02f}% difference between the root mean squared errors of the train set and the test set.\".format(levelOfFit))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fc63210584f3ee466bb25201efa98f9b27c7fc2"},"cell_type":"code","source":"df2.Country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ceaf884161292b0c6f52439e581adbbf086f801"},"cell_type":"markdown","source":"### Creating a Data Frame from Which to Base the Plots"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"06e05465328a8bc0479cf8a5fef9fcc36b5ec426"},"cell_type":"code","source":"plotting_df = pd.DataFrame(train_labels)\nplotting_df['PredictedSalary'] = train_predictions\nplotting_df['Country'] = train_features.index.map(df.Country)\nplotting_df['Education'] = train_features.index.map(df.Education)\nplotting_df['Experience'] = train_features.index.map(df.YearsCodingProf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ed992abfbe87089f16bc5042b4bcca08b6c9c2f"},"cell_type":"code","source":"plotting_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8c9a080dd5810782cf969c4487fa7998b603f49"},"cell_type":"code","source":"len(plotting_df.Country.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1cd1560e36e026cf044ffaa4b5b546aef4a284a6"},"cell_type":"code","source":"byCountry = plotting_df.groupby('Country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ed6a6f1c7de7ca9f80f84a7200a25ab6d443ffa"},"cell_type":"code","source":"colors = [plt.get_cmap('inferno')(1. * i/255) for i in range(0, 255, 15)]\ncountries = plotting_df.Country.unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fe06a2ab6554d1d2d7e222650e5876fcc7d0375"},"cell_type":"markdown","source":"Firstly, we'll create a scatter plot on a single axis."},{"metadata":{"trusted":true,"_uuid":"26f05ad4d449842ca2be276e2179f9ef6caea155"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(14, 14))\ni = 0\nfor a, b in byCountry:\n    plt.scatter(b.ConvertedSalary, b.PredictedSalary, color=colors[i], label=a, alpha=0.5)\n    i +=1\nplt.xlabel('Actual Salary')\nplt.ylabel('Predicted Salary')\nplt.legend()\nplt.title('Predicted v Actual Salary');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"583ddcdbced91e82dde92ea22c3f20ff1aa808a9"},"cell_type":"markdown","source":"And now we'll break the countries out into subplots."},{"metadata":{"trusted":true,"_uuid":"c6e2efa63050ee923102d032cf0ca301af11c3ad"},"cell_type":"code","source":"fig, ax = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(12, 12))\ncounter = 0\nfor i in range(4):\n    for j in range(4):\n        temp = byCountry.get_group(countries[counter])\n        ax[i][j].scatter(temp['ConvertedSalary'], temp['PredictedSalary'], color = colors[counter])\n        ax[i][j].set_title(countries[counter])\n        counter += 1\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ac1d98a016bd01511cab2498af772c110074715"},"cell_type":"markdown","source":"#### Breaking Down the Stats\nWe'll write a function, `create_correlations_table()`, that will return a data frame that shows us\n1. the category,\n2. the sample size,\n3. the Pearson's R value,\n4. the two-tailed p-value relative to that Pearson's R value (that is to say, the chances of getting so extreme a Pearson's R by dumb luck, rather than correlation), and\n5. The root-mean-squared-error value for each category.\n\nThe function has a `sample_size_cutoff` parameter for when we'll get to particularly small values, for which Pearson's R values are meaningless, set by default at zero."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8e52f8422678105f8c441b8c92de89c28d62677"},"cell_type":"code","source":"import scipy.stats as stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3bf79c06f384b341288241a011ca37a236363e93"},"cell_type":"code","source":"def create_correlations_table(groupedDf, sample_size_cutoff=0):\n    holder = []\n    for a, b in groupedDf:\n        if len(b) > sample_size_cutoff:\n            if type(a) == str:\n                category = a\n            else:\n                category = str(a)\n            temp = stats.pearsonr(b['ConvertedSalary'], b['PredictedSalary'])\n            RMSE = np.sqrt(mean_squared_error(b.ConvertedSalary, b.PredictedSalary))\n            holder.append([a, len(b), temp[0], temp[1], RMSE])\n        else:\n            continue\n\n    correlations = pd.DataFrame(holder, columns = ['Country', 'Sample Size', 'Pearson R', 'Probability', 'RMSE'])\n    correlations.sort_values('RMSE', inplace=True)\n    return correlations","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"38f7e5d738fb050e127ca34569901899a33f6d78"},"cell_type":"code","source":"country_correlations = create_correlations_table(byCountry)\ncountry_correlations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86666821500171abfc26b5bdcedf012040d1d9b6"},"cell_type":"markdown","source":"The country with the best Pearson's R number for the correlation between predicted and actual salary is also the country with the highest RMSE score. This isn't novel, as accuracy of prediction isn't the same thing as accuracy of correlation."},{"metadata":{"_uuid":"32f43a43417607431ee8e1f470de1bd9728d9fab"},"cell_type":"markdown","source":"### Plotting by Experience\nWe'll follow the same procedure in examining the effect of experience on salary."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b69b67be7dc31b93120448f4787aebfab627e37a"},"cell_type":"code","source":"byExperience = plotting_df.groupby('Experience')\nyears = plotting_df.Experience.dropna().unique().tolist()\ncolors = [plt.get_cmap('inferno')(1. * i/255) for i in range(0, 255, 21)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3168d28b7697c401cc7165081c8f3b6ecff1abe4"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(14, 14))\ni = 0\nfor a, b in byExperience:\n    plt.scatter(b.ConvertedSalary, b.PredictedSalary, color=colors[i], label=a, alpha=0.5)\n    i +=1\nplt.xlabel('Actual Salary')\nplt.ylabel('Predicted Salary')\nplt.legend()\nplt.title('Predicted v Actual Salary');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0a403898db734714a810bd92da6ae1dfaa8aed5"},"cell_type":"code","source":"fig, ax = plt.subplots(4, 3, sharex=True, sharey=True, figsize=(12, 12))\ncounter = 0\nfor i in range(4):\n    for j in range(3):\n        if counter < len(years):\n            temp = byExperience.get_group(years[counter])\n            ax[i][j].scatter(temp['ConvertedSalary'], temp['PredictedSalary'], color = colors[counter])\n            ax[i][j].set_title(years[counter])\n            counter += 1\n        else:\n            pass\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc54747faba74357e852910212f73241c1ba808"},"cell_type":"code","source":"experience_correlations = create_correlations_table(byExperience)\nexperience_correlations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84cf33e1d6628c10f194370c7d64e979e7bccea9"},"cell_type":"markdown","source":"This is all very interesting. There's a better match here between correlation and root-mean-squared error. The hardest category to predict is thirty or more years, which is hardly surprising and there are many crossroads over thirty years. The easiest is at the opposite end of the scale, those developers who have just started out.\n\n## By Country and Experience\nHaving gone this far, it seems worthwhile to combine the two to see which has the strongest impact on salary."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f161ca7ccebeee2dad7677cddc43babc08aff662"},"cell_type":"code","source":"byCountryEx = plotting_df.groupby(['Country', 'Experience'])\ncountry_experience_df = create_correlations_table(byCountryEx, 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21f64a49c4251ae7fe77c924ef1fadbe6267546f"},"cell_type":"code","source":"country_experience_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d202e3972b33a4a272890cb95b0c69331a6ddeb"},"cell_type":"markdown","source":"We get our best RMSE scores for developers with 0-2 years of experience, irrespecitive of where they're based. Strangely, Switzerland is very erractic for the next experience level up, with the highest RMSE value of the lot.\n\n## Conclusions\nAll data analysis reports are only as good as the information on which their built. Surveys are not ideal tools for researching salaries. All surveys suffer from response bias by their nature, and this particular survey is unfortunate is its not identifying languages as primary, secondary and tertiary, say, or as combinations - frontend, backend, and so on.\n\nFor all that, our model is good with a 3% difference between the RMSE scores for our training and test sets and could be even better with a few more little tweaks."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6b66d4f5c84e9ba246e77443b65520cf05e4ea1f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}