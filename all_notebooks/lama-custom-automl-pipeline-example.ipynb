{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Step 0. Install LAMA"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"pip install lightautoml","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.1. Import necessary libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standard python libraries\nimport os\nimport time\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport matplotlib.pyplot as plt\nimport pickle\n\n# Imports from our package\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.pipelines.features.base import EmptyFeaturePipeline\nfrom lightautoml.pipelines.features.lgb_pipeline import LGBAdvancedPipeline, LGBSimpleFeatures\nfrom lightautoml.automl.base import AutoML\nfrom lightautoml.ml_algo.boost_lgbm import BoostLGBM\nfrom lightautoml.ml_algo.tuning.optuna import OptunaTuner\nfrom lightautoml.pipelines.ml.base import MLPipeline\nfrom lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\nfrom lightautoml.pipelines.selection.permutation_importance_based import NpPermutationImportanceEstimator, \\\n    NpIterativeFeatureSelector\n\nfrom lightautoml.transformers.base import LAMLTransformer, SequentialTransformer, UnionTransformer, ColumnsSelector\nfrom lightautoml.pipelines.utils import get_columns_by_role\nfrom lightautoml.dataset.roles import NumericRole\nfrom lightautoml.pipelines.features.base import FeaturesPipeline, TabularDataFeatures\nfrom lightautoml.reader.base import PandasToPandasReader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.2. Parameters "},{"metadata":{"trusted":true},"cell_type":"code","source":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 3 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 300 # Time in seconds for automl run\nTARGET_NAME = 'TARGET' # Target column name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.3. Fix torch number of threads and numpy seed "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.4. Example data load "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata = pd.read_csv('../input/lama-datasets/sampled_app_train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.5. (Optional) Some user feature preparation "},{"metadata":{},"cell_type":"markdown","source":"Cell below shows some user feature preparations to create task more difficult (this block can be omitted if you don't want to change the initial data):"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata['BIRTH_DATE'] = (np.datetime64('2018-01-01') + data['DAYS_BIRTH'].astype(np.dtype('timedelta64[D]'))).astype(str)\ndata['EMP_DATE'] = (np.datetime64('2018-01-01') + np.clip(data['DAYS_EMPLOYED'], None, 0).astype(np.dtype('timedelta64[D]'))\n                    ).astype(str)\n\ndata['constant'] = 1\ndata['allnan'] = np.nan\n\ndata['report_dt'] = np.datetime64('2018-01-01')\n\ndata.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 0.6. (Optional) Data splitting for train-test "},{"metadata":{},"cell_type":"markdown","source":"Block below can be omitted if you are going to train model only or you have specific train and test files:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain_data, test_data = train_test_split(data, \n                                         test_size=TEST_SIZE, \n                                         stratify=data[TARGET_NAME], \n                                         random_state=RANDOM_STATE)\nprint('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n              .format(train_data.shape, test_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ==== Custom AutoML pipeline ====\n\n\n## Step 1. Create Task and Reader"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntask = Task('binary', )\nreader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2. Setup columns roles"},{"metadata":{},"cell_type":"markdown","source":"Roles setup here set target column and base date, which is used to calculate date differences:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nroles = {'target': TARGET_NAME,\n         DatetimeRole(base_date=True, seasonality=(), base_feats=False): 'report_dt',\n         }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3. Create custom transformer and feature pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass GroupByTransformer(LAMLTransformer):\n\n    _fit_checks = ()\n    _transform_checks = ()\n    _fname_prefix = 'grb'\n\n\n    @property\n    def features(self):\n        \"\"\"Features list.\"\"\"\n\n        return self._features\n\n    def __init__(self):\n\n        super().__init__()\n        self.dicts = {}\n\n\n\n    def fit(self, dataset):\n\n        # set transformer names and add checks\n        for check_func in self._fit_checks:\n            check_func(dataset)\n        # set transformer features\n\n        # convert to accepted dtype and get attributes\n        dataset = dataset.to_pandas()\n        df = dataset.data\n        cat_cols = get_columns_by_role(dataset, 'Category')\n        num_cols = get_columns_by_role(dataset, 'Numeric')\n\n        feats = []\n        for cat in cat_cols:\n            for num in num_cols:  \n                feature = f'{self._fname_prefix}__{cat}_delta_mean_{num}'\n                _dict = df[[cat, num]].groupby(cat)[num].mean().to_dict()\n                self.dicts[feature] = {'cat': cat, 'num': num, 'values': _dict}\n                feats.append(feature)\n            \n        self._features = feats\n        return self\n\n\n    def transform(self, dataset):\n\n        # checks here\n        super().transform(dataset)\n        # convert to accepted dtype and get attributes\n        dataset = dataset.to_pandas()\n        df = dataset.data\n\n        # transform\n        roles = NumericRole()\n        outputs = []\n        for feat, value in self.dicts.items():\n            cat, num = value['cat'], value['num']\n            new_arr = (df[num] - df[cat].map(value['values'])).values.reshape(-1, 1)\n            output = dataset.empty().to_numpy()\n            output.set_data(new_arr, [f'{self._fname_prefix}__{cat}_delta_mean_{num}'], roles)\n            outputs.append(output)\n        # create resulted\n        return dataset.empty().to_numpy().concat(outputs)\n    \n    \nclass GroupByPipeline(FeaturesPipeline, TabularDataFeatures):\n\n\n    def __init__(self, feats_imp = None, top_category: int = 3, top_numeric: int = 3, **kwargs):\n        \"\"\"\n\n        \"\"\"\n        super().__init__(feats_imp=feats_imp)\n        self.top_category = top_category\n        self.top_numeric = top_numeric\n\n    def create_pipeline(self, train):\n\n\n        transformer_list = []\n\n        categories = get_columns_by_role(train, 'Category')\n        numerics = get_columns_by_role(train, 'Numeric')\n        cat_feats_to_select = []\n        num_feats_to_select = []\n        if len(categories) > self.top_category:\n            cat_feats_to_select = self.get_top_categories(train, self.top_category)\n        elif len(categories) > 0:\n            cat_feats_to_select = categories\n            \n        if len(numerics) > self.top_numeric:\n            num_feats_to_select = self.get_top_numeric(train, self.top_numeric)\n        elif len(numerics) > 0:\n            num_feats_to_select = numerics\n        \n        if (len(cat_feats_to_select) > 0) and (len(num_feats_to_select) > 0):\n            cat_processing = [\n\n                ColumnsSelector(keys=cat_feats_to_select + num_feats_to_select),\n                GroupByTransformer(),\n\n            ]\n            cat_processing = SequentialTransformer(cat_processing)\n            transformer_list.append(cat_processing)\n            \n        return UnionTransformer(transformer_list)\n    \n    def get_top_numeric(self, train, top_n = 5):\n\n        nums = get_columns_by_role(train, 'Numeric')\n        if len(nums) == 0:\n            return []\n\n        df = pd.DataFrame({'importance': 0, 'cardinality': 0}, index=nums)\n        # importance if defined\n        if self.feats_imp is not None:\n            feats_imp = pd.Series(self.feats_imp.get_features_score()).sort_values(ascending=False)\n            df['importance'] = feats_imp[feats_imp.index.isin(nums)]\n            df['importance'].fillna(-np.inf)\n\n        # check for cardinality\n        df['cardinality'] = -self.get_uniques_cnt(train, nums)\n        # sort\n        df = df.sort_values(by=['importance', 'cardinality'], ascending=[False, self.ascending_by_cardinality])\n        # get top n\n        top = list(df.index[:top_n])\n\n        return top","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4. Create feature selector."},{"metadata":{"trusted":true},"cell_type":"code","source":"#post selection\n\nmodel0 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'seed': 42, 'num_threads': N_THREADS}\n)\n\npie = NpPermutationImportanceEstimator()\nselector = ImportanceCutoffSelector(LGBSimpleFeatures(), model0, pie, cutoff=-99999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5. Create pipelines."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npipe = LGBAdvancedPipeline(top_intersections=2).append(GroupByPipeline(selector, 5, 5))\n\nmodel = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 128, 'seed': 1, 'num_threads': N_THREADS})\n\n\npipeline = MLPipeline([\n    (model),\n], pre_selection=selector, features_pipeline=pipe, post_selection=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6. Create AutoML."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time \nstart = time.time()\n\nautoml = AutoML(reader, [\n    [pipeline],\n], skip_conn=False, verbose=0)\n\noof_pred = automl.fit_predict(train_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))\ntime_automl = time.time() - start","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 7. Predict to test data and check scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntest_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME].values,\n                                           oof_pred.data[:, 0])))\ntest_automl = roc_auc_score(test_data[TARGET_NAME].values, test_pred.data[:, 0])\nprint('TEST score: {}'.format(test_automl))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}