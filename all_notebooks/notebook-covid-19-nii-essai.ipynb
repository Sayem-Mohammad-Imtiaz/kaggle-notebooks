{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/covid19-ct-scans'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.listdir(\"../input\"))\nfilenames = os.listdir(\"../input/covid19-ct-scans/ct_scans\")\nprint(filenames)\nprint(len(filenames))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib\n\nfrom scipy import ndimage\n#function to read the nii file\ndef read_nifti_file(filepath):\n    \"\"\"Read and load volume\"\"\"\n    # Read file\n    scan = nib.load(filepath)\n    # Get raw data\n    scan = scan.get_fdata()\n    return scan\n#function to normalize the image\ndef normalize(volume):\n    \"\"\"Normalize the volume\"\"\"\n    min = -1000\n    max = 400\n    volume[volume < min] = min\n    volume[volume > max] = max\n    volume = (volume - min) / (max - min)\n    volume = volume.astype(\"float32\")\n    return volume\n#function to resize the nii file\ndef resize_volume(img):\n    \"\"\"Resize across z-axis\"\"\"\n    # Set the desired depth\n    desired_depth = 64\n    desired_width = 128\n    desired_height = 128\n    # Get current depth\n    current_depth = img.shape[-1]\n    current_width = img.shape[0]\n    current_height = img.shape[1]\n    # Compute depth factor\n    depth = current_depth / desired_depth\n    width = current_width / desired_width\n    height = current_height / desired_height\n    depth_factor = 1 / depth\n    width_factor = 1 / width\n    height_factor = 1 / height\n    # Rotate\n    img = ndimage.rotate(img, 90, reshape=False)\n    # Resize across z-axis\n    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n    return img\ndef process_scan(path):\n    \"\"\"Read and resize volume\"\"\"\n    # Read scan\n    volume = read_nifti_file(path)\n    # Normalize\n    volume = normalize(volume)\n    # Resize width, height and depth\n    volume = resize_volume(volume)\n    return volume","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read the paths of the CT scans from the class directories\n#file named corona.nii are + and file named radiopaedia.nii are pneumonia\n#normal_ct_scans_paths = [\n    #os.path.join(os.getcwd(), \"../input/covid19-ct-scans/ct-scans\", file)\n    #for folder in os.listdir(\"../input/covid19-ct-scans/ct-scans\")\n#]\n#path=os.listdir(\"../input/covid19-ct-scans/ct-scans\")\n#for folder in path\n#print(folder)\nimport os\nimport numpy as np\n#fichiers = [f for f in os.listdir('../input/covid19-ct-scans/ct-scans') if isfile(join('../input/covid19-ct-scans/ct-scans', f)]\n#print(fichiers)\n#for f in os.listdir('../input/covid19-ct-scans/ct-scans'):\n    #print(f)\n#filenames_ct_scans = os.listdir(\"../input/covid19-ct-scans/ct_scans\")\n####\nprint(os.getcwd())\nabnormal_ct_scan_paths = [\n    #os.path.join(os.getcwd(), \"covid19-ct-scans/ct_scans\", x)\n    #os.listdir()\n    \"../input/covid19-ct-scans/ct_scans\"+\"/\"+x\n    for x in os.listdir(\"../input/covid19-ct-scans/ct_scans\") if x[0]=='c'\n]\nprint(\"----corona ct scan path----\")\nprint(abnormal_ct_scan_paths)\n#print(len(normal_scan_paths))\nnormal_ct_scan_paths = [\n    #os.path.join(os.getcwd(), \"covid19-ct-scans/ct_scans\", x)\n    \"../input/covid19-ct-scans/ct_scans\"+\"/\"+x\n    for x in os.listdir(\"../input/covid19-ct-scans/ct_scans\") if x[0]=='r'\n]\nprint(\"----radiopaedia ct scan path----\")\nprint(normal_ct_scan_paths)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport os\nfrom scipy import ndimage\n#function to read the nii file\ndef read_nifti_file(filepath):\n    \"\"\"Read and load volume\"\"\"\n    # Read file\n    scan = nib.load(filepath)\n    # Get raw data\n    scan = scan.get_fdata()\n    return scan\n#function to normalize the image\ndef normalize(volume):\n    \"\"\"Normalize the volume\"\"\"\n    min = -1000\n    max = 400\n    volume[volume < min] = min\n    volume[volume > max] = max\n    volume = (volume - min) / (max - min)\n    volume = volume.astype(\"float32\")\n    return volume\n#function to resize the nii file\ndef resize_volume(img):\n    \"\"\"Resize across z-axis\"\"\"\n    # Set the desired depth\n    desired_depth = 64\n    desired_width = 128\n    desired_height = 128\n    # Get current depth\n    current_depth = img.shape[-1]\n    current_width = img.shape[0]\n    current_height = img.shape[1]\n    # Compute depth factor\n    depth = current_depth / desired_depth\n    width = current_width / desired_width\n    height = current_height / desired_height\n    depth_factor = 1 / depth\n    width_factor = 1 / width\n    height_factor = 1 / height\n    # Rotate\n    img = ndimage.rotate(img, 90, reshape=False)\n    # Resize across z-axis\n    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n    return img\ndef process_scan(path):\n    \"\"\"Read and resize volume\"\"\"\n    # Read scan\n    volume = read_nifti_file(path)\n    # Normalize\n    volume = normalize(volume)\n    # Resize width, height and depth\n    volume = resize_volume(volume)\n    return volume\n#abnormal_scans = np.array([process_scan(path) for path in abnormal_ct_scan_paths])\nfor path in abnormal_ct_scan_paths:\n    print(path)\n#abnormal_scans = np.array([process_scan(path) for path in abnormal_ct_scan_paths])  \n#normal_scans = np.array([process_scan(path) for path in normal_ct_scan_paths])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r=read_nifti_file(abnormal_ct_scan_paths[0])\nprint(type(r))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abnormal_scans = np.array([process_scan(path) for path in abnormal_ct_scan_paths])\nnormal_scans = np.array([process_scan(path) for path in normal_ct_scan_paths])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(abnormal_scans)) #numpy.ndarray","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abnormal_labels = np.array([1 for i in range(len(abnormal_scans))])\nnormal_labels = np.array([0 for i in range(len(normal_scans))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data in the ratio 70-30 for training and validation.\nx_train = np.concatenate((abnormal_scans[:7], normal_scans[:7]), axis=0)\ny_train = np.concatenate((abnormal_labels[:7], normal_labels[:7]), axis=0)\nx_val = np.concatenate((abnormal_scans[7:], normal_scans[7:]), axis=0)\ny_val = np.concatenate((abnormal_labels[7:], normal_labels[7:]), axis=0)\nprint(\n    \"Number of samples in train and validation are %d and %d.\"\n    % (x_train.shape[0], x_val.shape[0])\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nfrom scipy import ndimage\n\n\n@tf.function\ndef rotate(volume):\n    \"\"\"Rotate the volume by a few degrees\"\"\"\n\n    def scipy_rotate(volume):\n        # define some rotation angles\n        angles = [-20, -10, -5, 5, 10, 20]\n        # pick angles at random\n        angle = random.choice(angles)\n        # rotate volume\n        volume = ndimage.rotate(volume, angle, reshape=False)\n        volume[volume < 0] = 0\n        volume[volume > 1] = 1\n        return volume\n\n    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n    return augmented_volume\n\n\ndef train_preprocessing(volume, label):\n    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n    # Rotate volume\n    volume = rotate(volume)\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label\n\n\ndef validation_preprocessing(volume, label):\n    \"\"\"Process validation data by only adding a channel.\"\"\"\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define data loaders.\ntrain_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nvalidation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n\nbatch_size = 2\n# Augment the on the fly during training.\ntrain_dataset = (\n    train_loader.shuffle(len(x_train))\n    .map(train_preprocessing)\n    .batch(batch_size)\n    .prefetch(2)\n)\n# Only rescale.\nvalidation_dataset = (\n    validation_loader.shuffle(len(x_val))\n    .map(validation_preprocessing)\n    .batch(batch_size)\n    .prefetch(2)\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndata = train_dataset.take(1)#returns nd array\nimages, labels = list(data)[0]\nimages = images.numpy()\nimage = images[0]\nprint(\"Dimension of the CT scan is:\", image.shape)\nplt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_slices(num_rows, num_columns, width, height, data):\n    \"\"\"Plot a montage of 20 CT slices\"\"\"\n    data = np.rot90(np.array(data))\n    data = np.transpose(data)\n    data = np.reshape(data, (num_rows, num_columns, width, height))\n    rows_data, columns_data = data.shape[0], data.shape[1]\n    heights = [slc[0].shape[0] for slc in data]\n    widths = [slc.shape[1] for slc in data[0]]\n    fig_width = 12.0\n    fig_height = fig_width * sum(heights) / sum(widths)\n    f, axarr = plt.subplots(\n        rows_data,\n        columns_data,\n        figsize=(fig_width, fig_height),\n        gridspec_kw={\"height_ratios\": heights},\n    )\n    for i in range(rows_data):\n        for j in range(columns_data):\n            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n            axarr[i, j].axis(\"off\")\n    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n    plt.show()\n\n\n# Visualize montage of slices.\n# 4 rows and 10 columns for 100 slices of the CT scan.\nplot_slices(4, 10, 128, 128, image[:, :, :40])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(width=128, height=128, depth=64):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n\n    inputs = keras.Input((width, height, depth, 1))\n\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(inputs)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=128, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Conv3D(filters=256, kernel_size=3, activation=\"relu\")(x)\n    x = layers.MaxPool3D(pool_size=2)(x)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.GlobalAveragePooling3D()(x)\n    x = layers.Dense(units=512, activation=\"relu\")(x)\n    x = layers.Dropout(0.3)(x)\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n\n    # Define the model.\n    model = keras.Model(inputs, outputs, name=\"3dcnn\")\n    return model\n\n# Build model.\nmodel = get_model(width=128, height=128, depth=64)\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile model.\ninitial_learning_rate = 0.0001\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n    metrics=[\"acc\"],\n)\n\n# Define callbacks.\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    \"3d_image_classification.h5\", save_best_only=True\n)\nearly_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n\n# Train the model, doing validation at the end of each epoch\nepochs = 100\nmodel.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    shuffle=True,\n    verbose=2,\n    callbacks=[checkpoint_cb, early_stopping_cb],\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20, 3))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"acc\", \"loss\"]):\n    ax[i].plot(model.history.history[metric])\n    ax[i].plot(model.history.history[\"val_\" + metric])\n    ax[i].set_title(\"Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].legend([\"train\", \"val\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load best weights.\nmodel.load_weights(\"3d_image_classification.h5\")\nprediction = model.predict(np.expand_dims(x_val[0], axis=0))[0]\nscores = [1 - prediction[0], prediction[0]]\n\nclass_names = [\"normal\", \"abnormal\"]\nfor score, name in zip(scores, class_names):\n    print(\n        \"This model is %.2f percent confident that CT scan is %s\"\n        % ((100 * score), name)\n    )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok\n#######visual explanations#######\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\nimport os\nimport datetime\nimport numpy as np\n#from deploy_config import*  #Import Paths and Model Config file\n#from loss_funnction_And_matrics import*  #Import Loss functions\n#from Resnet_3D import Resnet3D #import the model\nimport pandas as pd\nfrom tensorflow.keras.optimizers import Adam\nimport cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok\nINPUT_PATCH_SIZE=(128,128,64,1)\ninputs = tf.keras.Input(shape=INPUT_PATCH_SIZE, name='CT')\n#Model_3D=3dcnn(inputs,num_classes=2) # Load Model\nmod=get_model(128,128,64)\nModel_3D=mod(inputs,num_classes=2)\nModel_3D.load_weights('3d_image_classification.h5') #Load Weights\n#print('Loading The Model from this path--{}'.format(MODEL_PATH))\nModel_3D.summary()\n###---lAYER-Name--to-visualize--###\nLAYER_NAME='conv3d_3'\n# Create a graph that outputs target convolution and output\ngrad_model = tf.keras.models.Model([Model_3D.inputs], [Model_3D.get_layer(LAYER_NAME).output, Model_3D.output])\ngrad_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_OF_GPU=1\nDISTRIIBUTED_STRATEGY_GPUS=[\"gpu:0\",\"gpu:1\",\"gpu:2\"]\n##Network Configuration\nNUMBER_OF_CLASSES=2\nINPUT_PATCH_SIZE=(128,128,62, 2)\nTRAIN_NUM_RES_UNIT=3\nTRAIN_NUM_FILTERS=(64 ,64, 128, 256)\nTRAIN_STRIDES=( (2, 2, 2), (2, 2, 2), (2, 2, 2))\nTRAIN_CLASSIFY_ACTICATION=tf.nn.relu6\nTRAIN_KERNAL_INITIALIZER=tf.keras.initializers.VarianceScaling(distribution='uniform')\nTRAIN_CLASSIFY_LEARNING_RATE =0.0001\n#TRAIN_CLASSIFY_LOSS=Weighted_BCTL\n#3d_image_classification.h5\nOPTIMIZER=tf.keras.optimizers.Adam(lr=TRAIN_CLASSIFY_LEARNING_RATE,epsilon=1e-5)\nTRAIN_CLASSIFY_METRICS=tf.keras.metrics.acc()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok\ndef Get_Build_model(Input_patch_size,Model_weight,Layer_name):\n    inputs = tf.keras.Input(shape=Input_patch_size, name='CT')\n    #Model_3D=Resnet3D(inputs,num_classes=NUMBER_OF_CLASSES)\n    Model_3D=get_model(128,128,64)\n    Model_3D.load_weights(\"3d_image_classification.h5\")\n    #Model_3D.load_weights(Model_weight)\n    #print('Loading The Model from this path--{}'.format(MODEL_PATH))\n    Model_3D.summary()\n    Build_model=tf.keras.models.Model([Model_3D.inputs], [Model_3D.get_layer(Layer_name).output, Model_3D.output])\n    return Build_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\nimport os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport SimpleITK as sitk\nimport math\nimport cv2\n#from deploy_config import*\n#from loss_funnction_And_matrics import*\n#from Resnet_3D import Resnet3D\nfrom tensorflow.keras.optimizers import Adam\nfrom matplotlib import pyplot as plt\nfrom skimage.transform import resize\n#from Guided_GradCAM_3D_config import*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok\n# Function to get the image chunk fot guided GradCAM\ndef Get_image_array_Array_and_give_chunk(image_array,patch_slice_slice):\n\n    Devide_integer=image_array.shape[0] // patch_slice_slice\n    Reminder= image_array.shape[0] % patch_slice_slice\n    print('CT Volume_Shape={}'.format(image_array.shape))\n    print('Devide_integer={}'.format(Devide_integer))\n    print('Reminder={}'.format(Reminder))\n    print('Total of {} + {} ={} Should ={}'.format(patch_slice_slice*Devide_integer,Reminder,patch_slice_slice*Devide_integer+Reminder,image_array.shape[0]))\n\n    lastpatch_starts_from= (image_array.shape[0])-patch_slice_slice\n    print(lastpatch_starts_from)\n\n    patch_list=[]\n    patch_start=0\n    patch_end=patch_slice_slice\n    for i in range(Devide_integer):\n        #print(patch_start)\n        #print(patch_end)\n        ct_volume=image_array[patch_start:patch_end,:,:]\n        #print(ct_volume.shape)\n        patch_list.append(ct_volume)\n        patch_start+=patch_slice_slice\n        patch_end+=patch_slice_slice\n\n    last_slice_number_would_be=image_array.shape[0]\n    print(last_slice_number_would_be)\n    last_patch_When_making_nifty=(patch_slice_slice)-Reminder\n    print(last_patch_When_making_nifty)\n    Slice_will_start_from_here=last_slice_number_would_be-patch_slice_slice\n    print(Slice_will_start_from_here)\n    last_patch=image_array[Slice_will_start_from_here:,:,:]\n    last_patch.shape\n    patch_list.append(last_patch)\n\n    return patch_list,last_patch_When_making_nifty","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok\ndef Guided_GradCAM_3D(Grad_model,ct_io,Class_index):\n    # Create a graph that outputs target convolution and output\n    grad_model = Grad_model\n    input_ct_io=tf.expand_dims(ct_io, axis=-1)\n    input_ct_io=tf.expand_dims(input_ct_io, axis=0)\n    with tf.GradientTape() as tape:\n        conv_outputs, predictions = grad_model(input_ct_io)\n        loss = predictions[:, Class_index]\n    # Extract filters and gradients\n    output = conv_outputs[0]\n    grads = tape.gradient(loss, conv_outputs)[0]\n\n    ##--Guided Gradient Part\n    gate_f = tf.cast(output > 0, 'float32')\n    gate_r = tf.cast(grads > 0, 'float32')\n    guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n\n    # Average gradients spatially\n    weights = tf.reduce_mean(guided_grads, axis=(0, 1,2))\n    # Build a ponderated map of filters according to gradients importance\n    cam = np.ones(output.shape[0:3], dtype=np.float32)\n    for index, w in enumerate(weights):\n        cam += w * output[:, :, :, index]\n\n    capi=resize(cam,(ct_io.shape))\n    print(capi.shape)\n    capi = np.maximum(capi,0)\n    heatmap = (capi - capi.min()) / (capi.max() - capi.min())\n    return heatmap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok\ndef generate_guided_grad_cam(Nifti_path,Model_weight,Class_index,Input_patch_size_slice_number,Layer_name,Save_path):\n    # Reading the CT\n    img_path=Nifti_path\n    Class_index=Class_index\n    Model_weight=Model_weight\n    Layer_name=Layer_name\n    img_sitk = sitk.ReadImage(img_path, sitk.sitkFloat32)\n    image= sitk.GetArrayFromImage(img_sitk)\n    Input_patch_size=[Input_patch_size_slice_number,image.shape[1],image.shape[2],1]\n\n    get_grad_model=Get_Build_model(Input_patch_size,Model_weight,Layer_name)\n    ct_patch_chunk_List,last_patch_number=Get_image_array_Array_and_give_chunk(image_array=image,patch_slice_slice=Input_patch_size_slice_number)\n    first_heatmap=Guided_GradCAM_3D(get_grad_model,ct_patch_chunk_List[0],Class_index=Class_index)\n    heatmap_concat=first_heatmap\n    for i in range(1,(len(ct_patch_chunk_List)-1)):\n        from Resnet_3D import Resnet3D\n        get_heatmap=Guided_GradCAM_3D(get_grad_model,ct_patch_chunk_List[i],Class_index=Class_index)\n        heatmap_concat=np.concatenate((heatmap_concat, get_heatmap), axis=0)\n    last_heatmap=Guided_GradCAM_3D(get_grad_model,ct_patch_chunk_List[-1],Class_index=Class_index)\n    heatmap_concat=np.concatenate((heatmap_concat, last_heatmap[last_patch_number:,:,:]), axis=0)\n    s_itk_image = sitk.GetImageFromArray(heatmap_concat)\n    s_itk_image.CopyInformation(img_sitk)\n    sitk.WriteImage(s_itk_image, Save_path)\n    return ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok\nNIFTI_PATH='../input/covid19-ct-scans/ct_scans/coronacases_org_001.nii'\nMODEL_WEIGHT=\"3d_image_classification.h5\"\nCLASS_INDEX=1\nINPUT_PATCH_SIZE_SLICE_NUMBER=301\nLAYER_NAME='conv3d_3'\nSAVE_PATH='./'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ok last one\nif __name__ == '__main__':\n    img_path=NIFTI_PATH\n    Model_weight=MODEL_WEIGHT\n    Class_index=CLASS_INDEX\n    Input_patch_size_slice_number=INPUT_PATCH_SIZE_SLICE_NUMBER\n    Layer_name=LAYER_NAME\n    Save_path=SAVE_PATH\n    generate_guided_grad_cam(img_path,Model_weight,Class_index,Input_patch_size_slice_number,Layer_name,Save_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_WEIGHT=\"Path/of/Model/Weight/XXX.h5\"\nCLASS_INDEX=2 # Index of the class for which you want to see the Guided-gradcam\nINPUT_PATCH_SIZE_SLICE_NUMBER= ???# Input patch slice you want to feed at a time\nLAYER_NAME='conv3d_18' # Name of the layer from where you want to get the Guided-GradCAM\nNIFTI_PATH=\"imput/niftidata/path/XXX.nii.gz\"\nSAVE_PATH=\"/Output/niftydata/path/ML_Guided_GradCaN_XXXX.nii.gz\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_image_with_crop_or_pad(image, img_size=(64, 64, 64), **kwargs):\n    \"\"\"Image resizing. Resizes image by cropping or padding dimension\n     to fit specified size.\n    Args:\n        image (np.ndarray): image to be resized\n        img_size (list or tuple): new image size\n        kwargs (): additional arguments to be passed to np.pad\n    Returns:\n        np.ndarray: resized image\n    \"\"\"\n\n    assert isinstance(image, (np.ndarray, np.generic))\n    assert (image.ndim - 1 == len(img_size) or image.ndim == len(img_size)), \\\n        'Example size doesnt fit image size'\n\n    # Get the image dimensionality\n    rank = len(img_size)\n\n    # Create placeholders for the new shape\n    from_indices = [[0, image.shape[dim]] for dim in range(rank)]\n    to_padding = [[0, 0] for dim in range(rank)]\n\n    slicer = [slice(None)] * rank\n\n    # For each dimensions find whether it is supposed to be cropped or padded\n    for i in range(rank):\n        if image.shape[i] < img_size[i]:\n            to_padding[i][0] = (img_size[i] - image.shape[i]) // 2\n            to_padding[i][1] = img_size[i] - image.shape[i] - to_padding[i][0]\n        else:\n            from_indices[i][0] = int(np.floor((image.shape[i] - img_size[i]) / 2.))\n            from_indices[i][1] = from_indices[i][0] + img_size[i]\n\n        # Create slicer object to crop or leave each dimension\n        slicer[i] = slice(from_indices[i][0], from_indices[i][1])\n\n    # Pad the cropped image to extend the missing dimension\n    return np.pad(image[slicer], to_padding, **kwargs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##----Load---Image\nimport SimpleITK as sitk\nimg_path='../input/covid19-ct-scans/ct_scans/coronacases_org_001.nii'\nimg_sitk = sitk.ReadImage(img_path, sitk.sitkFloat32)\nimage= sitk.GetArrayFromImage(img_sitk)\nresized_img=resize_image_with_crop_or_pad(image=image,img_size=(128,128,64),mode='symmetric')\n#resized_img=image\nresized_img.shape\nio_img=tf.expand_dims(resized_img, axis=-1)\nprint(io_img.shape)\nio_img=tf.expand_dims(io_img, axis=0)\nprint(io_img.shape)\n###----index of the class\nCLASS_INDEX=2\n\n###--Compute GRADIENT\nwith tf.GradientTape() as tape:\n    conv_outputs, predictions = grad_model(io_img)\n    loss = predictions[:, CLASS_INDEX]\n\n# Extract filters and gradients\noutput = conv_outputs[0]\ngrads = tape.gradient(loss, conv_outputs)[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average gradients spatially\nweights = tf.reduce_mean(grads, axis=(0, 1,2))\n# Build a ponderated map of filters according to gradients importance\ncam = np.zeros(output.shape[0:3], dtype=np.float32)\n\nfor index, w in enumerate(weights):\n    cam += w * output[:, :, :, index]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom skimage.transform import resize\nfrom matplotlib import pyplot as plt\ncapi=resize(cam,(128,128,64))\n#print(capi.shape)\ncapi = np.maximum(capi,0)\nheatmap = (capi - capi.min()) / (capi.max() - capi.min())\nf, axarr = plt.subplots(2,3,figsize=(15,10));\nf.suptitle('Grad-CAM')\nslice_count=127\nslice_count2=30\n    \naxial_ct_img=np.squeeze(resized_img[slice_count, :,:])\naxial_grad_cmap_img=np.squeeze(heatmap[slice_count,:, :])\n\ncoronal_ct_img=np.squeeze(resized_img[:,slice_count2,:])\ncoronal_grad_cmap_img=np.squeeze(heatmap[:,slice_count2,:]) \n\nimg_plot = axarr[0,0].imshow(axial_ct_img, cmap='gray');\naxarr[0,0].axis('off')\naxarr[0,0].set_title('CT')\n    \nimg_plot = axarr[0,1].imshow(axial_grad_cmap_img, cmap='jet');\naxarr[0,1].axis('off')\naxarr[0,1].set_title('Grad-CAM')\n    \naxial_overlay=cv2.addWeighted(axial_ct_img,0.3,axial_grad_cmap_img, 0.6, 0)\n    \nimg_plot = axarr[0,2].imshow(axial_overlay,cmap='jet');\naxarr[0,2].axis('off')\naxarr[0,2].set_title('Overlay')\n\n\nimg_plot = axarr[1,0].imshow(coronal_ct_img, cmap='gray');\naxarr[1,0].axis('off')\naxarr[1,0].set_title('CT')\n    \nimg_plot = axarr[1,1].imshow(coronal_grad_cmap_img, cmap='jet');\naxarr[1,1].axis('off')\naxarr[1,1].set_title('Grad-CAM')\n    \nCoronal_overlay=cv2.addWeighted(coronal_ct_img,0.3,coronal_grad_cmap_img, 0.6, 0)\n    \nimg_plot = axarr[1,2].imshow(Coronal_overlay,cmap='jet');\naxarr[1,2].axis('off')\naxarr[1,2].set_title('Overlay')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}