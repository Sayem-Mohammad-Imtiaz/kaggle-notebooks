{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PRINCIPAL COMPONENT ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"Principal Component Analysis are data reduction methods used to reexpress multivariate data with fewer dimensions. The goal of these method is to re orient the data so that a multitude of original variables can be summarized with relatively few factors or components that capture the maximum possible information from the original variables.\n\nThe goal of PCA is to find the component z = [z1,z2,z3......] which are linear combinations of the original variables x and acoounts for maximum possible variance.","metadata":{}},{"cell_type":"markdown","source":"DESCRIPTION\nA macroeconomic dataset which provides a well known example for a highly collinear regression. This data frame consists of  6 economical variables, observed yearly from 1947-62\n1) GNP de flator GNP implicit price defaltor (gross national product)\n2) unemployed no. of unemployed\n3) Armed.Forces no. of people in armed forces\n4) populkation 'noninstitutionalized' population >= 14 years of age\n5) employed number of people employed.\n6) Gross National Product\n","metadata":{}},{"cell_type":"code","source":"#import all the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/principal-component-analysis/Longley (1).csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here python is able to correctly specify the data type of each variabels.","metadata":{}},{"cell_type":"markdown","source":"Since, we will be doing PCA on the data to reduce the dimesions let us go ahead and drop the target variable 'employed'.","metadata":{}},{"cell_type":"code","source":"df = data.drop('Employed',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation = df.corr()\ncorrelation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(36,6), dpi=140)\nfor j,i in enumerate(['pearson','kendall','spearman']):\n  plt.subplot(1,3,j+1)\n  correlation = df.dropna().corr(method=i)\n  sns.heatmap(correlation, linewidth = 2)\n  plt.title(i, fontsize=18)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = data['Employed']\nY.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this adds the constant term beta0 to the multiple linear regression\nX = sm.add_constant(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model =  sm.OLS(Y,X).fit()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see that we have a situation of \"High R^2 but few significant t ratios which cleary tells us that we have a situation of multicollinearity and this is violating our assumptions of \"No Multicollineatiy\" of OLS regression.","metadata":{}},{"cell_type":"code","source":"from statsmodels.multivariate.factor import Factor\nmodel = Factor(df).fit()\nmodel.plot_scree()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.multivariate.pca import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pc = PCA(df,\n         ncomp=2,\n         standardize=True,\n         demean=True,\n         normalize=False,\n         gls=False,\n         weights=None,\n         missing=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_comp = pc.loadings.T\ndf_comp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_factors = pc.factors\nX_factors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation = X_factors.corr()\ncorrelation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(X_factors.corr())\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that by using PCA technique we have completely eliminated the multicollinearity problem.","metadata":{}},{"cell_type":"code","source":"X_pca=sm.add_constant(X_factors) #This adds the constant beta0 to the multiple linear regression.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = sm.OLS(Y,X_pca).fit()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}