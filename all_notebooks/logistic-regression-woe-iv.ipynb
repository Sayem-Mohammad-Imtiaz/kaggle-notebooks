{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hiring is an expensive task. There is a process of recluiment, training, adaptation and maybe the quality is worse than the previouous employee. Keeping an experience employee is an important task for a company. Factors of attrition can be discovered doing analysis on this fictional table.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.stats import zscore\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import KBinsDiscretizer\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data engineering and data cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['y'] = (df.Attrition == 'Yes').astype(int)\ndf.drop('Attrition', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"StandardHours = 80\ndrop_fea = [\n  'Over18', \n  'EmployeeCount', \n  'StandardHours', \n  'EmployeeNumber', \n]\ndf.drop(drop_fea, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_fea = [\n  'Age',  \n  'DailyRate',\n  'DistanceFromHome',\n  'HourlyRate',\n  'MonthlyIncome', \n  'MonthlyRate',\n  'NumCompaniesWorked',\n  'PercentSalaryHike',\n  'TotalWorkingYears', \n  'TrainingTimesLastYear',\n  'YearsAtCompany', \n  'YearsInCurrentRole',\n  'YearsSinceLastPromotion', \n  'YearsWithCurrManager',\n]\n\ncat_fea = [\n  'BusinessTravel',\n  'Department',\n  'Education',\n  'EducationField',\n  'EnvironmentSatisfaction', # rating scale from 1 to 5\n  'Gender',\n  'JobInvolvement', # rating scale from 1 to 5\n  'JobLevel', # rating scale from 1 to 5\n  'JobRole',\n  'JobSatisfaction', # rating scale from 1 to 5\n  'MaritalStatus',\n  'OverTime',\n  'PerformanceRating', # rating scale from 1 to 5\n  'RelationshipSatisfaction', # rating scale from 1 to 5\n  'StockOptionLevel', # rating scale from 0 to 3\n  'WorkLifeBalance', # rating scale from 1 to 5\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New features, maybe they can be an important factor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PercentWorkingAtCompany'] = df['YearsAtCompany'] / df['TotalWorkingYears'] * 100\ndf['PercentCurrentRoleAtCompany'] = df['YearsInCurrentRole'] / df['YearsAtCompany'] * 100\ndf['PercentHourlyRate'] = df['HourlyRate'] / StandardHours * 100\nnum_fea += ['PercentWorkingAtCompany', 'PercentCurrentRoleAtCompany', 'PercentHourlyRate']\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"attrittion = df.y.value_counts()\ngo.Figure([go.Pie(values = attrittion.values, labels=attrittion.index)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[num_fea] = df[num_fea].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[num_fea].describe(percentiles=[0.01,0.05,0.95,0.99])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[num_fea].hist(figsize=(15, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If there is a category with less than 0.05 of presence, then is an irrelevant category.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_discrete_values(df, feature, umbral = 0.05):\n  aux = df[feature].value_counts(True).to_frame()\n  aux['x'] = np.where(aux[feature] < umbral, 'Other', aux.index)\n  if aux[aux['x'] == 'Other'][feature].sum() < umbral:\n    aux['x'].replace({'Other' : aux.index[0]}, inplace=True)\n  df[feature].replace(dict(zip(aux.index, aux.x)), inplace=True)\n  return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fea in cat_fea:\n  df = normalize_discrete_values(df, fea)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Attribute relevance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Continuous values to buckets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = df[num_fea].describe().T['max'] - df[num_fea].describe().T['min']\ncut_features = aux[aux >= 6]\ncut_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"range_to_int = dict()\nint_to_range = dict()\nfor fea in cut_features.index:\n  kb = KBinsDiscretizer(encode = 'ordinal', strategy = 'uniform')\n  if cut_features[fea] < 10:\n    kb.n_bins = 3\n  elif cut_features[fea] < 50:\n    kb.n_bins = 4\n  else:\n    kb.n_bins = 5\n  df[fea] = kb.fit_transform(df[[fea]])\n  ranges = ['%.2f|%.2f' % (a, b) for a, b in zip(kb.bin_edges_[0], kb.bin_edges_[0][1:])]\n  int_to_range[fea] = dict(zip(range(len(ranges)), ranges))\n  range_to_int[fea] = dict(zip(ranges, range(len(ranges))))\n  df[fea].replace(int_to_range[fea], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Weight of Evidence & Information Value\n### WoE & IV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_woe_iv(df, feature, target):\n  lst = []\n  for val in df[feature].unique():\n      lst.append({\n          'Value': val,\n          'All': len(df[df[feature] == val]),\n          'Good': len(df[(df[feature] == val) & (df[target] == 0)]),\n          'Bad': len(df[(df[feature] == val) & (df[target] == 1)])\n      })\n      \n  dset = pd.DataFrame(lst)\n  dset['Distr_Good'] = dset['Good'] / dset['Good'].sum()\n  dset['Distr_Bad'] = dset['Bad'] / dset['Bad'].sum()\n  dset['WoE'] = np.log(dset['Distr_Good'] / dset['Distr_Bad'])\n  dset = dset.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n  dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n  dset = dset.sort_values(by='WoE')\n  \n  return dset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iv_val = dict()\nwoe_val = dict()\nfor fea in cat_fea + num_fea:\n  woe = get_woe_iv(df, fea, 'y')\n  iv_val[fea] = woe['IV'].sum()\n  woe_val[fea] = woe['WoE'].sum()\n  print(fea)\n  print(woe)\n  print('WOE score: {:.2f}'.format(woe_val[fea]))\n  print('IV score: {:.2f}'.format(iv_val[fea]))\n  print()\niv_val = sorted(iv_val.items(), key=lambda item:item[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the most important features that can explain attrition.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"iv = pd.DataFrame(iv_val, columns=['Feature', 'IV'])\niv = iv[iv['IV'] > 0.1]\niv_fea = iv['Feature']\niv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point, the dataframe only contains strings. For continouos features, the range is transform to an ordinal value. One hot encodign is applied to categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for fea in num_fea:\n  df[fea].replace(range_to_int[fea], inplace=True)\n\ncat_str_fea = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\ndf = pd.concat([df, pd.get_dummies(df[cat_str_fea], drop_first=True)], axis=1, sort=False)\nreverse_onehotencoding = df[cat_str_fea].copy()\ndf.drop(cat_str_fea, axis=1, inplace=True)\naux = [(x.split('_')[0] in list(iv_fea)) for x in df.columns]\niv_ohe_fea = df.columns[aux]\n\ndf = df.astype(float)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[iv_ohe_fea].copy()\ny = df['y'].copy()\n\nrandom_state = 760110\nXt, Xv, yt, yv = train_test_split(X, y, random_state = random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression is a powerful model for binary classification. Its interpretation is easy to understand.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(C = 0.3, max_iter=len(Xt))\nmodel.fit(Xt, yt)\nprint('roc_auc_score\\t', roc_auc_score(yt, model.predict_proba(Xt)[:, 1]), roc_auc_score(yv, model.predict_proba(Xv)[:, 1]))\nprint('accuracy\\t', accuracy_score(yt, model.predict(Xt)), accuracy_score(yv, model.predict(Xv)))\nprint('precision\\t', precision_score(yt, model.predict(Xt)), precision_score(yv, model.predict(Xv)))\nprint('recall\\t\\t', recall_score(yt, model.predict(Xt)), recall_score(yv, model.predict(Xv)))\nprint('f1_score\\t', f1_score(yt, model.predict(Xt)), f1_score(yv, model.predict(Xv)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(yt, model.predict(Xt)) / len(Xt), annot=True, fmt=\".2f\")\nplt.show()\nsns.heatmap(confusion_matrix(yv, model.predict(Xv)) / len(Xv), annot=True, fmt=\".2f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'feature': iv_ohe_fea, 'coef': model.coef_[0]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(yt, model.predict_proba(Xt)[:, 1])\nplot_roc_curve(fpr, tpr)\nfpr, tpr, thresholds = roc_curve(yv, model.predict_proba(Xv)[:, 1])\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scorecard","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aux = Xt.copy()\naux['proba'] = model.predict_proba(aux)[:,1]\naux['proba'] = pd.cut(aux['proba'],include_lowest=True,bins=np.arange(0,1.1,0.1))\naux.proba.value_counts(True).sort_index().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instead of getting a probability, it is convenient to set points to every election of values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pdo = 11.5  \nbase_score = 326 \nbase_odds = 1 \nfactor = float(pdo) / np.log(2)\noffset = base_score - factor * np.log(base_odds)\n\nn = len(iv_ohe_fea)\nbetas = model.coef_[0]\nalpha = model.intercept_[0]\nfor feat, beta in zip(iv_ohe_fea, betas):\n    aux['P_' + feat] = np.ceil((-aux[feat] * beta + alpha / n) * factor + offset / n).astype(int)\naux['score'] = aux[[f for f in aux.columns if f[:2] == 'P_']].sum(axis=1)\naux.score.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux.score.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aux['r_score'] = pd.cut(aux.score, bins=range(min(aux.score) - 15,max(aux.score) + 15,15),include_lowest=True)\naux.r_score.value_counts().sort_index().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next table shows that with fewer points is more probable to get attrition as result.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"aux['target']  = yt\naux['n']= 1.0\naux['r_score'] = aux['r_score'].astype(str)\naux['proba'] = aux['proba'].astype(str)\naux[['r_score','proba','target','n']].groupby(['r_score', 'proba', 'target']).sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If someone is filling a survey to detect attrition, it easier just to sum points than running a model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for fea in [x for x in range_to_int.keys() if x in iv_ohe_fea]:\n  aux[fea].replace(int_to_range[fea], inplace=True)\n\nl_sc = []\nfor fea in iv_ohe_fea:\n  aux2 = aux[[fea, 'P_%s' % fea]].copy().drop_duplicates().reset_index(drop=True)\n  aux2.rename(columns={fea:'value','P_%s' % fea:'points'},inplace=True)\n  aux2['feature'] = fea\n  l_sc.append(aux2)\n\nsc = pd.concat(l_sc,ignore_index=True)\nsc = sc.groupby(['feature','value']).sum()\nsc.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}