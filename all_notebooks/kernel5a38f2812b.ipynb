{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Tags\n\n- Multi Classclasification\n- Dimension Reduction [PCA]\n- Accuracy, Precision, Recall, F1 Score\n- Neural Network - Deep Learning using Tensorflow"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.compat.v2.test.is_gpu_available(cuda_only=False,min_cuda_compute_capability=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## load data\ndata = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')\ndata_test = pd.read_csv('/kaggle/input/mnist-in-csv/mnist_train.csv')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"## understand data - first overview\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Shuffle the data\ndata = shuffle(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Shape of the data\nprint('Shape of the complete dataset: ',data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Seperating Features and labels\ny = data[['label']]\nX = data.drop(['label'], axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## For Test Data\ny_test = data_test[['label']]\nX_test = data_test.drop(['label'], axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of the Features: ', X.shape)\nprint('Shape of the Label: ', y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.label.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Analysis - First Level\n\n- number of examples are arounf 60,000 which is good\n- number of features seems to be on the higher side, we have to check how to work with that.\n- we have in total 10 classes , one for each number for 0 to 9"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nsome_digit = X.values[10].reshape(28,28)\nplt.imshow(some_digit, cmap=matplotlib.cm.binary, interpolation='nearest')\nplt.show()\n\nsome_digit = X.values[110].reshape(28,28)\nplt.imshow(some_digit, cmap=matplotlib.cm.binary, interpolation='nearest')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Analysis - Second Level\n\n- feature represent the 784 coloums which is binary of 28 * 28 image cell. \n- current image is the gray scale image.\n- most the pixels are empty always , as small area is used for the representation of the number."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"## to uderstand the data distribution of each class in the dataset\nsn.countplot(x=\"label\", data=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desc = X.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"desc.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## As we can see , plotting the max value for each pixel , edges and boundaries never had any value.\nplt.imshow(desc.values[7].reshape(28,28), cmap=matplotlib.cm.Blues, interpolation='nearest')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## As we can see , plotting the mean value for each pixel , edges and boundaries never had any value.\n## mostly values are concentrated in the middle part of the iage only.\nplt.imshow(desc.values[1].reshape(28,28), cmap=matplotlib.cm.Blues, interpolation='nearest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Analysis\n\n- We have almost equal number of examples for each class.\n- By seeing the graph of max and mean value at each pixel , we can confirm edges are almost never used , mostly data is concentrated in the middle only."},{"metadata":{"trusted":true},"cell_type":"code","source":" X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of training features ', X_train.shape)\nprint('Shape of validation features ', X_val.shape)\n\nprint('Shape of training label', y_train.shape)\nprint('Shape of validation label', y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## scale the data, same scaler should be used for test data also\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"input_dimension = X_train.shape[1] # this represent number of features\n\n### hyper parameters\nepochs = 20\nbatch_size = 256\n\n### model\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(input_dimension,), activation='relu', kernel_regularizer= tf.keras.regularizers.l1(0.001)))\nmodel.add(Dense(126, activation='relu'))\nmodel.add(Dense(54, activation='relu',kernel_regularizer= tf.keras.regularizers.l1(0.01)))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01),loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train.values, epochs=epochs, batch_size=batch_size,\n          validation_data=(X_val, y_val.values))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.plot( history.history['accuracy'], color='skyblue', linewidth=2, label='training acc')\nplt.plot( history.history['val_accuracy'], color='green', linewidth=2, label='val acc')\n\nplt.plot( history.history['loss'], color='skyblue', linewidth=2, linestyle='dashed', label=\"training loss\")\nplt.plot( history.history['val_loss'], color='green', linewidth=2, linestyle='dashed', label=\"val loss\")\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understand the affect of Change in learning rate\n- We will try to change the batch size , and understand the affect on model."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"input_dimension = X_train.shape[1] # this represent number of features\n\n### hyper parameters\nepochs = 20\nbatch_size = 256\n\n### model\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(input_dimension,), activation='relu', kernel_regularizer= tf.keras.regularizers.l1(0.001)))\nmodel.add(Dense(126, activation='relu'))\nmodel.add(Dense(54, activation='relu',kernel_regularizer= tf.keras.regularizers.l1(0.001)))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train.values, epochs=epochs, batch_size=batch_size,\n          validation_data=(X_val, y_val.values))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.plot( history.history['accuracy'], color='skyblue', linewidth=2, label='training acc')\nplt.plot( history.history['val_accuracy'], color='green', linewidth=2, label='val acc')\n\nplt.plot( history.history['loss'], color='skyblue', linewidth=2, linestyle='dashed', label=\"training loss\")\nplt.plot( history.history['val_loss'], color='green', linewidth=2, linestyle='dashed', label=\"val loss\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen in the first examples the loss curve is not smooth, so it make sense to ower the learning rate a bit to have a better result , As we can see lowering the learning rate not only help us to get the smooth loss curve , but the owerall accuracy was also improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print('Shape of predicction ', y_test_pred.shape)\nprint('Shape of test ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_test, y_test_pred)\nprint(conf_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## Dark color represent the correctness of classification\nplt.matshow(conf_mat , cmap = plt.cm.Blues)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TO ca;culate the error rate , sum instance of each class and than divide it\nrow_sum = conf_mat.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mat/ row_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Dark color represent the higher error\nnp.fill_diagonal(norm_conf_mx,0)\nplt.matshow(norm_conf_mx, cmap = plt.cm.Blues)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understand the feature reduction \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=512)\nX_train = pca.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.cumsum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of input after PCA', X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val = pca.transform(X_val)\nX_test = pca.transform(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"\n### hyper parameters\ninput_dimension = X_train.shape[1] # this represent number of features\nepochs = 20\nbatch_size = 256\n\n### model\nmodel = Sequential()\nmodel.add(Dense(256, input_shape=(input_dimension,), activation='relu', kernel_regularizer= tf.keras.regularizers.l1(0.001)))\nmodel.add(Dense(126, activation='relu'))\nmodel.add(Dense(54, activation='relu',kernel_regularizer= tf.keras.regularizers.l1(0.001)))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train.values, epochs=epochs, batch_size=batch_size,\n          validation_data=(X_val, y_val.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot( history.history['accuracy'], color='skyblue', linewidth=2, label='training acc')\nplt.plot( history.history['val_accuracy'], color='green', linewidth=2, label='val acc')\n\nplt.plot( history.history['loss'], color='skyblue', linewidth=2, linestyle='dashed', label=\"training loss\")\nplt.plot( history.history['val_loss'], color='green', linewidth=2, linestyle='dashed', label=\"val loss\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"conf_mat = confusion_matrix(y_test, y_test_pred)\nprint(conf_mat)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note : Model with PCA executed comparatively faster as without it. With PCA we may even chose smaller model since the number if inputs have decreased. Now lets use the data augmentation to increase the data set and achive better numbers. Since CNN perform better with image , next step should be to perform Classification on images using CNN acheive more than 97 % accuracy."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":1}