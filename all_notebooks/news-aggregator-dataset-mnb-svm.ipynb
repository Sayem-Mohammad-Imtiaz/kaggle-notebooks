{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter\nimport re\npd.set_option('display.max_colwidth', -1)\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# read the data from drive\ndf = pd.read_json('News_Category_Dataset_v2.json', lines=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# descriptive analysis of the dataset\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" The dataset timeline starts at 28-01-2012 and ends at 26-05-2018"},{"metadata":{"trusted":false},"cell_type":"code","source":"df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check for missing values if any\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1 = df.copy()\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df1.aut_name + ' ' + \ndf1['text'] = (df1.headline + ' ' + df1.short_description)\ndf1 = df1[(df1.text != ' ') | (df1.text != '')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Removing Punctuation\ndf1['text'] = df1['text'].str.replace('[^\\w\\s]','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1.text = [x.lower() for x in df1.text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1.category[df1.category=='THE WORLDPOST'] = 'WORLDPOST'\ndf1.category[df1.category=='GREEN'] = 'ENVIRONMENT'\ndf1.category[df1.category=='CULTURE & ARTS'] = 'ARTS'\ndf1.category[df1.category=='COMEDY'] = 'ENTERTAINMENT'\ndf1.category[(df1.category=='BLACK VOICES') | (df1.category=='LATINO VOICES') | (df1.category=='QUEER VOICES')] = 'VOICES'\ndf1.category[df1.category=='STYLE'] = 'STYLE & BEAUTY'\ndf1.category[df1.category=='ARTS & CULTURE'] = 'ARTS'\ndf1.category[df1.category=='COLLEGE'] = 'EDUCATION'\ndf1.category[df1.category=='SCIENCE'] = 'TECH'\ndf1.category[df1.category=='WEDDINGS'] = 'GOOD NEWS'\ndf1.category[df1.category=='TASTE'] = 'FOOD & DRINK'\ndf1.category[(df1.category=='PARENTING') | (df1.category=='FIFTY')] = 'PARENTS'\ndf1.category[df1.category=='WORLD NEWS'] = 'WORLDPOST'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":false},"cell_type":"code","source":"# distribution of categories in dataset\nplt.figure(figsize=(16,8))\nsns.countplot(df1.category, order=df1.category.value_counts().index, color='c')\nplt.xticks(rotation=90)\nplt.xlabel('Category',fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title('Distribution of Categories', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"#Popular category per month\n# a = df1.groupby(pd.Grouper(key='date', freq='M'))['category'].agg(lambda x:x.value_counts().index[0])\na = df1.category.value_counts()\n\nimport squarify\nimport matplotlib\n\nMEDIUM_SIZE = 12.5\nBIGGER_SIZE = 23\n\nplt.rc('font', size=MEDIUM_SIZE)\nplt.rc('figure', titlesize=BIGGER_SIZE)\n# #Utilise matplotlib to scale our goal numbers between the min and max, then assign this scale to our values.\n\nnorm = matplotlib.colors.Normalize(vmin=a.values.min(), vmax=a.values.max())\ncolors = [matplotlib.cm.Blues(norm(value)) for value in a.values]\n\nlbl= np.array(a.index)+ \" \\n \" + a.values.astype(\"str\")\n\nplt.figure(figsize=(12,8))\nsquarify.plot(sizes=a.values[:20], label=lbl[0:20], alpha=0.7, color=colors)\nplt.axis('off')\nplt.title(\"News Category TreeMap\")\nplt.tight_layout()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_words = ' '.join([text for text in df1['text']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from wordcloud import WordCloud\n\nplt.figure(figsize=(12,10))\nwordcloud = WordCloud(width=800, height=500, random_state=21, \n                      max_font_size=110, background_color='white').generate(all_words)\n\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count Vectorizer"},{"metadata":{"trusted":false},"cell_type":"code","source":"vect = CountVectorizer(min_df=5, stop_words='english')\nX = vect.fit_transform(df1.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(df1.category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":false},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = nb.predict(X_test)\nprint(f'train score: {nb.score(X_train, y_train):.4f}')\nprint(f'test score: {nb.score(X_test, y_test):.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('KAPPA SCORE: ',cohen_kappa_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"cm_nb = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10,8))\nax = sns.heatmap(cm_nb, annot=True, fmt='0.0f', annot_kws={'fontsize':12})\nax.xaxis.set_ticklabels(df1.category.unique(), rotation=10, fontsize=12)\nax.yaxis.set_ticklabels(df1.category.unique(), rotation=0, fontsize=12)\nplt.show()"},{"metadata":{},"cell_type":"raw","source":"print(classification_report(y_test, y_pred))"},{"metadata":{},"cell_type":"markdown","source":"### GridSearchCV"},{"metadata":{},"cell_type":"raw","source":"# nb_gs = MultinomialNB().fit(X_train, y_train)\n\n\nfrom sklearn.pipeline import Pipeline\n\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', MultinomialNB())])\n\ntext_clf = text_clf.fit(X_train, y_train)\n\n\nparameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n              'tfidf__use_idf': (True, False),\n              'clf__alpha': (1e-2, 1e-3)}\n\n# nb_gs = MultinomialNB()\ngs_nb = GridSearchCV(text_clf, parameters, n_jobs=-1)\ngs_nb = gs_nb.fit(X_train, y_train)\ngs_nb.best_score_\ngs_nb.best_params_"},{"metadata":{"trusted":false},"cell_type":"code","source":"def predict_cat(title):\n    stop = stopwords.words('english')\n    if title:\n        title = title.replace('[^\\w\\s]','')\n        title = ' '.join(x for x in  title.split(' ') if x not in stop)\n        cod = nb.predict(vect.transform([title]))\n        return le.inverse_transform(cod)[0]\n    else:\n        print('text cannot be blank')\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"predict_cat(\"India’s largest ever ‘eye in the sky’ will take on its neighbours\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = svc.predict(X_test)\nprint(f'train score: {svc.score(X_train, y_train):.4f}')\nprint(f'test score: {svc.score(X_test, y_test):.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred_logreg = logreg.predict(X_test)\nprint(f'train score: {logreg.score(X_train, y_train):.4f}')\nprint(f'test score: {logreg.score(X_test, y_test):.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('KAPPA SCORE: ',cohen_kappa_score(y_test,y_pred_logreg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF Vectorizer"},{"metadata":{"trusted":false},"cell_type":"code","source":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(df1.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"le = LabelEncoder()\ny = le.fit_transform(df1.category)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes with TF-IDF Vectorizer"},{"metadata":{"trusted":false},"cell_type":"code","source":"nb_tfidf = MultinomialNB()\nnb_tfidf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)y_pred = svc.predict(X_test)\nprint(f'train score: {svc.score(X_train, y_train):.4f}')\nprint(f'test score: {svc.score(X_test, y_test):.4f}')y_pred_tfidf = nb_tfidf.predict(X_test)\nnb_tfidf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('KAPPA SCORE: ',cohen_kappa_score(y_test,y_pred_tfidf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}