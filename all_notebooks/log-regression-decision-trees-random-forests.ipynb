{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook I made a clasification of the types of glass depending on its components by using a Logistic Regression; however, as the prediction results were not amazing on the test sample, I continued to make the classification by using Decision Trees and Random Forests, which gave, unsurprisingly, better results.  "},{"metadata":{},"cell_type":"markdown","source":"First steps, importing the packages that will be used and the database:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn import preprocessing\n\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db = pd.read_csv('/kaggle/input/glass/glass.csv')\ndb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the idea is to try and determine the type by using all these components; let's see how many types there are:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"db['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"So there are six types but very unequal distribution in each one, so it's important to make the train and test databases split have the same structure as the original one; also, let's have a closer look at all the elements:"},{"metadata":{"trusted":true},"cell_type":"code","source":"db.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elements = []\n\nfor col in db.columns[0:8]:\n    \n    elements.append(col)\n    \n    \nx  = db['Type']\nfor elem in elements:\n    \n    y = db[elem]\n    print(plt.bar(x, y, align='center', alpha=0.5))\n    plt.title(elem)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking also the variables' distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\ndb.hist(bins=50,figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"The data distribution looks very far from normal for most of the variables, but after some research it seems it's preferred not to normalize it, so I will leave it as it is and go on to the train - test split and running the regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split as tts\nx_train = db.iloc[:,0:8]\ny_train = db['Type']\nx_train,x_test, y_train, y_test = tts(x_train,y_train, test_size = 0.2, random_state = 18,stratify=db[\"Type\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\ny_train = np.array(y_train)\n\nlog_reg = LogisticRegression(C = 1, max_iter = 50)\nlog_reg.fit(x_train,y_train.ravel())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now on to check the performance of the Logistic Regression on the test sample by using the classification report:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = log_reg.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_predict)\n\nfrom sklearn.metrics import classification_report\nclassification_report(y_test, y_predict)\ntarget_names = ['1', '2','3','5','6','7']\nprint(classification_report(y_test, y_predict, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Decision Trees"},{"metadata":{},"cell_type":"markdown","source":"So the accuracy is very low, at 0.6, so I will continue testing the Decision Trees algorithm to see what I'll get.\nFor this, I'l be creating a pipeline that will allow to find faster which parameters are the best,I'm playing on the max_depth and the criterion parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = preprocessing.StandardScaler()\ndtreeClf = tree.DecisionTreeClassifier()\n\n\npipe = Pipeline(steps=[('sc', sc),('dtreeClf', dtreeClf)])\n\ncriterion = ['gini', 'entropy']\nmax_depth = [4,5,6,7,8,10]\n\n\nparameters = dict(dtreeClf__criterion=criterion, dtreeClf__max_depth=max_depth)\nclf = GridSearchCV(pipe, parameters)\nbest = clf.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report #67 accuracy w. grid search\nclassification_report(y_test, y_predict)\ntarget_names = ['1', '2','3','5','6','7']\nprint(classification_report(y_test, y_predict, target_names=target_names)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now the accuracy is better, at 0.65, but not amazing, let's see what parameters it chose:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Random Forests"},{"metadata":{},"cell_type":"markdown","source":"Last but not least, let's test also Random Forests:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = preprocessing.StandardScaler()\n\nrandomforestClf = RandomForestClassifier()\n\n\npipe = Pipeline(steps=[('sc', sc),('randomforestClf', randomforestClf)])\n\nn_estimators = [500, 600, 550, 300, 200,100]\ncriterion = ['gini', 'entropy']\nmax_depth = [5, None]\nmin_samples_split = [0.005, 0.01]\nmax_features = [0.05 , 0.1]\n\n#parameters = dict(pca__n_components=n_components,dtreeClf__criterion=criterion, dtreeClf__max_depth=max_depth)\nparameters = dict(randomforestClf__criterion=criterion, randomforestClf__max_depth=max_depth,\n                 randomforestClf__n_estimators = n_estimators,randomforestClf__min_samples_split =min_samples_split,\n                 randomforestClf__max_features = max_features)\nclf = GridSearchCV(pipe, parameters)\nbest = clf.fit(x_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = clf.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's the how the Random Forest performed:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report \nclassification_report(y_test, y_predict)\ntarget_names = ['1', '2','3','5','6','7']\nprint(classification_report(y_test, y_predict, target_names=target_names)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So at 0.77 accuracy it's the best result until now, based on this it's recommended to use Random Forests for this database."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}