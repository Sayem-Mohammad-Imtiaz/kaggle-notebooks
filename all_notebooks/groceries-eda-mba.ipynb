{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing relevant libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport math\nfrom scipy import special #comb, factorial\nfrom keras import backend as K\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score\n\n\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/groceries-dataset/Groceries_dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basic info about the dataset"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do we have nulls?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No, we don't"},{"metadata":{},"cell_type":"markdown","source":"# How many unique shopping items do we have?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['itemDescription'].unique().size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# When were the purchases made?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df['Date'] = pd.to_datetime(df['Date'])\nplt.figure(figsize=(10,7))\nsns.countplot(df['Date'].apply(lambda x: x.year))\nplt.title('When were the purchases made?')\nplt.xlabel('Year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the dataset contains the purchases made only in 2014 and 2015. Now let's see more specific distribution"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\ndf3 = df.groupby('Date').count()['itemDescription'].reset_index()\nplt.plot(df3['Date'],df3['itemDescription'])\nplt.xlabel('Date')\nplt.ylabel('Number of items bought')\nplt.title('Number of items sold (each day)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's reduce the noise by consider total count of items bougth **each month**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\ndf3 = df.copy()\ndf3['Date'] = df3['Date'].apply(lambda x: pd.to_datetime(f\"{x.year}/{x.month}/{1}\"))\n\n\ndf3 = df3.groupby('Date').count()['itemDescription'].reset_index()\nplt.plot(df3['Date'],df3['itemDescription'])\nplt.xlabel('Date')\nplt.ylabel('Number of items bought')\nplt.title('Number of items sold (each month)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get a better idea of whether there is some yearly trend, we will adjust the graph as follows:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df3 = df.copy()\ndf3['Year'] = df['Date'].apply(lambda x: x.year)\ndf3['Month'] = df['Date'].apply(lambda x: x.month)\ndf3.drop(['Member_number','Date'],axis=1,inplace=True)\ndf3 = df3.groupby(['Year','Month']).count().reset_index()\n\n\n\nd_2014 = df3[df3['Year'] == 2014]\nd_2015 = df3[df3['Year'] == 2015]\n\n\nplt.figure(figsize=(10,7))\nplt.plot(d_2014['Month'],d_2014['itemDescription'],label='2014')\nplt.plot(d_2015['Month'],d_2015['itemDescription'],label='2015')\nplt.title('Number of items sold (each month)')\nplt.xlabel('Month')\nplt.ylabel('item count')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"corr=d_2014.merge(right=d_2015,on='Month')[['itemDescription_x','itemDescription_y']].corr().values[0][1]\nprint(f'Correlation between sales in 2014 and 2015: {corr}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Couple notes can be made here:\n1. Besides February, sales in each month of 2015 were higher than in the same month of 2014.\n2. There doesn't seem to be any yearly trend: correlation between the sales in 2014 and 2015 is roughly 1%."},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the sales per each item"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1 = df.groupby('itemDescription').count().sort_values(by='Member_number',ascending=False).reset_index()\ndf1.rename(columns={'itemDescription': 'Item',\n                   'Member_number': 'Number of sales'},inplace=True)\ndf1.drop(['Date'],axis=1,inplace=True)\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary of the `Number of sales`"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1['Number of sales'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the histogram plotting the distribution of the `Number of sales`"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\ndf1['Number of sales'].hist(alpha=0.6)\nplt.xlabel('sales count')\nplt.ylabel('item count')\nplt.title(\"How many times each item was sold?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that half of the items were purchased less than $86$ times. However, there are some outliers. Let's have a look at what the outliers are."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1 = df.groupby('itemDescription').count().sort_values(by='Member_number',ascending=False).head(10).reset_index()\ndf1.drop(['Date'],axis=1,inplace=True)\ndf1.rename(columns={'itemDescription': 'Item',\n                   'Member_number' : 'Number of sales'},inplace=True)\nfig = px.bar(df1,\n             x='Item',\n             y='Number of sales',\n             title= 'Most purchased items')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's have a look at the customers that bought the most items."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1 = df.groupby('Member_number').count().sort_values(by='itemDescription',ascending=False).head(10).reset_index()\ndf1.drop(['Date'],axis=1,inplace=True)\ndf1.rename(columns={'itemDescription': 'Item count',\n                   'Member_number' : 'Customer ID'},inplace=True)\ndf1['Customer ID'] = df1['Customer ID'].astype(str)\nfig = px.bar(df1,\n             x='Customer ID',\n             y='Item count',\n             title='Customers that purchased the most items')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As things stand, our data is organized in a way that we can not determine how many items (and what items exactly) each customer bought PER EACH VISIT to the store. For example, have a look at the following table"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df[df['Member_number'] == 1000].sort_values(by='Date').head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the customer with an ID 1000 bought 3 items on July 4, 2014. The problem is though, we don't know how many times he went to the store, and what he bought per each visit. For example, it is possible that he went to the store 3 times, each time buying single item. It is also possible that he went to the store 2 times, the first time he bought pastry and snack, and the second time he bought milk. Or maybe he went to the store only once, and bought all 3 items at once. Which one is it? We don't know. But to perform any meaningful association analysis, we need to know. Since no information was provided, we will make the following assumption:\n\n> **Assumption**. On each day, arbitrary customer went to the store ONLY ONCE.\n\n\nWith this assumption in mind, we can reorganize the dataset in a following way:\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df1 = df.copy()\ndf1['itemDescription'] = df1['itemDescription'].apply(lambda x: [x,]).copy()\ndf1 = df1.groupby(['Member_number','Date']).agg(sum).reset_index()\ndf1.rename(columns={'itemDescription': 'Items bought'},inplace=True)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Items bought` now represents the set of all items which were bought during a single visit to the store."},{"metadata":{},"cell_type":"markdown","source":"Let's see how many items customers purchase per each visit to the store."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1['Basket size'] = df1['Items bought'].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df1['Basket size'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\ndf1['Basket size'].hist(alpha=0.6)\nplt.xlabel('item count')\nplt.ylabel('customer count')\nplt.title(\"How many items are being purchased each visit?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that most customers purchase 2 items per each visit to the store."},{"metadata":{},"cell_type":"markdown","source":"Now we will use [Association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) to check whether there are some patterns in the customers purchasing behavior. To generate set of relevant rules, we will use [Apriori algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm). Before proceeding, make sure that you are fimiliar with following concepts: association rule, support, confidence, lift (information regarding all of these concepts can be found in the [here](https://en.wikipedia.org/wiki/Association_rule_learning) (see section \"Useful Concepts\")).\n\n\nWe will only generate rules where support exceeds $0.1\\%$ and confidence exceeds $10\\%$\n    "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport mlxtend as ml\n\n\ndf1 = df.copy()\ndf1['itemDescription'] = df1['itemDescription'].apply(lambda x: [x,]).copy()\ndf1 = df1.groupby(['Member_number','Date']).agg(sum).reset_index()\ndf1.rename(columns={'itemDescription': 'Items bought'},inplace=True)\n\n\n\nall_items = df['itemDescription'].unique()\ndata = []\n\n\nfor transaction in df1['Items bought']:\n    row = []\n    for item in all_items:\n        if item in transaction:\n            row.append(1)\n        else:\n            row.append(0)\n    data.append(row)\n\ndf2 = pd.DataFrame(data,columns=all_items)    \ndf2 = df2.rename_axis('Transcation ID')\n            \n\nfrequent_itemsets = apriori(df2, min_support=0.001, use_colnames=True)\nrules = association_rules(frequent_itemsets, metric=\"lift\")\nrules.sort_values('confidence', ascending = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rules = rules[rules['confidence'] > 0.1].copy()\nrules.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how many assofication rules we are dealing with:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rows = rules.shape[0]\nprint(f'Number of rules: {rows}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sstatistics summary regarding `support`:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rules['support'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the support for all rules in our dataset is very low (i.e., the proportion of transactions that involve items from both baskets), which may be problematic, due to the fact that any results obtained from analysis may not be statistically significant."},{"metadata":{},"cell_type":"markdown","source":"Let's see the rules with the highest lift"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rules.sort_values(by='lift',ascending=False).head(10).iloc[:,:-2][['antecedents',\n                                                                  'consequents',\n                                                                   'consequent support',\n                                                                  'lift']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We see that itemsets (yogurt, whole milk) and (sausage) have the highest lift. That means that once we know that a customer has bought yogurt and whole milk, it becomes 2.2 times more likely that he will also buy sausage. But as we've pointed out, due to the fact that the support is very low, we cannot really determine whether this is just a fluke or a real association."},{"metadata":{},"cell_type":"markdown","source":"Similarly, we will check the the rules with the lower lift (i.e., rules where the items in antecedent and consequent are unlikely to be bought together)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rules.sort_values(by='lift',ascending=True).head(10).iloc[:,:-2][['antecedents',\n                                                                  'consequents',\n                                                                   'consequent support',\n                                                                  'lift']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the relation between support, confidence and lift"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sup = rules['support'].values\nconf = rules['confidence'].values\nlift = rules['lift'].values\n\nplt.figure(figsize=(10,6))\nsc = plt.scatter(sup,conf,c=lift)\nplt.colorbar(sc,label='Lift')\nplt.xlabel('support')\nplt.ylabel('confidence')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, most rules have both low confidence and low support. However, there is one rule that we may want to single out: the rule with the confidence of about $26\\%$:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rules[rules['confidence'] > 0.24]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the probability of a customer buying milk is roughly $16\\%$. But given that the customer has bougth yougurt and sausage, the probability of buying milk increases to $25\\%$ (this also signifies that the rule has a high lift). But again, support is very low (only $0.1\\%$). So while this rule seems to be the most promising out  of rules our dataset contains, due to the fact that the support is very low, one should not make any rash conclusions about the association between the two itemsets without further investigation. How could one investigate? One  option is to come up with a way to directly (or indirectly) ask those customers who bought yogurt, sausage and whole milk about why they buy these items together."},{"metadata":{},"cell_type":"markdown","source":"Based on the graph above, one could also see some rules with a relatively decent support (i.e., support exceeds $1\\%$), let's check what those rules are:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"rules[rules['support'] > 0.01]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not only confidence is low for both of these rules, but the lift is actually less than 1. For example, the probability of a customer buying whole milk is about $16\\%$. But if we know that the customer bought yogurt, then the probability of buying whole milk drops to $13\\%$. The upshot is: these rules are not of much interest to us."},{"metadata":{},"cell_type":"markdown","source":"### Conclusions:\n\n1. The dataset contains transactions made in 2014 and 2015.\n2. In each month of 2015 (besides February), the sales (i.e, total count of items sold) grew from a year earlier. Furthermore, the correlation between the sales of 2014 and 2015 is low (around 1%), which implies there is no yearly sales trend.\n3. The dataset contains 167 unique shopping items.\n4. Over the span of two years, half of the shopping items were bought less than 85 items each. The most popular items (i.e, those items that were bought more than 1k times) are:\n\n    - whole milk (bought 2502 times)\n    - other vegetables (bought 1898 times)\n    - rolls/buns (bought 1716 times)\n    - soda (bought 1514 times)\n    - yogurt (bought 1334 times)\n    - root vegetables (bought 1071 times)\n    - tropical fruit (bought 1032 times)\n5. Per each visit to the store, half of the customers only purchases 2 items (or less), and 95% of the customers purchase 5 items (or less).\n\n6. Using Apriori algorithm (AA), we've found that most rules have very low support (which implies that even if one finds one rule to have a strong association, the association might not be statistically significant). Furthermore, using AA we've found that most rules have low confidence and low lift (which signifies a weak association). The only rule which may have a meanignful association is $$\\text{(yogurt, sausage)} \\implies \\text{(whole milk)}$$\nthe reason why the rule may be meaningful is because of the highest confidence out of all rules ($\\approx 26\\%$), and one of the highest lifts ($\\approx 1.6$). This means that, once we know the customer has purchased yogurt and sausage, the probability of the customer also buying whole milk significantly increases. However, given that the support is low ($\\approx 0.1\\%$), one should be careful before making any conclusions about whether the association is significant."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}