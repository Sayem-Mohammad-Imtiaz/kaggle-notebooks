{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#or dirname, _, filenames in os.walk('/kaggle/input'):\n  # for filename in filenames:\n     #  print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, models, transforms\nimport torch.nn.functional as func\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import TensorDataset,DataLoader\nfrom torch import optim\nfrom torch import device as dev\nfrom sklearn.metrics import classification_report\nimport torch.utils.data as tdata\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nimport nltk\nfrom collections import Counter\nimport itertools\nfrom tqdm import tqdm_notebook\nfrom sklearn import model_selection\nprint(os.listdir(\"../input/imdb-review-dataset\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, itos, unk_index):\n        self._itos = itos\n        self._stoi = {word:i for i, word in enumerate(itos)}\n        self._unk_index = unk_index\n        \n    def __len__(self):\n        return len(self._itos)\n    \n    def word2id(self, word):\n        idx = self._stoi.get(word)\n        if idx is not None:\n            return idx\n        return self._unk_index\n    \n    def id2word(self, idx):\n        return self._itos[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SimpleTextTransformer:\n    def __init__(self, max_vocabulary_size):\n        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n        self.unk_index = 1\n        self.pad_index = 0\n        self.vocabulary = None\n        self.max_vocabulary_size = max_vocabulary_size\n        \n    def tokenize(self, text):\n        return nltk.tokenize.word_tokenize(text.lower())\n        \n    def build_vocabulary(self, tokens):\n        itos = []\n        itos.extend(self.special_words)\n        \n        token_counts = Counter(tokens)\n        for word, _ in token_counts.most_common(self.max_vocabulary_size - len(self.special_words)):\n            itos.append(word)\n            \n        self.vocabulary = Vocabulary(itos, self.unk_index)\n    \n    def transform(self, texts):\n        result = []\n        for text in texts:\n            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n            ids = [self.vocabulary.word2id(token) for token in tokens]\n            result.append(ids)\n        return result\n    \n    def fit_transform(self, texts):\n        result = []\n        tokenized_texts = [self.tokenize(text) for text in texts]\n        self.build_vocabulary(itertools.chain(*tokenized_texts))\n        for tokens in tokenized_texts:\n            tokens = ['<S>'] + tokens + ['</S>']\n            ids = [self.vocabulary.word2id(token) for token in tokens]\n            result.append(ids)\n        return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 9999\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nclass InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, label_id):\n        self.input_ids = input_ids\n        self.label_id = label_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n    return InputFeatures(ids, label_encoding[label])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_to_tensor(list_of_features):\n    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n    return text_tensor, labels_tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imdb_df = pd.read_csv('../input/imdb-review-dataset', encoding='latin-1', engine='python')\nimdb_df = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv', encoding='latin-1')\ndev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\ntest_df = imdb_df[(imdb_df.type == 'test')]\ntrain_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)\nmax_seq_len=200\nclasses = {'neg': 0, 'pos' : 1}\ntext2id = SimpleTextTransformer(10000)\n\ntrain_ids = text2id.fit_transform(train_df['review'])\nval_ids = text2id.transform(val_df['review'])\ntest_ids = text2id.transform(test_df['review'])\n\nprint(train_df.review.iloc[0][:160])\nprint(train_ids[0][:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(train_ids, train_df['label'])]\n\nval_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(val_ids, val_df['label'])]\n\ntest_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n                  for token_ids, label in zip(test_ids, test_df['label'])]\n\nprint(train_features[3].input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)\n\nprint(val_tensor[:2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(text2id.vocabulary))\nvocabulary_size = len(text2id.vocabulary) + 1\nprint(train_tensor.size())\nprint(train_labels.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TensorDataset(train_tensor, train_labels)\nval_dataset = TensorDataset(val_tensor, val_labels)\ntest_dataset = TensorDataset(test_tensor, test_labels)\n\nprint(train_dataset[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size = 50)\nval_loader = DataLoader(val_dataset, batch_size = 50)\ntest_loader = DataLoader(test_dataset, batch_size = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BestModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embed = nn.Embedding(vocabulary_size, 50, padding_idx = 0)\n        self.lstm = nn.LSTM(50, 500, batch_first = True)\n        self.linear  = nn.Linear(500, 1)\n        \n    def forward(self, x):\n        x = self.embed(x)\n        x, (hn, cn) = self.lstm(x)\n        hn = hn.view(hn.size()[1:])\n        out = self.linear(hn)\n        sig = torch.sigmoid(out)\n        sig = sig.view(-1)\n\n        return sig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BestModel().cuda()\noptimizer = optim.Adam(model.parameters(), lr = 2e-3)\ncriterion = nn.BCELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, train_loader, val_loader, optimizer, criterion, epochs, tries):\n    \n    val_loss_best = 100\n    check = 0\n    \n    for epoche in range(epochs):\n        model.train()\n        val_loss = 0\n        epoch_loss = 0\n        \n        for xx, yy in train_loader:            \n            xx = xx.cuda()\n            yy = yy.cuda()\n            optimizer.zero_grad()\n            pred = model.forward(xx)\n            loss = criterion(pred, yy.type(torch.float32))\n            epoch_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n        epoch_loss /= len(train_loader)\n        with torch.no_grad():\n            model.eval()\n            for xx,yy in val_loader:\n                xx = xx.cuda()\n                yy = yy.cuda()\n                pred = model.forward(xx)\n                loss = criterion(pred, yy.type(torch.float32))\n                val_loss += loss.item()\n            val_loss /= len(val_loader)\n            print(\"Epoch = \", epoche, \", Epoch_loss = \", epoch_loss, \", Val_loss = \", val_loss)\n            \n            if val_loss < val_loss_best:\n                check = 0\n                torch.save(model.state_dict(), \"../best_model.md\")\n                val_loss_best = val_loss\n            else:\n                check += 1\n                print(\"Acceptable error\", check)\n                if check == tries:\n                    print(\"Model trained\")   \n                    break\n                    \n    model.load_state_dict(torch.load(\"../best_model.md\"))   \n    model.eval()\n    model.cpu()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(model, train_loader, val_loader, optimizer, criterion, epochs = 100, tries = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nmodel.eval()\npreds = []\ntrue = []\nfor xx,yy in test_loader:\n    xx = xx.cuda()\n    model.cuda()\n    pred = model.forward(xx)\n    p = pred.tolist()\n    j = 0\n    for a in p:\n        if a > 0.5:\n            p[j] = 1\n        else:\n            p[j] = 0\n        j = j + 1\n    \n    preds.extend(p)\n    true.extend(yy.tolist())\n    target_names = ['pos', 'neg']\nprint(classification_report(true, preds, target_names=target_names))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}