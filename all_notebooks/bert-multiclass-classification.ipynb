{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport re\nimport gc\nimport os\nprint(os.listdir(\"../input\"))\nimport fileinput\nimport string\nimport tensorflow as tf\nimport zipfile\nfrom datetime import datetime\nimport datetime\nimport datetime\nprint(datetime.__file__)\nimport sys\nfrom tqdm  import tqdm\ntqdm.pandas()\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-11T16:11:16.955227Z","iopub.execute_input":"2021-07-11T16:11:16.95554Z","iopub.status.idle":"2021-07-11T16:11:19.170982Z","shell.execute_reply.started":"2021-07-11T16:11:16.955478Z","shell.execute_reply":"2021-07-11T16:11:19.169984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT implementation","metadata":{"_uuid":"64f2c3a0c296d5bb724b405d18a8df438db444da"}},{"cell_type":"code","source":"# !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py ","metadata":{"_uuid":"02a6443556928bda3f2527b7d803361de3e8e518","execution":{"iopub.status.busy":"2021-07-11T16:11:19.175193Z","iopub.execute_input":"2021-07-11T16:11:19.177377Z","iopub.status.idle":"2021-07-11T16:11:22.770171Z","shell.execute_reply.started":"2021-07-11T16:11:19.175457Z","shell.execute_reply":"2021-07-11T16:11:22.769518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:22.771596Z","iopub.execute_input":"2021-07-11T16:11:22.771875Z","iopub.status.idle":"2021-07-11T16:11:22.794012Z","shell.execute_reply.started":"2021-07-11T16:11:22.771827Z","shell.execute_reply":"2021-07-11T16:11:22.793444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"2b1a537730f8d6a59633a18e18443e422a3c7281","execution":{"iopub.status.busy":"2021-07-11T16:11:22.795144Z","iopub.execute_input":"2021-07-11T16:11:22.795408Z","iopub.status.idle":"2021-07-11T16:11:22.829154Z","shell.execute_reply.started":"2021-07-11T16:11:22.795363Z","shell.execute_reply":"2021-07-11T16:11:22.828482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bert-tensorflow==1.0.1\n!pip install --upgrade pip","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:22.830208Z","iopub.execute_input":"2021-07-11T16:11:22.830592Z","iopub.status.idle":"2021-07-11T16:11:37.187466Z","shell.execute_reply.started":"2021-07-11T16:11:22.830464Z","shell.execute_reply":"2021-07-11T16:11:37.186549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/ember-for-static-malware-analysis/train_metadata.csv')\ndf_test = pd.read_csv('../input/ember-for-static-malware-analysis/test_metadata.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:37.189054Z","iopub.execute_input":"2021-07-11T16:11:37.189375Z","iopub.status.idle":"2021-07-11T16:11:40.121591Z","shell.execute_reply.started":"2021-07-11T16:11:37.189321Z","shell.execute_reply":"2021-07-11T16:11:40.120859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_fake = pd.read_csv('../input/gan-fake/fake_data-15000.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:40.122991Z","iopub.execute_input":"2021-07-11T16:11:40.123275Z","iopub.status.idle":"2021-07-11T16:11:40.179571Z","shell.execute_reply.started":"2021-07-11T16:11:40.123227Z","shell.execute_reply":"2021-07-11T16:11:40.178951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.concat([df_fake, df_train])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:40.180901Z","iopub.execute_input":"2021-07-11T16:11:40.181572Z","iopub.status.idle":"2021-07-11T16:11:40.290586Z","shell.execute_reply.started":"2021-07-11T16:11:40.181427Z","shell.execute_reply":"2021-07-11T16:11:40.289758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:40.292019Z","iopub.execute_input":"2021-07-11T16:11:40.292477Z","iopub.status.idle":"2021-07-11T16:11:40.328341Z","shell.execute_reply.started":"2021-07-11T16:11:40.292289Z","shell.execute_reply":"2021-07-11T16:11:40.327242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_names = train_df[ train_df['label'] == -1 ].index\ntrain_df.drop(index_names, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:40.329468Z","iopub.execute_input":"2021-07-11T16:11:40.329924Z","iopub.status.idle":"2021-07-11T16:11:41.269706Z","shell.execute_reply.started":"2021-07-11T16:11:40.329797Z","shell.execute_reply":"2021-07-11T16:11:41.268851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop(train_df.index[-196000:], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:41.271165Z","iopub.execute_input":"2021-07-11T16:11:41.271659Z","iopub.status.idle":"2021-07-11T16:11:42.052447Z","shell.execute_reply.started":"2021-07-11T16:11:41.271429Z","shell.execute_reply":"2021-07-11T16:11:42.051626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:42.053855Z","iopub.execute_input":"2021-07-11T16:11:42.05431Z","iopub.status.idle":"2021-07-11T16:11:42.114343Z","shell.execute_reply.started":"2021-07-11T16:11:42.054115Z","shell.execute_reply":"2021-07-11T16:11:42.113385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:42.115675Z","iopub.execute_input":"2021-07-11T16:11:42.116112Z","iopub.status.idle":"2021-07-11T16:11:42.132917Z","shell.execute_reply.started":"2021-07-11T16:11:42.115915Z","shell.execute_reply":"2021-07-11T16:11:42.132054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.drop(df_test.index[:-50000], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:42.134396Z","iopub.execute_input":"2021-07-11T16:11:42.13484Z","iopub.status.idle":"2021-07-11T16:11:42.189938Z","shell.execute_reply.started":"2021-07-11T16:11:42.134752Z","shell.execute_reply":"2021-07-11T16:11:42.189164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:42.191044Z","iopub.execute_input":"2021-07-11T16:11:42.191476Z","iopub.status.idle":"2021-07-11T16:11:42.217185Z","shell.execute_reply.started":"2021-07-11T16:11:42.191282Z","shell.execute_reply":"2021-07-11T16:11:42.216631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_COLUMN = 'sha256'\nLABEL_COLUMN = 'label'\n\nlabel_list = [0, 1]","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:42.218936Z","iopub.execute_input":"2021-07-11T16:11:42.219385Z","iopub.status.idle":"2021-07-11T16:11:42.223287Z","shell.execute_reply.started":"2021-07-11T16:11:42.219335Z","shell.execute_reply":"2021-07-11T16:11:42.22235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_InputExamples = train_df.apply(lambda x: run_classifier.InputExample(guid=None,\n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)\n\nval_InputExamples = train_df.apply(lambda x: run_classifier.InputExample(guid=None, \n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:11:42.224741Z","iopub.execute_input":"2021-07-11T16:11:42.225202Z","iopub.status.idle":"2021-07-11T16:12:06.716596Z","shell.execute_reply.started":"2021-07-11T16:11:42.2251Z","shell.execute_reply":"2021-07-11T16:12:06.715761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_InputExamples\nval_InputExamples","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:12:06.717943Z","iopub.execute_input":"2021-07-11T16:12:06.718194Z","iopub.status.idle":"2021-07-11T16:12:06.726752Z","shell.execute_reply.started":"2021-07-11T16:12:06.718148Z","shell.execute_reply":"2021-07-11T16:12:06.725947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n\ndef create_tokenizer_from_hub_module():\n \n  with tf.Graph().as_default():\n    bert_module = hub.Module(BERT_MODEL_HUB)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    with tf.Session() as sess:\n      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n                                            tokenization_info[\"do_lower_case\"]])\n      \n  return tokenization.FullTokenizer(\n      vocab_file=vocab_file, \n      do_lower_case=do_lower_case)\n\ntokenizer = create_tokenizer_from_hub_module()","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:12:06.728008Z","iopub.execute_input":"2021-07-11T16:12:06.728499Z","iopub.status.idle":"2021-07-11T16:12:35.52701Z","shell.execute_reply.started":"2021-07-11T16:12:06.728447Z","shell.execute_reply":"2021-07-11T16:12:35.526196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:12:35.536657Z","iopub.execute_input":"2021-07-11T16:12:35.536901Z","iopub.status.idle":"2021-07-11T16:12:35.544521Z","shell.execute_reply.started":"2021-07-11T16:12:35.536852Z","shell.execute_reply":"2021-07-11T16:12:35.543405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQ_LENGTH = 80\n\n\ntrain_features = run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\nval_features = run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:12:35.545859Z","iopub.execute_input":"2021-07-11T16:12:35.546445Z","iopub.status.idle":"2021-07-11T16:38:35.988456Z","shell.execute_reply.started":"2021-07-11T16:12:35.546323Z","shell.execute_reply":"2021-07-11T16:38:35.987578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features\nval_features","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:35.989771Z","iopub.execute_input":"2021-07-11T16:38:35.99008Z","iopub.status.idle":"2021-07-11T16:38:36.027638Z","shell.execute_reply.started":"2021-07-11T16:38:35.990027Z","shell.execute_reply":"2021-07-11T16:38:36.026963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Sentence : \", train_InputExamples.iloc[0].text_a)\nprint(\"-\"*30)\nprint(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\nprint(\"-\"*30)\nprint(\"Input IDs : \", train_features[0].input_ids)\nprint(\"-\"*30)\nprint(\"Input Masks : \", train_features[0].input_mask)\nprint(\"-\"*30)\nprint(\"Segment IDs : \", train_features[0].segment_ids)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.028721Z","iopub.execute_input":"2021-07-11T16:38:36.029105Z","iopub.status.idle":"2021-07-11T16:38:36.04283Z","shell.execute_reply.started":"2021-07-11T16:38:36.029057Z","shell.execute_reply":"2021-07-11T16:38:36.040661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n                 num_labels):\n  \n  bert_module = hub.Module(\n      BERT_MODEL_HUB,\n      trainable=True)\n  bert_inputs = dict(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids)\n  bert_outputs = bert_module(\n      inputs=bert_inputs,\n      signature=\"tokens\",\n      as_dict=True)\n\n  # Use \"pooled_output\" for classification tasks on an entire sentence.\n  # Use \"sequence_outputs\" for token-level output.\n  output_layer = bert_outputs[\"pooled_output\"]\n\n  hidden_size = output_layer.shape[-1].value\n\n  # Create our own layer to tune for politeness data.\n  output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(\"loss\"):\n\n    # Dropout helps prevent overfitting\n    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    # Convert labels into one-hot encoding\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n    # If we're predicting, we want predicted labels and the probabiltiies.\n    if is_predicting:\n      return (predicted_labels, log_probs)\n\n    # If we're train/eval, compute loss between predicted and actual label\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, predicted_labels, log_probs)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.045687Z","iopub.execute_input":"2021-07-11T16:38:36.046233Z","iopub.status.idle":"2021-07-11T16:38:36.058673Z","shell.execute_reply.started":"2021-07-11T16:38:36.0461Z","shell.execute_reply":"2021-07-11T16:38:36.057715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_fn_builder(num_labels, learning_rate, num_train_steps,\n                     num_warmup_steps):\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]\n    label_ids = features[\"label_ids\"]\n\n    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n    \n    # TRAIN and EVAL\n    if not is_predicting:\n\n      (loss, predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      train_op = optimization.create_optimizer(\n          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n\n      # Calculate evaluation metrics. \n      def metric_fn(label_ids, predicted_labels):\n        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n        true_pos = tf.metrics.true_positives(\n            label_ids,\n            predicted_labels)\n        true_neg = tf.metrics.true_negatives(\n            label_ids,\n            predicted_labels)   \n        false_pos = tf.metrics.false_positives(\n            label_ids,\n            predicted_labels)  \n        false_neg = tf.metrics.false_negatives(\n            label_ids,\n            predicted_labels)\n        \n        return {\n            \"eval_accuracy\": accuracy,\n            \"true_positives\": true_pos,\n            \"true_negatives\": true_neg,\n            \"false_positives\": false_pos,\n            \"false_negatives\": false_neg\n            }\n\n      eval_metrics = metric_fn(label_ids, predicted_labels)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(mode=mode,\n          loss=loss,\n          train_op=train_op)\n      else:\n          return tf.estimator.EstimatorSpec(mode=mode,\n            loss=loss,\n            eval_metric_ops=eval_metrics)\n    else:\n      (predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      predictions = {\n          'probabilities': log_probs,\n          'labels': predicted_labels\n      }\n      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n  # Return the actual model function in the closure\n  return model_fn","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.06007Z","iopub.execute_input":"2021-07-11T16:38:36.060408Z","iopub.status.idle":"2021-07-11T16:38:36.074288Z","shell.execute_reply.started":"2021-07-11T16:38:36.060334Z","shell.execute_reply":"2021-07-11T16:38:36.073643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 2.0\n\n# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\nWARMUP_PROPORTION = 0.1\n\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 450\nSAVE_SUMMARY_STEPS = 100\n\n# Compute train and warmup steps from batch size\nnum_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\n# Specify output directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir='./bert-multiclass-sentiment',\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n\nnum_train_steps\nnum_warmup_steps","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.077056Z","iopub.execute_input":"2021-07-11T16:38:36.07729Z","iopub.status.idle":"2021-07-11T16:38:36.090287Z","shell.execute_reply.started":"2021-07-11T16:38:36.077243Z","shell.execute_reply":"2021-07-11T16:38:36.089585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_train_steps","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.091901Z","iopub.execute_input":"2021-07-11T16:38:36.092401Z","iopub.status.idle":"2021-07-11T16:38:36.101222Z","shell.execute_reply.started":"2021-07-11T16:38:36.092175Z","shell.execute_reply":"2021-07-11T16:38:36.100551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fn = model_fn_builder(\n  num_labels=len(label_list),\n  learning_rate=LEARNING_RATE,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps)\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": BATCH_SIZE})","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.103157Z","iopub.execute_input":"2021-07-11T16:38:36.103423Z","iopub.status.idle":"2021-07-11T16:38:36.114334Z","shell.execute_reply.started":"2021-07-11T16:38:36.103381Z","shell.execute_reply":"2021-07-11T16:38:36.113404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=False)\nval_input_fn = run_classifier.input_fn_builder(\n    features=val_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.1157Z","iopub.execute_input":"2021-07-11T16:38:36.116193Z","iopub.status.idle":"2021-07-11T16:38:36.525963Z","shell.execute_reply.started":"2021-07-11T16:38:36.115978Z","shell.execute_reply":"2021-07-11T16:38:36.525109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nprint(datetime.__file__)\nimport datetime\ndatetime.datetime.now()\n!python --version\nprint(tf.__version__)\n# !pip freeze","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:36.527277Z","iopub.execute_input":"2021-07-11T16:38:36.52771Z","iopub.status.idle":"2021-07-11T16:38:37.232227Z","shell.execute_reply.started":"2021-07-11T16:38:36.527538Z","shell.execute_reply":"2021-07-11T16:38:37.231403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Beginning Training!')\ncurrent_time = datetime.datetime.now()\nestimator.train(input_fn=train_input_fn, max_st eps=num_train_steps)\nprint(\"Training took time \", datetime.datetime.now() - current_time)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T16:38:37.233823Z","iopub.execute_input":"2021-07-11T16:38:37.234127Z","iopub.status.idle":"2021-07-11T19:53:21.979565Z","shell.execute_reply.started":"2021-07-11T16:38:37.234078Z","shell.execute_reply":"2021-07-11T19:53:21.977921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator.evaluate(input_fn=val_input_fn, steps=None)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T19:53:21.982083Z","iopub.execute_input":"2021-07-11T19:53:21.982402Z","iopub.status.idle":"2021-07-11T20:25:39.398722Z","shell.execute_reply.started":"2021-07-11T19:53:21.982344Z","shell.execute_reply":"2021-07-11T20:25:39.397803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getPrediction(in_sentences):\n  #A list to map the actual labels to the predictions\n  labels = [\"Benign\", \"Malicious\"]\n\n  #Transforming the test data into BERT accepted form\n  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] \n  \n  #Creating input features for Test data\n  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\n  #Predicting the classes \n  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n  predictions = estimator.predict(predict_input_fn)\n  return [(sentence, prediction['probabilities'],prediction['labels'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]","metadata":{"execution":{"iopub.status.busy":"2021-07-11T20:25:39.40035Z","iopub.execute_input":"2021-07-11T20:25:39.400672Z","iopub.status.idle":"2021-07-11T20:25:39.414942Z","shell.execute_reply.started":"2021-07-11T20:25:39.400617Z","shell.execute_reply":"2021-07-11T20:25:39.414159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_sentences = list(df_test['sha256'])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T20:25:39.418141Z","iopub.execute_input":"2021-07-11T20:25:39.418786Z","iopub.status.idle":"2021-07-11T20:25:39.466918Z","shell.execute_reply.started":"2021-07-11T20:25:39.418729Z","shell.execute_reply":"2021-07-11T20:25:39.466054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = getPrediction(pred_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T20:25:39.471541Z","iopub.execute_input":"2021-07-11T20:25:39.475986Z","iopub.status.idle":"2021-07-11T20:31:09.040062Z","shell.execute_reply.started":"2021-07-11T20:25:39.475925Z","shell.execute_reply":"2021-07-11T20:31:09.039299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc_labels = []\nact_labels = []\nfor i in range(len(predictions)):\n  enc_labels.append(predictions[i][2])\n  act_labels.append(predictions[i][3])","metadata":{"execution":{"iopub.status.busy":"2021-07-11T20:31:09.044195Z","iopub.execute_input":"2021-07-11T20:31:09.044458Z","iopub.status.idle":"2021-07-11T20:31:09.075771Z","shell.execute_reply.started":"2021-07-11T20:31:09.044408Z","shell.execute_reply":"2021-07-11T20:31:09.07498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2021-07-11T20:31:09.077011Z","iopub.execute_input":"2021-07-11T20:31:09.077443Z","iopub.status.idle":"2021-07-11T20:31:09.384787Z","shell.execute_reply.started":"2021-07-11T20:31:09.077271Z","shell.execute_reply":"2021-07-11T20:31:09.383992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(enc_labels, columns = ['label']).to_excel('./bert-multiclass-sentiment/bert-test-result2.xlsx', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-11T20:31:09.385799Z","iopub.execute_input":"2021-07-11T20:31:09.386082Z","iopub.status.idle":"2021-07-11T20:31:11.041943Z","shell.execute_reply.started":"2021-07-11T20:31:09.386032Z","shell.execute_reply":"2021-07-11T20:31:11.04105Z"},"trusted":true},"execution_count":null,"outputs":[]}]}