{"cells":[{"metadata":{"_cell_guid":"8520f698-8ae9-4158-aed7-91dfc8fc64c7","_uuid":"19e60a219b1b879152695b362641fd0529517ea4"},"cell_type":"markdown","source":"# TSA Claims Classification (Approve / Settle / Deny)  \n## (Part 1)\n\n## Overview\nIn this kernel, we will work toward building a model to predict whether a TSA claim is approved, settled, or denied. This is mostly a practice exercise, but it could have some very neat real-world uses!   \n  \nFor example, a travel insurance company could use it to help predict losses or adjust reimbursement. The TSA could also use a predictive model to help triage incoming claims, depending on the predicted class and the confidence of the prediction. \n\nNote: In all of these use cases, it would be ideal if we can predict the result using only information that is known before the claim is filed. Certain fields in the data set (like Disposition, Close Value) are generated after-the-fact so we will exclude them from the modeling analysis. \n\n## Workflow\nWe can do this task in 3 main phases: data cleaning, feature engineering, and modeling.\n* Data Cleaning: Our data is quite messy! We need to take out the outliers, account for missing values, and standardize formats. \n* Feature engineering: Our data (after cleaning) is an array of text columns! We need to do some engineering to turn the text into useful values for modeling. \n* Modeling: Finally, we can run some different models and see how accurate our predictions we can get.\n\nThe workflow is rather lengthy, so I'll break it into two parts. Data cleaning will be covered in this notebook, and the feature engineering / modeling will be [>here<](https://www.kaggle.com/perrychu/tsa-claims-classification-part-2?)"},{"metadata":{"_cell_guid":"f6b846c7-b9b0-4015-9617-464d1d34d92d","_uuid":"a8db0ae473845b3cd2919bb6d72c6f04066afa30"},"cell_type":"markdown","source":"# Phase One: Data Cleaning\nLets load up some necessary packages to start."},{"metadata":{"_cell_guid":"e9e540aa-dcb2-47c3-804c-ed1ef3ffd888","_uuid":"f71f8f9fc438391f6ae25f6de33c5395ed16baf7","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ceffe37-ad4f-46d6-a073-6d9204100d5e","_uuid":"8219988a3d6a4733fe676425530623365b38b827"},"cell_type":"markdown","source":"## Import Data\nFirst, let's load the data file. \n\nI'm renaming a few columns so we can preserve the original data while we do our feature engineering..\n\nWe have ~200k rows to start."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:22.740913Z","start_time":"2018-02-20T22:29:22.158493Z"},"_cell_guid":"29ea64b2-d99a-46e7-8a77-1795022c7e7e","_uuid":"e757ce34daa79a25de3aa5f2a329f2b0d683a444","collapsed":true,"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"#Read Kaggle file\ndf = pd.read_csv(\"../input/tsa_claims.csv\",low_memory=False)\n\n#Format columns nicely for dataframe index\ndf.columns = [s.strip().replace(\" \",\"_\") for s in df.columns]\n\n#Rename date columns\ndf[\"Date_Received_String\"] = df.Date_Received\ndf[\"Incident_Date_String\"] = df.Incident_Date\ndf.drop([\"Date_Received\",\"Incident_Date\"], axis=1, inplace=True)\n\nprint(\"Rows:\", len(df))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66fe65ce-265e-41b3-9dcb-62c746d6935f","_uuid":"470ffa7ac918a9b15dc4f2eec269c7a754c8ab5d","heading_collapsed":true},"cell_type":"markdown","source":"## Check Nulls\nLet's look at nulls by row and column.\n\n### Rows:\nAbout half our rows have at least one null value. A few nulls can be ok (depending on which columns they are in). However, about 2k rows have 6+ nulls. That is more indicative of a problem and might be something we want to fix."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:22.924902Z","start_time":"2018-02-20T22:29:22.742578Z"},"_cell_guid":"8529bdf2-9851-4c6e-a87a-9e79d7775606","_uuid":"734cba4e64fd24aad56a5f94bc0f3112fb79e5c2","collapsed":true,"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# Check distribution of nulls per row\ntemp = df.isnull().sum(axis=1).value_counts().sort_index()\nprint (\"Nulls    Rows     Cum. Rows\")\nfor i in range(len(temp)):\n    print (\"{:2d}: {:10d} {:10d}\".format(temp.index[i], temp[i], temp[i:].sum()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3242f532-7cbe-416d-abaa-d82272e040c6","_uuid":"8b312e7e009c209dbf98fe058c29c7f2d3d03a62"},"cell_type":"markdown","source":"### Columns:\nDisposition and Close Amount have a ton of nulls. That's ok since they are after-the-fact data so we are going to exclude them from the analysis anyway. There is a minor issue here... after digging into the source data, it appears that these values are being used as proxies for \"Status\" and \"Claim Amount\" in the later years (~2013-2015) when TSA stops providing \"Status\" and \"Claim Amount\". There isn't an easy workaround, and there's relatively less data in those years... so we're going to ignore this for now.\n\nNext, Airline, Airport, Claim Type, Claim Site, and Item have quite a few nulls. However, these are categorical values, so we can treat the nulls as \"Other\" or \"Missing\". Basically we will treat the nulls as being in their own category - knowing that data is missing can also be useful information.\n\nNext, Claim Amount, Incident Date, and Date Received are numerical values that have some nulls. We most likely need to throw these values away since there isn't a straightforward way to assign them a value.\n\nFinally, Status has 5 nulls. We have to discard those rows, since that's the variable we are trying to predict. "},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:23.098768Z","start_time":"2018-02-20T22:29:22.926656Z"},"_cell_guid":"ae009ec9-c481-4fa8-acb2-685a0cd8c48b","_uuid":"0bfd23e531ad3fde463f408add4af995d231568c","collapsed":true,"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# Check distribution of nulls per column\ndf.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:23.442641Z","start_time":"2018-02-20T22:29:23.100448Z"},"_cell_guid":"c3dbca26-5c02-4ad9-a25d-b962247036f6","_uuid":"aef1836b607f6d8f9f5df57e039546cc2eaac9ba","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"#Drop rows with too many nulls\ndf.dropna(thresh=6, inplace=True)\n\n#Fill NA for categorical columns\nfill_columns = [\"Airline_Name\",\"Airport_Name\",\"Airport_Code\",\"Claim_Type\",\"Claim_Site\",\"Item\"]\ndf[fill_columns] = df[fill_columns].fillna(\"-\")\n\n#Set NA Claim Amount to 0. Zeros are dropped later in the code.\ndf[\"Claim_Amount\"] = df.Claim_Amount.fillna(\"$0.00\")\n\n#Dropping these nulls later on:\n#  Incident Date / Date Received\n#  Status\n\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4df894a-4e50-4889-ba5b-a04cd46d04b8","_uuid":"0c8a8d1c1cef31df5c32198244b6be97ec1fb75a"},"cell_type":"markdown","source":"## Dependent (Target) Variable\n\nNow, let's look at the target variable we will  try to predict: Claim Status.\n\nThere are quite a range of values - some are inconsistent spelling of the cases we want to predict (Denied vs. Deny) while other are unsettled claims (-, Insufficient, In litigation)."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:23.771537Z","start_time":"2018-02-20T22:29:23.44427Z"},"_cell_guid":"0e3345e3-0662-4bf1-b911-dfc092ab49e3","_uuid":"6e1a50395c2bebe2324e97e0dc58b9587b3fce97","collapsed":true,"hidden":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"df.Status.str.split(\";\").map(lambda x: \"Null\" if type(x)==float else x[0]).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e9a1da6-45e3-4ce7-ae3c-ded1e9fec4f3","_uuid":"986f3b536a9c5a76cd58717cbb75bc98cd7cb721"},"cell_type":"markdown","source":"Let's collapse the inconsistent spellings and remove the non-final statuses."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:23.897044Z","start_time":"2018-02-20T22:29:23.773106Z"},"_cell_guid":"63c8044e-c74a-4f98-be2a-49ef3b72fc22","_uuid":"4fc42d22bff5a154ceb586376c9af36726235d43","collapsed":true,"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"valid_targets = ['Denied','Approved','Deny','Settled','Approve in Full', 'Settle']\n\ndf = df[df.Status.isin(valid_targets)]\ndf.Status.replace(\"Approve in Full\",\"Approved\",inplace=True)\ndf.Status.replace(\"Deny\",\"Denied\",inplace=True)\ndf.Status.replace(\"Settle\",\"Settled\",inplace=True)\n\nprint(df.Status.value_counts())\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a673faf5-b96f-4ee3-970e-2f09b207af37","_uuid":"d711e3cd95d0d9ceb29cd03b38bad1a4847f7b36"},"cell_type":"markdown","source":"## Independent (Feature) Variables"},{"metadata":{"ExecuteTime":{"end_time":"2018-02-13T06:07:34.012135Z","start_time":"2018-02-13T06:07:33.992601Z"},"_cell_guid":"7a1e530d-9360-4959-9edc-4417f7c60e95","_uuid":"66e978febcc98c628dca2ed15f3d0692f083cb2e","heading_collapsed":true},"cell_type":"markdown","source":"### Date Received\nDate Recieved is formatted consistently, but some of the dates are entered incorrectly. We know from the data source that we should only have records from 2002 to 2014. Let's drop the ones that don't fall in this range."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:24.568435Z","start_time":"2018-02-20T22:29:23.981467Z"},"_cell_guid":"d4faea91-180e-4e0c-82ce-c42b73455c9f","_uuid":"e10bfab7e365ad60a727b228bbea63362b248937","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"#Drop nulls\ndf.dropna(subset=[\"Date_Received_String\"], inplace=True)\n\n#Format datetime\ndf[\"Date_Received\"] = pd.to_datetime(df.Date_Received_String,format=\"%d-%b-%y\")\n\n#Check year range\ndf = df[df.Date_Received.dt.year.isin(range(2002,2014+1))]\n\nprint(df.Date_Received.dt.year.value_counts().sort_index())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32d71f5c-4ea6-43d2-8f3e-59e9c9dbd5bb","_uuid":"81ef40dcd2f10deae88be9aa8fe49ff2f9615eab","heading_collapsed":true},"cell_type":"markdown","source":"### Incident Date\nThere are also inconsistencies with Incident Date formats. Some are formatted as 20/jan/09 while others are 01/20/09. Let's make a function to standardize the format so we can conert the strings to DateTime objects."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:24.581333Z","start_time":"2018-02-20T22:29:24.570087Z"},"_cell_guid":"60d046e9-9b88-4d7a-85f7-90cc8359a413","_uuid":"9bbad88ed94ad3057dff686b804e9b380b5eacb2","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"month_dict = {\"jan\":1,\"feb\":2,\"mar\":3,\"apr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"aug\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dec\":12}\n\ndef format_dates(regex, date_string):\n    '''\n    Formats the date string from 2014 entries to be consistent with the rest of the doc \n    Inputs: \n        regex - compiled re with three groups corresponding to {day}/{month (abbrev.)}/{Year}\n        date_string - string to be formatted matching the regex\n    Outputs: \n        If regex match, return formatted string of form {Month}/{Day}/{Year}; else return original string\n    '''\n    m = regex.match(date_string)\n    if(m):\n        day, month, year = m.group(1,2,3)\n        return \"{}/{}/{}\".format(month_dict[month],day,\"20\"+year)\n    else:\n        return date_string\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7dfab529-994c-4476-b051-e0676beaa435","_uuid":"eb996df776272ddae3d1806f1b152e1dcf2efb17"},"cell_type":"markdown","source":"Incident Date includes both date and time. We'll separate out the time before formatting the dates. Unfortunatley many rows are missing the time component, so we end up not being able to use that piece. There are a few other inconsistencies we should fix. Then, we can finally apply the formatting function we wrote above."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:26.179761Z","start_time":"2018-02-20T22:29:24.582702Z"},"_cell_guid":"39674afb-1ce8-4e45-901e-3909105ef632","_uuid":"0521723a6d038554ac21519519a23fa195951886","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"#Drop nulls\ndf.dropna(subset=[\"Incident_Date_String\"], inplace=True)\n\n#Error correction for one value in Kaggle data set (looked up in original TSA data)\ndf.Incident_Date_String.replace(\"6/30/10\",\"06/30/2010 16:30\",inplace=True)\n\n#String formatting for consistency\ndf[\"Incident_Date_String\"] = df.Incident_Date_String.str.replace(\"-\",\"/\")\ndf[\"Incident_Date_String\"] = df.Incident_Date_String.str.lower()\n\n#Splitting up time (if exists otherwise will be date) and date components\ndf[\"Incident_Time\"] = df.Incident_Date_String.str.split(\" \").map(lambda x: x[-1])\ndf[\"Incident_Date\"] = df.Incident_Date_String.str.split(\" \").map(lambda x: x[0])\n\n#Could not find a reasonable translation for these entries... most look like \"02##\"\nregex = re.compile(r\"/[a-z]{3}/[0-9]{4}\")\ndf = df[df.Incident_Date.map(lambda x: not bool(regex.search(x)))].sort_values([\"Date_Received\"])\n\n#These are entries received in 2014. Formatting is different from other years but internally consistent.\nregex = re.compile(r\"(\\d*)/([a-z]{3})/(1[1-4])$\")\ndf[\"Incident_Date\"] = df.Incident_Date.map(lambda x: format_dates(regex,x) )\n#df[df.Incident_Date.map(lambda x: bool(regex.search(x)))].sort_values([\"Date_Received\"])\n\n#Format datetime, check year range, create year and month\ndf[\"Incident_Date\"] = pd.to_datetime(df.Incident_Date,format=\"%m/%d/%Y\")\ndf = df[df.Incident_Date.dt.year.isin(range(2002,2014+1))]\n\nprint(df.Incident_Date.dt.year.value_counts().sort_index())\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cab1d98d-a8d0-4afb-bfde-541191dd42c3","_uuid":"2a21e9c16a57a3683c556fa33c0abf5479524356"},"cell_type":"markdown","source":"### Airport Code / Name\nFirst, we notice some airport codes have multiple distinct airport names. It turns out this is just from excess whitespace, which is an easy fix."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:29:29.476618Z","start_time":"2018-02-20T22:29:29.171653Z"},"_cell_guid":"86dbc9e3-d33c-4e3e-bc15-9996e84bb16c","_uuid":"35905af422ceb2c250aa3f729636632b11b3caf6","collapsed":true,"trusted":true},"cell_type":"code","source":"#Check multiple Airport Names assigned to one Airport Code\ntemp = df.groupby(\"Airport_Code\").Airport_Name.nunique().sort_values(ascending=False)\nprint(df[df.Airport_Code.isin(temp[temp>1].index)].groupby(\"Airport_Code\").Airport_Name.unique().head())\nprint(\"\\n---\\n\")\n\n#Duplicates are from excess spaces\ndf[\"Airport_Code\"] = df.Airport_Code.str.strip()\ndf[\"Airport_Name\"] = df.Airport_Name.str.strip()\n\n#Check multiple Airport Names assigned to one Airport Code\ntemp = df.groupby(\"Airport_Code\").Airport_Name.nunique().sort_values(ascending=False)\nprint(df[df.Airport_Code.isin(temp[temp>1].index)].groupby(\"Airport_Code\").Airport_Name.unique().head())\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b7c07911-bb6b-49ab-adf6-13c4682c9115","_uuid":"856cd154d19a75dda2cede679ecf08b6f309b1a4"},"cell_type":"markdown","source":"Next, let's consolidate the airports that don't show very rarely in the claims. This reduces the dimensionality of our data.\n\nFrom looking at the distribution, I'm grouping airports which have less than 200 claims (<~.01% of data each)."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:03.464196Z","start_time":"2018-02-20T22:29:29.478256Z"},"_cell_guid":"941b59d3-dca3-4127-acec-1f54b1842844","_uuid":"2351cdf8f9c7ec268a8061243ba21f793e89e42c","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"#Look at tail distribution of claims by airport\ntemp = df.Airport_Code.value_counts()\nprint(\"Total: {} airports, {} complaints\".format(temp.count(),temp.sum()))\nfor num in range(1000,1,-100):\n    print(\"Under {}: {} airports, {} complaints\".format(num, temp[temp<num].count(),temp[temp<num].sum()))\n\nlevel = 200\n#plot distribution below level\n#temp[temp<level].count(), temp[temp<level].sum()\n#temp[temp<level].plot.bar()\n\n#Set airport and code to \"Other\" under level\ndef set_other(row, keep_items):\n    if row.Airport_Code in keep_items:\n        row[\"Airport_Code_Group\"] = row.Airport_Code\n        row[\"Airport_Name_Group\"] = row.Airport_Name\n    else:\n        row[\"Airport_Code_Group\"] = 'Other'\n        row[\"Airport_Name_Group\"] = 'Other'\n    return row\n\nkeep_set = set(temp[temp>=level].index)\ndf = df.apply(lambda x: set_other(x,keep_set),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4871ff6f-3259-4843-9e57-c74af0dc7bcd","_uuid":"101c88717b9ee142ab7dc2566f3364197db27bf6","heading_collapsed":true},"cell_type":"markdown","source":"### Airline Name\nFor airlines, let's consolidate names that seem to be subsidiary/parent or have inconsistent naming."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:03.698889Z","start_time":"2018-02-20T22:33:03.508479Z"},"_cell_guid":"e8348ca3-aed7-40ae-b9d8-d31e391de217","_uuid":"c082cf87bce39455d6ea12c24fba3034bac77460","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"df[\"Airline_Name\"] = df.Airline_Name.str.strip().str.replace(\" \",\"\")\ndf.Airline_Name.replace(\"AmericanEagle\",\"AmericanAirlines\",inplace=True)\ndf.Airline_Name.replace(\"AmericanWest\",\"AmericaWest\",inplace=True)\ndf.Airline_Name.replace(\"AirTranAirlines(donotuse)\",\"AirTranAirlines\",inplace=True)\ndf.Airline_Name.replace(\"AeroflotRussianInternational\",\"AeroFlot\",inplace=True)\ndf.Airline_Name.replace(\"ContinentalExpressInc\",\"ContinentalAirlines\",inplace=True)\ndf.Airline_Name.replace(\"Delta(Song)\",\"DeltaAirLines\",inplace=True)\ndf.Airline_Name.replace(\"FrontierAviationInc\",\"FrontierAirlines\",inplace=True)\ndf.Airline_Name.replace(\"NorthwestInternationalAirwaysLtd\",\"NorthwestAirlines\",inplace=True)\ndf.Airline_Name.replace(\"SkywestAirlinesAustralia\",\"SkywestAirlinesIncUSA\",inplace=True)\n\ndf.Airline_Name.value_counts().head(10)\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89143d78-a31b-4c1c-817d-ff9e660c6715","_uuid":"b675ed55da1eeb49b983a75cdaa80157e102b81b","heading_collapsed":true},"cell_type":"markdown","source":"### Claim Type, Claim site\nOnly a few values each here. We can leave them as-is."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:03.729523Z","start_time":"2018-02-20T22:33:03.700482Z"},"_cell_guid":"0d255aba-dc64-4d97-ae36-33bb8747cd4a","_uuid":"64b17181a0a245da71cc0f3159c2b298c19f7326","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"print(df.Claim_Type.value_counts())\nprint(df.Claim_Site.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e755e15-b1df-4e28-9065-b383d9e400b2","_uuid":"57d3da2beed8b539d8d3f3c5b817ee572d140a41","heading_collapsed":true},"cell_type":"markdown","source":"### Item\nItem is a tricky field - it is actually a comma and semi-colon separated list of categories but the naming schemes vary over time. For this analysis, we're going to ignore this data. It would be an interesting future exercise to extract a feature from this column."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:05.960809Z","start_time":"2018-02-20T22:33:03.761229Z"},"_cell_guid":"5c4ab4e8-dcd2-4a92-aa5c-53580b4770fe","_uuid":"a64e4cbc121be5f460dad9764e666c4fa1a17ded","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"#Isolating broadest item categories\n#Items column is a text list of all item categories. Sub categories are inconsistent across years.\ndf_item = df.Item.str.split(\"-\").map(lambda x: \"\" if type(x) == float else x[0])\ndf_item = df_item.str.split(r\" \\(\").map(lambda x: x[0])\ndf_item = df_item.str.split(r\" &\").map(lambda x: x[0])\ndf_item = df_item.str.split(r\"; \").map(lambda x: x[0])\ndf_item = df_item.str.strip()\n\ncategories = df_item.value_counts()\n\n#categories[[not bool(re.compile(\";\").search(x)) for x in categories.index]][0:]\n\ncategories[categories > 100]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"555963ef-ff44-4d42-9ed1-ef84ad828b8d","_uuid":"152d7fae1325cac4f0fc6fbf5a6b2159317a831a","heading_collapsed":true},"cell_type":"markdown","source":"### Claim Amount\nClaim amount is right skewed - there is a lot of data at the lower end then fewer and fewer at high values... all the way up to a single claim at \\$12 billion! However, there is also an anomoly of a bunch of values at $0...\n\nYou might remember that we set some of those zero values when looking at nulls. The others appear to come from the dataset using \"Close Amount\" as a proxy for \"Claim Amount\" for years where \"Claim Amount\" is missing. When a claim is denied, the \"Close Amount\" is zero. Unfortunately, there isn't any way to infer what the actual Claim $ was, so we will have to drop those rows."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:06.486658Z","start_time":"2018-02-20T22:33:05.962412Z"},"_cell_guid":"9ee2464e-6591-4c16-8672-28cf177bce84","_uuid":"539de43c72b03311a4e0ab9518930c828533313d","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"df[\"Claim_Amount\"] = df.Claim_Amount.str.strip()\ndf[\"Claim_Amount\"] = df.Claim_Amount.str.replace(\";\",\"\").str.replace(\"$\",\"\").str.replace(\"-\",\"0\")\ndf[\"Claim_Value\"] = df.Claim_Amount.astype(float)\n\ndf_copy = df.copy()\n\nprint(df.Claim_Value.describe())\nprint(df.Status.value_counts())\nprint(len(df))\n\nsns.distplot(df.Claim_Value[(df.Claim_Value>0)&(df.Claim_Value<500)])\n\ndf.Status[(df.Claim_Value>0)&(df.Claim_Value<1000)].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b6ba1b9-2554-4f90-9c3f-9dc463f86ab4","_uuid":"bc812f1d7a4f1faed9fe9661e54bc90b1dca09ce"},"cell_type":"markdown","source":"One interesting way to vizualize the data is to look at the distribution of claim value for each target class (approve / settle / deny). We can plot this as a histogram. Since the data is skewed, we will use a log-scale for the x-axis bins. \n\nUnder these conditions, it turns out each target class has a (roughly) the shape of a normal distributed at different means. However, since our x-axis bins are log-scale these aren't actual normal distributions.\n\nWe can also see the cluster of claims with claim value = 0."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:06.97647Z","start_time":"2018-02-20T22:33:06.488297Z"},"_cell_guid":"dfd01922-301b-4476-9720-ffe5765c11ae","_uuid":"aa22ca58cfd35fd207066721cf6679aae9e480fe","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"bins = [round(10**x) for x in (list(np.arange(0,4.1,.4))+[10])]\n\nbottom = -1\n\ndata = []\n\nfor x,top in enumerate(bins):\n    counts = df.Status[(df.Claim_Value>bottom)&(df.Claim_Value<=top)].value_counts()\n    for i in range(len(counts)):\n        data.append({\"bin\":(str(x)+\":\"+str(top)),\"label\":counts.index[i],\"count\":counts[i]})\n    bottom = top\n\ncounts_df = pd.DataFrame(data)\n\nsns.factorplot(x=\"bin\",y=\"count\",hue=\"label\",data=counts_df,kind=\"bar\",size=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1952f380-63f3-414d-bb11-fd59f93d9358","_uuid":"db253f043437ad895b610f0410536e99295bf43d"},"cell_type":"markdown","source":"Ok, now lets actually remove the zero values. "},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:07.03792Z","start_time":"2018-02-20T22:33:06.977977Z"},"_cell_guid":"16a8d5e1-867b-411d-8566-25cc74fc660f","_uuid":"e3cfcde4479df87ea680114f97cdfc946822d4bd","collapsed":true,"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"df = df[df.Claim_Value != 0]\n\nprint(df.Claim_Value.describe())\nprint(df.Status.value_counts())\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5c95dd3-3b13-4523-84c7-4b4b3cc0bd06","_uuid":"d4dd1f39396c1fd3641b2f1a80ec609672481286","heading_collapsed":true},"cell_type":"markdown","source":"### Close Amount\nNote: I also looked at the relationship between Close Amount and Claim Amount. I wasn't able to link the two in a way that made sense, but I'll leave these plots here in case it is interesting. Remember that we aren't using Close Amount because it is only known after the claim is settled - we would prefer to make our prediction before the claim is submitted."},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:07.28756Z","start_time":"2018-02-20T22:33:07.06596Z"},"_cell_guid":"b69e795d-ff22-4fcb-a976-96a8f60bcabc","_uuid":"008e134c9cbe0ea570222d6309894da41915fd28","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"df[\"Close_Amount\"] = df.Close_Amount.str.strip()\ndf[\"Close_Amount\"] = df.Close_Amount.str.replace(\";\",\"\").str.replace(\"$\",\"\")\ndf[\"Close_Value\"] = df.Close_Amount.astype(float)\ndf.Close_Value.describe()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:09.91143Z","start_time":"2018-02-20T22:33:07.28915Z"},"_cell_guid":"3e8290b0-edef-4fec-8ef2-8784a450fe82","_uuid":"6b769093a7e9b3e8ec1313a94308f865178af192","collapsed":true,"hidden":true,"trusted":true},"cell_type":"code","source":"plot_df = df[(df.Claim_Value < 200000) & (df.Close_Value <= 500000)]\n\nplt.scatter(plot_df.Claim_Value,plot_df.Close_Value,alpha=.2)\nplt.title(\"Combined\")\nplt.xlabel(\"Claim value\")\nplt.ylabel(\"Close value\")\nplt.show()\n\nfig,ax = plt.subplots(1,3)\nfig.set_size_inches(16,4)\n\nfor i,s in enumerate(plot_df.Status.unique()):\n    ax[i].scatter(plot_df[plot_df.Status==s].Claim_Value,plot_df[plot_df.Status==s].Close_Value,alpha=.2)\n    ax[i].set_title(s)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f7bb1c3-03c3-4040-8cb6-95466fd4baa0","_uuid":"49bac17832180b08fed7ab37ec220118a775c22c"},"cell_type":"markdown","source":"## Cleaning Complete!\nNow we have our clean data, so we can dig into modeling. Let's write the data out to a file for future use.**"},{"metadata":{"ExecuteTime":{"end_time":"2018-02-20T22:33:13.122116Z","start_time":"2018-02-20T22:33:09.912997Z"},"_cell_guid":"1551901a-b764-4122-9a62-00bf273f5422","_uuid":"c3c5f11bbfdc8ab4d42e3cd5e301617d5d5dd8c9","collapsed":true,"trusted":true},"cell_type":"code","source":"output_df = df.drop([\"Close_Amount\", \"Claim_Amount\", \"Disposition\",\n                     \"Date_Received_String\",\"Incident_Date_String\",\"Incident_Time\",\n                     \"Airport_Code\",\"Airport_Name\"],axis=1)\n\noutput_df.to_csv(\"tsa_claims_clean.csv\",index=False)\n\noutput_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3458a8cd-6104-4010-8e41-f85f7c1e25ef","_uuid":"1f4d83833c7af347024422a47f83e79bd83abe41"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"221px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}