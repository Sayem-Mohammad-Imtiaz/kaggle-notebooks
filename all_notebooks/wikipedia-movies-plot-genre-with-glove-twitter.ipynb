{"cells":[{"metadata":{"_uuid":"cd9112e8c9aefe0b211236afb818c2e5b1124c75"},"cell_type":"markdown","source":"This kaggle is for me to learn about text analysis with glove embeddings. I'm following [Approaching (Almost) Any NLP Problem on Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle), I use the glove-twitter word embeddings, reduce each sentence to a normalized vector-sum of all the word embeddings (so probably becomes like a weighted bag of words) and classify using the BaggingClassifier ensemble learner."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# code based on https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5abf7ea0327eb9db6b4649da721e5461640aa73c"},"cell_type":"markdown","source":"Load the training data from CSV"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# load data \ntrain = pd.read_csv('../input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv')\n# encode genre strings to unique numbers\nlbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train['Genre'].values)\nsns.distplot(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f393936d924bbcc77d05d69fd0386c75f5dde78"},"cell_type":"markdown","source":"In my opinion there are too many genres with few data points to categorize them.\nSo I will arbitrarily reduce them. Here I collect a set of \"single-word\" genre entries\nusing a naive algorithm."},{"metadata":{"trusted":true,"_uuid":"c30f974f2e37f433e4bbe312d15adcf2197218c7"},"cell_type":"code","source":"# collect single-word genre values\ngenre_counts = {}\nfor rowgenre in train['Genre']:\n    # skip compound genres\n    if re.search(r'[^\\w]', rowgenre) != None:\n        continue\n    if not rowgenre in genre_counts:\n        genre_counts[rowgenre] = 0\n    genre_counts[rowgenre] += 1\n# keep any single-word genre with >= 2 entries\nsimpler_genres = []\nsorted_keys = sorted(genre_counts.keys(), key=lambda x: -genre_counts[x])\nfor genre in sorted_keys:\n    if genre_counts[genre] >= 500:\n        simpler_genres.append(genre)\n(len(simpler_genres), simpler_genres[0:7])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83e9caafc9ab7be89697cb26f603de82c763ab5c"},"cell_type":"markdown","source":"Next I reduce the genres column to our simpler list, fitting whatever matches first."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0c956c3b7f72896ef9a6d3238fc8a0e8b46aac3f"},"cell_type":"code","source":"# create new encoded y using simplified genres list\ndef getSimplerGenre(rowgenre):\n    if rowgenre in simpler_genres:\n        return rowgenre\n    for s_genre in simpler_genres:\n        if s_genre in rowgenre:\n            return s_genre\n    return 'unknown'\nsimpler_genre_set = [getSimplerGenre(g) for g in train['Genre']]\n# count / describe simplified genre distribution\nsimpler_genre_set_count = {}\nfor sg in simpler_genre_set:\n    if not sg in simpler_genre_set_count:\n        simpler_genre_set_count[sg] = 0\n    simpler_genre_set_count[sg] += 1\nsns.barplot(x=list(simpler_genre_set_count.values()),y=list(simpler_genre_set_count.keys()),log=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5c38c0951cd222a14ad25057ebf51ad77b6dba0"},"cell_type":"markdown","source":"There are a lot of unknowns, I drop them from this dataset and save them for later."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4c0f8e1a80f296395fc60c88a30ce51daef57a75"},"cell_type":"code","source":"# create unified set of modified genre/plots\nsimpler_df = pd.DataFrame({'genre':simpler_genre_set, 'plot':train['Plot'].values})\n# save unknowns for later use\nunknown_df = simpler_df[simpler_df.genre == 'unknown']\n# exclude unknowns from simpler_df set\nsimpler_df.drop(unknown_df.index, inplace=True)\nsimpler_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15118f513aec146f9eb5442fe52b8cabf20f5c15"},"cell_type":"markdown","source":"Next I create a training/validation split data set. I also recreate `y` as to use the simplified genre set."},{"metadata":{"trusted":true,"_uuid":"e647dcd4abd898bbf6663adc6f5e887e71ce4a0e","scrolled":true},"cell_type":"code","source":"# encode new y values for plotting purposes\nlbl_enc2 = preprocessing.LabelEncoder()\ny = lbl_enc2.fit_transform(simpler_df['genre'].values)\n# split train/validation data\ntrain_x, validation_x, train_y, validation_y = train_test_split(simpler_df['plot'].values, y, stratify=y, test_size=0.1, shuffle=True)\n# show word count distributions of both sets\nwordcounter = np.vectorize(lambda s: len(s.split(' ')))\nf, axes = plt.subplots(1, 2, sharey=True)\nsns.distplot( wordcounter(train_x) , color=\"skyblue\", label=\"Training\", ax=axes[0])\nsns.distplot( wordcounter(validation_x) , color=\"red\", label=\"Validation\", ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73a0ab23cfa535eaefed35ca2bbdb28180e0a48f"},"cell_type":"markdown","source":"Load the glove-twitter embeddings."},{"metadata":{"trusted":true,"_uuid":"52d0582e91472cac75227602ba39299425819dfd"},"cell_type":"code","source":"# load glove-twitter embeddings\nf = open('../input/glove-twitter/glove.twitter.27B.25d.txt')\nembeddings_index = {}\nfor line in f.readlines():\n    values = line.split()\n    word = values[0] # first column is word, rest are coefs\n    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\nf.close()\nprint(f\"Found {len(embeddings_index.keys())} embeddings\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8022fc1b36002deb8f04d39bbc11040c8607bce2"},"cell_type":"markdown","source":"Building the sentence vectors. A sentence vector is a 1-d normalized result of the vector additions of all the word vectors of each word in the sentence."},{"metadata":{"trusted":true,"_uuid":"2a6e364ef5036a5d664a3e346290febc505b8f4c","scrolled":true},"cell_type":"code","source":"# build vector calculated from embeddings of each word in sentence\ndef buildSentenceVector(s):\n    lower_s = str(s).lower() # normalize case\n    tokens = word_tokenize(lower_s) # tokenize\n    # only use words of alphabet characters, no stop words\n    words = [w for w in tokens if (not w in stop_words) and w.isalpha()] \n    # build matrix\n    M = []\n    for w in words:\n        if w in embeddings_index:\n            M.append(embeddings_index[w])\n    # sum all coef terms across sentence\n    v = np.array(M).sum(axis=0)\n    # exit if v failed to calculate\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    # normalized output vector\n    return v / np.sqrt((v ** 2).sum())\n# build train/validation vectors for all rows\ntrain_vectors = [buildSentenceVector(x) for x in train_x]\nvalidation_vectors = [buildSentenceVector(x) for x in validation_x]\n(len(train_vectors), len(validation_vectors))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"300b633a197b97882781c1a89570111c23ed92ad","scrolled":false},"cell_type":"code","source":"train_x_glove = pd.DataFrame(train_vectors)\nvalidation_x_glove = pd.DataFrame(validation_vectors)\nvalidation_x_glove.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7385d0dd724dc15576c5b44600c4e0e499416a28"},"cell_type":"markdown","source":"Now set up the classifier and fit"},{"metadata":{"trusted":true,"_uuid":"ab65bc59fe0e25aef57e2ebcd370ebc4556ac13b","scrolled":true},"cell_type":"code","source":"# use BaggingClassifier to chose hyperparamters automatically\nclf = BaggingClassifier()\nclf.fit(train_x_glove, train_y)\n# I'm not sure what these warnings mean\nNone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6514362d72d23e1dd500e01d5ab72895395267e5","scrolled":false},"cell_type":"code","source":"# get predictions\npredictions = clf.predict(validation_x_glove)\n# check mean squared error\npercor = np.sum(predictions == validation_y) / len(validation_y)\nsqmse = np.sqrt(mean_squared_error(validation_y, predictions))\nprint (\"correct: %0.1f ; sqrt(mse(...)): %0.3f\" % (percor,sqmse))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10437a4581ee1aea6a33411f987d21d5355cbd8e"},"cell_type":"markdown","source":"Hmm, where did things go so wrong? Maybe a confusion matrix can help find out."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"886c79d55ffc4a07eb1a508fa7e649caa293b050"},"cell_type":"code","source":"# from https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return fig\n\n# display\nvalidation_y_texts = lbl_enc2.inverse_transform(validation_y)\nprediction_texts = lbl_enc2.inverse_transform(predictions)\ncm = confusion_matrix(validation_y_texts, prediction_texts)\nclass_names = list(lbl_enc2.classes_)\nprint_confusion_matrix(cm, class_names)\nNone","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6515ee474d10aff547abbed61490e7b4d8d40b39"},"cell_type":"markdown","source":"We have a lot of overlap between drama and comedy, which makes sense, because those occupy a bulk of the data, and also the normalization process excluded many fields that had both 'comedy' and 'drama' in the original description.\nLet me know any suggestions to optimize in the comments."},{"metadata":{"_uuid":"69d4298aa8d18df8e2ca0a010da66562e9283cd2"},"cell_type":"markdown","source":"Let's predict some of the unknowns I excluded earlier"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"90cee4eee2a0db25a21900f14e662e1b34d46aa4"},"cell_type":"code","source":"# grab random sample\nunknown_sample = unknown_df.sample(n=50)['plot'].values\nunknown_vectors = pd.DataFrame([buildSentenceVector(x) for x in unknown_sample])\n# predict and get texts\nunknown_predictions = clf.predict(unknown_vectors)\nunknown_guesses = lbl_enc2.inverse_transform(unknown_predictions)\n# display\npd.options.display.max_colwidth = 200\nsamples_df = pd.DataFrame({'plot':unknown_sample, 'guess':unknown_guesses})\nsamples_df.groupby('guess').apply(lambda df: df.sample(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68551c246738626f6f59259ffca5f86d1c57252a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}