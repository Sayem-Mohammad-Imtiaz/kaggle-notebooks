{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Modelling a wind farm with AutoML\nThis example uses the same data of [this notebook](https://www.kaggle.com/matteodefelice/modelling-wind-power-generation). In this case, rather than specifying a ML model, we use an automated machine-learning (AutoML) tool. There are many AutoML tools available, in this example I use [TPOT](http://epistasislab.github.io/tpot/). \n\nTPOT, available with open license for all the OSs (including Windows), optimises a scikit-learn pipeline via a Genetic Programming (GP) algorithm. Basically, the algorithm (as all the evolutionary algorithms) evolves a population (where each individual represents a pipeline) using mutation and crossover operators. \n\nMore info on TPOT can be found on this [Medium article](https://towardsdatascience.com/tpot-pipelines-optimization-with-genetic-algorithms-56ec44ef6ede) or on the [official scientific paper](https://dl.acm.org/doi/10.1145/2908812.2908918)","metadata":{}},{"cell_type":"code","source":"from tpot import TPOTRegressor\n\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We read here the training data: hourly generation of the Gordonbush wind farm for the years 2016-2018","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/gordonbush-wind/gordonbush-2016_2018.csv')\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A bit of data wrangling to create the input data: a column for each grid point per weather variable (two components of wind and wind speed).","metadata":{}},{"cell_type":"code","source":"x = df[['latitude', 'longitude', 'time', 'u100', 'v100', 'ws']]\nx = x.assign(point = x['latitude'].astype(str) + x['longitude'].astype(str))\nx = x.drop(['latitude', 'longitude'], axis = 1)\nx = x.pivot(index = 'time', columns = ['point'], values = ['u100', 'v100', 'ws'])\nx.columns = x.columns.to_flat_index().str.join('_')\nx = x.reset_index().drop('time', axis = 1)\nx.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We extract the output (more information on the data can be found on the [previous notebook](https://www.kaggle.com/matteodefelice/modelling-wind-power-generation))","metadata":{}},{"cell_type":"code","source":"y = df.loc[df['latitude'] == 58.25].loc[df['longitude'] == -4]['ActualGenerationOutput']\ny.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We setup the TPOT algorithm. **Very important**: the parameters used in this example have been chosen for a quick computation (<5 minutes). We use for the training **only the 5% of the total data** (`subsample` parameter). To maximise the performance I'd suggest using the 100% and possibly increasing the number of generations and the population size (default values are 100 and 100). The objective function is the R squared.","metadata":{}},{"cell_type":"code","source":"tpot = TPOTRegressor(generations=5, population_size=10, verbosity=2, random_state=41, \n                     scoring = 'r2', #r2\n                     n_jobs = 4, \n                     subsample = 0.05)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The algorithm runs using `x` and `y`. Normally the score (R squared computed in cross-validation) increases each generation due to the evolution of the population. ","metadata":{}},{"cell_type":"code","source":"tpot.fit(x, y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now `tpot` contains all the fitted pipelines and the information on the algorithm runs. We can see the best pipeline (the best individual). ","metadata":{}},{"cell_type":"code","source":"tpot.fitted_pipeline_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or even the Pareto front considering both complexity and performance. ","metadata":{}},{"cell_type":"code","source":"tpot.pareto_front_fitted_pipelines_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to evaluate the performance of this model on the year 2019. Then we load the data applying the same processing to create two new input/output datasets. ","metadata":{}},{"cell_type":"code","source":"\n\ndf_test = pd.read_csv('../input/gordonbush-wind/gordonbush-2019.csv')\nxt = df_test[['latitude', 'longitude', 'time', 'u100', 'v100', 'ws']]\nxt = xt.assign(point = xt['latitude'].astype(str) + xt['longitude'].astype(str))\nxt = xt.drop(['latitude', 'longitude'], axis = 1)\nxt = xt.pivot(index = 'time', columns = ['point'], values = ['u100', 'v100', 'ws'])\nxt.columns = xt.columns.to_flat_index().str.join('_')\nxt = xt.reset_index().drop('time', axis = 1)\n\nyt = df_test.loc[df_test['latitude'] == 58.25].loc[df_test['longitude'] == -4]['ActualGenerationOutput']\nxt.shape, yt.shape\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fit the best pipeline on this new data and calculate the correlation coefficient. The result is similar to the one we got using a neural network in the previous example, but in this case we didn't select the best model and - as said above - we used the parameters for a quick computation. Then setting `subsample` to 1 and increasing generations and populations *might* lead to even better results. ","metadata":{}},{"cell_type":"code","source":"y_hat_test = tpot.fitted_pipeline_.predict(xt)\nprint(scipy.stats.pearsonr(yt.values, y_hat_test.flatten()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the scatter data and the histograms. Also in this case, we can see that the ML model cannot predict the cases of zero generation probably due to outages (or curtailment?). ","metadata":{}},{"cell_type":"code","source":"plt.scatter(y = y_hat_test, x = yt.values)\nplt.xlabel('Testing wind generation')\nplt.ylabel('Best pipeline prediction')\nplt.title('Wind generation on the testing data')\nplt.grid(True)\n\nfig, axs = plt.subplots(2, 1, figsize=(12, 5))\nbins = np.linspace(0, 80, 20)\n\nsns.histplot(y_hat_test, bins = bins, ax = axs[0], kde = False).set(title='Predicted')\nsns.histplot(yt, bins = bins, ax = axs[1], kde = False).set(title = 'Observed (2019)')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can plot the first two weeks of data: in orange the actual generation and in blue the model output. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[20, 6])\nplt.plot(y_hat_test[0:335])\nplt.plot(yt.values[0:335])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}