{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predict HR Stay or Leave\n\nHere [HR Analytics](https://www.kaggle.com/giripujar/hr-analytics) dataset by [Giri Pujar](https://www.kaggle.com/giripujar) is used to create a classifier if a `HR` will stay or leave.\n\nUsing the `unbalanced dataset` of employees of the company to predict which employee might stay or leave the company. `SMOT` is used to deal with the unbalanced dataset. `SMOTE` (synthetic minority oversampling technique) is one of the most commonly used `oversampling` methods to solve the imbalance problem.\n\n![](https://media.giphy.com/media/l0DAI7ZQCXxSZzaO4/giphy.gif)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn import linear_model\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import (\n    GridSearchCV, StratifiedKFold, cross_val_score, learning_curve,\n    train_test_split\n)\nfrom sklearn.metrics import (\n    accuracy_score, classification_report, confusion_matrix, f1_score, log_loss,\n    precision_score, recall_score, roc_curve, roc_auc_score, precision_recall_curve, \n    auc\n)\nfrom sklearn.pipeline import Pipeline\n\n# Models\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\n\nfrom joblib import dump","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For seaborn colors\nsns.set(style='whitegrid', color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the dataset\ndf = pd.read_csv('/kaggle/input/hr-analytics/HR_comma_sep.csv')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_countplot(column, ax=None):\n    with sns.axes_style('ticks'):\n        sns.countplot(x=column, palette=sns.color_palette('rocket'), ax=ax)\n        sns.despine(offset=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at how much is the dataset imbalanced?\n\nnum_of_stay = round(len(df[df.left == 0]) / len(df) * 100, 2)\nprint(f'HR stay - {num_of_stay}%')\nprint(f'HR leave - {round(100 - num_of_stay, 2)}%')\n\nplot_countplot(df.left)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Nominal data` assigns names to each data point without placing it in some sort of order. For example, the results of a test could be each classified nominally as a **pass** or **fail**.\n\n`Ordinal data` groups data according to some sort of ranking system: it orders the data. For example, test results could be grouped in descending order by grade: **A, B, C, D, E and F**\n\nMore on difference between `nominal data` and `ordinal data` ðŸ‘‰ [Source](https://sciencing.com/difference-between-nominal-ordinal-data-8088584.html)"},{"metadata":{},"cell_type":"markdown","source":"### Working with ordinal data like the salary coloumn"},{"metadata":{"trusted":true},"cell_type":"code","source":"replacement = {\n    'low': 0, \n    'medium': 1, \n    'high': 2\n}\n\ndf.salary = df.salary.apply(lambda x: replacement[x])\ndf.salary[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Working with nominal data like deparment column"},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe = OneHotEncoder()\n\ndept_ohe_df = pd.DataFrame(df.Department)\ndept_ohe_df = pd.DataFrame(\n    ohe.fit_transform(dept_ohe_df[['Department']]).toarray()\n)\n\nprint(f'Unique Departments: {len(df.Department.unique())}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = []\nfor col_name in ohe.get_feature_names():\n    col_name = col_name.split('_')[1]\n    col_names.append(col_name)\n\ncol_names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_ohe_df.columns = col_names\ndept_ohe_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing one column from `dep_ohe_df` to avoid multi-corrliearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_ohe_df = dept_ohe_df.drop(['IT'], axis='columns')\ndept_ohe_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target column\ny = df[['left']]\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the ohe results and removing `left` column\n\ndf = df.drop(['Department', 'left'], axis='columns')\ndf = pd.concat([dept_ohe_df, df], axis='columns')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df.copy()\nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling the dataset\nfor column in x.columns:\n    x[column] = StandardScaler().fit_transform(x[column].values.reshape(-1, 1))\n    \nx.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Balancing the unbalanced data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating train and test datasets using x and y\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=10)\n\n# Creating train and cross-validation datasets using the x_train and y_train\nx_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n\nprint(f'Training set size: {len(x_train)}')\nprint(f'Validation set size: {len(x_cv)}')\nprint(f'Test set size: {len(x_test)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the train dataset into train and cross validation data sets before oversampling to avoid `oversampling to bleed data` for cross_val_score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Oversampling to balance the data\n\n_smote = SMOTE(random_state=0)\n\nsm_cols = x_train.columns\n\nx_train, y_train = _smote.fit_resample(x_train, y_train)\nx_train = pd.DataFrame(data=x_train, columns=sm_cols)\ny_train = pd.DataFrame(data=y_train, columns=['left'])\n\n# We can Check the numbers of our data\nprint(f'Length of oversampled data is {len(x_train)}')\n\nprint(f'Number of left no {len(y_train[y_train.left == 0])}')\nprint(f'Number of left yes {len(y_train[y_train.left == 1])}')\n\nprint(f'Proportion of left no data in oversampled data is {len(y_train[y_train.left == 0])/len(x_train)}')\nprint(f'Proportion of left yes data in oversampled data is {len(y_train[y_train.left == 1])/len(x_train)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Featrue Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Pearson Correlation\n\nplt.figure(figsize=(22, 12))\ncor = x.corr()\nsns.heatmap(cor, annot=True, cmap=sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For cross validation\nskf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names.remove('IT') # since IT is dropped\ncol_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cv = np.array(x_cv)\nx_cv = x_cv.astype('int')\ny_cv = np.array(y_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    LogisticRegression(), \n    SGDClassifier(), \n    KNeighborsClassifier(), \n    GaussianNB(), \n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC(),\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_val_score(models, x_cv, y_cv):\n    for model in models:\n        scores = []\n        for train, test in skf.split(x_cv, y_cv):\n            x_train, x_test = x_cv[train], x_cv[test]\n            y_train, y_test = y_cv[train], y_cv[test]\n\n            _smote = SMOTE(random_state=0)\n            x_train_sm, y_train_sm = _smote.fit_resample(x_train, y_train)\n\n            model.fit(x_train_sm, y_train_sm)\n\n            score = model.score(x_test, y_test)\n            scores.append(score)\n\n        print(f'== {model} ==')\n        print(f'Cross-Validation mean-score: {np.mean(score)}')\n        print()\n\n\ncross_val_score(models, x_cv, y_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recursive Feature Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe = RandomForestClassifier()\n\nrfe = RFE(rfe, n_features_to_select=5)\nrfe.fit(x_train, y_train.values.ravel())\n\nselector = rfe.support_\n\nprint(rfe.support_)\nprint(rfe.ranking_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we took RandomForestClassifier model with 5 features and RFE gave feature ranking as above, but the selection of number â€˜5â€™ was random. Now we need to find the optimum number of features, for which the accuracy is the highest. We do that by using loop starting with 1 feature and going up to 18. We then take the one for which the accuracy is highest."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(x.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rfe(model, x_cv, y_cv):\n    # number of features\n    nof_list = np.arange(1, 17 + 1)\n    high_score = 0\n\n    # variable to store the optimum features\n    nof = 0\n    score_list = []\n\n    for n in range(len(nof_list)):\n        x_train, x_test, y_train, y_test = train_test_split(\n            x_cv, y_cv, test_size=0.3, random_state=0\n        )\n\n        _smote = SMOTE(random_state=0)\n        x_train_sm, y_train_sm = _smote.fit_resample(x_train, y_train)\n\n        rfe = RFE(model, n_features_to_select=nof_list[n])\n        x_train_rfe = rfe.fit_transform(x_train_sm, y_train_sm)\n        x_test_rfe = rfe.transform(x_test)\n\n        model.fit(x_train_rfe, y_train_sm)\n\n        score = model.score(x_test_rfe, y_test)\n        score_list.append(score)\n\n        if score > high_score:\n            high_score = score\n            nof = nof_list[n]\n\n    return (nof, high_score)\n\n\nnof, high_score = rfe(RandomForestClassifier(), x_cv, y_cv)\n\nprint(\"Optimum number of features: %d\" % nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen from above code, the optimum number of features is `nof`. We now feed `nof` as number of features to RFE and get the final set of features given by RFE method, as follows"},{"metadata":{},"cell_type":"markdown","source":"### Performing Feature Elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier()\n\nrfe = RFE(model, n_features_to_select=nof)\nrfe.fit(x_train, y_train.values.ravel())\n\nselector = rfe.support_\n\nprint(rfe.support_)\nprint(rfe.ranking_)\n\nnum_of_selected_features = len(rfe.support_)\nprint(f'\\nNumber of features selected: {num_of_selected_features}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selected features\n\ncol = (x_train.columns)\nresult = itertools.compress(col, selector)\n\ncol_names = []\nfor c in result:\n    col_names.append(c)\n    print(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train[col_names]\nx_test = x_test[col_names]\n\nlen(col_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implementing the model\n\ncols = col_names.copy()\n\nx_train = x_train[cols]\ny_train = y_train['left']\n\nlogit_model = sm.Logit(y_train, x_train)\n\nresult = logit_model.fit()\n\nprint(result.summary2())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every Feature that we got from `Recursive Feature Elimination` is selected since no feature's `p-value is greater that 0.05`."},{"metadata":{},"cell_type":"markdown","source":"### Creating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rt_param_selection(x, y, nfolds):\n    criterion = ['gini', 'entropy']\n    max_features = ['auto', 'sqrt', 'log2']\n    param_grid = {'criterion': criterion, 'max_features': max_features}\n\n    grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=nfolds)\n    grid_search.fit(x, y)\n    grid_search.best_estimator_\n    return grid_search.best_estimator_\n\n\nskf = StratifiedKFold(n_splits=10)\nbest_estimator_ = rt_param_selection(x_train, y_train, skf)\nbest_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_score(best_estimator_, x_cv, y_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting learning curve\n\n_size = np.arange(0.01, 1.01, 0.01)\ntrain_sizes = np.array(_size)\nscoring = 'neg_mean_squared_error'\n\ntrain_sizes_abs, train_scores, cv_scores = learning_curve(\n    RandomForestClassifier(criterion='entropy'), \n    x_train, y_train, \n    train_sizes=train_sizes, cv=skf, scoring=scoring\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scores_mean = []\nfor row in train_scores:\n    _mean = row.mean()\n    train_scores_mean.append(_mean)\n    \ncv_scores_mean = []\nfor row in cv_scores:\n    _mean = row.mean()\n    cv_scores_mean.append(_mean)    \n    \ntrain_scores_mean = -np.array(train_scores_mean)\ncv_scores_mean = -np.array(cv_scores_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 5))\n\nax.plot(train_sizes_abs, train_scores_mean, label='Train')\nax.plot(train_sizes_abs, cv_scores_mean, label='Cross Validation')\n\nax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\nmodel = best_estimator_\nmodel.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict(x_test)\nprint(y_test_pred)\nprint(f\"\\nPrediction: \\n{pd.DataFrame(y_test_pred)[0].value_counts()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_test.values.reshape(1, -1)[0])\nprint()\nprint(f\"Actual: \\n{pd.DataFrame(y_test)['left'].value_counts()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_prob = model.predict_proba(x_test)\ny_test_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Model Score: {model.score(x_test, y_test)}')\nprint(f'f1-score: {f1_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'precision score: {precision_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'recall score: {recall_score(y_test, y_test_pred, average=\"weighted\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(\n    cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues\n):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(\n            j,\n            i,\n            format(cm[i, j], fmt),\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\"\n        )\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nprint(confusion_matrix(y_test, y_test_pred, labels=[1, 0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_test_pred, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Leave=1','Stay=0'], normalize= False,  title='Confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_test_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_roc_auc = roc_auc_score(y_test, model.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()\n\n# The blue farther from red-dotted the better model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaling = ('scale', StandardScaler())\nmodel = ('model', RandomForestClassifier(criterion='entropy'))\n\n# Steps in the pipeline\nsteps = [scaling, model]\n\npipe = Pipeline(steps=steps)\n\n# Fiitting the model\nmodel = pipe.fit(x_train, y_train)\n\n# Out-Of-Sample Forecast\ny_test_pred = model.predict(x_test)\n\n# Evaluation\nprint(f'Model Score: {model.score(x_test, y_test)}')\nprint(f'f1-score: {f1_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'precision score: {precision_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'recall score: {recall_score(y_test, y_test_pred, average=\"weighted\")}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision-Recall vs Threshold Chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_pred_y = model.predict(x_test) \nlog_probs_y = model.predict_proba(x_test) \n\nprecision, recall, thresholds = precision_recall_curve(y_test, log_probs_y[:, 1]) \npr_auc = auc(recall, precision)\n\nplt.title(\"Precision-Recall vs Threshold Chart\")\nplt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\nplt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\nplt.ylabel(\"Precision, Recall\")\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"lower left\")\nplt.ylim([0,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To control the threshold of probability abpve which we want to consider it has true"},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.45\npreds = np.where(model.predict_proba(x_test)[:,1] > THRESHOLD, 1, 0)\n\nresults_data = [\n    accuracy_score(y_test, preds), \n    recall_score(y_test, preds), \n    precision_score(y_test, preds), \n    f1_score(y_test, preds), \n    roc_auc_score(y_test, preds)\n]\nresults_indexes = [\"accuracy\", \"recall\", \"precision\", \"f1_score\", \"roc_auc_score\"]\nresults = pd.DataFrame(data=results_data, index=results_indexes)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, preds, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Leave=1','Stay=0'], normalize= False,  title='Confusion matrix')\n\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the model\ndump(model, 'model.joblib')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\nI'll wrap things up there. If you want to find some other answers then go ahead `edit` this kernel. If you have any `questions` then do let me know.\n\nIf this kernel helped you then don't forget to ðŸ”¼ `upvote` and share your ðŸŽ™ `feedback` on improvements of the kernel.\n\n![](https://media.giphy.com/media/qatu2fd5vCi7C/giphy.gif)\n\n---"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}