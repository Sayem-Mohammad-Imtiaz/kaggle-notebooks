{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1) Introduction"},{"metadata":{},"cell_type":"markdown","source":"This project uses database of used Skoda cars in UK @ https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes\n\nOur objective is to use machine learning alogrithms to predict the price of a used Skoda car in UK based on features like, Model, Year of introduction, Engine size etc. To achieve the objective we will require the following python libraries:\n1. scikit-learn for devloping prediction models\n2. pandas for data management\n3. seaborn for data visualization\n\nFurther, we will be using the following machine learning logarithms form scikit-learn library:\n1. LinearRegression\n2. RandomForestRegression\n3. GradientBoostingRegressor\n\nMoreover, to evaluate the models we will use two metrics:\n1. R square\n2. Mean Average Error"},{"metadata":{},"cell_type":"markdown","source":"# 2) Problem statement"},{"metadata":{},"cell_type":"markdown","source":"Our objective is to predict the price of a used Skoda car in UK based on the following features:\n1. Model of the Skoda car\n2. Year\tof introduction\n3. Transmission\ttype\n4. Mileage of the car\n5. Fuel Type used by the car\n6. Tax on the car\n7. Miles Per Gallon\t\n8. Engine Size of the car"},{"metadata":{},"cell_type":"markdown","source":"# 3) Detailed methodology"},{"metadata":{},"cell_type":"markdown","source":"Lets import the required libraries which include:\n1. numpy for numerical analysis and as a requirement for working with pandas\n2. pandas for data management\n3. mathplotlib and seaborn for data visualization"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the database of skoda used cars using pandas.\nThe \"data\" variable holds all of our data in pandas data frame."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/used-car-dataset-ford-and-mercedes/skoda.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3a) Data Description and Visualization"},{"metadata":{},"cell_type":"markdown","source":"Let's see the data types of values for each columns of the data frame using dtypes method in pandas."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a total of 9 columns in our data frame, 3 of them are catagorical.\nWe now use pandas describe() function for stastical summary of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 6,000+ entries in the database as well as the means and quantiles for each colum of data can be seen form the stastical summary.\nLet's visualize the scatterplots for each of these columns plotted against price."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.PairGrid(data, y_vars=[\"price\"], x_vars=[\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"])\ng.map(plt.scatter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scatterplots show a linear relationship of variables with price. Hence, this data can be used to feed to a supervised-regression machine learning logarithm for developing predictions.\n\nAdditionally, we also see some outliers in the data which needs to be removed. For this purpose we must proceed with the data cleaning to get rid of them first."},{"metadata":{},"cell_type":"markdown","source":"# 3b) Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"In the following cell, I have devloped a loop which takes non-string columns (price, year, mileage, tax, mpg, engineSize). For every column, I have sorted the data and have removed the last 2% of entries. Meaning, I have removed the top 2% of data. Top 2%, because, the outliers are mostly concentrated in higher percentiles. lower percentiles have not been removed.\n\nOnce, the loop deletes top 2% of data for every column, the modified data is stored in new variable called clean_data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_data= data\n\nfor col in [\"price\", \"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"]:\n    sorted_data= sorted_data.sort_values(by=[col])\n    num_of_outliers= len(sorted_data.index)*0.02\n    selected_values= len(sorted_data.index)-round(num_of_outliers,0)\n    sorted_data= sorted_data[:int(selected_values)]\n    \nclean_data= sorted_data\nclean_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With outliers removed, we have 5552 rows now. To see the difference, let us see the scatter plots for each variables against price using seaborn."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.PairGrid(clean_data, y_vars=[\"price\"], x_vars=[\"year\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"])\ng.map(plt.scatter)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see a clear difference in this plot from the earlier scatter plot. the outliers are removed and the data is now ready for feature engineering."},{"metadata":{},"cell_type":"markdown","source":"# 3c) Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"I have used get_dummies() function in pandas to code the catagorical data columns in our clean_data variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_coded_data= pd.get_dummies(clean_data, columns=['transmission','model', 'fuelType'])\nclean_coded_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Model, Transmission and Fuel type columns are coded properly.\n\nWe observe form the data that some columns have higher values on average than others. For instance, the engizeSize column has values form 0 to 2.5 while the price column has values form 995 to 91874.\n\nMachine learning prediction models work best if the data is scaled or standardized. I used the MinMaxScaler() from sci-kit learn to scale the values of each numerical column. It sets the values between 0-1 hence, devlpoping homoginity of scale in the whole data.\n\nHowever, interestingly, it did not improve my prediction in any way. I then, moved to use StandardScaler(), another function from sci-kit which transforms the values of columns into a normal distribution with mean of 0. Nevertheless, this again, did not bring any imporvement to the prediction.\n\nTherefore, I decided to move on without scaling and standardizing the data. However, I have left the following cell commented to show the logic I used for it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# code for scaling values with MinMaxScaler\n\n#from sklearn.preprocessing import MinMaxScaler\n#scaler= MinMaxScaler()\n\n#def scaleColumns(df, cols_to_scale):\n#    for col in cols_to_scale:\n#        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(clean_coded_data[col])),columns=[col])\n#    return df\n\n#clean_coded_data = scaleColumns(clean_coded_data, ['year','price', 'mileage', 'tax', 'mpg', 'engineSize'])\n#clean_coded_data\n\n# code for standardizing values with StandardScaler\n\n#from sklearn.preprocessing import StandardScaler\n#scaler= StandardScaler()\n\n#def scaleColumns(df, cols_to_scale):\n#    for col in cols_to_scale:\n#        df[col] = pd.DataFrame(scaler.fit_transform(pd.DataFrame(clean_coded_data[col])),columns=[col])\n#    return df\n\n#clean_coded_data = scaleColumns(clean_coded_data, ['year','price', 'mileage', 'tax', 'mpg', 'engineSize'])\n#clean_coded_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3d) Model Development and Testing\n"},{"metadata":{},"cell_type":"markdown","source":"Before we being using models, we must split the data into dependent and independent variables. x variable holds the independent colummns and y holds the price column."},{"metadata":{"trusted":true},"cell_type":"code","source":"x= clean_coded_data.drop(columns='price')\ny= clean_coded_data['price'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's split the data into taining and testing subsets. We have imported the train_test_split module form sci-kit. Further, the size of testing data is set to 30% which is not randomly seleted from data as random_state parimeter is set to 0.\n\nAnother module we have imported is metrics, which will be used for capturing R squre and Mean Average Error for every logarithm we will run."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nxTrain, xTest, yTrain, yTest= train_test_split(x, y, random_state=0, test_size=.30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will be using the following logarithms form sci-kit to tain the data:\n1. LinearRegression\n2. RandomForestRegression\n3. GradientBoostingRegressor\n\nFor each logarithm, I have printed the R square and Average Mean Error as benchmarks to campare their results."},{"metadata":{},"cell_type":"markdown","source":"**Using Linear Regression**"},{"metadata":{},"cell_type":"markdown","source":"We have imported the LinearRegression module form sci-kit learn. LinearRegression alogarithm works on the priciple of minizing the distance of each data point to the line of best fit.\n\n1. regressor variable sets the function LinearRegression() offered by LinearRegression module. \n2. fit() function is used to train xTrain and yTrain variables which hold our training data.\n3. predict() function is used to predict the price based on xTest which holds our testing data.\n4. predicted values of prices by the logarithm is stored in yPredict column.\n\nm11 and m21 holds the banchmarking valules (r square and mean average error)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nregressor= LinearRegression()\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm11= metrics.r2_score(yTest, yPredict)*100\nm21= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m11)\nprint('Mean Absolute Error: ', m21)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Random Forest Regressor**"},{"metadata":{},"cell_type":"markdown","source":"Now we will use RandomForestRegressor module form sci-kit learn. RandomForestRegressor works on developing decision trees for prediction. It has single parimeter, n_estimators, which sets the number of trees/nodes which will be formed for prediction. The higher the trees, the more accurate the prediction will be.\n\n1. regressor variable sets the function RandomForestRegressor() at n_estimators=150. \n2. fit() function is used to train xTrain and yTrain variables which hold our training data.\n3. predict() function is used to predict the price based on xTest which holds our testing data.\n4. predicted values of prices by the logarithm is stored in yPredict column.\n\nm12 and m22 holds the banchmarking valules (r square and mean average error)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor= RandomForestRegressor(n_estimators=150)\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm12= metrics.r2_score(yTest, yPredict)*100\nm22= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m12)\nprint('Mean Absolute Error: ', m22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Gradient Boosting Regressor**"},{"metadata":{},"cell_type":"markdown","source":"Finally, we have used GradientBoostingRegressor module form sci-kit learn. GradientBoostingRegressor is a powerful logarithm based on several weak regression logarithms which are combined to bring accuracy in prediction. It has several parameters but, we will use n_estimators and max-depth in the next steps to optamize the prediction model.\n\n1. regressor variable sets the function GradientBoostingRegressor at default n_estimators=100 and max_depth=3.\n2. fit() function is used to train xTrain and yTrain variables which hold our training data.\n3. predict() function is used to predict the price based on xTest which holds our testing data.\n4. predicted values of prices by the logarithm is stored in yPredict column.\n\nm13 and m23 holds the banchmarking valules (r square and mean average error)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nregressor= GradientBoostingRegressor()\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm13= metrics.r2_score(yTest, yPredict)*100\nm23= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m13)\nprint('Mean Absolute Error: ', m23)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parimeter Optimization for Gradient Boosting Regressor**"},{"metadata":{},"cell_type":"markdown","source":"We need to configure a pair of n_estimators and max_depth parimeters for GradientBoostingRegressor logarithm to bring maximum prediction accuracy.\n\nOur approach is to run a loop for 100 times which will pick random values of n_estimators and max_depth. These random values will be used to run the logarithm, and the corrosponding r square will be stored in a list named results. The results list will contain another list of 3 values, r square, n_estimators value and max_depth value."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import random\n\nresults= []\nfor i in range(100):\n    n_est= random.randrange(100,200)\n    max_dep= round(random.uniform(3,10),1)\n    \n    regressor= GradientBoostingRegressor(n_estimators=n_est, max_depth=max_dep)\n    regressor.fit(xTrain, yTrain)\n    yPredict= regressor.predict(xTest)\n    \n    r2= metrics.r2_score(yTest, yPredict)\n    \n    output=[r2, n_est, max_dep]\n    results.append(output)\n    \nresults[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to slect the list in results which has highest r square which has been done using max() function. This brings our optimal configuration of n_estimators and max_depth. Finally, for one last time, we will run the GradientBoostingRegression with these optimal parameter values and update the benchmak values m13 and m23."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"r2, n_estimators,max_depth= max(results)\n\nregressor= GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth)\nregressor.fit(xTrain, yTrain)\nyPredict= regressor.predict(xTest)\n\nm13= metrics.r2_score(yTest, yPredict)*100\nm23= metrics.mean_absolute_error(yTest, yPredict)\n\nprint('R Square %: ', m13)\nprint('Mean Absolute Error: ', m23)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3e) Model Results"},{"metadata":{},"cell_type":"markdown","source":"Since, we have stored the benchmarks for all models, we now create a pandas data frame to store them in a table."},{"metadata":{"trusted":true},"cell_type":"code","source":"metric_results= {'Model': ['linear Regression', 'Random Forest', 'Gradient Boosting'], \n                 'R Square': [m11, m12, m13], \n                 'MAE': [m21, m22, m23]}\nmetrics= pd.DataFrame(metric_results)\nmetrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on this table, we now conclude GradientBoostingRegressor to be efficient in bringing accurate predictions in used car prices. It has explained 95% of the variation in price of used Skoda cars in UK with an average error of below 910 dollars."},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction= pd.DataFrame({'actual price': yTest, 'predicted price': yPredict})\nsns.relplot(data=prediction, x='actual price', y='predicted price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe form the above graph that we can do better to predict the prices of more expensive cars. Our model brings good results in the less expenisve cars.\n"},{"metadata":{},"cell_type":"markdown","source":"# 4) Business Value of Results"},{"metadata":{},"cell_type":"markdown","source":"We are now able to explain the variation in Prices of used Skoda cars in UK. Our model have removed 95% of the uncertanity in Prices of these cars. Further, with an average error of 910$, we will be able to predict the prices using the following features:\n* Model of the Skoda car\n* Year of introduction\n* Transmission type\n* Mileage of the car\n* Fuel Type used by the car\n* Tax on the car\n* Miles Per Gallon\n* Engine Size of the car\n\nPrice prediction is an important metric for car dealers. It helps them to manage their profits. A car dealer dealing in Skoda cars can use this data to set buying price and manage his profit margin. Further, it also helps in inventory management. As expensive cars are bought less, a dealer can use this prediction model to optamize his inventory of expensive and affordable cars thus, minimizing his inventory costs.\n\nAnother use can be in car modification and repair service. Car modification businesses can set their prices based on features added and removed. For instance, an owner of workshop might be able to see the market value of converting a manual transmission car to automatic. The cofficients of transmission feature will help him see how much transmission type influence price of a car. Thus, he will be able to manage his comissions and profits.\n\nOn the other hand, a customer can make use of this model to know the market value of a used Skoda car. This will help him to pay the true market price for the car. Further, he will be aware of dealers comission and can bargain better while buying."},{"metadata":{},"cell_type":"markdown","source":"# 5) Further Scope"},{"metadata":{},"cell_type":"markdown","source":"The model can also be used to predict prices for other brands like BMW, Nissan, Toyota etc. \n\nAlthough we have removed 95% of the uncertaninty related to price variations, our mean average error can be improved. An inflated MAE with a high R square can make a model unhelpful.\n\nFinally, as already discussed, our model does a good job for prediction less expensive cars. However, it can be improved in price prediction for expensive used cars. This can be done either adding more data rows on expensive cars or dividing the data into expensive and less expensive cars, and building two different prediction models."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}