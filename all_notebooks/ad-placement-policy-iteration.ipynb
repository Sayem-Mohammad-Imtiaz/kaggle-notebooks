{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Predicting ad positioning using Q-Learning","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Placement of ads on website is the primary problem for companies that operate on ad revenue. The position where the ad is placed plays pivotal role on whether or not the ad will be clicked. Here we have the following choices:\n1. Place them randomly, or\n2. Place the ad on the same position\n\nThe problem with placing the ad on the same position is the user, after a certain time, will start ignoring the space since he's used to seeing ad at the place, he will end up ignoring that particular position hereafter. Hence, this will reduce the number of clicks on ads. The problem with the former option, placing them randomly, is it wouldn't take optimal positions into consideration. For instance, text beside images are viewed higher number of times than those text which are placed at a distance. It is infeasible to go through every website and repeat the procedure. \n\nSolution: Reinforcement Learning\n\nUsing Reinforcement Learning we can approximate the human behavior. \n\n### Why Reinforcement Learning? \nWe cannot use traditional Machine Learning here, since it requires:\n1. Huge data\n2. Features\n3. Tuning of many hyperparameters\n\nAnd we neither have huge data, nor features. The only data we have is the position of the banner/ad and whether or not it was clicked. We will use this dataset from Kaggle: https://www.kaggle.com/akram24/ads-ctr-optimisation. We will solve this problem using both model-based (Policy iteration) and model-free methods(Q-Learning & Monte-Carlo). We'll simplify some assumptions for model-based technique. \n\n### Notebook Layout\n1. MDP environment - Understanding the dataset\n2. Random Policy - placing the ads randomly on different webpages\n3. Max policy - placing the ad where it is clicked maximum number of times\n4. Model-based Method - Policy Iteration\n5. Model-free Method - Q-learning & Monte Carlo","metadata":{}},{"cell_type":"markdown","source":"#### Importing Libraries","metadata":{}},{"cell_type":"code","source":"# import routines\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dataset\n\nOur environment will be the dataset. It contains 10 ads position per row having values either 1, when the ad is clicked, or 0 when it is not. Every row can be considered as a state in the space, considering it kind of a navigation across multiple pages (on website, for instance) Lets load the dataset and visualize the first few rows.\n\n1. state = webpage\n2. action = placing the ad at any of the 10 positions on a webpage\n3. reward = +1 if the ad was clicked at the position; else 0\n4. Transition Probability: next webpage that user will end up in is random; therefore, it is 1/(total_webpages -1)","metadata":{}},{"cell_type":"code","source":"env = pd.read_csv('/kaggle/input/ads-ctr-optimisation/Ads_CTR_Optimisation.csv')\nenv.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random policy\n\nIf we were to not have Reinforcement Learning, we would place the ads randomly at given positions. We will now simulate the same.  ","metadata":{}},{"cell_type":"code","source":"# total rewards earned\nreward = 0\n# random policy: for every state, choose a random\n# position for displaying the ad\nfor x in range(len(env)):\n    action = np.random.randint(0, 10)\n    # if the guess was correct, increase the reward\n    if env.values[x][action] == 1:\n        reward += 1\nprint(\"Reward collected: {}\".format(reward))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Max Policy\nAnother question we might ask: is it right to display the ad where it is clicked the most number of times. For instance, there might be a certain position where the ad clicked with a higher probability. Since the values of the rows is either 1 or 0, we can sum across the columns and count the number of times ad in the position was clicked. ","metadata":{}},{"cell_type":"code","source":"clicked_counts = env.values.sum(axis=0)\ncounts = pd.DataFrame({\"ad\": np.arange(1, 11), \"counts\": clicked_counts})\ncounts.set_index(\"ad\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which indicates ad 5 was clicked 2695 times. So if we were to always place an ad on position 5, it would be click around 2695 times. But can we do better? ","metadata":{}},{"cell_type":"markdown","source":"## Dynamic Programming (Policy Iteration) ","metadata":{}},{"cell_type":"code","source":"len(env)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# starting with random policy, choose a random choice for every state in the environment\nstate_size = len(env)\n\nstate_list = []\nfor state in range(state_size):\n    state_list.append(state)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n# action could be placing the ad on any of the 10 positions\naction_space = np.arange(0, 10)\n\npolicy = [random.choice(action_space) for x in range(state_size)]\n# will take random action for the first time\nfirst_time = True\n\n# delta\nsmall_change = 1e-20\n# discount factor\ngamma = 0.9\nepisodes = 0\nmax_episodes = 500\n\nV = dict()\n# last positions reward will be 1 - terminal state\nV[10000] = 1\n\n# initially the value function for all states will be random values close to zero\nfor i in range(state_size):\n    V[i] = np.random.random()\ndeltas = []\nwhile episodes < max_episodes:\n    # policy evaluation (until convergence of state value function)\n    while True:\n        if episodes > max_episodes:\n            break\n        episodes += 1\n        if episodes % 100 == 0:\n            print(\"Current episode: {}\".format(episodes))\n        biggest_change = 0\n        # loop through every state present\n        for state in range(state_size):\n            old_V = V[state]\n            # take random action according to policy\n            action = policy[state]\n            #print(action)\n            new_state = random.choice(list(set(state_list) - set([state])))\n            #print(new_state)\n            reward = env.values[state][action]\n            #\n            V[state] = (reward + gamma * V[new_state])/9999\n            # We're calculating biggest change to have an idea on convergence. \n            # Initially, the changes will be huge, but as \n            # we update the values, they will tend towards a convergence point\n            biggest_change = max(biggest_change, abs(V[state] - old_V))\n        deltas.append(biggest_change)\n        if biggest_change < small_change:\n            break\n            \n    # policy improvement\n    policy_changed = False\n    for state in range(state_size):\n        best_val = -np.inf\n        best_action = -1\n        for action in action_space:\n            new_state = random.choice(list(set(state_list) - set([state])))\n            reward = env.values[state][action]\n            # calculate the action with the best\n            # future reward\n            future_reward = (reward + gamma * V[new_state])/9999\n            if future_reward > best_val:\n                best_val = future_reward\n                best_action = action\n        assert best_action != -1\n        # After convergence, the policy will not change since we would have already reached\n        # the optimum policy. So check if the policy is not updated, if not then stop. \n        if policy[state] != best_action:\n            policy_changed = True\n        policy[state] = best_action\n\n    if not policy_changed:\n        break\n\nend = time.time()\nprint(\"Time taken in seconds: \", end-start)\nprint(\"Total episodes trained: {}\".format(episodes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (8, 4)\nplt.plot(deltas, label=\"deltas\")\nplt.legend()\nplt.title(\"Convergence Plot - Dynamic Programming (Policy Iteration)\")\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total rewards earned\nreward = 0\n# random policy: for every state, choose a random\n# position for displaying the ad\nfor x in range(len(env)):\n    action = policy[x]\n    # if the guess was correct, increase the reward\n    if env.values[x][action] == 1:\n        reward += 1\nprint(\"Reward collected: {}\".format(reward))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Q-Learning\n\nThe reason for using Q-Learning here is :\n1. It is model free, so it doesn't require to know all the states.\n2. Intuitive to understand, and converges faster. \n\nWe will use Q-Learning with Epsilon Decay. Initially, we will start with a very high epsilon value. Which would make the model explore, instead of exploit. After a few iterations, we will reduce this \"exploration probability\" so the model will exploit, instead of exploring. After many iterations, the model would have high confidence and no more exploration is necessary. \n\nThis strategy is called as \"epsilon greedy\". We will use a decay rate to change the epsilon value such that it becomes lower after many iterations. \n","metadata":{}},{"cell_type":"code","source":"# using q-learning\nstates = len(env)\nactions = 10\n# initialize q-table with zeros. Initially all the q-values will be zero\nq_table = np.zeros((states + 1, actions))\n\nlearning_rate = 0.7\ngamma = 0.618\n\n# set the exploration probability to be very high initially. \nepsilon = 1.0\nmax_epsilon = 1.0\nmin_epsilon = 0.01\ndecay_rate = 0.01\nmax_episodes = 500","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def exploit(eps):\n    \"\"\"Randomizes a number to select\n    whether or not to expolit\"\"\"\n    return np.random.uniform() > eps\n\ndef random_action():\n    return np.random.randint(0, 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deltas = []\nreward = 0\nfor episode in range(max_episodes + 1):\n    if episode % 50 == 0:\n        print(\"Episode: {}\".format(episode))\n    biggest_change = 0\n    for state in range(states):\n        if exploit(epsilon):\n            action = random_action()\n        else:\n            action = np.argmax(q_table[state])\n        r = env.values[state][action]\n        reward += r\n        old_q = q_table[state][action]\n        new_state = random.choice(list(set(state_list) - set([state])))\n        q_table[state][action] += learning_rate*(r + gamma*np.max(q_table[new_state, :]) - \n                                                 q_table[state][action])\n        biggest_change = max(biggest_change, np.abs(q_table[state][action] - old_q))\n    # keep track of biggest changes\n    deltas.append(biggest_change)\n    # epsilon decay to reduce exploration and increase exploitation\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(deltas, label=\"deltas\")\nplt.legend()\nplt.title(\"Convergence Plot - Q-Learning\")\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\nrewards = 0\nfor state in range(states):\n    best_action = np.argmax(q_table[state, :])\n    r = env.values[state][best_action]\n    rewards += r\nprint(\"Reward collected: {}\".format(rewards))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Monte Carlo\n","metadata":{}},{"cell_type":"code","source":"def random_policy(state):\n    \"\"\"Returns a random available choice\"\"\"\n    return np.random.randint(0, 10)\n\ndef simulate(policy):\n    # store the reward-state-action triple\n    rsa = []\n    for s in range(env.shape[0]):\n        # 10% probability that the agent will act randomly\n        if np.random.uniform() < 0.1:\n            action = np.random.randint(0, 10)\n        else:\n            action = policy[s]\n        reward = env.values[s][action]\n        rsa.append((reward, s, action))\n        \n    # go reverse and distribute rewards\n    G = 0\n    gamma = 0.3\n    first = True\n    state_returns = []\n    for reward, s , a in reversed(rsa):\n        G = G + gamma * reward\n        if first:\n            first = False\n            continue\n        else:\n            state_returns.append((s, a, G))\n    # we want the rewards to be the way they were distributed\n    state_returns.reverse()\n    return state_returns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\n# creates a random policy for every timestep\npolicy = {s: np.random.randint(0, 10) for s in range(10000)}\nQ = np.zeros(env.values.shape)\nreturns = defaultdict(list)\ndeltas = []\nmax_iterations = 2000\nfor x in range(max_iterations):\n    if x % 100 == 0:\n        print(x)\n    # simulate an episode, and get the\n    # reward-state-action triple\n    state_returns = simulate(policy)\n    change = 0.0\n    for state, action, G in state_returns:\n        old_q = Q[state][action]\n        returns[(state, action)].append(G)\n        # calculates the average returns of being in this state\n        Q[state][action] = np.mean(returns[(state, action)])\n        # save the change so we can later on visualize the rate of convergence\n        change = np.maximum(change, np.abs(Q[state][action] - old_q).astype(np.int))\n    deltas.append(change)\n    for s in range(10000):\n        policy[s] = np.argmax(Q[s, :])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(deltas, label=\"deltas\")\nplt.legend()\nplt.title(\"Convergence Plot - Monte Carlo\")\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# once the policy is learnt, we will test it on the actual environment\nreward = 0\nfor s in range(10000 - 1):\n    a = np.argmax(Q[s, :])\n    reward += env.values[s][a]\nprint(reward)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}