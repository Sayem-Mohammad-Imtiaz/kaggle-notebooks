{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Breast Cancer Dataset\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\nAlso can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nAttribute Information:\n\n1) ID number\n2) Diagnosis (M = malignant, B = benign)\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 / area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry\nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is unrelated to the class .. It just helps displaying all outputs in a cell instead of just last one\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the breast cancer dataset in tensorflow from UCI.edu\nimport tensorflow as tf\ndataset= tf.keras.utils.get_file(\"breast_cancer_data\", \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\")\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the files by creating the columns as mentioned in the data above.\nimport pandas as pd\ncolumn_name=['ID_number','Diagnosis','Radius', 'Texture', 'Perimeter', 'Area','Smoothness','Compactness','Concavity','Concave_points','Symmetry','Fractal_dimension','Radius_se', 'Texture_se', 'Perimeter_se', 'Area_se','Smoothness_se','Compactness_se','Concavity_se','Concave_points_se','Symmetry_se','Fractal_dimension_se', 'Radius_worst','Texture_worst', 'Perimeter_worst', 'Area_worst','Smoothness_worst','Compactness_worst','Concavity_worst','Concave_points_worst','Symmetry_worst','Fractal_dimension_worst']\ndf=pd.read_csv(dataset, names=column_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring some of the data\ndf.head()\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the number of Maliginant and Benign patients in the data\nprint(\"Number of Maliginant: \",len(df[df[\"Diagnosis\"]=='M']))\nprint(\"Number of Benign: \",len(df[df[\"Diagnosis\"]=='B']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using seaborn to plot the data\nimport seaborn as sns\nimport matplotlib.pylab as plt\n%matplotlib inline\ndf.hist(figsize=(20,20),color='navajowhite')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As in the dataset we can see that diagnosis column consists of 'M'and 'B' but we need to convert them into numericals\ndf['Diagnosis']=df['Diagnosis'].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the Diagnosis data\nsns.distplot(df[\"Diagnosis\"],bins=20, kde=False, rug=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the unnecessary columns\ndata=df.drop(['ID_number'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split datasets, one for training & test, and using scaler to scale the datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\nX = data.loc[:, data.columns != 'Diagnosis']\nY = data['Diagnosis']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=60)\nX_train_scale = scalar.fit_transform(X_train)\nX_test_scale = scalar.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train_scale, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing the training score. \ntrain_score = logreg.score(X_train_scale, y_train)\nprint('Training accuracy is ', train_score)\n# Showing the testing score. \ntest_score = logreg.score(X_test_scale, y_test)\nprint('Testing accuracy is ', test_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the coefficient and intercept from above linear regression model\nlogreg.coef_\nlogreg.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating the probability\npred= logreg.predict(X_test_scale)\nprob= logreg.predict_proba(X_test_scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at the data set prediction and actual label\nrel=pd.DataFrame(prob)\nrel[\"pred\"]= pred\nrel[\"actual_Label\"]= y_test.to_list()\nrel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the recall, precision and F1 Score\nfrom sklearn.metrics import classification_report, confusion_matrix\n# Creating the confusion Matrix\nprint(confusion_matrix(y_test, pred))\n# Creating the classification report\nprint(classification_report(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the RoC Curve\nfrom sklearn import metrics\nfpr,tpr,thre= metrics.roc_curve(y_test, prob[:,1])\nplt.plot(fpr, tpr)\n\n# Calculate the area under the curve\nmetrics.auc(fpr,tpr)\nplt.plot([0,1], [0,1], 'k--')\nplt.title('ROC curve')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\n\n# Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that when we use logistic Regression model that the accuracy is 98% and we know Accuracy is not only oject to determine if the model is good or not. We have to look at the prediction, recall and F1 score as well. We can see that through classification matrix. We can see that the higher the AUC, better the model to predict. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncfm = confusion_matrix(y_test, pred)\n\ntrue_negative = cfm[0][0]\nfalse_positive = cfm[0][1]\nfalse_negative = cfm[1][0]\ntrue_positive = cfm[1][1]\n\nprint('Confusion Matrix: \\n', cfm, '\\n')\n\nprint('True Negative:', true_negative)\nprint('False Positive:', false_positive)\nprint('False Negative:', false_negative)\nprint('True Positive:', true_positive)\nprint('Correct Predictions', \n      round((true_negative + true_positive) / len(pred) * 100, 1), '%')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DNN Tensorflow Keras","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using normalization in the dataset\nimport tensorflow as tf\ntrain_set_x=tf.keras.utils.normalize(X, axis=1)\ntrain_set_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Using tensflow to build a model using various hidden layers\nfrom tensorflow.keras import models, layers, regularizers\nmodel = models.Sequential()\nmodel.add(layers.Dense(200, activation='relu', input_dim=30)) # Since we have 30 features we will get input_dim=30\nmodel.add(layers.Dense(120, activation = 'relu', kernel_initializer='uniform'))\nmodel.add(layers.Dense(55, activation = 'relu', kernel_initializer='uniform'))\nmodel.add(layers.Dense(1, kernel_initializer='uniform', activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have used tensorflow keras model, where we have used hidden layers of 200, 150 55 and ouput as 1. This is because  it is a binanry output of 0 and 1. Relu is used as activation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer = tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_set_x, Y, epochs=500,batch_size=50)# epochs is the iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(train_set_x, Y)\nprint(model.metrics_names[1], scores[1]*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy of this model is 96.30%, which is good but not better like logistic regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_keras = model.predict(train_set_x[:5])\npred_keras = model.predict_classes(train_set_x[:5])\nprint(prob_keras)\npred_keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\npred = model.predict_classes(train_set_x)\nprint(confusion_matrix(Y, pred))\nprint(classification_report(Y, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprob_keras = model.predict(train_set_x)\n#Plotting the RoC Curve\nfpr,tpr,thre= metrics.roc_curve(Y, prob_keras)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr)\nplt.title('ROC curve')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\n\n# Calculate the area under the curve\nmetrics.auc(fpr,tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the Precision, Recall and F1 scores are high and above 96. When we see the ROC curve its really nice curve and close towards 1. Here we can see the area under curve (AUC) is 99.56%. This is similar to logistic regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train_scale, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the Train Accuracy\ntrain_score = rfc.score(X_train_scale, y_train)\nprint(\"Train Accuracy:\",train_score)\n\n# Calculate the Test Accuracy\ntest_score = rfc.score(X_test_scale, y_test)\nprint(\"Test Accuracy:\",test_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the model\nrfc_pred = rfc.predict(X_test_scale)\nrfc_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the classification report\nprint(confusion_matrix(y_test, rfc_pred))\nprint(classification_report(y_test, rfc_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprob=rfc.predict_proba(X_test_scale)\n\n# Plot the ROC Curve for test\n\nfpr,tpr,thre=metrics.roc_curve(y_test, prob[:,1]) \nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.title('ROC curve')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\n\n# Area under curve\nmetrics.auc(fpr,tpr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, can see that all three models have very High accuracy and auc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}