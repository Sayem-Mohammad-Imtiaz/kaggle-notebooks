{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c873757f-a287-c9c6-e9c9-29cba2cb7ec5"},"source":"# Explore Mating Behavior"},{"cell_type":"markdown","metadata":{"_cell_guid":"ae5389b5-63c9-11c4-5601-a617513868bb"},"source":"## Fire up "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ef77584-3937-cd01-8b06-bb2a74778b60"},"outputs":[],"source":"import numpy as np  \nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cross_validation import train_test_split\nfrom ggplot import *"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b08c5b1d-d530-7982-40d6-de4a3fafe160"},"outputs":[],"source":"df = pd.read_csv('../input/baboon_mating.csv')\ndf1 = df\ndel df1['female_id']\ndel df1['male_id']\ndel df1['cycle_id']\ndf1.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"655c431c-69f9-80f3-07c9-a1790ac05514"},"source":"Seems we got two labels in this data set. Let's analyze them one by one. **Since it seems to be possible that the female and male themselves can influence both the mating and conception probabilities, I decide to delete those three columns first in order to better explore the influences of general biological factors on the labels.** "},{"cell_type":"markdown","metadata":{"_cell_guid":"0b19a0f2-8c6a-13b7-6fc6-94980957b0b0"},"source":"## Consorting Behavior"},{"cell_type":"markdown","metadata":{"_cell_guid":"88308b3f-122e-3f5c-8204-925620879ed7"},"source":"Before we start, we split the data set into training and testing set in order to ensure the feasibility of analysis. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b44fef2-a0e9-775a-1c22-0cda280a04c5"},"outputs":[],"source":"df2 = df1\ndel df2['conceptive']\ntrain_1,test_1 = train_test_split(df2,test_size=0.2,random_state=99)"},{"cell_type":"markdown","metadata":{"_cell_guid":"01673068-0ced-adca-66c8-33fd0af03944"},"source":"**A: Exploratory Data Analysis**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c576087-eebe-c5f2-4c85-8b2c71386599"},"outputs":[],"source":"print(train_1.describe(include = 'all'))"},{"cell_type":"markdown","metadata":{"_cell_guid":"a8f0bdda-651b-b7be-e62e-fa188bbfb682"},"source":"We first make a Correlation plot to briefly check the relationships between variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74194f58-ab2f-7356-7e69-6f647aed2b4d"},"outputs":[],"source":"Cor_matrxi = train_1.iloc[:,1:].corr(method='pearson', min_periods=1)\nprint(Cor_matrxi)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe46a24e-076e-6e8e-1b99-4974e3738e82"},"outputs":[],"source":"fig, ax = plt.subplots()\nheatmap = ax.pcolor(Cor_matrxi, cmap=plt.cm.Blues, alpha=0.8)\nfig = plt.gcf()\nfig.set_size_inches(6, 6)\nax.set_frame_on(False)\nax.set_yticks(np.arange(15) + 0.5, minor=False)\nax.set_xticks(np.arange(15) + 0.5, minor=False)\nax.set_xticklabels(train_1.columns[1:17], minor=False)\nax.set_yticklabels(train_1.columns[1:17], minor=False)\nplt.xticks(rotation=90)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a97a837e-f4ee-059a-c338-ee02636b2d81"},"source":"I am surprised about the fact that male and female genetic variables are not necessarily strongly connected with each other.  No biology background makes me confused about several facts, but I will try to understand it in a pure data mining way. Some variables are somehow redundant. For example, since we just have male_rank transform in the data set, it is of no sense to keep it alone, and the rank_interact can represent the ranks of male and female. Therefore, we just keep this variable of ranking for analysis. For the transform variables, I do not quite understand what do they mean, but considering the correlations between the original variables and them are not strong, I have decided to keep them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da184778-ff41-f235-f7a9-b60159340f9b"},"outputs":[],"source":"variables = ['consort','female_hybridscore','male_hybridscore','female_gendiv','male_gendiv','female_age','males_present','females_present','gen_distance_transform','rank_interact','female_age_transform','assort_index','gen_distance']"},{"cell_type":"markdown","metadata":{"_cell_guid":"c4c86220-80cf-5e9f-ebc1-c26125226e2e"},"source":"Then I am gonna draw the box plots of consorting results and the continuous variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76723b19-5a6f-4533-8988-ff1118ceb5a0"},"outputs":[],"source":"con = train_1[variables]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d38c8821-8585-363f-8285-5b05b3cd17be"},"outputs":[],"source":"for i in range(1,13):\n    g= ggplot(con,aes(x= 'consort',y=variables[i]))+geom_boxplot()+ggtitle('Box Plot of Consorting Result and '+variables[i])+theme_bw()\n    print(g)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b0ba4cfc-973e-fe45-45ad-2d8113cfa258"},"source":"I am afraid I don't see a lot of influential factors in this way. Several features that are worth notices are: rank interact and male present. "},{"cell_type":"markdown","metadata":{"_cell_guid":"7f74610e-6687-411c-6e4e-89f4f7661e84"},"source":"Next, we are gonna plot several scatter plots to further explore the influence of features on the consorting result."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d40f8fa-a833-f41e-0edf-eff77a5875b3"},"outputs":[],"source":"con['label'] = con['consort'].apply(lambda x:str(x))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4c4783c-89d4-6fbe-c374-22e7fc09e442"},"outputs":[],"source":"g=ggplot(con,aes(x='female_hybridscore',y='male_hybridscore',color='label')) +geom_point() +theme_bw()+facet_grid('label')+ggtitle('Hybrid Score VS Consorting Behavior')\nprint(g)"},{"cell_type":"markdown","metadata":{"_cell_guid":"41f767e2-0482-7d52-3b3f-5c42db15841c"},"source":"Seems the influence of hybridscore  is not significant."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8dd96ca-3bbd-bf59-6cde-f4212b936d15"},"outputs":[],"source":"g=ggplot(con,aes(x='female_gendiv',y='male_gendiv',color='label')) +geom_point() +theme_bw()+facet_grid('label')+ggtitle('Gen Div VS Consorting Behavior')\nprint(g)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cee37905-5d04-f14e-659c-7c864bace971"},"source":"Gendiv Variables show similar pattern"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"874f8235-142a-aacf-0258-7d69410fa0de"},"outputs":[],"source":"g=ggplot(con,aes(x='females_present',y='males_present',color='label')) +geom_point() +theme_bw()+facet_grid('label')+ggtitle('Present Data VS Consorting Behavior')\nprint(g)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b7c4c56-559f-306c-b9b8-733c982673ce"},"source":"Also, the influence of male/female present is not distinct enough. "},{"cell_type":"markdown","metadata":{"_cell_guid":"4be6ba33-88e7-5102-c3f4-27bf3ff64832"},"source":"The road of EDA is always long and tough. Therefore, I will not present all the possible EDA here. A general conclusion is that the features may not be quite distinct when doing classification. Therefore, I am not sure the effectiveness of potential classifiers."},{"cell_type":"markdown","metadata":{"_cell_guid":"cdf20c02-0de1-33a6-155f-16e3aba25654"},"source":"**B: Models Fit**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5dbe0873-2f1c-07cd-162e-12f608df8886"},"outputs":[],"source":"del con['label']"},{"cell_type":"markdown","metadata":{"_cell_guid":"511f85ca-0121-0daa-b0fa-e8860dd46e17"},"source":"I will try to fit seven classifiers in this case. Also, I will compare the result of models using all features and the features we choose to use above."},{"cell_type":"markdown","metadata":{"_cell_guid":"a1342735-a119-0a95-6eb6-a165408da736"},"source":"**All Features**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa145a41-1db7-36f5-78bc-c3d2f76c8380"},"outputs":[],"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1b0549c-74e2-732b-1624-975b1ffdb81b"},"outputs":[],"source":"Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    GradientBoostingClassifier(n_estimators=200)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72c95996-85f3-7493-bb23-21fb64cbb02d"},"outputs":[],"source":"All_features = train_1.iloc[:,1:]\nTest_features = test_1.iloc[:,1:]\nLabel = train_1.iloc[:,0]\nModel = []\nAccuracy = []\nfor clf in Classifiers:\n    fit=clf.fit(All_features,Label)\n    pred=fit.predict(Test_features)\n    Model.append(clf.__class__.__name__)\n    Accuracy.append(accuracy_score(test_1['consort'],pred))\n    prob = fit.predict_proba(Test_features)[:,1]\n    print('Accuracy of '+clf.__class__.__name__ +' is '+str(accuracy_score(test_1['consort'],pred)))\n    fpr, tpr, _ = roc_curve(test_1['consort'],prob)\n    tmp = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n    g = ggplot(tmp, aes(x='fpr', y='tpr')) +geom_line() +geom_abline(linetype='dashed')+ ggtitle('Roc Curve of '+clf.__class__.__name__)\n    print(g)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cfa43aeb-c7a9-915c-2eb7-3bdf6804b7b9"},"source":"We can see that the result of every model is not bad. The models with best performances are : Adaboost, SVC, Random Forest, Gradient Boosting and Logistic Regression. I will try this five classifiers with the second data set where some variables are dropped."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3fc2c340-b9fd-d6ad-071b-1791d0fae5c6"},"outputs":[],"source":"Classifiers_2 = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    RandomForestClassifier(n_estimators=200),\n    GradientBoostingClassifier(n_estimators=200)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b15d03e-db24-b83d-8b53-3777deece052"},"outputs":[],"source":"All_features_2 = con.iloc[:,1:]\nTest_features_2 = test_1[variables[1:]]\nLabel = con.iloc[:,0]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8de0c77b-74d0-de2f-3ee6-200b33c6440d"},"outputs":[],"source":"Model_2 = []\nAccuracy_2 = []\nfor clf in Classifiers_2:\n    fit=clf.fit(All_features_2,Label)\n    pred=fit.predict(Test_features_2)\n    Model_2.append(clf.__class__.__name__)\n    Accuracy_2.append(accuracy_score(test_1['consort'],pred))\n    prob = fit.predict_proba(Test_features_2)[:,1]\n    print('Accuracy of '+clf.__class__.__name__ +' is '+str(accuracy_score(test_1['consort'],pred)))\n    fpr, tpr, _ = roc_curve(test_1['consort'],prob)\n    tmp = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n    g = ggplot(tmp, aes(x='fpr', y='tpr')) +geom_line() +geom_abline(linetype='dashed')+ ggtitle('Roc Curve of '+clf.__class__.__name__)\n    print(g)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5e232904-7786-46b4-2808-b591999ffb1f"},"source":"We can see that the result of the same model with two data set have ignorable difference. The importance of features can be further tested by the feature importance of the tree model. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6d2a10a-5c3d-8d39-0817-86499171e45b"},"outputs":[],"source":"Model = GradientBoostingClassifier(n_estimators=200)\nFit = Model.fit(All_features,Label)\nimportances = Model.feature_importances_\nindices = np.argsort(importances)[::-1]\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(All_features.shape[1]), importances[indices],\n       color=\"r\",  align=\"center\")\nplt.xticks(range(All_features.shape[1]),indices)\nplt.xlim([-1, All_features.shape[1]])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"89450cdd-4492-c88d-1009-b53ed2c36c7c"},"source":"We can see that only features 13,4,10, 1, 11 and 12 have obvious influence on the model. They are:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24d099f1-b438-12ce-a5a6-3abd045e3e49"},"outputs":[],"source":"print(All_features.columns[13],All_features.columns[4],All_features.columns[10],All_features.columns[1],All_features.columns[11],All_features.columns[12])"},{"cell_type":"markdown","metadata":{"_cell_guid":"18add89c-e167-b3e8-4479-a947905549d9"},"source":"The variables we dropped before, as well as the ones ranked below them, are all not influential to the results."},{"cell_type":"markdown","metadata":{"_cell_guid":"10728941-3dfb-d540-8527-097ccd180865"},"source":"## Conclusion"},{"cell_type":"markdown","metadata":{"_cell_guid":"23d983cd-29d6-2583-72d9-194cc67aaef4"},"source":"In short, the kernel can be concluded with the following points:\n\nA: Without considering the uniqueness of each individual, the most influential feature is the assort index. Following is the genetic distance.\n\nB: Features of the female social rank, age as well as the genetic makeup are all not important in terms of the mating rate. We can guess that this is a male-dominant community where male controls the authority of mating."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}