{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Flower classification with resnet"},{"metadata":{},"cell_type":"markdown","source":"License: public domain."},{"metadata":{},"cell_type":"markdown","source":"This is based on the material presented in [Lesson 1: Image classification](https://course.fast.ai/videos/?lesson=1) and [Lesson 2: Data cleaning and production; SGD from scratch](https://course.fast.ai/videos/?lesson=2) (the first half) of the [Practical Deep Learning for Coders, v3](https://course.fast.ai) course by fast.ai."},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{},"cell_type":"markdown","source":"Make sure the code is reloaded automatically and plots are displayed inline."},{"metadata":{"trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the fast.ai libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.vision import *\nfrom fastai.metrics import error_rate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the data"},{"metadata":{},"cell_type":"markdown","source":"Use the [Flower Color Images](https://www.kaggle.com/olgabelitskaya/flower-color-images) dataset published by Olga Belitskaya.\n\n> The content is very simple: 210 images (128x128x3) with 10 species of flowering plants and the file with labels flower-labels.csv. Photo files are in the .png format and the labels are the integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL_MAP = {\n    0: 'phlox',\n    1: 'rose',\n    2: 'calendula',\n    3: 'iris',\n    4: 'leucanthemum maximum',\n    5: 'bellflower',\n    6: 'viola',\n    7: 'rudbeckia laciniata',\n    8: 'peony',\n    9: 'aquilegia' \n}\n\n# Need to set this to a writable directory.\nMODEL_DIR = '/kaggle/working'\nDATA_DIR = '../input/flower_images/flower_images'\nLABELS = 'flower_labels.csv'\nIMAGE_SIZE = 204  # decreasing this negatively affects the results\n\ndf = pd.read_csv(f'{DATA_DIR}/{LABELS}')\ndf.replace(to_replace=LABEL_MAP, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfms = get_transforms(do_flip=False)\ndata = ImageDataBunch.from_df(DATA_DIR, df, ds_tfms=tfms, size=IMAGE_SIZE)\ndata.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show_batch(rows=5, figsize=(12, 12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.classes), data.c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training: resnet34"},{"metadata":{},"cell_type":"markdown","source":"We will use a [convolutional neural network](http://cs231n.github.io/convolutional-networks), which will be trained for 4 epochs (4 cycles through all the data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = cnn_learner(data, models.resnet34, metrics=error_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.model_dir = MODEL_DIR\nlearn.save('stage-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\nlosses, idxs = interp.top_losses()\nlen(data.valid_ds) == len(losses) == len(idxs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(9, figsize=(15, 11))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_confusion_matrix(figsize=(12, 12), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is not good: lots of mispredictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused(min_val=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model tuning"},{"metadata":{},"cell_type":"markdown","source":"So far, we've been using the model exactly as is.  We've added a few layers to the network and only trained those. It's fast to train and works reasonably well if the network is trained on similar data. See the [Visualizing and Understanding Convolutional Networks](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) paper by Matthew D. Zeiler and Rob Fergus to understand why.\n\nSo let's allow to train the whole network by calling `unfreeze` and compare the error with the previous result."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's revert back to the previous model and look at the learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('stage-1');  # ; omits the output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the plot, pick the learning rate range before the loss starts getting worse and train the whole model."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(1e-6, 1e-2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare the error with the previous result."},{"metadata":{},"cell_type":"markdown","source":"Now let's plot the confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12, 12), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model has improved, but there are still misses."},{"metadata":{},"cell_type":"markdown","source":"## Training: resnet50"},{"metadata":{},"cell_type":"markdown","source":"Let's see if a different network architecture produces better results. See the [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) paper by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = ImageDataBunch.from_df(DATA_DIR, df, ds_tfms=tfms, size=IMAGE_SIZE, bs=64).normalize(imagenet_stats)\nlearn = cnn_learner(data, models.resnet50, metrics=error_rate)\nlearn.model_dir = MODEL_DIR\nlearn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage-1-50')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12, 12), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better results this time."},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused(min_val=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.plot_top_losses(4, figsize=(10, 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if it's possible to improve the model even further based on the plotted learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(3, max_lr=slice(1e-6, 1e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12, 12), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp.most_confused(min_val=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exporting the model"},{"metadata":{},"cell_type":"markdown","source":"Let's export the model so it can be used in production."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.path = Path(MODEL_DIR)\nPKL_FILE = Path('resnet-flowers.pkl')\nlearn.export(file=PKL_FILE)\n!ls $MODEL_DIR/$PKL_FILE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After committing the kernel, the model will be available in the Output section."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}