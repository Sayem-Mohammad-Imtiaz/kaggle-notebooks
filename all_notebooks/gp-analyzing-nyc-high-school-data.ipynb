{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nWhat we know from researching the sources for our datasets:\n\n- Only high school students take the SAT, so we'll want to focus on high schools.\n- New York City is made up of five boroughs, which are essentially distinct regions.\n- New York City schools fall within several different school districts, each of which can contains dozens of schools.\n- Our data sets include several different types of schools. We'll need to clean them so that we can focus on high schools only.\n- Each school in New York City has a unique code called a DBN, or district borough number.\n- Aggregating data by district will allow us to use the district mapping data to plot district-by-district differences.\n\n\n## DataSet\n\nThis dataset has been provided and prepared by dataquest.io, This dataset is a continuation of their data cleaning certification program for python. Over the last three mission within this certification program, we explored relationships between SAT scores and demographic factors in New York City Public schools. \n\n- **SAT scores by school:** SAT scores for each high school in New York City.\n- **School attendance:** Attendance information for each school in New York City.\n- **Class size:** Information on class size for each school.\n- **AP test results:** Advanced Placement (AP) exam results for each high school (passing an optional AP exam in a particular subject can earn a student college credit in that subject).\n- **Graduation outcomes:** The percentage of students who graduated, and other outcome information.\n- **Demographics:** Demographic information for each school.\n- **School survey:** Surveys of parents, teachers, and students at each school.\n\n\n### Goal\n\nExplore the dataset to find how High Schools with high average SAT Scores are different than other General Education schools in their district.\n\n### Project Goal\n\nThe goal of this project is to demonstrate pandas and numpy fundamentals regarding data cleaning tasks and data story telling."},{"metadata":{},"cell_type":"markdown","source":"### Setting Up The Environment\n\nFor now, we will read in each dataset into the pandas dataframe and then store the dataframes into a dictionary. We choose to store the dataframes within a dictionary for convenience, otherwise, it would be hard to remember all the dataframes in use within this project\n\n#### Steps\n1. Import the needed modules.\n2. Create a list that will store the file name of the datasets we want to import.\n3. Create an empty dictionary.\n4. Create a function that will, read in the datasets and make a key with file name within the list, but without the .csv attachment.\n5. Check to see if the function worked as intended."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Import need modules and datasets\nimport pandas as pd\nimport numpy\nimport re\n\n#Create a list that contains all your datasets\ndata_files = [\n    \"ap_2010.csv\",\n    \"class_size.csv\",\n    \"demographics.csv\",\n    \"graduation.csv\",\n    \"hs_directory.csv\",\n    \"sat_results.csv\"\n]\n\n#Create a function that will store all are datasets within a dictionary\ndata = {}\n\nfor f in data_files:\n    d = pd.read_csv(\"../input/nyc-high-school-data/nyc_highschool_data/schools/{0}\".format(f))\n    data[f.replace(\".csv\", \"\")] = d\n    \n#Check the dictionary\nprint(data.keys())\nprint('\\n')\n\n#Check dataframes\nfor d in data:\n    print(data[d].head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading In The Surveys\n\nWhen we printed the rows for all dataframes above we notice that the DBN column represents a key for each unique school. Given that we have a unique key represented in each dataframe, we can use these columns to combine dataframes later on. If we check the survey dataframes we notice that dbn column is lower case and not the upper case (DBN) like the rest of the dataframes within the data dictionary. This problem of column name consistency will make it hard to combine such dataframes together later on. Thus, we should look to update the column names in the survey dataframes to match those in the data dictionary. Once adjusted, we should look to filter out columns that will not be needed for analysis later on. Once we have completed the filter, we will import the new combined survey dataframe into the data dictionary with the rest of the dataframes in this project.\n\n#### Steps\n1. Read in both survey datasets in pandas dataframes.\n2. Explore both survey dataframes and find out if there are any inconsistencies with the rest of the dataframes in the data dictionary.\n3. Combine both survey dataframes together using concat.\n4. Create a new column named DBN which will hold the values from the other lower case dbn column.\n5. Create a list of column names we want to filter in the combined survey dataframe.\n6. Filter the the combined survey dataframe with the list of column names.\n7. Add the combined survey dataframe in the data dictionary with the rest of the dataframes."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Read in .txt datasets using the encoding for windows-1252\nall_survey = pd.read_csv(\"../input/nyc-high-school-data/nyc_highschool_data/schools/survey_all.txt\", delimiter=\"\\t\", encoding='windows-1252')\nd75_survey = pd.read_csv(\"../input/nyc-high-school-data/nyc_highschool_data/schools/survey_d75.txt\", delimiter=\"\\t\", encoding='windows-1252')\n\nprint(all_survey.head(3))\nprint('\\n')\nprint(d75_survey.head(3))\n\n#Combine datasframes using concat\nsurvey = pd.concat([all_survey, d75_survey], axis=0)\n\n#Create a new column that aligns with the rest of the dataframes inside the dictionary.\n#Change dbn to DBN by creating a new column and brining over all the same values.\nsurvey[\"DBN\"] = survey[\"dbn\"]\n\n#Preselct the columns that we need to complete future tasks\n#Create a list that contains all the columns that we need\nsurvey_fields = [\n    \"DBN\", \n    \"rr_s\", \n    \"rr_t\", \n    \"rr_p\", \n    \"N_s\", \n    \"N_t\", \n    \"N_p\", \n    \"saf_p_11\", \n    \"com_p_11\", \n    \"eng_p_11\", \n    \"aca_p_11\", \n    \"saf_t_11\", \n    \"com_t_11\", \n    \"eng_t_11\", \n    \"aca_t_11\", \n    \"saf_s_11\", \n    \"com_s_11\", \n    \"eng_s_11\", \n    \"aca_s_11\", \n    \"saf_tot_11\", \n    \"com_tot_11\", \n    \"eng_tot_11\", \n    \"aca_tot_11\",\n]\n\n#Filter the columns in the survey dataframe by using the survey_field list\nsurvey = survey.loc[:,survey_fields]\n\n#Add the survey dataframe into the data dictionary with the rest of the datasets\ndata[\"survey\"] = survey","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add DBN columns\n\nLike the survey columns above there are two other dataframes within the data dictionary that do not hold up the same column named standard of DBN. The hs_directory is adjustment is similar to the survey dataframes in that we will just transfer over the values from the lower case dbn to the new upper case column DBN. However, the class_size dataframe is different in that there is no DBN column. the class_size dataframe contains a CSD and a SCHOOL CODE column that when together create the unique DBN KEY like the other dataframes. However, CSD still needs to be adjusted to fit the standard of DBN values.\n\n#### Steps\n1. Create a new DBN column within the hs_directory dataframe that will hold the values of the old dbn column in the same dataframe.\n2. Create a function that will look at a string and see if it contains more than one character, if it contains one or more character return the string as is, else return the string with a 0 in front of it.\n3. Create a new column named padded_csd that will hold the results for the string function by applying it to the CSD column in the class_size dataframe.\n4. Create a new column named DBN in the class_size dataframe, this column will contain a concat of values from the padded_csd column and SCHOOL CODE column.\n5. update the class_size dataframe within the data dictionary."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#We want to aline the DBN column name with the rest of the datasets within the data dictionary\n#Change dbn to DBN by creating a new column and bring over the same values from the old column\ndata[\"hs_directory\"][\"DBN\"] = data[\"hs_directory\"][\"dbn\"]\n\n#Update the csd to match the format need to align itself with the DBN columns in the other datasets\n#Create a function that will add zero to the front of a string with less than one character.\ndef pad_csd(num):\n    string_representation = str(num)\n    if len(string_representation) > 1:\n        return string_representation\n    else:\n        return \"0\" + string_representation\n    \n#Create a new column namded padded_csd that will take in the applied function results from the csd column  \ndata[\"class_size\"][\"padded_csd\"] = data[\"class_size\"][\"CSD\"].apply(pad_csd)\n\n#Create a new column named DBN within the class_size dataframe to align itself with the other datasets\n#Combine both padded_csd and SCHOOL CODE to create the properally formated DBN column values\ndata[\"class_size\"][\"DBN\"] = data[\"class_size\"][\"padded_csd\"] + data[\"class_size\"][\"SCHOOL CODE\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert columns to numeric\n\nWhen looking at the sat_results dataframe we notice that all the columns datatypes are objects, meaning we cannot perform any numerical analysis on them. This is a problem because we want to find out whether the total SAT schools show any insight into demographics or population within schools in NYC. Additionally, when looking at the hs_directory dataframe we notice that the location 1 column holds the geographic location of the schools. We want to extract the latitude and longitude from the location 1 column so that it is easier to find geological patterns later in the analysis. Since we will be pulling the latitude and longitude each string will contain its own column within the dataframe.\n\n#### Steps \n1. Explore the datatypes within the sat_results dataframe.\n2. Create list of columns from the sat_results dataframe that we want to change to numeric datatypes.\n3. Create a for loop that  the list and changes the datatype to numeric.\n4. Create a column named sat_score that sums all the values within the sat scores.\n5. Create two functions that will either find the lat or long to finding, spliting, replace the location string in the location 1 column within the hs_directory.\n6. Create two columns named lat and lon that uses the newly created lat or long function to apply the function to the location 1 column.\n7. Change the newly created lat and long column in the hs_directory dataframe to numeric datatype and then update the dataframe within the data dictionary."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Explore the datatypes for the sat_results dataframe\nprint(data['sat_results'].info())\n\n#Convert the SAT score columns to numeric dtypes\n#Create a list of the three columns we want to change\ncols = ['SAT Math Avg. Score', 'SAT Critical Reading Avg. Score', 'SAT Writing Avg. Score']\n\n#Write a for loop that changes the the columns datatypes to numeric\nfor c in cols:\n    data[\"sat_results\"][c] = pd.to_numeric(data[\"sat_results\"][c], errors=\"coerce\")\n\n#Create a new column that totals all the SAT Scores\ndata['sat_results']['sat_score'] = data['sat_results'][cols[0]] + data['sat_results'][cols[1]] + data['sat_results'][cols[2]]\n\n#We want the longitutde and latitude in seperate columns within the datafram\n#Createa function that will split the current location string so that all we have left is the longitude value\ndef find_lat(loc):\n    coords = re.findall(\"\\(.+, .+\\)\", loc)\n    lat = coords[0].split(\",\")[0].replace(\"(\", \"\")\n    return lat\n#Createa function that will split the current location string so that all we have left is the latitude value\ndef find_lon(loc):\n    coords = re.findall(\"\\(.+, .+\\)\", loc)\n    lon = coords[0].split(\",\")[1].replace(\")\", \"\").strip()\n    return lon\n\n#Create a column that holds the lat or longitude by using the new function inside the apply function\ndata[\"hs_directory\"][\"lat\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lat)\ndata[\"hs_directory\"][\"lon\"] = data[\"hs_directory\"][\"Location 1\"].apply(find_lon)\n\n#Convert the lat and lon values back into numeric datatypes\ndata[\"hs_directory\"][\"lat\"] = pd.to_numeric(data[\"hs_directory\"][\"lat\"], errors=\"coerce\")\ndata[\"hs_directory\"][\"lon\"] = pd.to_numeric(data[\"hs_directory\"][\"lon\"], errors=\"coerce\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Condense datasets\n\nGiven that the goal of this project is to analyze differences in Highschools across the General Education school districts, we will begin by condensing the dataframe to fit our needs. Next, we want to find the averages across all columns within the dataframe for future analysis.\n\nClass_size Columns:\n- **Number of students/Seats filled:** the total number of students.\n- **Number of sections:** number of class rooms.\n- **Average class size:** average number of students in classroom.\n- **Size of smallest class:** The smallest number of students within a classroom.\n- **Size of largest class:** The largest number of students within a classroom.\n- **Schoolwide pupil - teacher ratio:** Teacher to student ratio.\n\nOnce the class_size dataframe has been filtered and updated, we move onto the demographic dataframe where we will also filter by the most recent school year data. Moving onto the graduation dataframe, we want the DBN to be as unique as possible, so we will opt to filter the dataframe where the cohort is the most recent and the demographic column to contain the total cohort.\n\n#### Steps\n1. Find the unique values for the GRADE AND PROGRAM TYPE columns in the class_size dataframe.\n2. Update the class_size dataframe by filtering the GRADE column by keeping grades 09-12.\n3. Update the class_size dataframe by filtering the PROGRAM TYPE column by keeping the program GEN ED only.\n4. update the class_size dataframe within the data dictionary.\n5. Update the demographics dataframe by filtering the School year column to have the most recent years.\n6. Update the demographics dataframe within the data dictionary.\n7. Update the graduation dataframe by filtering the cohort column to contain them most recent year.\n8. Update the graduation dataframe by filtering the demographic column to contain the total cohort group.\n9. Update the graduation dataframe within the data dictionary.\n"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Check the unique values for the GRADE and PROGRAM TYPE columns in the class_size dataframe\nprint(data['class_size']['GRADE '].value_counts())\nprint('\\n')\nprint(data['class_size']['PROGRAM TYPE'].value_counts())\nprint('\\n')\n\n#Filter class_size dataset to contain highschools only and with a general education program type\nclass_size = data[\"class_size\"]\nclass_size = class_size[class_size[\"GRADE \"] == \"09-12\"]\nclass_size = class_size[class_size[\"PROGRAM TYPE\"] == \"GEN ED\"]\n\n#Group the class size dataset by DBN and aggregate values by the mean\nclass_size = class_size.groupby(\"DBN\").agg(numpy.mean)\nclass_size.reset_index(inplace=True)\nprint(class_size.head(2))\n\n#return the newly adjusted class_size dataset back into the dataset dictionary \ndata[\"class_size\"] = class_size\n\n#Filter the demographics dataset to contain the most recent school years\ndata[\"demographics\"] = data[\"demographics\"][data[\"demographics\"][\"schoolyear\"] == 20112012]\n\n#Filter graduation dataset to contain the most recent school year and demographic to total cohort only\ndata[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Cohort\"] == \"2006\"]\ndata[\"graduation\"] = data[\"graduation\"][data[\"graduation\"][\"Demographic\"] == \"Total Cohort\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert AP scores to numeric\n\nTo better understand if there is a correlation between ap exams and sat scores across schools, We will need to convert three columns datatypes in the ap_2010 dataframe to numeric. Given that ap exams exist in schools that are academically challenging, it stands to reason that high schools with less funding or lack of academic rigor will not participate in ap exams.\n\n#### Steps\n1. Find out the ap_2010 dataframes columns datatype.\n2. Create a list of column names from the ap_2010 dataframe that we want to change to numeric.\n3. Create a for loop that will convert those columns from the list to numeric datatype.\n4. Make sure to update the ap_2010 dataframe within the data dictionary."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Check the column datatypes for the ap_2010 dataframe\nprint(data['ap_2010'].info())\n\n#Create a list of column names that we want to change to numeric\ncols = ['AP Test Takers ', 'Total Exams Taken', 'Number of Exams with scores 3 4 or 5']\n\n#create a for loop that will change the column values into a numeric datatype\nfor col in cols:\n    data[\"ap_2010\"][col] = pd.to_numeric(data[\"ap_2010\"][col], errors=\"coerce\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combine the datasets\n\nNow that we have cleaned all the dataframes to a functional degree, we will start by merging all the dataframes within the data dictionary together by the DBN column. However, we must be careful in how we merge our data together because it could introduce an influx of null values within the combined dataframe. To combat this issue I will use a combination of left joins and inner joins to minimize the null value counts.\n\n#### Steps\n1. Create a new dataframe called combined with the sat_results dataframe inserted into it.\n2. Left join the combined dataframe with ap_2010 and graduation dataframes on DBN individually.\n3. Create a list of dataframes remaining that we want to merge.\n4. Create a for loop that takes the list of dataframes we want to merge, and merges them with an inner join by DBN.\n5. Fill any numeric datatype null values by using the columns mean.\n6. Fill the remainder null string type values with the number zero."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Combined all the data sets using left join or inner join\ncombined = data[\"sat_results\"]\n\n#Combine ap_2010 and graduation data set to the combined dataframe\ncombined = combined.merge(data[\"ap_2010\"], on=\"DBN\", how=\"left\")\ncombined = combined.merge(data[\"graduation\"], on=\"DBN\", how=\"left\")\n\n#Createa list of datasets we want to merge into the combined dataframe\nto_merge = [\"class_size\", \"demographics\", \"survey\", \"hs_directory\"]\n\n#create a for loop that will merge the list of datasets using a inner join\nfor m in to_merge:\n    combined = combined.merge(data[m], on=\"DBN\", how=\"inner\")\n\n#Fill any null values with the mean of their columns\ncombined = combined.fillna(combined.mean())\n\n#Fill the rest of the null values with zero if they are not numeric datatype\ncombined = combined.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add a school district column for mapping\n\nIn order to find the school's district for each highschool easier, I will create a column named school_dist with each schools district number. The school district number can be found within the first two characters within DBN values, thus we will need to create a function that will extract those two strings and put them into our new school_dist column.\n\n#### Steps\n1. Check the DBN column in the combined dataframe to understand its format.\n2. Create a function that will strip the first two strings from any string value.\n3. Create a column called school_dist that will use the newly created function to strip the values from the DBN column."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Check the DBN column in the combined dataframe\nprint(combined['DBN'].head(3))\n\n#Create a function that will take the first two characters from a string\ndef get_first_two_chars(dbn):\n    return dbn[0:2]\n\n#Create a new column named school_district that will use the new fucntion to pull two characters from the DBN column\ncombined[\"school_dist\"] = combined[\"DBN\"].apply(get_first_two_chars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find correlations\n\nWe want to find if there are any strong correlations with the sat_score column, since we speculate that sat scores can be impacted by numerous factors.\n\n#### Steps\n1. Find the correlation for all columns within the dataset save it to a Series named correlations\n2. Update the Correlation series by finding the correlation for the sat_score column."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Find Correlations throughout all the columns within the dataframe\ncorrelations = combined.corr()\n\n#Find the correlations that align with the sat_score column\ncorrelations = correlations[\"sat_score\"]\nprint(correlations.head(40))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting survey correlations\n\nThe 2011 NYC School Survey dictionary for terms can be found [here](https://data.cityofnewyork.us/Education/NYC-School-Survey-2011/mnz3-dyi8), and can be download as and xls file.\n\n#### Steps\n1. Remove DBN from the survey_fields list.\n2. import matplotlib.\n3. Make a barplot from the correlation between sat_score and the column names inside survey_field list.\n\n#### Analysis\n\n- **rr_s:**: The student response rate shows decent correlation, which makes sense because students who take an SAT test are likely to be involved academically.\n- **N_s, N_t, N-P:** Shows strong correlation due to being actualy SAT test scores.\n- **saf_t_11:** sat_scores shows correlation to how teachers feel safe and respected within their classrooms.\n- **saf_s_11:** sat_scores shows correlation to how students feel safe and respected within their classrooms, however the correlation is slightly stronger than saf_t_11.\n- **aca_s_10:** The academic expectations of the students show a strong correlation with the sat scores.\n\nIn conclusion: It looks like there is a strong correlation to sat scores when it comes to teachers’ and students’ environments. According to the correlation, there may be an indication that sat_scores are impacted by how safe and respected teachers and students feel within their classrooms. Additionally, sat_scores might be impacted by the expectation of the student academics, meaning if a student who expects a low score might not score high on the SAT. Lastly, there seems to be an interesting correlation between the engagement of students and their sat scores."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Remove DBN since it's a unique identifier, not a useful numerical value for correlation.\nsurvey_fields.remove(\"DBN\")\n\n#import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#plot the correlations of sat_score and columns in the survey_field list.\ncombined.corr()[\"sat_score\"][survey_fields].plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Investingating Safety Scores.\n\nGiven the decent correlation of safety and respect with SAT scores, I looked to investigate it further by developing a scatter plot. The scatter plot on the left indicates the Student’s safety respect score with SAT scores on the y-axis and the plot on the right identifies the teacher’s safety response with SAT scores on the y-axis.\n\n#### Safety Score and SAT Score\n\nAfter looking at both graphs, it’s apparent that there is a weak positive correlation on both teacher and student graphs. The points look to cluster near the middle with outliers scatter through different safety response numbers. There are points in which higher safety responses do not mean an increase in SAT scores.\n\n#### Avg Borough Safety Score\n\nAcross all boroughs, teachers are scoring their safety and respect higher than students. Brooklyn looks to have the lowest scores across both teachers and student safety scores. The highest score across both teachers and students in Queens and Manhattan.\n\n#### Steps\n1. Determine the figure size.\n2. Determine how to format your subplots.\n3. Create a scatter plot for both teachers and students against sat_score.\n4. Group the combined dataframe by borough and find the average safety score for each."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Createa subplot of \nfig = plt.figure(figsize = (16,5))\nax1 = fig.add_subplot(1,2,1)\nax2 = fig.add_subplot(1,2,2)\n\nax1.scatter(combined['saf_s_11'],combined['sat_score'])\nax1.set_title('Student Safety Response')\nax2.scatter(combined['saf_t_11'], combined['sat_score'])\nax2.set_title('Teacher Safety Response')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Find the average Safety and Respect score for teachers and students by borough\nboro_s = combined.groupby('boro').agg(numpy.mean)['saf_s_11']\nboro_t = combined.groupby('boro').agg(numpy.mean)['saf_t_11']\n\nprint(boro_s)\nprint(boro_t)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Investigating Racial Differences In SAT Scores\n\nWhen looking at the correlation between race and SAT scores, we notice that the white and Asian demographics have a strong positive correlation. The black demographic has a weak negative correlation with SAT scores while the Hispanic demographic has a moderate negative correlation with SAT scores. \n\n#### Explore Schools With Low SAT Scores\n\nWhen looking at the correlation between SAT score and hispanic_per column, there is a downtrend when hispanic_per numbers increase. In other words, when the hispanic_per numbers increase we see a decrease in sat_scores.\n\n\n#### Steps\n1. Create a list that holds the race columns we will use for the barplot.\n2. Create a barplot that shows the correlation between sat scores and race.\n3. Create a scatter plot of sat_score and hispanic_per column.\n4. Find the school name for schools with 95% hispanic per.\n5. Find the school name for schools with 10% hispanic per and greater than 1800 sat_score.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a single graph that has several bar plots that shows correlation of sat scores and different demographics\n\n#Create a list of columns that we want to include for race\nrace_fields= ['white_per', 'asian_per', 'black_per', 'hispanic_per']\n\n#create a bar plot that shows the correlation between sat scores and race\ncombined.corr()['sat_score'][race_fields].plot.bar(title = 'SAT Score by Race')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a scatter plot of hispanic_per and sat_score columns\nfig, ax3 = plt.subplots()\nax3.scatter(combined['hispanic_per'], combined['sat_score'])\nax3.set_title('Correlation Between SAT Score and Hispanic Demographic')\nax3.set_ylabel('SAT Score')\nax3.set_xlabel('Hispanic Per')\nax3.set_ylim(800,2200)\nax3.set_xlim(0,110)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a series that shows the school names for schools with hispanic_per greater than 95%\nhispanic_greater_95 = combined[combined['hispanic_per'] > 95 ]['SCHOOL NAME']\nprint(hispanic_greater_95)\nprint('\\n')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The schools listed above are more geared towards recent immigrants to the U.S and are focused on helping students learn English, this could be attributed to the low SAT scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a series taht shows the school names for schools with hispanic_per less than 10% and avg SAT score greater than 1800.\nhispanic_lower_10 = combined[(combined['hispanic_per'] < 10) & (combined['sat_score'] > 1800)]['SCHOOL NAME']\nprint(hispanic_lower_10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The schools listed above are more focused on Science and may receive financial help to run those programs. Additionally, these schools require students to pass an entrance exam to enter, although this would not explain the low Hispanic population. Given that these schools are focused on sciences and technical skills, could be an indication of why their SAT scores are on the high side."},{"metadata":{},"cell_type":"markdown","source":"### Investigating Gender Differences In SAT Score\n\n\n#### Bar plot Observation\n\nWhen looking at the correlation between gender and SAT Scores, we notice that both genders show weak signs of correlation. Males show a weak negative correlation, while females show a weak positive correlation. \n\n#### Scatter plot Observation\n\nThe scatter plot graphs confirm are barplot observations as it shows that gender has a weak correlation with SAT Scores. However, both the percentage of female and male students in school are clustered around the range of 40 to 60 percent. Furthermore, all the high SAT Scores for both genders are above the cluster and do not span below or above the initial cluster bounds of 40 to 60 percent.\n\n#### School Name Observations\n\n\n\n#### Steps\n1. Create a gender field that will hold the columns names for gender.\n2. Create a barplot that shows the correlation of gender and SAT Scores.\n3. Create a scatter plot for female SAT Scores.\n4. Create a scatter plot for male SAT Scores.\n5. Find the school names for both genders where schools have a gender_per greater than 60 percent and a SAT Score higher than 1700.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a barplot that shows the correlation for both genders against SAT scores\n\n#Create a gender list\ngender_field = ['male_per', 'female_per']\n\n#Create a barplot\ncombined.corr()['sat_score'][gender_field].plot.bar(title = 'Correlation of Gender and SAT Scores')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a scatter plot of both genders and their SAT scores\n#male_per scatter plot with SAT Scores\ncombined.plot.scatter('male_per', 'sat_score', title = 'Male SAT Scores')\n\n#female scatter plot with SAT Scores\ncombined.plot.scatter('female_per', 'sat_score', title = 'Female SAT Scores')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the school names where female_per is greater than 60 percent and the SAT Scores are greater 1700 \nfemale_greater_60 = combined[(combined['female_per'] > 60) & (combined['sat_score'] > 1700)]['SCHOOL NAME']\nprint(female_greater_60)\nprint('\\n')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The schools that meet the criteria above have a strong emphasis on college preparation and humanities studies. There are also more schools that fit these criteria for females than males."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find the school names where male_per is greater than 60 percent and the SAT Scores are greater 1700 \nmale_greater_60 = combined[(combined['male_per'] > 60) & (combined['sat_score'] > 1700)]['SCHOOL NAME']\nprint(male_greater_60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There is only one school that meets the criteria above, this school emphasizes STEM subjects and college preparation. "},{"metadata":{},"cell_type":"markdown","source":"### Investigating The Relationship between AP Scores and SAT Scores\n\nAfter adjusting the ap_scores to ap_per, we notice that the positive correlation between ap_per and sat_score is moderate in strength. Some points show the positive correlation of ap_per and sat_score, however, the majority of points are clustered around 0 to 40% ap_per with a sat_score between 1000 to 1300. Moreover, across the board of ap_per percentages, the ap_scores stay consistent within the range of 1000 to 1300, these results don’t seem too convincing in showing that an increase in ap test takers increases ap scores.\n\n#### Steps \n1. Create a column that holds the results for the division of ap test takers by total enrollment.\n2. Find the correlation between ap_per and sat_score.\n3. Create a scatter plot of ap_per and sat_score."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a column that holds the results for the division of ap test takers by total enrollment\ncombined['ap_per'] = combined['AP Test Takers '] / combined['total_enrollment']\n\nprint(combined.corr()['ap_per']['sat_score'])\n\n#Create a scatter plot with ap_per and sat_score\ncombined.plot.scatter('ap_per', 'sat_score', title = 'Ap Scores and SAT Scores')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}