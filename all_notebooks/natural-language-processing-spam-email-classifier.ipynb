{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://www.facebook.com/codemakerz\"><img src=\"https://scontent.ffjr1-4.fna.fbcdn.net/v/t1.0-9/36189148_736466693143793_2172101683281133568_n.png?_nc_cat=107&_nc_eui2=AeHzxv3SUcQBOfijLP-cEnHkX4z9XQXdeau__2MlErWZ1x07aZ1zx1PzJUDDxL6cpr7oPqYiifggXDptgtP8W5iCoDRjcdILDBYZ5Ig40dqi8Q&_nc_oc=AQmMCNXdzelFB2rdtpk8wN8nC410Wm2yKupYfYS1FxHNejTF0Jhr1G3WIZORKRF3TvFpohMB8Puw29Txxan8CW05&_nc_ht=scontent.ffjr1-4.fna&oh=7b13627e991a4d1b508923041bd7bc22&oe=5D8A7B03\" />\n</a>\nFollow Us:\nFacebook: https://www.facebook.com/codemakerz\n\n<h1>Natural Language Processing - Email Ham & Spam</h1>\n<h3>As a learner i am also looking for new things, Help us with your suggestion and ideas. </h3>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom nltk import FreqDist\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mails = pd.read_csv('/kaggle/input/spam-and-ham/spam.csv',encoding= 'latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mails.head() # HAM - GOOD EMAILS, SPAM - BAD EMAILS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mails.isnull().sum()  #There are no missing value except unnamed column, we dont need those cols.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mails.v1.value_counts()  # We can see there are total 747 spam and 4825 ham","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mails.v1.value_counts().plot(kind=\"bar\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find most popular spam token\ndf_spam = df_mails[df_mails.v1 == 'spam']\ndf_spam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Spacy for tokenization\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\".\".isalpha()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_token=[]\nfamous_keyword = []\nfor spam in np.array(df_spam.v2):\n    doc = nlp(spam.lower())\n    for token in doc:\n        # add famous keywords\n        if token.pos_ == \"NOUN\" or token.pos_ == 'PRON' or token.pos_ == 'PROPN':\n            if not token.text in famous_keyword and not token.is_stop and token.text.isalpha():\n                famous_keyword.append(token.text)\n        # add all spam tokens                              \n        if not token.is_stop and not token.text.isdigit() and token.text.isalpha():\n            spam_token.append(token.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So these are keywords which u will get usually in spam messages\nfamous_keyword[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_token[0:10] # so these are most common unique spam keywords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency Distribution\nfreq_spam = FreqDist(spam_token)\nfreq_spam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so we can see that mostly spam keywrods are entry, free, prize, claim which make sense. In regular life when\n# you recieve any spam message they include these keywords.\nplt.figure(figsize=(15,10))\nfreq_spam.plot(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Train & Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport re # for regula rexpression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before splitting data we will try to reduce the dimensionality of tfid matrix by filtering stop words and \n# lemmatization.\n\ncorpus = []\nfor i in range(df_mails.shape[0]):\n    msg = re.sub('[^a-zA-Z]', ' ', df_mails.v2[i] ) # we will remove all the special characters\n    msg = msg.lower() # change it to lower case\n    doc = nlp(msg) # create spacy document\n    # remove stop words and perform. lemmatization\n    tokens_no_stop = [token.lemma_ for token in doc if not token.is_stop and not token.text.isspace()]\n    msg = ' '.join(tokens_no_stop) # join all the tokens to make sentence.\n    corpus.append(msg) # append to corpus list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Above whole step is to reduce the dimensionality nd to provide only valid text to you model.\ncorpus[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = corpus # Email\ny = df_mails[\"v1\"] # Result Ham or Spam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state=34)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Term Frequency Inverse Document"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nclf = LinearSVC()\nclf.fit(X_train_tfidf,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline"},{"metadata":{},"cell_type":"raw","source":"We applied vecotirzer for train dataset, same we have to apply for test dataset also, In our case there are less step. But there may be case where you have to apply tokenization, lemmatization, stemming, stop words. In this case you have apply all the step again which will take an extra effort. So better way to do it is defining a pipeline. So next time when we need to repeat those step, we can simply call that pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n                     ('clf', LinearSVC()),\n])\n\n# Feed the training data through the pipeline\ntext_clf.fit(X_train, y_train)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = text_clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions # See prediction is in text form whichis very good. IN other machine learning algorithm we\n# need to apply labelencoder.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ncm = metrics.confusion_matrix(predictions, y_test)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we can see we are getting a good prediction and recall for both the cases(ham & spam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.accuracy_score(y_test, predictions)) # we got an accuray of 98% which is really amazing.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test New Email"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_clf.predict([\"Weekly Lottery Participation. Win upto $10,000.\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_clf.predict([\"Hello Sir. How are you?\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So it is working as we expected. Try some more messages. !!! \n# Thank you... !! UPVOTE if you like the code. ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}