{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](http://)Now we'll expand on SHAP values, seeing how aggregating many SHAP values can give more detailed alternatives to permutation importance and partial dependence plots.\n\n# SHAP Values Review\n\nShap values show how much a given feature changed our prediction (compared to if we made that prediction at some baseline value of that feature).\n\nFor example, consider an ultra-simple model.\n    $$y = 4 * x1 + 2 * x2$$\n\nIf $x1$ takes the value 2, instead of a baseline value of 0, then our SHAP value for $x1$ would be 8 (from 4 times 2)...\n\nThese are harder to calculate with the sophisticated models we use in practice. But through some algorithmic cleverness, Shap values allow us to decompose any prediction into the sum of effects of each feature value, yielding a graph like this:\n\n![Imgur](https://i.imgur.com/JVD2U7k.png)\n\n[Link to larger view](https://i.imgur.com/JVD2U7k.png)*\n\nIn addition to this nice breakdown for each prediction, the [Shap library](https://github.com/slundberg/shap) offers great visualizations of groups of Shap values. We will focus on two of these visualizations. These visualizations have conceptual similarities to permutation importance and partial dependence plots.\n\n# Summary Plots\n\n[Permutation importance](https://www.kaggle.com/dansbecker/permutation-importance) is great because it created simple numeric measures to see which features mattered to a model. This helped us make comparisons between features easily, and you can present the resulting graphs to non-technical audiences\n\nBut it doesn't tell you how each features matter. If a feature has medium permutation importance, that could mean it has\n- a large effect for a few predictions, but no effect in general, or\n- a medium effect for all predictions\n\nSHAP summary plots give us a birds-eye view of feature importance and what is driving it. We'll walk through an example plot for the soccer data:\n\n![Imgur](https://i.imgur.com/Ew9X3su.png)\n\nThis plot is made of many dots. Each dot has three characteristics:\n- Vertical location shows what feature it is depicting\n- Color shows whether that feature was high or low for that row of the dataset\n- Horizontal location shows whether the effect of that value caused a higher or lower prediction.\n\nFor example, the point in the upper left was for a team that scored few goals, reducing the prediction by 0.25.\n\nSome things you should be able to easily pick out:\n- The model ignored the `Red` and `Yellow & Red` features. \n- Usually `Yellow Card` doesn't affect the prediction, but there is an extreme case where a high value caused a much lower prediction.\n- High values of Goal scored caused higher predictions, and low values caused low predictions\n\nIf you look for long enough, there's a lot of information in this graph. You'll face some questions to test how you read them in the exercise.","metadata":{"_uuid":"586faae56d8281305f5072e09ec225ecd4f048e9"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('../input/fifa-2018-match-statistics/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","metadata":{"_uuid":"ba585b079666610db600583fd030b4951366fb09","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get the SHAP values for all validation data with the following code.","metadata":{"_uuid":"8bbb07770b4ed27c0cb774d4849c8a69293adeae"}},{"cell_type":"code","source":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(val_X)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values[1], val_X)","metadata":{"_uuid":"8763fcc977f877ad93adffd90eee7d5fa554fd87","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code isn't too complex. But there are a few caveats.\n\n- When plotting, we call `shap_values[1]`.  For classification problems, there is a separate array of SHAP values for each possible outcome. In this case, we index in to get the SHAP values for the prediction of \"True\".\n- Calculating SHAP values can be slow. It isn't a problem here, because this dataset is small.  But you'll want to be careful when running these to plot with reasonable sized datasets.  The exception is when using an `xgboost` model, which SHAP has some optimizations for and which is thus much faster.\n\n# SHAP Dependence Contribution Plots\n\nWe've previously used Partial Dependence Plots to show how a single feature impacts predictions. These are insightful and relevant for many real-world use cases. Plus, with a little effort, they can be explained to a non-technical audience. But there's a lot they don't show.  For instance, what is the distribution of effects? Is the effect of having a certain value pretty constant, or does it vary a lot depending on the values of other feaures. SHAP dependence contribution plots provide a similar insight to PDP's, but they add a lot more detail.\n\n![Imgur](https://i.imgur.com/uQ2JmBm.png)\n\nStart by focusing on the shape, and we'll come back to color in a minute.  Each dot represents a row of the data. The horizontal location is the actual value from the dataset, and the vertical location shows what having that value did to the prediction.  The fact this slopes upward says that the more you possess the ball, the higher the model's prediction is for winning the *Man of the Game* award. The spread suggests that other features must interact with Ball Possession %.  For example, here we have highlighted two points with similar ball possession values. That value caused one prediction to increase, and it caused the other prediction to decrease.\n\n![Imgur](https://i.imgur.com/tFzp6jc.png)\n\nFor comparison, a simple linear regression would produce plots that are perfect lines, without this spread. This suggests we delve into the interactions, and the plots include color coding to help do that. While the primary trend is upward, you can visually inspect whether that varies by dot color. Consider the following very narrow example for concreteness.\n\n![Imgur](https://i.imgur.com/NVB3eNW.png)\n\nThese two points stand out spatially as being far away from the upward trend. They are both colored purple, indicating the team scored one goal. You can interpret this to say **In general, having the ball increases a team's chance of having their player win the award. But if they only score one goal, that trend reverses and the award judges may penalize them for having the ball so much if they score that little.**\n\nOutside of those few outliers, the interaction indicated by color isn't very dramatic here. But sometimes it will jump out at you.\n\n# Dependence Contribution Plots in Code\nWe get the dependence contribution plot with the following code. The only line that's different from the `summary_plot` is the last line.","metadata":{"_uuid":"c73cfefb21abb961582fa900b040ffa33c685484"}},{"cell_type":"code","source":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# calculate shap values. This is what we will plot.\nshap_values = explainer.shap_values(X)\n\n# make plot.\nshap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")","metadata":{"_uuid":"e4b3a7372c01c980b05555ab7b6e19aacc245c6d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you don't supply an argument for `interaction_index`, Shapley uses some logic to pick one that may be interesting.","metadata":{"_uuid":"b432bc5db150f490d6a318f65816b238279eb9c7"}},{"cell_type":"markdown","source":"# Exercises","metadata":{"_uuid":"08bd2898fbe41d6dd389a70df056bd425db24ae4"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport shap\n\n# Environment Set-Up for feedback system.\nimport sys\nsys.path.append('../input/ml-insights-tools')\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom ex5 import *\nprint(\"Setup Complete\")\n\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/hospital-readmissions/train.csv')\ny = data.readmitted\nbase_features = ['number_inpatient', 'num_medications', 'number_diagnoses', 'num_lab_procedures', \n                 'num_procedures', 'time_in_hospital', 'number_outpatient', 'number_emergency', \n                 'gender_Female', 'payer_code_?', 'medical_specialty_?', 'diag_1_428', 'diag_1_414', \n                 'diabetesMed_Yes', 'A1Cresult_None']\n\n# Some versions of shap package error when mixing bools and numerics\nX = data[base_features].astype(float)\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# For speed, we will calculate shap values on smaller subset of the validation data\nsmall_val_X = val_X.iloc[:150]\nmy_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(train_X, train_y)","metadata":{"_uuid":"aaedb39e4d2e49ce3270a3b20b55d74fee9862dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"_uuid":"8c0201063c89ad006e26d6923c29c554a06ec81d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first few questions require examining the distribution of effects for each feature, rather than just an average effect for each feature.  Run the following cell for a summary plot of the shap_values for readmission. It will take about 20 seconds to run.","metadata":{"_uuid":"1c3d3ae528ffda2e7e83d2c1a2a69eb6d020cec2"}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(my_model)\nshap_values = explainer.shap_values(small_val_X)\n\nshap.summary_plot(shap_values[1], small_val_X)","metadata":{"_uuid":"df585f87ba7fd5bc493ed3dd5aee0e89342a9ace","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 1\n\nWhich of the following features has a bigger range of effects on predictions (i.e. larger difference between most positive and most negative effect)\n- `diag_1_428` or\n- `payer_code_?`","metadata":{"_uuid":"93f3c2b8d78c289210f3a9d051bed226112893f1"}},{"cell_type":"code","source":"# set following variable to 'diag_1_428' or 'payer_code_?'\nfeature_with_bigger_range_of_effects = 'diag_1_428'\nq_1.check()","metadata":{"_uuid":"cc05d7e7e8b5b27d9ee6f64500c4f78049603341","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment the line below to see the solution and explanation","metadata":{"_uuid":"3f7642bdc72fb20a9e830313eeff144b9bd6d926"}},{"cell_type":"code","source":"#q_1.solution()","metadata":{"_uuid":"6648cb62ed8f5359007eb898ff166d9686005e98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 2\n\nDo you believe the range of effects sizes (distance between smallest effect and largest effect) is a good indication of which feature will have a higher permutation importance? Why or why not?  \n\nIf the **range of effect sizes** measures something different from **permutation importance**: which is a better answer for the question \"Which of these two features does the model say is more important for us to understand when discussing readmission risks in the population?\"\n\nUncomment the following line after you've decided your answer.","metadata":{"_uuid":"124e62ed81645823aadcc101fdd0f8f0c89692f6"}},{"cell_type":"code","source":"q_2.solution()","metadata":{"_uuid":"149600c545a612665609af0c44c74870229da9a2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 3\n\nBoth `diag_1_428` and `payer_code_?` are binary variables, taking values of 0 or 1.\n\nFrom the graph, which do you think would typically have a bigger impact on predicted readmission risk:\n- Changing `diag_1_428` from 0 to 1\n- Changing `payer_code_?` from 0 to 1\n\nTo save you scrolling, we have included a cell below to plot the graph again (this one runs quickly).","metadata":{"_uuid":"ea77dc3c7e341b2d7a942348aac0bab7fb87d8e9"}},{"cell_type":"code","source":"shap.summary_plot(shap_values[1], small_val_X)","metadata":{"_uuid":"64a15512cc6c5cb51a0cbdcc778a99dc5af72792","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set following var to \"diag_1_428\" if changing it to 1 has bigger effect.  Else set it to 'payer_code_?'\nbigger_effect_when_changed = 'diag_1_428'\nq_3.check()","metadata":{"_uuid":"f87f06bf3aae8a64a78d1114b27efda62665e51f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For a solution and explanation, uncomment the line below","metadata":{"_uuid":"99f53ac69a4328b523f978e5d63c69e679eb3035"}},{"cell_type":"code","source":"#q_3.solution()","metadata":{"_uuid":"e81af9801e4726e6fb27c3f9b308bdb619ec8997","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 4\n\nSome features (like `number_inpatient`) have reasonably clear separation between the blue and pink dots. Other variables like `num_lab_procedures` have blue and pink dots jumbled together, even though the SHAP values (or impacts on prediction) aren't all 0.\n\nWhat do you think you learn from the fact that `num_lab_procedures` has blue and pink dots jumbled together? Once you have your answer, uncomment the line below to verify your solution.","metadata":{"_uuid":"02bb5ca7a434ca0b37276e5bd306c260ef1a9370"}},{"cell_type":"code","source":"q_4.solution()","metadata":{"_uuid":"3771a589e054e7a4ffcaddd41f94894b1cd9f292","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 5\n\nConsider the following SHAP contribution dependence plot. \n\nThe x-axis shows `feature_of_interest` and the points are colored based on `other_feature`.\n\n![Imgur](https://i.imgur.com/zFdHneM.png)\n\nIs there an interaction between `feature_of_interest` and `other_feature`?  \nIf so, does `feature_of_interest` have a more positive impact on predictions when `other_feature` is high or when `other_feature` is low?\n\nUncomment the following code when you are ready for the answer.\n","metadata":{"_uuid":"33fab12e98e0114ff42dff3d24931fff111fe37e"}},{"cell_type":"code","source":"q_5.solution()","metadata":{"_uuid":"08a47bd448235e2c55bce0cfefa2877ba362e631","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question 6\n\nReview the summary plot for the readmission data by running the following cell:","metadata":{"_uuid":"2d38a41b480808e6a788830510c9c0c3e7a993c5"}},{"cell_type":"code","source":"shap.summary_plot(shap_values[1], small_val_X)","metadata":{"_uuid":"f2b2c7fb126f59c520df665c98b4bc7ed923d1bc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both **num_medications** and **num_lab_procedures** share that jumbling of pink and blue dots.\n\nAside from `num_medications` having effects of greater magnitude (both more positive and more negative), it's hard to see a meaningful difference between how these two features affect readmission risk.  Create the SHAP dependence contribution plots for each variable, and describe what you think is different between how these two variables affect predictions.\n\nAs a reminder, here is the code you previously saw to create this type of plot.\n\n    shap.dependence_plot(feature_of_interest, shap_values[1], val_X)\n    \nAnd recall that your validation data is called `small_val_X`.","metadata":{"_uuid":"efd2312e6af6152904cddcd51a00741bd8b57ecd"}},{"cell_type":"code","source":"shap.dependence_plot('num_lab_procedures', shap_values[1], small_val_X)\nshap.dependence_plot('num_medications', shap_values[1], small_val_X)","metadata":{"_uuid":"c7a2fb594dc899a32d8138fcc019bc4af83fcd5b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then uncomment the following line to compare your observations from this graph to the solution.","metadata":{"_uuid":"b47c67a3dc9eafdd3a6e9ddf6ad36a4ad4f3053a"}},{"cell_type":"code","source":"q_6.solution()","metadata":{"_uuid":"17d44d8ed14f5fa21136955a56e1c15f49143eb0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it!  Machine Learning models should not feel like black boxes any more, because you have the tools to inspect them and understand what they learn about the world. \n\nThis is an excellent skill for debugging models, building trust, and learning insights to make better decisions. These techniques have revolutionized how I do data science, and I hope they do the same for you.\n\nReal data science involves an element of exploration. I hope you find an interesting dataset to try these techniques on (Kaggle has a lot of [free datasets](https://www.kaggle.com/datasets) to try out). If you learn something interesting about the world, share your work [in this forum](https://www.kaggle.com/learn-forum/66354). I'm excited to see what you do with your new skills.","metadata":{"_uuid":"1ca445a7272d26e71f1ec4a4d1d17e6b3109ea2f"}}]}