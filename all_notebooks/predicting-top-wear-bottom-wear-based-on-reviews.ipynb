{"cells":[{"metadata":{"_uuid":"5315020687f05a2ae8985011b0c2680c6df543c4"},"cell_type":"markdown","source":"## Text Analysis\n\n#### Dataset : Women's E-Commerce Clothing Reviews\n\n### 1. Pre-Processing of Data\n   * Remove all extra charecters such as punctuations, non charecters, etc\n   * Tokenisation\n   * Lametisation of data. (preffered over stemming as stemming can corrupt data in some cases)"},{"metadata":{"_uuid":"1fe9088b8a1c224e1ef75b714d79dca05a8fc6ed","scrolled":false,"trusted":true},"cell_type":"code","source":"# Importing Required Variables\nimport sys,math, copy, time, os\nimport re\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# print(os.listdir(\"../input/womens-ecommerce-clothing-reviews\"))\n\n\nnp.set_printoptions(threshold=np.nan)\n# Reading the Data\noriginal_data = clothing_review = pd.read_csv(\"../input/Womens Clothing E-Commerce Reviews.csv\")\nclothing_review = clothing_review.dropna(subset=['Review Text'])\nclothing_review = clothing_review.dropna(subset=['Clothing ID'])\nclothing_review = clothing_review.loc[clothing_review['Department Name'].isin([\"Tops\",\"Bottoms\"])]\ngrouped_review = clothing_review.groupby([\"Clothing ID\"])['Review Text'].apply(' ::: '.join).reset_index()\n\n\n#Getting Keywords\ntopWear = [ \"top\",\"blouse\",\"shirt\",\"upper\",\"dress\",\"torso\",\"tank\",\"sleeve\",\"body\",\"sweater\"]\nbottomWear = [\"pant\",\"jean\",\"slack\",\"skirt\",\"leg\",\"waist\",\"lower\",\"thigh\",\"trouser\",\"flare\"]\n\n\n# positivewords = []\n# with open(\"../input/positive-and-negetive-words/positive_words\", 'r') as readfile :\n#     temp = readfile.readline().strip()\n#     while temp != \"\" :\n#         positivewords.append(temp)\n#         temp = readfile.readline().strip()\n\n# negetivewords = []\n# with open(\"../input/positive-and-negetive-words/negetive_words\", 'r') as readfile :\n#     temp = readfile.readline().strip()\n#     while temp != \"\" :\n#         positivewords.append(temp)\n#         temp = readfile.readline().strip()\n\nwordnet_lemmatizer = WordNetLemmatizer()\n\nkeyWords = topWear + bottomWear# + positivewords + negetivewords\n\nfor i in range(len(keyWords)) :\n    keyWords[i] = wordnet_lemmatizer.lemmatize(keyWords[i])\n    \n# Clearing the data from extra characters\ndata = []\nactual_labels = []\nfor i in range(len(grouped_review[\"Review Text\"])):\n    j = grouped_review[\"Review Text\"][i].lower()\n    j = re.sub(r'[^A-Za-z ]', '', j)\n    data.append(j)\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    actual_labels.append(id_cloth)\n    \n# Tokenising the data\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i in range(len(data)) :\n    data[i] = tokenizer.tokenize(data[i])\n\n# Getting the list of stop words\nstopWords = list(stopwords.words('english'))\nstopWords = [re.sub(r'[^A-Za-z ]', '', j) for j in stopWords]\n\n\n# Lemmatizing and removing stop words\nwordnet_lemmatizer = WordNetLemmatizer()\ndataFiltered = []\nfor each_review in data :\n    temp = []\n    for word in each_review : \n        if not word in stopWords and word in keyWords:\n            temp.append(wordnet_lemmatizer.lemmatize(word))\n    dataFiltered.append(temp)\n\n\n# dataFiltered.append(keyWords)\n\n# Creating the word list\n\nwordList = list(keyWords)\nwordList.sort()\n\nnumber_of_reviews = len(dataFiltered)\nwordListIndex = { wordList[i]: i for i in range(len(wordList))}\nnDocsPerWord = {i : 0 for i in wordList}\n\nfor i in range(len(actual_labels)) :\n    if actual_labels[i] == \"Tops\" :\n        actual_labels[i] = 0\n    else :\n        actual_labels[i] = 1\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa7cb6e708ddfa27db441388b1f783aec7c5f676"},"cell_type":"markdown","source":"### 2. Creation of TF matrix"},{"metadata":{"_uuid":"1bfebfa1b6d0387de932fed3f140d2edd7045376","trusted":true},"cell_type":"code","source":"tf = np.zeros(shape=(number_of_reviews,len(wordList)))\n\nfor i in range(len(dataFiltered)):\n    this_doc_accounted = []\n    for j in dataFiltered[i] :\n        print(j)\n        if j in topWear :\n            tf[i][wordListIndex[j]] = 1\n        elif j in bottomWear :\n            tf[i][wordListIndex[j]] = -1\n        elif j in keyWords :\n            tf[i][wordListIndex[j]]\n        if not j in this_doc_accounted :\n            this_doc_accounted.append(j)\n            print(j in nDocsPerWord)\n            nDocsPerWord[j] += 1\n            \ntf_normalized = copy.deepcopy(tf)\ntf_normalized = tf_normalized / tf_normalized.max(axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"071dd4a8ebe4e5d9ddf68caf6a5015326060d27a"},"cell_type":"markdown","source":"### 3. Creation of TF-IDF matrix from calculated TF matrix"},{"metadata":{"_uuid":"fbd834530563141569f688311237713f57813697","trusted":true},"cell_type":"code","source":"tfIdf = copy.deepcopy(tf)\n\nfor i in range(number_of_reviews) :\n    for k in dataFiltered[i]:\n        j = wordListIndex[k]\n        if tfIdf[i][j] != 0 :\n            tfIdf[i][j] = tfIdf[i][j]*math.log(number_of_reviews/nDocsPerWord[wordList[j]])\n\n\ntfIdf_normalized = copy.deepcopy(tfIdf)\ntfIdf_normalized = tfIdf_normalized / tfIdf_normalized.max(axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"123cb9542038aa382a21ea1634feb19e504e8c51"},"cell_type":"markdown","source":"### 4. Adding Keyword as a dummy to detect clusters"},{"metadata":{"trusted":true,"_uuid":"e53796d49df46680dc3decf4be3d517bd4bd11b9"},"cell_type":"code","source":"temprow = np.zeros(len(wordList))\nfor i in range(len(temprow)) :\n    if wordList[i] in topWear :\n        temprow[i] = 1\n\n\ntfIdf = np.vstack([tfIdf, temprow])\ntf = np.vstack([tf, temprow])\ntfIdf_normalized = np.vstack([tfIdf_normalized, temprow])\ntf_normalized = np.vstack([tf_normalized, temprow])\n\nprint(np.isnan(np.min(tfIdf)))\nprint(np.isnan(np.min(tf)))\nprint(np.isnan(np.min(tfIdf_normalized)))\nprint(np.isnan(np.min(tf_normalized)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac5993439ac0dbe7b6df5cccae5c6ba4e42b148c"},"cell_type":"markdown","source":"### Clustering on tf"},{"metadata":{"trusted":true,"_uuid":"7cd82cfcdda61d14eed96dd3e3022a12f5029539"},"cell_type":"code","source":"# K-means Clustering\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\nkmeans_clothing = KMeans(n_clusters=2,random_state=0).fit(tf)\nkmeans_centroids = kmeans_clothing.cluster_centers_\n\nkmeans_labels= kmeans_clothing.labels_\ntop_label = kmeans_labels[-1]\ncorrect = 0\nfor i in range(len(kmeans_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if kmeans_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct/ (len(kmeans_labels) -1))\n\n# Agg Clustering ---------------------------------\nfrom sklearn.cluster import AgglomerativeClustering\n\nprint((~tf.any(axis=1)).any())\n# temp = np.append(tf, np.ones((len(tf),1)),axis=1)\nagg_iris = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(tf)\n#Getting labels\nagg_labels = agg_iris.labels_\n\ntop_label = agg_labels[-1]\ncorrect = 0\nfor i in range(len(agg_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if agg_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct/ (len(agg_labels) -1))\n\n# GMM Clustering ---------------------------------\n\ngm_labels = GaussianMixture(2).fit_predict(tf)\n\nprint(gm_labels)\ntop_label = gm_labels[-1]\ncorrect = 0\nfor i in range(len(gm_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if gm_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct/ (len(gm_labels) -1))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a33cb8d1812c6337f443ff76362de084daf4668"},"cell_type":"markdown","source":"### Clustering on Tfidf"},{"metadata":{"trusted":true,"_uuid":"dfb1ac1fba8dfd4788f1f805c797af96579a70b8","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\nkmeans_clothing = KMeans(n_clusters=2,random_state=0).fit(tfIdf)\nkmeans_centroids = kmeans_clothing.cluster_centers_\n\nkmeans_labels= kmeans_clothing.labels_\ntop_label = kmeans_labels[-1]\ncorrect = 0\nfor i in range(len(kmeans_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if kmeans_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct += 1\n\nprint (correct/ (len(kmeans_labels) -1))\n\n# Agg Clustering ---------------------------------\nfrom sklearn.cluster import AgglomerativeClustering\n\n\nagg_iris = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(tfIdf)\n#Getting labels\nagg_labels = agg_iris.labels_\n\ntop_label = agg_labels[-1]\ncorrect = 0\nfor i in range(len(agg_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if agg_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct/ (len(agg_labels) -1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ca4004dab7fa825e99a573cd5dde9aaaaa1906d"},"cell_type":"markdown","source":"* #### From LSA using TF matrix"},{"metadata":{"_uuid":"25d1abbd62d3ca7f8d11d98a666a2440013d1f16","trusted":true,"scrolled":true},"cell_type":"code","source":"tf_matrix = tf # D x V matrix \nA = tf_matrix.T \n\nU, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\n\nK =  2 # number of components\n\nA_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # D x V matrix \n\ndocs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n\n# print (A_reduced)\n# print (docs_rep)\n# print (terms_rep)\n\nkey_word_indices = [wordList.index(key_word) for key_word in keyWords] # vocabulary indices \n\nkey_words_rep = terms_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n\n\nsvd_start = time.time()\nquery_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nsvd_end = time.time()\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\n\nmax_iter = 5\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    print(rank + 1, \") Cosine value : \", float(query_doc_cos_dist[sort_index]) ,\"\\n\", clothing_review[\"Review Text\"].iloc[sort_index],\"\\n\")\n    max_iter -= 1\n    if max_iter == 0 :\n        break\n\n;","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0c6d17eafad7dcfdd2053e2e620002656c127be"},"cell_type":"markdown","source":"### SVD Clustering"},{"metadata":{"trusted":true,"_uuid":"36e96166b508add778e8b7262934fd47e6ee015c"},"cell_type":"code","source":"\nkmeans_clothing = KMeans(n_clusters=2,random_state=0).fit(docs_rep)\n\n\nkmeans_labels= kmeans_clothing.labels_\ntop_label = kmeans_labels[-1]\ncorrect = 0\nfor i in range(len(kmeans_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if kmeans_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct += 1\n\nprint (correct/ (len(kmeans_labels) -1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b225e2a3278a8833e0d62fef4c6f495531818bd7"},"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nprint(np.isnan(np.min(docs_rep)))\n\n\nagg_iris = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(docs_rep)\n\n\n#Getting labels\nagg_labels = agg_iris.labels_\n\ntop_label = agg_labels[-1]\ncorrect = 0\nfor i in range(len(agg_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if agg_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct/ (len(agg_labels) -1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60c4229158731df1513e909be8901f7bdbaf79d7"},"cell_type":"code","source":"gm_labels = GaussianMixture(2).fit_predict(docs_rep)\n\nprint(gm_labels)\ntop_label = gm_labels[-1]\ncorrect = 0\nfor i in range(len(gm_labels) -1 ) :\n    compare = grouped_review.iloc[i][\"Clothing ID\"]\n    temp = clothing_review.loc[clothing_review['Clothing ID'] == compare]\n    id_cloth = temp.iloc[0].loc['Department Name']\n    if gm_labels[i] == top_label:\n        if id_cloth == \"Tops\" :\n            correct += 1\n    else :\n        if id_cloth == \"Bottoms\" :\n            correct +=1\n\nprint (correct/ (len(gm_labels) -1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"823f58fb806ea03122dccd41fe7c4aa010c0739e"},"cell_type":"code","source":"import itertools\n\nii = itertools.count(docs_rep.shape[0])\ntree = [{'node_id': next(ii), 'left': x[0], 'right':x[1]} for x in agg_iris.children_]\n\n# print(tree)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f857b8d2d8bf39f5366fc0e5babb6766983182f9"},"cell_type":"markdown","source":"### 5. Plotting"},{"metadata":{"_uuid":"9a4df8df15b88fcccf6c86f7e160897a77396b3e","scrolled":false,"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nprint(len(docs_rep[:,0]),len(actual_labels))\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=actual_labels) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=agg_labels[-1]) # the query \nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\nplt.plot()\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=agg_labels[:-1]) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=agg_labels[-1]) # the query \nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.show()\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=kmeans_labels[:-1]) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=kmeans_labels[-1]) # the query \nplt.show()\nplt.scatter(docs_rep[:,0][:-1], docs_rep[:,1][:-1], c=gm_labels[:-1]) # all documents \nplt.scatter(docs_rep[:,0][-1], docs_rep[:,1][-1], marker='+', c=gm_labels[-1]) # the query \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f703e43dec3eb884224587e743b0b6b0d4c30987"},"cell_type":"markdown","source":"## Finding heirarical patterns"},{"metadata":{"trusted":true,"_uuid":"50e097f7975262d01981c717a98ebc6699eb90d3"},"cell_type":"code","source":"all_data = original_data.dropna()\n\ntops = []\nfor i in range(1,len(agg_labels) -1) :\n    if agg_labels[i] == 1 :\n        tops.append( clothing_review.iloc[i].loc[\"Clothing ID\"] )\n\n\ntop_data_pandas = all_data[all_data['Clothing ID'].isin(tops)]\ntop_data_pandas = top_data_pandas.reset_index(drop=True)\nprint(top_data_pandas.shape)\n\ntop_data = np.zeros(shape=(top_data_pandas.shape[0],3))\nprint(top_data.shape)\n\nfor  index, row in top_data_pandas.iterrows() :\n    top_data[index][0] = int(row[\"Age\"])\n    top_data[index][1] = int(row[\"Rating\"])\n    top_data[index][2] = int(row[\"Recommended IND\"])\n\n\nagg_tops = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(top_data)\nagg_tops_labels= agg_tops.labels_\nprint(top_data.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e632a222f72a0ba12a9463070a8112966b58dc17"},"cell_type":"markdown","source":"#### Top data TFIDF"},{"metadata":{"trusted":true,"_uuid":"7de75c9a5605d586c90dc680caa59572d09c5d15"},"cell_type":"code","source":"data = []\nprint(top_data_pandas.shape)\nfor i in top_data_pandas[\"Review Text\"]:\n    j = i.lower()\n    j = re.sub(r'[^A-Za-z ]', '', j)\n    data.append(j)\n    \n# Tokenising the data\ntokenizer = RegexpTokenizer(r'\\w+')\nfor i in range(len(data)) :\n    data[i] = tokenizer.tokenize(data[i])\n\n\n# Getting the list of stop words\nstopWords = list(stopwords.words('english'))\nstopWords = [re.sub(r'[^A-Za-z ]', '', j) for j in stopWords]\n\n# Lemmatizing and removing stop words\nwordnet_lemmatizer = WordNetLemmatizer()\ndataFiltered = []\nfor each_review in data :\n    temp = []\n    for word in each_review : \n        if not word in stopWords :\n            temp.append(wordnet_lemmatizer.lemmatize(word))\n    dataFiltered.append(temp)\n\n\n\n# Creating the word list\nwordList = np.array(dataFiltered)\nwordList = np.hstack(wordList)\nwordList = list(set(wordList))\nwordList.sort()\nnumber_of_reviews = len(dataFiltered)\nwordListIndex = { wordList[i]: i for i in range(len(wordList))}\nnDocsPerWord = {i : 0 for i in wordList}\n\ntf_top = np.zeros(shape=(number_of_reviews,len(wordList)))\nprint(tf_top.shape, top_data.shape)\n\nfor i in range(len(dataFiltered)):\n    this_doc_accounted = []\n    for j in dataFiltered[i] :\n        tf_top[i][wordListIndex[j]] += 1\n        if not j in this_doc_accounted :\n            this_doc_accounted.append(j)\n            nDocsPerWord[j] += 1\n\ntfIdf_top = copy.deepcopy(tf_top)\n\nprint(tfIdf_top.shape)\nfor i in range(number_of_reviews) :\n    for k in dataFiltered[i]:\n        j = wordListIndex[k]\n        if tfIdf_top[i][j] != 0 :\n            tfIdf_top[i][j] = tfIdf_top[i][j]*math.log(number_of_reviews/nDocsPerWord[wordList[j]])\n            \n#\nprint(top_data.shape, tfIdf_top.shape)\ntop_data = np.concatenate((top_data, tfIdf_top), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db72dd862dbcc4e16366a02822436a8df87f81ac"},"cell_type":"markdown","source":"#### Agglomorative clustering to find relevance"},{"metadata":{"trusted":true,"_uuid":"156eaa75dae8cf71eb06e3fefdc060b341525daa"},"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\ntop_data = top_data / top_data.max(axis=0)\n\ntf_matrix = top_data # D x V matrix \nA = tf_matrix.T \n\nU, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\n\nK =  2 # number of components\n\nA_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # D x V matrix \n\ndocs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n\n# print (A_reduced)\n# print (docs_rep)\n# print (terms_rep)\n\nkey_word_indices = [wordList.index(key_word) for key_word in keyWords] # vocabulary indices \n\nkey_words_rep = terms_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n\ndef removeOutliers(x):\n    to_ret = []\n    for i in x :\n        print(i)\n        if abs(i[0] - i[1]) < 500:\n            print(\"oh ya\")\n            to_ret.append(i)\n    print (to_ret)\n    return np.array(to_ret)\n    \n\n# docs_rep = removeOutliers(docs_rep)\nprint(docs_rep.shape)\n\nagg_tops = AgglomerativeClustering(n_clusters= 2,linkage=\"average\",affinity=\"manhattan\").fit(docs_rep)\nnew_labels = agg_tops.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3e31e4bf32fd98f1d85ea3ad120aaf83dc7a3d9"},"cell_type":"code","source":"print(np.where(new_labels==1))\nprint(docs_rep[177])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c07d50b70800c09d32b636a0d5a2f7671b657516"},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.scatter(top_data[:,0], top_data[:,1],c= agg_tops_labels) # all documents \nplt.show()\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nx =top_data[:,0]\ny =top_data[:,1]\nz =top_data[:,2]\n\nax.scatter(x, y, z, c=agg_tops_labels, marker='o')\nax.set_xlabel('Age')\nax.set_ylabel('Rating')\nax.set_zlabel('Recommended IND')\nplt.show()\n\nplt.scatter(docs_rep[:,0][:], docs_rep[:,1][:], c=new_labels[:]) # all documents \nplt.show()\n\n\nplt.scatter(docs_rep[:,0][:], docs_rep[:,1][:], c=top_data[:,2]) # all documents \nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}