{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import and look through the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import the data and see all the stats that are availble.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"nba=pd.read_csv('/kaggle/input/nba-mvp-votings-through-history/mvp_votings.csv')\nnba.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view all of the columns in the dataframe\npd.set_option('display.max_columns', None)\nnba","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In one big dataframe, all players receiving mvp votes are included by year.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Annotate the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Can use the .idxmax() method to get the index for the max value in a column. We will use this to create a yes/no column indicating who won the mvp.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check methodology of getting index of player\n\n#data for 2017-18 season\nex=nba[nba['season'].isin(['2017-18'])]\n\n#get index of mvp winner\nindex=[ex['win_pct'].idxmax()]\n\n#now see who the player is by calling the index of the 'player' column\nex['player'][index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nba[nba['season'].isin(['2015-16']) & nba['player'].isin(['LeBron James'])].index[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set whole column to 'No', then just change to 'Yes' for mvp winners\nnba['Mvp?']='No'\n\n#for every season\nfor season in nba['season'].value_counts().index:\n    \n    #isolate data from that season\n    season_df=nba[nba['season'].isin([season])]\n    \n    #get the index of player with most mvp points\n    index=[season_df['points_won'].idxmax()]\n    \n    #change player's 'Mvp?' entry to yes\n    nba['Mvp?'][index]='Yes'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nba","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#move this new column next to mvp voting data\n\n#save column,remove it from dataframe, then insert it where we want it\nsave=nba['Mvp?']\nnba.drop(labels=['Mvp?'], axis=1, inplace = True)\nnba.insert(10, 'Mvp?', save)\nnba","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't have to do this or understand all those steps, I just wanted to make it easier to see who won the mvp each season.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nba['Mvp?'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(nba['season'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verify that we have an mvp for each season.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Trying out the first model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This first model will model each season separately, hence the for loop going though each individual season. I selected what I initially thought would be the most relevant criteria for determining who wins the mvp.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If the machine learning code is confusing, then I suggest you go to the building a machine learning model notebook and/or the interpreting a machine learning model notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n#blank dataframe that we will add to\npredicted_df=pd.DataFrame()\n\n#create model for each season\nfor season in nba['season'].value_counts().index:\n    \n    #isolate season data\n    season_df=nba[nba['season'].isin([season])]\n    y=season_df['award_share']\n    features=['per', 'ts_pct', 'usg_pct', 'g', 'mp_per_g', 'pts_per_g', 'trb_per_g',\n       'ast_per_g', 'stl_per_g', 'blk_per_g', 'fg_pct', 'fg3_pct', 'ft_pct',\n       'ws', 'ws_per_48','win_pct']\n    X=season_df[features]\n    train_X, val_X, train_y, val_y = train_test_split(X, y,random_state=1, test_size=0.4)\n    basic_model = DecisionTreeRegressor(random_state=1)\n    basic_model.fit(train_X, train_y)\n    predictions=basic_model.predict(val_X)\n    \n    #modify test dataframe to show predictions too\n    val_Xdf=val_X\n    \n    #add column of predictions\n    val_Xdf['Prediction']=predictions\n    \n    #add the correct values\n    val_Xdf['award_share']=val_y\n    \n    #add column for the season\n    val_Xdf['season']=season\n    \n    #add column for player name- this is a bit tricky because we need the index of player as it is in the 'nba' dataframe\n    #resetting index creates a column of the original indices that we can use to refer to the indices in the 'nba' dataframe\n    val_Xdf['player']=[season_df['player'][index] for index in val_Xdf.reset_index()['index']]\n    \n    #same methodology here\n    val_Xdf['Mvp?']=[season_df['Mvp?'][index] for index in val_Xdf.reset_index()['index']]\n    \n    #add this dataframe to the dataframe of all the seasons' predictions\n    predicted_df=predicted_df.append(val_Xdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can see all of the factors influencing the model, the actual award share, the predicted award share, and the column indicating whether or not the player won the mvp for that year. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2017-18'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When I isolate one of the years, I notice that I only see 6 players from that year. This is problematic because we should not test the model on a subset of mvp candidates. This is due to how the data was split into training and testing data. Sci-kit learn will randomly select testing data without considering what year each testing point is from. We need to manually split the data to ensure that each year is kept in tact.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['per', 'ts_pct', 'usg_pct', 'bpm', 'g', 'mp_per_g', 'pts_per_g', 'trb_per_g',\n       'ast_per_g', 'stl_per_g', 'blk_per_g', 'fg_pct', 'fg3_pct', 'ft_pct',\n       'ws', 'ws_per_48','win_pct']\n\n#have to specify train test split so that we can group seasons together\n#make first 30 seasons the training data and the last 8 the testing data\ntraining_seasons=['1980-81', '1981-82', '1984-85', '1982-83', '1998-99', '1996-97',\n       '1990-91', '1997-98', '1988-89', '2001-02', '1985-86', '2000-01',\n       '2007-08', '1991-92', '1993-94', '2006-07', '1986-87', '1995-96',\n       '1987-88', '2013-14', '1999-00', '2012-13', '2004-05', '2003-04',\n       '1994-95', '2011-12', '2009-10', '1983-84', '1989-90', '1992-93']\ntesting_seasons=['2017-18', '2010-11', '2002-03', '2014-15', '2008-09', '2005-06',\n       '2016-17', '2015-16']\n\n#training data\ntraining_data=nba[nba['season'].isin(training_seasons)]\ntrain_X=training_data[features]\ntrain_y=training_data['award_share']\n\n#testing data\ntesting_data=nba[nba['season'].isin(testing_seasons)]\nval_X=testing_data[features]\nval_y=testing_data['award_share']\n\nbasic_model = DecisionTreeRegressor(random_state=1)\nbasic_model.fit(train_X, train_y)\npredictions=basic_model.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Manually splitting the data allows for full seasons to be kept intact.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#put testing data and predictions into new dataframe\npredicted_df=pd.DataFrame()\nval_Xdf=pd.DataFrame(val_X)\nval_Xdf['Prediction']=predictions\nval_Xdf['award_share']=val_y\nval_Xdf['season']=[nba['season'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['player']=[nba['player'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['Mvp?']=[nba['Mvp?'][index] for index in val_Xdf.reset_index()['index']]\npredicted_df=predicted_df.append(val_Xdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add who we think would've won the mvp based on our predicted numbers. This is the same methodology as adding the 'Mvp?' column earlier in the notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column indicating whether player actually won the mvp\npredicted_df['Mvp prediction']='No'\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    index=season_df['Prediction'].idxmax()\n    mvp=predicted_df['player'][index]\n    \n    #will only change for the mvp winner, otherwise all others players will be 'no'\n    predicted_df['Mvp prediction'][index]='Yes'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How to evaluate the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's find a season where we incorrectly predicted the mvp. Just by seeing the preview of the dataframe and knowing the Tim Duncan won the 2002-03 mvp, let's see what went wrong in that year's prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2002-03'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare Dirk Nowitzki to the true mvp, Tim Duncan and see if the model's prediction is realistic (Dirk actually finished 7th in mvp voting...)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2002-03']) & predicted_df['player'].isin(['Tim Duncan','Dirk Nowitzki'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is interesting because Dirk actually has some impressive stats and when you look at the data used for the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_list=[]\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    predicted_list.append(season_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total=0\nfor df in predicted_list:\n    if df['Mvp?'].equals(df['Mvp prediction'])==True:\n        total+=1\ntotal/len(predicted_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only got half of the predictions right- let's look at the ones we got wrong and the ones we got right.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons=[]\nright_seasons=[]\nfor df in predicted_list:\n    if df['Mvp?'].equals(df['Mvp prediction'])==False:\n        wrong_seasons.append(df.reset_index()['season'][0])\n    else:\n        right_seasons.append(df.reset_index()['season'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2017-18'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This one was way off because James Harden was almost the unanimous winner, but was predicted to have just a 7% award share.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2010-11'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2005-06'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The actual mvp winners are getting very small award share predictions. Even if an algorithm doesn't think they'll winmvp, they shouldn't finish 8th or barely get an award share.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"right_seasons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2015-16'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try some new models. The criteria I used for that above model included all of the stats that I deemed relevant or most important. These are stats I commonly see cited when experts debate the mvp race. I'm going to create a new model that is all about team success- how your stats correlate with winning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nba.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['bpm', 'g', 'mp_per_g','ws', 'ws_per_48','win_pct']\n#have to specify train test split so that we can group seasons together\n#make first 30 seasons the training data and the last 8 the testing data\ntraining_seasons=['1980-81', '1981-82', '1984-85', '1982-83', '1998-99', '1996-97',\n       '1990-91', '1997-98', '1988-89', '2001-02', '1985-86', '2000-01',\n       '2007-08', '1991-92', '1993-94', '2006-07', '1986-87', '1995-96',\n       '1987-88', '2013-14', '1999-00', '2012-13', '2004-05', '2003-04',\n       '1994-95', '2011-12', '2009-10', '1983-84', '1989-90', '1992-93']\ntesting_seasons=['2017-18', '2010-11', '2002-03', '2014-15', '2008-09', '2005-06',\n       '2016-17', '2015-16']\n# train_X, val_X, train_y, val_y = train_test_split(X, y,random_state=1, test_size=0.4)\ntrain_X=nba[nba['season'].isin(training_seasons)][features]\ntrain_y=nba[nba['season'].isin(training_seasons)]['award_share']\nval_X=nba[nba['season'].isin(testing_seasons)][features]\nval_y=nba[nba['season'].isin(testing_seasons)]['award_share']\n\nbasic_model = DecisionTreeRegressor(random_state=1)\nbasic_model.fit(train_X, train_y)\npredictions=basic_model.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df=pd.DataFrame()\nval_Xdf=pd.DataFrame(val_X)\nval_Xdf['Prediction']=predictions\nval_Xdf['award_share']=val_y\nval_Xdf['season']=[nba['season'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['player']=[nba['player'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['Mvp?']=[nba['Mvp?'][index] for index in val_Xdf.reset_index()['index']]\npredicted_df=predicted_df.append(val_Xdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column indicating whether player actually won the mvp\npredicted_df['Mvp prediction']=''\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    mvp=predicted_df['player'][season_df['Prediction'].idxmax()]\n    for player in season_df['player']:\n        row=predicted_df[predicted_df['season'].isin([season]) & predicted_df['player'].isin([player])].index[0]\n        if player==mvp:\n            predicted_df['Mvp prediction'][row]='Yes'\n        else:\n            predicted_df['Mvp prediction'][row]='No'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_list=[]\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    predicted_list.append(season_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons=[]\nright_seasons=[]\nfor df in predicted_list:\n    if df['Mvp?'].equals(df['Mvp prediction'])==False:\n        wrong_seasons.append(df.reset_index()['season'][0])\n    else:\n        right_seasons.append(df.reset_index()['season'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only one of these was also predicted wrong in the last model. 5/8 is still better than last time though.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2005-06'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New model that is pure based on player output- team success not taken into account.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nba.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['fga', 'fg3a', 'fta', 'per', 'ts_pct', 'usg_pct',\n       'g', 'mp_per_g', 'pts_per_g', 'trb_per_g',\n       'ast_per_g', 'stl_per_g', 'blk_per_g', 'fg_pct', 'fg3_pct', 'ft_pct']\n#have to specify train test split so that we can group seasons together\n#make first 30 seasons the training data and the last 8 the testing data\ntraining_seasons=['1980-81', '1981-82', '1984-85', '1982-83', '1998-99', '1996-97',\n       '1990-91', '1997-98', '1988-89', '2001-02', '1985-86', '2000-01',\n       '2007-08', '1991-92', '1993-94', '2006-07', '1986-87', '1995-96',\n       '1987-88', '2013-14', '1999-00', '2012-13', '2004-05', '2003-04',\n       '1994-95', '2011-12', '2009-10', '1983-84', '1989-90', '1992-93']\ntesting_seasons=['2017-18', '2010-11', '2002-03', '2014-15', '2008-09', '2005-06',\n       '2016-17', '2015-16']\n# train_X, val_X, train_y, val_y = train_test_split(X, y,random_state=1, test_size=0.4)\ntrain_X=nba[nba['season'].isin(training_seasons)][features]\ntrain_y=nba[nba['season'].isin(training_seasons)]['award_share']\nval_X=nba[nba['season'].isin(testing_seasons)][features]\nval_y=nba[nba['season'].isin(testing_seasons)]['award_share']\n\nbasic_model = DecisionTreeRegressor(random_state=1)\nbasic_model.fit(train_X, train_y)\npredictions=basic_model.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df=pd.DataFrame()\nval_Xdf=pd.DataFrame(val_X)\nval_Xdf['Prediction']=predictions\nval_Xdf['award_share']=val_y\nval_Xdf['season']=[nba['season'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['player']=[nba['player'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['Mvp?']=[nba['Mvp?'][index] for index in val_Xdf.reset_index()['index']]\npredicted_df=predicted_df.append(val_Xdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column indicating whether player actually won the mvp\npredicted_df['Mvp prediction']=''\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    mvp=predicted_df['player'][season_df['Prediction'].idxmax()]\n    for player in season_df['player']:\n        row=predicted_df[predicted_df['season'].isin([season]) & predicted_df['player'].isin([player])].index[0]\n        if player==mvp:\n            predicted_df['Mvp prediction'][row]='Yes'\n        else:\n            predicted_df['Mvp prediction'][row]='No'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_list=[]\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    predicted_list.append(season_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons=[]\nright_seasons=[]\nfor df in predicted_list:\n    if df['Mvp?'].equals(df['Mvp prediction'])==False:\n        wrong_seasons.append(df.reset_index()['season'][0])\n    else:\n        right_seasons.append(df.reset_index()['season'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2005-06'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What if instead of determining award share as a number, we just use the metric of whether or not they won the mvp? The 'Mvp?' column is currently filled with yes/no strings, but I could replace it with true/false booleans that will allow for a model to be created (can't used strings in this ML model).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in range(len(nba)):\n    if nba['Mvp?'][row]=='Yes':\n        nba['Mvp?'][row]=True\n    else:\n        nba['Mvp?'][row]=False\nnba['Mvp?']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nba['Mvp?'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try this out on our most successful model, which was based mainly on team success.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nba[nba['season'].isin(['1980-81'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['bpm', 'g', 'mp_per_g','ws', 'ws_per_48','win_pct']\n#have to specify train test split so that we can group seasons together\n#make first 30 seasons the training data and the last 8 the testing data\ntraining_seasons=['1980-81', '1981-82', '1984-85', '1982-83', '1998-99', '1996-97',\n       '1990-91', '1997-98', '1988-89', '2001-02', '1985-86', '2000-01',\n       '2007-08', '1991-92', '1993-94', '2006-07', '1986-87', '1995-96',\n       '1987-88', '2013-14', '1999-00', '2012-13', '2004-05', '2003-04',\n       '1994-95', '2011-12', '2009-10', '1983-84', '1989-90', '1992-93']\ntesting_seasons=['2017-18', '2010-11', '2002-03', '2014-15', '2008-09', '2005-06',\n       '2016-17', '2015-16']\n# train_X, val_X, train_y, val_y = train_test_split(X, y,random_state=1, test_size=0.4)\ntrain_X=nba[nba['season'].isin(training_seasons)][features]\ntrain_y=nba[nba['season'].isin(training_seasons)]['Mvp?']\nval_X=nba[nba['season'].isin(testing_seasons)][features]\nval_y=nba[nba['season'].isin(testing_seasons)]['Mvp?']\n\nbasic_model = DecisionTreeRegressor(random_state=1)\nbasic_model.fit(train_X, train_y)\npredictions=basic_model.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df=pd.DataFrame()\nval_Xdf=pd.DataFrame(val_X)\nval_Xdf['Prediction']=predictions\nval_Xdf['Mvp?']=val_y\nval_Xdf['season']=[nba['season'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['player']=[nba['player'][index] for index in val_Xdf.reset_index()['index']]\n# val_Xdf['Mvp?']=[nba['Mvp?'][index] for index in val_Xdf.reset_index()['index']]\npredicted_df=predicted_df.append(val_Xdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column indicating whether player actually won the mvp\npredicted_df['Mvp prediction']=''\nfor index in predicted_df.reset_index()['index']:\n    if predicted_df['Prediction'][index]==True:\n        predicted_df['Mvp prediction'][index]='Yes'\n    else:\n        predicted_df['Mvp prediction'][index]='No'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this model, any given year is not constrained to one mvp winner.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df['Mvp prediction'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For 8 years, there are 9 predicted winners. This is not far off. Let's see how many years I got correct.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_list=[]\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    predicted_list.append(season_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#can't compare boolean to integer columns, can only compare one by one\n#have to see if any cell doesn't match up\nwrong_seasons=[]\nright_seasons=[]\nfor df in predicted_list:\n    df=df.reset_index()\n    for row in range(len(df)):\n        if df['Mvp?'][row]!=df['Prediction'][row]:\n            wrong_seasons.append(df['season'][row])\n#         wrong_seasons.append(df.reset_index()['season'][0])\n#     else:\n#         right_seasons.append(df.reset_index()['season'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5/8 were incorrect","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2015-16'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try it with the player output method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['fga', 'fg3a', 'fta', 'per', 'ts_pct', 'usg_pct',\n       'g', 'mp_per_g', 'pts_per_g', 'trb_per_g',\n       'ast_per_g', 'stl_per_g', 'blk_per_g', 'fg_pct', 'fg3_pct', 'ft_pct']\ntraining_seasons=['1980-81', '1981-82', '1984-85', '1982-83', '1998-99', '1996-97',\n       '1990-91', '1997-98', '1988-89', '2001-02', '1985-86', '2000-01',\n       '2007-08', '1991-92', '1993-94', '2006-07', '1986-87', '1995-96',\n       '1987-88', '2013-14', '1999-00', '2012-13', '2004-05', '2003-04',\n       '1994-95', '2011-12', '2009-10', '1983-84', '1989-90', '1992-93']\ntesting_seasons=['2017-18', '2010-11', '2002-03', '2014-15', '2008-09', '2005-06',\n       '2016-17', '2015-16']\n# train_X, val_X, train_y, val_y = train_test_split(X, y,random_state=1, test_size=0.4)\ntrain_X=nba[nba['season'].isin(training_seasons)][features]\ntrain_y=nba[nba['season'].isin(training_seasons)]['Mvp?']\nval_X=nba[nba['season'].isin(testing_seasons)][features]\nval_y=nba[nba['season'].isin(testing_seasons)]['Mvp?']\n\nbasic_model = DecisionTreeRegressor(random_state=1)\nbasic_model.fit(train_X, train_y)\npredictions=basic_model.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df=pd.DataFrame()\nval_Xdf=pd.DataFrame(val_X)\nval_Xdf['Prediction']=predictions\nval_Xdf['Mvp?']=val_y\nval_Xdf['season']=[nba['season'][index] for index in val_Xdf.reset_index()['index']]\nval_Xdf['player']=[nba['player'][index] for index in val_Xdf.reset_index()['index']]\npredicted_df=predicted_df.append(val_Xdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df['Mvp prediction']=''\nfor index in predicted_df.reset_index()['index']:\n    if predicted_df['Prediction'][index]==True:\n        predicted_df['Mvp prediction'][index]='Yes'\n    else:\n        predicted_df['Mvp prediction'][index]='No'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_list=[]\nfor season in predicted_df['season'].value_counts().index:\n    season_df=predicted_df[predicted_df['season'].isin([season])]\n    predicted_list.append(season_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons=[]\nfor df in predicted_list:\n    df=df.reset_index()\n    for row in range(len(df)):\n        if df['Mvp?'][row]!=df['Prediction'][row]:\n            wrong_seasons.append(df['season'][row])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 1/8 predicted correctly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_df[predicted_df['season'].isin(['2002-03'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try one more metric- first place votes. This should weed out the lower tier players from having much influence on the algorithm. Usually only three of four players will get any first place votes in a given year.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Create some methods so that it's easier to mimic these models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_model(features,metric):\n    training_seasons=['1980-81', '1981-82', '1984-85', '1982-83', '1998-99', '1996-97',\n           '1990-91', '1997-98', '1988-89', '2001-02', '1985-86', '2000-01',\n           '2007-08', '1991-92', '1993-94', '2006-07', '1986-87', '1995-96',\n           '1987-88', '2013-14', '1999-00', '2012-13', '2004-05', '2003-04',\n           '1994-95', '2011-12', '2009-10', '1983-84', '1989-90', '1992-93']\n    testing_seasons=['2017-18', '2010-11', '2002-03', '2014-15', '2008-09', '2005-06',\n           '2016-17', '2015-16']\n    train_X=nba[nba['season'].isin(training_seasons)][features]\n    train_y=nba[nba['season'].isin(training_seasons)][metric]\n    val_X=nba[nba['season'].isin(testing_seasons)][features]\n    val_y=nba[nba['season'].isin(testing_seasons)][metric]\n\n    basic_model = DecisionTreeRegressor(random_state=1)\n    basic_model.fit(train_X, train_y)\n    return basic_model.predict(val_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_val_X(features):\n    return nba[nba['season'].isin(testing_seasons)][features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_val_y(metric):\n    return nba[nba['season'].isin(testing_seasons)][metric]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df(predictions,val_X,val_y):\n    predicted_df=pd.DataFrame()\n    val_Xdf=pd.DataFrame(val_X)\n    val_Xdf['Prediction']=predictions\n    val_Xdf['Mvp?']=[nba['Mvp?'][index] for index in val_Xdf.reset_index()['index']]\n    val_Xdf['season']=[nba['season'][index] for index in val_Xdf.reset_index()['index']]\n    val_Xdf['player']=[nba['player'][index] for index in val_Xdf.reset_index()['index']]\n    return predicted_df.append(val_Xdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_list(df):\n    predicted_list=[]\n    for season in df['season'].value_counts().index:\n        season_df=df[df['season'].isin([season])]\n        predicted_list.append(season_df)\n    return predicted_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['bpm', 'g', 'mp_per_g','ws', 'ws_per_48','win_pct']\npredictions=predict_model(features,'votes_first')\np=get_df(predictions,get_val_X(features),get_val_y('votes_first'))\np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create empty 'Mvp prediction' column that can be modified\np['Mvp prediction']='No'\n\n#for every season\nfor season in p['season'].value_counts().index:\n    \n    #isolate data from that season, reset index\n    season_df=p[p['season'].isin([season])]\n    \n    #find index player with most first place votes\n    winner=season_df['Prediction'].idxmax()\n    \n    #go through indices of full dataframe by calling 'index' column\n    p['Mvp prediction'][winner]='Yes'          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list=create_list(p)\nwrong_seasons=[]\nright_seasons=[]\nfor df in list:\n    if df[df['Mvp?'].isin([True])].reset_index()['player'][0]==df[df['Mvp prediction'].isin(['Yes'])].reset_index()['player'][0]:\n        right_seasons.append(df.reset_index()['season'][0])\n    else:\n        wrong_seasons.append(df.reset_index()['season'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4/8 correct. let's try this with the player output strategy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_season(df,season):\n    return df[df['season'].isin([season])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['fga', 'fg3a', 'fta', 'per', 'ts_pct', 'usg_pct',\n       'g', 'mp_per_g', 'pts_per_g', 'trb_per_g',\n       'ast_per_g', 'stl_per_g', 'blk_per_g', 'fg_pct', 'fg3_pct', 'ft_pct']\npredictions=predict_model(features,'votes_first')\np=get_df(predictions,get_val_X(features),get_val_y('votes_first'))\np['Mvp prediction']='No'\nfor season in p['season'].value_counts().index:\n    season_df=p[p['season'].isin([season])]\n    winner=season_df['Prediction'].idxmax()\n    p['Mvp prediction'][winner]='Yes'\nlist=create_list(p)\nwrong_seasons=[]\nright_seasons=[]\nfor df in list:\n    if df[df['Mvp?'].isin([True])].reset_index()['player'][0]==df[df['Mvp prediction'].isin(['Yes'])].reset_index()['player'][0]:\n        right_seasons.append(df.reset_index()['season'][0])\n    else:\n        wrong_seasons.append(df.reset_index()['season'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrong_seasons","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5/8 correct.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}