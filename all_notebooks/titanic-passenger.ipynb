{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first kaggle notebook, and I am new to Machine Learning. so if I have done any mistake or missed anything then please feel free to guide me. It'll be a great help for me...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Using Titanic dataset: train and test dataset, for predicting and analysing the survival of Passengers based on different features such as - Age, Sex, Pclass, Embarked, Fair, Siblings and parents&children. First detailed EDA analysis is done on train dataset and by fitting different algorithms and models will predict the passenger survival on test data set. In this, we are using following models for fitting and predicting:\n#### 1. Logistic Regression\n#### 2. SVC and LinearSVC\n#### 3. SGDClassifier\n#### 4. KNeighborsClassifier\n#### 5. DecisionTreeClassifier\n#### 6. RandomForest\n#### 7. AdaBoostClassifier\n#### 8. NaiveBayes\n#### 9. XGBoost\n#### 10. Perceptron","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"------------------------------------------------------------------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1. Importing the libraries and the datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='white', color_codes=True)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the datasets\n\ntrain = pd.read_csv('../input/titanic-datasets/train.csv')\ntest = pd.read_csv('../input/titanic-datasets/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Datasets is been divided into train and test. All the models will be fitted to train sets and later the best model will use test data to predict.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"--------------------------------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like, Age, Cabin and Embarked has missing values. Cabin has almost 80% of misssing values, which will contribute no good to prediction, so i will remove the Cabin from both train and test datasets. Age has few missing values, so later i'll impute average age values based on Pclass in it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot('Pclass', 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### in above, Pclass 1 has the higher survival rate than the other two class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### In test dataset Survived column is missing, that's for we have to predict the survival chance by fitting the model to train dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# relation between features and survival\nsurvived = train[train['Survived']==0]\nnot_survived = train[train['Survived']==1]\n\nprint(\"Survived: %i (%.1f%%)\" % (len(survived), float(len(survived))/len(train)*100))\nprint(\"Not_Survived: %i (%.1f%%)\" % (len(not_survived), float(len(not_survived))/len(train)*100))\nprint(\"Total: %i\" % len(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pclass vs. Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# total passengers in different Pclass\ntrain.Pclass.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that most of the passengers are from Pclass 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# survival in different Pclass\nPclass_survived = train.groupby('Pclass').Survived.value_counts()\nPclass_survived","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Survival rate is more is higher class(Pclass 1) than other two class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Pclass_survived.unstack(level=0).plot(kind='bar',subplots=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pclass_survived_avg = train[['Pclass','Survived']].groupby(['Pclass'], as_index=False).mean()\nPclass_survived_avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pclass_survived_avg.plot(kind='bar', subplots=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sex vs. Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of males and females boarded\ntrain.Sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot('Sex','Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though female is less than male, survival of females are greater than male. Females and children are more likely to survive than males","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_survived = train.groupby('Sex').Survived.value_counts()\nsex_survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_survived.unstack(level=0).plot(kind='bar', subplots=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seeing the survival based on Pclass\nsns.factorplot('Sex', 'Survived', hue='Pclass', size=4, aspect=2, data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Observing the above fatorplot, almost all the females survived in higher Pclass(1 and2). In Pclass 3, females are less survived. for males, those were from Pclass_1, survived but their survival were less than than of females. in lower Pclass very few males were survived.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x='Pclass', y='Survived', hue='Sex', col='Embarked', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Emabarked vs. Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Embarked.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('Embarked').Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot('Embarked', 'Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Embarked_survived_avg = train[['Embarked','Survived']].groupby(['Embarked'], as_index=False).mean()\nEmbarked_survived_avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Embarked_survived_avg.plot(kind='bar', subplots=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sex vs. Survived","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# showing the survival rate based on Embarked, Pclass, Sex by violinplot\n\nfig = plt.figure(figsize=(15,5))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nsns.violinplot('Embarked', 'Age', hue = 'Survived', data=train, split=True, ax=ax1)\nsns.violinplot('Pclass', 'Age', hue = 'Survived', data=train, split=True, ax=ax2)\nsns.violinplot('Sex', 'Age', hue = 'Survived', data=train, split=True, ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In above violinplots, in Pclass section, all the mid-aged passengers survived in class_1, in class 2 and class 3, older people were less likely to survived. Similarly, from Sex section, almost all the females were survived from children to older age, males in older age were less likely to survived.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution plot based on survival rate\n\ntotal_survived = train[train['Survived']==1]\ntotal_not_survived = train[train['Survived']==0]\n\ntotal_male_survived = train[(train['Survived']==1) & (train['Sex']=='male')]\ntotal_female_survived = train[(train['Survived']==1) & (train['Sex']=='female')]\n\nmale_not_survived = train[(train['Survived']==0) & (train['Sex']=='male')]\nfemale_not_survived = train[(train['Survived']==0) & (train['Sex']=='female')]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(111)\n\nsns.distplot(total_survived['Age'].dropna().values, kde=True, bins=range(0,81,1), color='blue')\nsns.distplot(total_not_survived['Age'].dropna().values, kde=True, bins=range(0,81,1), color='red', axlabel = 'Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that, survival rate of children aged between 0 to 12/or 14 is greater. passengers between 15 to 30 or 35 is less survived. again survival chances increased for the mid-aged people till 40. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.distplot(total_male_survived['Age'].dropna().values, kde=True, bins=range(0,81,1), color='blue')\nsns.distplot(male_not_survived['Age'].dropna().values, kde=True, bins=range(0,81,1), color='red', axlabel= 'Male Age')\n\nplt.subplot(122)\nsns.distplot(total_female_survived['Age'].dropna().values, kde=True, bins=range(0,81,1), color='blue')\nsns.distplot(female_not_survived['Age'].dropna().values, kde=True, bins=range(0,81,1), color='red', axlabel= 'Female Age')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Survival rate for male children aged between 0 to 14 is greater and older male passenger. Survival rate for female above age 25 something is greater than that of below 25.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n## Feature Selection & Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Missing Values in train dataset\n# 2. defining a function for missing values\n\ndef missing_value(train):\n    total = train.isnull().sum().sort_values(ascending=False)\n    percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent'])\n    return missing_data\n\nmissing_value(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see here that Age has 177 missing values and Cabin has 687 missing values. Cabin has almost all the values missing so we can drop cabin from dataset, and for Age, we will replace all themissing values with average Age based on Pclass.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_value(test):\n    total = test.isnull().sum().sort_values(ascending=False)\n    percent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent'])\n    return missing_data\n\nmissing_value(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here also, Age and Cabin row has the most values missing, since Cabin has almost all the missing values so we will drop the Cabin from test dataset and For Age, will follow the same as for in train_datatset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average Age for different Pclass\n\ntrain.groupby(['Pclass']).Age.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"similarly for test_dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.groupby(['Pclass']).Age.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the function which will used later on for replacing missing values for Age.\n\ndef Age_approx(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 38\n        elif Pclass==2:\n            return 30\n        else:\n            return 25\n    else:\n        return Age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the function which will used later on for replacing missing values for Age.\n\ndef age_approx(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass==1:\n            return 41\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'] = train[['Age','Pclass']].apply(Age_approx, axis=1)\ntest['Age'] = test[['Age','Pclass']].apply(age_approx, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the embarked missing value \ntrain.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the categorical value for Sex and Embarked in both train and test dataset\n# to avoid dummy trap, using drop_first=True\n\ntrain_dummied = pd.get_dummies(train, columns=[\"Sex\"], drop_first = True)\ntrain_dummied = pd.get_dummies(train_dummied, columns=[\"Embarked\"], drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dummied = pd.get_dummies(test, columns=[\"Sex\"], drop_first=True)\ntest_dummied = pd.get_dummies(test_dummied, columns=[\"Embarked\"], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dummied.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dummied.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_dummied.drop(['Survived'], axis=1)\ny=train_dummied['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\nindependent_scalar = StandardScaler()\nX_train = independent_scalar.fit_transform (X_train) #fit and transform\nX_test = independent_scalar.transform (X_test) # only transform\ntest_dummied = independent_scalar.transform(test_dummied)\n\n## Feature Scaling  and categorical encoding is not required for tree-based model model.\n# Random Forest\n# Decision Tree\n# Adaboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###    Fitting the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### 1. Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" For Logistic Regression, accuracy is 77.5%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2. SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train,y_train)\ny_pred_svc = svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, y_pred_svc))\nprint(confusion_matrix(y_test, y_pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For SVC, accuracy is 80.14%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3. LinearSVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_svc = LinearSVC()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_svc.fit(X_train,y_train)\ny_pred_linear_svc = linear_svc.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_linear_svc))\nprint(confusion_matrix(y_test, y_pred_linear_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For LinearSVM, accuracy is 77.9%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 4. SGDClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_sgd = clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_sgd))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for SGDClassifier, accuracy is 69.66%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5. KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred_knn))\nprint(confusion_matrix(y_test, y_pred_knn))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6. DecisionTree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ny_pred_dt = dt.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_pred_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_pred_dt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For DecisionTreeClassifier, accuracy is 76%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 7. Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators = 100)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\n\nprint(confusion_matrix(y_test,y_pred_rf))\nprint(accuracy_score(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test,y_pred_rf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for Random Forest, accuracy is 79%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 8. Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train,y_train)\ny_pred_nb = nb.predict(X_test)\n\nprint(accuracy_score(y_test,y_pred_nb))\nprint(classification_report(y_test,y_pred_nb))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Naive Bayes, accuracy is 76%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 9. AdaBoost Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = AdaBoostClassifier()\nabc.fit(X_train,y_train)\ny_pred_abc = abc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred_abc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For AdaBoostClassifier, accuracy is 77.5%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 10. XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For XGBoost, accuracy is 80.14%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 11. Perceptron","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = Perceptron(max_iter=5, tol=None)\nclf.fit(X_train, y_train)\ny_pred_perceptron = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,y_pred_perceptron)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Perceptron, accuracy is 71.53%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ................Best fitted model is SVC and XGBoost.....................","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Above used all the individuals model are just fitted on data which is randomly splitted but for only once. Models are fitted on just train data, and predicted on test data. This can't give the most accurate results, as only few data is fitted and another set is predicted. Here we will use cross-Validation technique, where data is splited in n different sets and fitted and predicted simultanuously on every sets, so that every sets are been trained for models and then the mean is calculated for every accuracy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---------------------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### KNN model tuning ---\n##### using Grid Search & cross-validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import ShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k_values = np.array([1,3,5,7,9,11,13,15])\nparam_grid = dict(n_neighbors=k_values)\nmodel = KNeighborsClassifier()\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = 10, scoring = 'accuracy')\ngrid_result = grid.fit(X_train,y_train)\n\nprint('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, sdv,param in zip(means, stds, params):\n    print('%f (%f) with: %r' % (mean, sdv, param))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"by using GridSearch, we got that best estimator for KNN is n_neighbors=11, with accuracy 82.9% which can be improved by taking n_neighbors more into account.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Estimating the accuracy for every model while using cross-validation technique","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('SVC', SVC()))\nmodels.append(('Linear_SVC', LinearSVC()))\nmodels.append(('SGD', SGDClassifier(max_iter=5, tol=None)))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier(n_estimators = 100)))\nmodels.append(('abc', AdaBoostClassifier()))\nmodels.append(('Perceptron', Perceptron(max_iter=5, tol=None)))\nmodels.append(('XGBoost', XGBClassifier()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results =[]\nnames =[]\nfor name, model in models:\n    cv_result = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'accuracy')\n    results.append(cv_result)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_result.mean(), cv_result.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### checking the model performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining Learning curve function\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining validation curve function\n\ndef plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, cv)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(param_range, train_mean, color='r', marker='o', markersize=5, label='Training score')\n    plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='r')\n    plt.plot(param_range, test_mean, color='g', linestyle='--', marker='s', markersize=5, label='Validation score')\n    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n    plt.grid() \n    plt.xscale('log')\n    plt.legend(loc='best') \n    plt.xlabel('Parameter') \n    plt.ylabel('Score') \n    plt.ylim(ylim)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot learning curves LOGISTIC REGRESSION\ntitle = \"Learning Curves (Logistic Regression)\"\nplot_learning_curve(lr, title, X_train, y_train, ylim=(0.7, 1.01), cv=10, n_jobs=1);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot validation curve lOGISTIC REGRESSION\ntitle = 'Validation Curve (Logistic Regression)'\nparam_name = 'C'\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] \ncv = 10\nplot_validation_curve(estimator=lr, title=title, X=X_train, y=y_train, param_name=param_name,\n                      ylim=(0.5, 1.01), param_range=param_range);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot learning curves with SVM\n\ntitle = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = 10\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"well, I don't know what happened here, I might have done something wrong I guess. If you can guide me it'll be great help to me to understand.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# validation plot with SVM\n\ntitle = 'Validation Curve (Naive Bayes)'\nparam_name = 'gamma'\nparam_range = np.logspace(-6,-1,5) \ncv = 10\nplot_validation_curve(estimator=svc, title=title, X=X_train, y=y_train, param_name=param_name,\n                      ylim=(0.5, 1.01), param_range=param_range);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learning plot with NAIVE BAYES\n\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\ntitle = \"Learning Curves (Naive Bayes)\"\nplot_learning_curve(nb, title, X_train, y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=4);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Algorithm Comparison","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15,8))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGBoost and SVC has the most accurate result with the accuracy of XGBoost = 83% and SVC = 84%, followed by AdaBoost(81.9%) and RandomForest(81.3%). Earlier in case without cross-validation, accuracy score of XGBoost and SVC was 80.14%.\nwe can do Hyperparameter Tuning for each model and can find even more accurate result.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Prediction on Validation dataset i.e., test dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"for prediction we will use XGBoost model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_valid_xgb = xgb.predict(test_dummied)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ---------- Hyperparameter Tuning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 1. Logistictic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)\nprint(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\n\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in above, i am using data X,y which is not scaled. and the [accuracy is 80.72%, with solver 'lbfgs']","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining  the mode and parameters\n\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in above, i am using scaled data, X_train and y_train.. for which accuracy is [82.42% and solver is 'newton-cg']","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 2. Ridge Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeClassifier\n\nmodel = RidgeClassifier()\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n# define grid search\ngrid = dict(alpha=alpha)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[accuracy is 80.97% with alpha 0.1]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 3. KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[accuracy is 83.49%, metric - 'manhattan', n_neighbors = 15, weight = 'uniform']","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 4. SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[accuracy is 83.98% using 'C':1.0, 'gamma': scale, 'kernel': rbf]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 5. Bagged Decision Tree(Bagging)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\n# define models and parameters\nmodel = BaggingClassifier()\nn_estimators = [10, 100, 1000]\n# define grid search\ngrid = dict(n_estimators=n_estimators)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[accuracy is 81.03%, n_estimators(no. of trees) = 100]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 6. Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define models and parameters\n\nmodel = RandomForestClassifier()\nn_estimators = [10, 100, 1000]\nmax_features = ['sqrt', 'log2']\n# define grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[accuracy is 81.61%] ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 7. Stocastic Gradient Boosting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\n# define models and parameters\nmodel = GradientBoostingClassifier()\nn_estimators = [10, 100, 1000]\nlearning_rate = [0.001, 0.01, 0.1]\nsubsample = [0.5, 0.7, 1.0]\nmax_depth = [3, 7, 9]\n# define grid search\ngrid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"accuracy is 84%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### --- 8. XGBoost Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining model and parameters\n\nmodel = XGBClassifier()\nn_estimators = [1000]\nlearning_rate = [0.01, 0.1]\nsubsample = [0.8,1.0]\nmax_depth = [3]\ncolsample_bytree = [0.8, 0.9, 1.0]\ngamma = [1]\n# define grid search\ngrid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth, \n            colsample_bytree = colsample_bytree, gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score='raise')\ngrid_result = grid_search.fit(X_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy is 0.845127 using {'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.8}","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---------------------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### -------- Model Ranking ----------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### 1. XGBoost - (accuracy: 84.51%)\n##### 2. SGD - (accuracy: 84.40%)\n##### 3. SVC - (accuracy: 83.98%)\n##### 4. KNN - (accuracy: 83.49%)\n##### 5. Logistic Regression - (accuracy: 82.42%)\n##### 6. Random Forest - (accuracy: 81.7%)\n##### 7. Ridge Classifier - (accuracy: 80.97%)\n##### 8. Bagging - (accuracy - 80.7%)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}