{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all my aim is creating new variables as categorical variable and  getting a good model.","metadata":{}},{"cell_type":"markdown","source":"# ***Reading Data***","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"../input/star-type-classification/Stars.csv\") #Import data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is good that we don't have any null value.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see some variables are highly spread.","metadata":{}},{"cell_type":"code","source":"for i in \"Color\",\"Spectral_Class\",\"Type\":\n    print(i+\":\"+str(df[i].unique())+\"\\n\") \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to adjust Color variable.","metadata":{}},{"cell_type":"code","source":"for i in [\"Yellowish White\",\"Blue White\",\"Blue white\",\"Blue-white\",\"Whitish\",\"yellow-white\",\"white\",\"Blue-White\"]:\n    df.loc[df[\"Color\"]==i,\"Color\"]=\"White\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Color\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df[\"Color\"]=='Pale yellow orange',\"Color\"]=\"Orange\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Color\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in ['White-Yellow','yellowish','Yellowish']:\n        df.loc[df[\"Color\"]==i,\"Color\"]=\"Yellow\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Color\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[df[\"Color\"]=='Orange-Red',\"Color\"]=\"Red\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Color\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Color adjustment is done.","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization**","metadata":{}},{"cell_type":"markdown","source":"First of all I want to look again to the table to visualize easily.","metadata":{}},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=df.groupby(\"Type\").mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=data.index,y=\"Temperature\",data=data)\nplt.title(\"Temperature vs Type\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can create a new variable by using this information, I can create a temp variable which contains\nlow-temp and and high-temp. low-temp ones are type 1 and type 2 while others are high temperature.","metadata":{}},{"cell_type":"code","source":"sns.barplot(x=data.index,y=\"L\",data=data)\nplt.title(\"L vs Type\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For type 0,1,2 L variable is incredibly low, while for type 3 it is  moderate and for 4-5 it is high. I can create new categorical variable by looking this information.","metadata":{}},{"cell_type":"code","source":"sns.barplot(x=data.index,y=\"R\",data=data)\nplt.title(\"R vs Type\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still I can separte 0-1-2-3-4 and 5 by using this information.","metadata":{}},{"cell_type":"code","source":"sns.barplot(x=data.index,y=\"A_M\",data=data)\nplt.title(\"A_M vs Type\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This information still leads to us a new categorical variable.","metadata":{}},{"cell_type":"code","source":"data2=df[\"Color\"].value_counts()\ndata2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=data2.index,y=df[\"Color\"].value_counts())\nplt.title(\"Numbers of stars by their colors\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still we can see the correlations by looking this correlation map. A_M variable highly negative correlated with Type so it is important for us.","metadata":{}},{"cell_type":"markdown","source":"Now we need to look for outliers.","metadata":{}},{"cell_type":"code","source":"def outlier_graph(data,column):\n    plt.figure(figsize=(5,3))\n    sns.boxplot(data[column])\n    plt.title(\"{} distribution\".format(column))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in [\"Temperature\",\"L\",\"R\",\"A_M\"]:\n    outlier_graph(df,i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are too many outilers in R and L but remove them can lead to wrong model to us because variables already seperated in non-uniform way. Removing these outlierswill cause a lack of information so I don't touch them.","metadata":{}},{"cell_type":"markdown","source":"# **Creating New Variables**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Temperature_Cat=[]\nfor i in df[\"Temperature\"]:\n    if i > 6000:\n        Temperature_Cat.append(\"Temp_High\")\n    else:\n        Temperature_Cat.append(\"Temp_Low\")\nlen(Temperature_Cat)\ndf[\"Temperature_Cat\"]=Temperature_Cat        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L_Cat=[]\nfor i in df[\"L\"]:\n    if i >= 100000:\n        L_Cat.append(\"L_High\")\n    elif 25000<= i < 100000:\n        L_Cat.append(\"L_Moderate\")\n    else:\n        L_Cat.append(\"L_Low\")        \nlen(L_Cat)\ndf[\"L_Cat\"]=L_Cat  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R_Cat=[]\nfor i in df[\"R\"]:\n    if i > 400:\n        R_Cat.append(\"R_High\")\n    else:\n        R_Cat.append(\"R_Low\")\nlen(R_Cat)\ndf[\"R_Cat\"]=R_Cat        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"A_M_Cat=[]\nfor i in df[\"A_M\"]:\n    if i > 0:\n        A_M_Cat.append(\"A_M_High\")\n    else:\n        A_M_Cat.append(\"A_M_Low\")\nlen(A_M_Cat)\ndf[\"A_M_Cat\"]=A_M_Cat        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we created all of our variables, now we can create dummies and start modelling.","metadata":{}},{"cell_type":"code","source":"df=pd.get_dummies(data=df,columns=[\"Color\",\"Spectral_Class\",\"Temperature_Cat\",\"L_Cat\",\"R_Cat\",\"A_M_Cat\"],drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I deliberately dropped the first values because if other values are 0, our model will know that dropped one is 1 so it is unnecessary to put all values and putting all the values in would inflate our model.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are ready for modelling.","metadata":{}},{"cell_type":"markdown","source":"# **Modelling**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler,StandardScaler,QuantileTransformer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score,StratifiedKFold,train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=df.drop([\"Type\"],axis=1)\ny=df[\"Type\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\nprint(\"x_train\",len(x_train))\nprint(\"x_test\",len(x_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Cross Validation & Modelling**","metadata":{}},{"cell_type":"markdown","source":"I will try KNN, SVC,Random Forest,Decision Tree. I won't use  Hyperparameter Tuning or Grid Search since our variables are pretty clear and I don't want to push too hard my model, I will go basic. ","metadata":{}},{"cell_type":"code","source":"Classifiers=[]\nScores=[]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Start with KNN","metadata":{}},{"cell_type":"code","source":"for i in [5,6,7,8,9,10]:\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    Scores.append(cross_val_score(knn, x_test, y_test, cv=5).mean())\n    Classifiers.append(\"Knn{}\".format(str(i)))\n    plt.subplots()\n    sns.heatmap(confusion_matrix(y_test, knn.predict(x_test)),annot=True)\n    plt.title(\"Knn{}\".format(str(i)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVC","metadata":{}},{"cell_type":"code","source":"svc=SVC(random_state = 5)\nsvc.fit(x_train,y_train)\nScores.append(cross_val_score(svc, x_test, y_test, cv=5).mean())\nClassifiers.append(\"Svc\")\nsns.heatmap(confusion_matrix(y_test,svc.predict(x_test)),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest","metadata":{}},{"cell_type":"code","source":"for i in [30,60,80,100]:\n    rf=RandomForestClassifier(n_estimators=i,random_state = 5)\n    rf.fit(x_train,y_train)\n    Scores.append(cross_val_score(rf, x_test, y_test, cv=5).mean())\n    plt.subplots()\n    sns.heatmap(confusion_matrix(y_test, rf.predict(x_test)),annot=True)\n    plt.title(\"Rf{}\".format(str(i)))\n    Classifiers.append(\"Rf{}\".format(str(i)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision Tree","metadata":{}},{"cell_type":"code","source":"dtc=DecisionTreeClassifier(random_state = 5)\ndtc.fit(x_train,y_train)\nScores.append(cross_val_score(dtc, x_test, y_test, cv=5).mean())\nsns.heatmap(confusion_matrix(y_test, dtc.predict(x_test)),annot=True)\nClassifiers.append(\"Dtc\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize Scores.","metadata":{}},{"cell_type":"code","source":"graph_data= pd.DataFrame(list(zip(Classifiers,Scores)),columns =['Classifiers', 'Scores']) \ngraph_data=graph_data.sort_values(\"Scores\",ascending=False)\nplt.figure(figsize=(16,8))\nsns.barplot(x=graph_data[\"Classifiers\"],y=graph_data[\"Scores\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graph_data.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that Random Forest and Decision Tree are the best models.We can combine them.","metadata":{}},{"cell_type":"markdown","source":"# **Ensemble Model**","metadata":{}},{"cell_type":"code","source":"Last_Model = VotingClassifier(estimators = [('dtc', DecisionTreeClassifier(random_state = 5)),\n                                        ('Rf60', RandomForestClassifier(n_estimators=60,random_state = 5)),\n                                        ('Rf30', RandomForestClassifier(n_estimators=30,random_state = 5))],\n                                        voting = \"hard\", n_jobs = -1)\nLast_Model = Last_Model.fit(x_train, y_train)\nprint(accuracy_score(Last_Model.predict(x_test),y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy seems %100 but it is because we have only 240 data, If we would have more it will decrease.","metadata":{}},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_test, Last_Model.predict(x_test)),annot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have no problem all the predicts are correct but still we need bigger dataset to create better model. 100% accuracy can be misleading.","metadata":{}}]}