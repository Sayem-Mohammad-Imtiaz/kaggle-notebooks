{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import GradientBoostingClassifier \n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import metrics\nimport seaborn as sns\n\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Reading the data into Pandas DataFrame.\n* Describing the data.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/churn-in-telecoms-dataset/bigml_59c28831336c6604c800002a.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See types of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to check how many nulls, int, objects we have in the data to do a better preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are no null columns. It is wery important where we learn data, because most of the algorithms don’t know how to deal with nulls so they have to be replaced"},{"metadata":{},"cell_type":"markdown","source":" We can see than not all the columns are numerical, we will need to dig dipper for the object type"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('churn')['phone number'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_True = df[\"churn\"][df[\"churn\"] == True]\nprint (\"Churn Percentage = \"+str( (y_True.shape[0] / df[\"churn\"].shape[0]) * 100 ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Процент данных показывающих целевую группу \"отток\" 0.14')\ny = df[\"churn\"].value_counts()\nsns.barplot(y.index, y.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, there are more churn = False then True. we will need to balance the data before making predictions. for balancing we will use synthetic data from SMOTE: Synthetic Minority Over-sampling Technique"},{"metadata":{},"cell_type":"markdown","source":"from the data description, we can see that phone number is unique - therefor it not provides us information we can learn. we will drop phone number column and enumerate all the categorial objects columns. enumeration advantage is for easier use of the algorithms witch often accept only numbers"},{"metadata":{},"cell_type":"markdown","source":"we enumerate with encoder-decoder to have a fast way to switch between the two if needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([\"phone number\"], axis = 1, inplace=True)\n\nlabel_encoder = preprocessing.LabelEncoder()\n\ndf['state'] = label_encoder.fit_transform(df['state'])\ndf['international plan'] = label_encoder.fit_transform(df['international plan'])\ndf['voice mail plan'] = label_encoder.fit_transform(df['voice mail plan'])\ndf['churn'] = label_encoder.fit_transform(df['churn'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the correlation matrix\ncorr = df.corr()\n\n# plot the heatmap\nfig = plt.figure(figsize=(5,4))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n            linewidths=.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see strong correlation between the features:\n\ntotal day/eve/night/intl charge - total day/eve/night/intl minutes we can assume they charge per call time.\n\nanother correlation is between voice mail plan and number vmail mail massages.\n\ncorrelation with churn:\n\ninternational plan total day minutes total day charge customers service call"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will normalize our data so the prediction on all features will be at the same scale\nX = df.iloc[:,0:19].values\ny = df.iloc[:,19].values\n#nurmalize the data\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\ndfNorm = pd.DataFrame(X_std, index=df.index, columns=df.columns[0:19])\n# # add non-feature target column to dataframe\ndfNorm['churn'] = df['churn']\ndfNorm.head(10)\n\nX = dfNorm.iloc[:,0:19].values\ny = dfNorm.iloc[:,19].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the dataset into Train and Test data according to the dimensions needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.15 , random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape , y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# help func 1\nresults_test = {}\nresults_train = {}\ndef prdict_date(algo_name,X_train,y_train,X_test,y_test,verbose=0):\n    algo_name.fit(X_train, y_train)\n    Y_pred = algo_name.predict(X_test)\n    acc_train = round(algo_name.score(X_train, y_train) * 100, 2)\n    acc_val = round(algo_name.score(X_test, y_test) * 100, 2)\n    results_test[str(algo_name)[0:str(algo_name).find('(')]] = acc_val\n    results_train[str(algo_name)[0:str(algo_name).find('(')]] = acc_train\n    if verbose ==0:\n        print(\"acc train: \" + str(acc_train))\n        print(\"acc test: \"+ str(acc_val))\n    else:\n        return Y_pred\n\n#help func 2\ndef conf(algo_name,X_test, y_test):\n    y_pred = algo_name.predict(X_test)\n    forest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0])\n    sns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [\"1\", \"0\"] , yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.title(str(algo_name)[0:str(algo_name).find('(')])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# smote\nwe do sentetic data only on the train: SMOTE creates synthetic observations of the minority class (churn) by:\n\nFinding the k-nearest-neighbors for minority class observations (finding similar observations) Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.\n\nwe use smote only on the training data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=0, k_neighbors=4)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data labels before SMOTE:\nimport collections\ncollections.Counter(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#after SMOTE:\nimport collections\ncollections.Counter(y_train_res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{},"cell_type":"markdown","source":"# 1.RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=80, random_state=0, criterion='entropy')\nprdict_date(random_forest,X_train_res,y_train_res,X_test,y_test)\nprint(classification_report(y_test, random_forest.predict(X_test)))\nconf(random_forest,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train: Gradient Boosting\ngbc = GradientBoostingClassifier(loss='deviance', learning_rate=0.2, n_estimators=200 , max_depth=6)\nprdict_date(gbc,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, gbc.predict(X_test)))\nconf(gbc,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Метод опорных векторов (SVM)\nwe can’t know ahead witch type of kernel will predict the best results- therefor we tried multiple different kernels types."},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear svm:\n\nsvm = SVC(kernel='linear', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# linear rbf:\nsvm = SVC(kernel='rbf', probability=True, gamma=10)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  poly svm:\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train: SVM\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, svm.predict(X_test)))\nconf(svm,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K Neighbors Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we will try to find witch K is the best on our data\n# first, we will look which give us the best predictions on the train:\nfrom sklearn import model_selection\n\n#Neighbors\nneighbors = [x for x in list(range(1,20)) if x % 2 == 0]\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\n#Perform 10-fold cross validation on training set for odd values of k:\nseed=0\n\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    scores = model_selection.cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    #print(\"k=%d %0.2f (+/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\n\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint(( \"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k])))\n\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# then on the test:\ncv_preds = []\n\n#Perform 10-fold cross validation on testing set for odd values of k\nseed=0\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    preds = model_selection.cross_val_predict(knn, X_test, y_test, cv=kfold)\n    cv_preds.append(metrics.accuracy_score(y_test, preds)*100)\n    #print(\"k=%d %0.2f\" % (k_value, 100*metrics.accuracy_score(test_y, preds)))\n\noptimal_k = neighbors[cv_preds.index(max(cv_preds))]\nprint(\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_preds[optimal_k]))\n\nplt.plot(neighbors, cv_preds)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Test Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 2)\nprdict_date(knn,X_train_res,y_train_res,X_test,y_test)\n\nprint(classification_report(y_test, knn.predict(X_test)))\nconf(knn,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logr = LogisticRegression()\nprdict_date(logr,X_train,y_train,X_test,y_test)\n\nprint(classification_report(y_test, logr.predict(X_test)))\nconf(logr,X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test =pd.DataFrame(list(results_test.items()),\n                      columns=['algo_name','acc_test'])\ndf_train =pd.DataFrame(list(results_train.items()),\n                      columns=['algo_name','acc_train'])\ndf_results = df_test.join(df_train.set_index('algo_name'), on='algo_name')\ndf_results.sort_values('acc_test',ascending=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}