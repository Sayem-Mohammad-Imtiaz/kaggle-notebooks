{"cells":[{"metadata":{"_uuid":"7b32182c6deec5e9b7881211585833baae2fdd67","_cell_guid":"16e99168-bfb6-4eee-b05f-70364092219d"},"cell_type":"markdown","source":"This notebook essentially covers a basic tutorial for Support Vector Machine. I am going to use the mobile prediction data for this excerise. \n\nNote: \n    1) This data set is not a great data set to practise SVM classification on, I used it to simple try out the SVM. \n    2) If you have a better data set then I would recommend use that or IRIS Data set is great for this problem. \n\nThe below topics are covered in this Kernal. \n- Data prepocessing \n- Target value Analysis\n- SVM\n- Linear SVM\n- SV Regressor\n- Non Linear SVM with kernal - RBF ( note: you can also try poly )\n- Non Linear SVR "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"59be0dafdd352d41c1e7eb2db65c015cd1d67b12","_cell_guid":"44fd9b5e-4374-44c8-9c62-2b092a54c68b"},"cell_type":"markdown","source":"**DATA PREPROCESSING**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ndf.head()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"b380904bd662112cb8f9ba79c416633962c9d46f","_cell_guid":"88bb234a-9d85-4e29-8aa8-88e54345743a","trusted":true},"cell_type":"code","source":"# checking if there is any missing value\ndf.isnull().sum().max()\ndf.columns","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"19a148d76b45f5728572ef26f4d450397c99a2f5","_cell_guid":"b002a54e-383b-4f76-9617-16722f4ae7b4"},"cell_type":"markdown","source":"**TARGET VALUE ANALYSIS**"},{"metadata":{"_uuid":"5a35aaad3708055baa0f5961d405984e33f58ace","_cell_guid":"460450c3-c004-4aa4-aa5d-df2286f70fe5","trusted":true},"cell_type":"code","source":"#understanding the predicted value - which is hot encoded, in real life price won't be hot encoded.\ndf['price_range'].describe(), df['price_range'].unique()\n\n# there are 4 classes in the predicted value","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"cbc0da9a7d1407a89d7b1a626581cbe05f486ae4","_cell_guid":"39880eac-14e2-4ae3-95fd-cc59bb3be576","trusted":true},"cell_type":"code","source":"corrmat = df.corr()\nf,ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corrmat,vmax=0.8,square=True,annot=True,annot_kws={'size':8})","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"aebd8eaef413193325f162368a21dd33b882c83a","_cell_guid":"09309689-3aea-4b56-bcf5-ed1592acf867","trusted":false,"collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,4))\nplt.scatter(y=df['price_range'],x=df['battery_power'],color='red')\nplt.scatter(y=df['price_range'],x=df['ram'],color='Green')\nplt.scatter(y=df['price_range'],x=df['n_cores'],color='blue')\nplt.scatter(y=df['price_range'],x=df['mobile_wt'],color='orange')\n# clearly we can see that each of the category has different set of value ranges ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6224bf0ccb965f5d4ff26629819054e32ff14c18","collapsed":true,"_cell_guid":"3165f683-cb9a-4987-9706-ae0daeda9af8","trusted":false},"cell_type":"code","source":"# Try plots using seaborn\n#sns.swarmplot(x='battery_power',y='ram',data=df,hue='price_range')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d601d3021de933308076bc236132185e24294bbc","collapsed":true,"_cell_guid":"59fd23fe-7b70-4915-809e-94eba411d38b","trusted":false},"cell_type":"code","source":"#sns.pairplot(df,size=2.5)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0da42e6d68e5e50a22243df17a9797c9ddb28008","_cell_guid":"aab929d8-5b3a-4a47-a0b0-b8d831078552"},"cell_type":"markdown","source":"Now in the data set there is no need to create dummy variables or handle missing data as data set doesn't have any missing data \n\n**SUPPORT VECTOR MACHINES AND METHODS : **"},{"metadata":{"_uuid":"7166482c61754c0ff3df01e6c6d0416bb1b2bfc6","_cell_guid":"bd158f89-8437-4417-9c4c-f3bddc84a0d3","trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\ny_t = np.array(df['price_range'])\nX_t = df\nX_t = df.drop(['price_range'],axis=1)\nX_t = np.array(X_t)\n\nprint(\"shape of Y :\"+str(y_t.shape))\nprint(\"shape of X :\"+str(X_t.shape))\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_t = scaler.fit_transform(X_t)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"5be1d3ac05124b701ce0a4cd51cfaafe5da7731d","_cell_guid":"d2117cb9-654e-4d8a-aa71-992156eeb6b1","trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(X_t,y_t,test_size=.20,random_state=42)\nprint(\"shape of X Train :\"+str(X_train.shape))\nprint(\"shape of X Test :\"+str(X_test.shape))\nprint(\"shape of Y Train :\"+str(Y_train.shape))\nprint(\"shape of Y Test :\"+str(Y_test.shape))","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"9f0c7c02180fc639afd8b166c0d36c3f3c237d48","_cell_guid":"1a841504-52fe-4926-900f-b7a98160f215","trusted":true},"cell_type":"code","source":"for this_C in [1,3,5,10,40,60,80,100]:\n    clf = SVC(kernel='linear',C=this_C).fit(X_train,Y_train)\n    scoretrain = clf.score(X_train,Y_train)\n    scoretest  = clf.score(X_test,Y_test)\n    print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,scoretrain,scoretest))","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"74cefa3fcd0748c92d96cfab6f343f73496d15b1","_cell_guid":"0ced714f-0c0e-44d1-aef5-496b545de189","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score,StratifiedKFold,LeaveOneOut\nclf1 = SVC(kernel='linear',C=20).fit(X_train,Y_train)\nscores = cross_val_score(clf1,X_train,Y_train,cv=5)\nstrat_scores = cross_val_score(clf1,X_train,Y_train,cv=StratifiedKFold(5,random_state=10,shuffle=True))\n#Loo = LeaveOneOut()\n#Loo_scores = cross_val_score(clf1,X_train,Y_train,cv=Loo)\nprint(\"The Cross Validation Score :\"+str(scores))\nprint(\"The Average Cross Validation Score :\"+str(scores.mean()))\nprint(\"The Stratified Cross Validation Score :\"+str(strat_scores))\nprint(\"The Average Stratified Cross Validation Score :\"+str(strat_scores.mean()))\n#print(\"The LeaveOneOut Cross Validation Score :\"+str(Loo_scores))\n#print(\"The Average LeaveOneOut Cross Validation Score :\"+str(Loo_scores.mean()))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bedfd3a4a9849d79f0aebded7ba900f06188a59b"},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\nfor strat in ['stratified', 'most_frequent', 'prior', 'uniform']:\n    dummy_maj = DummyClassifier(strategy=strat).fit(X_train,Y_train)\n    print(\"Train Stratergy :{} \\n Score :{:.2f}\".format(strat,dummy_maj.score(X_train,Y_train)))\n    print(\"Test Stratergy :{} \\n Score :{:.2f}\".format(strat,dummy_maj.score(X_test,Y_test)))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"258cde3bb984854f8efc776456704621546ba28a","_cell_guid":"f769b170-b8c1-40b5-b2a4-e77147c51f3d","trusted":true},"cell_type":"code","source":"# plotting the decision boundries for the data \n#converting the data to array for plotting. \nX = np.array(df.iloc[:,[0,13]])\ny = np.array(df['price_range'])\nprint(\"Shape of X:\"+str(X.shape))\nprint(\"Shape of y:\"+str(y.shape))\nX = scaler.fit_transform(X)","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"e30d5991bb9ec84737df5db10b55082a52150b0a","collapsed":true,"_cell_guid":"fb52356e-54f7-440a-8a93-493ddc6d9828","trusted":true},"cell_type":"code","source":"# custome color maps\ncm_dark = ListedColormap(['#ff6060', '#8282ff','#ffaa00','#fff244','#4df9b9','#76e8fc','#3ad628'])\ncm_bright = ListedColormap(['#ffafaf', '#c6c6ff','#ffaa00','#ffe2a8','#bfffe7','#c9f7ff','#9eff93'])","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"249279d2398462aefadbbc1fd5bf186c8f8bd110","_cell_guid":"3f9d9ffb-01a0-43fc-b133-1bd4b7aee27a","trusted":true},"cell_type":"code","source":"plt.scatter(X[:,0],X[:,1],c=y,cmap=cm_dark,s=10,label=y)\nplt.show()","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"c331b468bf4898754fd9adf9231df2bbb086bf86","_cell_guid":"822fa316-06b0-4207-8bc8-348713a62b92","trusted":true},"cell_type":"code","source":"h = .02  # step size in the mesh\nC_param = 1 # No of neighbours\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf1 = SVC(kernel='linear',C=C_param)\n    clf1.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min()-.20, X[:, 0].max()+.20\n    y_min, y_max = X[:, 1].min()-.20, X[:, 1].max()+.20\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf1.predict(np.c_[xx.ravel(), yy.ravel()])   # ravel to flatten the into 1D and c_ to concatenate \n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cm_bright)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_dark,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"SVM Linear Classification (kernal = linear, Gamma = '%s')\"% (C_param))\n\nplt.show()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"cea93036779601331a4e8e009b5e2e6116e603f3","_cell_guid":"29b5c886-cedc-4791-8dc0-b2da8f7febd0","trusted":true},"cell_type":"code","source":"print(\"The score of the above :\"+str(clf1.score(X,y)))","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"30f878663a0ac5fc43adbd7f598370f8ad3049ba","_cell_guid":"f26fe33c-fbac-4527-9ba3-8896d7621821","trusted":true},"cell_type":"code","source":"# Linear Support vector machine with only C Parameter \nfrom sklearn.svm import LinearSVC\n\nfor this_C in [1,3,5,10,40,60,80,100]:\n    clf2 = LinearSVC(C=this_C).fit(X_train,Y_train)\n    scoretrain = clf2.score(X_train,Y_train)\n    scoretest  = clf2.score(X_test,Y_test)\n    print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,scoretrain,scoretest))\n\n","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"c7164190b883c32dbe1a7530e403ad6410aad53a","_cell_guid":"83f1b6d3-5f89-48c3-8375-3e3d9dfe106f"},"cell_type":"markdown","source":"Apparently we got better scores with SVC where we defined the kernal as linear than with just LinearSVC\n\nThe LinearSVC class is based on the liblinear library, which implements an optimized algorithm for linear SVMs.\n1. It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features: its training time complexity is roughly O(m × n).\n\nThe SVC class is based on the libsvm library, which implements an algorithm that supports the kernel trick.\n1. The training time complexity is usually between O(m2 × n) and O(m3 × n). \n1. LinearSVC is much faster than SVC(kernel=\"linear\")\n"},{"metadata":{"_uuid":"812ac486811dd7fc2116d04fb053eb8c3fb09ee5","collapsed":true,"_cell_guid":"3978887a-e22b-4ce8-8e6f-64d47591aaf9","trusted":false},"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvr = SVR(kernel='linear',C=1,epsilon=.01).fit(X_train,Y_train)\nprint(\"{:.2f} is the accuracy of the SV Regressor\".format(svr.score(X_train,Y_train)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"824f056757dad9ac79ac567a0ec284f2edcef122","_cell_guid":"a1e04aaa-9fbd-41ec-a15e-0e1f9cc67163"},"cell_type":"markdown","source":"* SVM supports linear and nonlinear regression.\n* SVM Regression tries to fit as many instances as possible on the decision boundary while limiting margin violations.\n* The width of the decision boundary is controlled by a hyperparameter ϵ. \n"},{"metadata":{"_uuid":"219bc19e91cf8fde8bfdefb2ce47ffacadded2ac","_cell_guid":"23da8689-2020-4e30-9990-0a7a69646a7e"},"cell_type":"markdown","source":"**NON LINEAR SVM **\n\nA method to Handle Non linear relationships in our data set is to use polynomial Kernal or using a similarity function with our SVM.\n\nWe will use the Gaussian Radial Basis Function(RBF) function for the same. to handle this in Sklearn there is a Gamma hyperparameter. \nCheck the Gausian RBF Function - for more info. \n\nTechnically, the gamma parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points. Intuitively, a small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. **In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other.**\n\nInitution : we create different landmarks and then check how far the training examples are from the landmark. In practise, if we have n training examples then we will have n landmarks and we will thus create a feature set of n values with n landmarks. When the training example is closest to a landmark the value the variance will be small and when far the value will be large and hence we will associate the close to the landmark example with a 1 and those that are far with a 0. This ability makes the SVM very powerful. "},{"metadata":{"_uuid":"4f00573c1050ffa1c827277fab4df5663c8b50af","_cell_guid":"3e6ee5f0-ebad-40d5-a3f4-8bb034e2e069","trusted":true},"cell_type":"code","source":"# SMV with RBF KERNAL AND ONLY C PARAMETER \n\nfor this_C in [1,5,10,25,50,100]:\n    clf3 = SVC(kernel='rbf',C=this_C).fit(X_train,Y_train)\n    clf3train = clf3.score(X_train,Y_train)\n    clf3test  = clf3.score(X_test,Y_test)\n    print(\"SVM for Non Linear \\n C:{} Training Score : {:2f} Test Score : {:2f}\\n\".format(this_C,clf3train,clf3test))","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"8cb1bbc811ddfb888d2713d457e7819ec8b95259","_cell_guid":"785e7823-68ea-457b-aa0f-cdcc63009e7a","trusted":true},"cell_type":"code","source":"h = .02  # step size in the mesh\nC_param = 1 # No of neighbours\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf1 = SVC(kernel='rbf',C=C_param)\n    clf1.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()\n    y_min, y_max = X[:, 1].min(), X[:, 1].max()\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf1.predict(np.c_[xx.ravel(), yy.ravel()])   # ravel to flatten the into 1D and c_ to concatenate \n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cm_bright)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_dark,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"SVM Linear Classification (kernal = linear, Gamma = '%s')\"% (C_param))\n\nplt.show()","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"7d5e6aebafc97cbbac2dd2c999a73a94a5a37140","_cell_guid":"65c30a5e-ff26-450b-8bcf-b2e3e7facc1b","trusted":true},"cell_type":"code","source":"# SVM WITH RBF KERNAL, C AND GAMMA HYPERPARAMTER \nfor this_gamma in [.1,.5,.10,.25,.50,1]:\n    for this_C in [1,5,7,10,15,25,50]:\n        clf3 = SVC(kernel='rbf',C=this_C,gamma=this_gamma).fit(X_train,Y_train)\n        clf3train = clf3.score(X_train,Y_train)\n        clf3test  = clf3.score(X_test,Y_test)\n        print(\"SVM for Non Linear \\n Gamma: {} C:{} Training Score : {:2f} Test Score : {:2f}\\n\".format(this_gamma,this_C,clf3train,clf3test))","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"5d7eac401e043fb97096220d268b7f64cd15bbd9","_cell_guid":"1bdc1023-2463-4061-838d-d88b24c9abef","trusted":false,"collapsed":true},"cell_type":"code","source":"# grid search method \nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [1,5,7,10,15,25,50],\n              'gamma': [.1,.5,.10,.25,.50,1]}\nGS = GridSearchCV(SVC(kernel='rbf'),param_grid,cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b18d724b4a80badef44189454b4790f0e88ac4e0","_cell_guid":"c5743c93-6e00-4d86-b46f-7f2581663655","trusted":false,"collapsed":true},"cell_type":"code","source":"GS.fit(X_train,Y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b9d9add3fb4fc19e57e5726b2933bf23be2f063","_cell_guid":"5d4d4803-2858-4767-bffc-b3692dd04f10","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"the parameters {} are the best.\".format(GS.best_params_))\nprint(\"the best score is {:.2f}.\".format(GS.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f767e48ec82fbd49d3d7d8c925aee489e35711d","collapsed":true,"_cell_guid":"bf90ef41-1cca-4d3c-b544-fa5a2b41b2fd","trusted":false},"cell_type":"code","source":"# Kernalized SVM machine \nsvr2 = SVR(degree=2,C=100,epsilon=.01).fit(X_train,Y_train)\nprint(\"{:.2f} is the accuracy of the SV Regressor\".format(svr2.score(X_train,Y_train)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8b6b3b800dee86d7f26e21cf36c3d57e035b41e","collapsed":true,"_cell_guid":"44b3e578-a4a2-48b3-8da6-22c532c2c831"},"cell_type":"markdown","source":"We can notice that the kernalised Support vector machine regressor gives better accuracy than the previous Linear Regressor(non kernal) SVM. Never the less one, needs to understand the data one is work on before trying out various methods. Cross validation techniques are useful.\n\nI may futher add Cross Validation techniques for your use."},{"metadata":{"_uuid":"a058f56b11f09d5249ee05feaf392e160914a262","collapsed":true,"_cell_guid":"d04d0cb5-cd1f-4c82-be02-8be365a05016","trusted":false},"cell_type":"code","source":"test = test.drop(['id'],axis=1)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7346ac57853254a4ec52994c84c100e160dc92aa","collapsed":true,"_cell_guid":"32576733-0a12-4a81-beec-a22358aff6f4","trusted":false},"cell_type":"code","source":"test_mat = np.array(test)\ntest = scaler.fit_transform(test_mat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b007b4b6654c90db0ab84d8b2369d69b50c9752a","collapsed":true,"_cell_guid":"4a8b849d-482c-4550-9bbe-7125b3621e23","trusted":false},"cell_type":"code","source":"clf4 = SVC(kernel='rbf',C=25,gamma=.1).fit(X_train,Y_train)\nprediction = clf4.predict(test_mat)\npred = pd.DataFrame(prediction)\npred.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93a6d7562c2960d64ffdfe36da3ccbced09b7c3c","collapsed":true,"_cell_guid":"c2c43670-f196-4877-b4bb-d50e21c0fc39","trusted":false},"cell_type":"code","source":"prediction = svr2.predict(test_mat)\npred = pd.DataFrame(prediction)\npred.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c55062d7f85989948e7aace59e0b259ad93e5d01","_cell_guid":"f3f7a7ad-b79b-4a2b-9e5c-1d2b695931bc"},"cell_type":"markdown","source":"We have predicted the value of the test set that was provided to us in the data set and we can from the previous 2 blocks that our predictions are pretty accurate. Looks Good. !! Enjoy !! \nPost your comments for Discussio"},{"metadata":{"_uuid":"567087b80cdd838b1500860873b71979ffa38ddd","_cell_guid":"031f39c8-52cf-4fb7-ae49-1c004b60834c"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"b24a7fcab188d5d3a16159b63dcd386ae8cd3651","_cell_guid":"bebaa2f6-b9ea-497f-abdc-8d476aabc911"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"dd5026919004dfee96c7776c94de5a097a6fa36e","_cell_guid":"f114dd7e-e31b-4680-b0b2-12ea5e32b425"},"cell_type":"markdown","source":""}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}