{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Hi, welcome to my project! Today we will be classifying wine color based on its features using Decision Tree algorithms and then perform a regression in order to predict a continuous value. \n#### We will be using the wine quality data set for these exercises. This data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is Wine_Quality_Data.csv","metadata":{}},{"cell_type":"markdown","source":"### Let's import all libraries we will need at the beginning of the analysis","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import the dataset and examine the features, then look for null values and delete rows which contain them:","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfilepath = '../input/wine-quality/Wine_Quality_Data.csv'\ndata = pd.read_csv(filepath, sep=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.color.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.color.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see our dataset does not contain any null values in its fields, so we can continue with replacing or encoding our label column.","metadata":{}},{"cell_type":"markdown","source":"### Let's convert our color label to an integer, this is a quick way to do it using Pandas.","metadata":{}},{"cell_type":"markdown","source":"**White=0, Red=1** ","metadata":{}},{"cell_type":"code","source":"data['color'] = data['color'].replace('white',0).replace('red',1).astype(np.int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[:,-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[:,-1].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting our dataset:","metadata":{}},{"cell_type":"markdown","source":"### Now we have to split our data intro train and test sets, in this project we will use StratifiedShuffleSplit. If possible, preserve the indices of the split for later.","metadata":{}},{"cell_type":"code","source":"feature_cols = [x for x in data.columns if x not in 'color']\nfeature_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Split the data into two parts with 1000 points in the test data\n# This creates a generator\nstrat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1000, random_state=42)\n\n# Get the index values from the generator\ntrain_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data['color']))\n\n# Create the data sets\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'color']\n\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, 'color']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now check the percent composition of each quality level in the train and test data sets. The data set is mostly white wine, as can be seen below.","metadata":{}},{"cell_type":"code","source":"y_test.value_counts(normalize=True).sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts(normalize=True).sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Classifier:\nLet's define our model without setting limits on maximum depth, features, or leaves.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Determine how many nodes are present and the depth of this tree:","metadata":{}},{"cell_type":"code","source":"dt.tree_.node_count, dt.tree_.max_depth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.classes_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's define a function which can compute error metrics of our model:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef measure_error(y_true, y_pred, label):\n    return pd.Series({'accuracy':accuracy_score(y_true, y_pred),\n                      'precision': precision_score(y_true, y_pred),\n                      'recall': recall_score(y_true, y_pred),\n                      'f1': f1_score(y_true, y_pred)},\n                      name=label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predicting y with our model for x_train and x_test:","metadata":{}},{"cell_type":"code","source":"y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's use our function with its corresponding arguments:","metadata":{}},{"cell_type":"code","source":"a=measure_error(y_train, y_train_pred, 'train')\nb=measure_error(y_test, y_test_pred, 'test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c=pd.concat([a,b],axis=1)\nc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The decision tree predicts a little better on the training data than the test data, which is consistent with (mild) overfitting. Also notice the perfect recall score for the training data. In many instances, this prediction difference is even greater than that seen here.**","metadata":{}},{"cell_type":"markdown","source":"### More meaningful:","metadata":{}},{"cell_type":"code","source":"# The error on the training and test data sets\ny_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\n\ntrain_test_full_error = pd.concat([measure_error(y_train, y_train_pred, 'train'),\n                              measure_error(y_test, y_test_pred, 'test')],\n                              axis=1)\n\ntrain_test_full_error\n### END SOLUTION","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's use grid search with cross validation to find the best parameters of our decision tree. ","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth':range(1, dt.tree_.max_depth+1, 2),\n              'max_features': range(1, dt.n_features_+1)}\n\nGR = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  n_jobs=-1)\n\nGR = GR.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GR.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GR.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of nodes and the maximum depth of the tree:","metadata":{}},{"cell_type":"code","source":"GR.best_estimator_.tree_.node_count, GR.best_estimator_.tree_.max_depth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GR.classes_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's measure the errors on the train and test sets as before and compare them to those from the previous tree:","metadata":{}},{"cell_type":"code","source":"y_train_pred_gr = GR.predict(X_train)\ny_test_pred_gr = GR.predict(X_test)\n\ntrain_test_gr_error = pd.concat([measure_error(y_train, y_train_pred_gr, 'train'),\n                                 measure_error(y_test, y_test_pred_gr, 'test')],\n                                axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_gr_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These test metrics are a little better than the previous ones. So it would seem like the previous example overfit the data, but only slightly.","metadata":{}},{"cell_type":"markdown","source":"## Confusion matrix for both training and testing datasets: ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(y_train,y_train_pred_gr, labels=GR.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=GR.classes_)\ndisp.plot(cmap='Blues')\nplt.title('Confusion matrix for training dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm=confusion_matrix(y_test,y_test_pred_gr, labels=GR.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=GR.classes_)\ndisp.plot(cmap='Blues')\nplt.title('Confusion matrix for testing dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree Regressor:","metadata":{}},{"cell_type":"markdown","source":"In this part of the project we will develop a DTR model which can help us predict a continuous label, in this case we will deal with residual sugar being our label.","metadata":{}},{"cell_type":"code","source":"### BEGIN SOLUTION\nfeature_cols = [x for x in data.columns if x != 'residual_sugar']\n\n# Create the data sets\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'residual_sugar']\n\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, 'residual_sugar']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we can see that all of the feature columns are numerical type which is suit to use for regression algorithms.","metadata":{}},{"cell_type":"code","source":"data[feature_cols].dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndr = DecisionTreeRegressor().fit(X_train, y_train)\n\nparam_grid = {'max_depth':range(1, dr.tree_.max_depth+1, 2),\n              'max_features': range(1, dr.n_features_+1)}\n\nGR_sugar = GridSearchCV(DecisionTreeRegressor(random_state=42),\n                     param_grid=param_grid,\n                     scoring='neg_mean_squared_error',\n                      n_jobs=-1)\n\nGR_sugar = GR_sugar.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the best parameters found by the GridSeachCV:","metadata":{}},{"cell_type":"code","source":"GR_sugar.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of nodes and the maximum depth of the tree. This tree has lots of nodes, which is not so surprising given the continuous data.","metadata":{}},{"cell_type":"code","source":"GR_sugar.best_estimator_.tree_.node_count, GR_sugar.best_estimator_.tree_.max_depth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's compute error metrics on train and test data sets. Take into account that this case of study is continuous, so we will use mean squared error and coefficient of determination (r2 score):","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\ny_train_pred_gr_sugar = GR_sugar.predict(X_train)\ny_test_pred_gr_sugar  = GR_sugar.predict(X_test)\n\ntrain_test_gr_sugar_error = pd.Series({'train': mean_squared_error(y_train, y_train_pred_gr_sugar),\n                                         'test':  mean_squared_error(y_test, y_test_pred_gr_sugar)},\n                                          name='MSE').to_frame().T\n\ntrain_test_gr_sugar_r2 = pd.Series({'train': r2_score(y_train, y_train_pred_gr_sugar),\n                                         'test':  r2_score(y_test, y_test_pred_gr_sugar)},\n                                          name='R2 score').to_frame().T\n\npd.concat([train_test_gr_sugar_error, train_test_gr_sugar_r2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting of actual vs predicted residual sugar:","metadata":{}},{"cell_type":"markdown","source":"We could create a new dataframe with actual test label and predicted as columnn, then set the first one as index and use the plot tool:","metadata":{}},{"cell_type":"code","source":"ph_test_predict = pd.DataFrame({'test':y_test.values,\n                                'predict': y_test_pred_gr_sugar}).set_index('test').sort_index()\nph_test_predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context('notebook')\nsns.set_style('white')\nfig = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nph_test_predict = pd.DataFrame({'test':y_test.values,\n                                'predict': y_test_pred_gr_sugar}).set_index('test').sort_index()\n\nph_test_predict.plot(marker='o', ls='', ax=ax)\nax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or we could just use the scatter plot from matplotlib:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(6,6))\nax = plt.axes()\nax.scatter(y_test,y_test_pred_gr_sugar)\n\nax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In order to display the decision trees we built, we require an additional command line program (GraphViz) and Python library (PyDotPlus). GraphViz can be installed with a package manager on Linux and Mac. For PyDotPlus, either pip or conda (conda install -c conda-forge pydotplus) can be used to install the library.\n\n# Displaying decision trees: \n\n### First decision tree, where wine color was predicted and the number of features and/or splits are not limited.\n### Last decision tree, where wine color was predicted but a grid search was used to find the optimal depth and number of features.","metadata":{}},{"cell_type":"code","source":"!conda install -c conda-forge pydotplus -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from io import StringIO\nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz\nimport pydotplus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### BEGIN SOLUTION\n# Create an output destination for the file\ndot_data = StringIO()\n\nexport_graphviz(dt, out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n\n# View the tree image\nfilename = 'wine_tree.png'\ngraph.write_png(filename)\nImage(filename=filename) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an output destination for the file\ndot_data = StringIO()\n\nexport_graphviz(GR.best_estimator_, out_file=dot_data, filled=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n\n# View the tree image\nfilename = 'wine_tree_prune.png'\ngraph.write_png(filename)\nImage(filename=filename) \n### END SOLUTION","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}