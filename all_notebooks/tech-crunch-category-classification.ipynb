{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import pertinent libraries\nimport re\nimport pandas as pd # CSV file I/O (pd.read_csv)\nfrom nltk.corpus import stopwords\nimport numpy as np #linear algebra\nimport sklearn #sci-kit ML\nimport nltk #natural language processing\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score ,confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/techcrunch_posts.csv')\n#cleaning the data\ndf[\"content\"] = df[\"content\"].str.replace(\"\\\\n\", \" \")\n#dropping 'id', 'img_src', 'url'\ndf.drop(['id', 'img_src', 'url', 'date'], axis = 1, inplace =True)\ndf.dropna(subset=['authors'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78d1fad2271a11c60208827468b401af375f11de"},"cell_type":"code","source":"#create new column concatenating title and content \ndf['info'] = df[['title', 'content']].apply(lambda x: ' '.join(str(value) for value in x), axis=1)\n#remove content\ndf.drop(['content'], axis = 1, inplace =True)\n#remove articles in which there are no authors\n#display first row\npd.set_option('display.max_colwidth', -1)\ndf.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52d713b1bdd56d040161698f1624e6f678b00bab"},"cell_type":"code","source":"# pdf = df\n# pdf['category']=pdf['category'].astype('category').cat.codes\n# pdf['section']=pdf['section'].astype('category').cat.codes\n# pdf['authors']=pdf['authors'].astype('category').cat.codes\n# pdf.corr(method='pearson')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dde6442860746c988580a95bd93dafe6887113e"},"cell_type":"markdown","source":"<font size=\"5\">We note a poor correlation between category/section and category/authors.</font>\n"},{"metadata":{"trusted":true,"_uuid":"2f478403176ab21246c9e5ad281f9e16e9b65d37"},"cell_type":"code","source":"df.groupby(by=\"category\", sort = False).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b43b6d0eb25a7874f958d5431996f2f69811f2a"},"cell_type":"code","source":"newdf = df.groupby('category').filter(lambda x : len(x)>150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7739379397ebc5e4442f7e1b48037c78ca52f2a3"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(35,10))\nsns.countplot(x = \"category\", data = newdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6acf43302f4b341f41aa69f98726fe413db3f50c"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(12,12))\nnewdf['category'].value_counts().plot.pie( autopct = '%1.1f%%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47ff3efcf77b4ecd8c15055fdd72d41e7010887c"},"cell_type":"code","source":"total_authors = newdf.authors.nunique()\narticle_cnt = newdf.shape[0]\nprint('Total Number of authors : ', total_authors)\nprint('avg articles written by per author: ' + str(article_cnt//total_authors))\nprint('Total news counts : ' + str(article_cnt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8202ee38cd4c4e56c8680cfb11e402929af2fec"},"cell_type":"code","source":"authors_article_cnt = newdf.authors.value_counts()\nsum_articles = 0\nauthor_cnt = 0\nfor author_articles in authors_article_cnt:\n    author_cnt += 1\n    if author_articles < 80:\n        break\n    sum_articles += author_articles\nprint('{} authors write {} articles, so {} % of authors produce {} % of Tech Crunch articles'.\n      format(author_cnt, sum_articles, format((author_cnt*100/total_authors), '.2f'), format((sum_articles*100/article_cnt), '.2f')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8e74d843bdad44b48e3ac7671da0746d837233f"},"cell_type":"code","source":"newdf.authors.value_counts()[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48a9fe0997831fc7b1118c4a5f275f1c9256bb1e"},"cell_type":"code","source":"#Investigating the content of Tech Crunch's top author and filtering low category counts\nauthor_name = 'Natasha Lomas'\nauthor_articles_instance = newdf[newdf['authors'] == author_name]\nauthordf = author_articles_instance.groupby(by='category').filter(lambda x: len(x) > 10)\nauthordf = authordf.groupby(by='category').size()\nauthordf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35494dbfc7e53bbef01e896eeef31e7e50b961ac"},"cell_type":"code","source":"#Observing the most popular genres for Sarah Perez, we note that author likely corresponds to category\nfig, ax = plt.subplots(1, 1, figsize=(10,10))\nauthordf.plot.pie( autopct = '%1.1f%%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c731bd52acddc953bea93e165b627c8d088d808"},"cell_type":"code","source":"# Split the data into train and test.\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(newdf[['info', 'authors']], newdf['category'], test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98b6990557a0a7724f38bc768428f0e7a1ea05c3"},"cell_type":"code","source":"# Convert pandas series into numpy array\nX_train = np.array(X_train);\nX_test = np.array(X_test);\nY_train = np.array(Y_train);\nY_test = np.array(Y_test);\ncleanTitles_train = [] #To append processed titles\ncleanTitles_test = [] #To append processed titles\nnumber_reviews_train = len(X_train) #Calculating the number of reviews\nnumber_reviews_test = len(X_test) #Calculating the number of reviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd37c8ba256726dcbf290d597d4e95f3df4cf145"},"cell_type":"code","source":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nlemmetizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ndef get_words(titles_list):\n    titles = titles_list[0]   \n    author_names = [x for x in titles_list[1].lower().replace('and',',').replace(' ', '').split(',') if x != '']\n    titles_only_letters = re.sub('[^a-zA-Z]', ' ', titles)\n    words = nltk.word_tokenize(titles_only_letters.lower())\n    stops = set(stopwords.words('english'))\n    meaningful_words = [lemmetizer.lemmatize(w) for w in words if w not in stops]\n    return ' '.join(meaningful_words + author_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2cf94e096d5ed8538d1a6656fdb17aea09c5721"},"cell_type":"code","source":"#cleaning excess data in X_train\nfor i in range(0,1063):\n    np.delete(X_train, len(X_train)-i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99715f7bf4dcc0503ccd820b7a5d24a1840bea25"},"cell_type":"code","source":"for i in range(0,number_reviews_train):\n    cleanTitle = get_words(X_train[i]) #Processing the data and getting words with no special characters, numbers or html tags\n    cleanTitles_train.append( cleanTitle )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82be486a4e84288e44712f7ecf00b79c262dee2a"},"cell_type":"code","source":"for i in range(0,number_reviews_test):\n    cleanTitle = get_words(X_test[i]) #Processing the data and getting words with no special characters, numbers or html tags\n    cleanTitles_test.append( cleanTitle )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2901c3b060f77203efae836c26b2633aa3f8a64d"},"cell_type":"code","source":"vectorize = sklearn.feature_extraction.text.TfidfVectorizer(analyzer = \"word\", max_features=1000)\ntfidwords_train = vectorize.fit_transform(cleanTitles_train)\nX_train = tfidwords_train.toarray()\n\ntfidwords_test = vectorize.transform(cleanTitles_test)\nX_test = tfidwords_test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6382bff4a4051b424e57c09de1ee4ee0c28c472"},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\nmodel.fit(X_train,Y_train)\nY_predict = model.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d03d053d476f2c6786faeda71ea85f0c111d7fe4"},"cell_type":"code","source":"logistic_Regression = LogisticRegression()\nlogistic_Regression.fit(X_train,Y_train)\nY_predict = logistic_Regression.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe56d65e8e89fa2349a9290a72d2edeec26023af"},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel = BaggingClassifier(random_state=0, n_estimators=10)\nmodel.fit(X_train, Y_train)\nprediction = model.predict(X_test)\nprint('Accuracy of bagged KNN is :', accuracy_score(prediction, Y_test)*100, '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72d25cf0e5b384699a7fb1e4a4dbe1616cdffdc2"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB(alpha=0.1)\nmodel.fit(X_train,Y_train)\nY_predict = model.predict(X_test)\naccuracy = accuracy_score(Y_test,Y_predict)*100\nprint(format(accuracy, '.2f'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a2d102f06c80135a0e89288575d5627d0cdd979"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}