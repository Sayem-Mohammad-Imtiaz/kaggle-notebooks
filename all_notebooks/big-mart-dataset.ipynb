{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We import all the necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install feature-engine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom feature_engine.categorical_encoders import RareLabelCategoricalEncoder,OneHotCategoricalEncoder","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/big-mart-sales-dataset/Train_UWu5bXk.csv')\ntest=pd.read_csv('/kaggle/input/big-mart-sales-dataset/Test_u94Q5KV.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the above data we see that minimum visibility is 0 which cannot be as ever product have visibility in the store"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the above data we see that minimum visibility is 0 which cannot be as every product have visibility in the store"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_variables=train.select_dtypes(include='object')\n\ncat_variables.drop(columns='Item_Identifier',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot(x,y,**kwargs):\n    sns.boxplot(x=x,y=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We plot the box plot for all the categorical columns except for Item Identifier as it has high sparsity it needs cleaning before we can plot\nf=pd.melt(train,id_vars='Item_Outlet_Sales',value_vars=cat_variables)\n\ng=sns.FacetGrid(f,col='variable',sharey=True,col_wrap=3,height=2,sharex=False,size=5)\n\ng=g.map(boxplot,'value','Item_Outlet_Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot we cannot see much difference in the salesprice with respect to the variables except for Outlet Identifier and Outlet type which show a slight difference"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot does not show any relationship between each other"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.barplot(x='Outlet_Size',y='Item_Outlet_Sales',hue='Outlet_Type',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot we understand that outlet size does not have much affect on Item outlet sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.barplot(x='Outlet_Establishment_Year',y='Item_Outlet_Sales',hue='Outlet_Type',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also from the above plot we can see that Item outlet sales does not depend on the year of establishment"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nsns.barplot(x='Outlet_Location_Type',y='Item_Outlet_Sales',hue='Outlet_Type',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot we understand that outlet location has a significant impact on outlet sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We split the item identifier so as to classify the product much easily\ntrain['Item_Classification']=train['Item_Identifier'].str[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same in the test set as well\ntest['Item_Classification']=test['Item_Identifier'].str[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create the x and y values for the train set\nX=train\n\nX=X.drop(columns='Item_Outlet_Sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train['Item_Outlet_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create the train and test set\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=X_train['Outlet_Location_Type'],columns=X_train['Outlet_Type'],values=X_train['Outlet_Size'],aggfunc=pd.Series.mode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the size of grocery store and supermarket1 is small and the other two is medium"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We fill the outlet size with the information we got from the above table, where grocery store and supermarket type1 have size as small \nX_train.loc[X_train['Outlet_Type'] == 'Grocery Store','Outlet_Size'] = X_train.loc[X_train['Outlet_Type'] == 'Grocery Store', 'Outlet_Size'].fillna('Small')\n\nX_train.loc[X_train['Outlet_Type'] == 'Supermarket Type1','Outlet_Size'] = X_train.loc[X_train['Outlet_Type'] == 'Supermarket Type1', 'Outlet_Size'].fillna('Small')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We fill the X_test values by the same method\nX_test.loc[X_test['Outlet_Type'] == 'Grocery Store','Outlet_Size'] = X_test.loc[X_test['Outlet_Type'] == 'Grocery Store', 'Outlet_Size'].fillna('Small')\n\nX_test.loc[X_test['Outlet_Type'] == 'Supermarket Type1','Outlet_Size'] = X_test.loc[X_test['Outlet_Type'] == 'Supermarket Type1', 'Outlet_Size'].fillna('Small')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same operation on the test set\n\ntest.loc[test['Outlet_Type'] == 'Grocery Store','Outlet_Size'] = test.loc[test['Outlet_Type'] == 'Grocery Store', 'Outlet_Size'].fillna('Small')\n\ntest.loc[test['Outlet_Type'] == 'Supermarket Type1','Outlet_Size'] = test.loc[test['Outlet_Type'] == 'Supermarket Type1', 'Outlet_Size'].fillna('Small')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We groupby the weight column with respect to item type and item fat content by which we can weight of each product \nX_train['Item_Weight']=X_train.groupby(['Item_Fat_Content','Item_Type'])['Item_Weight'].apply(lambda x :x.fillna(x.mean()))\n\nX_test['Item_Weight']=X_test.groupby(['Item_Fat_Content','Item_Type'])['Item_Weight'].apply(lambda x :x.fillna(x.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['Item_Weight']=X_test['Item_Weight'].fillna(test['Item_Weight'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We performt the same for the test dataset \n\ntest['Item_Weight']=test.groupby(['Item_Fat_Content','Item_Type'])['Item_Weight'].apply(lambda x :x.fillna(x.mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are still missing values in the test set, which we have to fill but as the missing values are little we will use mean. Which will not cause much distortion."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we perform the mean imputation to fill the final missing value\ntest['Item_Weight']=test['Item_Weight'].fillna(test['Item_Weight'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We the values 0 in item visibilty with 25th quantile value with the assumption that they have lowest visibilty\nX_train.loc[X_train['Item_Visibility']==0,'Item_Visibility']=np.quantile(train['Item_Visibility'],0.25)\n\nX_test.loc[X_test['Item_Visibility']==0,'Item_Visibility']=np.quantile(train['Item_Visibility'],0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same in the test dataset\n\ntest.loc[test['Item_Visibility']==0,'Item_Visibility']=np.quantile(train['Item_Visibility'],0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create a new type of item fat content with respect to item classification which is non-consumable\nX_train.loc[X_train['Item_Classification']=='NC','Item_Fat_Content']='Non Consumable'\n\nX_test.loc[X_test['Item_Classification']=='NC','Item_Fat_Content']='Non Consumable'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same procedure the on the test set\ntest.loc[test['Item_Classification']=='NC','Item_Fat_Content']='Non Consumable'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Item_Fat_Content'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see from the column that there are mulitple type with similar meaning, which we replace\nX_train['Item_Fat_Content']=X_train['Item_Fat_Content'].replace({'LF':'Low Fat','low fat':'Low Fat','reg':'Regular'})\n\nX_test['Item_Fat_Content']=X_test['Item_Fat_Content'].replace({'LF':'Low Fat','low fat':'Low Fat','reg':'Regular'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same in the test set\n\ntest['Item_Fat_Content']=test['Item_Fat_Content'].replace({'LF':'Low Fat','low fat':'Low Fat','reg':'Regular'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We create a list of categorical varibles to understand the sparsity\ncat_variables=list(X_train.select_dtypes(include='object'))\n\ncat_variables.remove('Item_Identifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We plot the graph to find the values with low percentage\ncount=1\nplt.figure(figsize=(20,10))\nfor col in cat_variables:\n    \n    temp=pd.Series(X_train[col].value_counts()/len(X_train))\n    \n    # make plot with the above percentages\n    plt.subplot(3,3,count)\n    fig = temp.sort_values(ascending=False).plot.bar()\n    fig.set_xlabel(col)\n\n    # add a line at 5 % to flag the threshold for rare categories\n    fig.axhline(y=0.05, color='red')\n    fig.set_ylabel('Percentage Count')\n    count+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We enable rare label for the outlet type\nrc=RareLabelCategoricalEncoder(tol=0.05,n_categories=10,variables=['Item_Type'])\n\nrc.fit(X_train)\n\nX_train=rc.transform(X_train)\n\nX_test=rc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same in the test set \n\ntest=rc.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We change the astype of establishment year to string\nX_train['Outlet_Establishment_Year']=X_train['Outlet_Establishment_Year'].astype('str')\n\nX_test['Outlet_Establishment_Year']=X_test['Outlet_Establishment_Year'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same in the test set\n\ntest['Outlet_Establishment_Year']=test['Outlet_Establishment_Year'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We drop the item identifier column from the dataset\nX_train=X_train.drop(columns='Item_Identifier')\n\nX_test=X_test.drop(columns='Item_Identifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same the in the test dataset\ntest=test.drop(columns='Item_Identifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(seed=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform one hot categorical encoding\nohce=OneHotCategoricalEncoder(drop_last=True)\n\nohce.fit(X_train)\n\nX_train=ohce.transform(X_train)\n\nX_test=ohce.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We do the same with the test set\n\ntest=ohce.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We scale the values in the dataset\n\ncol=list(X_train.columns)\n\nsc=StandardScaler()\n\nsc.fit(X_train)\n\nX_train=pd.DataFrame(sc.transform(X_train),columns=col)\n\nX_test=pd.DataFrame(sc.transform(X_test),columns=col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the same in the dataset\n\ntest=pd.DataFrame(sc.transform(test),columns=col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the linear regression on the model\n\nregressor_lc=LinearRegression()\n\nregressor_lc.fit(X_train,y_train)\n\ny_pred_lc=regressor_lc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We check the accuracy of the model\n\nmse=mean_squared_error(y_pred_lc,y_test)\n\nr2=r2_score(y_pred_lc,y_test)\n\nprint('The Mean Squared error is {}\\nThe r2 Score is {}'.format(np.sqrt(mse),r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the randomforest regression \nregressor_rf=RandomForestRegressor(random_state=0)\n\nregressor_rf.fit(X_train,y_train)\n\ny_pred_rf=regressor_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We check the accuracy of the model\n\nmse=mean_squared_error(y_pred_rf,y_test)\n\nr2=r2_score(y_pred_rf,y_test)\n\nprint('The Mean Squared error is {}\\nThe r2 Score is {}'.format(np.sqrt(mse),r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perfom the gradient boosting regressor\n\nregressor_gb=GradientBoostingRegressor(random_state=0)\n\nregressor_gb.fit(X_train,y_train)\n\ny_pred_gb=regressor_gb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We check the accuracy of the model\n\nmse=mean_squared_error(y_pred_gb,y_test)\n\nr2=r2_score(y_pred_gb,y_test)\n\nprint('The Mean Squared error is {}\\nThe r2 Score is {}'.format(np.sqrt(mse),r2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We perform the Extra Tree Regression\n\nregressor_er=ExtraTreesRegressor()\n\nregressor_er.fit(X_train,y_train)\n\ny_pred_er=regressor_er.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We check the accuracy of the model\n\nmse=mean_squared_error(y_pred_er,y_test)\n\nr2=r2_score(y_pred_er,y_test)\n\nprint('The Mean Squared error is {}\\nThe r2 Score is {}'.format(np.sqrt(mse),r2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After running all the model we got gradient boosting with highest accuracy of mse as 1092.5224396901426 with an r2 score of 0.31296265607023954. \nwhich is the best model out of all the others."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}