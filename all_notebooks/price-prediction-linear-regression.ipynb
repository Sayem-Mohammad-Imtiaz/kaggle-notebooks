{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Python Final Project : Predicting the costs of used cars\n\n## Done By: Sukhmani Kaur Bedi\n\n## Task:\n\n`Explore and visualize the dataset`\n\n`Build a linear regression model to predict the prices of used cars`\n\n`Generate a set of insights and recommendations that will help the business`","metadata":{}},{"cell_type":"markdown","source":"## Step1- Pre-processing the data\n\nIn order to get descriptive statistics about the data and start getting a sense of the distributions and the relationship of these variables, cleaning of the raw data is very improtant. \n\n#### Conversion required for below features\n\n- Mileage - Its continuous, need to remove kmpl, convert to float64\n-  Engine -Its continuous, need to remove CC, convert to int64\n-  Power - Its continuous, need to remove bhp, convert to float64\n-  Seats - Its numerical and discrete, convert to int64\n-  New_Price - Its numericial and continuous; convert to float64 and remove lakh or Cr. and convert 1Cr = 100Lakh\n\n#### Grouping categorical columns \n- Extracted Brand names from the Names column and created a new column Car_Brand \n- Classification of the Car_Brands into Luxury, medium and non Luxury brands on the basis of Premium brand names \n- Created a New column Region: Created a dictionary to map the cities to North/East/West/South.\n\n#### Computing Missing values for:\n\n- Mileage\n- Engine \n- Power\n- Seats\n- New_Price\n- Price\n\n\n## Step 2:- Visulaizing the data\n- Univariate Analysis\n- Bivariate and Multi-Variate Analysis\n- Checking for Outliers\n\n\n## Step 3 :- Transformation\n- Transforming highly skewed variables\n\n\n\n## Step4 :- One hot encoding\n- Creating dummy variables for categorical data \n\n\n## Step 5:-  Model Building\n- Creating Linear regression model \n- Adding Stats on model\n- Checking the accuracy \n","metadata":{}},{"cell_type":"markdown","source":"#### FEATURES:\n\nName: The brand and model of the car.\n\nLocation: The location in which the car is being sold or is available for purchase.\n\nYear: The year or edition of the model.\n\nKilometers_Driven: The total kilometres driven in the car by the previous owner(s) in KM.\n\nFuel_Type: The type of fuel used by the car.\n\nTransmission: The type of transmission used by the car.\n\nOwner_Type: Whether the ownership is Firsthand, Second hand or other.\n\nMileage: The standard mileage offered by the car company in kmpl or km/kg\n\nEngine: The displacement volume of the engine in cc.\n\nPower: The maximum power of the engine in bhp.\n\nSeats: The number of seats in the car.\n\nNew_Price: The price of a new car of the same model in Lakhs\n\nPrice: The price of the used car in INR Lakhs.\n","metadata":{}},{"cell_type":"markdown","source":"## Notes:\n- There is only one dataset provided. I will be splittig the data set into Train and Test to build the linear regression model to predict the prices of `used_cars`.\n- The `dependent variable` is `Price` which indicates the the price of used cars in INR Lakhs and the rest of the variables are considered to be independent variables.","metadata":{}},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport pylab\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n# Removes the limit from the number of displayed columns and rows.\n# This is so I can see the entire dataframe when I print it\npd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\npd.set_option('display.max_rows', 300)\n\n# To enable plotting graphs in Jupyter notebook\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and explore the data","metadata":{}},{"cell_type":"code","source":"! ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_car = pd.read_csv('../input/cars-data/used_cars_data1.csv') # reading the data \ndata= data_car.copy() # copying the data set\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Overview of the data","metadata":{}},{"cell_type":"code","source":"# Shape of the data \nprint(f'There are {data.shape[0]} rows and {data.shape[1]} columns.')  # f-string\n\n# I'm now going to look at 10 random rows\n# I'm setting the random seed via np.random.seed so that\n# I get the same random results every time\nnp.random.seed(1)\ndata.sample(n=10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:-\n\n1. There are 7253 rows and 14 attributes. \n    \n2. The data needs to be modified for example by removing extra `CC`in `Engine`, `bhp`in `Power` and `Lakh`in `New_Price`.\n    \n  ","metadata":{}},{"cell_type":"code","source":"data.info() #checking the info of the data","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum().sort_values(ascending =False) # Summing up the null values and sorting them in descending order","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:- \n    \n1. There are null values in `Mileage`, `Engine`, `Power`, `Seats`, `New_Price` and `Price`\n\n2. The data types must be fixed. For example `Mileage`, `Engine`, `New_Price` and `Power`. ","metadata":{}},{"cell_type":"code","source":"#Checking if there are any duplicate values in the data set \ndata.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are duplicate values in the data set.","metadata":{}},{"cell_type":"markdown","source":"### Step1- Pre-processing the data","metadata":{}},{"cell_type":"markdown","source":"#### 1. Mileage\n\nWe have car mileage in two units, kmpl and km/kg.\n\nAfter a quick research on the internet it is clear that these 2 units are used for cars of 2 different fuel types.\n\nkmpl - kilometers per litre - is used for petrol and diesel cars.\nkm/kg - kilometers per kg - is used for CNG and LPG based engines.\n\nWe have the variable `Fuel_type` in our data. Let us check if this observations holds true in our data also.","metadata":{}},{"cell_type":"code","source":"# Create 2 new columns after splitting the mileage values.\nkm_per_unit_fuel = []\nmileage_unit = []\n\nfor observation in data[\"Mileage\"]:\n    if isinstance(observation, str):\n        if (\n            observation.split(\" \")[0]\n            .replace(\".\", \"\", 1)\n            .isdigit()  # first element should be numeric\n            and \" \" in observation  # space between numeric and unit\n            and (\n                observation.split(\" \")[1]\n                == \"kmpl\"  # units are limited to \"kmpl\" and \"km/kg\"\n                or observation.split(\" \")[1] == \"km/kg\"\n            )\n        ):\n            km_per_unit_fuel.append(float(observation.split(\" \")[0]))\n            mileage_unit.append(observation.split(\" \")[1])\n        else:\n            # To detect if there are any observations in the column that do not follow\n            # the expected format [number + ' ' + 'kmpl' or 'km/kg']\n            print(\n                \"The data needs further processing. All values are not similar \",\n                observation,\n            )\n    else:\n        # If there are any missing values in the mileage column,\n        # we add corresponding missing values to the 2 new columns\n        km_per_unit_fuel.append(np.nan)\n        mileage_unit.append(np.nan)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No print output from the function above. The values are all in the expected format or NaNs\n# Add the new columns to the data\n\ndata[\"km_per_unit_fuel\"] = km_per_unit_fuel\ndata[\"mileage_unit\"] = mileage_unit\n\n# Checking the new dataframe\ndata.head(5)  # looks good!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us check if the units correspond to the fuel types as expected.\ndata.groupby(by=[\"Fuel_Type\", \"mileage_unit\"]).size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data type and the units of `Mileage`has been fixed.\n\nAs expected, km/kg is for CNG/LPG cars and kmpl is for Petrol and Diesel cars.","metadata":{}},{"cell_type":"markdown","source":"#### 2. Engine \n\nThe data dictionary suggests that `Engine` indicates the displacement volume of the engine in CC.\nWe will make sure that all the observations follow the same format - [numeric + \" \" + \"CC\"] and create a new numeric column from this column. \n\nThis time, lets use a regrex to make all the neccesary checks.","metadata":{}},{"cell_type":"code","source":"# re module provides support for regular expressions\nimport re\n\n# Create a new column after splitting the engine values.\nengine_num = []\n\n# Regex for numeric + \" \" + \"CC\"  format\nregex_engine = \"^\\d+(\\.\\d+)? CC$\"\n\nfor observation in data[\"Engine\"]:\n    if isinstance(observation, str):\n        if re.match(regex_engine, observation):\n            engine_num.append(float(observation.split(\" \")[0]))\n        else:\n            # To detect if there are any observations in the column that do not follow [numeric + \" \" + \"CC\"]  format\n            print(\n                \"The data needs furthur processing. All values are not similar \",\n                observation,\n            )\n    else:\n        # If there are any missing values in the engine column, we add missing values to the new column\n        engine_num.append(np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No print output from the function above. The values are all in the same format - [numeric + \" \" + \"CC\"] OR NaNs\n# Add the new column to the data\n\ndata[\"engine_num\"] = engine_num\n\n# Checking the new dataframe\ndata.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Power \n\nThe data dictionary suggests that `Power` indicates the maximum power of the engine in bhp.\nWe will make sure that all the observations follow the same format - [numeric + \" \" + \"bhp\"] and create a new numeric column from this column, like we did for `Engine`","metadata":{}},{"cell_type":"code","source":"# Create a new column after splitting the power values.\npower_num = []\n\n# Regex for numeric + \" \" + \"bhp\"  format\nregex_power = \"^\\d+(\\.\\d+)? bhp$\"\n\nfor observation in data[\"Power\"]:\n    if isinstance(observation, str):\n        if re.match(regex_power, observation):\n            power_num.append(float(observation.split(\" \")[0]))\n        else:\n            # To detect if there are any observations in the column that do not follow [numeric + \" \" + \"bhp\"]  format\n            # that we see in the sample output\n            print(\n                \"The data needs furthur processing. All values are not similar \",\n                observation,\n            )\n    else:\n        # If there are any missing values in the power column, we add missing values to the new column\n        power_num.append(np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that some Null values in power column exist as 'null bhp' string.\nLet us replace these with NaNs","metadata":{}},{"cell_type":"code","source":"power_num = []\n\nfor observation in data[\"Power\"]:\n    if isinstance(observation, str):\n        if re.match(regex_power, observation):\n            power_num.append(float(observation.split(\" \")[0]))\n        else:\n            power_num.append(np.nan)\n    else:\n        # If there are any missing values in the power column, we add missing values to the new column\n        power_num.append(np.nan)\n\n# Add the new column to the data\ndata[\"power_num\"] = power_num\n\n# Checking the new dataframe\ndata.head(10)  # looks good now","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. New_Price \n\nWe know that `New_Price` is the price of a new car of the same model in INR Lakhs.(1 Lakh = 100, 000)\n\nThis column clearly has a lot of missing values. We will impute the missing values later. For now we will only extract the numeric values from this column.","metadata":{}},{"cell_type":"code","source":"# Create a new column after splitting the New_Price values.\nnew_price_num = []\n\n# Regex for numeric + \" \" + \"Lakh\"  format\nregex_power = \"^\\d+(\\.\\d+)? Lakh$\"\n\nfor observation in data[\"New_Price\"]:\n    if isinstance(observation, str):\n        if re.match(regex_power, observation):\n            new_price_num.append(float(observation.split(\" \")[0]))\n        else:\n            # To detect if there are any observations in the column that do not follow [numeric + \" \" + \"Lakh\"]  format\n            # that we see in the sample output\n            print(\n                \"The data needs furthur processing. All values are not similar \",\n                observation,\n            )\n    else:\n        # If there are any missing values in the New_Price column, we add missing values to the new column\n        new_price_num.append(np.nan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not all values are in Lakhs. There are a few observations that are in Crores as well\n\nLet us convert these to lakhs. 1 Cr = 100 Lakhs","metadata":{}},{"cell_type":"code","source":"new_price_num = []\n\nfor observation in data[\"New_Price\"]:\n    if isinstance(observation, str):\n        if re.match(regex_power, observation):\n            new_price_num.append(float(observation.split(\" \")[0]))\n        else:\n            # Converting values in Crore to lakhs\n            new_price_num.append(float(observation.split(\" \")[0]) * 100)\n    else:\n        # If there are any missing values in the New_Price column, we add missing values to the new column\n        new_price_num.append(np.nan)\n\n# Add the new column to the data\ndata[\"new_price_num\"] = new_price_num\n\n# Checking the new dataframe\ndata.head(5)  # Looks ok","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing for variable columns","metadata":{}},{"cell_type":"markdown","source":"The `Name` column in the current format might not be very useful in our analysis.\nSince the name contains both the brand name and the model name of the vehicle, the column would have to many unique values to be useful in prediction.\n\n\nCreating a new column as the `Car_Brand` which tells the brand of the car and `Name` as the model of the car.","metadata":{}},{"cell_type":"markdown","source":"#### 1. Car Brand Name","metadata":{}},{"cell_type":"code","source":"brand = []\nfor row, sentence in enumerate(data['Name']):\n    if sentence.split(' ')[0] == 'Land':\n        brand.append(' '.join(sentence.split(' ')[0:2]))\n    elif sentence.split(' ')[0] == 'OpelCorsa':\n        brand.append('Opel')\n    elif sentence.split(' ')[0] == 'ISUZU':\n        brand.append('Isuzu')\n    else:\n        brand.append(sentence.split(' ')[0])\ndata['Car_Brand'] = brand ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The number of unique values of Car_Brand', data['Car_Brand'].nunique())\nprint(data['Car_Brand'].value_counts())","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nsns.countplot(y=\"Car_Brand\", data=data, order=data['Car_Brand'].value_counts().index);\nplt.title('Car Brand')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Car Model Name","metadata":{}},{"cell_type":"code","source":"# Extract Model Names\ndata[\"Model\"] = data[\"Name\"].apply(lambda x: x.split(\" \")[1].lower())\n\n# Check the data\n\ndata[\"Model\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nsns.countplot(y=\"Model\", data=data, order=data[\"Model\"].value_counts().index[1:30])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear from the above charts that out dataset contains used cars from luxury as well as budget friendly brands.\n\nWe can create a new variable using this information. We will bin all our cars in 3 categories -\n\n1. Non Luxury\n2. Mid Range\n3. Luxury Cars","metadata":{}},{"cell_type":"markdown","source":"#### 3. Car_category","metadata":{}},{"cell_type":"code","source":"data.groupby([\"Car_Brand\"])[\"Price\"].mean().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output is very close to our expectation (domain knowledge), in terms of brand ordering. Mean price of a used Lamborghini is 120 Lakhs and that of cars from other luxury brands follow in a descending order.\n\nTowards the bottom end we have the more budget friendly brands.\n\nWe can see that there is some missingness in our data. Let us come back to creating this variable once we have removed missingness from the data.","metadata":{}},{"cell_type":"markdown","source":"### Missing value Treatment","metadata":{}},{"cell_type":"code","source":"# Basic summary stats - Numeric variables\ndata.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n1. S.No. clearly has no interpretation here but as discussed earlier let us drop it only after having looked at the initial linear model.\n2. Kilometers_Driven values have an incredibly high range. We should check a few of the extreme values to get a sense of the data.\n3. Minimum and maximum number of seats in the car also warrent a quick check. On an average a car seems to have 5 seats, which is about right.\n4. We have used cars being sold at less than a lakh rupees and as high as 160 lakh, as we saw for Lamborghini earlier. We might have to drop some of these outliers to build a robust model.\n5. Min Mileage being 0 is also concerning, we'll have to check what is going on.\n6. Engine and Power mean and median values are not very different. Only someone with more domain knowledge would be able to comment furthur on these attributes.\n7. New price range seems right. We have both budget friendly Maruti cars and Lamborghinis in our stock. Mean being twice that of the median suggests that there are only a few very high range brands, which again makes sense.","metadata":{}},{"cell_type":"code","source":"# Check Kilometers_Driven extreme values\ndata.sort_values(by=[\"Kilometers_Driven\"], ascending=False).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the first row here is a data entry error. A car manufactured as recently as 2017 having been driven 6500000 kms is almost impossible.\n\nThe other observations that follow are also on a higher end. There is a good chance that these are outliers. We'll look at this furthur while doing the univariate analysis.","metadata":{}},{"cell_type":"code","source":"# Check Kilometers_Driven Extreme values\ndata.sort_values(by=[\"Kilometers_Driven\"], ascending=True).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After looking at the columns - Year, New Price and Price these entries seem feasible.\n\n1000 might be default value in this case. Quite a few cars having driven exactly 1000 km is suspicious.","metadata":{}},{"cell_type":"code","source":"# Check seats extreme values\ndata.sort_values(by=[\"Seats\"], ascending=True).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Audi A4 having 0 seats is clearly a data entry error. This column warrents some outlier treatment or we can treat seats == 0 as a missing value. Overall, there doesn't seem not much to be concerned about here. ","metadata":{}},{"cell_type":"code","source":"# Let us check if we have a similar car in our dataset.\ndata[data[\"Name\"].str.startswith(\"Audi A4\")]\n# Looks like an Audi A4 typically has 5 seats.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us replace #seats in ro\n\n# Let us replace #seats in row index 3999 form 0 to 5\n\ndata.loc[3999, \"Seats\"] = 5.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check seats extreme values\ndata.sort_values(by=[\"Seats\"], ascending=False).head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course, a Toyota Qualis has 10 seats and so does a Tata Sumo. We don't see any data entry error here.","metadata":{}},{"cell_type":"code","source":"# Check Mileage - km_per_unit_fuel extreme values\ndata.sort_values(by=[\"km_per_unit_fuel\"], ascending=True).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will have to treat Mileage = 0 as missing values","metadata":{}},{"cell_type":"code","source":"# Check Mileage - km_per_unit_fuel extreme values\ndata.sort_values(by=[\"km_per_unit_fuel\"], ascending=False).head(10)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Maruti Wagon R and Maruti Alto CNG versions are budget friendly cars with high mileage so these data points are fine.","metadata":{}},{"cell_type":"code","source":"# looking at value counts for non-numeric features\n\nnum_to_display = 10  # defining this up here so it's easy to change later\nfor colname in data.dtypes[data.dtypes == \"object\"].index:\n    val_counts = data[colname].value_counts(dropna=False)  # Will also show the NA counts\n    print(val_counts[:num_to_display])\n    if len(val_counts) > num_to_display:\n        print(f\"Only displaying first {num_to_display} of {len(val_counts)} values.\")\n    print(\"\\n\\n\")  # just for more space in between","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we haven't dropped the original columns that we processed, we have a few redunadant output here.\n\nWe had checked cars of different `Fuel_Type` earlier, but we did not encounter the 2 electric cars. Let us check why.","metadata":{}},{"cell_type":"code","source":"data.loc[data[\"Fuel_Type\"] == \"Electric\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mileage values for these cars are NaN, that is why we did not encounter these earlier with groupby.\n\nElectric cars are very new in the market and very rare in our dataset. We can consider dropping these two observations if they turn out to be outliers later. There is a good chance that we will not be able to create a good price prediction model for electric cars, with the currently available data.","metadata":{}},{"cell_type":"markdown","source":"New Price for 6247 entries is missing. We need to explore if we can impute these or we should drop this column altogether.","metadata":{}},{"cell_type":"markdown","source":"### Missing Value Treatment\n\n\nBefore we start looking at the individual distributions and interactions, let's quickly check the missingness in the data.","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 2 Electric car variants don't have entries for Mileage.\n* Engine displacement information of 46 observations is missing and maximum power of 175 entries is missing.\n* Information about number of seats is not avaliable for 53 entries.\n* New Price as we saw earlier has a huge missing count. We'll have to see if there is a pattern here.\n* Price is also missing for 1234 entries. Since price is our response variable that we want to predict, we will have to drop these rows when we actually build a model. These rows will not be able to help us in modelling or model evaluation. But while we are analysing the distributions and doing missing value imputations, we will keep using information from these rows.","metadata":{}},{"cell_type":"code","source":"# Drop the redundant columns.\ndata.drop(columns=[\"Mileage\", \"mileage_unit\", \"Engine\", \"Power\", \"New_Price\"], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at a few rows where #seats is missing\ndata[data[\"Seats\"].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll impute these missing values one by one, by taking median number of seats for the particular car,\n# using the Brand and Model name\ndata.groupby([\"Car_Brand\", \"Model\"], as_index=False)[\"Seats\"].median()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Impute missing Seats\ndata[\"Seats\"] = data.groupby([\"Car_Brand\", \"Model\"])[\"Seats\"].transform(lambda x: x.fillna(x.median()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check 'Seats'\ndata[data[\"Seats\"].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Maruti Estilo can accomodate 5\ndata[\"Seats\"] = data[\"Seats\"].fillna(5.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will use similar methods to fill missing values for engine, power and new price\ndata[\"engine_num\"] = data.groupby([\"Car_Brand\", \"Model\"])[\"engine_num\"].transform(lambda x: x.fillna(x.median()))\n\ndata[\"power_num\"] = data.groupby([\"Car_Brand\", \"Model\"])[\"power_num\"].transform(lambda x: x.fillna(x.median()))\n\ndata[\"new_price_num\"] = data.groupby([\"Car_Brand\", \"Model\"])[\"new_price_num\"].transform(lambda x: x.fillna(x.median()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are still some NAs in power and new_price_num.\n# There are a few car brands and models in our dataset that do not contain the new price information at all.\n# Now we'll have to estimate the new price using the other features.\n# KNN imputation is once of the imputation methods that can be used for this.\n# This sklearn method requires us to encode categorical variables, if we are using them for imputation.\n# In this case we'll use only a select numeric features for imputation\n\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=3, weights=\"uniform\")  # 3 Nearest Neighbours\ntemp_data_for_imputation = data[[\"engine_num\", \"power_num\", \"Year\", \"km_per_unit_fuel\", \"new_price_num\", \"Seats\"]]\ntemp_data_for_imputation = imputer.fit_transform(temp_data_for_imputation)\ntemp_data_for_imputation = pd.DataFrame(temp_data_for_imputation, \n                                      columns = [\"engine_num\", \"power_num\", \"Year\", \"km_per_unit_fuel\", \n                                                 \"new_price_num\", \"Seats\"],\n)\n\n# Add imputed columns to the original dataset\ndata[\"new_price_num\"] = temp_data_for_imputation[\"new_price_num\"]\ndata[\"power_num\"] = temp_data_for_imputation[\"power_num\"]\ndata[\"km_per_unit_fuel\"] = temp_data_for_imputation[\"km_per_unit_fuel\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the redundant columns.\ndata.drop(columns=[\"Name\", \"S.No.\"], inplace=True) \n# Drop the rows where 'Price' == NaN and proceed to modelling\ndata = data[data[\"Price\"].notna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'There are {data.shape[0]} rows and {data.shape[1]} columns after dropping NAN from the target variable')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the value counts and unique number of Car_Brand after dropping the missing values of Price\nprint('The number of unique values of Car_Brand', data['Car_Brand'].nunique())\nprint(data['Car_Brand'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now data for Hindustan and OpelCorsa has dropped. \n","metadata":{}},{"cell_type":"markdown","source":"Let us try to feature the Location column. \nSince the location in the data set is of India, we can further `group the states according to their region`. For Example, `Northern`,`Eastern` and so on. ","metadata":{}},{"cell_type":"code","source":"data['Location'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Grouping the states according to their region in India\nNothern = ['Delhi','Jaipur']\n\nWestern =['Mumbai', 'Pune','Ahmedabad' ]\n\nEastern = ['Kolkata' ]\n\nSouthern = ['Hyderabad', 'Kochi', 'Coimbatore', 'Chennai', 'Banglore']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Defining a user function for continents\ndef Regions(x):\n    if x in Nothern:\n        return 'North_India'\n    elif x in Western:\n        return 'West_India'\n    elif x in Eastern:\n        return 'East_India'\n    else:\n        return 'South_India'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Region'] = data['Location'].apply(Regions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let us look at unique regions\ndata['Region'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Descriptive Statistics ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.3f' % x) \nnp.random.seed(1)\ndata.sample(n=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.3f' % x)\ndata.describe().T # Descriptive statistics for Numerical columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :-**\n    \n1. The `average kilometer driven` by cars is `58738.380km` and `maximum kilometer driven` is `6500000km`.There is huge difference seen between the maximum and the minimum value of kilometeres driven which might be a sign of outliers or extreme values. \n\n2. The `average mileage` offered by the car comapny is `18.325` and the `maximum mileage` provided is `33.54`. Not much difference between the maximum and mininum values of mileage. \n\n3. The `average size` of the `engine` is `1621` and the `maximum size` of the `engine` is `5998`. There is huge difference seen between the maximum and the minimum value for the size of the engine which might be a sign of outliers or extreme values. \n\n4. The `average power` of the `engine` is `112.661` and the `maximum power` provided by the `engine` is `560`. There is huge difference seen between the maximum and the minimum value for power which might be a sign of outliers or extreme values. \n\n5. The `average price` of the `new car` is `1,875,734.175` and the `average price` of the `used cars` is `947,946`. There is huge difference seen for both of the columns between the maximum and the minimum value and might be a sign of outliers or extreme values. \n\n6. `50%` of the `cars` in the data set are of `2014 edition`. \n\n7. There are average cars in the data set is a 5 seater car.","metadata":{}},{"cell_type":"code","source":"data.describe(exclude = np.number).T # Descriptive statistics for Categorical columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations :-**\n\n\n1. Maximum number of cars sold or available for purchase is in Mumbai.\n\n2. There are 5 unique categories of Fuel type out of which Diesel has the most frequency.\n\n3. There are 2 unique categories of Transmission out of which Manual has the maximum frequenxy.\n\n4. There are 4 unique categories of Owner_Type out of which there are maximum First hand cars.\n\n5. There are 31 uniquie categories of Car_Brand out of which Maruti is the most popular brand.\n\n6. There are 4 unique regions (categoriezed according to the location). Maximum sale and purchase of car is done in South_India","metadata":{}},{"cell_type":"markdown","source":"### Step 2\n\n### Before we further process the data, let's have a look at the graphical visualization of the data to understand it in a better way!","metadata":{}},{"cell_type":"markdown","source":"### Univariate analysis","metadata":{}},{"cell_type":"code","source":"# Function to create barplots that indicate percentage for each category.\n\ndef perc_on_bar(plot, feature):\n    '''\n    plot\n    feature: categorical feature\n    the function won't work if a column is passed in hue parameter\n    '''\n    total = len(feature) # length of the column\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()/total) # percentage of each class of the category\n        x = p.get_x() + p.get_width() / 2 - 0.05 # width of the plot\n        y = p.get_y() + p.get_height()           # hieght of the plot\n        ax.annotate(percentage, (x, y), size = 10) # annotate the percantage \n    plt.show() # show the plot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nax = sns.countplot(data['Owner_Type'],palette='winter')\nplt.xticks(rotation=45)\nperc_on_bar(ax,data['Owner_Type'])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are maximum cars maximum First hand owners in the data set and very few with Third hand owners and above\n- A substantial drop can be seen between First hand, Second hand owners and Thrid hand owners. \n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nax = sns.countplot(data['Fuel_Type'],palette='winter')\nplt.xticks(rotation=45)\nperc_on_bar(ax,data['Fuel_Type'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are 53.2% Diesel cars, 45.6% Petrol cars and very few CNG and LPG cars. \n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nax = sns.countplot(data['Transmission'],palette='winter')\nplt.xticks(rotation=45)\nperc_on_bar(ax,data['Transmission'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are maximum Manual cars (71.4%)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nax = data.Location.value_counts().plot.bar()\nplt.xticks(rotation=60)\nperc_on_bar(ax,data['Location'])","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Maximum selling and buying of used cars is done in Mumbai and the least is done in Ahmedabad\n- Approximately equal split can be seen for Kochi, Coimbatore and Pune, and for Kolkata and Jaipur.\n ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,12))\nax = data.Car_Brand.value_counts().sort_values(ascending=False).plot.bar()\nplt.xticks(rotation=90)\nperc_on_bar(ax,data['Car_Brand'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are significantly more cars for Maruti in comparision with the others.\n- There is only a marginal drop percentage for Hundai in comparison with Maruti. However, there is a significant drop     percentage for Honda in comaprison with Maruti and Hundai.   \n- Percentage of cars with brand name Mercedes_Benz, Ford and Volkswagen are approximately same.\n","metadata":{}},{"cell_type":"markdown","source":"### Lets plot histogram for all numerical columns","metadata":{}},{"cell_type":"code","source":"# lets plot histogram of all plots\nfrom scipy.stats import norm\nall_col = data.select_dtypes(include = np.number).columns.tolist()\n\nplt.figure(figsize = (17, 75))\n\nfor i in range(len(all_col)):\n    plt.subplot(18, 3, i+1)\n    #plt.hist(data[all_col[i]])\n    sns.distplot(data[all_col[i]], kde=True)\n    plt.tight_layout()\n    plt.title(all_col[i],fontsize=20)\n    \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Year is skewed towards left which means that there are very few models of edition 2005 and less.\n- There are maximum cars of the edition 2014. There is approximately an equal split of cars with edition 2015 and 2016. \n- Each variable is skewed towards right except for `Mileage` and `Seats`\n- Mileage follows a noraml distribution with 18 as average mileage offered by the car company\n\nIn order to improve the skeweness these variables must be transformed.","metadata":{}},{"cell_type":"markdown","source":"## Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"### Lets look at correlations","metadata":{}},{"cell_type":"code","source":"numeric_columns = data.select_dtypes(include = np.number).columns.tolist()\n\n# sorting correlations w.r.t Price  \ncorr = data[numeric_columns].corr().sort_values(by = ['Price'], ascending = False) \n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (13, 10))\n\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap = 'seismic', annot = True, fmt = \".1f\", vmin = -1, vmax = 1, center = 0, square = False,\n            linewidths = .7, cbar_kws = {\"shrink\": .5});\nplt.title(' Correlation Matrix', fontstyle = 'italic')\nplt.tick_params(axis='both', labelsize = 12, labelrotation = 45)\nplt.show()\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data, corner = True )","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `Price` is highly positively correlated  with `Engine(0.7)`, `Power(0.8)` and `New_Price(0.6)`\n- Other highly positively correlated variables are:-\n    - `New_Price` with `Engine(0.6)` and `New_Price` with`Power(0.7)`\n    - A very strong correlation is seen between `Power`and `Engine (0.9)` \n- Other negatively correlated variables are:-\n    - `Mileage` with `Power-0.5)` and `Mileage` with `Engine (-0.6)`\n    \n    \n \nThe correlation between `Power and  Engine`, `Mileage and Power` and `Mileage and Engine` indicates a strong multicollinearity. ","metadata":{}},{"cell_type":"markdown","source":"## Let us look at the graph of those variables that are highly correlated with Price","metadata":{}},{"cell_type":"markdown","source":"### Price vs Engine vs Categorical Variables( with hue as Fuel_Type, Owner_Type and Transmission and Region)","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\ntitle_dict = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'bold',\n        'size': 16\n        }\n\nlab_dict = {'family': 'serif',\n              'color': 'black',\n              'size': 14\n              }\n\nplt.figure(figsize=[20,15])\ngrid = plt.GridSpec(3, 2, wspace=0.6, hspace=0.4) #Defining the grid space\n\n#Plotting Price vs Engine vs Transmission\n\nax0 = plt.subplot(grid[0, 0])\nsns.scatterplot(y= 'Price', x = 'engine_num', hue = 'Transmission', data = data)\nax0.set_ylabel('Price', fontdict=lab_dict)\nax0.set_xlabel('Engine', fontdict=lab_dict)\nplt.title('Price vs Engine vs Transmission', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(0.99,1))\n\n#Plotting Price vs Engine vs Fuel_Type\nax1 = plt.subplot(grid[0, 1])\nsns.scatterplot(y= 'Price', x = 'engine_num', hue = 'Fuel_Type', data = data)\nax1.set_ylabel('Price', fontdict=lab_dict)\nax1.set_xlabel('Engine', fontdict=lab_dict)\nplt.title('Price vs Engine vs Fuel_Type', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(0.99,1))\n\n#Plotting Price vs Engine vs Owner_Type\nax2 = plt.subplot(grid[1, 0])\nsns.scatterplot(y= 'Price', x = 'engine_num', hue = 'Owner_Type', data = data)\nax2.set_ylabel('Price', fontdict=lab_dict)\nax2.set_xlabel('Engine', fontdict=lab_dict)\nplt.title('Price vs Engine vs Owner_Type', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(0.99,1))\n\n\n#Plotting Price vs Engine vs Region\nax3 = plt.subplot(grid[1, 1])\nsns.scatterplot(y= 'Price', x = 'engine_num', hue = 'Region', data = data)\nax3.set_ylabel('Price', fontdict=lab_dict)\nax3.set_xlabel('Engine', fontdict=lab_dict)\nplt.title('Price vs Engine vs Region', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(0.99,1))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are more expensive Automatic cars with bigger engine size in comparison with Manual cars. \n- There is one Automatic car of INR 160 Lakh and engine size 3000 and one Automatic car with engine size 6000 and price above 40 Lakh  (Will have a look at it). Also, one car with very small engine size and price less than 20Lakh can be seen. (Shall look at it)\n\n\n- Diesel cars with engine size less than or equal to 3000 are more expensive than the others. However as the engine size increases after 3000, the price of Petrol cars also increases. \n- No CNG and LPG cars can be seen. \n- One Electric car with very small engine size and price less than 20Lakh can be seen. (Shall look at it)\n- There is one Automatic Diesel car of INR 160 Lakh and engine size 3000 and one Automatic Petrol car with engine size 6000 and price above 40 Lakh (Will have a look at it).\n\n- There are maximum first hand cars in the data set. As the car exchanges hands the price value of the car drops. However, there is one Third hand of INR 120 Lakh with engine size 5200 (Shall look at it).\n\n- Majority of the cars purchase and sales is done in South India. Majority of the cars with bigger engine size is either purchased or sold in South India.  \n\n- Now shall look up the extreme cases detected above","metadata":{}},{"cell_type":"code","source":"data[(data['Transmission'] =='Automatic') & (data['Owner_Type'] =='First') & (data['Price'] == 160)]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[(data['Transmission'] =='Automatic') & (data['Fuel_Type'] == 'Electric') & (data['Region'] == 'South_India')]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[(data['Owner_Type'] == 'Third') & (data['engine_num']> 5000)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Land Rover Range Rover 3.0 Diesel LWB Vogue is the most expensive car. (Will check the after treating outliers) \n\n- Mahindra E Verito D4 is the only car with minimum engine size. (Will check after treating outliers)\n\n- Lamborghini Gallardo Coupe is the only expensive Third hand petrol car. (Will check after treating outliers)","metadata":{}},{"cell_type":"markdown","source":"### Price vs Power vs Categorical Variables( with hue as Fuel_Type, Owner_Type, Region and Transmission)","metadata":{}},{"cell_type":"code","source":"\ntitle_dict = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'bold',\n        'size': 16\n        }\n\nlab_dict = {'family': 'serif',\n              'color': 'black',\n              'size': 14\n              }\n\nplt.figure(figsize=[20,15])\ngrid = plt.GridSpec(3, 2, wspace=0.6, hspace=0.4) #Defining the grid space\n\n#Plotting Price vs Power vs Transmission\n\nax0 = plt.subplot(grid[0, 0])\nsns.scatterplot(y= 'Price',x = 'power_num', hue = 'Transmission', data = data)\nax0.set_ylabel('Price', fontdict=lab_dict)\nax0.set_xlabel('Power', fontdict=lab_dict)\nplt.title('Price vs Power vs Transmission', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(1,1))\n\n#Plotting Price vs Power vs Fuel_Type\nax1 = plt.subplot(grid[0, 1])\nsns.scatterplot(y= 'Price', x = 'power_num', hue = 'Fuel_Type', data = data)\nax1.set_ylabel('Price', fontdict=lab_dict)\nax1.set_xlabel('Power', fontdict=lab_dict)\nplt.title('Price vs Power vs Fuel_Type', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(1,1))\n\n           \n#Plotting Price vs Power vs Owner_Type\nax2 = plt.subplot(grid[1,0])\nsns.scatterplot(y= 'Price', x = 'power_num', hue = 'Owner_Type', data = data)\nax2.set_ylabel('Price', fontdict=lab_dict)\nax2.set_xlabel('Power', fontdict=lab_dict)\nplt.title('Price vs Power vs Owner_Type', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(0.97,1))\n\n#Plotting Price vs Power vs Region\nax3 = plt.subplot(grid[1, 1])\nsns.scatterplot(y= 'Price', x = 'power_num', hue = 'Region', data = data)\nax3.set_ylabel('Price', fontdict=lab_dict)\nax3.set_xlabel('Power', fontdict=lab_dict)\nplt.title('Price vs Power vs Region', fontdict = title_dict )\nplt.legend(bbox_to_anchor=(0.97,1))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Automatic cars provides more power than manual cars and are more expensive in comparison with the manual cars.\n- There are more expensive Diesel cars with power less than 300 and more number of expensive Petrol cars with power greater than 300\n- There are mostly first hand cars that provides more power. However, there is one thrid hand with maximum power (will look at it)\n- According to the data set, South India has wide range of cars (in terms of `price`) that provides more power in comparison with the other regions. However, there is one car in North India with maximum power (Will look at it)","metadata":{}},{"cell_type":"code","source":"print(data[(data['Owner_Type'] == 'Third') & (data['power_num'] > 500)])\nprint('-'*90)\nprint(data[(data['Region'] == 'North_India') & (data['power_num']> 500)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Its the same data was and was adressed previously as well. ","metadata":{}},{"cell_type":"markdown","source":"### Price vs New_Price ","metadata":{}},{"cell_type":"code","source":"title_dict = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'bold',\n        'size': 16\n        }\n\nlab_dict = {'family': 'serif',\n              'color': 'black',\n              'size': 14\n              }\n\n\nplt.figure(figsize=(10,8))\nsns.lmplot(y= 'Price', x = 'new_price_num', data = data)\nplt.ylabel('Price', fontdict=lab_dict)\nplt.xlabel('New_Price', fontdict=lab_dict)\nplt.title('Price vs New_Price ', fontdict = title_dict )\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are a lot of extreme values which must be taken into consideration and treated. ","metadata":{}},{"cell_type":"markdown","source":"#### Plotting categorical variables with Price","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Fuel_Type', columns = 'Transmission', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap \nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);\nplt.title('Price vs Fuel_Type vs Transmission ', fontdict = title_dict );","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Automatic disel used cars have a higher price in comparison with the others","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Owner_Type', columns = 'Transmission', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap \nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);\nplt.title('Price vs Owner_Type vs Transmission ', fontdict = title_dict );","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Automatic First hand cars have a higher price in comparison with the others. \n","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Region', columns = 'Fuel_Type', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap \nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);\nplt.title('Price vs Fuel_Type vs Region ', fontdict = title_dict );","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most expensive electric cars are purchased or sold only in South India and West India. \n\nThe price of Diesel type used cars is maximum in South India\n\n","metadata":{}},{"cell_type":"markdown","source":"#### Price change with the year of editon","metadata":{}},{"cell_type":"code","source":"#Price vs Year vs Fuel Type\nplt.figure(figsize = (15, 7))\nsns.lineplot(x = 'Year', y = 'Price', hue = 'Fuel_Type', ci = 95, data = data);\nplt.title('Price vs Year vs Fuel_Type ', fontdict = title_dict );","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- For Diesel and Petrol, the price of used cars has shown a steady increase. However, Diesel cars are more costly than Petrol cars.\n- CNG cars have shown their appearance since 2005 and LPG cars from 2007. Not much increase in price can be observed.\n- The price for electric cars is constant across the years. \n\n\n","metadata":{}},{"cell_type":"code","source":"#Price vs Year vs Transmission\nplt.figure(figsize = (15, 7))\nsns.lineplot(x = 'Year', y = 'Price', hue = 'Transmission', ci = 95, data = data);\nplt.title('Price vs Year vs Transmission ', fontdict = title_dict );","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The price for manual cars has shown a smooth incline over the period of time.\n\nFor automatic cars, the increase in price was not steady across the years. 2 major drops can be seen, one in 2004 and the other in 2007. From 2008, a step incline can be seen in the price for Automatic cars.","metadata":{}},{"cell_type":"code","source":"#Price vs Year vs Owner Type\nplt.figure(figsize = (15, 7))\nsns.lineplot(x = 'Year', y = 'Price', hue = 'Owner_Type', ci = 95, data = data);\nplt.title('Price vs Year vs Owner_Type ', fontdict = title_dict );","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Steep incline can be seen for the price of first hand cars over the years and from 2017 and 2013 a decline can be seen in the price of second and third hand cars respectively.","metadata":{}},{"cell_type":"code","source":"#Price vs Year vs Region\nplt.figure(figsize = (15, 7))\nsns.lineplot(x = 'Year', y = 'Price', hue = 'Region', ci = 95, data = data);\nplt.title('Price vs Year vs Region ', fontdict = title_dict );","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In  North India and West India, from 2018 a decline in the price of used cars can be seen. However, no decline in price is observed in South India","metadata":{}},{"cell_type":"markdown","source":"#### Plotting varaibles with multicollinearity.","metadata":{}},{"cell_type":"code","source":"title_dict = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'bold',\n        'size': 16\n        }\n\nlab_dict = {'family': 'serif',\n              'color': 'black',\n              'size': 14\n              }\n\nplt.figure(figsize=[10,10])\ngrid = plt.GridSpec(3, 2, wspace=0.5, hspace=0.4) #Defining the grid space\n\n#Plotting Mileage vs Engine \n\nax0 = plt.subplot(grid[0, 0])\nsns.regplot(x= 'km_per_unit_fuel', y = 'engine_num',  data = data)\nax0.set_xlabel('Mileage', fontdict=lab_dict)\nax0.set_ylabel('Engine', fontdict=lab_dict)\nplt.title('Mileage vs Engine',  fontdict = title_dict )\n\n\n#Plotting Mileage vs Power\nax1 = plt.subplot(grid[0, 1])\nsns.regplot(x= 'km_per_unit_fuel', y = 'power_num', data = data)\nax1.set_xlabel('Mileage', fontdict=lab_dict)\nax1.set_ylabel('Power', fontdict=lab_dict)\nplt.title('Mileage vs Power', fontdict = title_dict )\n\n\n           \n#Plotting Power vs Engine \n\nax2 = plt.subplot(grid[1,0])\nsns.regplot(x= 'power_num', y = 'engine_num', data = data)\nax2.set_xlabel('Power', fontdict=lab_dict)\nax2.set_ylabel('Engine', fontdict=lab_dict)\nplt.title('Power vs Engine', fontdict = title_dict )\nplt.show();\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is a strong negative correlation between Mileage and Power and Mileage and Engine\n- There is strong positive correlation between Power and Engine. \n\nWill drop them on the basis of Variance Influence Factor while testing Multicollinearity","metadata":{}},{"cell_type":"markdown","source":"#### Price vs Seats ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 5))\nsns.scatterplot(y = 'Price', x = 'Seats',hue = 'Fuel_Type' ,data = data);","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more number of 5, 6, 7, 8, 9, 10 seater diesel cars in comparision with the other fuel tpyes. However, there are majorly petrol 2 seater cars ","metadata":{}},{"cell_type":"markdown","source":"#### Will furthur classify the Car_Brands into Luxury, medium and non Luxury brands on the basis of Premium brand names ","metadata":{}},{"cell_type":"code","source":" #Trifurcating Brand into:-\n    \nLuxury_Brand = ['Audi', 'BMW', 'Bentley', 'Jaguar', 'Mercedes-Benz', \n                'Mini','Land Rover', 'Mitsubishi', 'Porsche', 'Skoda', 'Volvo', 'Lamborghini']\nMedium_Brand = ['Chevrolet', 'Toyota','Volkswagen', \n                'Hyundai', 'Honda'] \nNon_Luxury_Brand = ['Ambassador','Datsun', 'Fiat', 'Force', \n                    'Ford', 'Isuzu', 'Jeep', 'Mahindra', 'Nissan', 'Renault', 'Tata', 'Smart', 'Maruti'] \n \n# Defining a function to map the brands \ndef Luxury(x):\n    if x in Luxury_Brand:\n        return 'Luxury_Brand'\n    elif x in Medium_Brand :\n        return 'Medium_Brand '\n    else:\n        return 'Non_Luxury_Brand'\n\n# Apyplying the function to our data\ndata['Brand'] = data['Car_Brand'].apply(Luxury)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Brand.value_counts()\ndata['Brand'] = data['Brand'].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"New feature created in the data and it's data type has been fixed","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Brand', columns = 'Region', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap \nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"South India has higher luxury cars as compared to the others. ","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Brand', columns = 'Fuel_Type', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap \nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Diesel model of luxury brand have higher price value in comparison with the others.\n","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Brand', columns = 'Owner_Type', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap\nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The price value of first hand and second hand luxury brand cars are realtively higher than the other brands.","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Seats', columns = 'Brand', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap \nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The price of a 2 seater luxury brand is higher than the other.\n- Luxury brand car has a range of models from 2 seater to 7 seater cars. \n\n","metadata":{}},{"cell_type":"code","source":"df_hm =data.pivot_table(index = 'Brand', columns = 'Transmission', values = \"Price\", aggfunc = np.median)\n# Draw a heatmap \nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(df_hm, cmap = 'coolwarm', linewidths = .5, annot = True, ax = ax);","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Automatic Luxury brand cars are the most expensive cars.","metadata":{}},{"cell_type":"markdown","source":"`Obervations` \n    \n    1. Overall the demand and the price for Automatic used cars have been increased during the period of years  \n    2. There is a a steep increase for the purchase and sale of used cars in Southern Region of India. \n    3. The sale and purchase of Luxury brand cars is majorly done in Southern region of India.  \n    4. There are only two records for Automatic Electric car which is observed in Southern and Western region of India\n    5. Increase in purchase and sale of first hand cars has shown a steady increase over the period of years. \n    6. Power and Engine of the cars are highly correlated.\n    7. Mileage is negatively coorelated with Power and Engine of the car.\n    ","metadata":{}},{"cell_type":"markdown","source":"### Transforming numerical columns ","metadata":{}},{"cell_type":"code","source":"#Transforming the columns \ndata['Kilometers_Driven_log'] = np.log(data['Kilometers_Driven'])\n\ndata['Price_log'] = np.log(data['Price'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Skewness check for Kilometers Driven')\nprint(data['Kilometers_Driven'].skew())\nprint(data['Kilometers_Driven_log'].skew())\nprint('-'*30)\n\n\nprint('Skewness check for Price')\nprint(data['Price'].skew())\nprint(data['Price_log'].skew())\nprint('-'*30)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above output shows that the skewness value has came down for each column that was transformed which confirms that the distribution has been treated for highly extreme values.","metadata":{}},{"cell_type":"code","source":"##### Let us drop the initial columns \ndata.drop(['Kilometers_Driven'], axis =1, inplace =True)\ndata.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transformation of  columns\n#set pandas to display more rows\n\nfrom scipy.stats import norm\nimport scipy.stats as stats\npd.set_option('display.max_rows', 200)\n\n\nfig = plt.figure(figsize=[20,35]);\ngrid = plt.GridSpec(5, 2, wspace=0.5, hspace=0.3);\nx = ['Price_log', 'Kilometers_Driven_log'];\n\n#loop to populate boxplots within subplots\nfor i, a in enumerate(x):\n    exec(f'ax{i}0 = plt.subplot(grid[i,0]);')\n    exec(f'sns.distplot(data[a], ax=ax{i}0);')\n    exec(f'ax{i}0.set_title(a);')\n    exec(f'ax{i}1 = plt.subplot(grid[i,1]);')\n    exec(f'stats.probplot(data[a],dist=\"norm\",plot=pylab);')\n        \nfig.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:-**\n Skewness has reduced for all the variables and can see their transformation towards normailty as well. \n Price_log and Kilometers_Driven_log now has a normal distribution","metadata":{}},{"cell_type":"code","source":"numeric_columns = data.select_dtypes(include = np.number).columns.tolist()\n\n# sorting correlations w.r.t Price  \ncorr = data[numeric_columns].corr().sort_values(by = ['Price_log'], ascending = False) \n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize = (15, 10))\n\n\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap = 'seismic', annot = True, fmt = \".1f\", vmin = -1, vmax = 1, center = 0, square = False,\n            linewidths = .7, cbar_kws = {\"shrink\": .5});\n\nplt.title(' Correlation Matrix after transformation', fontstyle = 'italic')\nplt.tick_params(axis='both', labelsize = 12, labelrotation = 45)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data, corner=True)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now `Year` is also correlated with `Price`","metadata":{}},{"cell_type":"markdown","source":"## Create Dummy Variables\n\nValues like `Mumbai`or`Pune`cannot be read into an equation. Using substitutes like `1 for Mumbai`, `2 for Pune` and so on would end up implying some baseless assumption which should be avoided. However, `Owner_Type` can be given substitutes such as `1 for First owner`, `2 for Second Owner` and so on. \n\nFor the rest of the categorical variables dummy variables are creates and `drop_frist =True`is set. If we do not drop one of the dummies, then they become linearly related and violate model assumptions.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = data.copy()\n\nind_vars = data1.drop([\"Price_log\", 'Price'], axis=1)\ndep_var = data1[[\"Price_log\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_cat_vars(x):\n    x = pd.get_dummies(\n        x,\n        columns=x.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),\n        drop_first=True,\n    )\n    return x\n\n\nind_vars_num = encode_cat_vars(ind_vars)\nind_vars_num.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"#split the data into train and test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n\nx_train, x_test, y_train, y_test = train_test_split(ind_vars_num, dep_var, test_size=0.3, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Size of the training and testing set\nprint('The size of the training and testing sets are: \\n')\nprint('X train size', x_train.size)\nprint('-'*30)\nprint('X test size', x_test.size)\nprint('-'*30)\nprint('y train size', y_train.size)\nprint('-'*30)\nprint('y test size', y_test.size)\n\n# Shape of the training and testing set\nprint('The shape of the training and testing sets are: \\n')\nprint('X train size', x_train.shape)\nprint('-'*30)\nprint('X test size', x_test.shape)\nprint('-'*30)\nprint('y train size', y_train.shape)\nprint('-'*30)\nprint('y test size', y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choose Model, Train and Evaluate","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\n\n# Statsmodel api does not add a constant by default. We need to add it explicitly.\nx_train = sm.add_constant(x_train)\n# Add constant to test data\nx_test = sm.add_constant(x_test)\n\n\ndef build_ols_model(train):\n    # Create the model\n    olsmodel = sm.OLS(y_train[\"Price_log\"], train)\n    return olsmodel.fit()\n\n\nolsmodel1 = build_ols_model(x_train)\nprint(olsmodel1.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** Observation**\n- P value of a variable indicates if the variable is significant or not. If we consider significance level to be 0.05 (5%) than any variable with p-values less than 0.05 would be considered significant \n\n\n\n\nand rest of the values will be dropped one by one.\n\n- Negative values of coefficient shows that, Life expectancy deceases with their increase.\n- Positive values of coefficient shows that, Life expectancy inceases with their increase.\n- But these variables might contain Multicollinearity which affects the p values, so we first need to deal with multicollinearity and then look for p values \n","metadata":{}},{"cell_type":"markdown","source":"* Both the R-squared and Adjusted R squared of our model are very high. This is a clear indication that we have been able to create a very good model that is able to explain variance in price of used cars for upto 95% \n* The model is not an underfitting model.\n* To be able to make statistical inferences from our model, we will have to test that the linear regression assumptions are followed.\n* Before we move on to assumption testing, we'll do a quick performance check on the test data.","metadata":{}},{"cell_type":"code","source":"import math\n\n# RMSE\ndef rmse(predictions, targets):\n    return np.sqrt(((targets - predictions) ** 2).mean())\n\n\n\n\n# MAE\ndef mae(predictions, targets):\n    return np.mean(np.abs((targets - predictions)))\n\n\n# Model Performance on test and train data\ndef model_pref(olsmodel, x_train, x_test):\n\n    # Insample Prediction\n    y_pred_train_pricelog = olsmodel.predict(x_train)\n    y_pred_train_Price = y_pred_train_pricelog.apply(math.exp)\n    y_train_Price = y_train[\"Price_log\"]\n\n    # Prediction on test data\n    y_pred_test_pricelog = olsmodel.predict(x_test)\n    y_pred_test_Price = y_pred_test_pricelog.apply(math.exp)\n    y_test_Price = y_test[\"Price_log\"]\n\n    print(\n        pd.DataFrame(\n            {\n                \"Data\": [\"Train\", \"Test\"],\n                \"RMSE\": [\n                    rmse(y_pred_train_Price, y_train_Price),\n                    rmse(y_pred_test_Price, y_test_Price),\n                ],\n                \"MAE\": [\n                    mae(y_pred_train_Price, y_train_Price),\n                    mae(y_pred_test_Price, y_test_Price),\n                ], \n                 \n                \n            }\n        )\n    )\n    \n\n#Checking model performance\nmodel_pref(olsmodel1, x_train, x_test)  # High Overfitting.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Root Mean Squared Error of train and test data is starkly different, indicating that our model is overfitting the train data. \n* Mean Absolute Error indicates that our current model is able to predict used cars prices within mean error of 11.05 lakhs on test data.\n* The units of both RMSE and MAE are same - Lakhs in this case. But RMSE is greater than MAE because it peanalises the outliers more.\n","metadata":{}},{"cell_type":"markdown","source":"### Checking the Linear Regression Assumptions","metadata":{}},{"cell_type":"markdown","source":"1. No Multicollinearity\n2. Mean of residuals should be 0\n3. No Heteroscedacity\n4. Linearity of variables\n5. Normality of error terms","metadata":{}},{"cell_type":"markdown","source":"#### Let's check Multicollinearity using VIF scores\n\n##### TEST FOR MULTICOLLINEARITY\n\n* Multicollinearity occurs when predictor variables in a regression model are correlated. This correlation is a problem because predictor variables should be independent.  If the correlation between variables is high, it can cause problems when we fit the model and interpret the results. When we have multicollinearity the linear model, The coefficients that the model suggests are unreliable.\n\n* There are different ways of detecting(or  testing) multi-collinearity, one such way is Variation Inflation Factor.\n\n* **Variance  Inflation  factor**:  Variance  inflation  factors  measure  the  inflation  in  the variances of the regression parameter estimates due to collinearities that exist among the  predictors.  It  is  a  measure  of  how  much  the  variance  of  the  estimated  regression coefficient $\\β_k$ is “inflated”by  the  existence  of  correlation  among  the  predictor variables in the model. \n\n* General Rule of thumb: If VIF is 1 then there is no correlation among the kth predictor and the remaining predictor variables, and  hence  the variance of β̂k is not inflated at all. Whereas if VIF exceeds 5 or is close to exceeding 5, we say there is moderate VIF and if it is 10 or exceeding 10, it shows signs of high multi-collinearity.","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\ndef checking_vif(train):\n    vif = pd.DataFrame()\n    vif[\"feature\"] = train.columns\n\n    # calculating VIF for each feature\n    vif[\"VIF\"] = [\n        variance_inflation_factor(train.values, i) for i in range(len(train.columns))\n    ]\n    return vif\n\n\nprint(checking_vif(x_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are a few variables with high VIF.\n# Our current model is extreamly complex. Let us first bin the Brand and Model columns.\n# This wouldn't essentially reduce multicollinearity in the data, but it will help us make the dataset more managable\n\n#Will create a new column Car_Category by doing the following:_\ndata.groupby([\"Car_Brand\", \"Model\"])['new_price_num'].mean().sort_values(ascending=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will create a new variable Car Category by binning the new_price_num\n# Create a new variable - Car Category\ndf1= data.copy()\ndf1[\"Car_Category\"] = pd.cut(\n    x=data[\"new_price_num\"],\n    bins=[0, 15, 30, 50, 200],\n    labels=[\"Budget_Friendly\", \"Mid-Range\", \"Luxury_Cars\", \"Ultra_luxury\"],\n)\n# car_category.value_counts()\n\n# Drop the Brand and Model columns.\ndf1.drop(columns=[\"Car_Brand\", \"Model\", 'Brand', 'Region'], axis=1, inplace=True)# We will create a new variable Car Category by binning the new_price_num\ndf1.columns","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will have to create the x and y datasets again\ny = df1[['Price_log']]\nX = df1.drop(['Price_log', 'Price'], axis=1)\n\n\ndef encode_cat_vars(x):\n    x = pd.get_dummies(\n        x,\n        columns=x.select_dtypes(include=[\"object\", \"category\"]).columns.tolist(),\n        drop_first=True,\n    )\n    return x\n\n\nX = encode_cat_vars(X)\nX.head()\n\n\n\n# Splitting data into train and test\nx_train, x_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1)\n\nprint(\"Number of rows in train data =\", x_train.shape[0])\nprint(\"Number of rows in train data =\", x_test.shape[0], \"\\n\\n\")\n\n# Statsmodel api does not add a constant by default. We need to add it explicitly.\nx_train = sm.add_constant(x_train)\n# Add constant to test data\nx_test = sm.add_constant(x_test)\n\n# Fit linear model on new dataset\nolsmodel2 = build_ols_model(x_train)\nprint(olsmodel2.summary())","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The R squared and adjusted r squared values have decreased, but are still quite high indicating that we have been able to capture most of the information of the previous model even after reducing the number of predictor features. \n\nAs we try to decrease overfitting, the r squared of our train model is expected to decrease","metadata":{}},{"cell_type":"code","source":"# Check VIF\nprint(checking_vif(x_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The R squared and adjusted r squared values have decreased, but are still quite high indicating that we have been able to capture most of the information of the previous model even after reducing the number of predictor features by a huge extent.\n\nAs we try to decrease overfitting, the r squared of our train model is expected to decrease.","metadata":{}},{"cell_type":"code","source":"# Checking model performance\nmodel_pref(olsmodel2, x_train, x_test)  # No Overfitting.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The RMSE on train data has increased but has been reduced for the test data. \n* The RMSE values on both the dataset being close to each other indicate that the model is not overfitting the training data anymore.\n* Reducing overfitting has caused the MAE to decrease on testing data\n","metadata":{}},{"cell_type":"markdown","source":"We have managed to control overfitting and reduce the test data error.\n\nLet us now remove multicollinearity from the model.","metadata":{}},{"cell_type":"markdown","source":"## Removing Multicollinearity\n * To remove multicollinearity\n  1. Drop every column one by one, that has VIF score greater than 5.\n  2. Look at the adjusted R square of all these models\n  3. Drop the Variable that makes least change in Adjusted-R square\n  4. Check the VIF Scores again\n  5. Continue till you get all VIF scores under 5","metadata":{}},{"cell_type":"code","source":"# Method to drop all the multicollinear column and choose which one we should drop\ndef treating_multicollinearity(high_vif_columns, x_train, x_test):\n    \"\"\"\n    Drop every column that has VIF score greater than 5, one by one.\n    Look at the adjusted R square of all these models\n    Look at the RMSE of all these models on test data\n    \"\"\"\n    adj_rsq_scores = []\n    rmse_test_data = []\n\n    # build ols models by dropping one of these at a time and observe the Adjusted R-squared\n    for cols in high_vif_columns:\n        train = x_train.loc[:, ~x_train.columns.str.startswith(cols)]\n        test = x_test.loc[:, ~x_test.columns.str.startswith(cols)]\n        # Create the model\n        olsres = build_ols_model(train)\n        # Adj R-Sq\n        adj_rsq_scores.append(olsres.rsquared_adj)\n        # RMSE (Test data)\n        y_pred_test_pricelog = olsres.predict(test)\n        y_pred_test_Price = y_pred_test_pricelog.apply(math.exp)\n        y_test_Price = y_test[\"Price_log\"]\n        rmse_test_data.append(rmse(y_pred_test_Price, y_test_Price))\n\n    # Add new Adj_Rsq and RMSE after dropping each colmn\n    temp = pd.DataFrame(\n        {\n            \"col\": high_vif_columns,\n            \"Adj_rsq_after_dropping_col\": adj_rsq_scores,\n            \"Test RMSE\": rmse_test_data,\n        }\n    ).sort_values(by=\"Adj_rsq_after_dropping_col\", ascending=False)\n\n    print(temp)\n    print(\"\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_vif_columns = [\n    \"engine_num\",\n    \"power_num\",\n    \"new_price_num_log\",\n    \"Fuel_Type\",\n    \"car_category\",\n]\ntreating_multicollinearity(high_vif_columns, x_train, x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping cars_category would have the maximum impact on predictive power of the model (amongst the variables being considered)\n# We'll drop engine_num and check the vif again\n\n# Drop 'engine_num' from train and test\ncol_to_drop = \"engine_num\"\nx_train = x_train.loc[:, ~x_train.columns.str.startswith(col_to_drop)]\nx_test = x_test.loc[:, ~x_test.columns.str.startswith(col_to_drop)]\n\n# Check VIF now\nvif = checking_vif(x_train)\nprint(\"VIF after dropping \", col_to_drop)\nprint(vif)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping engine_num has brought the VIF of power_num below 5\n# new_price_num, Fuel_Type and car_category still show some multicollinearity\n\n# Check which one of these should we drop next\nhigh_vif_columns = [\n    \"new_price_num\",\n    \"Fuel_Type\",\n    \"car_category\",\n]\ntreating_multicollinearity(high_vif_columns, x_train, x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop 'new_price_num' from train and test since the RMSE and Adj. Rsq is not affected much by this variable\ncol_to_drop = \"new_price_num\"\nx_train = x_train.loc[:, ~x_train.columns.str.startswith(col_to_drop)]\nx_test = x_test.loc[:, ~x_test.columns.str.startswith(col_to_drop)]\n\n# Check VIF now\nvif = checking_vif(x_train)\nprint(\"VIF after dropping \", col_to_drop)\nprint(vif)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have removed multicollinearity from the data now\n\nFuel_Type variables are showing high vif because most cars are either diesel and petrol. \nThese two features are correlated with each other.\n\nWe will not drop this variable from the model because this will not affect the interpretation of other features in the model","metadata":{}},{"cell_type":"code","source":"# Fit linear model on new dataset\nolsmodel3 = build_ols_model(x_train)\nprint(olsmodel3.summary())\n\nprint(\"\\n\\n\")\n\n# Checking model performance\nmodel_pref(olsmodel3, x_train, x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model R-squared and Adjusted R squared is same as the previous model - olsmodel2.\nRemoval of multicollinear variable has not causes any information loss in the model.\n\nThe RMSE of the model on train data has increased.\nBefore we can make inferences from this model, let us ensure that other model assumptions are followed.","metadata":{}},{"cell_type":"markdown","source":"### Checking Assumption 2: Mean of residuals should be 0","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.3f' % x) \n\nresiduals = olsmodel3.resid\nnp.mean(residuals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Mean of redisuals is very close to 0.","metadata":{}},{"cell_type":"markdown","source":"### Checking Assumption 3: Linearity of variables\n\nPredictor variables must have a linear relation with the dependent variable.\n\nTo test the assumption, we'll plot residuals and fitted values on a plot and ensure that residuals do not form a strong pattern. They should be randomly and uniformly scattered on the x axis.\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# predicted values\nfitted = olsmodel3.fittedvalues\n\n# sns.set_style(\"whitegrid\")\nsns.residplot(fitted, residuals, color=\"purple\", lowess=True)\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residual\")\nplt.title(\"Residual PLOT\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ### Checking Assumption 4: No Heteroscedasticity\n\n\nTEST FOR HOMOSCEDASTICITY\n\n* Homoscedacity - If the residuals are symmetrically distributed across the regression line , then the data is said to homoscedastic.\n\n* Heteroscedasticity- - If the residuals are not symmetrically distributed across the regression line, then the data is said to be heteroscedastic. In this case the residuals can form a funnel shape or any other non symmetrical shape.\n\nWe'll use `Goldfeldquandt Test` to test the following hypothesis\n\nNull hypothesis : Residuals are homoscedastic\nAlternate hypothesis : Residuals have hetroscedasticity\n\nalpha = 0.05 ","metadata":{}},{"cell_type":"code","source":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\nname = [\"F statistic\", \"p-value\"]\ntest = sms.het_goldfeldquandt(residuals, x_train)\nlzip(name, test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since p-value > 0.05 we cannot reject the Null Hypothesis that the residuals are homoscedastic. \n\nAssumptions 3 is also satisfied by our olsmodel3.","metadata":{}},{"cell_type":"markdown","source":"### Checking Assumption 5: Normality of error terms\n\nThe residuals should be normally distributed.","metadata":{}},{"cell_type":"code","source":"# Plot histogram of residuals\nsns.distplot(residuals)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot q-q plot of residuals\nimport pylab\nimport scipy.stats as stats\n\nstats.probplot(residuals, dist=\"norm\", plot=pylab)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The residuals have a close to normal distribution. Assumption 5 is also satisfied.\nWe should further investigate these values in the tails where we have made huge residual errors.\n\nNow that we have seen that olsmodel3 follows all the linear regression assumptions. Let us use that model to draw inferences.","metadata":{}},{"cell_type":"code","source":"print(olsmodel3.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observations from the model","metadata":{}},{"cell_type":"markdown","source":"1. With our linear regression model we have been able to capture ~90 variation in our data.\n2. The model indicates that the most significant predictors of price of used cars are - \n    - The year of manufacturing\n    - Number of seats in the car\n    - Power of the engine\n    - Mileage\n    - Kilometers Driven\n    - Location\n    - Fuel_Type\n    - Transmission - Automatic/Manual\n    - Car Category - budget brand to ultra luxury\n    The p-values for these predictors are <0.05 in our final model \n3. Newer cars sell for higher prices. 1 unit increase in the year of manufacture leads to [ exp(0.1170) = 1.12 Lakh ] increase in the price of the vehicle, when everything else is constant.\nIt is important to note here that the predicted values are log(price) and therefore coefficients have to converted accordingly to understand that influence in Price.\n4. As the number of seats increases, the price of the car increases - exp(0.0343) = 1.03 Lakhs\n5. Mileage is inversely correlated with Price. Generally, high Mileage cars are the lower budget cars.\nIt is important to note here that correlation is not equal to causation. That is to say that increase in Mileage does not lead to a drop in prices. It can be understood in such a way that the cars with high mileage do not have a high power engine and therefore have low prices.\n6. Kilometers Driven have a negative relationship with the price which is intuitive. A car that has been driven more will have more wear and tear and hence sell at a lower price, everything else being 0.\n7. The categorical variables are a little hard to interpret. But it can be seen that all the car_category variables in the dataset have a positive relationship with the Price and the magnitude of this positive relationship increases as the brand category moves to the luxury brands. It will not be incorrect to interpret that the dropped car_category variable for budget friendly cars would have a negative relationship with the price (because the other 3 are increasingly positive.)","metadata":{}},{"cell_type":"markdown","source":"\n* Some southern markets tend to have higher prices. It might be a good strategy to plan growth in southern cities using this information. Markets like Kolkata(coeff = -0.2) are very risky and we need to be careful about investments in this area.\n* We will have to analyse the cost side of things before we can talk about profitability in the business. We should gather data regarding that.\n* The next step post that would be to cluster different sets of data and see if we should make multiple models for different locations/car types.","metadata":{}},{"cell_type":"markdown","source":"# Add-on: Analyzing predictions where we were way off the mark","metadata":{}},{"cell_type":"code","source":"# Extracting the rows from original data frame df where indexes are same as the training data\noriginal_df = data[data.index.isin(x_train.index.values)].copy()\n\n# Extracting predicted values from the final model\nresiduals = olsmodel3.resid\nfitted_values = olsmodel3.fittedvalues\n\n# Add new columns for predicted values\noriginal_df[\"Predicted price_log \"] = fitted_values\noriginal_df[\"Predicted Price\"] = fitted_values.apply(math.exp)\noriginal_df[\"residuals\"] = residuals\noriginal_df[\"Abs_residuals\"] = residuals.apply(math.exp)\noriginal_df[\"Difference in Lakhs\"] = np.abs(\n    original_df[\"Price\"] - original_df[\"Predicted Price\"]\n)\n\n# Let us look at the top 20 predictions where our model made highest extimation errors (on train data)\noriginal_df.sort_values(by=[\"Difference in Lakhs\"], ascending=False).head(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* A 2017 Land Rover, whose new model sells at 230 Lakhs and the used version sold at 160 Lakhs was predicted to be sold at 32L. It is not apparent after looking at numerical predictors, why our model predicted such low value here. This could be because all other land rovers in our data seems to have sold at lower prices.\n* The second one in the list here is a Porsche cayenne that was sold at 2 Lakhs but our model predicted the price as 85.4. This is most likely a data entry error. A 2019 manufactured Porsche selling for 2 Lakh is highly unlikely. With all the information we have, the predicted price 85L seems much more likely. We will be better off dropping this observation from our current model. If possible, the better route would be to gather more information here.\n* There are a few instances where the model predicts lesser than the actual selling price. These could be a cause for concern. The model predicting lesser than potential selling price is not good for business.\n* Let us quickly visualise some of these observations. ","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(\n    original_df[\"Difference in Lakhs\"],\n    original_df[\"Price\"],\n    hue=original_df[\"Fuel_Type\"],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most outliers are the Petrol cars. Our model predicts that resale value of diesel cars is higher compared to petrol cars. This is probably the cause of these outliers.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}