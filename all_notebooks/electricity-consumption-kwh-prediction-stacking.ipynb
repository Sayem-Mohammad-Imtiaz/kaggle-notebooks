{"cells":[{"metadata":{"id":"kqOwI1fAJyGw"},"cell_type":"markdown","source":"[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RN93yQ0kqAwlgAQZLVs9fAezk_5DryAd?usp=sharing)"},{"metadata":{"id":"nesnuYAX0Sz0"},"cell_type":"markdown","source":"# Set Up"},{"metadata":{"id":"ydIxJECotlz9","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nfrom numpy import percentile\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import RidgeCV","execution_count":null,"outputs":[]},{"metadata":{"id":"o6kGTfosBckW","trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","execution_count":null,"outputs":[]},{"metadata":{"id":"Ab7FBUSy0aG1"},"cell_type":"markdown","source":"# Data Analysis and Preprocessing"},{"metadata":{"id":"MrhmGm3utqaG","trusted":true},"cell_type":"code","source":"# pandas couldn't identify the type of the following two columns\nrecs_df = pd.read_csv(\"../input/residential-energy-consumption-survey/recs2009_public.csv\",dtype={\"NOCRCASH\":\"object\",\"NKRGALNC\":\"object\"})","execution_count":null,"outputs":[]},{"metadata":{"id":"gmtwRdlOy-M6"},"cell_type":"markdown","source":"We take advantage of pandas profiling to get an overview of the data and a quick analysis. Due to the large size of the report, I chose minimal report without correlation calculations. I commented the following lines, since the report was too large for Google Colab to save."},{"metadata":{"id":"mjuAQHYgvy7M","trusted":true},"cell_type":"code","source":"# profile = ProfileReport(recs_df, title=\"RECS Dataset Profile\",minimal=True)\n# profile.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{"id":"g3ObwsAo5SZ1","trusted":true},"cell_type":"code","source":"# profile.to_file(\"recs_report.html\")","execution_count":null,"outputs":[]},{"metadata":{"id":"et4vRi7mOggu"},"cell_type":"markdown","source":"Some important insights from a glance at the report:\n\n* over 900 columns, high dimensional data --> curse-of-dimensionality a great concern\n\n* many columns with almost constant values (zeros for example) --> redundant columns\n\n* no missing values reported (missing values are most likely already imputetd by the center who did the survey)\n\n* many columns with skewed distribution, must keep an eye on these when doing regression"},{"metadata":{"id":"xAXuBRtW0MCd"},"cell_type":"markdown","source":"## Target Variable Analysis"},{"metadata":{"id":"SC9SlHmLEJb-","outputId":"ea368928-960c-4b23-a401-aa005d4cb76f","trusted":true},"cell_type":"code","source":"recs_df['KWH'].hist()","execution_count":null,"outputs":[]},{"metadata":{"id":"vIYKoVWsLw7b"},"cell_type":"markdown","source":"we can see that the distribution is a bit skewed. Since our end goal is to predict this value, and it's continuous, we need a regression model. However, based on linear regression assumptions, we ideally want our target variable and predictors be normally distributed. "},{"metadata":{"id":"4FDqKM5s5bYr"},"cell_type":"markdown","source":"## Correlations"},{"metadata":{"id":"7cYA4EV7MSDA"},"cell_type":"markdown","source":"We will have a look at the correlation among variables in our dataset."},{"metadata":{"id":"WAR1lkEAcz-l","trusted":true},"cell_type":"code","source":"def get_high_corr_df(df,positive_threshold=0.4):\n  corr = df.corr().stack().reset_index().drop_duplicates()\n  corr.columns = ['FEATURE_1', 'FEATURE_2', 'CORRELATION']\n  high_corr = corr[((corr['FEATURE_1'] != corr['FEATURE_2'] ) & ((corr['CORRELATION'] >= positive_threshold) | (corr['CORRELATION'] <= positive_threshold*-1)))]\n  return high_corr","execution_count":null,"outputs":[]},{"metadata":{"id":"ctELhQlNfWwC","outputId":"b898a756-babd-459e-e1cc-11d908729366","trusted":true},"cell_type":"code","source":"high_corr = get_high_corr_df(recs_df)\nhigh_corr[((high_corr['FEATURE_1'] == 'KWH'))]","execution_count":null,"outputs":[]},{"metadata":{"id":"DHjtdm1H1fd0"},"cell_type":"markdown","source":"There is a perfect correlation of 1 between KWH and BTUEL. If we have a look at the codebook provided on the website, we'll see that they are representing the same thing (total electricity site usage), but in different units (one in kw/h and the other one in thousand BTU). Furthermore, there are columns that show categories of KWH such as electricity usage for air-conditioning. I personally think, we should drop these columns for training a prediction model, otherwise they would serve a data-leak or cheating for the model. They are part of our target variable."},{"metadata":{"id":"TCQ4mRqvu4jZ","trusted":true},"cell_type":"code","source":"KWH_cheat_columns = [c for c in recs_df.columns if ((\"KWH\" in c and len(c)>3) or \"BTUEL\" in c) ]\nrecs_df.drop(columns=KWH_cheat_columns,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"Dqb2dmM3Q1bf"},"cell_type":"markdown","source":"drop id, unique value"},{"metadata":{"id":"QC6ZZjXsQgt3","trusted":true},"cell_type":"code","source":"recs_df.drop(columns=[\"DOEID\"],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"-rUvapZMRJcq"},"cell_type":"markdown","source":"drop the imputation flag columns, extra information that (I personally think) is irrelevant to prediction. Moreover, mostly zeros and also no noticable correlation found with \"KWH\" "},{"metadata":{"id":"hNzLORBaRcP6","trusted":true},"cell_type":"code","source":"imputation_columns = [c for c in recs_df.columns if c.startswith(\"Z\")]\nrecs_df.drop(columns=imputation_columns,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"VRLFVWZdVJ7q"},"cell_type":"markdown","source":"there are some columns that almost have a constant value, such as DOLKEROTH with mostly zeros, or AGEHHMEMCAT11 with mostly -2.\n\nFirst, we'll find these columns."},{"metadata":{"id":"ydJ4LHpKVJDv","outputId":"66584cfd-0b36-4a86-fb94-a2bb6160881d","trusted":true},"cell_type":"code","source":"columns_with_constant_values = []\nfor c in recs_df.columns:\n  value_frequencies = recs_df[c].value_counts(normalize=True)\n  if value_frequencies.max()>=0.85:\n    columns_with_constant_values.append(c)\nprint(columns_with_constant_values)\nprint(len(columns_with_constant_values))","execution_count":null,"outputs":[]},{"metadata":{"id":"hYxDfI1GbL_f"},"cell_type":"markdown","source":"Then, see if any of them have a meaningful correlation with our target variable."},{"metadata":{"id":"bqCLZ77sbAkD","outputId":"a43eca99-099c-4c0a-c0d7-0bc0d7e444b1","trusted":true},"cell_type":"code","source":"high_corr[((high_corr['FEATURE_1'] == 'KWH') & (high_corr['FEATURE_2'].isin(columns_with_constant_values)))]","execution_count":null,"outputs":[]},{"metadata":{"id":"-2rZgXyf3TVE"},"cell_type":"markdown","source":"We'll drop the redundant columns."},{"metadata":{"id":"ouqW76h8bW-w","trusted":true},"cell_type":"code","source":"recs_df.drop(columns=columns_with_constant_values,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"V5WEOQ6wblub"},"cell_type":"markdown","source":"now, we look for highly correlated (close to 1 or -1) pairs. If such pairs exist perhaps we could get rid of one of the variables, and reduce number of columns for building a model."},{"metadata":{"id":"ikNBCUN2cfBx","trusted":true},"cell_type":"code","source":"high_corr = get_high_corr_df(recs_df) #calculate again, after deleting so many columns\nredundant_sets=[]\nfor i,row in high_corr[(high_corr['CORRELATION']>=0.90)|(high_corr['CORRELATION']<=-0.90)].iterrows():\n  f1 = row['FEATURE_1']\n  f2 = row['FEATURE_2']\n  fset = {f1,f2}\n  belongs_to_sets = []\n  for j in range(len(redundant_sets)):\n    if len(redundant_sets[j].intersection(fset))!=0:  \n      belongs_to_sets.append(j)\n\n  if len(belongs_to_sets)==0:\n    redundant_sets.append(fset)\n  elif len(belongs_to_sets)==1:\n    redundant_sets[belongs_to_sets[0]].update(fset)\n  else:\n    sets_to_merge = [redundant_sets[j] for j in belongs_to_sets]\n    for sm in sets_to_merge:\n      redundant_sets.remove(sm)\n      fset.update(sm)\n    redundant_sets.append(fset)\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"3OJtVhPFlsPf","outputId":"fad5b255-2f12-4a60-ca23-0b27abf077a3","trusted":true},"cell_type":"code","source":"for s in redundant_sets:\n  print(s)","execution_count":null,"outputs":[]},{"metadata":{"id":"mCryfMdfoLQc"},"cell_type":"markdown","source":"we can see from the names of each group that the values are highly related, and we can just use one of them as the representor of that group. for example, for the {'PELHOTWA', 'ELWATER'} pair, most people chose \"not applicable\" for the first one, and 0 (not electricity used for heating water) for the second one. So when they don't use it, they don't pay for it. the same information. we can just use one."},{"metadata":{"id":"BhtAn3R3pSG8","trusted":true},"cell_type":"code","source":"for s in redundant_sets:\n  s.pop()\n  recs_df.drop(columns=list(s),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"dV5TmI5_3ytE","outputId":"77e50adb-3847-48ba-e493-40912aed5b84","trusted":true},"cell_type":"code","source":"sns.heatmap(recs_df.corr())","execution_count":null,"outputs":[]},{"metadata":{"id":"0mv24_Rv42Sg"},"cell_type":"markdown","source":"Even after removing redundant columns, there is still noticable correlation between some columns in data. When using linear regression, we ideally want the predictors to have no correlation with each other."},{"metadata":{"id":"ViaiCYNl5jZ6"},"cell_type":"markdown","source":"## Noise and Outliers"},{"metadata":{"id":"Xz77H8Hu0EgX"},"cell_type":"markdown","source":"Linear Regression is sensitive to outliers and we must take care of them before building a prediction model.\n\nStrategy for dealing with outliers: If the number of outliers found for a column make up less than 1% of the rows, remove the rows, else replace the outliers with mean.\n\nInitially, the threshold was higher (I tried a range of values from 20% to 1%). I noticed that even with a low threshold as much as 2%, we would still lose about 25% of the rows after outlier removal. So in order not to lose too much data, I used the afromentioned strategy. I think it's not ideal, but a quick and general solution for now."},{"metadata":{"id":"HOwWr_0GxGLB","outputId":"fc090f59-16a6-4baa-a2f9-6ce611d7dbb0","trusted":true},"cell_type":"code","source":"rows = set()\nfor c,column in recs_df.iteritems():\n  if column.dtype=='object':\n    continue\n\n  q25, q75 = percentile(column, 25), percentile(column, 75)\n  iqr = q75 - q25\n  cut_off = iqr * 1.5\n  lower, upper = q25 - cut_off, q75 + cut_off\n  outliers = recs_df[(recs_df[c] < lower) | (recs_df[c] > upper)] \n  if len(outliers)>0:\n    percentage = outliers.shape[0]/recs_df.shape[0]*100\n    print('%s #outliers: %d %d%%' % (c,outliers.shape[0],percentage))\n    if percentage<1:\n      recs_df.drop(index=outliers.index,inplace=True)\n\n    else:\n      recs_df[c].where(((recs_df[c] < lower) | (recs_df[c] > upper)),recs_df[c].mean())","execution_count":null,"outputs":[]},{"metadata":{"id":"Bu5RdMMG55WG","outputId":"8539e6bd-be3c-4e07-8014-2e4cf5034795","trusted":true},"cell_type":"code","source":"recs_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"RYdOCB9B8wuJ"},"cell_type":"markdown","source":"## Skewness"},{"metadata":{"id":"3q1LfqaJ_b3W"},"cell_type":"markdown","source":"Pandas profiling revealed that distribution of a lot of columns are higly skewed. Therefore the normality assumption for linear regression will be violated. We will detect skewed columns and try to transform into a normal distribution witn log transformation. The rule of thumb is if the skewness is not within [-1,1] range that the distribution is skewed. But in the profiling report, I noticed almost no column has a perfect normal distribuion and most have skewness around 2-3. So I set the threshold a bit higher in order not to transform too many columns and completely transform the data."},{"metadata":{"id":"eLP8CWLa6sfB","outputId":"bec46c89-eb83-49fc-b08d-3937a535eae9","trusted":true},"cell_type":"code","source":"for c in recs_df.columns:\n  if recs_df[c].dtype=='object':\n    continue\n  skew = recs_df[c].skew()\n  if skew>2 or skew<-2:\n    print('%s skew before transforamtion: %f' % (c,skew))\n    recs_df[c] = np.log(recs_df[c] + 1 - min(recs_df[c]))\n    print('%s skew after log transforamtion: %f' % (c,recs_df[c].skew()))","execution_count":null,"outputs":[]},{"metadata":{"id":"LaQ8Tz4i81Tm"},"cell_type":"markdown","source":"## Scaling"},{"metadata":{"id":"cvy5x1jt-gao"},"cell_type":"markdown","source":"The unit of the values in the columns are not uniform. Some came from multi-answer questions so only have a categorical value. Some have continues values with large numbers. \n\nHowever, we will not use nueral networks or linear models sensitive to unscaled data.So we don't have to worry about this much for now. (Also, linear regression makes no assumption about the scale of the variables).\n\n"},{"metadata":{"id":"5gqIQrIz9UdP"},"cell_type":"markdown","source":"## Train Test Sets"},{"metadata":{"id":"_yiyjrDZE5gH","trusted":true},"cell_type":"code","source":"X = recs_df.loc[:,recs_df.columns!='KWH'].copy()\ny = recs_df['KWH'].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"qbPMa6cM9d8H"},"cell_type":"markdown","source":"## Dealing with Categorical Values"},{"metadata":{"id":"XvIWpKwn_H0P"},"cell_type":"markdown","source":"For dealing with categorical values, we'll use label encoding. We will fit the encoder only on training set so we don't leak any information to test set and help the models cheat!"},{"metadata":{"id":"9jpyZ8thE6HH","outputId":"3b0a0a8a-e285-44e0-b7c4-ba0603557047","trusted":true},"cell_type":"code","source":"categorical_columns = [c for c in recs_df.columns if recs_df[c].dtype=='object']\nfor c in categorical_columns:\n  print(c)\n  encoder = LabelEncoder()\n  encoder.fit(X_train[c].values)\n  X_train[c] = encoder.transform(X_train[c])\n  X_test[c] = encoder.transform(X_test[c])","execution_count":null,"outputs":[]},{"metadata":{"id":"34dsdwpB9Oeo"},"cell_type":"markdown","source":"# Prediction "},{"metadata":{"id":"1pRljwOB9yN_"},"cell_type":"markdown","source":"We'll first try two separate regression models to predict KWH. Then we'll see if we can improve the prediction by stacking the two and building an ensemble model. "},{"metadata":{"id":"f2RMcVqbBMS0","outputId":"9411c080-2b4b-4d8c-b99b-ee18f1850fb8","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nestimators = [('lgbm', GradientBoostingRegressor(random_state=42,max_depth=5)),\n              ('lr', LinearRegression()),\n              ('rc', RidgeCV())]\n\nstack = StackingRegressor(estimators=estimators[:-1],final_estimator=estimators[-1][1])\nprint(\"Individual resutls:\")\nfor estimator in estimators:\n  print(\"model: %s, score: %f\" %(estimator[0],estimator[1].fit(X_train, y_train).score(X_test, y_test)))\n  \nprint(\"################\\nEnsemble result\")\nstack.fit(X_train, y_train).score(X_test, y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}