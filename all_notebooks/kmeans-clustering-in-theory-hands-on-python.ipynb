{"cells":[{"metadata":{"colab_type":"text","id":"qeA-D2bNFKvu"},"cell_type":"markdown","source":"# K-Means Clustering - Detailed Tutorial with Hands-on"},{"metadata":{"colab_type":"text","id":"WxWu2r2BFfMN"},"cell_type":"markdown","source":"#### Clustering falls under the Unserpervised Machine Leaning Family. \n\nMore on the Machine Learning Classification:\n\n![](https://raw.githubusercontent.com/nadarsubash/articles/master/ML_Classification.jpeg)\n*Disclaimer: this may not be an exhaustive list, but important ones for someone starting into Machine Learning World*\n\n## K-Means is a Heuristic Algorithm which groups together relatively closer objects into K Clusters \nK-Means is one of the most widely used Clustering algorithm "},{"metadata":{"colab":{},"colab_type":"code","id":"r1K-DortFOGF"},"cell_type":"markdown","source":"##### Steps involved in K-Means algorithm"},{"metadata":{},"cell_type":"markdown","source":"### Step 1) Let's first take some sample data *(what better than iris data)*"},{"metadata":{},"cell_type":"markdown","source":"We are taking very few samples (records) of each species for this Tutorial <br>\n*Complete dataset is freely available [here](https://archive.ics.uci.edu/ml/datasets/Iris)*"},{"metadata":{},"cell_type":"markdown","source":"|Data #\t|sepal length in cm\t|sepal width in cm\t|class|\n|-----|-----|-----|-----|\n|1\t|6.3\t|3.3\t|Iris-virginica|\n|2\t|5.8\t|2.7\t|Iris-virginica|\n|3\t|7.1\t|3\t|Iris-virginica|\n|4\t|5.1\t|3.5\t|Iris-setosa|\n|5\t|4.9\t|3\t|Iris-setosa|\n|6\t|4.7\t|3.2\t|Iris-setosa|\n|7\t|4.6\t|3.1\t|Iris-setosa|\n|8\t|7\t|3.2\t|Iris-versicolor|\n|9\t|6.4\t|3.2\t|Iris-versicolor|\n|10\t|6.9\t|3.1\t|Iris-versicolor|\n|11\t|5.5\t|2.3\t|Iris-versicolor|"},{"metadata":{},"cell_type":"markdown","source":"for simplicity sake, we shall only consider two dimentional data i.e. only sepal length and width"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Lets plot a scatter chart, which shall help us in guessing the no. of clusters we may need\n\nimport matplotlib.pyplot as plt\nx = [6.3, 5.8, 7.1, 5.1, 4.9, 4.7, 4.6, 7, 6.4, 6.9, 5.5]\ny = [3.3, 2.7, 3, 3.5, 3, 3.2, 3.1, 3.2, 3.2, 3.1, 2.3]\nplt.scatter(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2) Decide no. of clusters *(randomly based on above plot)* "},{"metadata":{},"cell_type":"markdown","source":"Again for simplicity sake, we go with **k=3**, as our aim is to understand what happens in the background of the K-Means algorithm.\n\n*N.B.* We can use Elbow method using Within-Cluster-Sum-of-Squares (WCSS) in Python to decide optimized number of *k*. This is explained in detail in the exercise **below**  *(KMeans clustering using package from SciKitLearn)*"},{"metadata":{},"cell_type":"markdown","source":"### Step 3) Identify the Centroids *$C_{i}$*"},{"metadata":{},"cell_type":"markdown","source":"Let's take Data # 1, 4 & 8 as the centroid for the first iteration <br>\n$C_{1}$ = (6.3, 3.3) <br>\n$C_{2}$ = (5.1, 3.5) <br>\n$C_{3}$ = (7.0, 3.2) <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nx = [6.3, 5.8, 7.1, 5.1, 4.9, 4.7, 4.6, 7, 6.4, 6.9, 5.5]\ny = [3.3, 2.7, 3, 3.5, 3, 3.2, 3.1, 3.2, 3.2, 3.1, 2.3]\n\ncx = [6.3, 5.1, 7]\ncy = [3.3, 3.5, 3.2]\n\nplt.scatter(x,y)\nplt.scatter(cx,cy,label='Centroids',color='red',marker='1')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4) Measure the distance between the Centroids and each of the data points"},{"metadata":{},"cell_type":"markdown","source":"This distance shall help us decide which point is closeset to which Centroid, which in turn shall help us decide to form clusters (groups)"},{"metadata":{},"cell_type":"markdown","source":"Let's use **Manhattan Distance** for this calculation <br>\n*You may as well check other distance calculation methods like Euclidean, Minkowski distance etc*"},{"metadata":{},"cell_type":"markdown","source":"Manhattan Distance between data point **X**(x1, y1) and centroid **C**(x2, y2) is: <br>\nDistance **dist(X, C)** = |x1 – x2| + |y1 – y2|\n\n**ITERATION 1**"},{"metadata":{},"cell_type":"markdown","source":"|Data #\t|sepal length in cm\t|sepal width in cm\t|Dist. from C1\t|Dist. from C2\t|Dist. from C3\t|Cluster tagged|\n|---|-------|-------|-------|-------|------|---|\n|1\t|6.3\t|3.3\t|0.00\t|1.00\t|0.60\t|C1|\n|2\t|5.8\t|2.7\t|1.10\t|0.10\t|1.70\t|C2|\n|3\t|7.1\t|3.0\t|0.50\t|1.50\t|0.10\t|C3|\n|4\t|5.1\t|3.5\t|1.00\t|0.00\t|1.60\t|C2|\n|5\t|4.9\t|3.0\t|1.70\t|0.70\t|2.30\t|C2|\n|6\t|4.7\t|3.2\t|1.70\t|0.70\t|2.30\t|C2|\n|7\t|4.6\t|3.1\t|1.90\t|0.90\t|2.50\t|C2|\n|8\t|7.0\t|3.2\t|0.60\t|1.60\t|0.00\t|C3|\n|9\t|6.4\t|3.2\t|0.00\t|1.00\t|0.60\t|C1|\n|10\t|6.9\t|3.1\t|0.40\t|1.40\t|0.20\t|C3|\n|11\t|5.5\t|2.3\t|1.80\t|0.80\t|2.40\t|C2|"},{"metadata":{},"cell_type":"markdown","source":"### Step 5) Now calculate new centroid location based on the 'Cluster tagged'"},{"metadata":{},"cell_type":"markdown","source":"Formula for calculating new centroids:\n$$C_{x}=(\\frac{1}{n})\\sum_{i=1}^{n}x_{i}$$    $$C_{y}=(\\frac{1}{n})\\sum_{i=1}^{n}y_{i}$$"},{"metadata":{},"cell_type":"markdown","source":"##### New Centroids are:\n$C_{1}$ = (6.35, 3.25) <br>\n$C_{2}$ = (5.1, 2.97) <br>\n$C_{3}$ = (7.0, 3.1) <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nx = [6.3, 5.8, 7.1, 5.1, 4.9, 4.7, 4.6, 7, 6.4, 6.9, 5.5]\ny = [3.3, 2.7, 3, 3.5, 3, 3.2, 3.1, 3.2, 3.2, 3.1, 2.3]\n\ncx = [6.35, 5.1, 7]\ncy = [3.25, 2.97, 3.1]\n\nplt.scatter(x,y)\nplt.scatter(cx,cy,label='New Centroids',color='red',marker='1')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 6) Iterate Step 4 and Step 5 till there is no change to the Centroid"},{"metadata":{},"cell_type":"markdown","source":"In this example, there is no change to the Centroid in the 2nd Iteration, hence we can assume this to be the most optimum Centroid\n\n**ITERATION 2**"},{"metadata":{},"cell_type":"markdown","source":"|Data #\t|sepal length in cm\t|sepal width in cm\t|Dist. from C1\t|Dist. from C2\t|Dist. from C3\t|Cluster tagged\t|\n|---|---|---|---|---|---|---|\n|\t1\t|\t6.3\t|\t3.3\t|\t0.0\t|\t1.5\t|\t0.5\t|\tC1\t|\n|\t9\t|\t6.4\t|\t3.2\t|\t0.0\t|\t1.5\t|\t0.5\t|\tC1\t|\n|\t2\t|\t5.8\t|\t2.7\t|\t1.1\t|\t0.4\t|\t1.6\t|\tC2\t|\n|\t4\t|\t5.1\t|\t3.5\t|\t1.0\t|\t0.5\t|\t1.5\t|\tC2\t|\n|\t5\t|\t4.9\t|\t3\t|\t1.7\t|\t0.2\t|\t2.2\t|\tC2\t|\n|\t6\t|\t4.7\t|\t3.2\t|\t1.7\t|\t0.2\t|\t2.2\t|\tC2\t|\n|\t7\t|\t4.6\t|\t3.1\t|\t1.9\t|\t0.4\t|\t2.4\t|\tC2\t|\n|\t11\t|\t5.5\t|\t2.3\t|\t1.8\t|\t0.3\t|\t2.3\t|\tC2\t|\n|\t3\t|\t7.1\t|\t3\t|\t0.5\t|\t2.0\t|\t0.0\t|\tC3\t|\n|\t8\t|\t7\t|\t3.2\t|\t0.6\t|\t2.1\t|\t0.1\t|\tC3\t|\n|\t10\t|\t6.9\t|\t3.1\t|\t0.4\t|\t1.9\t|\t0.1\t|\tC3\t|"},{"metadata":{},"cell_type":"markdown","source":"##### Let's see pictorial representation of the above formed clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = [6.3, 6.4]\ny1 = [3.3, 3.2]\nx2 = [5.8, 5.1, 4.9, 4.7, 4.6, 5.5]\ny2 = [2.7, 3.5, 3, 3.2, 3.1, 2.3]\nx3 = [7.1, 7, 6.9]\ny3 = [3, 3.2, 3.1]\nplt.scatter(x1,y1,color='r')\nplt.scatter(x2,y2,color='b')\nplt.scatter(x3,y3,color='g')\nplt.scatter(cx,cy,label='Centroids',color='red',marker='1')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Let's compare above plot with the Pictorial representation of the Original Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = [6.3, 5.8, 7.1]\ny1 = [3.3, 2.7, 3]\nx2 = [5.1, 4.9, 4.7, 4.6]\ny2 = [3.5, 3, 3.2, 3.1]\nx3 = [7, 6.4, 6.9, 5.5]\ny3 = [3.2, 3.2, 3.1, 2.3]\nplt.scatter(x1,y1,color='r')\nplt.scatter(x2,y2,color='b')\nplt.scatter(x3,y3,color='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmm, our KMeans exercise seems to have fit the objects relatively accurate :)\n\nThis concludes the Tutorial for K-Means clustering"},{"metadata":{},"cell_type":"markdown","source":"-----"},{"metadata":{},"cell_type":"markdown","source":"#### Let's take a look at the newly created clusters vis-a-vis Actual Class as per the original data"},{"metadata":{},"cell_type":"markdown","source":"|\tCluster tagged\t|\tActual Class\t|\n|---|---|\n|\tC1\t|\tIris-virginica\t|\n|\tC1\t|\tIris-versicolor\t|\n|\tC2\t|\tIris-virginica\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-setosa\t|\n|\tC2\t|\tIris-versicolor\t|\n|\tC3\t|\tIris-virginica\t|\n|\tC3\t|\tIris-versicolor\t|\n|\tC3\t|\tIris-versicolor\t|"},{"metadata":{},"cell_type":"markdown","source":"As you can see, there are some discrepencies. The two main reason for this discrepency is:<br>\n**1)Inadequate Features:** We have considered only Sepal Length and Width and have not taken into consideration Petal Length & Width <br>\n**2)Low Sample:** We have considered very few Data Points <br>\n\nAbove pointers are a good learing lessons and stresses on the point  how critical is to identify right number of Features and Samples"},{"metadata":{},"cell_type":"markdown","source":"----"},{"metadata":{},"cell_type":"markdown","source":"# Python Exercise:"},{"metadata":{},"cell_type":"markdown","source":"## Let's use KMeans package from Scikit learn to perform this exercise on entire Iris dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#to read and format the Iris file\nimport pandas as pd \n\n#package to perform Kmeans algorithm\nfrom sklearn.cluster import KMeans \n\n#For numerical functions\nimport numpy as np\n\n#for graphs\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read the Iris dataset\n\ndata = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add headers to the file\nattributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\ndata.columns = attributes\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting class names (string) to numbers which will aid us in plotting\ndata['class-num'] = data['class'].map( {'Iris-setosa': 0,'Iris-versicolor': 1,'Iris-virginica': 2} )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define the features to be used for the algo i.e. remove column 'class' which is not required for the algo\nX = data.drop(columns=[\"class\",\"class-num\"])\n\n#Let's see the first five records in the file\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the data characteristic\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next Step is to identify optimum number of clusters (k)\nHere we use **Elbow Method** *(clear elbow formed at the optimum cluster)* \n\nWe plot the graph of Within-Cluster-Sum-of-Squares **(WCSS)** against the Total Number of clusters to view formation of elbow to arrive at the optimum number of cluster **k** \n\n**Idea is** - as the number of Clusters increases, sum of distance between the objects and it's respective clusters reduces *(objects get closer to their respective Cluster)*. But hey, we don't want to overfit either...correct. Hence the quest for optimum **k**"},{"metadata":{},"cell_type":"markdown","source":"WCSS is computed as:<br>\n$$WCSS=\\sum_{i=1}^k(\\sum_{j=1}^n (X_{j}-C_{i})^2)$$  <br>\n*where:* <br>\n*$C_{k}$ is centroid for observation $X_{i}$* <br>\n*k is number of centroids* <br>\n*n is number of objects within respective centroid* <br>"},{"metadata":{},"cell_type":"markdown","source":"### Some of the important parameters used in KMeans Functions:\n\nWe can define predefined number of clusters/centroids using the **n_clusters** parameter. Initial selection of the cluster centers (coordinates) can be done using various techniques using **init** parameter. Options are:<br>\n*‘k-means++’* : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence<br>\n*‘random’*: choose k observations (rows) at random from data for the initial centroids.<br><br>\nWe can also decide the number of time the k-means algorithm will be run with *different centroid seeds* using **n_init** parameter. The final results will be the best output of n_init consecutive runs in terms of inertia.<br><br>\n**max_iter** - Maximum number of iterations of the k-means algorithm for a single run.<br>\n**random_state** - Determines random number generation for centroid initialization. Use an int to make the randomness deterministic."},{"metadata":{"trusted":true},"cell_type":"code","source":"wcss = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 10), wcss)\nplt.title('Elbow-Method using WCSS')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Within-Cluster-Sum-of-Squares')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from above plot that prominent elbows are formed between at 2, 3 and 4<br>\nNoticeably, WCSS reduces marginally below 3 and hence we select 3 as optimum **k**"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\nkmeans.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia = kmeans.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"Clusters\"] = inertia\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's plot the data based on cluster created using KMeans algo"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = plt.axes(projection=\"3d\")\nax.scatter3D(X['sepal_length'],X['sepal_width'],X['petal_length'],c=X['Clusters'],cmap='hsv')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's compare the above plot with Original data (class)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = plt.axes(projection=\"3d\")\nax.scatter3D(data['sepal_length'],data['sepal_width'],data['petal_length'],c=data['class-num'],cmap='hsv')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the visualization of the Iris data on SandDance *(pretty cool image)*\n![](https://raw.githubusercontent.com/nadarsubash/articles/master/SandDance-Iris.jpg)"},{"metadata":{},"cell_type":"markdown","source":"### KMeans Cluster has been able to largely group the points accurately...well almost. This is a good begining!\n#### ....and the end of this Tutorial with Hands-on Exercise!"}],"metadata":{"colab":{"name":"Clustering - KMeans - Tutorial.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}