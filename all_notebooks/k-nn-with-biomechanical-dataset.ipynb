{"cells":[{"metadata":{},"cell_type":"markdown","source":"# INTRODUCTION\n\nIn this kernel, I implement a k-NN algorithm to classify dataset.\n\n    * What is KNN ?\n    The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n\nThe KNN Algorithm\n  \n   1. Load the data\n   2. Initialize K to your chosen number of neighbors\n\n3. For each example in the data\n\n    3.1 Calculate the distance between the query example and the current example from the data.\n\n    3.2 Add the distance and the index of the example to an ordered collection\n\n4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n\n5. Pick the first K entries from the sorted collection\n\n6. Get the labels of the selected K entries\n\n7. If regression, return the mean of the K labels\n\n8. If classification, return the mode of the K labels"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our class type shouldn't be the object. So, I changed it to integer."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['class'] = [1 if each == 'Normal' else 0 for each in data['class']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to define what is x and what is y ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.iloc[:,0:6]\ny = data.iloc[:,6].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalization of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (x - np.mean(x)) / (np.max(x) - np.min(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I'm going to seperate the train set and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is initialization of k-NN and calling the fit function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{}-nn score : {}\".format(3, knn.score(x_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k = 3 gives us 0.77 accuracy but we can use the different k values to find optimum."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\nfor each in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    sc = knn2.score(x_test, y_test)\n    scores.append(sc)\n    \nplt.plot(range(1, 20), scores, color = 'purple')\nplt.xlabel('k values')\nplt.ylabel('Scores')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this graph, we can see the k = 15 gives us best score in this data."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}