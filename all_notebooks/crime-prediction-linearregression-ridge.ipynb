{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # graphical display\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as MSE\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.0 - Import dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"all_data = pd.read_csv('../input/real-estate-dataset/data.csv')\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 - Separate train and test set. Test set will be used to compute predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df, y_train, y_test = train_test_split(all_data.drop(columns='CRIM'), all_data['CRIM'], test_size=0.21, random_state=0)\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.0 - Investigate quality and content of data"},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 - Data Quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Dataframe Infos\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Dataset is composed of {} rows and {} columns'.format(train_df.shape[0], train_df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Column RM contains missing values. This will be addressed later"},{"metadata":{},"cell_type":"markdown","source":"#### 2.2 - Data Content"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=7, figsize=(20,7))\n\nfor col, ax in zip(train_df.columns, axes.flatten()):\n    print(col)\n    sns.distplot(train_df[col], ax=ax,kde_kws = {'bw' : 10}) #kde_kws = {'bw' : 10} manually added to prevent \"Selected KDE bandwidth is 0.\" error\n    ax.set_title(col)\n    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.0 - Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 - Normal Distribution"},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.1 - Let's first evaluate the R^2 value between a given column and the CRIM target variable. Do a log transform provide better relationship?"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlinMod = []\n\nfor col in train_df.columns.drop(['CHAS']):\n    \n    #Simple model\n    X = train_df[col].fillna(value=train_df[col].median()).values\n    lr.fit(X[:,np.newaxis],y_train)\n    score_s=lr.score(X[:,np.newaxis], y_train)\n    \n    #only logarithm\n    X_log = np.log1p(X)\n    lr.fit(X_log[:,np.newaxis], y_train)\n    score_l=lr.score(X_log[:,np.newaxis], y_train)\n    \n    linMod.append({\n        'simple': score_s,\n        'log': score_l,\n    })\n    \nlinMod = pd.DataFrame(linMod)\nlinMod['features'] = train_df.columns.drop(['CHAS'])\nlinMod.sort_values(by='simple', ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.2 - Graphically show R^2 results"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(np.arange(linMod.shape[0]), linMod['simple'], color='C0', alpha =0.5, s=75, label='Simple model')\nplt.scatter(np.arange(linMod.shape[0]), linMod['log'], color='C1',alpha =0.5, s=75,label='log1p(feature)')\nplt.xticks(np.arange(linMod.shape[0]), linMod['features'], rotation=90)\nplt.ylabel('R^2 score')\nplt.legend()\nplt.title('Comparison between [x vs. y] and [log1p(x) vs. y]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here I extract the features that seem to benefit from a log transformation.\n\ncol_lg = linMod.loc[linMod['log']>linMod['simple'], 'features']\ncol_lg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.3 - Confirm results"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's try, in fact, to calculate the MAE of a LinearModel\n\n# Take the columns \"as they are\"\ndf_simple = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_simple[c].fillna(df_simple[c].median(), inplace=True)\n    \n# Take the log1p(x) only for the columns where the log1p(x) has a higher R^2 score\ndf_log = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_log[c].fillna(df_log[c].median(), inplace=True)\nfor c in col_lg:\n    df_log[c] = np.log1p(df_log[c])\n    \n# Take the log1p(x) for all columns\ndf_alllog = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_alllog[c].fillna(df_alllog[c].median(), inplace=True)\n    df_alllog[c] = np.log1p(df_alllog[c])\n    \n\n# Scale the features\nscaler = StandardScaler()\nX_s = scaler.fit_transform(df_simple.values)\nX_l = scaler.fit_transform(df_log.values)\nX_al = scaler.fit_transform(df_alllog.values)\n#y = scaler.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\n\n#simple model\nlr.fit(X_s, y_train)\nprint('MSE for simple model: {:.2f}'.format(MSE(y_train, lr.predict(X_s))))\n\n#log model\nlr.fit(X_l, y_train)\nprint('MSE for log model: {:.2f}'.format(MSE(y_train, lr.predict(X_l))))\n\n#ALL log model\nlr.fit(X_al, y_train)\nprint('MSE for ALL log model: {:.2f}'.format(MSE(y_train, lr.predict(X_al))))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 - Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.1 - Column ZN contains several zeros"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I add a binary column for the features RAD, ZN and TAX\n\ncont_col = train_df.drop(columns=['CHAS']).columns\n\n# Take the log1p(x) only for the columns where the log1p(x) has a higher R^2 score\ndf_log = train_df.drop(columns=['CHAS']).copy() \nfor c in train_df.columns.drop(['CHAS']):\n    df_log[c].fillna(df_log[c].median(), inplace=True)\nfor c in col_lg:\n    df_log[c] = np.log1p(df_log[c])\n    \ndf_log['ZN_binary'] = [1 if x>0 else 0 for x in df_log['ZN']]\ndf_log['RAD_binary'] = [1 if x>20 else 0 for x in df_log['RAD']]\ndf_log['TAX_binary'] = [1 if x>600 else 0 for x in df_log['TAX']]\n\nX_l = scaler.fit_transform(df_log.values)\n\n#log model\nlr.fit(X_l, y_train)\nprint('MSE for log model after binary addition: {:.2f}'.format(MSE(y_train, lr.predict(X_l))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.2 - Add Polynomial Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add polynomial features to continuous columns\n\nfor c in cont_col:\n    for d in [0.5, 2, 3]:\n        name = '{}**{}'.format(c, d)\n        df_log[name] = df_log[c]**d\n        \nX_l = scaler.fit_transform(df_log.values)\n\n#log model\nlr.fit(X_l, y_train)\nprint('MSE for log model after polynomial feature: {:.2f}'.format(MSE(y_train, lr.predict(X_l))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.0 Create Preprocessing function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df1, df2): # df1 is the dataframe to preprocess, based on df2 informations\n    \n    df1 = df1.copy() #work on a copy\n    \n    #set column names\n    cont_col = df1.drop(columns=['CHAS']).columns\n    col_lg = ['MEDV', 'NOX', 'DIS', 'RM', 'ZN']\n    \n    #compute log transform\n    for c in cont_col:\n        df1[c].fillna(df2[c].median(), inplace=True)\n    for c in col_lg:\n        df1[c] = np.log1p(df1[c])\n        \n    #Feature engineering\n    df1['ZN_binary'] = [1 if x>0 else 0 for x in df1['ZN']]\n    df1['RAD_binary'] = [1 if x>20 else 0 for x in df1['RAD']]\n    df1['TAX_binary'] = [1 if x>600 else 0 for x in df1['TAX']]\n    \n    #Polynomial features\n    for c in cont_col:\n        for d in [0.5, 2, 3]:\n            name = '{}**{}'.format(c, d)\n            df1[name] = df1[c]**d\n            \n    #One-Hot Encoding\n    df1 = pd.get_dummies(df1, dummy_na=False)\n    \n    return df1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.0 - Models\n\n### 5.1 Preprocess the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_preprocessed = preprocess(train_df, train_df)\ntest_df_preprocessed = preprocess(test_df, train_df)\n\n#ensure same columns\ntest_df_preprocessed = test_df_preprocessed.reindex(columns=train_df_preprocessed.columns, fill_value=0) #Ensure same columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ntrain_df_preproc_scaled = scaler.fit_transform(train_df_preprocessed)\ntest_df_preproc_scaled = scaler.transform(test_df_preprocessed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg = LinearRegression() #creates the object\nlinreg.fit(train_df_preproc_scaled, y_train) #fit the model using the train data rescaled\nmae_model1 = MSE(y_test, linreg.predict(test_df_preproc_scaled))\nprint('MSE Linear Regression: {:.6f}'.format(mae_model1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or alternatively:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create pipeline object\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('logreg', LinearRegression())#, ))\n])\n\n#Create Cross-Validation object\ngrid = {}\n\n#Create shufflesplit cross-validation\ngrid_cv = GridSearchCV(pipe, grid, cv=KFold(n_splits=5, shuffle=True), return_train_score=True, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the model and get the results in a DataFrame\ngrid_cv.fit(train_df_preprocessed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg_predictions = grid_cv.predict(test_df_preprocessed)\nprint('MSE on test set using Linear Regression: {:.2f}'.format(MSE(y_test, linreg_predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Ridge Regression\n#### 5.3.1 Combine Grid search and Cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create pipeline object\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('ridge', Ridge(alpha=1))#, ))\n])\n\n#Create Cross-Validation object\ngrid = {'ridge__alpha': np.logspace(-1,5, num=100)}\n\n#Create shufflesplit cross-validation\ngrid_cv = GridSearchCV(pipe, grid, cv=KFold(n_splits=5), return_train_score=True, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the model\ngrid_cv.fit(train_df_preprocessed, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compute predicitons on the test set\nridge_predictions = grid_cv.predict(test_df_preprocessed)\nprint('MSE on test set using Ridge Regression: {:.2f}'.format(MSE(y_test, ridge_predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.2 Extract best parameters and compute predictions on full dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_alpha = grid_cv.best_params_\nbest_alpha['ridge__alpha']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.4 Retrain the model using the full dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = all_data['CRIM'].values\nX = all_data.drop(columns='CRIM')\n\nX_proc = preprocess(X, X)\nscaler = StandardScaler()\nX_proc_scaled = scaler.fit_transform(X_proc.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha=best_alpha['ridge__alpha'])\nridge.fit(X_proc_scaled, target)\nridge_predictions = ridge.predict(X_proc_scaled)\nprint('MSE on test set using Ridge Regression: {:.2f}'.format(MSE(target, ridge_predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df = pd.DataFrame()\npred_df[\"CRIM\"] = all_data[\"CRIM\"]\npred_df[\"abs_err\"] = abs(all_data[\"CRIM\"] - ridge_predictions)\npred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df.to_csv('Predictions_ridge.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}