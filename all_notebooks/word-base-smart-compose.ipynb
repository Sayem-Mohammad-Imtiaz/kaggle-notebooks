{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer,GRU,LeakyReLU\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.python.keras.utils import tf_utils\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model,load_model\n\nimport unicodedata\nimport re\nimport numpy as np\nimport os\nimport time\nimport shutil\n\nimport pandas as pd\nimport numpy as np\nimport string, os \ntf.__version__","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-20T11:29:08.266771Z","iopub.execute_input":"2021-05-20T11:29:08.267176Z","iopub.status.idle":"2021-05-20T11:29:13.331842Z","shell.execute_reply.started":"2021-05-20T11:29:08.267073Z","shell.execute_reply":"2021-05-20T11:29:13.331057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('../input/hinglish-data/valid_training_sentences_n_grams_f_3_freqcount5000.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:29:16.03682Z","iopub.execute_input":"2021-05-20T11:29:16.037221Z","iopub.status.idle":"2021-05-20T11:29:16.616016Z","shell.execute_reply.started":"2021-05-20T11:29:16.037178Z","shell.execute_reply":"2021-05-20T11:29:16.615261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:29:18.462969Z","iopub.execute_input":"2021-05-20T11:29:18.463353Z","iopub.status.idle":"2021-05-20T11:29:18.468898Z","shell.execute_reply.started":"2021-05-20T11:29:18.463321Z","shell.execute_reply":"2021-05-20T11:29:18.467974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=data['sentences']#[:20000]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:29:19.016022Z","iopub.execute_input":"2021-05-20T11:29:19.016427Z","iopub.status.idle":"2021-05-20T11:29:19.021277Z","shell.execute_reply.started":"2021-05-20T11:29:19.016394Z","shell.execute_reply":"2021-05-20T11:29:19.020343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef generate_dataset():\n  \n     \n    output = []\n    for line in corpus:\n        token_list = line.split(' ')\n        for i in range(1, len(token_list)):\n            data = []\n            x_ngram =  token_list[:i+1]\n            x_ngram.insert(0, '<start>')\n            x_ngram.append('<end>')\n            y_ngram=token_list[i+1:]\n            y_ngram.insert(0, '<start>')\n            y_ngram.append('<end>')\n            data.append(' '.join(x_ngram))\n            data.append(' '.join(y_ngram))\n            output.append(data)\n    print(\"Dataset prepared with prefix and suffixes for teacher forcing technique\")\n    dummy_df = pd.DataFrame(output, columns=['input','output'])\n    return output, dummy_df \n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:29:19.62527Z","iopub.execute_input":"2021-05-20T11:29:19.625598Z","iopub.status.idle":"2021-05-20T11:29:19.632784Z","shell.execute_reply.started":"2021-05-20T11:29:19.625568Z","shell.execute_reply":"2021-05-20T11:29:19.631675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LanguageIndex():\n    def __init__(self, lang):\n        self.lang = lang\n        self.word2idx = {}\n        self.idx2word = {}\n        self.vocab = set()\n        self.create_index()\n    def create_index(self):\n        for phrase in self.lang:\n            self.vocab.update(phrase.split(' '))\n        self.vocab = sorted(self.vocab)\n        self.word2idx[\"<pad>\"] = 0\n        self.idx2word[0] = \"<pad>\"\n#         self.word2idx[\" \"]=1\n#         self.idx2word[1]=\" \"\n        for i,word in enumerate(self.vocab):\n            self.word2idx[word] = i + 1\n            self.idx2word[i+1] = word\n\ndef max_length(t):\n    return max(len(i) for i in t)\n\ndef load_dataset():\n    pairs,df = generate_dataset()\n    out_lang = LanguageIndex(sp for en, sp in pairs)\n    in_lang = LanguageIndex(en for en, sp in pairs)\n    input_data = [[in_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n    output_data = [[out_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n\n    max_length_in, max_length_out = max_length(input_data), max_length(output_data)\n    input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=max_length_in, padding=\"post\")\n    output_data = tf.keras.preprocessing.sequence.pad_sequences(output_data, maxlen=max_length_out, padding=\"post\")\n    return input_data, output_data, in_lang, out_lang, max_length_in, max_length_out, df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:29:21.680617Z","iopub.execute_input":"2021-05-20T11:29:21.680978Z","iopub.status.idle":"2021-05-20T11:29:21.691617Z","shell.execute_reply.started":"2021-05-20T11:29:21.680948Z","shell.execute_reply":"2021-05-20T11:29:21.69059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data, teacher_data, input_lang, target_lang, len_input, len_target, df  = load_dataset()\n\n\ntarget_data = [[teacher_data[n][i+1] for i in range(len(teacher_data[n])-1)] for n in range(len(teacher_data))]\ntarget_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, maxlen=len_target, padding=\"post\")\ntarget_data = target_data.reshape((target_data.shape[0], target_data.shape[1], 1))\n\n# Shuffle all of the data in unison. This training set has the longest (e.g. most complicated) data at the end,\n# so a simple Keras validation split will be problematic if not shuffled.\n\np = np.random.permutation(len(input_data))\ninput_data = input_data[p]\nteacher_data = teacher_data[p]\ntarget_data = target_data[p]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:29:26.537464Z","iopub.execute_input":"2021-05-20T11:29:26.537802Z","iopub.status.idle":"2021-05-20T11:30:41.521294Z","shell.execute_reply.started":"2021-05-20T11:29:26.537771Z","shell.execute_reply":"2021-05-20T11:30:41.520332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_target","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:49.854132Z","iopub.execute_input":"2021-05-20T11:30:49.854509Z","iopub.status.idle":"2021-05-20T11:30:49.861067Z","shell.execute_reply.started":"2021-05-20T11:30:49.854475Z","shell.execute_reply":"2021-05-20T11:30:49.859715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:52.333953Z","iopub.execute_input":"2021-05-20T11:30:52.334292Z","iopub.status.idle":"2021-05-20T11:30:52.348828Z","shell.execute_reply.started":"2021-05-20T11:30:52.334258Z","shell.execute_reply":"2021-05-20T11:30:52.348004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_lang.word2idx) \n#input_lang.word2idx","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:52.754816Z","iopub.execute_input":"2021-05-20T11:30:52.755154Z","iopub.status.idle":"2021-05-20T11:30:52.760767Z","shell.execute_reply.started":"2021-05-20T11:30:52.755101Z","shell.execute_reply":"2021-05-20T11:30:52.759762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\nBUFFER_SIZE = len(input_data)\nBATCH_SIZE = 128\nembedding_dim = 100\nunits = 512\nvocab_in_size = len(input_lang.word2idx)\nvocab_out_size = len(target_lang.word2idx)\ndf.iloc[60:65]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:55.749834Z","iopub.execute_input":"2021-05-20T11:30:55.750187Z","iopub.status.idle":"2021-05-20T11:30:55.761217Z","shell.execute_reply.started":"2021-05-20T11:30:55.750153Z","shell.execute_reply":"2021-05-20T11:30:55.760406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# # Create the Encoder layers first.\n# encoder_inputs = Input(shape=(len_input,))\n# encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n\n# # Use this if you dont need Bidirectional LSTM\n# # encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n# # encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n\n# encoder_lstm = Bidirectional(LSTM(units=units, return_sequences=True, return_state=True))\n# encoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\n# state_h = Concatenate()([fstate_h,bstate_h])\n# state_c = Concatenate()([bstate_h,bstate_c])\n# encoder_states = [state_h, state_c]\n\n\n# # Now create the Decoder layers.\n# decoder_inputs = Input(shape=(None,))\n# decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n# decoder_lstm = LSTM(units=units*2, return_sequences=True, return_state=True)\n# decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n# # Two dense layers added to this model to improve inference capabilities.\n# decoder_d1 = Dense(units)\n# decoder_d11=LeakyReLU(alpha=0.05)\n# decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n# decoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_lstm_out))))\n\n\n# # Finally, create a training model which combines the encoder and the decoder.\n# # Note that this model has three inputs:\n# model = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n\n# # We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n# # Adam is used because it's, well, the best.\n\n# model.compile(optimizer=tf.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:30:58.958066Z","iopub.execute_input":"2021-05-20T11:30:58.958482Z","iopub.status.idle":"2021-05-20T11:31:03.698199Z","shell.execute_reply.started":"2021-05-20T11:30:58.95845Z","shell.execute_reply":"2021-05-20T11:31:03.697389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create the Encoder layers first.\nencoder_inputs = Input(shape=(len_input,))\nencoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n\n# Use this if you dont need Bidirectional LSTM\n# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n\nencoder_lstm = Bidirectional(LSTM(units=units, return_sequences=True, return_state=True))\nencoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\nstate_h = Concatenate()([fstate_h,bstate_h])\nstate_c = Concatenate()([bstate_h,bstate_c])\nencoder_states = [state_h, state_c]\n\n\n# Now create the Decoder layers.\ndecoder_inputs = Input(shape=(None,))\ndecoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\ndecoder_lstm = LSTM(units=units*2, return_sequences=True, return_state=True)\ndecoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n# Two dense layers added to this model to improve inference capabilities.\ndecoder_d1 = Dense(units)\ndecoder_d2=LeakyReLU()\ndecoder_d3 = Dense(vocab_out_size, activation=\"softmax\")\ndecoder_out = decoder_d3(Dropout(rate=.2)(decoder_d2(decoder_d1(Dropout(rate=.2)(decoder_lstm_out)))))\n\n\n# Finally, create a training model which combines the encoder and the decoder.\n# Note that this model has three inputs:\nmodel = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n\n# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n# Adam is used because it's, well, the best.\n\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T11:32:08.011921Z","iopub.execute_input":"2021-05-20T11:32:08.012281Z","iopub.status.idle":"2021-05-20T11:32:10.421468Z","shell.execute_reply.started":"2021-05-20T11:32:08.012247Z","shell.execute_reply":"2021-05-20T11:32:10.42069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\n# Note, we use 20% of our data for validation.\nepochs = 5\n\nhistory = model.fit([input_data, teacher_data], target_data,\n                 batch_size= BATCH_SIZE,\n                 epochs=epochs,\n                 validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:32:41.113065Z","iopub.execute_input":"2021-05-20T14:32:41.113465Z","iopub.status.idle":"2021-05-20T15:09:41.594074Z","shell.execute_reply.started":"2021-05-20T14:32:41.113432Z","shell.execute_reply":"2021-05-20T15:09:41.593222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoder_model.save('model11/encoder_model.h5')\n# inf_model.save('model11/inf_model.h5')\n\n\n# from keras.models import model_from_json\n# from keras.models import load_model\n\n\n# inf_model_json= inf_model.to_json()\n# encoder_model_json= encoder_model.to_json()\n\n\n\n# with open(\"model11/inf_model.json\", \"w\") as json_file:\n#     json_file.write(inf_model_json)\n# with open(\"model11/encoder_model.json\", \"w\") as json_file:\n#     json_file.write(encoder_model_json)\n\n# #serialize weights to HDF5\n# inf_model.save_weights(\"model11/inf_model_weights.h5\")\n# encoder_model.save_weights(\"model11/encoder_model_weights.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-05-20T14:27:46.462932Z","iopub.execute_input":"2021-05-20T14:27:46.463293Z","iopub.status.idle":"2021-05-20T14:27:46.61651Z","shell.execute_reply.started":"2021-05-20T14:27:46.463261Z","shell.execute_reply":"2021-05-20T14:27:46.615637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the encoder model from the tensors we previously declared.\nencoder_model = Model(encoder_inputs, [encoder_out, state_h, state_c])\n\n# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n\ninf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n# We'll need to force feed the two state variables into the decoder each step.\nstate_input_h = Input(shape=(units*2,), name=\"state_input_h\")\nstate_input_c = Input(shape=(units*2,), name=\"state_input_c\")\ndecoder_res, decoder_h, decoder_c = decoder_lstm(\n    decoder_emb(inf_decoder_inputs), \n    initial_state=[state_input_h, state_input_c])\ninf_decoder_out = decoder_d3(decoder_d2(decoder_d1(decoder_res)))\ninf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n                  outputs=[inf_decoder_out, decoder_h, decoder_c])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:09:45.884315Z","iopub.execute_input":"2021-05-20T15:09:45.884642Z","iopub.status.idle":"2021-05-20T15:09:46.118819Z","shell.execute_reply.started":"2021-05-20T15:09:45.884614Z","shell.execute_reply":"2021-05-20T15:09:46.11803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##greedy approch\n\n# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\ndef translate_greedy(input_sentence, infenc_model, infmodel):\n    sv = sentence_to_vector(input_sentence, input_lang)\n    #print(sv)\n    sv = sv.reshape(1,len(sv))\n    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n    \n    i = 0\n    start_vec = target_lang.word2idx[\"<start>\"]\n    stop_vec = target_lang.word2idx[\"<end>\"]\n    \n    cur_vec = np.zeros((1,1))\n    cur_vec[0,0] = start_vec\n    cur_word = \"<start>\"\n    output_sentence = \"\"\n\n    while cur_word != \"<end>\" and i < (len_target-1):\n        i += 1\n        if cur_word != \"<start>\":\n            output_sentence = output_sentence + \" \" + cur_word\n        x_in = [cur_vec, sh, sc]\n        [nvec, sh, sc] = infmodel.predict(x=x_in)\n        cur_vec[0,0] = np.argmax(nvec[0,0])\n        cur_word = target_lang.idx2word[np.argmax(nvec[0,0])]\n    return output_sentence","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:09:48.803187Z","iopub.execute_input":"2021-05-20T15:09:48.803612Z","iopub.status.idle":"2021-05-20T15:09:48.815934Z","shell.execute_reply.started":"2021-05-20T15:09:48.803573Z","shell.execute_reply":"2021-05-20T15:09:48.813025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Beam search decoding\nimport numpy as np\n\ndef predicts(xin,infmodel,prev_p):\n    \n    \n    [v, sh, sc] = infmodel.predict(x=xin)\n    w=list((-v[0,0]).argsort()[0:3])\n    p=sorted(v[0,0],reverse=True)[0:3]\n    max_p_arg=np.argmax([i*np.log(prev_p) for i in p])\n    max_p=max([i*np.log(prev_p) for i in p])\n    \n    \n    return [np.array(w[max_p_arg]).reshape(1,1),sh,sc],max_p\n    \n\ndef Beam_search_decoder(inf,infmodel):\n    \n    start_vec = target_lang.word2idx[\"<start>\"]\n    stop_vec = target_lang.word2idx[\"<end>\"]\n    \n    cur_vec = np.zeros((1,1))\n    cur_vec[0,0] = start_vec\n    cur_word = \"<start>\"\n    x_in = [cur_vec,inf[1],inf[2]]\n    [nvec, sh, sc] = infmodel.predict(x=x_in)\n    \n    v=nvec[0][0]\n    li=[]\n    probs=[]\n    lis=[]\n\n    w1=(-v).argsort()[0]\n    p1=sorted(v,reverse=True)[0]\n    li.append([[w1],p1])\n\n    w2=(-v).argsort()[1]\n    p2=sorted(v,reverse=True)[1]\n    li.append([[w2],p2])\n\n    w3=(-v).argsort()[2]\n    p3=sorted(v,reverse=True)[2]\n    li.append([[w3],p3])\n\n    \n    \n    for j in li:\n        cur_vec[0,0]=j[0][0]\n        \n        p=j[1]\n        i=1\n        cur_word=''\n        xin=[cur_vec, sh, sc]\n       \n        while cur_word != \"<end>\" and i < (len_target-1):\n            xin,p=predicts(xin,infmodel,p)\n            \n            cur_word = target_lang.idx2word[xin[0][0,0]]\n            j[0].append(xin[0][0,0])\n            \n            \n            i+=1\n            \n            if cur_word == \"<end>\" or i >= (len_target-1):\n                probs.append(p)\n        \n        lis.append(j[0])\n    st=''\n    for i in lis[np.argmax(probs)][:-1]:\n        st=st+' '+target_lang.idx2word[i]\n            \n    return st\n        \n          \n        \n    \n\n\ndef translate_beam(input_sentence, infenc_model, infmodel):\n    sv = sentence_to_vector(input_sentence, input_lang)\n    sv = sv.reshape(1,len(sv))\n    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n    \n    lis=Beam_search_decoder([emb_out, sh, sc],infmodel)\n    return lis","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:09:53.331458Z","iopub.execute_input":"2021-05-20T15:09:53.331776Z","iopub.status.idle":"2021-05-20T15:09:53.34827Z","shell.execute_reply.started":"2021-05-20T15:09:53.331746Z","shell.execute_reply":"2021-05-20T15:09:53.347059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output is 1-D: [timesteps/words]\ndef sentence_to_vector(sentence, lang):\n\n    pre = sentence\n    vec = np.zeros(len_input)\n    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n    for i,w in enumerate(sentence_list):\n        vec[i] = w\n    return vec  \n    \n#Note that only words that we've trained the model on will be available, otherwise you'll get an error.\n\n\ntest = [\n    'here',\n  'how are',\n   'have a nice',\n   'bhai kya',\n  'kaisa h',\n    'chal',\n      'or kya',\n   'let',\n  'good',\n  'happy',\n    'Let me',\n  'Let me know if',\n    'bahut',\n      'accha'\n]\n\n\n\n        \n        \n\n\noutput = []  \nfor t in test:\n    t='<start> '+t+' <end>'\n    output.append({\"Input seq\":t.lower(), \"Pred. Seq\":translate_greedy(t.lower(), encoder_model, inf_model)})\n    #print(t)\n\nresults_df = pd.DataFrame.from_dict(output) \nresults_df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:09:56.531962Z","iopub.execute_input":"2021-05-20T15:09:56.532326Z","iopub.status.idle":"2021-05-20T15:09:59.247431Z","shell.execute_reply.started":"2021-05-20T15:09:56.532294Z","shell.execute_reply":"2021-05-20T15:09:59.246642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output is 1-D: [timesteps/words]\ndef sentence_to_vector(sentence, lang):\n\n    pre = sentence\n    vec = np.zeros(len_input)\n    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n    for i,w in enumerate(sentence_list):\n        vec[i] = w\n    return vec  \n    \n#Note that only words that we've trained the model on will be available, otherwise you'll get an error.\n\n\ntest = [\n   'here',\n  'how are',\n   'have a nice',\n   'bhai kya',\n  'kaisa h',\n    'chal',\n      'or kya',\n   'let',\n  'good',\n  'happy',\n    'Let me',\n  'Let me know if',\n    'bahut',\n      'accha'\n]\n\n\n\n        \n        \n\noutput = []  \nfor t in test:\n    t='<start> '+t+' <end>'\n    output.append({\"Input seq\":t.lower(), \"Pred. Seq\":translate_beam(t.lower(), encoder_model, inf_model)})\n    #print(t)\n\nresults_df = pd.DataFrame.from_dict(output) \nresults_df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T15:10:24.143159Z","iopub.execute_input":"2021-05-20T15:10:24.143489Z","iopub.status.idle":"2021-05-20T15:10:29.957841Z","shell.execute_reply.started":"2021-05-20T15:10:24.143461Z","shell.execute_reply":"2021-05-20T15:10:29.956958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.models import model_from_json\n# from keras.models import load_model\n\n\n# inf_model_json= inf_model.to_json()\n# encoder_model_json= encoder_model.to_json()\n\n\n\n# with open(\"inf_model.json\", \"w\") as json_file:\n#     json_file.write(inf_model_json)\n# with open(\"encoder_model.json\", \"w\") as json_file:\n#     json_file.write(encoder_model_json)\n\n# #serialize weights to HDF5\n# inf_model.save_weights(\"inf_model_weights.h5\")\n# encoder_model.save_weights(\"encoder_model_weights.h5\")\n\n# encoder_model.save('encoder_model.h5')\n# inf_model.save('inf_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## using GRU model\n\n# Create the Encoder layers first.\nencoder_inputs = Input(shape=(len_input,))\nencoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n\n# Use this if you dont need Bidirectional LSTM\n# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n\nencoder_lstm = Bidirectional(LSTM(units=units, return_sequences=True, return_state=True))\nencoder_out, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm(encoder_emb(encoder_inputs))\nstate_h = Concatenate()([fstate_h,bstate_h])\nstate_c = Concatenate()([bstate_h,bstate_c])\nencoder_states = [state_h, state_c]\n\n\n# Now create the Decoder layers.\ndecoder_inputs = Input(shape=(None,))\ndecoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\ndecoder_lstm = LSTM(units=units*2, return_sequences=True, return_state=True)\ndecoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n# Two dense layers added to this model to improve inference capabilities.\ndecoder_d1 = Dense(units, activation=\"relu\")\ndecoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\ndecoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_lstm_out))))\n\n\n# Finally, create a training model which combines the encoder and the decoder.\n# Note that this model has three inputs:\nmodel = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n\n# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n# Adam is used because it's, well, the best.\n\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T02:18:49.669918Z","iopub.execute_input":"2021-05-20T02:18:49.670316Z","iopub.status.idle":"2021-05-20T02:18:50.326408Z","shell.execute_reply.started":"2021-05-20T02:18:49.670292Z","shell.execute_reply":"2021-05-20T02:18:50.325242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create the Encoder layers first.\nencoder_inputs = Input(shape=(len_input,))\nencoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n\n# Use this if you dont need Bidirectional LSTM\n# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n\n\nencoder_lstm = GRU(units=units, return_sequences=True, return_state=True)\nencoder_out, state_h= encoder_lstm(encoder_emb(encoder_inputs))\n# state_h = Concatenate()([fstate_h,bstate_h])\n# state_c = Concatenate()([bstate_h,bstate_c])\nencoder_states = state_h\n\n\n# Now create the Decoder layers.\ndecoder_inputs = Input(shape=(None,))\ndecoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\ndecoder_lstm = GRU(units=units, return_sequences=True, return_state=True)\ndecoder_lstm_out, _= decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n# Two dense layers added to this model to improve inference capabilities.\ndecoder_d1 = Dense(units, activation=\"relu\")\ndecoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\ndecoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_lstm_out))))\n\n\n# Finally, create a training model which combines the encoder and the decoder.\n# Note that this model has three inputs:\nmodel = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n\n# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n# Adam is used because it's, well, the best.\n\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T05:08:56.522914Z","iopub.execute_input":"2021-05-20T05:08:56.523307Z","iopub.status.idle":"2021-05-20T05:08:57.179368Z","shell.execute_reply.started":"2021-05-20T05:08:56.523275Z","shell.execute_reply":"2021-05-20T05:08:57.178475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\n# Note, we use 20% of our data for validation.\nepochs = 1\n\nhistory = model.fit([input_data, teacher_data], target_data,\n                 batch_size= BATCH_SIZE,\n                 epochs=epochs,\n                 validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T05:09:35.132069Z","iopub.execute_input":"2021-05-20T05:09:35.132686Z","iopub.status.idle":"2021-05-20T05:11:41.91185Z","shell.execute_reply.started":"2021-05-20T05:09:35.132651Z","shell.execute_reply":"2021-05-20T05:11:41.910913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the encoder model from the tensors we previously declared.\nencoder_model = Model(encoder_inputs, [encoder_out, state_h])\n\n# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n\ninf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n# We'll need to force feed the two state variables into the decoder each step.\nstate_input_h = Input(shape=(units,), name=\"state_input_h\")\n#state_input_c = Input(shape=(units*2,), name=\"state_input_c\")\ndecoder_res, decoder_h = decoder_lstm(\n    decoder_emb(inf_decoder_inputs), \n    initial_state=state_input_h)\ninf_decoder_out = decoder_d2(decoder_d1(decoder_res))\ninf_model = Model(inputs=[inf_decoder_inputs, state_input_h], \n                  outputs=[inf_decoder_out, decoder_h])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T05:12:37.037095Z","iopub.execute_input":"2021-05-20T05:12:37.037669Z","iopub.status.idle":"2021-05-20T05:12:37.278226Z","shell.execute_reply.started":"2021-05-20T05:12:37.037625Z","shell.execute_reply":"2021-05-20T05:12:37.27735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##greedy approch\n\n# Given an input string, an encoder model (infenc_model) and a decoder model (infmodel),\ndef translate_greedy(input_sentence, infenc_model, infmodel):\n    sv = sentence_to_vector(input_sentence, input_lang)\n    #print(sv)\n    sv = sv.reshape(1,len(sv))\n  \n    [emb_out, sh] = infenc_model.predict(x=sv)\n    \n    i = 0\n    start_vec = target_lang.word2idx[\"<start>\"]\n    stop_vec = target_lang.word2idx[\"<end>\"]\n    \n    cur_vec = np.zeros((1,1))\n    cur_vec[0,0] = start_vec\n    cur_word = \"<start>\"\n    output_sentence = \"\"\n\n    while cur_word != \"<end>\" and i < (len_target-1):\n        i += 1\n        if cur_word != \"<start>\":\n            output_sentence = output_sentence + \" \" + cur_word\n        x_in = [cur_vec, sh]\n        [nvec, sh] = infmodel.predict(x=x_in)\n        cur_vec[0,0] = np.argmax(nvec[0,0])\n        cur_word = target_lang.idx2word[np.argmax(nvec[0,0])]\n    return output_sentence","metadata":{"execution":{"iopub.status.busy":"2021-05-20T05:14:39.106315Z","iopub.execute_input":"2021-05-20T05:14:39.106674Z","iopub.status.idle":"2021-05-20T05:14:39.114595Z","shell.execute_reply.started":"2021-05-20T05:14:39.106645Z","shell.execute_reply":"2021-05-20T05:14:39.113746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Output is 1-D: [timesteps/words]\ndef sentence_to_vector(sentence, lang):\n\n    pre = sentence\n    #pre = '<start> ' + pre + ' <end>'\n    vec = np.zeros(len_input)\n    sentence_list = [lang.word2idx[s] for s in pre.split(' ')]\n    for i,w in enumerate(sentence_list):\n        vec[i] = w\n    return vec  \n    \n#Note that only words that we've trained the model on will be available, otherwise you'll get an error.\n\n\ntest = [\n    'here',\n  'how are',\n   'have a nice',\n   'bhai kya',\n  'kaisa ho',\n    'chal fir',\n      'or kya',\n   'ok',\n  'good',\n  'than',\n    'Let me',\n  'Let me know if',\n    'by',\n      'kl'\n]\n\n\n\n        \n        \n\n\noutput = []  \nfor t in test:\n    \n    output.append({\"Input seq\":t.lower(), \"Pred. Seq\":translate_greedy(t.lower(), encoder_model, inf_model)})\n    #print(t)\n\nresults_df = pd.DataFrame.from_dict(output) \nresults_df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T05:14:44.30216Z","iopub.execute_input":"2021-05-20T05:14:44.302535Z","iopub.status.idle":"2021-05-20T05:14:47.162045Z","shell.execute_reply.started":"2021-05-20T05:14:44.302505Z","shell.execute_reply":"2021-05-20T05:14:47.161305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**for bidirectional GRU**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create the Encoder layers first.\nencoder_inputs = Input(shape=(len_input,))\nencoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n\n# Use this if you dont need Bidirectional LSTM\n# encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n# encoder_out, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n\n\nencoder_lstm = GRU(units=units, return_sequences=True, return_state=True)\nencoder_out, state_h= encoder_lstm(encoder_emb(encoder_inputs))\n# state_h = Concatenate()([fstate_h,bstate_h])\n# state_c = Concatenate()([bstate_h,bstate_c])\nencoder_states = state_h\n\n\n# Now create the Decoder layers.\ndecoder_inputs = Input(shape=(None,))\ndecoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\ndecoder_lstm = GRU(units=units, return_sequences=True, return_state=True)\ndecoder_lstm_out, _= decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n# Two dense layers added to this model to improve inference capabilities.\ndecoder_d1 = Dense(units, activation=\"relu\")\ndecoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\ndecoder_out = decoder_d2(Dropout(rate=.2)(decoder_d1(Dropout(rate=.2)(decoder_lstm_out))))\n\n\n# Finally, create a training model which combines the encoder and the decoder.\n# Note that this model has three inputs:\nmodel = Model(inputs = [encoder_inputs, decoder_inputs], outputs= decoder_out)\n\n# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n# Adam is used because it's, well, the best.\n\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=['sparse_categorical_accuracy'])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]}]}