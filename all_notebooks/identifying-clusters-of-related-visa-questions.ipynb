{"cells":[{"metadata":{"_uuid":"f5a7934d3bebf6c3854f2864fdcd9d0a1bae4db9"},"cell_type":"markdown","source":"[](http://)**This is a fork from an original kernel on the StackOverflow questions, applied to Visa Questions database (that you can find in my profile)**"},{"metadata":{"_cell_guid":"527f8013-12fd-1810-0fb3-b9786b824131","_uuid":"491f663472161624dd1a318ce2e85657535f0b62"},"cell_type":"markdown","source":"Use LSA to identify related questions"},{"metadata":{"_uuid":"4fe2bd7380de07edbd9e07a8434fde0ec638018d"},"cell_type":"markdown","source":"resources on LSA : http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/\n\n**Why is LSA?**\n\n*Latent Semantic Analysis is a **technique for creating a vector representation of a document.** Having a vector representation of a document gives you a way to compare documents for their similarity by calculating the distance between the vectors. This in turn means you can do handy things like classifying documents to determine which of a set of known topics they most likely belong to.*\n\n**What is tf-idf?**\n\n*term frequency-inverse document frequency, or tf-idf for short.\ntf-idf is pretty simple and I won’t go into it here, but the gist of it is that **each position in the vector corresponds to a different word, and you represent a document by counting the number of times each word appears.** Additionally, you normalize each of the word counts by the frequency of that word in your overall document collection, to give less frequent terms more weight.*\n\n**How does LSA work?**\n\n*LSA is quite simple, you just use SVD to perform **dimensionality reduction on the tf-idf vectors**–that’s really all there is to it!*\n\n"},{"metadata":{"_cell_guid":"7475d44f-5f80-f377-a244-f893f525f36e","_uuid":"4ff2c9eac146c5f52c225c64553ebdf0e0945682","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nltk\nfrom multiprocessing import Pool\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport re\nfrom itertools import chain\nfrom collections import Counter\nimport pickle\nimport scipy.io as scio\nfrom sklearn.decomposition import TruncatedSVD\nimport scipy.spatial.distance as distance\nimport scipy.cluster.hierarchy as hierarchy\nfrom scipy.stats import pearsonr","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc53c232-7d90-b512-68e2-b252020d0c39","_uuid":"5dc1c35f8b925e004b17505016a5c9962f1b7d35","trusted":true,"scrolled":true},"cell_type":"code","source":"\"\"\"this is the data from Python Questions from StackOverflow\"\"\"\ndat = pd.read_csv(\"../input/pythonquestions/Questions.csv\", encoding='latin1')\ndat['Title'].fillna(\"None\", inplace=True)\ndat['Score'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c62a466055aee9b9fca7b0ba90735a48302c6814"},"cell_type":"code","source":"\"\"\"this is the data from Visa Questions\"\"\"\nvisa_question_data = pd.read_table(\"../input/visa-questions-by-expat-in-china/visaQuestions.txt\",header=None)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8efb7f69-23ae-129f-dbf0-82d01e8fc915","_uuid":"a89185a8f219fd13e08fc4c0d68d4be17b5ec3ca"},"cell_type":"markdown","source":"**Data look like this**\n\n\tId\tOwnerUserId\tCreationDate\tScore\tTitle\tBody\n"},{"metadata":{"trusted":true,"_uuid":"b95090363f702ebe9fa5c7ee061714fa1fa661b0"},"cell_type":"code","source":"visa_question_data.iloc(0)[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d94a6c6-e705-2b34-2bc3-170a9913f433","_uuid":"1b35227f52aa147fe8653f611f3c18d58aab9e44","trusted":true,"scrolled":true},"cell_type":"code","source":"dat.iloc(0)[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"44c5675a-af0c-c90b-27e1-cca6724378a4","_uuid":"abbd109033f4d024bf72b601bb2b845120866a76","trusted":true},"cell_type":"code","source":"# select a sample - results will improve without sampling in tf-idf caluculations, but due to\n# Kaggle kernel memory limit we have to make a compromise here.\nselected_ids = np.random.choice(range(dat.shape[0]), 10000, replace=False)\nsample = dat.loc[selected_ids, :]\nsample.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c04453b-3a7d-22b9-0506-eaf3825caae3","_uuid":"d6e400283b4411de5d2df7934590c5f8438eabcc","trusted":true,"scrolled":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23d7ad0f9f61a750a9d472b60a907095b0f263e1"},"cell_type":"markdown","source":"**DATA CLEANING**\n - purify strings\n - combine title and body"},{"metadata":{"_cell_guid":"2cd93cf7-f747-2d5e-288e-bfa95015e310","_uuid":"3447e966fcbd3fb92b32e94b227b9636ac6d0f96","trusted":true},"cell_type":"code","source":"def purify_string(html):\n    \"\"\"\n    this will apply to the sample\n    \"\"\"\n    return re.sub('(\\r\\n)+|\\r+|\\n+', \" \", re.sub('<[^<]+?>', '', html))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8d5dc1d-542d-e727-35c0-02644611bf16","_uuid":"d6fdd46f40cc6ea4f46c8747da2879325631caaa","trusted":true},"cell_type":"code","source":"corpus = sample.ix[:, 'Body'].apply(purify_string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b869a6552e2cac455006b102566fa2768f1c96c4"},"cell_type":"code","source":"visa_questions = visa_question_data.loc[:, 0].apply(purify_string)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d993582-63f1-ebf8-963f-c7f006578fcc","_uuid":"d10b94a4562f577013b4857992bfb5d7f14a08ed","trusted":true},"cell_type":"code","source":"def combine_title_body(tnb):\n    return tnb[0] + \" \" + tnb[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7d3c7af9edbb0e7472c35b9502d74a7ef45fdc6"},"cell_type":"markdown","source":"*Pool(8)* come from the multiprocessing module, [multiprocessing docs ](https://docs.python.org/2/library/multiprocessing.html)\n\n> multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. It runs on both Unix and Windows.\n\n> The multiprocessing module also introduces APIs which do not have analogs in the threading module. A prime example of this is the Pool object which offers a convenient means of parallelizing the execution of a function across multiple input values, distributing the input data across processes (data parallelism). The following example demonstrates the common practice of defining such functions in a module so that child processes can successfully import that module. This basic example of data parallelism using Pool,"},{"metadata":{"_cell_guid":"9cb4859e-eed7-c7fd-e29c-edbef8d897ee","_uuid":"11e0e5ea83fcd1e95eb452e2678b8101ab013f83","trusted":true},"cell_type":"code","source":"p = Pool(8)\ncombined_corpus = p.map(combine_title_body, zip(dat['Title'], corpus))\np.close()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc632662-b7c2-eed4-9e12-aad1c3321c38","_uuid":"dc037d3b2998a97676c2c3d7d4e3eed9804b2331","trusted":true,"scrolled":true},"cell_type":"code","source":"visa_questions_list = list(visa_questions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7267668cefda3c10fdcbe19d7fd9ca83477eaa15"},"cell_type":"markdown","source":"**Next step of cleaning is Stemming and Lemmatizing**\n\n> Stemming and lemmatization\n> For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n> \nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n> am, are, is -> be \n> car, cars, car's, cars' -> car\n\n> The result of this mapping of text will be something like:\n> \n> the boy's cars are different colors ->the boy car be differ color\n\n[source Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n\n**Stemming and Lemmatizing is applied to tokens, after Tokenizing the corpus**\n> Tokenization\n> Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. \n\n[source Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"},{"metadata":{"trusted":true,"_uuid":"1e45b6b83a1ca48c9b6f9047bc84c830fd472200"},"cell_type":"code","source":"lem = WordNetLemmatizer()\ndef cond_tokenize(t):\n    if t is None:\n        return []\n    else:\n        return [lem.lemmatize(w.lower()) for w in word_tokenize(t)]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ecbb0904-c13c-846b-eedb-0db264a1e163","_uuid":"15c04da18916fd385aff38960a158bbc505badcb","trusted":true},"cell_type":"code","source":"p = Pool(8)\ntokens = list(p.imap(cond_tokenize, combined_corpus))\np.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dc644b191b15590a10d08ddd3d57f67e3642427"},"cell_type":"code","source":"p = Pool(8)\nvisa_tokens = list(p.imap(cond_tokenize, visa_questions_list))\np.close()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a34eccce-f045-bdc5-62fc-8f46f3178837","_uuid":"3ff7af550a13c4310b17005fad5a4ac9add42450","trusted":true},"cell_type":"code","source":"# stops = stopwords.words('english')\npure_tokens = [\" \".join(sent) for sent in tokens]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e345fbd777e98d5da6f8715542bb5adcd9de27cf"},"cell_type":"code","source":"pure_visa_tokens = [\" \".join(sent) for sent in visa_tokens]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09442586-b7e0-1932-5a09-38cbfddaaab3","_uuid":"a8ab4cd7b6bc2902a6cf3d7f176ba325b557376a","trusted":true,"scrolled":true},"cell_type":"code","source":"i = 7\nprint(visa_tokens[i]) # this are the single lemmatized and stemmed tokens\nprint(\"\\n\")\nprint(pure_visa_tokens[i]) # these are the tokens combined in original form","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e956662abf3d828eb10432e0c721f90806e0e68"},"cell_type":"markdown","source":"**TFIDF section**\n\n> In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, **is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.**[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.[2]\n\n> Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n> \n> One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n\n[from wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"},{"metadata":{"_cell_guid":"2c958ef8-b773-a294-a93c-7270a3aad586","_uuid":"96103b14d336fa6f9a18c86474ee891e668886c8","trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df=1, max_features=2000, stop_words='english', ngram_range=[1, 1], sublinear_tf=True)\ntfidf = vectorizer.fit_transform(pure_visa_tokens) # this is the vector matrix of the tfidf","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"44bc1be1-05c9-dd10-171d-a9ceb56cf2d6","_uuid":"d02bc59cd5ffb46e93c7d955798f43cde230c531","trusted":true},"cell_type":"code","source":"idfs = pd.DataFrame([[v, k] for k, v in vectorizer.vocabulary_.items()], columns=['id', 'word']).sort_values('id')\nidfs['idf'] = vectorizer.idf_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"902a1e3b2775f1b20c44aae9910d5515947ed5b1"},"cell_type":"code","source":" # *this is the IDFS vector that can be used to examine how the TFIDF worked*\nprint(idfs.sort_values('idf').head(40))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ef01b61-878b-ad4b-e73a-898be872398f","_uuid":"ed1bf1d1b8237e551a3838f5e5add13a5f49d817"},"cell_type":"markdown","source":"**Compress using SVD**\n\n> SVD is used to get rid of redundant data, that is, for **dimensionality reduction.** For example, if you have two variables, one is humidity index and another one is probability of rain, then their correlation is so high, that the second one does not contribute with any additional information useful for a classification or regression task. The eigenvalues in SVD help you determine what variables are most informative, and which ones you can do without.\n\n[stackoverflow](https://stackoverflow.com/questions/9590114/importance-of-pca-or-svd-in-machine-learning)\n\n[Data Mining algorithms](https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition)"},{"metadata":{"_cell_guid":"e4891e6b-f2a9-5e26-cd26-11fbdeda2efb","_uuid":"0f2630786a321f3abaa99114ea64d6b68c6465d3","trusted":true},"cell_type":"code","source":"tsvd = TruncatedSVD(n_components=500) # TODO this n_components=500 is a hyperparameter, look into it\ntransformed = tsvd.fit_transform(tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ed499c0-49a1-70b2-42bc-a725b211e41e","_uuid":"c4c2a144adf510d8f9546eb47d7323fc8e2608a2","trusted":true,"scrolled":true},"cell_type":"code","source":"np.sum(tsvd.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c447624-1c1a-c23a-298d-95c51afcf2c9","_uuid":"3a853fa4c3ea1af83ffe67b1a40de04961c938e6","trusted":true},"cell_type":"code","source":"transformed.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7973e1a-fecb-4e0e-9b39-c74cd50d6aeb","_uuid":"91c37939cc4866fe9dfc7fa299342c15909aab86"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"bf6d53b54b0145bece63e1ba21f52869cf61a4e0"},"cell_type":"markdown","source":"**Choosing a metric**\n\nnow that we have the SVD reducted TFIDF we need to choose a text metric to actually give a score to every entry\n\n**Cosine similarity**\n\n> Cosine similarity calculates similarity by measuring the cosine of angle between two vectors. \n> With cosine similarity, we need to convert sentences into vectors. One way to do that is to use bag of words with either TF (term frequency) or TF-IDF (term frequency- inverse document frequency). The choice of TF or TF-IDF depends on application and is immaterial to how cosine similarity is actually performed — which just needs vectors. TF is good for text similarity in general, but TF-IDF is good for search query relevance.\n\nhttps://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50"},{"metadata":{"_cell_guid":"c1b4fa41-d954-b6ce-1066-710bbd4bf358","_uuid":"3723f7e484d6ac5bd7f85ef73964a5c13ec95d7b","trusted":true},"cell_type":"code","source":"# calculate pairwise cosine distance\nD = distance.pdist(transformed, 'cosine')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4aec0642543977cdb3900ffac4817fd7654b8b6"},"cell_type":"markdown","source":"**Calculate clustering with scipy.cluster**\n> scipy.cluster.hierarchy.linkage\n> scipy.cluster.hierarchy.linkage(y, method='single', metric='euclidean', optimal_ordering=False)[source]\n> Perform hierarchical/agglomerative clustering.\n> \n> The input y may be either a 1d condensed distance matrix or a 2d array of observation vectors.\n\n[scipy cluster docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)"},{"metadata":{"_cell_guid":"80f03f54-ed8c-b6d1-1995-62814b56cba9","_uuid":"72ae4ec982c03eaabd2bb7bc731908f66c2bcbfb","trusted":true},"cell_type":"code","source":"# hierarchical clustering - tree calculation\nL = hierarchy.linkage(D)\n#TODO : look into this ValueError: The condensed distance matrix must contain only finite values.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3a593e8-4a4e-3cc5-610c-b21009173774","_uuid":"87f313d706b9b8613f7113ad292cfee1f29fdb06","trusted":true,"scrolled":true},"cell_type":"code","source":"# mean distance between clusters\nnp.mean(D)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"def97dcc-d535-95d9-90ca-18b6776de25d","_uuid":"c4135a4f3e6aae259d2f3932345ba978ad5cb0be","trusted":true},"cell_type":"code","source":"# split clusters by criterion. Here 0.71 is used as the inconsistency criterion. Adjust the\n# number to change cluster sizes\n# TODO : this is the second hyperparameters, look into it\n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster\ncls = hierarchy.fcluster(L, 0.71, criterion='inconsistent')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ece1c02-6629-83b6-22c8-854b76a66f87","_uuid":"d806766fd853e0db091683831fd4eb8e92229177","trusted":true},"cell_type":"code","source":"df_cls = pd.DataFrame({'Pos': selected_ids, 'Cluster': cls})\ncnts = df_cls.groupby('Cluster').size().sort_values(ascending=False)\ncnts.sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f63a37cb-f753-8127-734d-edb0570d464d","_uuid":"720f379f4640c30452acd4126fcfcff61d09169c","trusted":true},"cell_type":"code","source":"# add clusters to question data\nbc = pd.concat([sample, df_cls.set_index('Pos')], axis=1)\nbc.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"777e9ddf-648c-6e5b-7cc3-46607489ea84","_uuid":"d3428a8ce575bf3b9e8ccb5241f1e597735f6069","trusted":true},"cell_type":"code","source":"# calculate cluster stats\nstats = bc.groupby('Cluster')['Score'].describe().unstack()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9bc0ef2b-abe4-3d91-337e-ddae9aaef2dc","_uuid":"888de8016ef55ae61bdc26a5db21ad956375aa5a","trusted":true},"cell_type":"code","source":"stats.sort_values(ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4f202ad-aa79-914b-98fb-e8b2719ba0c0","_uuid":"6fade2d8fc39af99b99c04ec6df4ecedc2de55de","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.hlines([0], xmin=0, xmax=np.max(stats['count']) + 5, alpha=0.5)\nplt.vlines([1], ymin=0, ymax=np.max(stats['mean']) + 50, alpha=0.5)\nplt.scatter(stats['count'], stats['mean'], alpha=0.3)\nplt.title(\"cluster mean score vs cluster size\")\nplt.xlabel(\"cluster size\")\nplt.ylabel(\"mean score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c762587-d933-2be2-5c81-7943347d9e27","_uuid":"f342d4d94c84aefcbff775c258d31244aaec2cda"},"cell_type":"markdown","source":"### Check if clusters make sense"},{"metadata":{"_cell_guid":"e8c6f75e-1337-1d87-337f-cb2ec17e69b7","_uuid":"d710e0d90ee4b1fc2a5f149c1a141582fefbc3e4","trusted":true},"cell_type":"code","source":"bc.loc[bc['Cluster'] == cnts.index[0]][['Score', 'Title', 'Body']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a868a0e-88e8-33d5-ba7c-336516a87364","_uuid":"492643f6f442dcaa153d1b5532ff86d12d587d09","trusted":true},"cell_type":"code","source":"bc.loc[bc['Cluster'] == cnts.index[1]][['Score', 'Title', 'Body']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4cdab38-6cd8-d4bc-ca30-63de7f74e29f","_uuid":"a59b7deb211dff46f4bbb2e4977331bb527f35df","trusted":true},"cell_type":"code","source":"bc.loc[bc['Cluster'] == cnts.index[2]][['Score', 'Title', 'Body']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5187ea92-68d2-f747-e9c1-bd16953f4c4e","_uuid":"198b2955a3baa131d04fa0a7aaaf916bc228ad21"},"cell_type":"markdown","source":"We can improve our clusters by increasing sample size, using entire dataset to calculate tf-idf, adjusting cluster splitting criterion, using non-exclusive clustering techniques etc.\n\nNext steps:\n\n 1. Use clusters and most significant words in questions to generate question tags automatically\n 2. Use an autoencoder to perform semantical hashing for better estimates of question relatedness"},{"metadata":{"_cell_guid":"d545a2ed-e2c0-5427-d564-dad9a985aefd","_uuid":"806e5916e34c41f819db4ce0568568bb6ac96547","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}