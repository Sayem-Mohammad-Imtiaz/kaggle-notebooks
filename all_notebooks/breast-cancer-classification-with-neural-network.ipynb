{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns # data visualization library ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nbreast_cancer = load_breast_cancer()\nclass_names = list(breast_cancer.target_names)\nlabel_dict ={0:\"malignant\", 1:\"benign\"}  # labels 0 and 1 correspond to class names malignant and benign\n\nX, y = breast_cancer.data, breast_cancer.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_values, counts = np.unique(y_train, return_counts=True) # returns target values 0 and 1\ndiagnosis_labels=map(lambda x:label_dict[x],target_values)# map target value to diagnosis labels\n\nplt.bar(list(diagnosis_labels),counts)  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' \nAttribute Information:\n\n1) ID number \n2) Diagnosis (M = malignant, B = benign) \n\n3-32) Ten real-valued features are computed for each cell nucleus: \n\na) radius (mean of distances from center to points on the perimeter) \nb) texture (standard deviation of gray-scale values) \nc) perimeter \nd) area \ne) smoothness (local variation in radius lengths) \nf) compactness (perimeter^2 / area - 1.0) \ng) concavity (severity of concave portions of the contour) \nh) concave points (number of concave portions of the contour) \ni) symmetry \nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed \nfor each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is \nWorst Radius.\n'''\n\nimport pandas as pd\n\nprint(breast_cancer.keys())\nprint(breast_cancer.feature_names)\n\n# String convertor to convert scientific numbers to strings to read and understand the data better\n\nexample_datapoint=pd.Series(X_train[60]).apply(lambda x: '%.3f' % x)\nprint(example_datapoint)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' \nIn the data-set there are high possibilities for some features to be correlated. PCA is essentially a method that \nreduces the dimension of the feature space in such a way that new variables are orthogonal to each other (i.e. \nthey are independent or not correlated). \n\nFrom the cancer data-set we see that it has 30 features, so letâ€™s reduce it to only 3 principal features and then \nwe can visualize the scatter plot of these new independent variables. Before applying PCA, we scale our data such \nthat each feature has unit variance. This is necessary because fitting algorithms highly depend on the scaling of \nthe features.\n'''\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler() #Instantiate the scaler\n\nscaler.fit(X_train) # Compute the mean and standard which will be used in the next command\nX_train_scaled=scaler.transform(X_train)# fit and transform can be applied together and I leave that for simple exercise\n\nscaler.fit(X_test)\nX_test_scaled=scaler.transform(X_test)\n\n# we can check the minimum and maximum of the scaled features which we expect to be 0 and 1\nprint(\"After scaling minimum\", X_train_scaled.min(axis=0))\n\npca=PCA(n_components=3)\npca.fit(X_train_scaled)\n\nX_train_pca=pca.transform(X_train_scaled)\nX_test_pca=pca.transform(X_test_scaled) \n\n#let's check the shape of X_pca array\nprint(\"shape of X_train_pca\", X_train_pca.shape)\nprint(\"shape of X_test_pca\", X_test_pca.shape)\n\nex_variance=np.var(X_train_pca,axis=0)\nex_variance_ratio = ex_variance/np.sum(ex_variance)\n# Here we see that first 2 components contributes to 86% of the total variance\nprint(ex_variance_ratio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nNow, since the PCA components are orthogonal to each other and they are not correlated, we can expect to see \nmalignant and benign classes as distinct. We plot the malignant and benign classes based on the \nfirst two principal components.\n'''\nXax=X_train_pca[:,0]\nYax=X_train_pca[:,1]\nlabels=y_train\ncdict={0:'red',1:'green'}\nlabl={0:'Malignant',1:'Benign'}\nmarker={0:'*',1:'o'}\nalpha={0:.3, 1:.5}\nfig,ax=plt.subplots(figsize=(7,5))\nfig.patch.set_facecolor('white')\nfor l in np.unique(labels):\n    ix=np.where(labels==l)\n    ax.scatter(Xax[ix],Yax[ix],c=cdict[l],s=40,\n           label=labl[l],marker=marker[l],alpha=alpha[l])\n# for loop ends\nplt.xlabel(\"First Principal Component\",fontsize=14)\nplt.ylabel(\"Second Principal Component\",fontsize=14)\nplt.legend()\nplt.show()\n# please check the scatter plot of the remaining component and you will understand the difference","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(units = 5, activation = 'relu', input_dim=3),\n    tf.keras.layers.Dense(units = 3, activation = 'relu'),\n    tf.keras.layers.Dense(units = 1, activation = 'sigmoid')\n])\n\nmodel.compile(optimizer='adam', \n              loss='mean_squared_error', \n              metrics=['accuracy'])\n\nmodel.fit(X_train_pca,y_train, batch_size = 10, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nWith a simple randomization of target values, we get a test accuracy of 46% \nWith a 3-Layered Neural Network, we get a test accuracy of 67%.\nWith the same Neural Network after Dimensionality Reduction using PCA we get a test accuracy of 92%\n'''\n\nimport random\nfrom sklearn.metrics import accuracy_score\n\ndef Rand(start, end, num): \n    res = [] \n    for j in range(num): \n        res.append(random.randint(start, end)) \n    return res \n\nmy_randoms = Rand(0,1,len(y_test))\nprint(accuracy_score(y_test, my_randoms))\nresults = model.evaluate(X_test_pca, y_test)\nprint('test loss, test acc:', results)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}