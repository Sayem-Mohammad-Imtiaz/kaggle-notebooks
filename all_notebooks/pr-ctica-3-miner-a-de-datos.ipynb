{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Análisis de la criminalidad en California, USA\n\nEste trabajo se corresponde a la **práctica 3** de la asignatura de **Minería de Datos**, sus autores son:\n\n+ Javier García Sigüenza\n+ Álvaro Navarro López-Menchero","metadata":{}},{"cell_type":"markdown","source":"## Objetivo\n\nEl objetivo es crear un modelo que pueda ayudar a determinar la importancia a asignar a un incidente, en base a la gravedad que se cree que tendrá este según una serie de datos aportados, para que se le pueda dar mayor o menor importancia. Una importancia baja corresponden a un incidente de gravedad baja, donde no se cree que se vayan a registrar heridos ni muertos. Uno de importancia media es donde se cree que se vaya a registrar heridos pero no muertos, y uno de importancia alta es donde la gravedad será alta, ya que el modelo ha predicho que habrá muertos o muertos y heridos. Como elemento extra también se ha buscado que el resultado obtenido sea explicable, para así evitar el funcionamiento de tipo caja negra. \n\nPor lo tanto el objetivo final es obtener un modelo capaz de ayudar de asistente a la hora de priorizar un incidente frente a otro en caso de falta de recursos en un cuerpo policial, a la vez que se consigue cierta explicabilidad del modelo para que este pueda ser de mayor confianza por parte del usuario.\n\nEl [dataset principal](https://www.kaggle.com/jameslko/gun-violence-data) es una recopilación de características presente en una serie de crímenes y fue filtrado para solo utilizar el Estado de **California**. Además, los datos fueron enriquecidos con otro [dataset extra](https://simplemaps.com/data/us-cities), que permitía conocer la población y densidad de población del lugar donde se cometió el crimen, así como completar datos relacionados con la latitud y longitud en algunos casos donde esta información no estaba disponible.","metadata":{}},{"cell_type":"markdown","source":"## Se importan las librerías y funciones necesarias","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport math\nfrom datetime import datetime\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:19:48.863447Z","iopub.execute_input":"2021-06-21T12:19:48.864081Z","iopub.status.idle":"2021-06-21T12:19:50.67488Z","shell.execute_reply.started":"2021-06-21T12:19:48.863926Z","shell.execute_reply":"2021-06-21T12:19:50.672507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n#  para construir gráficas y realizar análisis exploratorio de los datos\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:19:59.205546Z","iopub.execute_input":"2021-06-21T12:19:59.206196Z","iopub.status.idle":"2021-06-21T12:19:59.21414Z","shell.execute_reply.started":"2021-06-21T12:19:59.206134Z","shell.execute_reply":"2021-06-21T12:19:59.212284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Se realiza la carga de datos","metadata":{}},{"cell_type":"code","source":"data_folder = \"../input/crimen\"\nfile_name = \"gun-violence-data_01-2013_03-2018-california.csv\"\npath = os.path.join(data_folder,file_name)\ndata = pd.read_csv(path)\ndata.drop(columns = data.columns[0], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:00.796765Z","iopub.execute_input":"2021-06-21T12:20:00.797574Z","iopub.status.idle":"2021-06-21T12:20:01.568363Z","shell.execute_reply.started":"2021-06-21T12:20:00.797518Z","shell.execute_reply":"2021-06-21T12:20:01.565106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocesado de los datos","metadata":{}},{"cell_type":"code","source":"# Creamos un backup inicial\ndata_backup = data.copy()\n# Reindexamos por si hubiera algún problema con los datos cargados\ndata.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:02.741866Z","iopub.execute_input":"2021-06-21T12:20:02.742852Z","iopub.status.idle":"2021-06-21T12:20:02.758117Z","shell.execute_reply.started":"2021-06-21T12:20:02.742782Z","shell.execute_reply":"2021-06-21T12:20:02.756573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos la cantidad de datos\nn_data = len(data)\nprint(f\"Tenemos {n_data} filas de datos.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:03.03904Z","iopub.execute_input":"2021-06-21T12:20:03.040374Z","iopub.status.idle":"2021-06-21T12:20:03.050943Z","shell.execute_reply.started":"2021-06-21T12:20:03.04029Z","shell.execute_reply":"2021-06-21T12:20:03.048617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La columna **incident_characteristics** tiene mucha información relevante, pero ya que el objetivo del proyecto es conseguir un predictor de la gravedad del indicente a priori esta sería problemática, ya que proporciona información del resultado del indicente. Por ello es eliminada.","metadata":{}},{"cell_type":"code","source":"# Mostramos los datos\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:04.216436Z","iopub.execute_input":"2021-06-21T12:20:04.21697Z","iopub.status.idle":"2021-06-21T12:20:04.273467Z","shell.execute_reply.started":"2021-06-21T12:20:04.216918Z","shell.execute_reply":"2021-06-21T12:20:04.271542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Renombramos la columna **gun_stolen**, ya que este nombre se utilizará más adelante para una nueva columna.","metadata":{}},{"cell_type":"code","source":"data.rename(columns={'gun_stolen':'raw_gun_stolen'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:05.441955Z","iopub.execute_input":"2021-06-21T12:20:05.442525Z","iopub.status.idle":"2021-06-21T12:20:05.450764Z","shell.execute_reply.started":"2021-06-21T12:20:05.442472Z","shell.execute_reply":"2021-06-21T12:20:05.448737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eliminamos aquellas columnas que no vamos a utilizar","metadata":{}},{"cell_type":"code","source":"cols_remove = [\"incident_id\", \"incident_url\", \"source_url\", \"participant_age\", \"state\", \"sources\", \"incident_url_fields_missing\", \"location_description\",\"notes\", \"participant_name\", \"participant_relationship\", \"participant_status\", \"gun_type\"]\nprint(f\"Vamos a borrar {len(cols_remove)} columnas de las {len(data.columns)} que hay en el dataframe\")\ndata.drop(cols_remove, inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:05.752304Z","iopub.execute_input":"2021-06-21T12:20:05.752819Z","iopub.status.idle":"2021-06-21T12:20:05.767953Z","shell.execute_reply.started":"2021-06-21T12:20:05.752772Z","shell.execute_reply":"2021-06-21T12:20:05.766072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Ahora tenemos {len(data.columns)} columnas\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:05.907131Z","iopub.execute_input":"2021-06-21T12:20:05.907626Z","iopub.status.idle":"2021-06-21T12:20:05.916708Z","shell.execute_reply.started":"2021-06-21T12:20:05.907582Z","shell.execute_reply":"2021-06-21T12:20:05.914332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(columns = [\"incident_characteristics\"], inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:06.410387Z","iopub.execute_input":"2021-06-21T12:20:06.41085Z","iopub.status.idle":"2021-06-21T12:20:06.448596Z","shell.execute_reply.started":"2021-06-21T12:20:06.410799Z","shell.execute_reply":"2021-06-21T12:20:06.446735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cambiamos los valores NaN en **participant_gender**, **participant_age_group**, **gun_stolen** y **participant_type** por el valor \"-\".","metadata":{}},{"cell_type":"code","source":"data[\"participant_gender\"] = data[\"participant_gender\"].fillna(\"-\")\ndata[\"participant_age_group\"] = data[\"participant_age_group\"].fillna(\"-\")\ndata[\"raw_gun_stolen\"] = data[\"raw_gun_stolen\"].fillna(\"-\")\ndata[\"participant_type\"] = data[\"participant_type\"].fillna(\"-\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:06.493261Z","iopub.execute_input":"2021-06-21T12:20:06.494013Z","iopub.status.idle":"2021-06-21T12:20:06.527778Z","shell.execute_reply.started":"2021-06-21T12:20:06.493939Z","shell.execute_reply":"2021-06-21T12:20:06.525084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se va a utilizar la latitude y longitud para obtener clusteres y poder agrupar por zonas, es por ello que no podemos utilizar los casos donde no se ha definido este valor. Primero comprobamos que casos existen.","metadata":{}},{"cell_type":"code","source":"len(data[data[[\"latitude\"]].isna()[\"latitude\"]])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:06.601234Z","iopub.execute_input":"2021-06-21T12:20:06.601977Z","iopub.status.idle":"2021-06-21T12:20:06.6232Z","shell.execute_reply.started":"2021-06-21T12:20:06.601899Z","shell.execute_reply":"2021-06-21T12:20:06.620038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[data[[\"longitude\"]].isna()[\"longitude\"]])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:06.642098Z","iopub.execute_input":"2021-06-21T12:20:06.643565Z","iopub.status.idle":"2021-06-21T12:20:06.663596Z","shell.execute_reply.started":"2021-06-21T12:20:06.643439Z","shell.execute_reply":"2021-06-21T12:20:06.661578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemos ver existen el mismo caso de incidentes donde no se tiene la latitud que la longitud. En esta situación existen dos posibilidades, o eliminar estas entradas, o imputar los datos a partir de una segunda fuente de datos. Esta segunda opción fue la elegida, ya que al tener la ciudad se puede obtener unas coordenadas aproximadas.","metadata":{}},{"cell_type":"code","source":"data_population = pd.read_csv(\"../input/crimen/uscities.csv\")\ndata_population.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:06.713131Z","iopub.execute_input":"2021-06-21T12:20:06.713643Z","iopub.status.idle":"2021-06-21T12:20:07.13195Z","shell.execute_reply.started":"2021-06-21T12:20:06.713598Z","shell.execute_reply":"2021-06-21T12:20:07.13045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Filtramos los datos solo a California, ya que es el Estado que nos interesa.","metadata":{}},{"cell_type":"code","source":"data_population = data_population[data_population[\"state_name\"] == \"California\"]\ndata_population.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:07.133727Z","iopub.execute_input":"2021-06-21T12:20:07.134328Z","iopub.status.idle":"2021-06-21T12:20:07.183871Z","shell.execute_reply.started":"2021-06-21T12:20:07.134282Z","shell.execute_reply":"2021-06-21T12:20:07.181395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El conjunto de datos extra tiene otros datos interesante como son la población y la densidad de población, por lo que vamos a añadir también estos datos para enriquecer nuestro dataset.","metadata":{}},{"cell_type":"code","source":"#Eliminamos \" (county)\" para poder unir los datos mas facilmente\n#data[\"city_or_county\"] = data[\"city_or_county\"].replace({' \\(county\\)': ''}, regex=True)\n#Iteramos las filas del DataFrame con iterrows() para realizarlo de forma más obtima a través\n#del indice (index) y los datos de la fila (row)\nfor index, row in data.iterrows():\n    #Imputamos la latitud\n    if pd.isnull(row[\"latitude\"]):\n        #En ocasiones no se encuentra la ciudad y se lanza una excepción, en esos casos se deja el valor NaN\n        #y la columna será borrada a posteriori\n        try:\n            lat = data_population[data_population[\"city\"] == row[\"city_or_county\"]][\"lat\"]\n            data.at[index, \"latitude\"] = lat\n        except:\n            pass\n    #Imputamos la longitud\n    if pd.isnull(row[\"longitude\"]):\n        try:\n            lng = data_population[data_population[\"city\"] == row[\"city_or_county\"]][\"lng\"]\n            data.at[index, \"longitude\"] = lng\n        except:\n            pass\n    #Unimos los datos de población\n    try:\n        population = data_population[data_population[\"city\"] == row[\"city_or_county\"].strip()][\"population\"]\n        data.at[index, \"population\"] = population\n    except:\n        pass\n    #Unimos los datos de densidad\n    try:\n        density = data_population[data_population[\"city\"] == row[\"city_or_county\"].strip()][\"density\"]\n        data.at[index, \"density\"] = density\n    except:\n        pass\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:07.186806Z","iopub.execute_input":"2021-06-21T12:20:07.187246Z","iopub.status.idle":"2021-06-21T12:20:45.380054Z","shell.execute_reply.started":"2021-06-21T12:20:07.187202Z","shell.execute_reply":"2021-06-21T12:20:45.378428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[data[[\"population\"]].isna()[\"population\"]])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.382131Z","iopub.execute_input":"2021-06-21T12:20:45.382487Z","iopub.status.idle":"2021-06-21T12:20:45.398168Z","shell.execute_reply.started":"2021-06-21T12:20:45.38245Z","shell.execute_reply":"2021-06-21T12:20:45.396528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[data[[\"density\"]].isna()[\"density\"]])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.399646Z","iopub.execute_input":"2021-06-21T12:20:45.400046Z","iopub.status.idle":"2021-06-21T12:20:45.41611Z","shell.execute_reply.started":"2021-06-21T12:20:45.400004Z","shell.execute_reply":"2021-06-21T12:20:45.414602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[data[[\"latitude\"]].isna()[\"latitude\"]])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.417801Z","iopub.execute_input":"2021-06-21T12:20:45.418163Z","iopub.status.idle":"2021-06-21T12:20:45.438859Z","shell.execute_reply.started":"2021-06-21T12:20:45.418124Z","shell.execute_reply":"2021-06-21T12:20:45.436934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data[data[[\"longitude\"]].isna()[\"longitude\"]])","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.441621Z","iopub.execute_input":"2021-06-21T12:20:45.442132Z","iopub.status.idle":"2021-06-21T12:20:45.458891Z","shell.execute_reply.started":"2021-06-21T12:20:45.442077Z","shell.execute_reply":"2021-06-21T12:20:45.457386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.463679Z","iopub.execute_input":"2021-06-21T12:20:45.464162Z","iopub.status.idle":"2021-06-21T12:20:45.480573Z","shell.execute_reply.started":"2021-06-21T12:20:45.464114Z","shell.execute_reply":"2021-06-21T12:20:45.478826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Eliminamos todas las entradas que contengan nulos en la latitud, longitud, población o densidad de población.","metadata":{}},{"cell_type":"code","source":"data = data[data[\"latitude\"].notna()]\ndata = data[data[\"longitude\"].notna()]\ndata = data[data[\"population\"].notna()]\ndata = data[data[\"density\"].notna()]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.483397Z","iopub.execute_input":"2021-06-21T12:20:45.483777Z","iopub.status.idle":"2021-06-21T12:20:45.514425Z","shell.execute_reply.started":"2021-06-21T12:20:45.483736Z","shell.execute_reply":"2021-06-21T12:20:45.51279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.516555Z","iopub.execute_input":"2021-06-21T12:20:45.517136Z","iopub.status.idle":"2021-06-21T12:20:45.526802Z","shell.execute_reply.started":"2021-06-21T12:20:45.517067Z","shell.execute_reply":"2021-06-21T12:20:45.525049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reindexamos por si hubiera algún problema con los datos cargados\ndata.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.529279Z","iopub.execute_input":"2021-06-21T12:20:45.52976Z","iopub.status.idle":"2021-06-21T12:20:45.543299Z","shell.execute_reply.started":"2021-06-21T12:20:45.529694Z","shell.execute_reply":"2021-06-21T12:20:45.541573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A continuación se van a crear diversos flags relacionados con la edad, el sexo y elementos del crimen.","metadata":{}},{"cell_type":"code","source":"# Columnas que creamos\ndata[\"number_implicates\"] = 0\ndata[\"exist_suspect\"] = False\ndata[\"male_involved\"] = False\ndata[\"female_involved\"] = False\ndata[\"gun_stolen\"] = False\ndata[\"gun_involved\"] = False\ndata[\"child_involved\"] = False\ndata[\"teen_involved\"] = False\ndata[\"adult_involved\"] = False","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.547068Z","iopub.execute_input":"2021-06-21T12:20:45.548001Z","iopub.status.idle":"2021-06-21T12:20:45.567441Z","shell.execute_reply.started":"2021-06-21T12:20:45.547931Z","shell.execute_reply":"2021-06-21T12:20:45.565872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Añadimos campos de fecha interesantes para poder trabajar con más información","metadata":{}},{"cell_type":"code","source":"# Creamos las columnas para la información\ndata[\"year\"] = \"initial_value\"\ndata[\"id_month\"] = \"initial_value\"\ndata[\"month\"] = \"initial_value\"\ndata[\"day\"] = \"initial_value\"","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.569838Z","iopub.execute_input":"2021-06-21T12:20:45.570223Z","iopub.status.idle":"2021-06-21T12:20:45.579541Z","shell.execute_reply.started":"2021-06-21T12:20:45.570185Z","shell.execute_reply":"2021-06-21T12:20:45.578759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creamos el diccionario para pasar de id del mes al propio mes\ndict_index_month = {1: \"january\", 2: \"february\", 3: \"march\", 4: \"april\", \n                    5: \"may\", 6: \"june\", 7: \"july\", 8: \"august\", \n                    9: \"september\", 10: \"october\", 11: \"november\", 12: \"december\"}","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:45.580583Z","iopub.execute_input":"2021-06-21T12:20:45.580921Z","iopub.status.idle":"2021-06-21T12:20:45.596875Z","shell.execute_reply.started":"2021-06-21T12:20:45.580887Z","shell.execute_reply":"2021-06-21T12:20:45.594762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recorremos cada fila del dataframe \nfor index, row in data.iterrows():\n    # Cada fila es tipo str\n    date_tmp = row[\"date\"]\n    data.at[index, \"year\"] = pd.to_datetime(date_tmp, format='%Y-%m-%d').year\n    data.at[index, \"id_month\"] = pd.to_datetime(date_tmp, format='%Y-%m-%d').month\n    data.at[index, \"month\"] = dict_index_month[data.at[index, \"id_month\"]]\n    data.at[index, \"day\"] = pd.to_datetime(date_tmp, format='%Y-%m-%d').day","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-06-21T12:20:45.599483Z","iopub.execute_input":"2021-06-21T12:20:45.599998Z","iopub.status.idle":"2021-06-21T12:20:56.088446Z","shell.execute_reply.started":"2021-06-21T12:20:45.599955Z","shell.execute_reply":"2021-06-21T12:20:56.086882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[[\"date\", \"year\", \"month\", \"id_month\", \"day\"]].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:56.090484Z","iopub.execute_input":"2021-06-21T12:20:56.090908Z","iopub.status.idle":"2021-06-21T12:20:56.112614Z","shell.execute_reply.started":"2021-06-21T12:20:56.090867Z","shell.execute_reply":"2021-06-21T12:20:56.110617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primero tratamos las variables relacionadas con la edad y el sexo.","metadata":{}},{"cell_type":"code","source":"#Iteramos las filas del DataFrame con iterrows() para realizarlo de forma más obtima a través\n#del indice (index) y los datos de la fila (row)\nfor index, row in data.iterrows():\n    #Comprobamos la presencia de hombres\n    if \"male\" in row[\"participant_gender\"].lower():\n        data.at[index, \"male_involved\"] = True\n    #Comprobamos la presencia de mujeres\n    if \"female\" in row[\"participant_gender\"].lower():\n        data.at[index, \"female_involved\"] = True\n    #Comprobamos la presencia de niños\n    if \"child\" in row[\"participant_age_group\"].lower():\n        data.at[index, \"child_involved\"] = True\n    #Comprobamos la presencia de adolescentes\n    if \"teen\" in row[\"participant_age_group\"].lower():\n        data.at[index, \"teen_involved\"] = True\n    #Comprobamos la presencia de adultos\n    if \"adult\" in row[\"participant_age_group\"].lower():\n        data.at[index, \"adult_involved\"] = True\ndata[[\"participant_gender\", \"participant_age_group\", \"male_involved\", \"female_involved\", \"child_involved\", \\\n     \"teen_involved\", \"adult_involved\"]].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:56.114792Z","iopub.execute_input":"2021-06-21T12:20:56.115259Z","iopub.status.idle":"2021-06-21T12:20:58.423756Z","shell.execute_reply.started":"2021-06-21T12:20:56.115211Z","shell.execute_reply":"2021-06-21T12:20:58.421819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora tratamos las variables relacionadas con el crimen.","metadata":{}},{"cell_type":"code","source":"for index, row in data.iterrows():\n    #Comprobamos si ha habido armas implicada en el incidente\n    if row[\"raw_gun_stolen\"] != \"-\":\n        data.at[index, \"gun_involved\"] = True\n    #Comprobamos si hay armas robadas implicadas en el incidente\n    all_guns = row[\"raw_gun_stolen\"].split(\"||\")\n    stolen_gun = False\n    for gun_info in all_guns:\n        if \"stolen\" in gun_info.lower():\n            stolen_gun = True\n    if stolen_gun:\n        data.at[index, \"gun_stolen\"] = True\n    #Comprobamos el número de implicados en el incidente\n    if row[\"participant_type\"] != \"-\":\n        all_involveds = row[\"participant_type\"].split(\"||\")\n        data.at[index, \"number_implicates\"] = len(all_involveds)\n    # Comprobamos si se han identificado sospechosos\n    if \"suspect\" in row[\"participant_type\"].lower():\n        data.at[index, \"exist_suspect\"] = True\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:20:58.426047Z","iopub.execute_input":"2021-06-21T12:20:58.426438Z","iopub.status.idle":"2021-06-21T12:21:00.873005Z","shell.execute_reply.started":"2021-06-21T12:20:58.426401Z","shell.execute_reply":"2021-06-21T12:21:00.870573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[[\"gun_involved\", \"gun_stolen\", \"number_implicates\", \"exist_suspect\"]].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:00.875436Z","iopub.execute_input":"2021-06-21T12:21:00.875912Z","iopub.status.idle":"2021-06-21T12:21:00.895352Z","shell.execute_reply.started":"2021-06-21T12:21:00.875857Z","shell.execute_reply":"2021-06-21T12:21:00.893216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Se añade la variable salida\n\nSe clsifica en:\n- 0: Importancia baja -> sin heridos ni muertos\n- 1: Importancia media -> con heridos, pero sin muertos\n- 2: Importancia alta -> con muertos","metadata":{}},{"cell_type":"code","source":"def get_class(n_killed, n_injured):\n    if n_killed==0 and n_injured==0:\n        return 0\n    elif n_killed==0 and n_injured>0:\n        return 1\n    elif n_killed>0:\n        return 2\n    else:\n        raise Exception(\"Valores incorrectos tratando la variable respuesta\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:00.897508Z","iopub.execute_input":"2021-06-21T12:21:00.89801Z","iopub.status.idle":"2021-06-21T12:21:00.923053Z","shell.execute_reply.started":"2021-06-21T12:21:00.897958Z","shell.execute_reply":"2021-06-21T12:21:00.920901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Se utilizará n_killed (fallecidos) y n_injured (heridos) para asignar la variable\n# Declaramos las columnas\ny_column = \"severity\"\n# Inicializamos la clase al valor 0: sin heridos ni muertos\ndata[y_column] = 0\n# Recorremos cada fila del dataframe y asignamos el valor adecuado\nn_rows = len(data)\nfor index, row in data.iterrows():\n    y_value = get_class(row[\"n_killed\"], row[\"n_injured\"]) \n    data.at[index, y_column] = y_value\n\n# Por último, como la clase ha sido generada mediante dos columnas\n# habrá dependencia lineal, borramos dichas variables\ndata.drop([\"n_killed\", \"n_injured\"], inplace=True, axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:00.925521Z","iopub.execute_input":"2021-06-21T12:21:00.926079Z","iopub.status.idle":"2021-06-21T12:21:02.938125Z","shell.execute_reply.started":"2021-06-21T12:21:00.926018Z","shell.execute_reply":"2021-06-21T12:21:02.937104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Balanceo de los datos","metadata":{}},{"cell_type":"markdown","source":"Primero de todo, mostramos los datos relativos a cada clase para poder observar la proporción entre ellas.","metadata":{}},{"cell_type":"code","source":"# obtener algunas estadísticas sobre los datos\ncategories = sorted(data[y_column].unique(), reverse=False)\nhist= Counter(data[y_column]) \nprint(f'Total de instancias -> {data.shape[0]}')\ndistribution_classes = data[y_column].value_counts(normalize=True) * 100\nprint(f'Distribución de clases -> {distribution_classes.to_list()}')\n\nprint(f'Categorías -> {categories}')\nprint(f'Categoría del comentario -> {data[y_column][0]}')\n\nn_classes = len(categories)\nprint(f\"Tenemos {n_classes} clases en el dataset: {categories}\")\n\ncategories_names = [\"Importancia baja\",\"Importancia media\", \"Importancia alta\"]# 0, 1, 2\nprint(f\"Dichas {n_classes} clases se corresponden a: {categories_names}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:02.939499Z","iopub.execute_input":"2021-06-21T12:21:02.939842Z","iopub.status.idle":"2021-06-21T12:21:02.990399Z","shell.execute_reply.started":"2021-06-21T12:21:02.939809Z","shell.execute_reply":"2021-06-21T12:21:02.988161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gráfico de barras\nfig = go.Figure(layout=go.Layout(height=400, width=600))\nfig.add_trace(go.Bar(x=categories, y=[hist[cat] for cat in categories]))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:02.996675Z","iopub.execute_input":"2021-06-21T12:21:02.997171Z","iopub.status.idle":"2021-06-21T12:21:03.078441Z","shell.execute_reply.started":"2021-06-21T12:21:02.997122Z","shell.execute_reply":"2021-06-21T12:21:03.076602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gráfico de tarta\namounts = []\ncolors = [\"purple\", \"blue\", \"red\"]\nfor cat in categories:\n    amount_tmp = len(data.loc[data[y_column]==cat,])\n    amounts.append(amount_tmp)\nplt.pie(amounts, labels=categories_names, startangle = 90, autopct='%1.2f%%', shadow=True, colors=colors)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:03.081743Z","iopub.execute_input":"2021-06-21T12:21:03.082182Z","iopub.status.idle":"2021-06-21T12:21:03.322875Z","shell.execute_reply.started":"2021-06-21T12:21:03.082142Z","shell.execute_reply":"2021-06-21T12:21:03.320982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se observa que las proporciones de cada clase son parecidas por lo que **no es necesario realizar un balanceo explícito de los datos**.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:03.325627Z","iopub.execute_input":"2021-06-21T12:21:03.326144Z","iopub.status.idle":"2021-06-21T12:21:03.372851Z","shell.execute_reply.started":"2021-06-21T12:21:03.326092Z","shell.execute_reply":"2021-06-21T12:21:03.370037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clusters\n\nSe van a crear dos clusters diferentes, para dividir los diferentes casos en base a las áreas geográficas donde tuvieron lugar, así como en base a la densidad de población de la ciudad donde tiene lugar el incidente.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nlatitude = data[\"latitude\"].to_list()\nlongitude = data[\"longitude\"].to_list()\ncluster_data = list(zip(latitude, longitude))\nsum_of_squared_distances = []\nnum_clusters = range(1,10)\nfor k in num_clusters:\n    kmeans = KMeans(n_clusters=k)\n    kmeans = kmeans.fit(cluster_data)\n    sum_of_squared_distances.append(kmeans.inertia_)\n\nplt.plot(num_clusters, sum_of_squared_distances, 'bx-')\nplt.xlabel('Valor de k')\nplt.ylabel('Suma de las distancias al cuadrado')\nplt.title('Método del codo para buscar una k optima')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:03.375989Z","iopub.execute_input":"2021-06-21T12:21:03.376837Z","iopub.status.idle":"2021-06-21T12:21:16.768823Z","shell.execute_reply.started":"2021-06-21T12:21:03.37677Z","shell.execute_reply":"2021-06-21T12:21:16.766835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\n\nnum_clusters = range(2,10)\nfor k in num_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(cluster_data) + (k + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=k, random_state=10)\n    cluster_labels = clusterer.fit_predict(cluster_data)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(cluster_data, cluster_labels)\n    print(\"For n_clusters =\", k,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(cluster_data, cluster_labels)\n\n    y_lower = 10\n    for i in range(k):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / k)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / k)\n    ax2.scatter(np.array(cluster_data)[:, 0], np.array(cluster_data)[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % k),\n                 fontsize=14, fontweight='bold')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:21:16.772Z","iopub.execute_input":"2021-06-21T12:21:16.772591Z","iopub.status.idle":"2021-06-21T12:23:17.109264Z","shell.execute_reply.started":"2021-06-21T12:21:16.772525Z","shell.execute_reply":"2021-06-21T12:23:17.108055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede observar por ambos criterios el mejor es con k=2.","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=2)\nkmeans = kmeans.fit(cluster_data)\nkmeans.labels_\ndata[\"area_cluster\"] = kmeans.labels_","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:17.110882Z","iopub.execute_input":"2021-06-21T12:23:17.111187Z","iopub.status.idle":"2021-06-21T12:23:18.249779Z","shell.execute_reply.started":"2021-06-21T12:23:17.111159Z","shell.execute_reply":"2021-06-21T12:23:18.248706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One hot del cluster\npop_dens_cluster = pd.get_dummies(data[\"area_cluster\"])\npop_dens_cluster.columns = [\"area_0\", \"area_1\"]\ndata = pd.concat([data, pop_dens_cluster], axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:18.251229Z","iopub.execute_input":"2021-06-21T12:23:18.251554Z","iopub.status.idle":"2021-06-21T12:23:18.296477Z","shell.execute_reply.started":"2021-06-21T12:23:18.251522Z","shell.execute_reply":"2021-06-21T12:23:18.29521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a realizar un segundo cluster para la densidad de población.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nlatitude = data[\"density\"].to_list()\ncluster_data = np.reshape(np.array(latitude), (len(latitude), -1))\nsum_of_squared_distances = []\nnum_clusters = range(1,10)\nfor k in num_clusters:\n    kmeans = KMeans(n_clusters=k)\n    kmeans = kmeans.fit(cluster_data)\n    sum_of_squared_distances.append(kmeans.inertia_)\n\nplt.plot(num_clusters, sum_of_squared_distances, 'bx-')\nplt.xlabel('Valor de k')\nplt.ylabel('Suma de las distancias al cuadrado')\nplt.title('Método del codo para buscar una k optima')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:18.298157Z","iopub.execute_input":"2021-06-21T12:23:18.298494Z","iopub.status.idle":"2021-06-21T12:23:19.528981Z","shell.execute_reply.started":"2021-06-21T12:23:18.298462Z","shell.execute_reply":"2021-06-21T12:23:19.527872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El mejor número de clusters parece ser k=5.","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=5)\nkmeans = kmeans.fit(cluster_data)\nkmeans.labels_\ndata[\"pop_dens_cluster\"] = kmeans.labels_","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:19.530574Z","iopub.execute_input":"2021-06-21T12:23:19.531012Z","iopub.status.idle":"2021-06-21T12:23:19.663478Z","shell.execute_reply.started":"2021-06-21T12:23:19.530969Z","shell.execute_reply":"2021-06-21T12:23:19.662411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=data, x=\"latitude\", y=\"longitude\", hue=\"pop_dens_cluster\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:19.667455Z","iopub.execute_input":"2021-06-21T12:23:19.667809Z","iopub.status.idle":"2021-06-21T12:23:20.44516Z","shell.execute_reply.started":"2021-06-21T12:23:19.667777Z","shell.execute_reply":"2021-06-21T12:23:20.44389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#One hot del cluster\npop_dens_cluster = pd.get_dummies(data[\"pop_dens_cluster\"])\npop_dens_cluster.columns = [\"pop_dens_0\", \"pop_dens_1\", \"pop_dens_2\", \"pop_dens_3\", \"pop_dens_4\"]\ndata = pd.concat([data, pop_dens_cluster], axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.446633Z","iopub.execute_input":"2021-06-21T12:23:20.44695Z","iopub.status.idle":"2021-06-21T12:23:20.49542Z","shell.execute_reply.started":"2021-06-21T12:23:20.446921Z","shell.execute_reply":"2021-06-21T12:23:20.494144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.496854Z","iopub.execute_input":"2021-06-21T12:23:20.497153Z","iopub.status.idle":"2021-06-21T12:23:20.50438Z","shell.execute_reply.started":"2021-06-21T12:23:20.497122Z","shell.execute_reply":"2021-06-21T12:23:20.502936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Últimos pasos\n\nPor último, borramos columnas inservivbles y comprobamos la distribución por años de los datos.","metadata":{}},{"cell_type":"code","source":"# Borramos las columnas que no necesitaremos en adelante\nlist_remove_columns = [\"address\", \"congressional_district\", \"raw_gun_stolen\", \"n_guns_involved\", \"participant_age_group\", \"participant_gender\", \"participant_type\", \"state_house_district\", \"state_senate_district\", ]\ndata.drop(columns = list_remove_columns, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.506331Z","iopub.execute_input":"2021-06-21T12:23:20.506794Z","iopub.status.idle":"2021-06-21T12:23:20.52545Z","shell.execute_reply.started":"2021-06-21T12:23:20.506749Z","shell.execute_reply":"2021-06-21T12:23:20.523859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lista con los años\nlist_years = sorted(list(set(data[\"year\"])))\n# Guardamos la cantidad para cada año\nn_crimes = {}\nfor year in list_years:\n    n_crimes[year] = len(data.loc[data[\"year\"] == year,])\n# Creamos las variables y el dataframe\nyears = list(n_crimes.keys())\ncrimes = list(n_crimes.values())\ndf_n_crimes = pd.DataFrame({'year': years, 'n_crimes': crimes})","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.526611Z","iopub.execute_input":"2021-06-21T12:23:20.526945Z","iopub.status.idle":"2021-06-21T12:23:20.560466Z","shell.execute_reply.started":"2021-06-21T12:23:20.526911Z","shell.execute_reply":"2021-06-21T12:23:20.559384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_n_crimes","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.562005Z","iopub.execute_input":"2021-06-21T12:23:20.562283Z","iopub.status.idle":"2021-06-21T12:23:20.572681Z","shell.execute_reply.started":"2021-06-21T12:23:20.562254Z","shell.execute_reply":"2021-06-21T12:23:20.571421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se observa que apenas hay isntancias de 2013, debido a esto se procede a eliminarlos para que no estén entorpeciendo las próximas tareas.","metadata":{}},{"cell_type":"code","source":"print(f\"Inicialmente hay {len(data)} filas de información\")\n# Creamos un backup\ndata_backup = data.copy()\n# Guardamos los índices a borrar\nlist_index_2013 = list(data.loc[data[\"year\"] == 2013,].index)\n# Borramos los datos\ndata = data.drop(list_index_2013)\n# Reseteamos los índices\ndata = data.reset_index()\nprint(f\"Tras eliminar los datos de 2013 hay {len(data)} filas de información\")\nprint(f\"Se han eliminado {abs(len(data)-len(data_backup))} filas\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.574442Z","iopub.execute_input":"2021-06-21T12:23:20.574983Z","iopub.status.idle":"2021-06-21T12:23:20.60184Z","shell.execute_reply.started":"2021-06-21T12:23:20.574949Z","shell.execute_reply":"2021-06-21T12:23:20.600285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exportar","metadata":{}},{"cell_type":"code","source":"data.to_csv(\"preprocessed_data.csv\", encoding='utf-8-sig')\ndata.to_json(\"preprocessed_data.json\")","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.603251Z","iopub.execute_input":"2021-06-21T12:23:20.603538Z","iopub.status.idle":"2021-06-21T12:23:20.96315Z","shell.execute_reply.started":"2021-06-21T12:23:20.60351Z","shell.execute_reply":"2021-06-21T12:23:20.962001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualiazación\n","metadata":{}},{"cell_type":"markdown","source":"## Importamos las librerías y funciones necesarias","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.967257Z","iopub.execute_input":"2021-06-21T12:23:20.967603Z","iopub.status.idle":"2021-06-21T12:23:20.972677Z","shell.execute_reply.started":"2021-06-21T12:23:20.96757Z","shell.execute_reply":"2021-06-21T12:23:20.971295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n#  para construir gráficas y realizar análisis exploratorio de los datos\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.express as px","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.97449Z","iopub.execute_input":"2021-06-21T12:23:20.974944Z","iopub.status.idle":"2021-06-21T12:23:20.987621Z","shell.execute_reply.started":"2021-06-21T12:23:20.974898Z","shell.execute_reply":"2021-06-21T12:23:20.986307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:20.989222Z","iopub.execute_input":"2021-06-21T12:23:20.98965Z","iopub.status.idle":"2021-06-21T12:23:21.001107Z","shell.execute_reply.started":"2021-06-21T12:23:20.989612Z","shell.execute_reply":"2021-06-21T12:23:20.999842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Carga los datos","metadata":{}},{"cell_type":"code","source":"data_folder = \"\"\nfile_name = \"preprocessed_data.csv\"\npath = os.path.join(data_folder,file_name)\ndata = pd.read_csv(path)\n# Al cargar el csv se añade un índice nuevo inservible: lo eliminamos\ndata.drop(columns = data.columns[0], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:21.003881Z","iopub.execute_input":"2021-06-21T12:23:21.004385Z","iopub.status.idle":"2021-06-21T12:23:21.088157Z","shell.execute_reply.started":"2021-06-21T12:23:21.004334Z","shell.execute_reply":"2021-06-21T12:23:21.086944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T12:23:21.089817Z","iopub.execute_input":"2021-06-21T12:23:21.090215Z","iopub.status.idle":"2021-06-21T12:23:21.122216Z","shell.execute_reply.started":"2021-06-21T12:23:21.090173Z","shell.execute_reply":"2021-06-21T12:23:21.120723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Funciones auxiliares","metadata":{}},{"cell_type":"code","source":"# Funcies que dada una lista devuelve un dataframe y el diccionario que lo compone\n# Dado el dataframe y la columna\ndef get_dic_df_data_value(data, column):\n    d = {}\n    list_values = sorted(list(set(data[column])))\n    for value in list_values:\n        d[value] = len(data.loc[data[column] == value,])\n    keys = list(d.keys())\n    values = list(d.values())\n    column_name = \"n_crimes_\" + column\n    df = pd.DataFrame({column: keys, column_name: values})\n    return d, df\n# Dado el dataframe y la lista de columnas\ndef get_dic_df_data_column(data, list_columns, name):\n    d = {}\n    for value in list_columns:\n        d[value] = len(data.loc[data[value] == True,])\n    keys = list(d.keys())\n    values = list(d.values())\n    column_name = \"n_crimes_\" + name\n    df = pd.DataFrame({name: keys, column_name: values})\n    return d, df\n# Función devuelve la carpeta donde guardar las figuras\ndef check_folder(file_name, folder_name, sup_folder=\"other_data\"):\n    # sub_folder = os.path.join(folder_name, os.path.join(sup_folder, file_name)) # <- varias carpetas\n    sub_folder = folder_name\n    if not os.path.exists(sub_folder):\n        # print(f\"Creamos la carpeta: {sub_folder}\")\n        os.makedirs(sub_folder)\n    return sub_folder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Estadísticas descriptivas","metadata":{}},{"cell_type":"markdown","source":"### Datos del dataframe","metadata":{}},{"cell_type":"markdown","source":"Aquí se muestran los datos del proppio conjunto, todas las columnas y las primeras filas.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora, se muestran los tipos del conjunto de datos, en concreto, el tipo de cada columna.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En esta sección, se muestran características de las columnas, número máximo, mínimo, etc. Esto es de suma importancia para identificar columnas relevantes a la hroa de visualizar.","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede observar en esta descripción de las columnas, hay datos que no tiene sentido analizar en profundidad, por ello se seleccionan las de mayor importancia para el estudio del trabajo.","metadata":{}},{"cell_type":"markdown","source":"### Correlaciones","metadata":{}},{"cell_type":"markdown","source":"Aquí se muestra, para las columnas más relevantes, el mapa de calor con las relaciones entre las distitnas variables.","metadata":{}},{"cell_type":"code","source":"columns_sd = ['population', 'density', 'number_implicates', 'exist_suspect',\n       'male_involved', 'female_involved', 'gun_stolen', 'gun_involved',\n       'child_involved', 'teen_involved', 'adult_involved', 'year']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mostramos el mapa de calor entre las distintas variables del conjunto de datos\nplt.figure(figsize=(14,10))\nsns.heatmap(data[columns_sd].corr('spearman'), annot=True, linewidth=3)\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n# Obtenemos la carpeta donde guardar el gráfico\nsub_folder = check_folder(\"heat_map\", \"figs\")\n# Exportamos a JPG\nfig_jpg = os.path.join(sub_folder, \"heat_map\" + '.jpg')\nplt.savefig(fig_jpg)\n# Exportamos a PDF\n# fig_pdf = os.path.join(sub_folder, \"heat_map\" + '.pdf')\n# plt.savefig(fig_pdf)\n# Mostramos el plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se pueden extraer conclusioens interesantes de estas relaciones, algunas son:\n\n+ La población y la densidad están (positivamente) fuertemente relacionadas.\n+ Una fuerte conexión entre la presencia de adultos y de hombres.\n+ La presencia de adultos va ligada a que no estén presentes adolescentes o niños.\n+ El año del accidente tiene una relación muy fuerte y positiva con la presencia de armas.\n+ Relación fuerte y positiva entre el número de implicados y la detección de sospechosos.","metadata":{}},{"cell_type":"markdown","source":"Dado que la relación más importante es, a priori, la de la población y la densidad de la población, se expone aquí la relación entre ambas.","metadata":{}},{"cell_type":"code","source":"# Como la correlación de mayor interés ha sido la de población y densidad \n# la mostramos de nuevo únicamente entre ellas con relplot\nsns.relplot(x='population', y='density', data=data)\n# Obtenemos la carpeta donde guardar el gráfico\nsub_folder = check_folder(\"rel_poblation_density\", \"figs\")\n# Exportamos a JPG\nfig_jpg = os.path.join(sub_folder, \"rel_poblation_density\" + '.jpg')\nplt.savefig(fig_jpg)\n# Exportamos a PDF\n# fig_pdf = os.path.join(sub_folder, \"rel_poblation_density\" + '.pdf')\n# plt.savefig(fig_pdf)\n# Mostramos el plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se observa algún valor atípico al aumentar la población.","metadata":{}},{"cell_type":"markdown","source":"### Boxplot\n\n\nAhora, se van a mostrar la dispersión de las variables numéricas, en concreto, se mostrará la dispersión del número de implicados, la población y la densidad de la población.","metadata":{}},{"cell_type":"code","source":"# Número de implicados\nplt.figure(figsize=(10, 8))\nsns.boxplot(data=data[\"number_implicates\"])\nplt.xlabel('Crímen')\nplt.ylabel('Número de implicados')\n# Obtenemos la carpeta donde guardar el gráfico\nsub_folder = check_folder(\"box_plot_n_implicates\", \"figs\")\n# Exportamos a JPG\nfig_jpg = os.path.join(sub_folder, \"box_plot_n_implicates\" + '.jpg')\nplt.savefig(fig_jpg)\n# Exportamos a PDF\n# fig_pdf = os.path.join(sub_folder, \"box_plot_n_implicates\" + '.pdf')\n# plt.savefig(fig_pdf)\n# Mostramos el plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Población\nplt.figure(figsize=(10, 8))\nsns.boxplot(data=data[\"population\"])\nplt.xlabel('Crímen')\nplt.ylabel('Población')\n# Obtenemos la carpeta donde guardar el gráfico\nsub_folder = check_folder(\"box_plot_population\", \"figs\")\n# Exportamos a JPG\nfig_jpg = os.path.join(sub_folder, \"box_plot_population\" + '.jpg')\nplt.savefig(fig_jpg)\n# Exportamos a PDF\n# fig_pdf = os.path.join(sub_folder, \"box_plot_population\" + '.pdf')\n# plt.savefig(fig_pdf)\n# Mostramos el plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Densidad\nplt.figure(figsize=(10, 8))\nsns.boxplot(data=data[\"density\"])\nplt.xlabel('Crímen')\nplt.ylabel('Densidad')\n# Obtenemos la carpeta donde guardar el gráfico\nsub_folder = check_folder(\"box_plot_density\", \"figs\")\n# Exportamos a JPG\nfig_jpg = os.path.join(sub_folder, \"box_plot_density\" + '.jpg')\nplt.savefig(fig_jpg)\n# Exportamos a PDF\n# fig_pdf = os.path.join(sub_folder, \"box_plot_density\" + '.pdf')\n# plt.savefig(fig_pdf)\n# Mostramos el plot\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quien **menos dispersión** tiene es la **densidad**, hay una **gran presencia de valores atípicos** que se salen del cuarto cuartil.","metadata":{}},{"cell_type":"markdown","source":"## Gráficas","metadata":{}},{"cell_type":"markdown","source":"Se muestran gráficas para cada una de las columnas de relevancia en el dataset, con agrupaciones y evoluciones temporales. Tanto para el conjunto total como para cada subconjunto en función de la severidad.","metadata":{}},{"cell_type":"code","source":"print(f\"El total de crímenes registrados es de {len(data)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_crimes_cluster, df_n_crimes_cluster = get_dic_df_data_value(data, \"area_cluster\")\ndf_n_crimes_cluster.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tam_fig = 14\nxticks_position = range(math.floor(min(data[\"year\"])), math.ceil(max(data[\"year\"]))+1)\n\n# Gráficas de implicados agrupados por año\ndef show_crimes_year(data, sup_folder):\n    plt.figure(figsize=(14,6))\n    n_crimes_year, df_n_crimes_year = get_dic_df_data_value(data, \"year\")\n    #df_n_crimes_year.plot(x=\"year\",y=\"n_crimes_year\", fontsize=15, kind=\"bar\")\n    ax = sns.barplot(x=\"year\", y=\"n_crimes_year\", data=df_n_crimes_year)\n    ax.set_title(\"Cantidad de crímenes por año\")\n    ax.set_xlabel(\"Año\")\n    ax.set_ylabel('Cantidad de crímenes')\n    # Obtenemos la carpeta donde guardar el gráfico\n    sub_folder = check_folder(\"crimes_year\", \"figs\", sup_folder)\n    # Exportamos a JPG\n    fig_jpg = os.path.join(sub_folder, sup_folder+\"_crimes_year\" + '.jpg')\n    plt.savefig(fig_jpg)\n    # Exportamos a PDF\n    # fig_pdf = os.path.join(sub_folder, sup_folder+\"_crimes_year\" + '.pdf')\n    # plt.savefig(fig_pdf)\n    # Mostramos el plot\n    plt.show()\n    \n# Gráficas de crímenes agrupados por el cluster\ndef show_crimes_cluster(data, sup_folder):\n    n_crimes_cluster, df_n_crimes_cluster = get_dic_df_data_value(data, \"area_cluster\")\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(tam_fig,6))\n    fig.tight_layout(pad=6.0)\n    # Subplot 1\n    df_n_crimes_cluster.plot(ax=axes[0], x=\"area_cluster\",y=[\"n_crimes_area_cluster\"], fontsize=15, kind=\"bar\")\n    axes[0].set_title(\"Cluster en base al área geográfica\")\n    axes[0].tick_params('x', labelrotation=0)\n    axes[0].set_xlabel(\"Área\")\n    axes[0].set_ylabel('Cantidad de crímenes')\n    axes[0].legend([\"Cantidad de crímenes\"], loc=\"upper right\")\n    # Subplot 2\n    n_crimes_cluster, df_n_crimes_cluster = get_dic_df_data_value(data, \"pop_dens_cluster\")\n    df_n_crimes_cluster.plot(ax=axes[1], x=\"pop_dens_cluster\",y=[\"n_crimes_pop_dens_cluster\"], fontsize=15, kind=\"bar\")\n    axes[1].set_title(\"Cluster en base a la densidad de la población\")\n    axes[1].tick_params('x', labelrotation=0)\n    axes[1].set_xlabel(\"Densidad de población\")\n    axes[1].set_ylabel('Crímenes')\n    axes[1].legend([\"Cantidad de crímenes\"], loc=\"upper right\")\n    # Obtenemos la carpeta donde guardar el gráfico\n    sub_folder = check_folder(\"crimes_clusters\", \"figs\", sup_folder)\n    # Exportamos a JPG\n    fig_jpg = os.path.join(sub_folder, sup_folder+\"_crimes_cluster\" + '.jpg')\n    plt.savefig(fig_jpg)\n    # Exportamos a PDF\n    # fig_pdf = os.path.join(sub_folder, sup_folder+\"_crimes_cluster\" + '.pdf')\n    # plt.savefig(fig_pdf)\n    # Mostramos el plot\n    plt.show()\n    \n# Gráficas de implicados en función del año\ndef show_implicates_year(data, sup_folder):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(tam_fig,6))\n    fig.tight_layout(pad=6.0)\n    # Subplot 1\n    sns.lineplot(ax=axes[0], data=data.groupby(by='year')['number_implicates'].sum(), label=\"Suma total de los implicados\")\n    axes[0].set_xlabel(\"Año\", size=15)\n    axes[0].set_ylabel(\"Suma total\", size=15)\n    axes[0].tick_params(labelsize=15)\n    axes[0].set_title(\"Evolución temporal del número total de implicados\", size=15)\n    axes[0].set_xticks(xticks_position)\n    # Subplot 2\n    sns.lineplot(ax=axes[1], data=data.groupby(by='year')['number_implicates'].mean(), label=\"Valor medio de los implicados\")\n    axes[1].set_xlabel(\"Año\", size=15)\n    axes[1].set_ylabel(\"Valor medio\", size=15)\n    axes[1].tick_params(labelsize=15)\n    axes[1].set_title(\"Evolución temporal del número medio de implicados\", size=15)\n    axes[1].set_xticks(xticks_position)\n    # Obtenemos la carpeta donde guardar el gráfico\n    sub_folder = check_folder(\"implicates_year\", \"figs\", sup_folder)\n    # Exportamos a JPG\n    fig_jpg = os.path.join(sub_folder, sup_folder+\"_implicates_year\" + '.jpg')\n    plt.savefig(fig_jpg)\n    # Exportamos a PDF\n    # fig_pdf = os.path.join(sub_folder, sup_folder+\"_implicates_year\" + '.pdf')\n    # plt.savefig(fig_pdf)\n    # Mostramos el plot\n    plt.show()\n    \n    \n# Gráficas del género\ndef show_genre(data, sup_folder):\n    plt.figure(figsize=(tam_fig, 10))\n    sns.lineplot(data=data.groupby(by='year')['male_involved'].sum(), label=\"Cantidad de hombres identifciados\")\n    sns.lineplot(data=data.groupby(by='year')['female_involved'].sum(), label=\"Cantidad de mujeres identificadas\")\n    # Total\n    sns.lineplot(data=data.groupby(by='year')['date'].count(), linestyle = \"--\", label=\"Total\")\n    plt.xlabel(\"Año\", size=20)\n    plt.ylabel(\"Cantidad\", size=20)\n    plt.tick_params(labelsize=20)\n    plt.legend(loc=2,prop={'size':20})\n    plt.title(\"Evolución temporal de los hombres y las mujeres implicadas en crímenes\", size=15)\n    plt.xticks(xticks_position)\n    # Obtenemos la carpeta donde guardar el gráfico\n    sub_folder = check_folder(\"genre\", \"figs\", sup_folder)\n    # Exportamos a JPG\n    fig_jpg = os.path.join(sub_folder, sup_folder+\"_genre\" + '.jpg')\n    plt.savefig(fig_jpg)\n    # Exportamos a PDF\n    # fig_pdf = os.path.join(sub_folder, sup_folder+\"_genre\" + '.pdf')\n    # plt.savefig(fig_pdf)\n    # Mostramos el plot\n    plt.show()\n    \n    \n# Gráficas de la edad\ndef show_age(data, sup_folder):\n    plt.figure(figsize=(tam_fig, 10))\n    sns.lineplot(data=data.groupby(by='year')['child_involved'].sum(), label=\"Cantidad de niños indentificados\")\n    sns.lineplot(data=data.groupby(by='year')['teen_involved'].sum(), label=\"Cantidad de adolescentes indentificados\")\n    sns.lineplot(data=data.groupby(by='year')['adult_involved'].sum(), label=\"Cantidad de adultos indentificados\")\n    # Total\n    sns.lineplot(data=data.groupby(by='year')['date'].count(), linestyle = \"--\", label=\"Total\")\n    plt.xlabel(\"Año\", size=20)\n    plt.ylabel(\"Cantidad\", size=20)\n    plt.tick_params(labelsize=20)\n    plt.legend(loc=2,prop={'size':20})\n    plt.title(\"Evolución temporal de los adultos, adolescentes y niños identificados en crímenes\", size=15)\n    plt.xticks(xticks_position)\n    # Obtenemos la carpeta donde guardar el gráfico\n    sub_folder = check_folder(\"age\", \"figs\", sup_folder)\n    # Exportamos a JPG\n    fig_jpg = os.path.join(sub_folder, sup_folder+\"_age\" + '.jpg')\n    plt.savefig(fig_jpg)\n    # Exportamos a PDF\n    # fig_pdf = os.path.join(sub_folder, sup_folder+\"_age\" + '.pdf')\n    # plt.savefig(fig_pdf)\n    # Mostramos el plot\n    plt.show()\n    \n    \n# Gráficas de las armas\ndef show_guns(data, sup_folder):\n    plt.figure(figsize=(tam_fig, 10))\n    sns.lineplot(data=data.groupby(by='year')['gun_stolen'].sum(), label=\"Cantidad de armas robadas\")\n    sns.lineplot(data=data.groupby(by='year')['gun_involved'].sum(), label=\"Cantidad de armas implicadas\")\n    # Total\n    sns.lineplot(data=data.groupby(by='year')['date'].count(), linestyle = \"--\", label=\"Total\")\n    plt.xlabel(\"Año\", size=20)\n    plt.ylabel(\"Cantidad\", size=20)\n    plt.tick_params(labelsize=20)\n    plt.legend(loc=2,prop={'size':20})\n    plt.title(\"Evolución temporal de las armas y armas robadas indetificadas en crímenes\", size=15)\n    plt.xticks(xticks_position)\n    # Obtenemos la carpeta donde guardar el gráfico\n    sub_folder = check_folder(\"guns\", \"figs\", sup_folder)\n    # Exportamos a JPG\n    fig_jpg = os.path.join(sub_folder, sup_folder+\"_guns\" + '.jpg')\n    plt.savefig(fig_jpg)\n    # Exportamos a PDF\n    # fig_pdf = os.path.join(sub_folder, sup_folder+\"_guns\" + '.pdf')\n    # plt.savefig(fig_pdf)\n    # Mostramos el plot\n    plt.show()\n    \n    \n# Sospechosos\ndef show_suspects(data, sup_folder):\n    plt.figure(figsize=(tam_fig, 10))\n    sns.lineplot(data=data.groupby(by='year')['exist_suspect'].sum(), label=\"Cantidad de sospechosos detectados\")\n    # Total\n    sns.lineplot(data=data.groupby(by='year')['date'].count(), linestyle = \"--\", label=\"Total\")\n    plt.xlabel(\"Año\", size=20)\n    plt.ylabel(\"Cantidad\", size=20)\n    plt.tick_params(labelsize=20)\n    plt.legend(loc=2,prop={'size':20})\n    plt.title(\"Evolución temporal de los sospechosos indetificados en crímenes\", size=15)\n    plt.xticks(xticks_position)\n    # Obtenemos la carpeta donde guardar el gráfico\n    sub_folder = check_folder(\"suspects\", \"figs\", sup_folder)\n    # Exportamos a JPG\n    fig_jpg = os.path.join(sub_folder, sup_folder+\"_suspects\" + '.jpg')\n    plt.savefig(fig_jpg)\n    # Exportamos a PDF\n    # fig_pdf = os.path.join(sub_folder, sup_folder+\"_suspects\" + '.pdf')\n    # plt.savefig(fig_pdf)\n    # Mostramos el plot\n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cantidad de crímentes\nprint(\"Mostramos las gráficas del número de crímenes por año\")\nshow_crimes_year(data, \"data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El año con menos críemenes detectados es, con diferencia, 2018.","metadata":{}},{"cell_type":"code","source":"# Cluster geográfico\nprint(\"Agrupamos los crímenes por los clústers\")\nshow_crimes_cluster(data, \"data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aquí tenemos dos clusters distintos\n+ número de **crímenes agrupados por cluster geográfico**: el área 1 tiene menos cantidad de crímenes.\n+ número de **crímenes agrupados por cluster poblacional**: en una densidad media es donde menos crímenes suceden.","metadata":{}},{"cell_type":"code","source":"# Implicados\nprint(\"Mostramos las gráficas de los implicados\")\nshow_implicates_year(data, \"data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se observa que la **suma** de crímenes se **reduce** en **2018**, pero la **media aumenta**. Eso se debe a que, como se ha comentado en la primera gráfica aquí expuesta, la cantidad de datos es mucho menor en 2018.","metadata":{}},{"cell_type":"code","source":"# Género\nprint(\"Mostramos las gráficas en función del género\")\nshow_genre(data, \"data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En una **gran proporción** de los crímenes hay implicados **hombres**. Sin embargo, en una **proporción** muy **reducida** lo están las mujeres.","metadata":{}},{"cell_type":"code","source":"# Edad\nprint(\"Mostramos las gráficas en función de la edad\")\nshow_age(data, \"data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La presencia de **adultos** es muy elevada, a diferencia de niños y adolescentes.","metadata":{}},{"cell_type":"code","source":"# Armas involucradas y robadas\nprint(\"Mostramos las armas involucradas y si fueron robadas\")\nshow_guns(data, \"data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La cantidad de armas robadas implicadas es muy baja, pero la presencia *per se* de armas es muy alta, de hecho, están implicadas en el 100% de casos de 2018.","metadata":{}},{"cell_type":"code","source":"# Sospechosos detectados\nprint(\"Mostramos las gráficas de los sospechosos detectados\")\nshow_suspects(data, \"data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En cuanto a los **sospechosos** detectados, la proporción de sospechosos detectados es, apróximadamente del **70%**.","metadata":{}},{"cell_type":"code","source":"# Creamos una función que dado un dataframe obtengamos las gráficas importantes\ndef show_graphics(data, sup_folder):\n    # Cantidad de crímentes\n    print(\"Mostramos las gráficas del número de crímenes por año\")\n    show_crimes_year(data, sup_folder)\n    # Clusters\n    print(\"Agrupamos los crímenes por los clústers\")\n    show_crimes_cluster(data, sup_folder)\n    # Implicados\n    print(\"Mostramos las gráficas de los implicados\")\n    show_implicates_year(data, sup_folder)\n    # Género\n    print(\"Mostramos las gráficas en función del género\")\n    show_genre(data, sup_folder)\n    # Edad\n    print(\"Mostramos las gráficas en función de la edad\")\n    show_age(data, sup_folder)\n    # Armas involucradas y robadas\n    print(\"Mostramos las armas involucradas y si fueron robadas\")\n    show_guns(data, sup_folder)\n    # Sospechosos detectados\n    print(\"Mostramos las gráficas de los sospechosos detectados\")\n    show_suspects(data, sup_folder)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Separamos los datos por su valor de clase\n\nRealizamos gráficas para cada valor de clase, tendremos 3 dataframes.\n\n**Clases**(severity):\n+ **0** - Importancia **baja**\n+ **1** - Importancia **media**\n+ **2** - Importancia **alta**","metadata":{}},{"cell_type":"markdown","source":"#### Clase 0\n\nAquí se van a mostrar las gráficas para el conjunto de datos de importancia baja(clase 0).","metadata":{}},{"cell_type":"code","source":"df_0 = data.loc[data[\"severity\"]==0,].copy()\nshow_graphics(df_0, \"df_0\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para la **clase 0** se observa que se es muy pareja a las gráficas del dataframe completo, pero hay algún matiz en el número **total** y **medio** de los **implicados**. En esta ocasión la media no suba de manera tan líneal, sino que de 2016 a 2017 ese crecimiento acelera negativamente.\n\nTambién se aprecia que el número de sospechosos detectados es mucho más alto en relación a la cantidad total de crímenes.","metadata":{}},{"cell_type":"markdown","source":"#### Clase 1\n\nAquí se van a mostrar las gráficas para el conjunto de datos de importancia media(clase 1).","metadata":{}},{"cell_type":"code","source":"df_1 = data.loc[data[\"severity\"]==1,].copy()\nshow_graphics(df_1, \"df_1\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para la **clase 1** se observa que, pese a ser muy parecida al dataframe completo, el mayor matiz ocurre con los **sopechosos detectados**.\n\nEl **sospechosos** detectados: la proporción de sospechosos detectados es, apróximadamente, algo inferior al 50%.","metadata":{}},{"cell_type":"markdown","source":"#### Clase 2\n\nAquí se van a mostrar las gráficas para el conjunto de datos de importancia alta(clase 2).","metadata":{}},{"cell_type":"code","source":"df_2 = data.loc[data[\"severity\"]==2,].copy()\nshow_graphics(df_2, \"df_2\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para la **clase 2** se observa que se parece sobre todo a la **clase 1**, en este caso el crecimiento de la **media** de los **implicados** es aún mayor.","metadata":{}},{"cell_type":"markdown","source":"## Conclusiones de la visualización","metadata":{}},{"cell_type":"markdown","source":"Se ha observado que todas las gráficas tienen comportamientos similares, la **clase 0** es la que más se parece al **original**. En cambio, la **clase 1** se asemeja más a la **clase 2**, esto puede corroborarse, por ejemplo, apreciando que, pese a que el porcentaje de hombres detectados siempre es muy alto, en las clases 1 y 2 es aún más alto.\n\nEntre la clase **0** y las clases **1** y **2** (que son muy similares) hay algunas diferencia que podría provocar que ocurran diferencias a la hora de la predicción, dificultando la diferenciación entre estas dos últimas clases.\n\nSe ha descartado automatizar el 100% del código para tener cada subdataframe separado y así poder comentarlo. De la misma manera las primeras gráficas (relativas al conjunto de datos completo) no usan la función `show_graphics` para poder comentar cada gráfica de manera aislada.","metadata":{}},{"cell_type":"markdown","source":"# Predicción","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport math\nfrom datetime import datetime\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport time\n#SKLearn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc\n#Modelos\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try except para instalarlo si fuera necesario\ntry:\n    import shap\nexcept:\n    !pip install shap\n    import shap","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comenzamos el apartado de predicción estableciendo la **seed** a utilizar para intentar que los resultados sean los más repetibles posible.","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Primero cargamos los datos y mostramos que han sido cargados correctamente.","metadata":{}},{"cell_type":"code","source":"data = pd.read_json('data/preprocessed_data.json')\ndata.drop(columns=[\"index\"], inplace=True)\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Antes de comenzar a realizar las predicciones escalamos los datos y definimos una serie de funciones auxiliares.","metadata":{}},{"cell_type":"code","source":"#Seleccionamos solo las variables que se van a usar para el entrenamiento\ntrain_variables = [\"population\", \"density\", \"number_implicates\", \"exist_suspect\", \"male_involved\", \\\n                   \"female_involved\", \"gun_stolen\", \"gun_involved\", \"child_involved\", \\\n                   \"teen_involved\", \"adult_involved\", \"area_0\", \"area_1\", \"pop_dens_0\", \\\n                   \"pop_dens_1\", \"pop_dens_2\", \"pop_dens_3\", \"pop_dens_4\"]\nx = data[train_variables].copy()\ny = data[\"severity\"].copy()\nscaler = MinMaxScaler()\nx = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)\nx.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Matriz de confusión (normalizada)\")\n    else:\n        print('Matriz de confusión (sin normalizar)')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    \n    \ndef calculate_confusion_matrix(y_test, y_pred):\n    #            Predict label\n    #            _________\n    #            |        |\n    # True label |        |\n    #            |________|\n    #\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(10, 8))\n    plot_confusion_matrix(conf_matrix, classes=['Baja', 'Media', 'Alta'],\n                          title='Confusion matrix, without normalization')\n    plt.show()\n    \n\ndef train_with_CV_class(model, x, y, n_splits = 5, number_decimals = 2, show_confusion_matrix = True):\n    #Declaramos la función que generará las particiones del CV\n    cv = KFold(n_splits=n_splits) \n    print(f\"Se entrenará el modelo utilizando CV con k = {n_splits}\")\n    #Variable para acumular la media de los scores sobre los diferentes folds\n    mean_score = 0.0\n    iteration = 0\n    #A la hora de calcula la matriz de confusión se podría utilizar el último modelo entrenado en las\n    #particiones de K-Fold pero otra alternativa es obtener la predicción de y_test y los valores\n    #reales de y_test y acumularlos para mostrar al final una matriz de confusión que represente el conjunto\n    #de modelos entrenados sobre las diferentes particiones\n    y_test_hist = np.array([])\n    y_pred_hist = np.array([])\n    for train_index, test_index in cv.split(x):\n        #Generamos el conjunto de entramiento para la partición K\n        x_train, y_train = x.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = x.iloc[test_index], y.iloc[test_index]\n        #Entrenamos el modelo\n        model.fit(x_train, y_train)\n        mean_score = mean_score + model.score(x_test,y_test)\n        print(f\"Accuracy en K = {iteration}: \", model.score(x_test,y_test))\n        #Calculamos la predicción para añadirlo al histórico\n        y_pred = model.predict(x_test)\n        #Guardamos los datos\n        y_test_hist = np.append(y_test_hist, y_test)\n        y_pred_hist = np.append(y_pred_hist, y_pred)\n        iteration += 1\n    #Obtenemos la media de los resultados\n    resultado = np.round(mean_score/n_splits,number_decimals)\n    print(\"Resultado medio del Cross Validation: \", resultado)\n    if show_confusion_matrix:\n        print(\"-------------------------\")\n        calculate_confusion_matrix(y_test_hist, y_pred_hist)\n    return resultado","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comenzamos la predicción de la importancia del evento a través de modelos sencillos. ","metadata":{}},{"cell_type":"markdown","source":"## Regresión logistica","metadata":{}},{"cell_type":"markdown","source":"Primero hacemos un entrenamiento base con CV con k = 5.","metadata":{}},{"cell_type":"code","source":"#Entrenamos el modelo\nmodel = LogisticRegression(max_iter=2000)\n_ = train_with_CV_class(model, x, y, number_decimals=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este primer modelo se puede ver como si bien entre la clase 0 y las otras dos la separación es bastante clara, sin embargo el modelo presenta más problemas para diferenciar las clases 1 y 2. Para intentar mejorar el resultado realizamos una busqueda de parámetros para ajustar mejor el modelo.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nmodel = LogisticRegression(max_iter=2000)\n#Definimos los parámetros a editar\nc = [1, 10, 100, 1000]\nparam_grid = {'C': c}\n#Realizamos la busqueda\nsearch = HalvingGridSearchCV(model, param_grid, random_state=0)\nsearch.fit(x, y)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Entrenamos el modelo\nmodel = LogisticRegression(C=100,max_iter=2000)\nresult = train_with_CV_class(model, x, y, number_decimals=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El resultado obtenido ha sido muy similar al original y no se ha obtenido una gran mejora.","metadata":{}},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"markdown","source":"Comenzamos las prueas con KNN realizando una busqueda del mejor valor de **k**.","metadata":{}},{"cell_type":"code","source":"#Entrenamos el modelo\nmin_result = None\nmin_k = None\nk_values = [5, 10, 20, 50]\nfor k in k_values:\n    print(f\"Entrenando con k (KNN) = {k}\")\n    model = KNeighborsClassifier(n_neighbors=k)\n    result = train_with_CV_class(model, x, y, show_confusion_matrix=False)\n    if min_result is None or min_result < result:\n        min_result = result\n        min_k = k\nprint(f\"El mejor valor de k es {min_k}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El mejor valor obtenido es el de **k** = 10.","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=10)\nresult = train_with_CV_class(model, x, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso se puede ver como el resultado obtenido ha mejorado al de la regresión logistica, además de que la confusión entre clases ha disminuido.","metadata":{}},{"cell_type":"markdown","source":"# SVC","metadata":{}},{"cell_type":"code","source":"model = SVC(kernel=\"poly\")\nresult = train_with_CV_class(model, x, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En el caso de SVC no se ha obtenido ninguna mejora respecto a los modelos anteriores.","metadata":{}},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"markdown","source":"A partir de este punto continuaremos las pruebas con **ensembles**, ya que son modelos que suelen aportar una mayor precisión.","metadata":{}},{"cell_type":"code","source":"#Medimos el tiempo para compararlo luego con la búsqueda de parámetros\nstart = time.time()\n#Entrenamos el modelo\nmodel = RandomForestClassifier(max_depth=2, random_state=0)\ntrain_with_CV_class(model, x, y, number_decimals=4)\nend = time.time()\nprint(f\"Tiempo de entrenamiento: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El resultado obtenido ha sido el peor hasta el momento, pero Random Forest puede llegar a mejorar notablemente su resultado si se ajustan adecuadamente los valores, por ello se va a realizar una búsqueda de parámetros más optimos.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nmodel = RandomForestClassifier(random_state=0)\n#Definimos los parámetros a editar\nmax_depth = [2, 5, 10, None]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 5]\nparam_grid = {'max_depth': max_depth, 'min_samples_split': min_samples_split, \\\n              'min_samples_leaf': min_samples_leaf}\n#Realizamos la busqueda\nsearch = HalvingGridSearchCV(model, param_grid, random_state=0)\nsearch.fit(x, y)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Entrenamos el modelo\nmodel = RandomForestClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=0)\nresult = train_with_CV_class(model, x, y, number_decimals=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede ver el resultado ha mejorado notablemente, pasando a ser el mejor modelo obtenido hasta el momento. También se puede ver una confusión menor este las clases y que en todos las clases mayoritaria es la correcta. El resultado obtenido ha sido de una precisión del **63.2%**.","metadata":{}},{"cell_type":"markdown","source":"## AdaBoostClassifier (Boosting)","metadata":{}},{"cell_type":"markdown","source":"Continuamos probando otros modelos **ensembles**.","metadata":{}},{"cell_type":"code","source":"#Medimos el tiempo para compararlo luego con la búsqueda de parámetros\nstart = time.time()\n#Entrenamos el modelo\nmodel = AdaBoostClassifier(random_state=0)\ntrain_with_CV_class(model, x, y, number_decimals = 4)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nmodel = AdaBoostClassifier(random_state=0)\n#Definimos los parámetros a editar\nn_estimators = [25, 50, 100, 200]\nlearning_rate = [0.1, 0.5, 1, 2, 5]\nparam_grid = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n#Realizamos la busqueda\nsearch = HalvingGridSearchCV(model, param_grid, random_state=0)\nsearch.fit(x, y)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Entrenamos el modelo\nmodel = AdaBoostClassifier(n_estimators=200, learning_rate=1, random_state=0)\nresult = train_with_CV_class(model, x, y, number_decimals=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso el ajuste de parámetros no ha supuesto una gran mejora y el resultado obtenido es peor al de otros modelos anteriores.","metadata":{}},{"cell_type":"markdown","source":"# Stacking","metadata":{}},{"cell_type":"markdown","source":"El stacking nos permite combinar diferentes modelos para obtener uno que suele ser más preciso a cambio de una mayor complejidad. En este caso se utilizará el stacking a partir de los tres modelos que mejores resultados han proporcionado hasta el momento.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\n# Definimos los modelos base\nbase_models = list()\nbase_models.append(('lr', LogisticRegression(C=100,max_iter=2000)))\nbase_models.append(('knn', KNeighborsClassifier(n_neighbors=10)))\nbase_models.append(('rf', RandomForestClassifier(max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=0)))\n# Definimos el modelo del meta learner\nmeta_learner = LogisticRegression(max_iter=2000)\n\n# Definimos el stacking ensemble\nmodel = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = train_with_CV_class(model, x, y, number_decimals=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El resultado obtenido es hasta ahora el mejor, con un **63.67%** frente al **63.2%** del Random Forest.","metadata":{}},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"markdown","source":"XGBoost es un modelo interesante, ya que el su paquete trae diferentes opciones de visualización y es compatible con otros tantos paquetes de Python.","metadata":{}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(action='ignore', category=UserWarning)\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\ndef train_with_CV_xgboost(model, x, y, n_splits = 5, number_decimals = 2, show_confusion_matrix = True):\n    #Declaramos la función que generará las particiones del CV\n    cv = KFold(n_splits=n_splits) \n    print(\"Se entrenará el modelo utilizando CV \")\n    #Variable para acumular la media de los scores sobre los diferentes folds\n    mean_score = 0.0\n    iteration = 0\n    y_test_hist = np.array([])\n    y_pred_hist = np.array([])\n    for train_index, test_index in cv.split(x):\n        #Generamos el conjunto de entramiento para la partición K\n        x_train, y_train = x.iloc[train_index], y.iloc[train_index]\n        x_test, y_test = x.iloc[test_index], y.iloc[test_index]\n        #Entrenamos el modelo\n        model.fit(x_train, y_train)\n        mean_score = mean_score + accuracy_score(y_test, model.predict(x_test))\n        print(f\"Accuracy en K = {iteration}: \", accuracy_score(y_test, model.predict(x_test)))\n        #Calculamos la predicción para añadirlo al histórico\n        y_pred = model.predict(x_test)\n        #Guardamos los datos\n        y_test_hist = np.append(y_test_hist, y_test)\n        y_pred_hist = np.append(y_pred_hist, y_pred)\n        iteration += 1\n    #Obtenemos la media de los resultados\n    resultado = np.round(mean_score/n_splits,number_decimals)\n    print(\"Resultado medio del Cross Validation: \", resultado)\n    if show_confusion_matrix:\n        print(\"-------------------------\")\n        calculate_confusion_matrix(y_test_hist, y_pred_hist)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost nos permite hacer realizar clasificación tanto con **XGBClassifier** como **XGBRegressor**, si bien lo normal sería utilizar XGBClassifier, el uso de XGBRegressor permite realizar una comparación a través de la curva ROC y otras ventajas que se mostrarán más adelante. Para comprobar que ambos son igualmente eficaces se van a entrenar ambos y así se podrá observar como los valores obtenidos son muy similares.","metadata":{}},{"cell_type":"markdown","source":"## XGBoost (Classifier)","metadata":{}},{"cell_type":"code","source":"#Medimos el tiempo para compararlo luego con la búsqueda de parámetros\nstart = time.time()\nmodel = xgb.XGBClassifier(num_class=3, eval_metric=\"mlogloss\")\ntrain_with_CV_xgboost(model, x, y, number_decimals = 4)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nmodel = xgb.XGBClassifier(num_class=3, eval_metric=\"mlogloss\")\n#Definimos los parámetros a editar\neta = [0.01, 0.05, 0.1, 0.3, 0.5] #Learning rate\ngamma = [0, 0.5, 1]\nmax_depth = [5, 6, 7]\nparam_grid = {'eta': eta, 'gamma':gamma, 'max_depth':max_depth}\n#Realizamos la busqueda\nsearch = HalvingGridSearchCV(model, param_grid, random_state=0)\nsearch.fit(x, y)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Entrenamos el modelo\nmodel = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=3, eval_metric=\"mlogloss\", eta=0.1, gamma=1, \\\n                        max_depth=7)\ntrain_with_CV_xgboost(model, x, y, number_decimals=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"La precisión obtenida con XGBClassifier ha sido de **63.47%**, siendo bastante cercana a la obtenida mediante stacking. A continuación repetiremos las pruebas con XGBRegressor.","metadata":{}},{"cell_type":"markdown","source":"## XGBoost (Regressor)","metadata":{}},{"cell_type":"code","source":"#Medimos el tiempo para compararlo luego con la búsqueda de parámetros\nstart = time.time()\nmodel = xgb.XGBRegressor(objective=\"multi:softmax\", num_class=3, eval_metric=\"mlogloss\")\ntrain_with_CV_xgboost(model, x, y, number_decimals = 4)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nmodel = xgb.XGBRegressor(objective=\"multi:softmax\", num_class=3, eval_metric=\"mlogloss\")\n#Definimos los parámetros a editar\neta = [0.01, 0.05, 0.1, 0.3, 0.5] #Learning rate\ngamma = [0, 0.5, 1]\nmax_depth = [5, 6, 7]\nparam_grid = {'eta': eta, 'gamma':gamma, 'max_depth':max_depth}\n#Realizamos la busqueda\nsearch = HalvingGridSearchCV(model, param_grid, random_state=0)\nsearch.fit(x, y)\nend = time.time()\nprint(f\"Tiempo de búsqueda: {end - start:.2f}s\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Entrenamos el modelo\nmodel = xgb.XGBRegressor(objective=\"multi:softmax\", num_class=3, eval_metric=\"mlogloss\", eta=0.1, gamma=1, \\\n                        max_depth=7)\ntrain_with_CV_xgboost(model, x, y, number_decimals=4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede ver, los resultados obtenidos por XGBClassifier y XGBRegressor son iguales.","metadata":{}},{"cell_type":"markdown","source":"# Comparando los mejores modelos","metadata":{}},{"cell_type":"markdown","source":"Los modelos que mejores resultados han obtenido son el modelo de Stacking, con un 63.67% de precisión, y XGBoost con un 63.47% de precisión. Para tener un elemento de comparación extra vamos a utilizar la curva ROC.","metadata":{}},{"cell_type":"code","source":"def calcula_roc_data(model, x, y, n_splits=5, direct_train = False):\n    cv = KFold(n_splits=n_splits)\n    n_classes = max(y) + 1\n    #Binarizamos la variable Y\n    if not direct_train:\n        y = label_binarize(y, classes=list(range(n_classes)))\n    y_test_hist = np.array([])\n    y_score_hist = np.array([])\n    #Creamos el dataset con Y binarizada\n    for train_index, test_index in cv.split(x):\n        #Generamos el conjunto de entramiento para la partición K\n        x_train, y_train = x.iloc[train_index], y[train_index]\n        x_test, y_test = x.iloc[test_index], y[test_index]\n        #Entrenamos el modelo utilizando el paradigma \"One VS Rest\"\n        random_state = np.random.RandomState(0)\n        if direct_train:\n            classifier = model.fit(x_train, y_train)\n            y_score = classifier.predict(x_test)\n        else:\n            classifier = OneVsRestClassifier(model).fit(x_train, y_train)\n            y_score = classifier.decision_function(x_test)\n        #Guardamos los datos en el histórico\n        if not direct_train:\n            y_test_hist = np.append(y_test_hist, y_test)\n        else:\n            y_test_hist = np.append(y_test_hist, label_binarize(y_test, classes=list(range(n_classes))))\n        y_score_hist = np.append(y_score_hist, y_score)\n    #Definimos los diccionarios donde guardaremos los resultados\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    # Redimensionamos los arrays\n    y_test_hist = np.reshape(y_test_hist, (-1, n_classes))\n    y_score_hist = np.reshape(y_score_hist, (-1, n_classes))\n    # Calculamos la curva ROC curve y el area de esta para cada clase\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_hist[:, i], y_score_hist[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    # Calculamos el 'micro-average ROC curve' y la area ROC\n    fpr[\"micro\"], tpr[\"micro\"], thresholds_micro = roc_curve(y_test_hist.ravel(), y_score_hist.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    return tpr, fpr, roc_auc\n        \ndef show_curve_roc(tpr, fpr, roc_auc, model_name):\n    plt.figure(figsize=(10, 8))\n    lw = 2\n    n_classes = max(y) + 1\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i],\n                 lw=lw, label=f'Curva ROC (area = %0.2f) - Clase {i}' % roc_auc[i])\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='Curva micro-average ROC (area = {0:0.2f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='black', linestyle=':', linewidth=4)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'Curva ROC para el modelo de {model_name}')\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Definimos el modelo\nmodel = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5)\ntpr, fpr, roc_auc = calcula_roc_data(model, x, y)\nshow_curve_roc(tpr, fpr, roc_auc, \"regresión polinómica con parámetros ajustados\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = xgb.XGBRegressor(objective=\"multi:softprob\", num_class=3, eval_metric=\"mlogloss\")\ntpr, fpr, roc_auc = calcula_roc_data(model, x, y, direct_train=True)\nshow_curve_roc(tpr, fpr, roc_auc, \"regresión polinómica con parámetros ajustados\")\nmodel.fit(x, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se observar, ambos modelos presentan una precisión muy similar, obtenido el modelo de Stacking una área bajo la curva de 0.84, frente a XGBoost, que obtiene un área bajo la curva de 0.83.\n\nSi bien la precisión del modelo de Stacking es ligeramente superior el modelo de XGBoost presenta una ventaja, ya que este tiene una mayor explicabilidad, por lo que es preferible a pesar de suponer una pequeña disminución de precisión.","metadata":{}},{"cell_type":"markdown","source":"# Análisis de características","metadata":{}},{"cell_type":"markdown","source":"El primer elemento que vamos a explorar con XGBoost es la importancia de las diferentes features a la hora de obtener su resultado.","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBRegressor(objective=\"multi:softmax\", num_class=3, eval_metric=\"mlogloss\", eta=0.1, gamma=1, \\\n                        max_depth=7).fit(x, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(model)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede ver las variables más importantes a la hora de determinar la importancia del incidente es la población y la densidad de población del lugar donde tiene lugar, seguido del número de implicados y si existe el conocimiento de que haya un arma involucrada.\n\nEsto nos aporta información, pero se encuentra limitada, para aumentar la explicabilidad del modelo vamos a hacer uso de [SHAP](https://github.com/slundberg/shap), que es una técnica aplicable a XGBoost para obtener un mayor conocimiento sobre los elementos que han dado lugar a la predicción del modelo.\n\nPrimero vamos a comenzar explorando la relevancia de las features en cada clase.","metadata":{}},{"cell_type":"code","source":"shap.initjs()\nmodel = xgb.XGBClassifier(num_class=3, eval_metric=\"mlogloss\", eta=0.1, gamma=1, max_depth=7).fit(x, y)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(x,approximate=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clase 0: Importancia baja","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values[0], x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este primer caso se pueden observar diferentes elementos:\n- La existencia de adultos involucrados hace menos probable que una incidencia tenga un importancia baja.\n- La densidad de población baja incentiva una gravedad menor del incidente.\n- El area 0, la zona norte de California (San Francisco, San José y Sacramento) suele tener incidentes de mayor gravedad que la zona sur (Los Ángeles).\n- Curiosamente, el hecho de que se haya reportado que hay un arma robada suele implicar que el incidente suele tener menor gravedad.","metadata":{}},{"cell_type":"markdown","source":"## Clase 1: Importancia media","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values[1], x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En el caso de los incidentes de importancia/gravedad media la relevancia de los diferentes elementos es menos marcada, pero entre ellos destacan:\n- Un mayor número de implicados suele conllevar mayor posibilidades de que sea un incidente de gravedad media aunque hay cierto ruido.\n- Una mayor densidad de población suele provocar una tendencia hacia una importancia media.\n- La presencia de adolescentes y niños suele llevar a una mayor probabilidad de que sea un incidente de prioridad alta. ","metadata":{}},{"cell_type":"markdown","source":"## Clase 2: Importancia alta","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values[2], x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En el caso de los incidentes de prioridad alta hay diferentes elementos a destacar:\n- Un mayor número de implicados tiende a dar lugar a que la importancia del evento sea alta.\n- El hecho de que haya un arma robada reduce la posibilidad de que sea un evento de prioridad alta.\n- La existencia de un sospechoso reduce la probabilidades de que la incidencia sea de gravedad alta.","metadata":{}},{"cell_type":"markdown","source":"# Escala única","metadata":{}},{"cell_type":"markdown","source":"Al ser realmente las clases creadas una escala ascendente de importancia/gravedad del incidente esto nos permite también realizar una escala de importancia, donde a mayor valor mayor importancia, permitiendonos utilizar una regresión y así observar la influencia de las características del indicente de forma global y en una escala única y continua.\n\nVamos a comenzar a analizar casos a nivel individual.","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBRegressor(eval_metric=\"mlogloss\", eta=0.1, gamma=1, max_depth=7).fit(x, y)\n\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\nexplainer = shap.Explainer(model)\nshap_values = explainer(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analizando caso de importancia baja","metadata":{}},{"cell_type":"code","source":"shap.plots.waterfall(shap_values[7])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.force(shap_values[7], matplotlib=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se puede observar que los dos elementos más relevantes han sido la existencia de un sospechoso y un número bajo de implicados.","metadata":{}},{"cell_type":"markdown","source":"## Analizando caso de importancia media","metadata":{}},{"cell_type":"code","source":"shap.plots.waterfall(shap_values[2])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.force(shap_values[2], matplotlib=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso, al pertenecer a un caso de importancia media, se pueden ver que los elementos más relevantes tanto aumentan como disminuyen la gravedad, siendo los elementos que más disminuyen la gravedad el hecho de que exista un sospechoso y sea una densidad relativamente baja, mientras que aumenta la gravedad que el número de implicados sea mayor que en el caso anterior, que haya una mujer involucrada, que tiene lugar en la zona norte de California y la presencia de un adulto.","metadata":{}},{"cell_type":"markdown","source":"## Analizando caso de importancia alta","metadata":{}},{"cell_type":"code","source":"shap.plots.waterfall(shap_values[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.force(shap_values[0], matplotlib=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso se puede observar la gran relevancia que le da a la existencia de un sospechoso el modelo, pudiendo ver como elementos como el no haberse realizado en la zona norte de California y una densidad de población baja disminuyen la importancia de la incidencia, pero en una cantidad muy pequeña comparada a la importancia de la no existencia de un sospechoso.","metadata":{}},{"cell_type":"markdown","source":"# Análisis global de la importancia de un incidente","metadata":{}},{"cell_type":"markdown","source":"Al igual que en el caso anterior se ha analizado la influencia global de las variables por cada clase, en este caso se va a analizar todos los casos conjuntamente, representando la importancia del incidente de forma continua. ","metadata":{}},{"cell_type":"code","source":"explainer = shap.Explainer(model)\nshap_values = explainer(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.plots.beeswarm(shap_values, max_display=20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Al analizar todas los casos de forma continua respecto a su severidad se pueden observar las tendencias de forma más clara, ya que en los casos de severidad media se podía apreciar una menor determinación de la relevancia de las características. En este caso se pueden observar los siguientes elementos:\n- La existencia de un sospechoso disminuye la gravedad del incidente, llegando a influir en gran cantidad la no existencia de un sospechoso a la hora de aumentar la gravedad de un incidente.\n- A mayor número de implicados, mayor importancia del incidente.\n- A mayor densidad de población, mayor probabilidad de que incidente sea más grave.\n- La zona norte de California (area_0) tiende a tener incidentes más graves.\n- En general, la presencia de mujeres da lugar a incidentes más graves. \n- El conocimiento de que existe un arma robada suele disminuir la importancia del incidente.\n- La presencia de adolescente suele provocar eventos más graves.\n- El resto de variables suelen aumentar o disminuir la gravedad del incidente en base a otras características, o directamente no suelen tener relevancia.\n\nEl hecho de que varias variables referentes a los clusters tengan relevancia tan baja posiblemente se deba a que su información se encuentra contenida en otras variables y el modelo obtenga esta información de esas variables.","metadata":{}},{"cell_type":"markdown","source":"# Conclusiones","metadata":{}},{"cell_type":"markdown","source":"Se ha conseguido obtener un modelo que tiene una buena precisión prediciendo cuando un evento tendrá una importancia/gravedad baja, pero presenta mayores problemas para diferenciar entre eventos de gravedad media y alta. Sin embargo, esto es un buen resultado, ya que en caso de agrupar los eventos de prioridad media y alta se podría distinguir con bastante precisión entre incidentes prioritarios y no prioritarios, siendo de ayudar a la hora de dar prioridad a los diferentes eventos cuando existiese una falta de recursos.\n\nTambién es de destacar que se ha obtenido una buena explicabilidad del modelo, lo que permitiría al usuario entender mejor los resultados que el modelo le está proporcionando, para así darle mayor confianza sobre la predicción obtenida.","metadata":{}},{"cell_type":"markdown","source":"# Líneas futuras de trabajo","metadata":{}},{"cell_type":"markdown","source":"El resultado obtenido ha sido satisfactorio pero hay diversos elementos en los que se podría profundizar:\n- **Enriquecimiento de los datos**: Hay diversos datos a nivel socioeconómico que se podrían utilizar para enriquecer más los datos, ya que aparte de los datos de población, densidad de población y área añadidos a cada caso se podría utilizar, por ejemplo, el PIB per cápita de la zona.\n- **Combinación de categorías**: El principal problema que presenta el modelo es el de diferenciar entre los casos de importancia media e importancia alta, por ello se podría contemplar la idea de reformular el problema como la división de los casos entre prioritarios y no prioritarios.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}