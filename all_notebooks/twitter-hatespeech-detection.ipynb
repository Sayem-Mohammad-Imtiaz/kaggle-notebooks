{"cells":[{"metadata":{"id":"_Tq4bt1aq7wO"},"cell_type":"markdown","source":"# Importing libraries","execution_count":null},{"metadata":{"id":"WlEYQ3oLoer8","trusted":true},"cell_type":"code","source":"## HOW WE TRAINED THE MODEL \n\n\n\nimport os\nimport joblib\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport pickle\nimport re\n\n\nimport nltk.corpus\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{"id":"jp4hIa8zkrKm"},"cell_type":"markdown","source":"### If you dont have ntlk then install it!","execution_count":null},{"metadata":{"id":"kogrNXO4ofbT","trusted":true},"cell_type":"code","source":"# nltk.download(\"all\")","execution_count":null,"outputs":[]},{"metadata":{"id":"3SlPS13jk_AJ"},"cell_type":"markdown","source":"# Importing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8z2tkXMnlCED","outputId":"4a6bbf3c-4775-4063-9962-3d220e730fb1","trusted":true},"cell_type":"code","source":"#LOADING DATA\ntrain = pd.read_csv(\"/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/twitter-sentiment-analysis-hatred-speech/test.csv\")\n\n\n#PLOTTING HISTOGRAM\ntrain['label'].hist()\nplt.show()\n\n\n#TURNING LABELS INTO CATEGORIES\ntrain['label'] = train['label'].astype('category')\ntrain.drop(columns=['id'], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"YZSE_pmPlwY6"},"cell_type":"markdown","source":"## Our dataset is imbalanced, so just to balance it we loaded many dataset and combined the datasets so that we get a balanced data set containing nearly equal number of both the classes","execution_count":null},{"metadata":{"id":"djozz6nA3_6n","outputId":"9b2024fd-8cd2-4b40-f421-1a37ec9ec7e0","trusted":true},"cell_type":"code","source":"\ntrain_2 = pd.read_csv(\"/kaggle/input/hatespeechlabeleddata/labeled_data.csv\")\ntrain_2['class'].hist()\nplt.show()\n\n#create new columns containing \ntrain_2['label'] = 1\n# this dataset contains three levels of hate speech level 0,1,2\n# We have added all hate tweets to our existing data so that we can get a balanced data\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"tTBXZ3fm7qjG","trusted":true},"cell_type":"code","source":"#Creating dataframe\ntrain_2 = pd.DataFrame(train_2[['label',\"tweet\"]])","execution_count":null,"outputs":[]},{"metadata":{"id":"6tv8Dvq686U0","trusted":true},"cell_type":"code","source":"#Concatinating both dataframes\ntrain = [train, train_2]\nresult = pd.concat(train)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"8yzZz7Eo9HGc","outputId":"f956770a-e71d-4433-d6a8-5d923e798051","trusted":true},"cell_type":"code","source":"# shuffling data and reseting index\nresult=shuffle(result)\nresult = result.reset_index(drop=True)\n\n#Plotting histogram\nresult['label'].hist()","execution_count":null,"outputs":[]},{"metadata":{"id":"ePRvMfKZm9Lq"},"cell_type":"markdown","source":"## Now we have nearly equal data of both the classes","execution_count":null},{"metadata":{"id":"2gnEcasp9_OL","outputId":"efac37ea-8dc6-41ef-fbaa-6dfc9718d4e0","trusted":true},"cell_type":"code","source":"#checking for null values\nresult.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"6iY26rvondHR","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"qg-LP1KVnd4k"},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"id":"BCkcg5T_p8Bl","trusted":true},"cell_type":"code","source":"# Storing stopwords of english language from nltk library\nsw = set(stopwords.words(\"english\"))\n\n# remove stop words\ndef filter_words(word_list):\n    useful_words = [ w for w in word_list if w not in sw ]\n    return(useful_words)\n\n\n\ndef preprocess_data(dataset):\n    data = dataset.copy()\n\n    #Removing punctuations, special characters and lemmatizing words to their base form\n    data['text_lem'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]',' ',text)) for text in lis]) for lis in data['tweet']]\n    \n    a=[]\n    for text in data['text_lem']:\n        word_list = word_tokenize(text)\n        text=filter_words(word_list)\n        a.append(text)  \n    \n    train_text = []\n    for i in a:\n        sent=''\n        for  j in i:\n            sent += str(j) + ' '\n        train_text.append(sent)\n\n    data['cleaned_tweets'] = train_text\n    \n    #Using TF-IDF vectorizer\n    vect = TfidfVectorizer(ngram_range = (1,3)).fit(data['cleaned_tweets'])\n    \n    #Transforming our data using the vector trained on training data.  \n    vectorized_tweets = vect.transform(data['cleaned_tweets'])\n    \n    return vectorized_tweets, vect\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"6GzX4LM7p-C9","trusted":true},"cell_type":"code","source":"#storing preprocessed data in data_train and vector in vect\ndata_train,vect  = preprocess_data(result)\ndata_target = np.array(result[\"label\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"cw0gadLh-vjn","outputId":"8d7121ba-16b9-43c2-fd10-74ed2cd5f4bc","trusted":true},"cell_type":"code","source":"\nprint(data_train.shape, data_target.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"2tQ9ivJCo_3J"},"cell_type":"markdown","source":"## Train Test Split","execution_count":null},{"metadata":{"id":"mCDyHZLprgtp","outputId":"bad0728a-017f-434d-88f0-8e9f7e35ab8c","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(data_train, data_target, test_size=0.2, random_state=42)\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"pNvZZXBKtcnW","trusted":true},"cell_type":"code","source":"# Helping_Function to show Cross Val Scores\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Gsh8QOYvpHrj"},"cell_type":"markdown","source":"## Training the MODEL","execution_count":null},{"metadata":{"id":"AqJ1OL5npJx2","trusted":true},"cell_type":"code","source":"# we are using logistic regression  \nlg_reg_clf = LogisticRegression(C=50, penalty='l2', solver='lbfgs')","execution_count":null,"outputs":[]},{"metadata":{"id":"lRpveKUPj1Ra","outputId":"e132cad7-6368-48e7-bcb3-c1c7d1a74a71","trusted":true},"cell_type":"code","source":"#Calculating cross-val score\nscore = cross_val_score(lg_reg_clf, X_train, y_train, cv=7)","execution_count":null,"outputs":[]},{"metadata":{"id":"rY6H8ljUpiqf"},"cell_type":"markdown","source":"# CV score:","execution_count":null},{"metadata":{"id":"HSdmashGkCMY","outputId":"71e5a3dd-4cc1-4d26-889f-a516a738c589","trusted":true},"cell_type":"code","source":"#Display CV score:\ndisplay_scores(score)","execution_count":null,"outputs":[]},{"metadata":{"id":"17r5NNQMppVz"},"cell_type":"markdown","source":"## Now training model on our data and checking its accuracy on our test","execution_count":null},{"metadata":{"id":"IyhzwOQI1Yyb","outputId":"ddb8e012-f708-4452-c55d-3c41052d8495","trusted":true},"cell_type":"code","source":"model = lg_reg_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"LlsID8Q4pyvO"},"cell_type":"markdown","source":"# Accuracy and confusion matrix:","execution_count":null},{"metadata":{"id":"byPTaxIH1jlJ","outputId":"d5da8fe3-e0dd-4a24-ca1d-28d6fd1748cb","trusted":true},"cell_type":"code","source":"print(\"Accuracy   :\\t\",lg_reg_clf.score(X_test,y_test))\nsns.heatmap(confusion_matrix(lg_reg_clf.predict(X_test), y_test),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"zghhdALHfpiQ","trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"id":"zEVtwVuhqJQt"},"cell_type":"markdown","source":"# Saving model and vector ","execution_count":null},{"metadata":{"id":"E7E7EG4CfpxQ","trusted":false},"cell_type":"code","source":"with open('model','wb') as f:\n    pickle.dump(model,f)\n    \nwith open('vector','wb') as f:\n    pickle.dump(vect,f)\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"PAEm6IHxfp5P","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"Naa4g308fqDN","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}