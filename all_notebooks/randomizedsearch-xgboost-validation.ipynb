{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score as cv\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Introduction**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ntest_df = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'na_count':train_df.isna().sum()}\ntrain_na = pd.DataFrame(data =d)\ntrain_na['percentage'] = train_df.isna().sum()/19158 * 100\ntrain_na","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train_df.isnull(), cbar=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'na_count':test_df.isna().sum()}\ntest_na = pd.DataFrame(data=d)\ntest_na['percentage'] = test_df.isna().sum() / 2129 * 100\ntest_na","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(test_df.isnull(), cbar=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing**","metadata":{}},{"cell_type":"code","source":"train_df.last_new_job.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.company_type.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.company_size.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.city.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Firstly, we observed that the city column has so many unique value and problem can be occur during one hot encoder process","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop('city',axis=1)\ntest_df = test_df.drop('city',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If any column has more than 15% Null values, it might be drop\ntrain_df = train_df.drop(['company_size','company_type'],axis=1)\ntest_df = test_df.drop(['company_size','company_type'],axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distribution_plot(data,column):\n    sns.countplot(data=data, x=column)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_list = list(train_df.select_dtypes(include=['object']).columns)\nobject_list\nfor i in object_list:\n    distribution_plot(train_df,i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['relevent_experience'] = train_df['relevent_experience'].replace({'Has relevent experience':1,'No relevent experience':0})\ntest_df['relevent_experience'] = test_df['relevent_experience'].replace({'Has relevent experience':1,'No relevent experience':0})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['last_new_job'] = train_df['last_new_job'].replace({'never':0,'>4':5}).astype('float')\ntest_df['last_new_job'] = test_df['last_new_job'].replace({'never':0,'>4':5}).astype('float')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We have handled information regarding experience and last_new_job, how often they are changing job ?\ntrain_df['experience']= train_df['experience'].replace({'<1':0,'>20':21}).astype('float')\ntest_df['experience'] = test_df['experience'].replace({'<1':0,'>20':21}).astype('float')\n\ntrain_df['experience_per_job'] = train_df['experience'] / [x + 1 for x in train_df['last_new_job']]\ntest_df['experience_per_job'] = test_df['experience'] / [x + 1 for x in test_df['last_new_job']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pipeline**","metadata":{}},{"cell_type":"code","source":"numerical_cols = test_df.select_dtypes(exclude = ['object']).columns\nnumerical_cols = numerical_cols[1:-1]\ncategorical_cols = test_df.select_dtypes(include = ['object'] ).columns\n\nimp_mean_numerical = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_most_frequent_categorical = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n\nnumerical_transformer_imputer = imp_mean_numerical\n\ncategorical_transformer_simple = Pipeline(steps=[\n    ('imputer',imp_most_frequent_categorical),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\ndata_transformer_simple = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer_imputer, numerical_cols),\n        ('cat', categorical_transformer_simple, categorical_cols)\n    ])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.iloc[:,1:]\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.loc[:,train_df.columns != 'target']\ny = train_df.loc[:,train_df.columns == 'target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.drop('enrollee_id',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBOOST**","metadata":{}},{"cell_type":"code","source":"target_pipeline_xgbclas = Pipeline(steps=[\n                                    ('preprocessor',data_transformer_simple),\n                                    ('model',XGBClassifier())\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'model__colsample_bytree': [0.3, 0.7],\n    'model__n_estimators': [25,50,100],\n    'model__max_depth': range(4,8),\n    \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomized_model = RandomizedSearchCV(target_pipeline_xgbclas,params,cv=3,n_jobs=-1,verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomized_model.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best parameters found: \", randomized_model.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_model.best_score_)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_results = cv(randomized_model.best_estimator_,X,y,cv=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('XGBOOST Mean of Cross validation is = ' + str(cv_results.mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = randomized_model.best_estimator_.predict(X_test)\n\n# Create and print the confusion matrix\ncm = confusion_matrix(y_test, test_predictions)\nprint(cm)\n\n# Print the true positives (actual 1s that were predicted 1s)\nprint(\"The XGBOOST number of true positives is: {}\".format(cm[1,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = precision_score(y_test, test_predictions)\n\n# Print the final result\nprint(\"The XGBOOST precision value is {0:5f}\".format(score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check train and test mean absolute error, if there is equality there is no underfitting\n\nprint('The training error is {0:.5f}'.format(\n  mae(y_train, randomized_model.best_estimator_.predict(X_train))))\nprint('The XGBOOST testing error is {0:.5f}'.format(\n  mae(y_test, randomized_model.best_estimator_.predict(X_test))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let the check f1 score \nprint('The XGBOOST f1_score  is {0:.5f}'.format(f1_score(y_test,test_predictions)\n  ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RandomForest**","metadata":{}},{"cell_type":"code","source":"target_pipeline_rf = Pipeline(steps=[\n                                    ('preprocessor',data_transformer_simple),\n                                    ('model',RandomForestClassifier())\n])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_params = {'model__max_depth' :[2,5,8,19],\n            'model__max_features':[2,5,8],\n            'model__n_estimators':[10,500,1000],\n            'model__min_samples_split':[2,5,10]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forest_randomized = RandomizedSearchCV(target_pipeline_rf,rf_params,cv=3,n_jobs=-1,verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forest_randomized.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best parameters found: \", random_forest_randomized.best_params_)\nprint(\"Lowest  rmse found: \", np.sqrt(np.abs(random_forest_randomized.best_score_)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_results = cv(random_forest_randomized.best_estimator_,X,y,cv=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('RandomForest Mean of Cross validation is = ' + str(cv_results.mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = random_forest_randomized.best_estimator_.predict(X_test)\n\n# Create and print the confusion matrix\ncm = confusion_matrix(y_test, test_predictions)\nprint(cm)\n\n# Print the true positives (actual 1s that were predicted 1s)\nprint(\"The RandomForest number of true positives is: {}\".format(cm[1,1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score = precision_score(y_test, test_predictions)\n\n# Print the final result\nprint(\"The RandomForest precision value is {0:.5f}\".format(score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check train and test mean absolute error, if there is equality there is no underfitting\n\nprint('The training error is {0:.5f}'.format(\n  mae(y_train, random_forest_randomized.best_estimator_.predict(X_train))))\nprint('The RandomForest testing error is {0:.5f}'.format(\n  mae(y_test, random_forest_randomized.best_estimator_.predict(X_test))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let the check f1 score \nprint('The RandomForest f1_score  is {0:.5f}'.format(f1_score(y_test,test_predictions)\n  ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGboost and RandomForest looks very similar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = randomized_model.best_estimator_.predict(test_df).astype('int')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/hr-analytics-job-change-of-data-scientists/aug_test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['target'] = target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_csv =test_df.iloc[:,[0,-1]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_csv.to_csv(\"/kaggle/working/submission_csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(submission_csv.shape)\nprint(submission_csv.isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_csv\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = randomized_model.best_estimator_._final_estimator.feature_importances_\nfeature_imp = pd.DataFrame(sorted(zip(feature_importance,X.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(50, 40))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('GBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp = feature_imp.rename(columns={'Value':'Percentage'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_imp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As you know we created new column As 'experience_per_job',then it become our 3rd important feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}