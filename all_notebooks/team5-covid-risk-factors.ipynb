{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text\nfrom nltk.corpus import stopwords\nimport pickle\nimport en_core_web_sm\nimport csv\nimport json\nimport nltk\nimport langid\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom scipy.spatial.distance import cosine, cdist\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/CORD-19-research-challenge/'\npath2 = '../input/CORD-19-research-challenge/Kaggle/target_tables/8_risk_factors/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading creating the dataframes\nmeta_df = pd.read_csv(path + 'metadata.csv') \nmeta_df = meta_df[['publish_time','title','abstract','cord_uid', 'doi', 'journal', 'url','pdf_json_files']]\n\ntarget_smoking_df = pd.read_csv(path2 +'Smoking Status.csv', index_col= 'Unnamed: 0')\ntarget_smoking_df = target_smoking_df[['Date', 'Study', 'Study Link', 'Journal', 'Study Type','Added on']]\ntarget_diabetes_df = pd.read_csv(path2 +'Diabetes.csv', index_col= 'Unnamed: 0')\ntarget_diabetes_df = target_diabetes_df[['Date', 'Study', 'Study Link', 'Journal', 'Study Type','Added on']]\ntarget_hypertension_df = pd.read_csv(path2+'Hypertension.csv', index_col= 'Unnamed: 0')\ntarget_hypertension_df = target_hypertension_df[['Date', 'Study', 'Study Link', 'Journal', 'Study Type','Added on']]\nprint('meta length:',len(meta_df))\nprint('smoking target table length:',len(target_smoking_df))\nprint('diabetes target table length:',len(target_diabetes_df))\nprint('hypertension target table length:',len(target_hypertension_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging the abstracts in the target tables\ntarget_smoking_df = target_smoking_df.merge(meta_df,how='inner', left_on='Study', right_on='title')\n#target_smoking_df = target_smoking_df[['Date','title','abstract']]\ntarget_diabetes_df = target_diabetes_df.merge(meta_df,how='inner', left_on='Study', right_on='title')\n#target_diabetes_df = target_diabetes_df[['Date','title','abstract']] >meta_df[['title', 'abstract']]\ntarget_hypertension_df = target_hypertension_df.merge(meta_df,how='inner', left_on='Study', right_on='title')\n#target_hypertension_df = target_hypertension_df[['Date','title','abstract']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nmeta_df.isna().sum().plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting rid of duplicates and Null Values in meta dataframe\nprint(\"Initial length:\",len(meta_df))\nmeta_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nmeta_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates:\",len(meta_df))\nmeta_df.dropna(axis=0, inplace=True, subset=['publish_time','title','abstract'])\nprint(\"After dropping N/A:\",len(meta_df))\nmeta_df.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting rid of duplicates and Null Values in target dataframes\nprint(\"Initial length-somking:\",len(target_smoking_df))\ntarget_smoking_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\ntarget_smoking_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates-somking:\",len(target_smoking_df))\ntarget_smoking_df.dropna(axis=0, inplace=True, subset = ['title','abstract'])\nprint(\"After dropping N/A-somking:\",len(target_smoking_df))\ntarget_smoking_df.reset_index(inplace=True)\nprint(\"Initial length-diabetes:\",len(target_diabetes_df))\ntarget_diabetes_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\ntarget_diabetes_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates-diabetes:\",len(target_diabetes_df))\ntarget_diabetes_df.dropna(axis=0, inplace=True, subset = ['title','abstract'])\nprint(\"After dropping N/A-diabetes:\",len(target_diabetes_df))\ntarget_diabetes_df.reset_index(inplace=True)\nprint(\"Initial length-hypertension:\",len(target_hypertension_df))\ntarget_hypertension_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\ntarget_hypertension_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates-hypertension:\",len(target_hypertension_df))\ntarget_hypertension_df.dropna(axis=0, inplace=True, subset = ['title','abstract'])\nprint(\"After dropping N/A-hypertension:\",len(target_hypertension_df))\ntarget_hypertension_df.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_smoking_df.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_diabetes_df.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_hypertension_df.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the year column\nmeta_df['date'] = pd.to_datetime(meta_df['publish_time'], format ='%Y-%m-%d',errors='coerce')\nmeta_df['year'] = pd.DatetimeIndex(meta_df['date']).year.fillna(0).astype(int)\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filtering our metadata based on publish year\nmeta_df = meta_df[meta_df['year']>2019]\nmeta_df.reset_index(inplace=True, drop=True)\nprint('After publish year filter',len(meta_df))\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(meta_df.abstract)):\n        meta_df.abstract[i] =  strip_numeric(meta_df.abstract[i]) #Remove digits\n        meta_df.abstract[i] =  strip_punctuation(str(meta_df.abstract[i]))  #Remove punctuation\n        meta_df.abstract[i] =  strip_multiple_whitespaces(str(meta_df.abstract[i])) #Remove multiple whitespaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(target_smoking_df.abstract)):\n        target_smoking_df.abstract[i] =  strip_numeric(target_smoking_df.abstract[i]) #Remove digits\n        target_smoking_df.abstract[i] =  strip_punctuation(str(target_smoking_df.abstract[i]))  #Remove punctuation\n        target_smoking_df.abstract[i] =  strip_multiple_whitespaces(str(target_smoking_df.abstract[i])) #Remove multiple whitespaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(target_diabetes_df.abstract)):\n        target_diabetes_df.abstract[i] =  strip_numeric(target_diabetes_df.abstract[i]) #Remove digits\n        target_diabetes_df.abstract[i] =  strip_punctuation(str(target_diabetes_df.abstract[i]))  #Remove punctuation\n        target_diabetes_df.abstract[i] =  strip_multiple_whitespaces(str(target_diabetes_df.abstract[i])) #Remove multiple whitespaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(target_hypertension_df.abstract)):\n        target_hypertension_df.abstract[i] =  strip_numeric(target_hypertension_df.abstract[i]) #Remove digits\n        target_hypertension_df.abstract[i] =  strip_punctuation(str(target_hypertension_df.abstract[i]))  #Remove punctuation\n        target_hypertension_df.abstract[i] =  strip_multiple_whitespaces(str(target_hypertension_df.abstract[i])) #Remove multiple whitespaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Turning everything lowecase\nfor i in range(len(meta_df.abstract)):\n    meta_df.abstract[i] = meta_df.abstract[i].lower()\nfor i in range(len(target_smoking_df.abstract)):\n    target_smoking_df.abstract[i] = target_smoking_df.abstract[i].lower()\nfor i in range(len(target_diabetes_df.abstract)):\n    target_diabetes_df.abstract[i] = target_diabetes_df.abstract[i].lower()\nfor i in range(len(target_hypertension_df.abstract)):\n    target_hypertension_df.abstract[i] = target_hypertension_df.abstract[i].lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making a back-up just in case :)\nbackup_meta=meta_df\nbackup_target_smoking=target_smoking_df\nbackup_target_diabetes=target_diabetes_df\nbackup_meta_target_hypertension=target_hypertension_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filtering for covid Literature (meta df)\nsearchfor = ['covid','corona','ncov']\nmeta_df = meta_df[meta_df['abstract'].str.contains('|'.join(searchfor), na = False, case=False)] #doesn't consider NA and is case insensitive\nmeta_df.reset_index(inplace=True, drop=True)\nprint('After applying covid lit.', len(meta_df))\nmeta_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning languages to each article and filtering on English (meta df)\nmeta_df['language']='unknown'\nfor i in range(len(meta_df['abstract'])):   \n    meta_df['language'][i]=langid.classify(meta_df['abstract'][i])[0]\nmeta_df=meta_df[meta_df.language.isin(['en'])]\nmeta_df.reset_index(inplace=True, drop=True)\nprint('After applying languege filter', len(meta_df))\nmeta_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing the stopwords and short words (less than 3)\nfor i in range(len(meta_df.abstract)):\n    meta_df.abstract[i] = remove_stopwords(meta_df.abstract[i])\n    meta_df.abstract[i] = strip_short(meta_df.abstract[i])\n    \nfor i in range(len(target_smoking_df.abstract)):\n    target_smoking_df.abstract[i] = remove_stopwords(target_smoking_df.abstract[i])\n    target_smoking_df.abstract[i] = strip_short(target_smoking_df.abstract[i])\n    \nfor i in range(len(target_diabetes_df.abstract)):\n    target_diabetes_df.abstract[i] = remove_stopwords(target_diabetes_df.abstract[i])\n    target_diabetes_df.abstract[i] = strip_short(target_diabetes_df.abstract[i])\n    \nfor i in range(len(target_hypertension_df.abstract)):\n    target_hypertension_df.abstract[i] = remove_stopwords(target_hypertension_df.abstract[i])\n    target_hypertension_df.abstract[i] = strip_short(target_hypertension_df.abstract[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making copies for stemming \nstemmed_meta = meta_df.copy()\n\nstemmed_smoking = target_smoking_df.copy()\n#stemmed_smoking = stemmed_smoking.drop(columns=['index'])\n\nstemmed_hypertension = target_hypertension_df.copy()\n#stemmed_hypertension = stemmed_hypertension.drop(columns=['index'])\n\nstemmed_diabetes = target_diabetes_df.copy()\n#stemmed_diabetes = stemmed_diabetes.drop(columns=['index'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stemming\nfor i in range(len(stemmed_meta.abstract)):\n    stemmed_meta.abstract[i] = stem_text(stemmed_meta.abstract[i])\n\nfor i in range(len(stemmed_smoking.abstract)):\n    stemmed_smoking.abstract[i] = stem_text(stemmed_smoking.abstract[i])\n\nfor i in range(len(stemmed_diabetes.abstract)):\n    stemmed_diabetes.abstract[i] = stem_text(stemmed_diabetes.abstract[i])\n\nfor i in range(len(stemmed_hypertension.abstract)):\n    stemmed_hypertension.abstract[i] = stem_text(stemmed_hypertension.abstract[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stemmed_meta.abstract[0])\nprint('')\nprint(stemmed_smoking.abstract[0])\nprint('')\nprint(stemmed_diabetes.abstract[0])\nprint('')\nprint(stemmed_hypertension.abstract[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing for the word cloud visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making a Matrix representing the count of each word in meta\nvectorizer_meta = CountVectorizer(max_features = 100)\nX_meta = vectorizer_meta.fit_transform(stemmed_meta.abstract)\n\n#Creating count of BOW for all articles in meta\ncorpus_meta = vectorizer_meta.get_feature_names()\ncorpus_meta = np.asarray(corpus_meta) #converting corpus to np.array\ncount_meta = pd.DataFrame(data=corpus_meta) #creating the data frame\ncount_meta.rename(columns={0 :'Key'}, inplace=True)\ncount_meta.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_meta.abstract)):\n    count_meta[str(i)] = X_meta[i].toarray()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_smoking = CountVectorizer(min_df=0.05)\nX_smoking = vectorizer_smoking.fit_transform(stemmed_smoking.abstract)\n\n#Creating count of BOW for all articles in target\ncorpus_smoking = vectorizer_smoking.get_feature_names()\ncorpus_smoking = np.asarray(corpus_smoking) #converting corpus to np.array\ncount_smoking = pd.DataFrame(data=corpus_smoking) #creating the data frame\ncount_smoking.rename(columns={0 :'Key'}, inplace=True)\ncount_smoking.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_smoking.abstract)):\n    count_smoking[str(i)] = X_smoking[i].toarray()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_diabetes = CountVectorizer(min_df=0.05)\nX_diabetes = vectorizer_diabetes.fit_transform(stemmed_diabetes.abstract)\n    \ncorpus_diabetes = vectorizer_diabetes.get_feature_names()\ncorpus_diabetes = np.asarray(corpus_diabetes) #converting corpus to np.array\ncount_diabetes = pd.DataFrame(data=corpus_diabetes) #creating the data frame\ncount_diabetes.rename(columns={0 :'Key'}, inplace=True)\ncount_diabetes.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_diabetes.abstract)):\n    count_diabetes[str(i)] = X_diabetes[i].toarray()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_hypertension = CountVectorizer(min_df=0.05)\nX_hypertension = vectorizer_hypertension.fit_transform(stemmed_hypertension.abstract)\n\ncorpus_hypertension = vectorizer_hypertension.get_feature_names()\ncorpus_hypertension = np.asarray(corpus_hypertension) #converting corpus to np.array\ncount_hypertension = pd.DataFrame(data=corpus_hypertension) #creating the data frame\ncount_hypertension.rename(columns={0 :'Key'}, inplace=True)\ncount_hypertension.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_hypertension.abstract)):\n    count_hypertension[str(i)] = X_hypertension[i].toarray()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding a total column\ncount_meta.loc[:,'Total'] = count_meta.sum(numeric_only=True, axis=1)\ncount_smoking.loc[:,'Total'] = count_smoking.sum(numeric_only=True, axis=1)\ncount_diabetes.loc[:,'Total'] = count_diabetes.sum(numeric_only=True, axis=1)\ncount_hypertension.loc[:,'Total'] = count_hypertension.sum(numeric_only=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sorting based on total counts\ncount_meta = count_meta.sort_values('Total', ascending = False)\ncount_smoking = count_smoking.sort_values('Total', ascending = False)\ncount_diabetes = count_diabetes.sort_values('Total', ascending = False)\ncount_hypertension = count_hypertension.sort_values('Total', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_meta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_smoking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_diabetes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_hypertension","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making smaller dfs\ncount_meta = count_meta['Total']\ncount_smoking = count_smoking['Total']\ncount_diabetes = count_diabetes['Total']\ncount_hypertension = count_hypertension['Total']   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_meta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_smoking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_diabetes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_hypertension.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now that we have our bag of words and their counts let's make some visualizations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#word cloud to see what our meta data is mostly about\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS\ntext = meta_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Smoking word cloud:\")\ntext = target_smoking_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Diabetes word cloud:\")\ntext = target_diabetes_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Hypertension word cloud:\")\ntext = target_hypertension_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we could have guessed our meta table is mostly about Covid and our target table is mostly about risk factors. \n### However, we should remove the noises that are not relevant to our topics and mess with our modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 100 words in meta we will choose the noise from them\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    display(count_meta.index.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"noise = ['re', 'charactist', 'dim','chronic' , 'obsv', 'intub','mortal', 'sev', 'charactist', 'obsv','conduct','admiss', 'factor', 'death', 'sex', 'male', 'old', 'march', 'blood', 'great', 'low', 'discharg', 'adult', 'bmi', 'logist', 'diff', 'respect', 'icu', 'fatal', 'main', 'numb', 'point', 'state', 'er', 'ratio','independ','characterist','critic','predict','progress','meta','odd','multivari','ill','non','laboratori','nlr','cohort','count', 'record', 'find', 'regress','median', 'score','observ', 'retrospect','survivor', 'assess','significantli', 'adjt', 'total','earli','index','help', 'analys', 'unit','admit', 'preval', 'peopl','analyz','common', 'scienc','februari','follow', 'signific','calcul', 'collect','like', 'evalu', 'examin', 'search','random','end','januari','suggest','background', 'articl', 'copyright', 'rightreserv', 'object', 'import', 'aim', 'covid','patient','cov','infect','diseas','sar','case','coronaviru','pandem','studi','health','sever','result','clinic','risk','data','respiratori','test','hospit','care','report','includ','time','effect','model','treatment','method','viru','outbreak','number','dai','increas','provid','spread','base','associ','acut','measur','develop','control','symptom','china','caus','rate','differ','countri','present','epidem','high','conclus','emerg','current','group','us','public','syndrom','posit','viral','need','level','confirm','transmiss','medic','novel','respons','popul','new','potenti','ag','prevent','identifi','analysi','compar','cell','manag','human','global','review','social','outcom','relat','world','year','drug','detect','import','impact','inform','protect','specif','activ','perform','higher','gener','wuhan','avail','show']\nprint(len(noise))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This next step takes a lot of Time!!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing noise words from meta\n#noise = ['re', 'charactist', 'dim','chronic' , 'obsv', 'intub','mortal', 'sev', 'charactist', 'obsv','conduct','admiss', 'factor', 'death', 'sex', 'male', 'old', 'march', 'blood', 'great', 'low', 'discharg', 'adult', 'bmi', 'logist', 'diff', 'respect', 'icu', 'fatal', 'main', 'numb', 'point', 'state', 'er', 'ratio','independ','characterist','critic','predict','progress','meta','odd','multivari','ill','non','laboratori','nlr','cohort','count', 'record', 'find', 'regress','median', 'score','observ', 'retrospect','survivor', 'assess','significantli', 'adjt', 'total','earli','index','help', 'analys', 'unit','admit', 'preval', 'peopl','analyz','common', 'scienc','februari','follow', 'signific','calcul', 'collect','like', 'evalu', 'examin', 'search','random','end','januari','suggest','background', 'articl', 'copyright', 'rightreserv', 'object', 'import', 'aim', 'covid','patient','cov','infect','diseas','sar','case','coronaviru','pandem','studi','health','sever','result','clinic','risk','data','respiratori','test','hospit','care','report','includ','time','effect','model','treatment','method','viru','outbreak','number','dai','increas','provid','spread','base','associ','acut','measur','develop','control','symptom','china','caus','rate','differ','countri','present','epidem','high','conclus','emerg','current','group','us','public','syndrom','posit','viral','need','level','confirm','transmiss','medic','novel','respons','popul','new','potenti','ag','prevent','identifi','analysi','compar','cell','manag','human','global','review','social','outcom','relat','world','year','drug','detect','import','impact','inform','protect','specif','activ','perform','higher','gener','wuhan','avail','show']\nfor i in range(len(stemmed_meta['abstract'])):\n    for j in range(len(noise)):\n        stemmed_meta['abstract'][i] = re.sub(noise[j], r'', stemmed_meta['abstract'][i]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing noise words from target tables\nfor i in range(len(stemmed_smoking['abstract'])):\n    for j in range(len(noise)):\n        stemmed_smoking['abstract'][i] = re.sub(noise[j], r'', stemmed_smoking['abstract'][i]) \n\nfor i in range(len(stemmed_diabetes['abstract'])):\n    for j in range(len(noise)):\n        stemmed_diabetes['abstract'][i] = re.sub(noise[j], r'', stemmed_diabetes['abstract'][i]) \n\nfor i in range(len(stemmed_hypertension['abstract'])):\n    for j in range(len(noise)):\n        stemmed_hypertension['abstract'][i] = re.sub(noise[j], r'', stemmed_hypertension['abstract'][i]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying the strip short and multiple white spaces on all documents again\nfor i in range(len(stemmed_meta.abstract)):\n    stemmed_meta.abstract[i] = strip_short(stemmed_meta.abstract[i], minsize = 2)\n    stemmed_meta.abstract[i] = strip_multiple_whitespaces(str(stemmed_meta.abstract[i]))\n\nfor i in range(len(stemmed_smoking.abstract)):\n    stemmed_smoking.abstract[i] = strip_short(stemmed_smoking.abstract[i], minsize = 2)\n    stemmed_smoking.abstract[i] = strip_multiple_whitespaces(str(stemmed_smoking.abstract[i]))\n\nfor i in range(len(stemmed_diabetes.abstract)):\n    stemmed_diabetes.abstract[i] = strip_short(stemmed_diabetes.abstract[i], minsize = 2)\n    stemmed_diabetes.abstract[i] = strip_multiple_whitespaces(str(stemmed_diabetes.abstract[i]))\n\nfor i in range(len(stemmed_hypertension.abstract)):\n    stemmed_hypertension.abstract[i] = strip_short(stemmed_hypertension.abstract[i], minsize = 2)\n    stemmed_hypertension.abstract[i] = strip_multiple_whitespaces(str(stemmed_hypertension.abstract[i]))\n\nprint(stemmed_meta['abstract'][0])\nprint('')\nprint(stemmed_smoking['abstract'][0])\nprint('')\nprint(stemmed_diabetes['abstract'][0])\nprint('')\nprint(stemmed_hypertension['abstract'][0])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We recreate the count dataframe for the target table to see the effects ==> smoking \nvectorizer_smoking = CountVectorizer(min_df=0.05)\nX_smoking = vectorizer_smoking.fit_transform(stemmed_smoking.abstract)\n#Creating count of BOW for all articles in target\ncorpus_smoking = vectorizer_smoking.get_feature_names()\ncorpus_smoking = np.asarray(corpus_smoking) #converting corpus to np.array\ncount_smoking = pd.DataFrame(data=corpus_smoking) #creating the data frame\ncount_smoking.rename(columns={0 :'Key'}, inplace=True)\ncount_smoking.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_smoking.abstract)):\n    count_smoking[str(i)] = X_smoking[i].toarray()[0]\ncount_smoking.loc[:,'Total'] = count_smoking.sum(numeric_only=True, axis=1)\ncount_smoking = count_smoking.sort_values('Total', ascending = False)\ncount_smoking = count_smoking['Total']\nprint (\"New smoking word cloud:\")\nd = {}\nd = count_smoking.to_dict()\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(d)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We recreate the count dataframe for the target table to see the effects ==> diabetes\nvectorizer_diabetes = CountVectorizer(min_df=0.05)\nX_diabetes = vectorizer_diabetes.fit_transform(stemmed_diabetes.abstract)\n#Creating count of BOW for all articles in target\ncorpus_diabetes = vectorizer_diabetes.get_feature_names()\ncorpus_diabetes = np.asarray(corpus_diabetes) #converting corpus to np.array\ncount_diabetes = pd.DataFrame(data=corpus_diabetes) #creating the data frame\ncount_diabetes.rename(columns={0 :'Key'}, inplace=True)\ncount_diabetes.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_diabetes.abstract)):\n    count_diabetes[str(i)] = X_diabetes[i].toarray()[0]\ncount_diabetes.loc[:,'Total'] = count_diabetes.sum(numeric_only=True, axis=1)\ncount_diabetes = count_diabetes.sort_values('Total', ascending = False)\ncount_diabetes = count_diabetes['Total']\nprint (\"New diabetes word cloud:\")\nd = {}\nd = count_diabetes.to_dict()\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(d)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We recreate the count dataframe for the target table to see the effects ==> hypertension \nvectorizer_hypertension = CountVectorizer(min_df=0.05)\nX_hypertension = vectorizer_hypertension.fit_transform(stemmed_hypertension.abstract)\n#Creating count of BOW for all articles in target\ncorpus_hypertension = vectorizer_hypertension.get_feature_names()\ncorpus_hypertension = np.asarray(corpus_hypertension) #converting corpus to np.array\ncount_hypertension = pd.DataFrame(data=corpus_hypertension) #creating the data frame\ncount_hypertension.rename(columns={0 :'Key'}, inplace=True)\ncount_hypertension.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_hypertension.abstract)):\n    count_hypertension[str(i)] = X_hypertension[i].toarray()[0]\ncount_hypertension.loc[:,'Total'] = count_hypertension.sum(numeric_only=True, axis=1)\ncount_hypertension = count_hypertension.sort_values('Total', ascending = False)\ncount_hypertension = count_hypertension['Total']\nprint (\"New hypertension word cloud:\")\nd = {}\nd = count_hypertension.to_dict()\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(d)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explain here about comorbid...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping duplicates again\n#stemmed_meta=stemmed_meta.drop(index=20963)\n#stemmed_meta.sort_values('abstract')\n#stemmed_meta2 = stemmed_meta.copy()\n#for i in range(len(stemmed_meta.abstract)):\n#    stemmed_meta2.title[i] = stem_text(stemmed_meta2.title[i])\nstemmed_meta.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_meta.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_meta.reset_index(inplace=True, drop=True)\nstemmed_smoking.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_smoking.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_smoking.reset_index(inplace=True, drop=True)\nstemmed_diabetes.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_diabetes.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_diabetes.reset_index(inplace=True, drop=True)\nstemmed_hypertension.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_hypertension.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_hypertension.reset_index(inplace=True, drop=True)\n#print(stemmed_meta.loc[12374,'abstract'])\n#print(stemmed_meta.loc[1402, 'abstract'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'meta_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_meta,outfile)\noutfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'smoking_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_smoking,outfile)\noutfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'diabetes_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_diabetes,outfile)\noutfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'hypertension_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_hypertension,outfile)\noutfile.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading stemmed datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the stemmed data\n#path3 = '../input/output/'\npath3 = '../input/stemmed-data/'\n#infile = open(path3+'meta_stemmed','rb')\ninfile = open(path3 +'meta_stemmed','rb')\nstemmed_meta = pickle.load(infile)\ninfile.close()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the stemmed data\ninfile = open(path3+'smoking_stemmed','rb')\nstemmed_smoking = pickle.load(infile)\ninfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the stemmed data\ninfile = open(path3+'hypertension_stemmed','rb')\nstemmed_hypertension = pickle.load(infile)\ninfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading the stemmed data\ninfile = open(path3+'diabetes_stemmed','rb')\nstemmed_diabetes = pickle.load(infile)\ninfile.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing for doc2vec model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the stemmed abstracts into a list\nmeta_list = list(stemmed_meta.abstract)\nsmoking_list = list(stemmed_smoking.abstract)\ndiabetes_list = list(stemmed_diabetes.abstract)\nhypertension_list = list(stemmed_hypertension.abstract)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting corpus\nmeta_corpus = [doc.split() for doc in meta_list]\nprint(meta_corpus[0])\nprint('')\nsmoking_corpus = [doc.split() for doc in smoking_list]\nprint(smoking_corpus[0])\nprint('')\ndiabetes_corpus = [doc.split() for doc in diabetes_list]\nprint(diabetes_corpus[0])\nprint('')\nhypertension_corpus = [doc.split() for doc in hypertension_list]\nprint(hypertension_corpus[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initiaize the model building vocabulary with the abstracts from meta\nmeta_sentences = [TaggedDocument(doc, [i]) for i, doc in enumerate(meta_corpus)]\nd2v_model = Doc2Vec(vector_size=20, min_count=5, workers=11, alpha=0.025, epochs=200)\nd2v_model.build_vocab(meta_sentences)\nd2v_model.train(meta_sentences, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\nd2v_model.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the model trainee with the orginal dataset\nfrom gensim.test.utils import get_tmpfile\nfname = get_tmpfile(\"my_doc2vec_model\")\nd2v_model.save(fname)\nmodel = Doc2Vec.load(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the model in a pickle file, because it takes at list half an hour to run...\npickle.dump(model, open(\"d2v_meta_saved.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the target smoking\nd2v_smoking = []\nfor i in range(len(smoking_corpus)):\n    model.random.seed(0)\n    d2v_smoking.append(model.infer_vector(smoking_corpus[i], epochs=200))\n\nd2v_smoking[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the target diabetes\nd2v_diabetes = []\nfor i in range(len(diabetes_corpus)):\n    model.random.seed(0)\n    d2v_diabetes.append(model.infer_vector(diabetes_corpus[i], epochs=200))\n\nd2v_diabetes[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the model to the target hypertension\nd2v_hypertension = []\nfor i in range(len(hypertension_corpus)):\n    model.random.seed(0)\n    d2v_hypertension.append(model.infer_vector(hypertension_corpus[i], epochs=200))\n\nd2v_hypertension[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking target 1 as an example in the filtered dataset\ncheck = stemmed_meta.title.str.contains(stemmed_smoking.title[1])\nprint(stemmed_smoking.title[1])\nprint('check:')\nprint(stemmed_meta[check])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmed_meta[check]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cheking the embedding for the model equivalent to the smoking 1 \nmodel[16819]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2v_smoking[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking that the similarity identifies the document on the original dataset\nsimilars = model.docvecs.most_similar(positive=[d2v_smoking[1]], topn=3)\nprint(similars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirming that the stemmed abstracts are identical #####CHECK HERE#####\nprint(stemmed_meta['title'][16819])\nprint(meta_list[16819])\nprint(smoking_list[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To get similar docs to the target:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making a function for getting the top similar to add to the target table\n\ndef get_similar_docs(target_df, meta_df, d2v_model, d2v_target):\n    \"\"\"\n      This function takes:\n      [1] a target table dataframe\n      [2] the metadata table dataframe\n      [3] doc2vec model based on the metadata abstracts\n      [4] doc2vec model of the target table obtained with the metadata doc2vec model\n\n      Both the target and the metadata tables should contain columns: title, abstract and pdf_json_files.\n\n      For this function to run successfully, \n      the following packages need to be installed:\n       from gensim.models.doc2vec import Doc2Vec\n       import pandas as pd\n\n      At the end it prints the value count of the final dataframes that contains the following columns:\n      ('index', 'original_db', 'similarity_percentage', 'title', 'abstract', 'pdf_json_files');\n\n      It mades 3 dataframes:\n      * not_target: it contains all the new docs found\n      * similar_to_target_df: it contains the original results from the similarity function (target articles + 1st, 2nd and 3rd most similar)\n      * new_docs_target_df: it contains the target articles + similar docs that are not in the target table\n      \n      At the end it returns the new_docs_target_df and the not_target.\n      \n    \"\"\"\n    # Run the similarity test assuming all titles are in the filtered dataset:\n    similar_to_target = []\n    for i in range(len(target_df.title)):\n        sim_test = d2v_model.docvecs.most_similar(positive=[d2v_target[i]], topn=3)\n        #this way the list could be used to create a dataframe\n        similar_to_target.append([target_df['index'][i], 'target', 1, \n                                  target_df.title[i], \n                                  target_df.abstract[i], \n                                  target_df.pdf_json_files[i]])\n        \n        similar_to_target.append([meta_df['index'][sim_test[0][0]], 'most similar', sim_test[0][1], \n                                  meta_df.title[sim_test[0][0]], \n                                  meta_df.abstract[sim_test[0][0]], \n                                  meta_df.pdf_json_files[sim_test[0][0]]])\n\n        #checking if the second and third most similar docs are in target table, if not then append them:\n        if meta_df.title[sim_test[1][0]] not in list(target_df.title):\n            similar_to_target.append([meta_df['index'][sim_test[1][0]], 'second most similar', sim_test[1][1], \n                                      meta_df.title[sim_test[1][0]], meta_df.abstract[sim_test[1][0]], meta_df.pdf_json_files[sim_test[1][0]]])\n        \n        elif meta_df.title[sim_test[2][0]] not in list(target_df.title):\n            similar_to_target.append([meta_df['index'][sim_test[2][0]], 'third most similar', sim_test[2][1], \n                                      meta_df.title[sim_test[2][0]], meta_df.abstract[sim_test[2][0]], meta_df.pdf_json_files[sim_test[2][0]]])\n\n# creating a dataframe with the top 3 most similar docs of the target ones!\n    df_colum = ['original_index', 'original_db', 'similarity_percentage', 'title', 'abstract', 'pdf_json_files']\n    similar_to_target_df = pd.DataFrame(similar_to_target, index=range(len(similar_to_target)), columns=df_colum)\n    # removing the duplicates\n    new_docs_target_df = similar_to_target_df.drop_duplicates(subset='title', keep=\"first\", inplace=False)\n    new_docs_target_df.reset_index(drop=True, inplace=True)\n    # filtering the target docs, and staying only with the new docs\n    not_target = new_docs_target_df[new_docs_target_df['original_db'] !='target']\n    not_target.reset_index(drop=True, inplace=True)\n    \n    print('From the orginal similarity test, we get a total of ' + str(len(similar_to_target_df)) +' articles, counting the target ones and their most similars from metadata.')\n    print('After filtering the duplicates from that dataframe, we get a total of ' + str(len(new_docs_target_df)) +' articles.')\n    print('Finally, after filtering the target ones, we end with a total of ' + str(len(not_target)) +' new possible articles for the target table.')\n    \n    #return not_target, similar_to_target_df, new_docs_target_df\n    return new_docs_target_df, not_target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To get relevant documents checking json files:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_relevant_docs(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    At the end it will print the number of relevant docs and \n    return a list of lists for each relevant article with its:\n        [0] = original index,\n        [1] = original body text \n        [2] = if target or not   \n    \n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    \n    for i in range(len(dataframe)):\n        ori_ind = dataframe.original_index[i]\n        ori_tab = dataframe.original_db[i]\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n            body = obj['body_text']\n            # having a list of parts of the text for better parsing\n            just_text = [body[d]['text'] for d in range(len(body))]\n            clean_body = [text.lower() for text in just_text]\n            clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n            clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n            clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n            clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n            clean_body = [strip_short(text) for text in clean_body]\n            stem_body = [stem_text(text) for text in clean_body]\n            relevant_parts = []\n            # check if the doc is related with the target\n            for t in range(len(stem_body)):\n                if target in stem_body[t]:\n                    # save the index of relevant parts\n                    relevant_parts.append(t)\n            # save the docs in a list that has: target_index, clean_json_body, original_json_body\n            if len(relevant_parts) != 0:\n                # convert json body_text into a text to have the original text \n                original_text=''\n                for d in range(len(body)):\n                    original_text = original_text+body[d]['text']\n                related_docs.append([ori_ind, original_text, ori_tab])\n                #related_docs.append([ori_ind, original_text, relevant_parts, clean_body])  \n        except:\n            TypeError\n        \n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To build a final dataframe of the new docs for the target table:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_relevant_docs_df(meta_df, target_df, relevant_docs_list):\n    \"\"\"\n    This function needs the metadata dataframe, the target dataframe and a list of relevant documents.\n    \n    The list of relevant documents must contain one list for each relevant doc, that \n    has 3 values: [0] = original index,\n                  [1] = original body text \n                  [2] = if target or not  \n                  \n    Finally this function returns a dataframe with all the relevant documents body text obtained from the json file\n    and its corresponding columns from the metadata table.\n    \n    \"\"\"\n    relevant_for_target = []\n    for i in range(len(relevant_docs_list)): \n    #this way the list could be used to create a dataframe\n        index_rev = relevant_docs_list[i][0]\n        #print('from relevant list')\n        #print(index_rev)\n        \n        if relevant_docs_list[i][2] != 'target':\n            df_index = list(meta_df[meta_df['index']== index_rev].index)[0]\n            relevant_for_target.append([index_rev, \n                                        meta_df.publish_time[df_index],  \n                                        meta_df.title[df_index], \n                                        meta_df.abstract[df_index], \n                                        meta_df.cord_uid[df_index], \n                                        meta_df.doi[df_index],\n                                        meta_df.journal[df_index],\n                                        meta_df.url[df_index],\n                                        meta_df.pdf_json_files[df_index],\n                                        relevant_docs_list[i][1],\n                                        relevant_docs_list[i][2]\n                                       ])\n        elif relevant_docs_list[i][2] == 'target':\n            df_index = list(target_df[target_df['index']== index_rev].index)[0]\n            #print('in target table')\n            #print(df_index)\n            \n            relevant_for_target.append([index_rev, \n                                        target_df.Date[df_index],  \n                                        target_df.Study[df_index], \n                                        target_df.abstract[df_index], \n                                        target_df.cord_uid[df_index], \n                                        target_df.doi[df_index],\n                                        target_df.journal[df_index],\n                                        target_df.url[df_index],\n                                        target_df.pdf_json_files[df_index],\n                                        relevant_docs_list[i][1],\n                                        relevant_docs_list[i][2]\n                                       ])\n        \n    # creating a dataframe with the relevant docs including all needed columns\n    df_colum = ['original_index', 'publish_time', 'title', 'abstract', 'cord_uid', 'doi',\n                    'journal', 'url', 'pdf_json_files', 'body_text', 'target_or_not']\n    relevant_df = pd.DataFrame(relevant_for_target, index=range(len(relevant_for_target)), columns=df_colum)\n    return relevant_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the function to get similar docs and obtaining **new_docs_target_df, not_in_target_df**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# For smoking target\nsmoking_new_docs, smoking_not_target = get_similar_docs(stemmed_smoking, stemmed_meta, model, d2v_smoking)\nsmoking_new_docs.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For diabetes\ndiabetes_new_docs, diabetes_not_target = get_similar_docs(stemmed_diabetes, stemmed_meta, model, d2v_diabetes)\ndiabetes_new_docs.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For hypertension\nhyper_new_docs, hyper_not_target = get_similar_docs(stemmed_hypertension, stemmed_meta, model, d2v_hypertension)\nhyper_new_docs.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting the **relevant documents based on their body text**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Modifying function so that it works with target table dataframe\ndef get_relevant_docs_mod(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    At the end it will print the number of relevant docs and \n    return a list of lists for each relevant article with its:\n        [0] = parts of the body where the target appears\n        [1] = clean body   \n    \n    IT IS MODIFIED SO IT WORKS WITH TARGET TABLE!!\n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    \n    for i in range(len(dataframe)):\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n            body = obj['body_text']\n            # having a list of parts of the text for better parsing\n            just_text = [body[d]['text'] for d in range(len(body))]\n            clean_body = [text.lower() for text in just_text]\n            clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n            clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n            clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n            clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n            clean_body = [strip_short(text) for text in clean_body]\n            stem_body = [stem_text(text) for text in clean_body]\n            relevant_parts = []\n            # check if the doc is related with the target\n            for t in range(len(stem_body)):\n                if target in stem_body[t]:\n                    # save the index of relevant parts\n                    relevant_parts.append(t)\n            # save the docs in a list \n            if len(relevant_parts) != 0:\n                related_docs.append([relevant_parts, clean_body])   \n        except:\n            TypeError\n            #print('error')\n            \n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stem_text('smoking')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking how many json files we have available\nprint('pdf json files we have available in original target table')\nprint(stemmed_smoking.pdf_json_files.notna().sum())\nprint('pdf json files we have available in new target table')\nprint(smoking_new_docs.pdf_json_files.notna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For smoking\nprint('Original relevant target articles: ')\nsmoking_target_check = get_relevant_docs_mod(stemmed_smoking, 'smoke')\nprint('New articles added: ')\nsmoking_new_relevant = get_relevant_docs(smoking_not_target, 'smoke')\nprint('Total relevant target articles: ')\nsmoking_relevant_docs = get_relevant_docs(smoking_new_docs, 'smoke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stem_text('diabetes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking how many json files we have available\nprint('pdf json files we have available in original target table')\nprint(stemmed_diabetes.pdf_json_files.notna().sum())\nprint('pdf json files we have available in new target table')\nprint(diabetes_new_docs.pdf_json_files.notna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For diabetes\nprint('Original relevant target articles: ')\ndiabetes_target_check = get_relevant_docs_mod(stemmed_diabetes, 'diabet')\nprint('New articles added: ')\ndiabetes_new_relevant = get_relevant_docs(diabetes_not_target, 'diabet')\nprint('Total relevant target articles: ')\ndiabetes_relevant_docs = get_relevant_docs(diabetes_new_docs, 'diabet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stem_text('hypertension')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking how many json files we have available\nprint('pdf json files we have available in original target table')\nprint(stemmed_hypertension.pdf_json_files.notna().sum())\nprint('pdf json files we have available in new target table')\nprint(hyper_new_docs.pdf_json_files.notna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For hypertension\nprint('Original relevant target articles: ')\nhyper_target_check = get_relevant_docs_mod(stemmed_hypertension, 'hypertens')\nprint('New articles added: ')\nhyper_new_relevant = get_relevant_docs(hyper_not_target, 'hypertens')\nprint('Total relevant target articles: ')\nhyper_relevant_docs = get_relevant_docs(hyper_new_docs, 'hypertens')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating the relevant docs dataframes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# For smoking\nsmoking_relevant_df = build_relevant_docs_df(stemmed_meta, stemmed_smoking, smoking_relevant_docs)\nsmoking_relevant_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoking_relevant_df.target_or_not.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle.dump(smoking_relevant_df, open(\"smoking_new_target.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For diabetes\ndiabetes_relevant_df = build_relevant_docs_df(stemmed_meta, stemmed_diabetes,diabetes_relevant_docs)\ndiabetes_relevant_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_relevant_df.target_or_not.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle.dump(diabetes_relevant_df, open(\"diabetes_new_target.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For hypertension\nhyper_relevant_df = build_relevant_docs_df(stemmed_meta, stemmed_hypertension,hyper_relevant_docs)\nhyper_relevant_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_relevant_df.target_or_not.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pickle.dump(hyper_relevant_df, open(\"hypertension_new_target.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating summaries for new target tables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### name of smoke_df could be changed to the dataframe name which is currently worked on for the relevant risk factor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#smoke_df = pickle.load(open('smoking_new_target.pkl', 'rb'))\nsmoke_df = smoking_relevant_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#diabetes_df = pickle.load(open('diabetes_new_target.pkl', 'rb'))\ndiabetes_df = diabetes_relevant_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hypertension_df = pickle.load(open('hypertension_new_target.pkl', 'rb'))\nhypertension_df = hyper_relevant_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### creating two new columns in the dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_df['sent']=''     # this column will store tokenized version of the body text\nsmoke_df['summary']=''  # this column will store the summaries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_df['sent']=''     # this column will store tokenized version of the body text\ndiabetes_df['summary']=''  # this column will store the summaries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hypertension_df['sent']=''     # this column will store tokenized version of the body text\nhypertension_df['summary']=''  # this column will store the summaries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### tokenizing the full body text in sentences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize # PASS THIS T THE BEGINNING\nfor i in range(len(smoke_df.body_text)):\n    smoke_df.sent[i]=sent_tokenize(smoke_df.body_text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(diabetes_df.body_text)):\n    diabetes_df.sent[i]=sent_tokenize(diabetes_df.body_text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(hypertension_df.body_text)):\n    hypertension_df.sent[i]=sent_tokenize(hypertension_df.body_text[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### defining a count function specialized on a given word","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def countOccurences(sentence, word):     \n    # split the string by spaces in a \n    a = sentence.split()  \n    # search for pattern in a \n    count = 0\n    for i in range(0, len(a)):           \n        # if match found increase count  \n        if (word == a[i]): \n            count = count + 1             \n    return count ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identify different stemmed version of smok*\nprint(stem_text(\"smoker\"))\nprint(stem_text(\"smoking\"))\nprint(stem_text(\"smokers\"))\nprint(stem_text(\"smoke\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stem_text(\"diabetes\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stem_text(\"hypertension\"))\nprint(stem_text(\"hypertensive\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### identify the sentences which mention the risk factor the most","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### For smoking:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(smoke_df.body_text)):\n    # d is an internal dataframe for preprocessing on sentence level of the documents and count occurence of relveant words\n    d=pd.DataFrame(index=np.arange(len(smoke_df.sent[j])))\n    d['content']= 'none'\n    d['original']='none'\n    d['index']=d.index\n    d['count']= 0\n    for k in range(len(smoke_df.sent[j])): #iterating through sentences of a document\n        d.content[k]=smoke_df.sent[j][k]\n        d.original[k]=smoke_df.sent[j][k]\n        d.content[k] =  strip_numeric(d.content[k]) #Remove digits\n        d.content[k] =  strip_punctuation(d.content[k])  #Remove punctuation\n        d.content[k] =  strip_multiple_whitespaces(d.content[k]) #Remove multiple whitespaces   \n        d.content[k] = d.content[k].lower() # lower characters\n        d.content[k] = remove_stopwords(d.content[k]) #remove stopwords\n        d.content[k] = strip_short(d.content[k]) # remove short words\n        d.content[k] = stem_text(d.content[k]) #stem the words \n        sentence = d.content[k] # \n        word =stem_text(\"smoke\") # get first relevant word\n        word2 =stem_text(\"smoker\") # get second relevant word\n        d['count'][k]=float((countOccurences(sentence, word))) +float((countOccurences(sentence, word2))) #storing the amount of relevant words\n    x=d.loc[d['count'].idxmax()] # getting the dataframe d with the maximal amount of relevant words   \n    smoke_df.summary[j]=x['original'] # storing the sentence with the maximal amount of relevant words in the summary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### For diabetes:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(diabetes_df.body_text)):\n    # d is an internal dataframe for preprocessing on sentence level of the documents and count occurence of relveant words\n    d=pd.DataFrame(index=np.arange(len(diabetes_df.sent[j])))\n    d['content']= 'none'\n    d['original']='none'\n    d['index']=d.index\n    d['count']= 0\n    for k in range(len(diabetes_df.sent[j])): #iterating through sentences of a document\n        d.content[k]= diabetes_df.sent[j][k]\n        d.original[k]= diabetes_df.sent[j][k]\n        d.content[k] =  strip_numeric(d.content[k]) #Remove digits\n        d.content[k] =  strip_punctuation(d.content[k])  #Remove punctuation\n        d.content[k] =  strip_multiple_whitespaces(d.content[k]) #Remove multiple whitespaces   \n        d.content[k] = d.content[k].lower() # lower characters\n        d.content[k] = remove_stopwords(d.content[k]) #remove stopwords\n        d.content[k] = strip_short(d.content[k]) # remove short words\n        d.content[k] = stem_text(d.content[k]) #stem the words \n        sentence = d.content[k] # \n        word =stem_text(\"diabetes\") # get first relevant word     \n        d['count'][k]=float((countOccurences(sentence, word)))  #storing the amount of relevant words\n    x=d.loc[d['count'].idxmax()] # getting the dataframe d with the maximal amount of relevant words   \n    diabetes_df.summary[j]=x['original'] # storing the sentence with the maximal amount of relevant words in the summary\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### For hypertension:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(hypertension_df.body_text)):\n    # d is an internal dataframe for preprocessing on sentence level of the documents and count occurence of relveant words\n    d=pd.DataFrame(index=np.arange(len(hypertension_df.sent[j])))\n    d['content']= 'none'\n    d['original']='none'\n    d['index']=d.index\n    d['count']= 0\n    for k in range(len(hypertension_df.sent[j])): #iterating through sentences of a document\n        d.content[k]= hypertension_df.sent[j][k]\n        d.original[k]= hypertension_df.sent[j][k]\n        d.content[k] =  strip_numeric(d.content[k]) #Remove digits\n        d.content[k] =  strip_punctuation(d.content[k])  #Remove punctuation\n        d.content[k] =  strip_multiple_whitespaces(d.content[k]) #Remove multiple whitespaces   \n        d.content[k] = d.content[k].lower() # lower characters\n        d.content[k] = remove_stopwords(d.content[k]) #remove stopwords\n        d.content[k] = strip_short(d.content[k]) # remove short words\n        d.content[k] = stem_text(d.content[k]) #stem the words \n        sentence = d.content[k] # \n        word =stem_text(\"hypertension\") # get first relevant word     \n        d['count'][k]=float((countOccurences(sentence, word)))  #storing the amount of relevant words\n    x=d.loc[d['count'].idxmax()] # getting the dataframe d with the maximal amount of relevant words   \n    hypertension_df.summary[j]=x['original'] # storing the sentence with the maximal amount of relevant words in the summary\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### getting rid of columns which we don't need anymore","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_df = smoke_df.drop(columns=[\"sent\",\"body_text\"]) \n# body_text and sent are not necessary anymore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_df = diabetes_df.drop(columns=[\"sent\",\"body_text\"]) \n# body_text and sent are not necessary anymore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hypertension_df = hypertension_df.drop(columns=[\"sent\",\"body_text\"]) \n# body_text and sent are not necessary anymore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### display final dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hypertension_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### how the summaries looks like","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(smoke_df)):\n    print('')\n    print(\"Title:\",smoke_df.title[i],\",\" ,smoke_df.publish_time[i], \"(\",smoke_df.target_or_not[i], \")\")\n    print('')\n    print(\"Summary:\",smoke_df.summary[i])\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(diabetes_df)):\n    print('')\n    print(\"Title:\",diabetes_df.title[i],\",\" ,diabetes_df.publish_time[i], \"(\",diabetes_df.target_or_not[i], \")\")\n    print('')\n    print(\"Summary:\",diabetes_df.summary[i])\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(hypertension_df)):\n    print('')\n    print(\"Title:\",hypertension_df.title[i],\",\" ,hypertension_df.publish_time[i], \"(\",hypertension_df.target_or_not[i], \")\")\n    print('')\n    print(\"Summary:\",hypertension_df.summary[i])\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}