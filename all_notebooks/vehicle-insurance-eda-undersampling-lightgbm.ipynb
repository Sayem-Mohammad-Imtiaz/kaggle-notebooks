{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Welcome to my Notebook</h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import classification_report,roc_auc_score,confusion_matrix,accuracy_score,f1_score\n\nimport optuna\nfrom optuna.samplers import TPESampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')\ntest = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/test.csv')\nsample = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/sample_submission.csv')\n\nsns.set(style='white', context='notebook', palette='deep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Exploratory Data Analysis</h1>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = pd.concat([train.drop(['id','Response'],axis=1),test.drop('id',axis=1)],axis=0)\ny = train['Response']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's combine the train and test set so our transformations are easier as we don't have to apply them seperately to each set"},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Categorical Features Data Analysis</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(train,values=train['Response'].value_counts(),names=['Class 0','Class 1'],hole=0.6,labels={0:'Response = 0'},color_discrete_sequence=px.colors.sequential.Sunset)\nfig.show(showlegend=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the above visualisations that:\n\n* The Data is highly imbalanced, with 87.7% belonging to Class 0 and 12.3% belonging to Class 1. We will use both UnderSampling and OverSampling to balance the data out equally"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['Driving_License'],train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see that people who have the a driving license are more likely to be interested in getting insurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['Previously_Insured'],train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now,we see that people who are not previously insured are more likely to be interested in getting insurance, as one does not want to pay for multiple insurances"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['Vehicle_Damage'],train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People with vehicle damage are also more likely to be interested in getting insurance, which is self-explanatory"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['Vehicle_Age'],train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that people with older Vehicles are more likely to be interested. This is obvious, as the longer a vehicle is on the road, the more likely it is to have problems and issues as oppose to new cars"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['Gender'],train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Males are more likely to be interested in getting insured. This could be due to many possible reasons, but one could be just that more males in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(train['Age'],train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this visualisation, we see what ages are more likely to be interested in insurance. We see the following things:\n1. Ages between 20-27 are not very interested in getting insured. This can be because insurance is not their main priority, and they probably cannot afford it as they are most likely students\n2. Ages 30-55 are more interested in getting insured, as they use the car on a day to day basis to travel for work, so they are more in need to repairs \n3. Age 65+ are less likely to be interested in insurance as this is the retirment age and they do not have a need for a car any more, let alone insurance"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train['Age'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The reason for this boxplot was to see if there was any outliers(in this case, any extreme cases or accidental ages, e.g a 5 year old interested in insurance!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [20, 30, 40, 50, 60, 70,90]\nlabels = ['20-27', '28-39', '40-49', '50-59', '60-69', '70+']\nage_categories = pd.cut(train['Age'], bins, labels = labels,include_lowest = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(age_categories,train['Response'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After splitting the ages into 6 groups, we see that this visualisation confirms our observations from before; the main age groups interested in getting insurance are between 28-50."},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Numerical Features Data Analysis</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train['Annual_Premium'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that Annual Premium has a wide varietly of values"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.distplot(train['Annual_Premium'],label='Skewness: '+str(round(train['Annual_Premium'].skew(),4)))\ng = g.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Annual Premium is also skewed. We might try a log or square root transformation to mitigate the skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),annot=True,cmap='rainbow')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Feature Preprocessing</h1>"},{"metadata":{},"cell_type":"markdown","source":"Here we plot a correlation heatmap and see that there is no strong correlation between any feautures and the target feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features['Vehicle_Age'] = all_features['Vehicle_Age'].map({'> 2 Years':2,'1-2 Year':1,'< 1 Year':0})\nall_features['Vehicle_Damage'] = all_features['Vehicle_Damage'].map({'Yes':1,'No':0})\nall_features['Gender'] = all_features['Gender'].map({'Male':1,'Female':0}) \nall_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we just map categorical values to their appropriate numerical counterparts"},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Modelling</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = all_features.iloc[:len(train),:]\nX_test = all_features.iloc[len(train):,:]\n\nkf = StratifiedKFold(n_splits=12,shuffle=True,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define resplit our train and test set, and set up 12 Fold Stratified Cross Validation. It is important to stratify as this ensures that during our training and validation, we split according to target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index,val_index in kf.split(X,y):\n    X_train,X_val = X.iloc[train_index],X.iloc[val_index],\n    y_train,y_val = y.iloc[train_index],y.iloc[val_index],","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we define our validation set"},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Modelling Using Undersampling</h1>"},{"metadata":{},"cell_type":"markdown","source":"We will use `imblearn`'s RandomUnderSampler to undersample from the majority class so that they match"},{"metadata":{"trusted":true},"cell_type":"code","source":"rus = RandomOverSampler(random_state=42)\nX_rus,y_rus = rus.fit_sample(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Basic LightGBM</h1>"},{"metadata":{},"cell_type":"markdown","source":"Let's fit a vanilla LGBMClassifier on the undersampled data and evaluate it"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_rus = LGBMClassifier(random_state=42)\nlgb_rus.fit(X_rus,y_rus)\nprint(classification_report(y_val,lgb_rus.predict(X_val)))\nprint('ROC AUC Score: ' + str(roc_auc_score(y_val,lgb_rus.predict(X_val))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_val,lgb_rus.predict(X_val)),cmap='magma',annot=True,fmt='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model did suprisingly well with default parameters, but not fantastic. Let's optimize"},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Hyperparameter Tuning</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators',100,500)\n    num_leaves = trial.suggest_int('num_leaves',10,500)\n    max_depth = trial.suggest_int('max_depth',4,20)\n    learning_rate = trial.suggest_uniform('learning_rate',0.0001,1)\n    min_child_samples = trial.suggest_int('min_child_samples',10,50)\n    model = LGBMClassifier(n_estimators=n_estimators,num_leaves=num_leaves,\n    max_depth=max_depth,learning_rate=learning_rate,min_child_samples=min_child_samples)\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_rus,y_rus)\n    score = roc_auc_score(y_val,model.predict(X_val))\n    return score\n\nsampler = TPESampler(seed=42)\nstudy = optuna.create_study(sampler=sampler,direction='maximize')\nstudy.optimize(objective,n_trials=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params = study.best_params\nlgb_params['random_state'] = 42\nlgb = LGBMClassifier(**lgb_params)\nlgb.fit(X_rus, y_rus)\npreds = lgb.predict(X_val)\nprint(classification_report(y_val,lgb.predict(X_val)))\nprint('ROC AUC Score: ' + str(roc_auc_score(y_val,lgb.predict(X_val))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(confusion_matrix(y_val,lgb.predict(X_val)),cmap='magma',annot=True,fmt='g')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align='center'>Thanks and make sure to learn!</h1>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}