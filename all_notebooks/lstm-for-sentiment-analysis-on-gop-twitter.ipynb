{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n# Reference Code : https://www.kaggle.com/hassanamin/lstm-sentiment-analysis-data-imbalance-keras/edit","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport re\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true,"_uuid":"ada86f100b70b1d9069d3add59eb34f6e160f768"},"cell_type":"code","source":"# Load Data\ndata = pd.read_csv('../input/Sentiment.csv')\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\ndata = data[data.sentiment != \"Neutral\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a8116446e1bbfeb936cecfd5e1f3793fbf72078"},"cell_type":"code","source":"# Inspect data\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3369cacd3dcf0515146dacbc83f1a6c5849b3bd5"},"cell_type":"markdown","source":"# Preprocessing\n\n> A few things to notice here\n- \"RT @...\" in start of every tweet\n- a lot of special characters <br>\n> We have to remove all this noise also lets convert text into lower case.\n"},{"metadata":{"trusted":true,"_uuid":"f5418140454eba802f10c9dfc7df128433846237"},"cell_type":"code","source":"# Pre-Processing\ndata['text'] = data['text'].apply(lambda x: x.lower())\n# removing special chars\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndata['text'] = data['text'].str.replace('rt','')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"_uuid":"da2e68a5b58add0ff49d22302165410a9c83513d"},"cell_type":"markdown","source":"This looks better.<br>\nLets pre-process the data so that we can use it to train the model\n- Tokenize\n- Padding (to make all sequence of same lengths)\n- Converting sentiments into numerical data(One-hot form)\n- train test split\n"},{"metadata":{"trusted":true,"_uuid":"5e4e6c153edcbb239573982d84047c3571665cd4"},"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Padding\nX = pad_sequences(X)\n\nY = pd.get_dummies(data['sentiment']).values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/Test Split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"626e625fafa300e079f59e199553d49ca2040380"},"cell_type":"markdown","source":"# Defining Neural Network Architecture"},{"metadata":{"trusted":true,"_uuid":"02620d27ca585518896fc26a843c189704333f29"},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"114feaf3edde9b1fcb73a8c0daab0df3fc9190ec"},"cell_type":"code","source":"batch_size = 128\nNoOfEpochs = 30\nmodel.fit(X_train, Y_train, epochs = NoOfEpochs, batch_size=batch_size, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true,"_uuid":"7c19f6b675a0c0813335ad7c8018dd336f425d10"},"cell_type":"code","source":"Y_pred = model.predict_classes(X_test,batch_size = batch_size)\ndf_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\ndf_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\nprint(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\nprint(classification_report(df_test.true, df_test.pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"285e99f88a2aea0e05d4a9e14adb0e9a4f18a6f0"},"cell_type":"markdown","source":"# Solving Data Imbalance Problem\n"},{"metadata":{"trusted":true,"_uuid":"77d34e08559a99abed19fc406183d95cefde0914"},"cell_type":"code","source":"# Separate majority and minority classes\ndata_majority = data[data['sentiment'] == 'Negative']\ndata_minority = data[data['sentiment'] == 'Positive']\n\nbias = data_minority.shape[0]/data_majority.shape[0]\n# lets split train/test data first then \ntrain = pd.concat([data_majority.sample(frac=0.8,random_state=200),\n         data_minority.sample(frac=0.8,random_state=200)])\ntest = pd.concat([data_majority.drop(data_majority.sample(frac=0.8,random_state=200).index),\n        data_minority.drop(data_minority.sample(frac=0.8,random_state=200).index)])\n\ntrain = shuffle(train)\ntest = shuffle(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"966654b57046193bbc098afca35f2807137670f4"},"cell_type":"code","source":"print('positive data in training:',(train.sentiment == 'Positive').sum())\nprint('negative data in training:',(train.sentiment == 'Negative').sum())\nprint('positive data in test:',(test.sentiment == 'Positive').sum())\nprint('negative data in test:',(test.sentiment == 'Negative').sum())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"578ace90d0b900d0f8e4b50de57686ea5f0ed39f"},"cell_type":"markdown","source":"# Up-Samping To Improve Performance"},{"metadata":{"trusted":true,"_uuid":"b33b3fb2159021817a535b71cbc317c0a7f29b63"},"cell_type":"code","source":"# Separate majority and minority classes in training data for upsampling \ndata_majority = train[train['sentiment'] == 'Negative']\ndata_minority = train[train['sentiment'] == 'Positive']\n\nprint(\"majority class before upsample:\",data_majority.shape)\nprint(\"minority class before upsample:\",data_minority.shape)\n\n# Upsample minority class\ndata_minority_upsampled = resample(data_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples= data_majority.shape[0],    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\ndata_upsampled = pd.concat([data_majority, data_minority_upsampled])\n \n# Display new class counts\nprint(\"After upsampling\\n\",data_upsampled.sentiment.value_counts(),sep = \"\")\n\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values) # training with whole data\n\nX_train = tokenizer.texts_to_sequences(data_upsampled['text'].values)\nX_train = pad_sequences(X_train,maxlen=29)\nY_train = pd.get_dummies(data_upsampled['sentiment']).values\nprint('x_train shape:',X_train.shape)\n\nX_test = tokenizer.texts_to_sequences(test['text'].values)\nX_test = pad_sequences(X_test,maxlen=29)\nY_test = pd.get_dummies(test['sentiment']).values\nprint(\"x_test shape\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining Neural Network Architecture\n","attachments":{}},{"metadata":{"trusted":true,"_uuid":"267c579bf4bc1849ace11cd726d9738fc7452c21"},"cell_type":"code","source":"# model\nembed_dim = 128\nlstm_out = 192\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X_train.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.4, recurrent_dropout=0.4))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"_uuid":"65b38c50fc6c87c8a2391f58d6d625e247e7ebe7"},"cell_type":"code","source":"batch_size = 128\n# also adding weights\nclass_weights = {0: 1 ,\n                1: 1.6/bias }\nmodel.fit(X_train, Y_train, epochs = 30, batch_size=batch_size, verbose = 1,\n          class_weight=class_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true,"_uuid":"2f824e7f13fb6007e0c36b7c7ed87487c56f8ed1"},"cell_type":"code","source":"Y_pred = model.predict_classes(X_test,batch_size = batch_size)\ndf_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\ndf_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a830ff8379bb49018a25b828bd974069d6b4a78"},"cell_type":"code","source":"print(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\nprint(classification_report(df_test.true, df_test.pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a55937ad168314255a9f65d1779ce62c6f6d67ba"},"cell_type":"markdown","source":"\nSo the class imbalance is reduced significantly recall value for positive tweets (Class 1) improved from 0.54 to 0.74. It is always not possible to reduce it completely. \n\nYou may also noticed that the recall value for Negative tweets also decreased from 0.90 to 0.79  but this can be improved using training model to more epochs and tuning the hyper-parameters.\n"},{"metadata":{"trusted":true,"_uuid":"5906c0335b67e388fa813250cc6a5b711c832f28"},"cell_type":"code","source":"# running model to few more epochs\nmodel.fit(X_train, Y_train, epochs = 30, batch_size=batch_size, verbose = 1,\n          class_weight=class_weights)\nY_pred = model.predict_classes(X_test,batch_size = batch_size)\ndf_test = pd.DataFrame({'true': Y_test.tolist(), 'pred':Y_pred})\ndf_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\nprint(\"confusion matrix\",confusion_matrix(df_test.true, df_test.pred))\nprint(classification_report(df_test.true, df_test.pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}