{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/Pokemon.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the data out, How many pokemon are in the dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 13 features (columns) in our data.\n\nWe can see that not all pokemon have dual types e.g. Charmander, so lets replace any NaN values in Type 2 column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Type 2'].fillna(value='None',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check the data to see the NaN have bits been updated"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! lets start doing some visualisation to understand our data\n\n1. How much of each primary type are there?\n\n(pandas.Series.value_counts returns object containing counts of unique values)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Type 1'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Woaahhh so many water types!! we can see that there are not many primary flying types, hmm why dont we look at the type 2 count too!"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Type 2'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that having a second type is actually quite rare among all the 800 pokemon with almost 50% having no type at all.\n\nIn pokemon the Legendary pokemon, were always the coolest. Lets see how many there are?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Legendary'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Damn! The legendary Pok√©mon live up to their name of rarity with less than a 1/8 of Pokemon holding that status.\n\nI wonder if any of the other features in the dataset can indicate whether a pokemon is legendary or not!\n\nLets use a decision tree to model this problem!\n* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nlegendaryPokemon = data.loc[data['Legendary']==True]\nnormalPokemon = data.loc[data['Legendary']==False]\n# we will only use the pokemon battle stats + types to determine whether it is legendary or not \nlegendaryPokemon = legendaryPokemon[['Type 1','Type 2','Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed','Legendary']]\nnormalPokemon = normalPokemon[['Type 1','Type 2','Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed','Legendary']]\n\n# now we will randomly sample random non-legendary pokemon from the data set to balance our dataset\n\nsampledNormalPokemon = normalPokemon.sample(100)\n\n\nx = pd.concat([legendaryPokemon, sampledNormalPokemon])\nx = pd.get_dummies(x)\n# take last column as training labels and drop it from the training data\ny = x['Legendary']\nx = x.drop('Legendary', 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testNormalPokemon = pd.get_dummies(normalPokemon)\ntestNormalPokemon.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using the train_test_split to create train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(x, y, random_state = 47, test_size = 0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now that we have split our train, test data. Let's increase the amount of Legendary pokemon in our training data, \n# by creating synthetic examples using the SMOTE algorithm\nfrom imblearn.over_sampling import SMOTE\n\n# sampling ration of 1.0 will equally balance the binary classes\nsm = SMOTE(random_state=15,sampling_strategy= 1.0)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train_res.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(y_train_res == True).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier # for random forest classifier\nmodel = RandomForestClassifier(n_estimators=100,max_depth=7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the random forest classifier. \nmodel.fit(X_train_res, y_train_res)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting labels on the test set.\ny_pred =  model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the accuracy metric from sklearn.metrics library\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy Score on train data: ', accuracy_score(y_true=y_train_res, y_pred=model.predict(X_train_res)))\nprint('Accuracy Score on test data: ', accuracy_score(y_true=y_test, y_pred=y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(model.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = importances.plot.pie(y='importance', figsize=(10, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.tree \nimport graphviz \n\n# Extract single tree\nestimator = model.estimators_[4]\n\ndot_data = dot_data = sklearn.tree.export_graphviz(estimator, out_file=None, \n               feature_names=x.columns,  \n                class_names=['normal','legendary'] , filled=True, rounded=True,  special_characters=True)  \ngraph = graphviz.Source(dot_data) \n\ngraph","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}