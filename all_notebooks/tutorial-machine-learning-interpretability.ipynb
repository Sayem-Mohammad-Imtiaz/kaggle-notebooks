{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Interpreting your Machine Learning Model : Why and How\n\n[ _A tutorial made during the_ __[MLcourse.ai course](https://mlcourse.ai/)__ - 04/2019 - v1]\n\n`Christophe Rigon > datacog@free.fr`\n"},{"metadata":{},"cell_type":"markdown","source":"---\n### __IMPORTANT NOTE__: \n\nI have a serious problem installing et running properlly here one of the main librariy of this tutorial (SKATER). I don't understand why... i've tried hard but couldn't fix it.\n\nSo I invite you to **look at it directly on my Github at** :\n\n__https://github.com/cog-data/ML_Interpretability_tutorial/blob/master/Machine_Learning_Interpretability_tutorial.ipynb__\n\n... and to come-back here after if you wish to upvote it.\n\nSorry for the inconvenience.\n"},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":">_\"models are opinions embedded in mathematics\" - Cathy O'Neil (mathematician, data scientist and author of the famous \"Weapons of Math Destruction\")_\n\n>_\"Science without conscience is but the ruin of the Soul\" - Rabelais (who first quoted the word \"automaton\" in french during the 16th century)_\n\n"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://raw.githubusercontent.com/cog-data/ML_Interpretability_tutorial/master/img/mli_fun.jpg\" title=\"MLI Funny\" height=\"228\" width=\"405\"/>"},{"metadata":{},"cell_type":"markdown","source":"Machine Learning Interpretability of is a hot and crucial subject : machine learning algorithms are everywhere, becoming more and more ubiquitous, complex and efficient, and sometimes treated like __black-boxes__. They have already, and will have in the future, more and more impact on our society and our everyday's life. As (future) Data Scientist / Machine Learning Engineers, even if we clearly don't need interpretability all the time, I think we have a social and professional ethical duty trying to design models as **fair, accountable and transparent** as possible. Meaning : unbiasedness/non-discriminative, giving reliable results and being able to be queried to validate predictive decisions.The recent European GDRP (General Data Protection Regulation) resolution testifies to a citizen \"right to explanation\" of algorithmic decisions that \"significantly\" affect any individual. Further more, interpreting our machine learning processes could give us valuable insights for :\n\n- debugging\n- informing feature engineering\n- model comparisons\n- driving future data collection\n- informing human decision-making\n- and generally for better communication and trust building.\n\nThe intend of this tutorial is twofold :\n- to give a basic introduction to the subject, looking at different strategies for tackling the potential \"black-box\" problem in a **model-agnostic** fashion. \n- to have a practical grasp on some of the main frameworks actually available for machine learning interpretability like __ELI5, LIME, SKATER, or SHAP__ (and also PDPbox and FairML). \n\nAfter loading and providing a [description of the data](#descrip_data), a [quick EDA](#eda), and [training/evaluating our model](#model), we will be exploring more in depth differents interpretation techniques like : \n\n1. [Features Importance (without and with permutation)](#features_importance)\n2. [Partial Dependence Plots (PD plot)](#pd_plots) &  [Individual Conditional Expectation (ICE) plots](#ice_plots)\n3. [Model Prediction Explanations with Local Interpretation (LIME)](#lime)\n4. [SKATER model interpretation](#skater)\n5. [Building Interpretable Models with Surrogate Tree-based Models (SKATER)](#skater_tree)\n6. [Model Prediction Explanation with SHAP values](#shap)\n7. [Dependence and Interaction Plots (SHAP)](#shap_plots)\n8. [Bonus : auditing our \"black-box predictive model\" with FairML](#fairml)\n9. [Further readings and conclusion](#further_readings)\n\nWe will work :\n- on a real-world dataset about Red Wine Quality (after all ethics can, and should also be convivial ;)) \n- with an XGBoost model, which is a complex optimized distributed gradient boosting library providing a parallel tree boosting algorithm \n- and the learning task will be a classical supervised classification with binarized classes.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Load dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Elementary my Dear Watson...\nimport pandas as pd\nimport numpy as np\n\n### Graphic libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n### Some Scikit-learn utils\nfrom sklearn.model_selection import train_test_split\n\n### Metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\n\n### Models\nfrom xgboost import XGBClassifier, plot_importance\n\n########################################################\n### For an easier workflow, Interpretability libraries\n### will be installed/loaded on the fly of the tutorial\n########################################################\n\n### Some cosmetics add-ons\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the Red Wine Quality Dataset"},{"metadata":{},"cell_type":"markdown","source":"The Red Wine Quality dataset can be downloaded __[on this Kaggle page](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)__ or via the __[UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/wine+quality)__.\n\n\"This datasets is related to red variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference __[Cortez et al., 2009](http://dx.doi.org/10.1016/j.dss.2009.05.016)__. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading the csv dataset in a dataframe\ndf_raw = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\ndf_raw.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> I have chosen a small and clean dataset (1599 rows x 11+1 variables) for quick and easy (pre)processing."},{"metadata":{},"cell_type":"markdown","source":"<a name=\"descrip_data\"></a>\n## Data Description"},{"metadata":{},"cell_type":"markdown","source":"**Input variables (based on physicochemical tests)**:\n1. _fixed acidity_ : most acids involved with wine or fixed or nonvolatile (do not evaporate readily).\n2. _volatile acidity_ : the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.\n3. _citric acid_ : found in small quantities, citric acid can add 'freshness' and flavor to wines.\n4. _residual suga_ : the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet.\n5. _chlorides_ : the amount of salt in the wine.\n6. _free sulfur dioxide_ : the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine.\n7. _total sulfur dioxide_ : amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine.\n8. _density_ : the density of water is close to that of water depending on the percent alcohol and sugar content.\n9. _pH_ : describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale.\n10. _sulphates_ : a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant.\n11. _alcohol_ : the percent alcohol content of the wine.\n\n**Output variable (based on sensory data)**:\n12. _quality_ : score between 0 and 10 given by human wine tasters.\n<br><br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's visually check the first lines of our wine collection\ndf_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# types of data\ndf_raw.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Synthetic descriptive statistics\ndf_raw.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Reminder: the last attribute, ie 'quality', is our target/independant variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make for security a copy of the original dataframe before further processing \nwines = df_raw.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name=\"eda\"></a>\n# Quick EDA and Visualization"},{"metadata":{},"cell_type":"markdown","source":"Exploration data analysis and visualization are the **first tools** for interpreting and getting latent insights from data. They help us in identifying key features and meaningful representations, even with large datasets with for exemple dimensionality reduction techniques (PCA, t-SNE, etc). \n\nSo let's now make a quick exploration data analysis to see how the data are distributed and correlated. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting our target variable \n# and creating a usefull feature list of dependant variables\ntarget = 'quality'\nfeatures_list = list(wines.columns)\nfeatures_list.remove(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis (features and target 'quality')"},{"metadata":{},"cell_type":"markdown","source":"### Features distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"wines[features_list].hist(bins=40, edgecolor='b', linewidth=1.0,\n                          xlabelsize=8, ylabelsize=8, grid=False, \n                          figsize=(16,6), color='red')    \nplt.tight_layout(rect=(0, 0, 1.2, 1.2))   \nplt.suptitle('Red Wine Univariate Plots', x=0.65, y=1.25, fontsize=14);  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can notice that some features, like 'sulfur dioxide' or 'sulphates' for exemple, have a right skewed distribution and should, in a real context, probably be engineered into their log forms for better results."},{"metadata":{},"cell_type":"markdown","source":"### Target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"wines[target].hist(bins=40, edgecolor='b', linewidth=1.0,\n              xlabelsize=8, ylabelsize=8, grid=False, figsize=(6,2), color='red')    \nplt.tight_layout(rect=(0, 0, 1.2, 1.2))   \nplt.suptitle('Red Wine Quality Plot', x=0.65, y=1.25, fontsize=14);  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for visualizing correlations\nf, ax = plt.subplots(figsize=(10, 6))\ncorr = wines.corr()\nhm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=\"Reds\",fmt='.2f',\n            linewidths=.05)\nf.subplots_adjust(top=0.93)\nt= f.suptitle('Wine Attributes Correlation Heatmap', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the alcohol level has the strongest postive correlation (0.48) with the quality notation.\n\nThere is also an understandable negative correlation between 'pH' and the 'fixed acidity' of the wine: \n- the pH is the mesure of acidity/basicity with a scale between 0 (very acid) and 14 (very basic) with a midscale at 7 (neutral)\n- the definition of pH is the negative log of H+ ion (hydrogen ion)\n- so the more acidic is a solution, the more the concentration of H+ is hight, the more pH tends toward 0.\n- as indicated in the data description, most wines are acidic and have a pH of 3-4"},{"metadata":{},"cell_type":"markdown","source":"## Bivariate Features vs Target "},{"metadata":{},"cell_type":"markdown","source":"### exemple : 'alcohol' vs 'quality' plot : "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nsns.relplot(data=wines, x='alcohol', y=target, kind='line', height=5, aspect=2, color='red');    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can visualy confirm here the global positve correlation seen precedently between the 'alcohol' level (< 14) and the final quality score. Of course, feel free to try other features."},{"metadata":{},"cell_type":"markdown","source":"# Building Train and Test Datasets\n\nNow for sake of simplicity let's transform our target data to a binary classification problem ('Low' vs 'Hight' quality wine) and build our train and test datasets on a classical 70/30 ratio."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create our separate target vector \ny = wines.pop('quality')\n\n# mapping the target to a binary class at quality = 5\ny = y.apply(lambda x: 0 if x <= 5 else 1)\n\n# quickly check that we have a balanced target partition\ny.sum() / len(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# building train/test datasets on a 70/30 ratio\nX_train, X_test, y_train, y_test = train_test_split(wines, y, test_size=0.3, random_state=33)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name=\"model\"></a>\n# Training our classification model"},{"metadata":{},"cell_type":"markdown","source":"We will now instanciate and train an out-of-the-box **XGBoost classification model** on our train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# ML in two lines ;)\nxgb = XGBClassifier(objective='binary:logistic', random_state=33, n_jobs=-1)\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Performance Evaluation"},{"metadata":{},"cell_type":"markdown","source":"Now let's test our model and evaluate how it has performed with its predictions on the test data. We won't perform here any cross-validation on our model, as one should probably do in a real context."},{"metadata":{},"cell_type":"markdown","source":"# Making predictions on the test data and performance evaluation"},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions for test data\nxgb_predictions = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of performance"},{"metadata":{},"cell_type":"markdown","source":"### Accuracy, Precision/Recall, F1 Metrics & Confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We design a simple classification evaluative function\ndef evaluation_scores(test, prediction, target_names=None):\n    print('Accuracy:', np.round(metrics.accuracy_score(test, prediction), 4)) \n    print('-'*60)\n    print('classification report:\\n\\n', metrics.classification_report(y_true=test, y_pred=prediction, target_names=target_names)) \n    \n    classes = [0, 1]\n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=test, y_pred=prediction, labels=classes)\n    cm_frame = pd.DataFrame(data=cm, columns=pd.MultiIndex(levels=[['Predicted:'], classes], labels=level_labels), \n                            index=pd.MultiIndex(levels=[['Actual:'], classes], labels=level_labels))\n    \n    print('-'*60)\n    print('Confusion matrix:\\n')\n    print(cm_frame) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate predictions\nevaluation_scores(y_test, xgb_predictions, target_names=['Low Quality', 'Hight Quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AUC / ROC Curve"},{"metadata":{},"cell_type":"markdown","source":"AUC / ROC Curve is probably a better evaluations for this classification task :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the FPR and TPR for all thresholds of the classification\nprobs = xgb.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'red', label = 'ROC AUC score = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> So : F1 = 0.75 and AUC = 0.83 : well, not too bad without any tuning or feature engineering."},{"metadata":{},"cell_type":"markdown","source":"<br><br>\n>>> Now let's dive into the **interpretation of our model**:"},{"metadata":{},"cell_type":"markdown","source":"# The trade-off Accuracy / Interpretability"},{"metadata":{},"cell_type":"markdown","source":"In general machine learning design there is well known trade-offs between the bias and the variance of a model, or between precision and recall in classification algorithms for exemple.\n\nSimilarly, in ML Interpretability, there is the **Accuracy / Interpretability trade-off** rule of thumb which states that generally, the more accurate is a model (or a model of models, like ensemble's bagging and boosting), the more complex it is and so the more difficult it is to interpret it's outputs.\n\n<img src=\"https://raw.githubusercontent.com/cog-data/ML_Interpretability_tutorial/master/img/accuracy_interpretability.png\" title=\"The Accuracy vs Interpretability trade-off\" />\n\nMore, accuracy cannot stand alone for buiding trust on our models because of possible overfitting of the model, correlations of the features and noise in the data.\n\nSo we need more sophisticated metrics and analytics than just plain accuracy of the results."},{"metadata":{},"cell_type":"markdown","source":"# Default Model Interpretation Methods\n\n__Non-parametrics models__ like tree-based models as XGBoost are more difficult to interprete because their total number of parameters is not fixed and will grow with the volume of data used for the training. Parametrics models (ex: logistic regression) offers a contrario a first level of interpretation by the way of their coefficients (but in fact even this is not completely trivial). In that case, regularization techniques like L1-regularization/LASSO can be usefull to reduce the feature space and improving the interpretability of the model. For non-parametric models, fortunately, many of them like XGBoost give an access to interpretation methods like feature importance for helping us to understand the inner evaluation of the model for making his predictions."},{"metadata":{},"cell_type":"markdown","source":"<a name=\"features_importance\"></a>\n## Features importance"},{"metadata":{},"cell_type":"markdown","source":"To interpret a model, we basically need to know :\n\n- which features are the most important in the model\n- the effect of each feature on a particular prediction\n- the effects of each feature over a large number of predictions\n    \nNative global feature importance calculations that come with XGBoostare are based on the following parameters :\n\n- Feature Weights: based on the number of times a feature appears in a tree across the ensemble of trees\n- Coverage: the average coverage (number of samples affected) of splits which use the feature\n- Gain: the average gain of splits which use the feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ploting XGBoost default feature importances\nfig = plt.figure(figsize = (18, 10))\ntitle = fig.suptitle(\"Native Feature Importances from XGBoost\", fontsize=14)\n\nax1 = fig.add_subplot(2, 2, 1)\nplot_importance(xgb, importance_type='weight', ax=ax1, color='red')\nax1.set_title(\"Feature Importance with Feature Weight\");\n\nax2 = fig.add_subplot(2, 2, 2)\nplot_importance(xgb, importance_type='cover', ax=ax2, color='red')\nax2.set_title(\"Feature Importance with Sample Coverage\");\n\nax3 = fig.add_subplot(2, 2, 3)\nplot_importance(xgb, importance_type='gain', ax=ax3, color='red')\nax3.set_title(\"Feature Importance with Split Mean Gain\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As you can see, features are ordered in differents ways by these different criterions. We need a more **consistent** way of doing so. As we will see later, Shapley values for exemple offer us such a consistent way."},{"metadata":{},"cell_type":"markdown","source":"#  ELI5 Model Interpretation\n\nAccording to their documentation, __[ELI5](https://github.com/TeamHG-Memex/eli5)__ (\"Explain Like I'm 5\") is a Python library which helps to debug machine learning classifiers and regressors and explain their predictions in an easy to understand an intuitive way. ELI5 is a good starting point and support tree-based and parametric/linear models and also text processing and HashingVectorizer utilities from scikit-learn but **doesn't support true model-agnostic interpretations**."},{"metadata":{},"cell_type":"markdown","source":"## Installation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install eli5\nimport eli5\nfrom eli5.sklearn import PermutationImportance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importances\n\nWith XGBoost, ELI5 just use the same native feature importances computation methods (with the default \"gain\" parameter) which we have just seen earlier and  give us an easy and ergonomic way of displaying it with the `eli5.show_weights` method."},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(xgb.get_booster())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explaining Model Prediction Decisions with ELI5"},{"metadata":{},"cell_type":"markdown","source":"To make random forest predictions more interpretable, every prediction of the model can be presented as a sum of feature contributions (plus the bias), showing how the features lead to a particular prediction. ELI5 does it by showing weights for each feature depicting how influential it might have been in contributing to the final prediction decision across all trees. This is a good step in direction of model-agnostic interpretation but not entirely model-agnostic like, we will see it later, for LIME.\n\nLet's examine individual data-point predictions, one for each class (\"0\" : quality wine scored <= 5 with a label of \"Low Quality\" wine, and \"1\" with score > 5 and a label of \"Hight Quality\") with the `eli5.show_prediction` method."},{"metadata":{},"cell_type":"markdown","source":"### Predicting when a particular wine quality will be <= 5 ('Low Quality')"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_nb = 0\nprint('Reference:', y_test.iloc[wine_nb])\nprint('Predicted:', xgb_predictions[wine_nb])\neli5.show_prediction(xgb.get_booster(), X_test.iloc[wine_nb], \n                     feature_names=list(wines.columns), show_feature_values=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> In this successfully individual prediction, the top 3 influential features seems to be, after the bias, the pH, total sulfur dioxide, and chlorides.\n\n> NB: you can understand \"BIAS\" here as the expected average score output by the model, based on the distribution of the training set. If you want more explanation about his meaning in Eli5 and in this context (XGBoost model), you can check out this [Stackoverflow thread](https://stackoverflow.com/questions/49402701/eli5-explaining-prediction-xgboost-model)."},{"metadata":{},"cell_type":"markdown","source":"### Predicting when a particular wine quality will be > 5 ('Hight Quality')"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_nb = 4\nprint('Reference:', y_test.iloc[wine_nb])\nprint('Predicted:', xgb_predictions[wine_nb])\neli5.show_prediction(xgb.get_booster(), X_test.iloc[wine_nb], \n                     feature_names=list(wines.columns), show_feature_values=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> And in this one, sulfate, total sulfur dioxide, and volatile acidity seems to be prevalent."},{"metadata":{},"cell_type":"markdown","source":"> Notice how a feature like 'sulphates' play an opposite influential role in explaining model prediction between the two classes and acts like a flag in these two particular examples ."},{"metadata":{},"cell_type":"markdown","source":"### Features Permutation Importances"},{"metadata":{},"cell_type":"markdown","source":"Eli5 provides a way to compute feature importances for any black-box estimator by measuring how score decreases when a feature is not available; the method is also known as “permutation importance” or “Mean Decrease Accuracy (MDA). This is **more reliable** but this technique is **computationally slow** with a big number of features."},{"metadata":{},"cell_type":"markdown","source":"> __Tip:__ As in our case, If you don’t have a separate held-out dataset, you can fit `PermutationImportance` on the same data as used for training; this still allows to inspect the model, but doesn’t show which features are important for **generalization**. So you'd better use your _validation dataset_, if you have one, for computing your features permutation importances."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# we need to retrain a new model with arrays\n# as eli5 has a bug with Dataframes and XGBoost\n# cf. https://github.com/TeamHG-Memex/eli5/pull/261\nxgb_array = XGBClassifier(objective='binary:logistic', random_state=33, n_jobs=-1)\nxgb_array.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_permut = PermutationImportance(xgb_array, random_state=33).fit(X_train, y_train)\neli5.show_weights(feat_permut, feature_names = features_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> You can compare this result with the basic feature classification (ie without permutation). What are the differences ? What can you infer about the reliability of these features ?"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"pd_plots\"></a>\n# Partial Dependence Plots (PD plot)"},{"metadata":{},"cell_type":"markdown","source":"\"The partial dependence plot (PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model (J. H. Friedman 200127). A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonous or more complex.\" (from PDPbox documentation).\n\nSimply put, while feature importance shows **WHAT** variables most affect predictions, partial dependence plots show **HOW** a feature affects predictions. "},{"metadata":{},"cell_type":"markdown","source":"Here we will be using the dedicated __[PDPbox library](https://christophm.github.io/interpretable-ml-book/pdp.html)__. "},{"metadata":{},"cell_type":"markdown","source":"## Installation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pip install pdpbox\nfrom pdpbox import pdp, get_dataset, info_plots","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate PD Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pdp(model, df, feature, cluster_flag=False, nb_clusters=None, lines_flag=False):\n    \n    # Create the data that we will plot\n    pdp_goals = pdp.pdp_isolate(model=model, dataset=df, model_features=df.columns.tolist(), feature=feature)\n\n    # plot it\n    pdp.pdp_plot(pdp_goals, feature, cluster=cluster_flag, n_cluster_centers=nb_clusters, plot_lines=lines_flag)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how the feature 'alcohol' behave (with confidence interval):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the PD univariate plot\nplot_pdp(xgb, X_train, 'alcohol')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This PD plot show us that the alcohol level seems to have an increasing positive influence on the prediction of \"Hight Quality\" wines for values between 9.7 and 11.8. After that treshold the influence is still positive but decreasing progressively, and before that it was neutral/very slightly negative."},{"metadata":{},"cell_type":"markdown","source":"> explore by yourself important features like 'sulphates' and 'total sulfur dioxide'. What can your a priori infer from their PD Plot ?"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"ice_plots\"></a>\n## Univariate ICE plot"},{"metadata":{},"cell_type":"markdown","source":"ICE plots are similar to PD plots but offer a more detailled view about the behavior of near similar clusters around the PD plot average curve. ICE algorithm gives the user insight into the several variants of conditional relationships estimated by the black box."},{"metadata":{"trusted":true},"cell_type":"code","source":"# for ICE plot we must specify the numbers of similarity clusters we want\n# here 24\nplot_pdp(xgb, X_train, 'alcohol', cluster_flag=True, nb_clusters=24, lines_flag=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bivariate PD plot"},{"metadata":{},"cell_type":"markdown","source":"Let's now explore a bivariate PD plot between 'pH' and the 'fixed acidity' feature :"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_plot = ['pH', 'fixed acidity']\ninter1  =  pdp.pdp_interact(model=xgb, dataset=X_train, model_features=features_list, features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='grid')\n\n# we use plot_type='grid' as the default and better option 'contour' has a bug which is being corrected\n# cf. https://github.com/SauceCat/PDPbox/issues/40\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Going further: a [tutorial dedicated to PDPbox and ICE plots](https://towardsdatascience.com/introducing-pdpbox-2aa820afd312)."},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"skater\"></a>\n#  SKATER Model Interpretation"},{"metadata":{},"cell_type":"markdown","source":"SKATER is a relative new MLI framework and documentation is not easily accessible for now. This part is heavily inspired by [this article](https://www.oreilly.com/ideas/interpreting-predictive-models-with-skater-unboxing-model-opacity) and by an [extensive and really great tutorial on MLI made by Dipanjan Sarkar](https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739) which will also be referenced in the last \"Going further about MLI\" section at the end of this tutorial."},{"metadata":{},"cell_type":"markdown","source":"\"Skater is a unified framework to enable Model Interpretation for all forms of models to help one build an Interpretable machine learning system often needed for real world use-cases using a model-agnostic approach. It is an open source python library designed to demystify the learned structures of a black box model both globally (inference on the basis of a complete data set) and locally (inference about an individual prediction).\n\n<img src=\"https://raw.githubusercontent.com/cog-data/ML_Interpretability_tutorial/master/img/local_glocal_interpretation.png\" alt=\"accuracy-interpretability-trade-off\" title=\"The Accuracy vs Interpretability trade-off\" />\n\nSkater originally started off as a fork of LIME but then broke out as an independent framework of it's own with a wide variety of feature and capabilities for model-agnostic interpretation for any black-box models. The project was started as a research idea to find ways to enable better interpretability (preferably human interpretability) to predictive \"black boxes\" both for researchers and practioners.\" \n\nCheck out the __[the GitHub repository of SKATER](https://github.com/oracle/Skater)__ for more informations."},{"metadata":{},"cell_type":"markdown","source":"## Global / Local Interpretations\n\nPredictive models maps an input space to an output space. There is two types of interpretation algorithms :\n- **Global interpretation algorithms** who offers statistics and metrics on the joint distribution of the entire training set, which generally must reduce by  aggregating or subseting the feature space to be \"human-interpretable\".\n- **Local interpretation algorithms** which only deal with regions of the domain, such as the marginal distribution of a feature. "},{"metadata":{},"cell_type":"markdown","source":"## Creating an interpretation object"},{"metadata":{},"cell_type":"markdown","source":"The general workflow within the skater package is to create an interpretation, create a model, and run interpretation algorithms. Typically, an `Interpretation` consumes a dataset, and optionally some metadata like feature names and row ids. Internally, the `Interpretation` will generate a `DataManager` to handle data requests and sampling.\n\n- Local Models: to create a skater model based on a local function or method, pass in the predict function to an `InMemoryModel`. A user can optionally pass data samples to the examples keyword argument. This is only used to infer output types and formats. Out of the box, skater allows models return numpy arrays and pandas dataframes.\n\n- Operationalized Model: If your model is accessible through an API, use a `DeployedModel`, which wraps the requests library. `DeployedModels` require two functions, an input formatter and an output formatter, which speak to the requests library for posting and parsing. The input formatter takes a pandas DataFrame or a numpy ndarray, and returns an object (such as a dict) that can be converted to JSON to be posted. The output formatter takes a requests.response as an input and returns a numpy ndarray or pandas DataFrame."},{"metadata":{},"cell_type":"markdown","source":"## Installation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# installation of skater can be tricky, try :\n#!pip install skater\n!conda install --yes -c conda-forge Skater\n\n# check out skater installation instructions at \n# https://oracle.github.io/Skater/install.html\n\n# for testing the installation\n# ! python -c \"from skater.tests.all_tests import run_tests; run_tests()\"","execution_count":12,"outputs":[{"output_type":"stream","text":"Collecting package metadata: done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - skater\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    certifi-2019.3.9           |           py36_0         149 KB  conda-forge\n    conda-4.6.14               |           py36_0         2.1 MB  conda-forge\n    dill-0.2.9                 |           py36_0         113 KB  conda-forge\n    ds-lime-0.1.1.27           |             py_1         214 KB  conda-forge\n    multiprocess-0.70.7        |   py36h14c3975_0         171 KB  conda-forge\n    pathos-0.2.3               |             py_0          47 KB  conda-forge\n    pox-0.2.5                  |             py_0          23 KB  conda-forge\n    ppft-1.6.4.9               |           py36_0          58 KB  conda-forge\n    skater-1.0.2               |           py36_0          67 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         2.9 MB\n\nThe following NEW packages will be INSTALLED:\n\n  dill               conda-forge/linux-64::dill-0.2.9-py36_0\n  ds-lime            conda-forge/noarch::ds-lime-0.1.1.27-py_1\n  multiprocess       conda-forge/linux-64::multiprocess-0.70.7-py36h14c3975_0\n  pathos             conda-forge/noarch::pathos-0.2.3-py_0\n  pox                conda-forge/noarch::pox-0.2.5-py_0\n  ppft               conda-forge/linux-64::ppft-1.6.4.9-py36_0\n  skater             conda-forge/linux-64::skater-1.0.2-py36_0\n\nThe following packages will be UPDATED:\n\n  ca-certificates    pkgs/main::ca-certificates-2019.1.23-0 --> conda-forge::ca-certificates-2019.3.9-hecc5488_0\n\nThe following packages will be SUPERSEDED by a higher-priority channel:\n\n  certifi                                         pkgs/main --> conda-forge\n  conda                                           pkgs/main --> conda-forge\n  openssl              pkgs/main::openssl-1.0.2r-h7b6447c_0 --> conda-forge::openssl-1.0.2r-h14c3975_0\n\n\n\nDownloading and Extracting Packages\nconda-4.6.14         | 2.1 MB    | ##################################### | 100% \ndill-0.2.9           | 113 KB    | ##################################### | 100% \ncertifi-2019.3.9     | 149 KB    | ##################################### | 100% \nmultiprocess-0.70.7  | 171 KB    | ##################################### | 100% \nppft-1.6.4.9         | 58 KB     | ##################################### | 100% \nds-lime-0.1.1.27     | 214 KB    | ##################################### | 100% \nskater-1.0.2         | 67 KB     | ##################################### | 100% \npox-0.2.5            | 23 KB     | ##################################### | 100% \npathos-0.2.3         | 47 KB     | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import skater\nfrom skater.core.explanations import Interpretation\nfrom skater.model import InMemoryModel","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Workflow : Interpretation object > in-memory Model > Interpretation"},{"metadata":{"trusted":true},"cell_type":"code","source":"interpreter = Interpretation(training_data=X_test, training_labels=y_test, feature_names=features_list)\nim_model = InMemoryModel(xgb.predict_proba, examples=X_train, target_names=['Low Quality', 'Hight Quality'])","execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'X_test' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-ab4469cad309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInterpretation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mim_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInMemoryModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Low Quality'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hight Quality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importances with Skater\n\nFeature importance is generic term for the degree to which a predictive model relies on a particular feature. The skater feature importance implementation is based on an information theoretic criteria, measuring the entropy in the change of predictions, given a perturbation of a given feature. The intuition is that the more a model's decision criteria depend on a feature, the more we'll see predictions change as a function of perturbing a feature. The default method used is `prediction-variance` which is the mean absolute value of changes in predictions, given perturbations in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plots = interpreter.feature_importance.plot_feature_importance(im_model, ascending=True, progressbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Partial Dependence plots with SKATER\n\nPD plots can also be generated with the SKATER library : "},{"metadata":{},"cell_type":"markdown","source":"### PD plot of 'pH' affecting model prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"r = interpreter.partial_dependence.plot_partial_dependence(['pH'], im_model, grid_resolution=50, \n                                                           grid_range=(0,1), n_samples=1000, \n                                                           with_variance=True, figsize = (6, 4), n_jobs=-1)\nyl = r[0][1].set_ylim(0, 1) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There is a clear gap for pH = 3.5 +/- 0.1 relative to the effect of this feature on the prediction of the probability of the wine being of 'Hight Quality'"},{"metadata":{},"cell_type":"markdown","source":"### Bivariate PD Plot showing interactions between features 'pH' and 'fixed acidity' and their effect on the 'quality' classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# beware : this process is computationally slow/heavy\n\nplots_list = interpreter.partial_dependence.plot_partial_dependence([('pH', 'fixed acidity')], \n                                                                    im_model, grid_range=(0,1), \n                                                                    n_samples=1000,\n                                                                    figsize=(16, 6),\n                                                                    grid_resolution=100,\n                                                                    progressbar=True,\n                                                                    n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here, if we look carefully, we can see again the same effect but this time with an additional inhibitory/negative effect (the \"canyon\" in the middle of the feature sub-space) of the fixed acidy for values around 0.012 (NB: the effect is much more visible with an unique definition class, ie `target_names`, in the preceding InMemoryModel call)"},{"metadata":{},"cell_type":"markdown","source":"# Local Interpretations with Skater"},{"metadata":{},"cell_type":"markdown","source":"Local Interpretation could possibly be achieved in two ways: \n- Firstly, one could possibly approximate the behavior of a complex predictive model in the vicinity of a single input using a simple interpretable auxiliary or surrogate model (e.g. Linear Regressor)\n- Secondly, one could use the base estimator to understand the behavior of a single prediction using intuitive approximate functions based on inputs and outputs."},{"metadata":{},"cell_type":"markdown","source":"## Local Interpretable Model-Agnostic Explanations (LIME)"},{"metadata":{},"cell_type":"markdown","source":"LIME is an algorithm designed by Riberio Marco, Singh Sameer, Guestrin Carlos to access the behavior of the any base estimator (model) using interpretable surrogate models (e.g. linear classifier/regressor). \n\nSuch form of comprehensive evaluation helps in generating explanations which are **locally faithful but may not align with the global behavior**. \n\nBasically, LIME explanations are based on local surrogate models. Surrogate models are interpretable models (like a linear model or decision tree) that are learned on the predictions of the original black box model. But instead of trying to fit a global surrogate model, LIME focuses on fitting local surrogate models to explain why single predictions were made.\n\nFollowing is a standard high-level workflow for this:\n\n - Choose your instance of interest for which you want to have an explanation of the predictions of your black box model.\n - Perturb your dataset and get the black box predictions for these new points.\n - Weight the new samples by their proximity to the instance of interest.\n - Fit a weighted, interpretable (surrogate) model on the dataset with the variations.\n - Explain prediction by interpreting the local model.\n \nWe recommend you to read the [LIME chapter](https://christophm.github.io/interpretable-ml-book/lime.html) in Christoph Molnar's excellent book on Model Interpretation which talks about this in detail."},{"metadata":{},"cell_type":"markdown","source":"<a name=\"lime\"></a>\n## Explaining Model Predictions with Skater using LIME\n\nSkater can leverage LIME to explain model predictions. Typically, its `LimeTabularExplainer` class helps in explaining predictions on tabular (i.e. matrix) data. For numerical features, it perturbs them by sampling from a Normal(0,1) and doing the inverse operation of mean-centering and scaling, according to the means and stds in the training data. For categorical features, it perturbs by sampling according to the training distribution, and making a binary feature that is 1 when the value is the same as the instance being explained. The `explain_instance()` function generates explanations for a prediction. First, we generate neighborhood data by randomly perturbing features from the instance. We then learn locally weighted linear (surrogate) models on this neighborhood data to explain each of the classes in an interpretable way."},{"metadata":{},"cell_type":"markdown","source":"Since XGBoost has some issues with feature name ordering when building models with dataframes, so we will use our yet fitted xgb_array model to make LIME work without additional hassles of feature re-ordering. "},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = xgb_array.predict_proba(X_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n\nexp = LimeTabularExplainer(X_test.values, feature_names=features_list, discretize_continuous=True, class_names=['Low Quality', 'High Quality'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting when a particular wine quality will be <= 5 ('Low Quality')"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_nb = 0\nprint('Reference:', y_test.iloc[wine_nb])\nprint('Predicted:', predictions[wine_nb])\nexp.explain_instance(X_test.iloc[wine_nb].values, xgb_array.predict_proba).show_in_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that this model has taken this particular decision (82% chance that this wine will be scored as 'Low Quality') by puting forward the 'alcohol', 'sulphates' and 'volatile acidity' features, and we could know to which degree by looking at the associated values."},{"metadata":{},"cell_type":"markdown","source":"### Predicting when a particular wine quality will be > 5 ('Hight Quality')"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine_nb = 4\nprint('Reference:', y_test.iloc[wine_nb])\nprint('Predicted:', predictions[wine_nb])\nexp.explain_instance(X_test.iloc[wine_nb].values, xgb_array.predict_proba).show_in_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can again see that SKATER LIME gives a nice display showing which features were the most influential in the model taking the correct decision of predicting the wine quality score, here for a 'Hight Quality' wine."},{"metadata":{},"cell_type":"markdown","source":"<a name=\"skater_tree\"></a>\n## Tree Surrogates with SKATER"},{"metadata":{},"cell_type":"markdown","source":"We have see various ways to interpret machine learning models with features, dependence plots and even LIME. But can we build an approximation or a surrogate model which is more interpretable from a really complex black box model like our XGBoost model having hundreds of decision trees?\n\nHere in, we introduce the idea of using __`TreeSurrogates`__ as means for explaining a model's learned decision policies (for inductive learning tasks), which is inspired by the work of Mark W. Craven described as the TREPAN algorithm. \n\nWe recommend checking out the following excellent papers on the TREPAN algorithm to build surrogate trees.\n - [_Mark W. Craven(1996) EXTRACTING COMPREHENSIBLE MODELS FROM TRAINED NEURAL NETWORKS_](http://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.thesis.pdf)\n - [_Mark W. Craven and Jude W. Shavlik(NIPS, 96). Extracting Thee-Structured Representations of Thained Networks_](https://papers.nips.cc/paper/1152-extracting-tree-structured-representations-of-trained-networks.pdf)\n\nBriefly, Trepan constructs a decision tree in a best-first manner. It maintains a queue of leaves which are expanded into subtrees as they are removed from the queue. With each node in the queue, Trepan stores,\n\n - a subset of the training examples, \n - another set of instances (query instances),\n - a set of constraints. \n\nThe stored subset of training examples consists simply of those examples that reach the node. The query instances are used, along with the training examples, to select the splitting test if the node is an internal node or to determine the class\nlabel if it is a leaf. The constraint set describes the conditions that instances must satisfy in order to reach the node; this information is used when drawing a set of query instances for a newly created node. The process of expanding a node in Trepan is much like it is in conventional decision tree algorithms: a splitting test is selected for the node, and a child is created for each outcome of the test. Each child is either made a leaf of the tree or put into the queue for future expansion.\n\n\nFor Skater's implementation, for building explainable surrogate models, the base estimator (\"Oracle\") could be any form of a supervised learning predictive model - our black box model. The explanations are approximated using Decision Trees (both for Classification/Regression) by learning decision boundaries similar to that learned by the Oracle (predictions from the base model are used for learning the Decision Tree representation). The implementation also generates a fidelity score to quantify tree based surrogate model’s approximation to the Oracle. Ideally, the score should be 0 for truthful explanation both globally and locally. Let's check this out in action!\n\n__NOTE:__ The implementation is currently experimental and might change in future."},{"metadata":{},"cell_type":"markdown","source":"### Using the interpreter instance invoke call to the TreeSurrogate"},{"metadata":{"trusted":true},"cell_type":"code","source":"surrogate_explainer = interpreter.tree_surrogate(oracle=im_model, seed=33)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using the surrogate model to learn the decision boundaries learned by the base estimator\n - Reports the fidelity value when compared to the base estimator (closer to 0 is better)\n - Learner uses F1 score as the default metric of choice for classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = surrogate_explainer.fit(X_train, y_train, use_oracle=True, prune='pre', scorer_type='f1')\nprint('F1 score for the surrogate tree: ', f1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the Surrogate Tree"},{"metadata":{},"cell_type":"markdown","source":"Let's visualize our surrogate tree with the 'Low Quality' (score <= 5) class in pink and 'High Quality' class (score > 5) in red, but before let's display a reminder of the features names :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A reminder for referencing the originals feature names \n# since these names are not kept in the surrogate tree\npd.DataFrame([('X'+str(idx), feature) for (idx, feature) in enumerate(wines.columns)]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skater.util.dataops import show_in_notebook\n\n# 'Low Quality' (score <= 5) class in pink and 'High Quality' class (score > 5) in red\nsurrogate_explainer.plot_global_decisions(colors=['pink', 'red'], file_name='test_tree_sur.png', fig_size=(8,8))\n\nshow_in_notebook('test_tree_sur.png', width=1200, height=800);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Interesting rules from the surrogate tree\n\nHere are exemples of some interesting rules you can observe from the above tree:\n- If `alcohol` > 10.017 and `sulphates` > 0.575 and `total sulfur dioxide` <= 90.5 __→__ 93.2% chance that the wine will be classified as 'Hight Quality' one.\n- If `alcohol` > 11.45 and `sulphates` <= 0.575 __→__ 92.7% chance that the wine will also be classified as 'Hight Quality' one.\n\nOf course, feel free to derive more interesting rules from this and also from your own models !"},{"metadata":{},"cell_type":"markdown","source":"Let's look at how our surrogate model performs on the test dataset now..."},{"metadata":{},"cell_type":"markdown","source":"### Surrogate Model Performance Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using our evaluation_scores function \nsurrogate_predictions = surrogate_explainer.predict(X_test)\nevaluation_scores(y_test, surrogate_predictions, target_names=['low quality', 'hight quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the ROC AUC score for the tree surrogated model\nroc_auc = metrics.roc_auc_score(y_test, surrogate_predictions)\nprint('ROC AUC score: ', round(roc_auc, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> As expected, the model performance drops but still we get an overall F1 score of 71% as compared to our preceding boosted model's score of 75%. On another end the AUC shows a more significant drop from 0.83 with the initial XGB model to now a weak 0.71 which cast a legitimate doubt on the validity of the preceding rules we extracted from the tree."},{"metadata":{},"cell_type":"markdown","source":"<a name=\"shap\"></a>\n# Model Interpretation with SHAP"},{"metadata":{},"cell_type":"markdown","source":"**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on what they claim! (do check out the [SHAP NIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) for details)."},{"metadata":{},"cell_type":"markdown","source":"SHAP values can explain the output of any machine learning model but for complex ensemble models it can be slow. SHAP has c++ implementations supporting  *XGBoost*, *LightGBM*, *CatBoost*, and *scikit-learn* tree models.\n\nSHAP (SHapley Additive exPlanations) assigns each feature an importance value for a particular prediction. Its novel components include: the identification of a new class of additive feature importance measures, and theoretical results showing there is a unique solution in this class with a set of desirable properties. Typically, SHAP values try to explain the output of a model (function) as a sum of the effects of each feature being introduced into a conditional expectation. Importantly, for non-linear functions the order in which features are introduced matters. The SHAP values result from averaging over all possible orderings. Proofs from game theory show this is the only possible consistent approach. \n\nAn intuitive way to understand the Shapley value is the following: The feature values enter a room in random order. All feature values in the room participate in the game (= contribute to the prediction). The Shapley value __$ϕ_{ij}$__ is the average marginal contribution of feature value __$x_{ij}$__ by joining whatever features already entered the room before, i.e.\n\n$$\\phi_{ij}=\\sum_{\\text{All.orderings}}val(\\{\\text{features.before.j}\\}\\cup{}x_{ij})-val(\\{\\text{features.before.j}\\})$$\n\nThe following figure from the KDD 18 paper, [_Consistent Individualized Feature Attribution for Tree Ensembles_](https://arxiv.org/pdf/1802.03888.pdf) summarizes this in a nice way!\n\n![](https://i.imgur.com/6Rc1dsa.png)"},{"metadata":{},"cell_type":"markdown","source":"Let's now dive into SHAP and leverage it for interpreting our model:"},{"metadata":{},"cell_type":"markdown","source":"## Installation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ! pip install shap\n\nimport shap\n\n# load JS visualization code to notebook\nshap.initjs()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explain predictions with SHAP"},{"metadata":{},"cell_type":"markdown","source":"Shapley value is the average contribution of features which are predicting in different situation.\n\nSHAP provides multiple explainers for different kind of models.\n\n- TreeExplainer: Support XGBoost, LightGBM, CatBoost and scikit-learn models by Tree SHAP.\n- DeepExplainer (DEEP SHAP): Support TensorFlow and Keras models by using DeepLIFT and Shapley values.\n- GradientExplainer: Support TensorFlow and Keras models.\n- KernelExplainer (Kernel SHAP): Applying to any models by using LIME and Shapley values.\n\nFor more information about the Shapley values explanations check out [Christoph Molnar's book chapter on Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html) \n\nSo let's play with the Tree SHAP implementation integrated into XGBoost to explain the test dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# explain the model's predictions using SHAP values\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\nexplainer = shap.TreeExplainer(xgb)\nshap_values = explainer.shap_values(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_shap = pd.DataFrame(shap_values)\nX_shap.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get a matrix of SHAP values with the same shape as the original X_test set. Each row sums to the difference between the model output for that sample and the expected value of the model output (which is stored as `expected_value` attribute of the explainer). Typically this difference helps us in explaining why the model is inclined on predicting a specific class outcome."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Expected Value: ', explainer.expected_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importances with SHAP\n\nThis basically takes the average of the SHAP value magnitudes across the dataset and plots it as a simple bar chart."},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_test, plot_type=\"bar\", color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predicting when a particular wine quality will be <= 5 ('Low Quality')"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Reminder : features pushing the prediction higher than the base value are shown in red, those pushing the prediction lower are in blue."},{"metadata":{},"cell_type":"markdown","source":"> SHAP gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting that this wine will be scored as 'Low Quality'. The above explanation shows features each contributing to push the model output from the 'base value' (the average model output over the training dataset we passed) to the actual model output."},{"metadata":{},"cell_type":"markdown","source":"### Predicting when a particular wine quality will be > 5 ('Hight Quality')"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[4,:], X_test.iloc[4,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing and explaining multiple predictions"},{"metadata":{},"cell_type":"markdown","source":"SHAP can build beautiful interactive plots which can visualize and explain multiple predictions at once. Here we visualize model prediction decisions for the first 1000 test data samples."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[:1000,:], X_test.iloc[:1000,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> __Tip__: Notice the drop-down menus at the top of the graph with multiple display and ordering options and also on the left for isolating and highlighting multiple or individual features effects."},{"metadata":{},"cell_type":"markdown","source":"> The above visualization can be interacted with in multiple ways. The visualization shows for exemple some interesting model prediction pattern decisions. For exemple :\n>\n>- The first 90 test samples all probably are classified as 'Hight Quality' wines and that have a hight alcohol degree (> 11.3) that you can highlight by choosing in the left menu 'alcohol effects'\n>- If you display the 'alcohol' feature with the top drop-down menu, you can confirm that the alcohol level begin to play an important role in pushing upward the prediction at a level around 11.3.\n>- for 'pH', we can confirm a behavioral shift of the model after 3.5, as seen before. "},{"metadata":{},"cell_type":"markdown","source":"> What about 'sulphates' and 'total sulfur dioxide' that we saw before ? Does it confirm the first hypotheses you made with the PD plot ?"},{"metadata":{},"cell_type":"markdown","source":"I think you would agree that it's definitely interesting to see how we can find out patterns in SHAP displays which lead us to have a better understanding why the model is making specific decisions and help us being able to provide explanations for them."},{"metadata":{},"cell_type":"markdown","source":"## SHAP Summary Plot"},{"metadata":{},"cell_type":"markdown","source":"A SHAP value for a feature of a specific prediction represents how much the model prediction changes when we observe that feature.\n\nSHAP also enables us to use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the dataset. Features are sorted by the sum of the SHAP value magnitudes across all samples. "},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Notice that when the scatter points don't fit on a line they pile up to show density, and the color of each point represents the feature value of that individual."},{"metadata":{},"cell_type":"markdown","source":"> It is interesting to confirm again that hight values of 'alcohol' play positive important role in the final prediction, and hight values of 'total sulfur dioxide' seems a contrario to have a negative impact on predicting \"hight quality\" wines."},{"metadata":{},"cell_type":"markdown","source":"## SHAP Dependence Plots"},{"metadata":{},"cell_type":"markdown","source":"SHAP dependence plots show the effect of a single (or two) feature across the whole dataset. They plot a feature's value vs. the SHAP value of that feature across many samples. SHAP dependence plots are similar to partial dependence plots, but account for the interaction effects present in the features, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature can be chosen for coloring to highlight possible interactions."},{"metadata":{},"cell_type":"markdown","source":"### PD plot of 'pH' (and influence of 'fixed acidity') affecting model prediction"},{"metadata":{},"cell_type":"markdown","source":"We can use the `dependence_plot` method for ploting the effect of a feature on the prediction with or without the influence of another feature. If you don't give any `interaction_index` parameter, SHAP will decide by itself and propose automatically an interaction feature for you."},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot(ind='pH', interaction_index='fixed acidity',\n                     shap_values=shap_values, \n                     features=X_test,  \n                     display_features=X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Again, you can play also with 'total sulfur dioxide' and 'sulphates' for exemple"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"fairml\"></a>\n# Hands-on FairML (Bonus)"},{"metadata":{},"cell_type":"markdown","source":"__[FairML](https://github.com/adebayoj/fairml)__ is a new Python library that audits black-box predictive models. The basic idea behind FairML (and many other attempts to audit or interpret model behavior) is to measure a model’s dependence on its inputs by changing them. If a small change to an input feature dramatically changes the output, the model is sensitive to the feature.\n\nBut what if the input attributes are correlated?\n\nThe trick used here to counter this multicollinearity is **orthogonal projection**. FairML orthogonally projects the input to measure the dependence of the predictive model on each attribute. Orthogonal projection of vectors is important because it allows us to completely remove the linear dependence between attributes. If two vectors are orthogonal to one another, then no linear transformation of one vector can produce the other. This intuition underlies the feature dependence measure.\n\nOne advantage of FairML is that it can audit any classifier or regressor. FairML only requires that it has a `predict function`.\n\nSee [FairML: Auditing Black-Box Predictive Models](https://blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html) by the author for an exemple of use-case."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# ! pip install https://github.com/adebayoj/fairml/archive/master.zip\n\nfrom fairml import audit_model\nfrom fairml import plot_dependencies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nxgb_fair = XGBClassifier(objective='binary:logistic', random_state=33, n_jobs=-1)\n\nxgb_fair.fit(X_train.values, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we import the two key methods from fairml.\naudit_model takes:\n\n- (required) black-box function, which is the model to be audited\n- (required) sample_data to be perturbed for querying the function. This has to be a pandas dataframe with no missing data.\n\n- other optional parameters that control the mechanics of the auditing process, for example:\n  - number_of_runs : number of iterations to perform\n  - interactions : flag to enable checking model dependence on interactions.\n\naudit_model returns an overloaded dictionary where keys are the column names of input pandas dataframe and values are lists containing model  dependence on that particular feature. These lists of size number_of_runs.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# call audit model\nfeat_importances, _ = audit_model(xgb_fair.predict, X_train, distance_metric='accuracy', direct_input_pertubation_strategy='constant-zero',\n                                 number_of_runs=50, include_interactions=True)\n\n# print feature importance\nprint(feat_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate feature dependence plot\nfig = plot_dependencies(\n    feat_importances.median(),\n    reverse_values=False,\n    title=\"FairML feature dependence XGB model\",\n    fig_size=(8,3)\n    )\n\n# Print it in a file\nfile_name = \"fairml_wine_quality.png\"\nplt.savefig(file_name, transparent=False, bbox_inches='tight', dpi=250)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Cap'tain Obvious tip: red color indicates that the factor highly contributes to the model prediction (yeah, i know, \"what about the blue?\")"},{"metadata":{},"cell_type":"markdown","source":"> You can play with the different parameters like the `include_interactions` flag or with `direct_input_pertubation_strategy` referring to how to zero out a single variable with three different options:\n- 'constant-zero': replace with a random constant value\n- 'constant-median': replace with median constant value\n- 'random-sample': replace all values with a random permutation of the column"},{"metadata":{},"cell_type":"markdown","source":"> Well, is FairML itself really \"fair\" ? What do you think ?"},{"metadata":{},"cell_type":"markdown","source":"<a name=\"further_readings\"></a>\n# Last but not least : going further about MLI..."},{"metadata":{},"cell_type":"markdown","source":"Here's some curated ressources about Machine Learning Interpretability:\n\n- A __[practical and knowledgeable tutorial on Kaggle Learn](https://www.kaggle.com/learn/machine-learning-explainability)__ by Dan Becker, with interactives exercices (permutation importance, partial plots, Shap values)\n\n- A detailed and really great tutorial about MLI in 4 parts by Dipanjan (DJ) Sarkar :\n -  __[The Importance of Human Interpretable Machine Learning](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)__\n -  __[Model Interpretation Strategies](https://towardsdatascience.com/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739)__\n -  __[Hands-on Machine Learning Model Interpretation](https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608)__\n -  __Part 4 not published yet...__\n \n\n- Another nice tutorial : __[Interpretable Machine Learning with Python](http://savvastjortjoglou.com/intrepretable-machine-learning-nfl-combine.html)__ by Savvas Tjortjoglou\n\n- A great online book about MLI : __[Interpretable Machine Learning : A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)__ by Christoph Molnar\n\n- An article by qualified experts of the field (Patrick Hall & Al. from H2O.ai): __[Ideas on interpreting machine learning](https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning)__\n    \n- For the more greedy, the __[awesome-machine-learning-interpretability repository](https://github.com/jphall663/awesome-machine-learning-interpretability)__, dedicated to the subject with plenty of ressources.\n"},{"metadata":{},"cell_type":"markdown","source":"<br>\n<center>-----------------------------------------</center>\n<br>"},{"metadata":{},"cell_type":"markdown","source":"__Congratulations to those who have made it so far (and also to the others) !__\n\n_I hope that this tutorial has interested you and that it will be usefull and inspiring as an introduction to this important, complex and very interesting topic. It should be obvious now that human must stay in the loop, as machines don't have in fact any real semantic skills unless they are assigned by a human consensus, and may be AI should better stand for \"Additive Intelligence\". Anyway, the current preoccupation for this subject alone shows that ML has reached a sufficient maturity so that we, human beings, could now learn very interesting insights from the internal behavior of the very same ML/AI models we produce. And that's a nice and interesting cybernetics and cognitive feedback loop... so, who will explain the explainer?_\n\n_\"A votre santé\" ! / Cheers !_ "},{"metadata":{},"cell_type":"markdown","source":"`Christophe Rigon > datacog@free.fr`"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}