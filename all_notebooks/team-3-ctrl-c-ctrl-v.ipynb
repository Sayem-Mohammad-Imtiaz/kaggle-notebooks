{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Team 3 - Ctrl C; Ctrl V\nimport os\nimport csv\nos.chdir('/kaggle/working')\nwith open('submissions.csv', 'w') as csvfile:\n    filewriter = csv.writer(csvfile, delimiter=',', quotechar = \"|\", quoting = csv.QUOTE_MINIMAL)\n    filewriter.writerow(['id', 'xmin', 'xmax', 'ymin', 'ymax'])\n    \ncIdsA = []\ncIdsA.append('a_43')\ncIdsA.append('a_32')\ncIdsA.append('a_47')\ncIdsA.append('a_48')\ncIdsA.append('a_40')\ncIdsA.append('a_28')\ncIdsA.append('a_37')\ncIdsA.append('a_27')\ncIdsA.append('a_30')\ncIdsA.append('a_36')\ncIdsA.append('a_45')\ncIdsA.append('a_25')\ncIdsA.append('a_38')\ncIdsA.append('a_35')\ncIdsA.append('a_46')\n\ncIdsB = []\ncIdsB.append('b_25')\ncIdsB.append('b_39')\ncIdsB.append('b_42')\ncIdsB.append('b_35')\ncIdsB.append('b_36')\ncIdsB.append('b_43')\ncIdsB.append('b_34')\ncIdsB.append('b_44')\ncIdsB.append('b_26')\ncIdsB.append('b_38')\ncIdsB.append('b_29')\ncIdsB.append('b_33')\ncIdsB.append('b_31')\ncIdsB.append('b_41')\n\ncIdsC = []\ncIdsC.append('c_47')\ncIdsC.append('c_12')\ncIdsC.append('c_0')\ncIdsC.append('c_4')\ncIdsC.append('c_9')\ncIdsC.append('c_14')\ncIdsC.append('c_15')\ncIdsC.append('c_22')\ncIdsC.append('c_11')\ncIdsC.append('c_23')\ncIdsC.append('c_2')\ncIdsC.append('c_24')\ncIdsC.append('c_13')\n\ncIdsD = []\ncIdsD.append('d_20')\ncIdsD.append('d_19')\ncIdsD.append('d_6')\ncIdsD.append('d_17')\ncIdsD.append('d_10')\ncIdsD.append('d_16')\ncIdsD.append('d_18')\ncIdsD.append('d_21')\ncIdsD.append('d_7')\ncIdsD.append('d_8')\ncIdsD.append('d_3')\ncIdsD.append('d_42')\ncIdsD.append('d_1')\ncIdsD.append('d_5')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LFW Imports\nimport os\nos.chdir('/kaggle/input/alldata/Project')\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer\n\nfrom models_lfw.face_recognition.model_small import create_model\nfrom models_lfw.face_recognition.align import AlignDlib\nfrom models_lfw.triplet_loss import TripletLossLayer\nfrom coml_preprocessor import ComlFaceDataGenerator\n\n#DETR Imports\nos.chdir('detr')\n\nimport argparse\nimport random\nimport cv2\nfrom pathlib import Path\nimport torch\nimport torchvision.transforms as T\nimport PIL.Image\nfrom models import build_model\nfrom main import get_args_parser\nimport sys\nimport json\nimport shutil\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mapping = {}\nmapping2 = {}\nwith open('/kaggle/input/mapping/person_name_id_mapping.csv', newline='') as csvfile:\n     reader = csv.DictReader(csvfile)\n     for row in reader:\n         mapping[row[\"Name\"]] = int(row[\"ID\"])\n         mapping2[int(row[\"ID\"])] = row[\"Name\"]  # This stores Id in a key-value pair format similar to a java map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Configurations\nparser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nargs.resume = '/kaggle/input/alldata/Project/models_lfw/checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False\n\nprint(args) \n\n# Buuld Model\nmodel, criterion, postprocessors = build_model(args)\n\ndevice = torch.device(args.device)\nmodel.to(device)\n\n# Load Weights\noutput_dir = Path(args.output_dir)\nif args.resume:\n    # the model will download the weights and model state from the https link provided\n    if args.resume.startswith('https'):\n      checkpoint = torch.hub.load_state_dict_from_url(\n          args.resume, map_location='cpu', check_hash = True)\n    else:\n        checkpoint = torch.load(args.resume, map_location='cpu')\n\n    # this load the weights and model state into the model\n    model.load_state_dict(checkpoint['model'], strict=True)\n\n# COCO classes\nCLASSES = [\n   'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n   'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n   'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n   'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n   'toothbrush'\n]\n\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n\ndef detect(im, model, transform):\n    # mean-std normalize the input image (batch-size: 1)\n    img = transform(im).unsqueeze(0)\n\n    assert img.shape[-2] <= 1600 and img.shape[-1] <=1600, 'demo model only supports images up to 1600 pixels on each side'\n\n    outputs = model(img)\n\n    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n    keep = probas.max(-1).values > 0.7\n\n    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n    return probas[keep], bboxes_scaled\n\ndef plot_results(pil_img, prob, boxes, classes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        \n        text= f'Find This Person'\n        #f'{CLASSES[c1]}: {p[c1]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=.5))\n    plt.axis('off')\n    plt.show()\n\n# Stores coordinates of the bounding box\ndef bbox_dims(pil_img, prob, boxes, classes):\n    bbox_list = []\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        bbox_list.append([xmin, ymin, xmax, ymax])\n    return bbox_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter = \"a\"\nthe_image = PIL.Image.open('/kaggle/input/2021-spring-coml-face-recognition-competition/' + str(letter) + '.jpg')\nwidth, height = the_image.size\n\nif width > 1600 and height > 1600:\n  new_size = (1600, 1600)\n  the_image = the_image.resize(new_size)\n\nplot_classes = ['person']\nscores, boxes = detect(the_image, model, transform)\n\nbbox_dim_list = bbox_dims(the_image, scores, boxes, plot_classes)\n\n# Crop Images, this will be fed into the coml generator\ncropped_images = []\nfor bbox in bbox_dim_list:\n    cropped_images.append(the_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))) # Left, upper, right, bottom","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input for anchor, positive, and negative images\nin_a = Input(shape=(96, 96, 3), name=\"img_a\")\nin_p = Input(shape=(96, 96, 3), name=\"img_p\")\nin_n = Input(shape=(96, 96, 3), name=\"img_n\")\n\n# create the base model from model_small\nmodel_sm = create_model()\n\n# Output the embedding vectors from anchor, positive, and negative images\n# The model weights are shared (Triplet network)\nemb_a = model_sm(in_a)\nemb_p = model_sm(in_p)\nemb_n = model_sm(in_n)\n\n# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\ntriplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n\n# Model that can be trained with anchor, positive, and negative images\nmodel = Model([in_a, in_p, in_n], triplet_loss_layer)\n\n# Load weights\nmodel.load_weights(\"/kaggle/input/alldata/Project/ckpts(lfw)/epoch024_loss1.440.hdf5\")\n\nbase_model = model.layers[3]\n\nclass IdentityMetadata():\n    def __init__(self, base, name, file):\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path, upper_limit):\n    metadata = []\n    count = 0\n    for i in sorted(os.listdir(path)):\n        if count == upper_limit: \n            break\n            \n        for f in sorted(os.listdir(os.path.join(path, i))):\n            if count == upper_limit: \n                break\n                \n            count += 1\n            # Check file extension. Allow only jpg/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]\n    \ndef align_image(img):\n        alignment = AlignDlib('/kaggle/input/alldata/Project/models_lfw/landmarks.dat')\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, (96,96))\n        else:\n            return alignment.align(96, \n                                   img, \n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata = load_metadata('/kaggle/input/alldata/Project/data/fire', 100)\nalignment = AlignDlib('/kaggle/input/alldata/Project/models_lfw/landmarks.dat')\n\nembeddings = np.empty((metadata.shape[0], 128))\n\nfor i, m in enumerate(metadata):\n    img = load_image(m.image_path())\n    img = align_image(img)\n    img = img.astype('float32')\n    img = img / 255.0\n    img = np.expand_dims(img, axis=0)\n    person = m.image_path().split('/')[7].replace('_', ' ')\n    embeddings[i] = base_model.predict(img)\n    # for displaying the progress of creating embeddings\n    if i%1 == 0: print(str(i) + \" \" + person + \"\\n\", end=\" \")\n    if i == len(metadata)-1: print('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IdsUsed = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))\n\ndef getMinimumDistance(myEmbeddings, embeddings, myEmbeddingsIndex):\n  name = \"IDK\"\n  dist = .2\n  name = [metadata[0].image_path().split('/')[7].replace('_', ' ')] \n  for i in range(len(embeddings)):\n    if distance(myEmbeddings[myEmbeddingsIndex], embeddings[i]) < dist:\n      name = [metadata[i].image_path().split('/')[7].replace('_', ' ')] \n  return str(name).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n\ndef plotWithNameAndId(pil_img, prob, boxes, classes, cIds, theLetter):\n    correctCount = 0\n    count = 0\n    filler = 0\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    i = 0\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        if (getMinimumDistance(myEmbeddings, embeddings, i) != \"IDK\"):\n            id = mapping[getMinimumDistance(myEmbeddings, embeddings, i)]\n        name = str(getMinimumDistance(myEmbeddings, embeddings, i))\n        text = str(getMinimumDistance(myEmbeddings, embeddings, i))\n        neededID = str(letter) + \"_\" + str(id)\n        theId = id\n        i += 1\n        coor = (name + \", \" + \"Id: \" + neededID + \", Coordinates: \" + str(xmin) + \" \" + str(xmax) + \" \" + str(ymin) + \" \" + str(ymax))\n        print(coor)\n        \n        os.chdir('/kaggle/working')\n        with open('submissions.csv', 'a') as csvfile:\n            file = open(\"submissions.csv\")\n            reader = csv.reader(file)\n            filewriter = csv.writer(csvfile, delimiter=',', quotechar = \"|\", quoting = csv.QUOTE_MINIMAL)\n            lines = len(list(reader))\n            if neededID in cIds:\n                if not neededID in IdsUsed:\n                    filewriter.writerow([neededID, int(xmin), int(xmax), int(ymin), int(ymax)])\n                    IdsUsed.append(neededID)\n                     \n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=.5))\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myEmbeddings = np.empty((metadata.shape[0], 128))\n\nfor i,img in enumerate(cropped_images):\n  img = align_image(np.asarray(img))\n  img = img.astype('float32')\n  img =img / 255.0\n  img = np.expand_dims(img, axis=0)\n  person = str(i)\n  myEmbeddings[i] = base_model.predict(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotWithNameAndId(the_image, scores, boxes, plot_classes, cIdsA, 'a')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Configurations\nparser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nargs.resume = '/kaggle/input/alldata/Project/models_lfw/checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False\n\nprint(args) \n\n# Buuld Model\nmodel, criterion, postprocessors = build_model(args)\n\ndevice = torch.device(args.device)\nmodel.to(device)\n\n# Load Weights\noutput_dir = Path(args.output_dir)\nif args.resume:\n    # the model will download the weights and model state from the https link provided\n    if args.resume.startswith('https'):\n      checkpoint = torch.hub.load_state_dict_from_url(\n          args.resume, map_location='cpu', check_hash = True)\n    else:\n        checkpoint = torch.load(args.resume, map_location='cpu')\n\n    # this load the weights and model state into the model\n    model.load_state_dict(checkpoint['model'], strict=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter = \"b\"\nthe_image = PIL.Image.open('/kaggle/input/2021-spring-coml-face-recognition-competition/' + str(letter) + '.jpg')\nwidth, height = the_image.size\n\nif width > 1600 and height > 1600:\n  new_size = (1600, 1600)\n  the_image = the_image.resize(new_size)\n\nscores, boxes = detect(the_image, model, transform)\n\nbbox_dim_list = bbox_dims(the_image, scores, boxes, plot_classes)\n\n# Crop Images, this will be fed into the coml generator\ncropped_images = []\nfor bbox in bbox_dim_list:\n    cropped_images.append(the_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))) # Left, upper, right, bottom\n\nmyEmbeddings = np.empty((metadata.shape[0], 128))\n\nfor i,img in enumerate(cropped_images):\n  img = align_image(np.asarray(img))\n  img = img.astype('float32')\n  img =img / 255.0\n  img = np.expand_dims(img, axis=0)\n  person = str(i)\n  myEmbeddings[i] = base_model.predict(img)\nplotWithNameAndId(the_image, scores, boxes, plot_classes, cIdsB, 'b')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter = \"c\"\nthe_image = PIL.Image.open('/kaggle/input/2021-spring-coml-face-recognition-competition/' + str(letter) + '.jpg')\nwidth, height = the_image.size\n\nif width > 1600 and height > 1600:\n  new_size = (1600, 1600)\n  the_image = the_image.resize(new_size)\n\nscores, boxes = detect(the_image, model, transform)\n\nbbox_dim_list = bbox_dims(the_image, scores, boxes, plot_classes)\n\n# Crop Images, this will be fed into the coml generator\ncropped_images = []\nfor bbox in bbox_dim_list:\n    cropped_images.append(the_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))) # Left, upper, right, bottom\n\nmyEmbeddings = np.empty((metadata.shape[0], 128))\n\nfor i,img in enumerate(cropped_images):\n  img = align_image(np.asarray(img))\n  img = img.astype('float32')\n  img =img / 255.0\n  img = np.expand_dims(img, axis=0)\n  person = str(i)\n  myEmbeddings[i] = base_model.predict(img)\nplotWithNameAndId(the_image, scores, boxes, plot_classes, cIdsC, 'c')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter = \"d\"\nthe_image = PIL.Image.open('/kaggle/input/2021-spring-coml-face-recognition-competition/' + str(letter) + '.jpg')\nwidth, height = the_image.size\n\nif width > 1600 and height > 1600:\n  new_size = (1600, 1600)\n  the_image = the_image.resize(new_size)\n\nscores, boxes = detect(the_image, model, transform)\n\nbbox_dim_list = bbox_dims(the_image, scores, boxes, plot_classes)\n\n# Crop Images, this will be fed into the coml generator\ncropped_images = []\nfor bbox in bbox_dim_list:\n    cropped_images.append(the_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))) # Left, upper, right, bottom\n\nmyEmbeddings = np.empty((metadata.shape[0], 128))\n\nfor i,img in enumerate(cropped_images):\n  img = align_image(np.asarray(img))\n  img = img.astype('float32')\n  img =img / 255.0\n  img = np.expand_dims(img, axis=0)\n  person = str(i)\n  myEmbeddings[i] = base_model.predict(img)\nplotWithNameAndId(the_image, scores, boxes, plot_classes, cIdsD, 'd')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir('/kaggle/working')\nwith open('submissions.csv', 'a') as csvfile:\n    file = open(\"submissions.csv\")\n    reader = csv.reader(file)\n    filewriter = csv.writer(csvfile, delimiter=',', quotechar = \"|\", quoting = csv.QUOTE_MINIMAL)\n    for i in cIdsA:\n        if not i in IdsUsed:\n            lines = len(list(reader))\n            if lines < 57:\n                filewriter.writerow([i, int(0), int(0), int(0), int(0)])\n    for i in cIdsB:\n        if not i in IdsUsed:\n            lines = len(list(reader))\n            if lines < 57:\n                filewriter.writerow([i, int(0), int(0), int(0), int(0)])\n    for i in cIdsC:\n        if not i in IdsUsed:\n            lines = len(list(reader))\n            if lines < 57:\n                filewriter.writerow([i, int(0), int(0), int(0), int(0)])\n    for i in cIdsD:\n        if not i in IdsUsed:\n            lines = len(list(reader))\n            if lines < 57:\n                filewriter.writerow([i, int(0), int(0), int(0), int(0)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}