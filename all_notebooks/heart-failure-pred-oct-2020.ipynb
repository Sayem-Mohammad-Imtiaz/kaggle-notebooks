{"cells":[{"metadata":{},"cell_type":"markdown","source":"We're going to take a look at this small heart failure data set: <br>\nhttps://www.kaggle.com/andrewmvd/heart-failure-clinical-data\n\nautogen code to bring in a data set hosted by kaggle:\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First things first, lets read the article: <br>\n<b><a href=\"https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5\">The original article</a></b> \n<br> <br>\nSo this is a set of 299 patients who sufferend some sort of heart failure.  Note the data collected in 2015, so somewhat recent. <br> \n**<a href=\"https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5/tables/1\"> Table of variable descriptions</a>** <br>\n<br>\nI'm going to list these out here just so we have everything together in one spot <br><br>\n**Age:** Age of the patient in years (continuous integer) <br>\n**anaemia:** indicates a decrease in red blood cells or hemoglobin (binary) <br>\n**creatinine_phosphokinase:** Level of the CPK enzyme in the blood, measured in mcg/L (continuous) <br>\n&emsp;&emsp; <a href=\"https://www.hopkinslupus.org/lupus-tests/clinical-tests/creatine-phosphokinase-cpk/\"> more info on CPK enzyme here</a>  <br />\n**diabetes:** indicates the patient had diabetes (binary) <br>\n**ejection_fraction:** Percentage of blood leaving the heart at each contraction, as a percentage (continuous integer) <br>\n**high_blood_pressure:** indicates the patient has hypertension (binary) <br />\n**platelets:** count of platelets in the blood, measured in kiloplatelets/mL (continuous integer) <br>\n**serum_creatinine:** level of serum creatinine in the blood, measured in mg/dL (continuous) <br>\n**serum_sodium:** Level of serum sodium in the blood, measured in mEq/L (continuous)  <br>\n**sex:** Woman or man (binary) <br />\n**smoking:** indicates the patient was a smoker (binary) <br />\n**time:** Follow-up period (days) (int, continuous) <br />\n**DEATH_EVENT:** indicates paitient mortality during follow-up period (binary) <br />\n<br />\n<br /> Right off the bat I'm concerned about the time variable.  If a patient dies, does this cut short the follow-up period?  Also, at the time of the heart failure event, we wouldn't know what the follow-up period would be.  The authors of the article can make whatever case they want, but if I'm going to make a model that would be intended for use (calculating the probability of mortality for a given patient at the time of a heart failure event) then I can't in good faith include it. <br />\n<br /> I also want to mention that this is an <i> extremely </i> limited dataset.  We don't know what risk factors these patients might have (including chronic conditions), what their patter of care has been over the preceding months/years, what exacty was done during the heart failure event, etc.  "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reading our data set using pandas and taking a quick look at the variables included to make sure they meet expectations\ndf = pd.read_csv('/kaggle/input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"note - no null values\n\ndropping time"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['time'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Going to do a little bit of data exploration.  I might come back and do more if I find time."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot(df['age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of our patients are in the 40-70 range.  <br>\n<br>\nSince we have such a small data set I won't consider splitting it.  By that I mean we could train a model on the group <= 65 and and another on the > 65 group.  I would expect a model trained excusively on working age patients to perform better than a set that includes retirees.  From a US healthcare insurance perspective this would also make sense; the working class group would have commercial insurance and the majority of retirees would have Medicare.<br>\n<br>\nFor the same reason I'm not going to consider further limiting our data either (trimming or winsorizing outliers, removing noise, etc.)  I feel like this changes the overall scope of the problem, and isn't in the spirit of this particular exercise ."},{"metadata":{"trusted":true},"cell_type":"code","source":"#### looking at the distribution of age, splitting on death event ####\nsns.violinplot(x=\"sex\", y=\"age\", data=df, hue=\"DEATH_EVENT\", palette=\"pastel\", split=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No surprises here.  Going to run pairplots on the rest of these variables just to get an idea of what we're looking at.  I've already stated that I'm not planning on limiting this data set further than it already is.  If that wasn't the case we could spend more time here considering our options."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[['age','sex','anaemia','creatinine_phosphokinase','diabetes','DEATH_EVENT']],hue=\"DEATH_EVENT\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### going to change the palette so it's a bit easier to differentiate between these plots #####\nsns.pairplot(df[['age','sex','ejection_fraction','high_blood_pressure','platelets','DEATH_EVENT']],hue=\"DEATH_EVENT\",palette=\"dark\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[['age','sex','serum_creatinine','serum_sodium','smoking','DEATH_EVENT']],hue=\"DEATH_EVENT\",palette=\"bright\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets put together some models \n# setting up our training data set\nx = df.drop(['DEATH_EVENT'], axis=1).values\ny = df['DEATH_EVENT'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\n# using default training size.  It's worth considering a larger training size\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to run the classifier gauntlet so to speak.  As I discover more I'll add them here as a reference. <br>\n<br>\n<b> Logistic Regression </b>\n<br>\nfitting the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"creating predictions on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_pred = logreg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"compiling evaluation metrics for comparison <br>\n<br>\n<a href=\"https://en.wikipedia.org/wiki/Receiver_operating_characteristic\">I find the wikipedia article on ROC a very convenient reference</a>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, auc, accuracy_score, f1_score\nlog_cm = confusion_matrix(y_test, log_pred)\nlog_acc = accuracy_score(y_test, log_pred)\nlog_f1 = f1_score(y_test, log_pred)\nprint(str(log_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Support Vector Machines </b> <br>\nUsing the default value C=1.  If I remember I will go back and run this for different values of C <br>\nI could also play with different kernels, but I'm not really expecting this model to do well really"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsupp_vect = SVC()\nsupp_vect.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scv_pred = supp_vect.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_cm = confusion_matrix(y_test, scv_pred)\nsvc_acc = accuracy_score(y_test, scv_pred)\nsvc_f1 = f1_score(y_test, scv_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> K Nearest Neighbors </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nknn_scale = StandardScaler()\nknn_scale.fit(x)\nknn_x = knn_scale.transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_x_train, knn_x_test, knn_y_train, knn_y_test = train_test_split(knn_x,y,train_size = 0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nerr = []\nk = []\n\nfor i in range(1,30):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(knn_x_train,knn_y_train)\n    knn_pred_i = knn.predict(knn_x_test)\n    err.append(np.mean(knn_pred_i != knn_y_test))\n    k.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(k, err)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k = 19 seems to be a pretty good choice"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=19)\nknn.fit(knn_x_train,knn_y_train)\nknn_pred = knn.predict(knn_x_test)\n\nknn_cm = confusion_matrix(knn_y_test, knn_pred)\nknn_acc = accuracy_score(knn_y_test, knn_pred)\nknn_f1 = f1_score(knn_y_test, knn_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> The SciKit Random Forest model </b> \n<br> <br> I want to note that there's a SciKit Decision Tree model as well.  The Random Forest model almost always perfoms better so I'm going to skip it for now."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(x_train, y_train)\nrfc_pred = rfc.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_cm = confusion_matrix(y_test, rfc_pred)\nrfc_acc = accuracy_score(y_test, rfc_pred)\nrfc_f1 = f1_score(y_test, rfc_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b><a href=\"https://xgboost.readthedocs.io/en/latest/\">xgboost</a></b> - Extreme Gradient Boosted trees.  Right now (Oct 2020) this is one of the best performing ml algorithms out there."},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb_out = []\nxgb_est = []\nfor e in range(5,30):\n    classifier = XGBClassifier(n_estimators = e, max_depth=12, subsample=0.75)\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    xgb_out.append(accuracy_score(y_test,y_pred))\n    xgb_est.append(e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(xgb_est, xgb_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = XGBClassifier(n_estimators = 14, max_depth=12, subsample=0.75)\nclassifier.fit(x_train, y_train)\nxgb_pred = classifier.predict(x_test)\n\n\nxgb_cm = confusion_matrix(y_test, xgb_pred)\nxgb_acc = accuracy_score(y_test, xgb_pred)\nxgb_f1 = f1_score(y_test, xgb_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Catboost </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier\nclassifier = CatBoostClassifier()\nclassifier.fit(x_train, y_train, silent=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_pred = classifier.predict(x_test)\n\ncat_cm = confusion_matrix(y_test, cat_pred)\ncat_acc = accuracy_score(y_test, cat_pred)\ncat_f1 = f1_score(y_test, cat_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b><a href=\"https://lightgbm.readthedocs.io/en/latest/\"> lightgbm </a> </b> - This model has a lot of parameters that can be optimized.  If I get time I'll try to add to this."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgtrain = lgb.Dataset(x_train, label=y_train)\nlgtest = lgb.Dataset(x_test, label=y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {  \"boosting_type\":'gbdt', \n            \"class_weight\":None,\n            \"num_leaves\": 100,\n            \"objective\": 'binary',\n            \"metric\": 'auc',\n            \"verbose\": -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = lgb.train(params, lgtrain, 100, valid_sets=[lgtrain, lgtest], early_stopping_rounds=200, verbose_eval=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_pred = lgbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# need to optimize a threshold for these predictions \n# here we're going to maximize F1 by minimizing the negated F1-Score\ndef neg_f1(threshold, y_true, y_hat):\n    return -f1_score(y_true, y_hat > threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import minimize\nf1_opt = minimize(fun=neg_f1, x0=np.median(lgb_pred), args=(y_test,lgb_pred), method='nelder-mead')\nf1_opt.x[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_cm = confusion_matrix(y_test, lgb_pred > f1_opt.x[0])\nprint(lgb_cm)\nTN, FP, FN, TP = confusion_matrix(y_test, lgb_pred > f1_opt.x[0]).ravel()\nlgb_acc = (TP + TN)/(TP + TN + FP + FN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looking at accuracy.  We could also compare some of the other metrics I've been compiling as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the accuracy of the logistic model is: {}'.format(round(log_acc, 3)))\nprint('the accuracy of the supprot vector machine is: {}'.format(round(svc_acc, 3)))\nprint('the accuracy of the k nearest neighbor is: {}'.format(round(knn_acc, 3)))\nprint('the accuracy of the scikit random forest is : {}'.format(round(rfc_acc, 3)))\nprint('the accuracy of the xgboost model is: {}'.format(round(xgb_acc, 3)))\nprint('the accuracy of the catboost model is: {}'.format(round(cat_acc, 3)))\nprint('the accuracy of the lightboost model is: {}'.format(round(lgb_acc, 3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's more we can do to tune these models, but I don't feel like this particilar problem is well suited for this sort of thing considering how small our data set is.  These scores are not very good whatsoever, but I think that speaks to the data moreso than anything."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}