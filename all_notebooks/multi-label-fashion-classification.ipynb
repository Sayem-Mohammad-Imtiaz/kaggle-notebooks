{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%writefile utils.py\nimport os\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nfrom tqdm import tqdm\n\nmatplotlib.style.use('ggplot')\n\ndef clean_data(df):\n    drop_indices = []\n    print('[INFO]: Checking if all images are present')\n    for index, image_id in tqdm(df.iterrows()):\n        if not os.path.exists(f\"../input/fashion-product-images-small/images/{image_id.id}.jpg\"):\n            drop_indices.append(index)\n\n    print(f\"[INFO]: Dropping indices: {drop_indices}\")\n    df.drop(df.index[drop_indices], inplace=True)\n    return df\n\n# save the trained model to disk\ndef save_model(epochs, model, optimizer, criterion):\n    torch.save({\n                'epoch': epochs,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': criterion,\n                }, './model.pth')\n\n# save the train and validation loss plots to disk\ndef save_loss_plot(train_loss, val_loss):\n    plt.figure(figsize=(10, 7))\n    plt.plot(train_loss, color='orange', label='train loss')\n    plt.plot(val_loss, color='red', label='validataion loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig('./loss.jpg')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile label_dicts.py\nimport pandas as pd\nimport joblib\n\nfrom utils import clean_data\n\n\ndef save_label_dicts(df):\n    # remove rows from the DataFrame which do not have corresponding images\n    df = clean_data(df)\n\n    # we will use the `gender`, `masterCategory`. and `subCategory` labels\n    # mapping `gender` to numerical values\n    cat_list_gender = df['gender'].unique()\n    # 5 unique categories for gender\n    num_list_gender = {cat:i for i, cat in enumerate(cat_list_gender)}\n    # mapping `masterCategory` to numerical values\n    cat_list_master = df['masterCategory'].unique()\n    # 7 unique categories for `masterCategory`\n    num_list_master = {cat:i for i, cat in enumerate(cat_list_master)}\n    # mapping `subCategory` to numerical values\n    cat_list_sub = df['subCategory'].unique()\n    # 45 unique categories for `subCategory`\n    num_list_sub = {cat:i for i, cat in enumerate(cat_list_sub)}\n\n    joblib.dump(num_list_gender, './num_list_gender.pkl')\n    joblib.dump(num_list_master, './num_list_master.pkl')\n    joblib.dump(num_list_sub, './num_list_sub.pkl')\n\ndf = pd.read_csv('../input/fashion-product-images-small/styles.csv', usecols=[0, 1, 2, 3, 4, 4, 5, 6, 9])\nsave_label_dicts(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python label_dicts.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile dataset.py\nfrom torch.utils.data import Dataset\nfrom utils import clean_data\n\nimport torch\nimport joblib\nimport math\nimport cv2\nimport torchvision.transforms as transforms\n\ndef train_val_split(df):\n    df = clean_data(df)\n\n    # shuffle the dataframe\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    # 90% for training and 10% for validation\n    num_train_samples = math.floor(len(df) * 0.90)\n    num_val_samples = math.floor(len(df) * 0.10)\n\n    train_df = df[:num_train_samples].reset_index(drop=True)\n    val_df = df[-num_val_samples:].reset_index(drop=True)\n\n    return train_df, val_df\n\nclass FashionDataset(Dataset):\n    def __init__(self, df, is_train=True):\n        self.df = df\n        self.num_list_gender = joblib.load('./num_list_gender.pkl')\n        self.num_list_master = joblib.load('./num_list_master.pkl')\n        self.num_list_sub = joblib.load('./num_list_sub.pkl')\n        self.is_train = is_train\n\n        # the training transforms and augmentations\n        if self.is_train:\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((224, 224)),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomVerticalFlip(p=0.5),\n                transforms.ToTensor(), \n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            ])\n\n        # the validation transforms\n        if not self.is_train:\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n            ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        image = cv2.imread(f\"../input/fashion-product-images-small/images/{self.df['id'][index]}.jpg\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.transform(image)\n\n        cat_gender = self.df['gender'][index]\n        label_gender = self.num_list_gender[cat_gender]\n\n        cat_master = self.df['masterCategory'][index]\n        label_master = self.num_list_master[cat_master]\n\n        cat_sub = self.df['subCategory'][index]\n        label_sub = self.num_list_sub[cat_sub]\n\n        # image to float32 tensor\n        image = torch.tensor(image, dtype=torch.float32)\n        # labels to long tensors\n        label_gender = torch.tensor(label_gender, dtype=torch.long)\n        label_master = torch.tensor(label_master, dtype=torch.long)\n        label_sub = torch.tensor(label_sub, dtype=torch.long)\n\n        return {\n            'image': image,\n            'gender': label_gender,\n            'master': label_master,\n            'sub': label_sub\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile loss_functions.py\nimport torch.nn as nn\n\n# custom loss function for multi-head multi-category classification\ndef loss_fn(outputs, targets):\n    o1, o2, o3 = outputs\n    t1, t2, t3 = targets\n    l1 = nn.CrossEntropyLoss()(o1, t1)\n    l2 = nn.CrossEntropyLoss()(o2, t2)\n    l3 = nn.CrossEntropyLoss()(o3, t3)\n\n    return (l1 + l2 + l3) / 3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile models.py\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pretrainedmodels\n\nclass MultiHeadResNet50(nn.Module):\n    def __init__(self, pretrained, requires_grad):\n        super(MultiHeadResNet50, self).__init__()\n        if pretrained == True:\n            self.model = pretrainedmodels.__dict__['resnet50'](pretrained='imagenet')\n        else:\n            self.model = pretrainedmodels.__dict__['resnet50'](pretrained=None)\n\n\n        if requires_grad == True:\n            for param in self.model.parameters():\n                param.requires_grad = True\n            print('Training intermediate layer parameters...')\n        elif requires_grad == False:\n            for param in self.model.parameters():\n                param.requires_grad = False\n            print('Freezing intermediate layer parameters...')\n\n        # change the final layer\n        self.l0 = nn.Linear(2048, 5)\n        self.l1 = nn.Linear(2048, 7)\n        self.l2 = nn.Linear(2048, 45)\n\n    def forward(self, x):\n        # get the batch size only, ignore (c, h, w)\n        batch, _, _, _ = x.shape\n        x = self.model.features(x)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n        l0 = self.l0(x)\n        l1 = self.l1(x)\n        l2 = self.l2(x)\n        return l0, l1, l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile train.py\nimport pandas as pd\nimport torch\nimport torch.optim as optim\n\nfrom dataset import train_val_split, FashionDataset\nfrom torch.utils.data import DataLoader\nfrom models import MultiHeadResNet50\nfrom tqdm import tqdm\nfrom loss_functions import loss_fn\nfrom utils import save_model, save_loss_plot\n\n# define the computation device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = MultiHeadResNet50(pretrained=True, requires_grad=False).to(device)\n\n# learning parameters\nlr = 0.001\noptimizer = optim.Adam(params=model.parameters(), lr=lr)\ncriterion = loss_fn\nbatch_size = 32\nepochs = 40\n\ndf = pd.read_csv('../input/fashion-product-images-small/styles.csv', usecols=[0, 1, 2, 3, 4, 4, 5, 6, 9])\ntrain_data, val_data = train_val_split(df)\nprint(f\"[INFO]: Number of training sampels: {len(train_data)}\")\nprint(f\"[INFO]: Number of validation sampels: {len(val_data)}\")\n\n# training and validation dataset\ntrain_dataset = FashionDataset(train_data, is_train=True)\nval_dataset = FashionDataset(val_data, is_train=False)\n# training and validation data loader\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True\n)\nval_dataloader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False\n)\n\n# training function\ndef train(model, dataloader, optimizer, loss_fn, dataset, device):\n    model.train()\n    counter = 0\n    train_running_loss = 0.0\n    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n        counter += 1\n        \n        # extract the features and labels\n        image = data['image'].to(device)\n        gender = data['gender'].to(device)\n        master = data['master'].to(device)\n        sub = data['sub'].to(device)\n        \n        # zero-out the optimizer gradients\n        optimizer.zero_grad()\n        \n        outputs = model(image)\n        targets = (gender, master, sub)\n        loss = loss_fn(outputs, targets)\n        train_running_loss += loss.item()\n        \n        # backpropagation\n        loss.backward()\n        # update optimizer parameters\n        optimizer.step()\n        \n    train_loss = train_running_loss / counter\n    return train_loss\n\n# validation function\ndef validate(model, dataloader, loss_fn, dataset, device):\n    model.eval()\n    counter = 0\n    val_running_loss = 0.0\n    for i, data in tqdm(enumerate(dataloader), total=int(len(dataset)/dataloader.batch_size)):\n        counter += 1\n        \n        # extract the features and labels\n        image = data['image'].to(device)\n        gender = data['gender'].to(device)\n        master = data['master'].to(device)\n        sub = data['sub'].to(device)\n        \n        outputs = model(image)\n        targets = (gender, master, sub)\n        loss = loss_fn(outputs, targets)\n        val_running_loss += loss.item()\n        \n    val_loss = val_running_loss / counter\n    return val_loss\n\n# start the training\ntrain_loss, val_loss = [], []\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1} of {epochs}\")\n    train_epoch_loss = train(\n        model, train_dataloader, optimizer, loss_fn, train_dataset, device\n    )\n    val_epoch_loss = validate(\n        model, val_dataloader, loss_fn, val_dataset, device\n    )\n    train_loss.append(train_epoch_loss)\n    val_loss.append(val_epoch_loss)\n    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n    print(f\"Validation Loss: {val_epoch_loss:.4f}\")\n\n# save the model to disk\nsave_model(epochs, model, optimizer, criterion)\n# save the training and validation loss plot to disk\nsave_loss_plot(train_loss, val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python train.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}