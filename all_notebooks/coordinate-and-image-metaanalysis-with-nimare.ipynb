{"cells":[{"metadata":{"_uuid":"37d3b2156a59915f319833db6ed97784f77bf70d"},"cell_type":"markdown","source":"Tutorial adapted from [NiMARE examples](https://github.com/neurostuff/NiMARE/blob/98dd69b7b637ab61c90a8de0f45b36cdadc8154a/examples/nidm_pain_meta-analyses.ipynb)  prepared by Taylor Salo. "},{"metadata":{"_uuid":"97dca82d2b11775e8451707820c5c5afc4211371"},"cell_type":"markdown","source":"**Before you begin please make sure your kernel is [Internet connected](https://www.kaggle.com/product-feedback/63544)**"},{"metadata":{"trusted":true,"_uuid":"c527597bece84d1fa7da2aae4cb870fd0060abbe","_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"!pip install git+git://github.com/neurostuff/NiMARE.git@608516ec3034e356326dfe70df5e9ed77efd2be8\nimport os\nimport json\nimport numpy as np\nimport nibabel as nb\nimport tempfile\nfrom glob import glob\nfrom os.path import basename, join, dirname, isfile\n\nimport pandas as pd\nimport nibabel as nib\nimport pylab as plt\nfrom scipy.stats import t\nfrom nilearn.masking import apply_mask\nfrom nilearn.plotting import plot_stat_map\n\nimport nimare\nfrom nimare.meta.ibma import (stouffers, fishers, weighted_stouffers,\n                              rfx_glm, ffx_glm)\nfrom nimare.utils import t_to_z","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64e99248f60432a4589e1190a66459537b0a1a32"},"cell_type":"markdown","source":"# Exploring the data\nThis tutorial assumes that you did literature review and extracted coordinates of peaks of activation from relevant papers. Those could be saved in spreadsheeta (or CSV files). In our case we have to CSV files. One with all of the coordinates:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"639b0a2fc166e742f0fb08347f89288bc2c90709"},"cell_type":"code","source":"pd.read_csv('../input/coordinates.csv').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33978c9a58baa549fb00259ae26db447f516fb23"},"cell_type":"markdown","source":"...and one with information about the studies:"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"496f14e891c8201154db9a83b9351f4fd8ac425a"},"cell_type":"code","source":"pd.read_csv('../input/studies.csv').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36c94344fe92727da831035cf5a393f71f10a91e"},"cell_type":"markdown","source":"**Excercise: how many coordinates are there per study?**"},{"metadata":{"_uuid":"e5052393530207db2ec7543d42e7d856ef9cb68a"},"cell_type":"markdown","source":"Before using NiMARE we need to prepare the data in a format the library expects"},{"metadata":{"trusted":true,"_uuid":"5376062417ecea949e22acb3f3a9fc5d41c33de8"},"cell_type":"code","source":"dset_dict = {}\ncoords_df = pd.read_csv('../input/coordinates.csv')\nfor i, row in pd.read_csv('../input/studies.csv').iterrows():\n    this_study_coords = coords_df[coords_df['study_id'] == row[0]]\n    contrast = {\"sample_sizes\": [row[1]],\n                \"coords\": { \"space\": this_study_coords['space'].unique()[0],\n                            \"x\": list(this_study_coords['x']),\n                            \"y\": list(this_study_coords['y']),\n                            \"z\": list(this_study_coords['z'])}}\n    dset_dict[row[0]] = {\"contrasts\": {\"1\": contrast }}\nwith tempfile.NamedTemporaryFile(mode='w', suffix=\".json\") as fp:\n    json.dump(dset_dict, fp)\n    fp.flush()\n    db = nimare.dataset.Database(fp.name)\n    dset = db.get_dataset()\nmask_img = dset.mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad4fef34a84336b94652ae050ead75e018eb9294"},"cell_type":"code","source":"dset.data['pain_01']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dabea3bc0b0a28f96c023c4d9702a17248c06bd5"},"cell_type":"markdown","source":"# Coordinate based meta-analysis"},{"metadata":{"_uuid":"e53b5c15e0555d372ae7eae92662790f83b6ad1f"},"cell_type":"markdown","source":"## ALE"},{"metadata":{"_uuid":"56adfeced90da5090d4bfc6713ae8bb29099e673"},"cell_type":"markdown","source":"We will start with using coordinates of peaks and convolving them with gaussians. This is also known as the ALE method. First image is before statistical inference and the second is showing only statistically significant regions."},{"metadata":{"trusted":true,"_uuid":"e7fdb2babc3269e368ffd073c859124c6da835d4"},"cell_type":"code","source":"ale = nimare.meta.cbma.ALE(dset, ids=dset.ids)\nale.fit(n_iters=10)\nplot_stat_map(ale.results.images['ale'], cut_coords=[0, 0, -8],\n              draw_cross=False, cmap='RdBu_r', figure=plt.figure(figsize=(18,4)))\nplot_stat_map(ale.results.images['z_vfwe'], cut_coords=[0, 0, -8],\n              draw_cross=False, cmap='RdBu_r', figure=plt.figure(figsize=(18,4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40ca98f148925871af369ca6dbadd992839aecdb"},"cell_type":"markdown","source":"Note that we are only using 10 interations in permutation testing to speed things up. Normally you would use ~10,000 permutations to accuratelly establish the null distribution."},{"metadata":{"_uuid":"830a47efa99974f143c42331795e62276e395ae1"},"cell_type":"markdown","source":"## MKDA"},{"metadata":{"_uuid":"58ab32039bbfb8a07759f770f0b39fef13d3ff55"},"cell_type":"markdown","source":"An alternative to ALE is convolving with solid spheres and accounting for overlaps. This is known as the MKDA method"},{"metadata":{"trusted":true,"_uuid":"6bc0a7625521c7a5d27817014049b0a298a997c0"},"cell_type":"code","source":"mkda = nimare.meta.cbma.MKDADensity(dset, ids=dset.ids, kernel__r=10)\nmkda.fit(n_iters=10)\nplot_stat_map(mkda.results.images['vfwe'], cut_coords=[0, 0, -8],\n              draw_cross=False, cmap='RdBu_r', figure=plt.figure(figsize=(18,4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06bdc16f011e174b150da447709e02952b05d68e"},"cell_type":"markdown","source":"The results in this particular example are vey similar."},{"metadata":{"_uuid":"41c0f08b90a9b2c7e502697fc99d764ab6f197e9"},"cell_type":"markdown","source":"Want to explore more coordinate based data? Check out https://www.kaggle.com/chrisfilo/neurosynth"},{"metadata":{"_uuid":"51a9d8e34a9050cce2655425a45b26cc1c57a930"},"cell_type":"markdown","source":"# Image based meta-analysis"},{"metadata":{"_uuid":"c0e7d96eec25666a17766566427b7b63ed650b36"},"cell_type":"markdown","source":"So far we used only coordinates of peaks. This is a massive data reduction if you take into account the amount of data acquired by the scanner. How would the results of the analysis look if we had access to the underlying statistical maps?"},{"metadata":{"_uuid":"2dd2edfb5ad13eee0a1740567c539f6734a8c153"},"cell_type":"markdown","source":"## Get z-maps\nHere'we are going to prepare the maps for analysis. All maps will be masked and converted to numpy arrays. T maps will be converted to Z maps."},{"metadata":{"trusted":true,"_uuid":"465b6545426925966e294530edfa2e78156a8961"},"cell_type":"code","source":"z_imgs = []\nsample_sizes = []\nfor study in dset_dict.keys():\n    z_map_path = \"../input/stat_maps/%s.nidm/ZStatistic_T001.nii.gz\"%study\n    t_map_path = \"../input/stat_maps/%s.nidm/TStatistic.nii.gz\"%study\n    sample_size = dset_dict[study][\"contrasts\"][\"1\"][\"sample_sizes\"][0]\n    if os.path.exists(z_map_path):\n        z_imgs.append(nb.load(z_map_path))\n        sample_sizes.append(sample_size)\n    elif os.path.exists(t_map_path):\n        t_map_nii = nb.load(t_map_path)\n        # assuming one sided test\n        z_map_nii = nb.Nifti1Image(t_to_z(t_map_nii.get_fdata(), sample_size-1), t_map_nii.affine)\n        z_imgs.append(z_map_nii)\n        sample_sizes.append(sample_size)\n        \nz_data = apply_mask(z_imgs, mask_img)\nsample_sizes = np.array(sample_sizes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"678dbea9f8dc02152d3277e305dc4b907ed8e665"},"cell_type":"markdown","source":"Lets have a look at some of the  individual maps."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"1b1a0017f4b40f89bdc40eccf6c5d4a9fa45c670"},"cell_type":"code","source":"for z_img in z_imgs[:5]:\n    plot_stat_map(z_img, threshold=0, cut_coords=[0, 0, -8], \n                  draw_cross=False, figure=plt.figure(figsize=(18,4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47d70367f698a8cdcadd895113e2a2bf20da2e60"},"cell_type":"markdown","source":"## Fisher's"},{"metadata":{"trusted":true,"_uuid":"53b93c1accbbc0d610f0ffe4be08723bcde34a0e"},"cell_type":"code","source":"result = fishers(z_data, mask_img)\nplot_stat_map(result.images['ffx_stat'], threshold=0,\n              cut_coords=[0, 0, -8], draw_cross=False,\n              figure=plt.figure(figsize=(18,4)))\nplot_stat_map(result.images['log_p'], threshold=-np.log(.05),\n              cut_coords=[0, 0, -8], draw_cross=False,\n              cmap='RdBu_r', figure=plt.figure(figsize=(18,4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"808055ac863065fa6d6073f6453a9aa446a78369"},"cell_type":"markdown","source":"**Excercise: does this method distinguish positive and negative activation? Could you plot such map?**"},{"metadata":{"_uuid":"3a323bf0c7eadae42f9e6f824c0d5ac529ff6b17"},"cell_type":"markdown","source":"## Weighted Stouffer's\nWe can use sample_sizes to perform a weighted version of the above analysis."},{"metadata":{"trusted":true,"_uuid":"59a87f83aa1924c25361096781b86a8107d3391c"},"cell_type":"code","source":"result = weighted_stouffers(z_data, sample_sizes, mask_img)\nplot_stat_map(result.images['log_p'], threshold=-np.log(.05),\n              cut_coords=[0, 0, -8], draw_cross=False,\n              cmap='RdBu_r', figure=plt.figure(figsize=(18,4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85dbb61913b45103eb774588737abbc92286250e"},"cell_type":"markdown","source":"**Excercise: adjust the sample sizes to influence the weighted analysis.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}