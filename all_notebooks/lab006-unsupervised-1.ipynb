{"cells":[{"metadata":{},"cell_type":"markdown","source":"# K-means Clustering\nK-means clustering is one of the simplest and popular unsupervised machine learning algorithms."},{"metadata":{},"cell_type":"markdown","source":"![](http://www.learnbymarketing.com/wp-content/uploads/2015/01/method-k-means-steps-example.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n#generating some random data in a two-dimensional\nX= -2 * np.random.rand(100,2)\nX1 = 1 + 2 * np.random.rand(50,2)\nX[50:100, :] = X1\nplt.scatter(X[ : , 0], X[ :, 1], s = 50, c = 'b')\nplt.show()\n\n#k-means\nKmean = KMeans(n_clusters=2)\nKmean.fit(X)\n\n#Finding the centroid\nprint(Kmean.cluster_centers_)\n\nplt.scatter(X[ : , 0], X[ : , 1], s =50, c='b')\nplt.scatter(Kmean.cluster_centers_[0][0], Kmean.cluster_centers_[0][1], s=200, c='g', marker='s')\nplt.scatter(Kmean.cluster_centers_[1][0], Kmean.cluster_centers_[1][1], s=200, c='r', marker='s')\nplt.show()\n\n#Testing the algorithm\nprint(Kmean.labels_)\n\n#predicting the cluster of a data point\nsample_test=np.array([-3.0,-3.0])\nsecond_test=sample_test.reshape(1, -1)\nprint(Kmean.predict(second_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kmeans on Geyserâ€™s Eruptions Segmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets.samples_generator import (make_blobs,\n                                                make_circles,\n                                                make_moons)\nfrom sklearn.cluster import KMeans, SpectralClustering\n#from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nplt.style.use('fivethirtyeight')\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Import the data\ndf = pd.read_csv('../input/old-faithful/faithful.csv')\ndf = df[[\"eruptions\",\"waiting\"]]\n\n# Plot the data\nplt.figure(figsize=(6, 6))\nplt.scatter(df.iloc[:, 0], df.iloc[:, 1])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of raw data');\n\n# Standardize the data\nX_std = StandardScaler().fit_transform(df)\n\n# Run local implementation of kmeans\nkm = KMeans(n_clusters=2, max_iter=20, random_state=20)\nkm.fit(X_std)\ncentroids = km.cluster_centers_\n\n# Plot the clustered data\nfig, ax = plt.subplots(figsize=(6, 6))\nplt.scatter(X_std[km.labels_ == 0, 0], X_std[km.labels_ == 0, 1],\n            c='green', label='cluster 1')\nplt.scatter(X_std[km.labels_ == 1, 0], X_std[km.labels_ == 1, 1],\n            c='blue', label='cluster 2')\nplt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300,\n            c='r', label='centroid')\nplt.legend()\nplt.xlim([-2, 2])\nplt.ylim([-2, 2])\nplt.xlabel('Eruption time in mins')\nplt.ylabel('Waiting time to next eruption')\nplt.title('Visualization of clustered data', fontweight='bold')\nax.set_aspect('equal');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 1\nImplement k-means on Iris dataset with k=3"},{"metadata":{},"cell_type":"markdown","source":"# Principal Component Analysis (PCA) as Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndataset = load_iris()\n\nx = dataset.data;\ny = dataset.target.reshape(-1,1)\n\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\n#PCA\n#pca = PCA(n_components=2)\npca = PCA(0.7)\nfeatures = pca.fit_transform(x)\n\nprint(features.shape)\n\nprint(pca.explained_variance_ratio_)\n\n#classification\nencoder = OneHotEncoder()\ntargets = encoder.fit_transform(y)\n\ntrain_features, test_features, train_targets, test_targets = train_test_split(features,targets, test_size=0.2, random_state=100)\n\nmodel = Sequential()\n# first parameter is output dimension\nmodel.add(Dense(10, input_dim=features.shape[1], activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(10, input_dim=10, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n#we can define the loss function MSE or negative log lokelihood\n#optimizer will find the right adjustements for the weights: SGD, Adagrad, ADAM ...\nmodel.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.summary()\n\nmodel.fit(train_features, train_targets, epochs=20, batch_size=20, verbose=2)\n\nloss, accuracy = model.evaluate(test_features, test_targets)\n\nprint(\"Accuracy on the test dataset: %.2f\" % accuracy)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 2\nChange the data dimension using PCA from 1-4 and observe the results."},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 3\nImplement PCA on MNIST data and then use that data to classify output."},{"metadata":{},"cell_type":"markdown","source":"# Linear Discriminant Analysis (LDA)"},{"metadata":{},"cell_type":"markdown","source":"Implementing the Linear Discriminant Analysis algorithm, can use the predefined LinearDiscriminantAnalysis class made available to us by the scikit-learn library"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_wine\nimport pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nwine = load_wine()\nX = pd.DataFrame(wine.data, columns=wine.feature_names)\ny = pd.Categorical.from_codes(wine.target, wine.target_names)\n\n#create a DataFrame containing both the features and classes\ndf = X.join(pd.Series(y, name='class'))\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nX_lda = lda.fit_transform(X, y)\n\nlda.explained_variance_ratio_\n\nle = LabelEncoder()\ny = le.fit_transform(df['class'])\n\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.scatter(\n    X_lda[:,0],\n    X_lda[:,1],\n    c=y,\n    cmap='rainbow',\n    alpha=0.7,\n    edgecolors='b'\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X_lda, y, random_state=1)\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nconfusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checkpoint 4\n![](http://)Implement LDA on MNIST data and then use that data to classify output."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}