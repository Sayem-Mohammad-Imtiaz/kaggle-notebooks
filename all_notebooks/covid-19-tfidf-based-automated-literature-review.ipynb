{"cells":[{"metadata":{},"cell_type":"markdown","source":"### ** TFIDF-based Automated Literature Review**\n\n***\n\nThis notebook applied Natural Language Processing (NLP) and other AI techniques to generate insights in the support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up. \n<br>\n<br>Language is unstructured data that has been produced by people to be understood by other people. Text data is not random, it is governed by linguistic properties that make it very understandable to other people and also processable by computers !!\n\n***\n\n**Methodology:** The authors extracted a number of abstracts that openly studied COVID-19 and its derivatives. Each abstract was first embedded in a static TFIDF model. Then, the cosine similarity of a **dynamic user query** was computed against each abtract of the static corpus. By sorting the most similar articles to a given query, we can rapidly and easily access the most relevant articles. **This allows the medical research community, governments, and decision-makers to rapidly consult the latest findings and discoveries in a given knowledge area**  through automated literature review. Links to the full-text research paper are also embedded and directly clickable in the output dataframe.\n\nFurthermore, the authors proposed additional text-mining approaches to generate insights from the corpus of abstracts, including 1) wordcloud of COVID-19 abstracts, 2) Word2Vec model to retrieve the most similar words to a specific word (e.g., retrieve the most similar words to *\"origin\"*, *\"transmission\"*), and 3) t-SNE visualization of semantic clusters from the corpus.\n***\n**Highlights:**\n1.  **TFIDF-based Automated Literature Review**\n2.  Wordcloud of COVID-19 Abstracts\n3.  Word2Vec Model and Textual Similarities\n4.  TSNE-Visualization of Semantic Clusters\n***\n\n**Pros:**\n* Automation of Literature Review / Efficient Abstract Browser\n* Easy and Rapid Access to the Latest Findings in a Given Domain\n* Insightful Data Visualization Tools\n\n**Cons:**\n* An abstract is only a partial summary of a research paper.\n* Reduced Scope: Analysis of 5,058 abstracts out of 47,000 research papers.\n\n***"},{"metadata":{},"cell_type":"markdown","source":"# **Part 1: Data Extraction and Preparation**"},{"metadata":{},"cell_type":"markdown","source":"**Scope: ** To extract data, the code below scrapped the COVID-19 Open Research Dataset **(CORD-19)**, i.e., a resource of over 47,000 scholarly articles in JSON format, about COVID-19, SARS-CoV-2, and related coronaviruses. Among these articles, 26,408 articles, i.e., 56% of the corpus present abstracts that are believed to efficiently summarize the content of their respective articles. This notebook specifically focuses on the rapidly emerging literature that directly mention the terms *{\"Coronavirus\", \"Covid\", \"2019-nCov\", \"COVID-19\", \"SARS-CoV-2\"}* ; i.e., 5,058 abstracts that represent 19.1% of the total number of abstracts. We only considered the abstracts here, but the approach could be further applied to the text bodies."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_colwidth', 0)\nimport io\nimport os\nimport fnmatch\nimport json \nimport pandas as pd\nimport requests\nimport matplotlib.pyplot as plt\nfrom time import time \nimport en_core_web_sm\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize\nimport nltk\nimport operator\nimport tqdm\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.tokenize import word_tokenize\nimport gensim.corpora as corpora\nimport gensim\nfrom gensim.models import TfidfModel\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom IPython.core.display import HTML\nimport re\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nstopwords = nltk.corpus.stopwords.words('english')\nnlp = en_core_web_sm.load()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#### Time-Consuming\nt = time()\n\n#### Create Empty DataFrame ####\n\nABS = pd.DataFrame(columns = [\"Title\", \"Abstract\", \"URL\", \"Published Year\"])\n\n#### Retrieve articles URL ####\n\ndf = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\ndf=df[[\"title\", \"cord_uid\", \"sha\", \"url\", \"publish_time\"]]\n\n### Scrapping to retreive relevant articles ####\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            if fnmatch.fnmatch(filename, '*.json'):\n                path = os.path.join(dirname, filename)\n                data = json.load(open(path))\n                paper_id = data[\"paper_id\"]                \n            \n                try: \n                    abst = pd.DataFrame(data[\"abstract\"])\n                    abst =abst[[\"text\"]]\n                    abst=abst.rename(columns={\"text\": \"Abstract\"})\n\n                    art = pd.DataFrame(data[\"metadata\"])\n                    art =art[[\"title\"]]\n                    art=art.rename(columns={\"title\": \"Title\"})\n\n                    art[\"Abstract\"] = abst.iloc[0,0]\n                    subset = df[df[\"sha\"]==paper_id]\n                    art[\"URL\"] = subset.iloc[0,3]\n                    art[\"Published Year\"] = subset.iloc[0,4]\n                    #### Track articles that explicitly mention coronaviruses and close derivatives in their abstracts ####\n\n                    Covid_List = [\"COVID\", \"covid\", \"Covid\", \"coronavirus\", \\\n                                  \"Coronavirus\", \"2019-nCov\",  \"SARS-CoV-2\" ]                  \n                    \n                    if any(s in art.iloc[0, 1] for s in Covid_List):\n\n                        ABS = pd.concat([ABS, art], sort = False)\n                        \n                except:\n                    pass\n\n            \nABS = ABS [[\"URL\", \"Title\", \"Published Year\", \"Abstract\"]]\nABS[\"Published Year\"] = pd.DatetimeIndex(ABS[\"Published Year\"]).year \nABS = ABS.drop_duplicates()\nABS.to_csv(\"Relevant_COVID-19_Abstracts.csv\", index = False)\n\nprint(\"Number of Relevant Abstracts\", len(ABS))\n\n#### Embedding Hyperlinks in the DataFrame ####\n\nfor i in range(len(ABS)):\n    if ABS.iloc[i, 1] != \"\":\n        ABS.iloc[i, 1] = '<a href=\"{}\">{}</a>'.format(ABS.iloc[i, 0], ABS.iloc[i, 1])\n    else:\n        ABS.iloc[i, 1] = '<a href=\"{}\">{}</a>'.format(ABS.iloc[i, 0], \"Link to Article\")\n\nABS = ABS[[\"Title\", \"Published Year\", \"Abstract\"]]\n\ndisplay(HTML(ABS.head(2).to_html(escape=False)))\n\nprint('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.subplots(figsize = (10,6))\nplt.hist(ABS[\"Published Year\"],bins = 30, edgecolor =\"black\")\nplt.title(\"Coronavirus-Related Academic Publications \\n\", fontsize = 20, fontweight = \"bold\")\nplt.xlabel(\"Published Year\")\nplt.ylabel(\"Publications\")\nplt.savefig(\"COVID-19_Publications_Histogram.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 2: TFIDF-based Abstract Browser**"},{"metadata":{},"cell_type":"markdown","source":"**TFIDF** for **Term Frequencyâ€“Inverse Document Frequency**, is a numerical statistic that reflects how important a word is to a document in a corpus.\n\n\\begin{equation*}\ntfidf(t, d, D)  = tf(t,d) \\times idf(t,D)\n\\end{equation*}\nwith:\n\\begin{equation*}\n{\\displaystyle \\mathrm {tf} (t,d)=0.5+0.5\\cdot {\\frac {f_{t,d}}{\\max\\{f_{t',d}:t'\\in d\\}}}}\n\\end{equation*}\nand:\n\\begin{equation*}\n \\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}\n\\end{equation*}\n\nwhere tf(t,d) represents the number of times that term t occurs in document d, idf(t, D) is is the logarithmically scaled inverse fraction of the documents that contain the word t, and N is the total number of documents d in the corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"def automated_LR(user_query):\n    '''\n    Function Description: Tokenize and clean each the retrieved abstracts\n    Create a dictionary converting distinct words into distinct numerical IDs\n    Build a tfidf model on static corpus\n    Compute cosine similarity of a dynamic query against a static corpus of documents\n    \n    Display most similar abstracts to the dynamic query - cosine similarity scores also displayed\n    \n    ''' \n    #### Data Preparation of Static Corpus #### \n\n    data1 = ABS['Abstract'].values.tolist() # List of documents\n\n    file_docs = [[w.lower() for w in word_tokenize(text)] for text in data1]\n\n    dictionary = gensim.corpora.Dictionary(file_docs)\n\n    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in file_docs]\n\n    #### TF-IDF Model ####\n\n    tf_idf = gensim.models.TfidfModel(corpus)\n\n    #### Compute cosine similarity of a dynamic query against a static corpus of documents ####\n    \n    sims = gensim.similarities.Similarity('/kaggle/working',tf_idf[corpus], num_features=len(dictionary))\n\n    #### Process Dynamic USER QUERY ####\n\n    tokens = sent_tokenize(user_query)\n    file2_docs = []\n\n    for line in tokens:\n        file2_docs.append(line)\n\n    for line in file2_docs:\n        query_doc = [w.lower() for w in word_tokenize(line)]\n        query_doc_bow = dictionary.doc2bow(query_doc) \n\n    #### Retrieve & Display Most Similar Abstracts ####\n\n    query_doc_tf_idf = tf_idf[query_doc_bow]\n    ABS1 = ABS\n    ABS1[\"Similarity\"] = sims[query_doc_tf_idf]\n    ABS1 =ABS1.sort_values(by = \"Similarity\", ascending = False)\n    \n    display(HTML(ABS1.head(3).to_html(escape=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 3: Automated Literature Review**"},{"metadata":{},"cell_type":"markdown","source":"This approach would enable the medical research community to review rapidly the latest findings and discoveries in a given domain. Several examples are presented below. Using a dynamic query as an input, the *automated_LR* function returns the most relevant abstracts to that queries. Three examples are shown below to automate the literature review about *\"What is known about transmission, incubation, and environmental stability?\"* (i.e., Task1). Other tasks can be adressed similarly by adjusting dynamic user queries."},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> What is known about transmission, incubation, and environmental stability? **"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#### What is known about transmission, incubation, and environmental stability? ####\n\n#### Dynamic User Query 1 ####\n'''Provide a quick overview of findings and results'''\n\nuser_query = \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\"\n\nautomated_LR(user_query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#### Dynamic User Query 2 ####\n\nuser_query = \"Persistence and stability on a multitude of substrates and sources \\\n                    (e.g., nasal discharge, sputum, urine, fecal matter, blood).\"\n\nautomated_LR(user_query)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Dynamic User Query 3 ####\n\nuser_query = \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\"\n\nautomated_LR(user_query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, the literature review of other tasks (e.g.,*What do we know about COVID-19 risk factors?*, *What do we know about vaccines and therapeutics?*, *\"What has been published about medical care?\"*, or *What do we know about non-pharmaceutical interventions?*) can be automated by adjusting the user query with any task subsection, as follows."},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> What do we know about COVID-19 risk factors?**"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#### What do we know about COVID-19 risk factors?  ####\n\nuser_query = \"Data on potential risks factors: smoking, pre-existing pulmonary disease, co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other comorbidities, neonates and pregnant women.\"\n\nautomated_LR(user_query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> What do we know about vaccines and therapeutics?**"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#### What do we know about vaccines and therapeutics?  ####\n\nuser_query = \"Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\"\n\nautomated_LR(user_query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> What has been published about medical care?**"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#### What has been published about medical care?  ####\n\nuser_query = \"Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.\"\n\nautomated_LR(user_query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ** <font color=green> What do we know about non-pharmaceutical interventions?**"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#### What do we know about non-pharmaceutical interventions?  ####\n\nuser_query = \"Methods to control the spread in communities, barriers to compliance and how these vary among different populations.\"\n\nautomated_LR(user_query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 4: Wordcloud of COVID-19 Abstracts**"},{"metadata":{},"cell_type":"markdown","source":"In a wordcloud, the importance of each word is shown with font size. In this section, a wordcloud of the most frequent words appearing in corpus of COVID-19 abstracts is built. A number of preprocessing steps (e.g., tokenization, lemmatization) are required to build a word cloud. As expected, words such as *\"coronavirus\"*, *\"\"infection*, and *\"respiratory\"* are particularly prominent. Other words such as *\"vaccine\"* and *\"proteine\"* have also been extensively discussed in literature.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordcloud(df, name):\n\n    words_dict = {}\n    stopwords = nltk.corpus.stopwords.words('english')\n    stopwords.extend([\"also\", \"three\", \"may\"])\n\n    text = \" \".join(review for review in df[name])\n    tokenizer = RegexpTokenizer(r'\\w+')\n    TK = tokenizer.tokenize(text.lower())\n\n    filtered_words = list(filter(lambda word: word not in stopwords, TK))\n\n    filtered_words1 = [w for w in filtered_words if w.isalpha()]\n        \n    lemmatizer = WordNetLemmatizer()\n\n    for w in range(len(filtered_words1)):\n        filtered_words1[w] = lemmatizer.lemmatize(filtered_words1[w])\n\n    for word in filtered_words1:\n        words_dict[word] = words_dict.get(word, 0) + 1\n\n    sorted_d = sorted(words_dict.items(), key=operator.itemgetter(1), reverse=True)\n    \n    return WordCloud(max_words=200, max_font_size=50, relative_scaling=0.5, stopwords=stopwords,\n                background_color=\"white\").generate_from_frequencies(words_dict) \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.subplots(figsize = (10,8))\n \nplt.imshow(wordcloud(ABS, \"Abstract\"), interpolation='bilinear')\nplt.axis(\"off\")\nplt.title(\"Wordcloud of Abstracts \\n\", fontsize = 24, fontweight = \"bold\")\nplt.savefig(\"Abstracts_Wordcloud.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Part 5: Word2Vec Model and Text Similarities**"},{"metadata":{},"cell_type":"markdown","source":"A **Word2Vec** model (Word to Vector) was built using Gensim Python library to produce word embeddings. Using a large corpus of text as an input, a Word2vec model returns a vector (here, 100 dimensions) for each unique word in the corpus. The similarity between vectors is measured through the cosine similarity metric. Similar vectors represent words that are semantically related in the original corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"import en_core_web_sm\nnlp = en_core_web_sm.load()\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n\n\ndef text_data(df, name):\n    \n    corpus1 = []\n\n    tokenizer = RegexpTokenizer(r'\\w+')\n    stopwords = nltk.corpus.stopwords.words('english')\n           \n    for sentence in df[name]:\n            word_list = tokenizer.tokenize(sentence.lower())\n            word_list1 = [word for word in word_list if word.isalpha()]\n            word_list2 = [word for word in word_list1 if word not in stopwords]\n            corpus1.append(word_list2)\n\n    bigram = gensim.models.Phrases(corpus1, min_count=10, threshold=100)  # higher threshold fewer phrases.\n    trigram = gensim.models.Phrases(bigram[corpus1], threshold=100)\n\n        # Faster way to get a sentence clubbed as a trigram/bigram\n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n    def make_bigrams(texts):\n        return [bigram_mod[doc] for doc in texts]\n\n    def make_trigrams(texts):\n        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\n    corpus2 = make_bigrams(corpus1)\n\n    corpus2= make_trigrams(corpus2)\n\n    corpus2 = lemmatization(corpus2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n    \n    return corpus2\n\ncovid_corpus = text_data(ABS, \"Abstract\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from gensim.models import word2vec, KeyedVectors\nfilename = 'Word2Vec_Covid_100dimensions'\n\n#### Build Word2Vec model ####\n\nmodel = word2vec.Word2Vec(covid_corpus, size=100, window=8, min_count=10, workers=10)\nmodel.train(covid_corpus, total_examples=len(covid_corpus), epochs=15)\nmodel.wv.save(filename)\nword_vectors = KeyedVectors.load(filename)\n\n#### Example of Word Embedding ####\n\nprint(\"Word Embedding for 'Coronavirus'\", model.wv['coronavirus'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Most Similar Words to ... ####\n\n\nword = \"diagnostic\"\nprint(\"Most similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=10))\nprint(\"\\n\")\nword = \"surveillance\"\nprint(\"Most similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=10))\nprint(\"\\n\")\nword = \"origin\"\nprint(\"Most similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=20))\nprint(\"\\n\")\nword = \"transmission\"\nprint(\"Most similar words to {}:\".format(word))\nprint(word_vectors.most_similar(positive=word, topn=20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For instance, it is interesting to understand at a glance that the word ***\"origin\"*** is mainly linked to terms including the bigram ***\"interspecies_transmission\"***, and words such as ***\"pangolin\"***, ***\"evolution\"***, and ***\"zoonotic\"*** (FYI, A zoonosis is an infectious disease caused by a pathogen that has jumped from non-human animals to humans). <br><br>\n\nAlso, regarding what is known about transmission, it is worth having a look at ***\"transmission\"***'s most similar words, which include insightful words such as ***\"travel\"***, ***\"contact\"***, and ***\"fomite\"*** (FYI, A fomite is any inanimate object that, when contaminated with or exposed to infectious agents, can transfer disease to a new host)."},{"metadata":{},"cell_type":"markdown","source":"# **Part 6: T-SNE Visualization of Semantic Clusters**"},{"metadata":{},"cell_type":"markdown","source":"The **T-distributed Stochastic Neighbor Embedding (t-SNE)** dimensionality reduction technique was ultimately applied to project the 2D position of each word with its label. A machine learning **Kmean** algorithm was also implemented using *Scikit-learn* Python Library to partition n words into semantic clusters. To determine the optimal number of clusters K, the **elbow method** was used with below the plot of sum of squared distances for K in the range [1, 30]. If the plot looks like an arm, then the elbow on the arm is the optimal K. Here, **K =7**."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Word2Vec_Sorted(model):\n    ''' \n    Function to extract the word2vec embeddings \n    of the most frequent terms in the corpus\n    '''\n    stopwords.extend([\"also\", \"however\", \"could\"])\n    w2c = dict()  \n    \n    for item in model.wv.vocab:\n        w2c[item]=model.wv.vocab[item].count\n    w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n    w2cSortedList = list(w2cSorted.keys())\n    w2cSortedList = [word for word in w2cSortedList if word not in stopwords]\n    \n    return w2cSortedList\n\n\n#### Implementation of the elbow method to find the optimal number of clusters K ####\n\nSum_of_squared_distances = []\ntokens = []\n\nfor word in Word2Vec_Sorted(model):\n    tokens.append(model[word])\n\ntsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500)\nnew_values1 = tsne_model.fit_transform(tokens)\n\nK = range(1,30)\n\nfor k in tqdm(K):\n    km = KMeans(n_clusters=k)\n    km = km.fit(new_values1)\n    Sum_of_squared_distances.append(km.inertia_)\n    \n#### Plot the \"elbow\" curve ####\nplt.subplots(figsize = (10,6))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal Number of Clusters K')\nplt.savefig(\"Elbow_Method_Optimal_K.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(model, key_words):\n\n    \"Creates a TSNE model and plots it\"\n    labels = []\n    tokens = []\n    \n    #### For visualisation and clarity purposes, we only project the 300 most frequent words ####\n    \n    for word in Word2Vec_Sorted(model)[:300]:\n        tokens.append(model[word])\n        labels.append(word)\n\n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500)\n    new_values = tsne_model.fit_transform(tokens)\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n\n    clusters = KMeans(n_clusters=7)\n    clusters.fit(new_values)\n    y_kmeans = clusters.predict(new_values)\n    \n    colmap = {0: 'red', 1: 'green', 2: 'blue', 3 :'black', 4:'fuchsia', 5:'orange', 6:'grey', 7:'grey'}\n\n    dict1={}\n    for i in range(len(colmap)):\n        dict1[colmap[i]]=list(y_kmeans).count(i)/len(y_kmeans)*100\n    \n    plt.figure(figsize=(20,15))\n\n    plt.title('TSNE Model Vizualization \\n', fontsize = 20, fontweight = \"bold\")\n    plt.xlabel('Dimension 1')\n    plt.ylabel('Dimension 2')\n\n    for i in range(len(new_values)):\n        plt.scatter(x[i], y[i], color=colmap[y_kmeans[i]], s=12)\n        if labels[i] not in key_words:\n            plt.annotate(labels[i],\n                         xy=(x[i], y[i]),\n                         xytext=(5, 2),\n                         textcoords='offset points',\n                         ha='right',\n                         va='bottom', fontsize=12)\n        else:\n            \n            plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom', fontsize = 12, weight=\"bold\", color= 'red')\n    \n \n    plt.savefig(\"tsne_covid.png\")\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time()\n\n#### TSNE Data Visualization ####\n\nkey_words = ['origin', 'transmission', 'symptom', 'surveillance', 'diagnostic', 'patient', 'cause', 'outbreak', \"vaccine\"]\n\ntsne_plot(model, key_words)\n\nprint('Time to process data: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above TSNE visualization of Word2Vec embeddings, we can distinguish several clusters among which we can recognize semantic similarities including for instance, *medical treatment,  vaccine research, epidemiological research, covid-19 detection, transmission, causes and consequences of the disease*... Inter-word distance in the 2D plane is an indication of inter-word similarity."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}