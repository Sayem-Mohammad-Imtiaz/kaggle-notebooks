{"nbformat_minor":1,"nbformat":4,"cells":[{"outputs":[],"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"66f26e2b-2cc9-4103-954f-767d615398bd","_uuid":"197b151ae9b22172bc4033744a8d275ec8b70f96"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Let's look at the data first\ndata = pd.read_csv('../input/tsa_claims.csv')\nprint(\"Number of samples in the data  : \", data.shape[0])\nprint(\"Columns in the dataset  : \", list(data.columns))\nprint(\" \")\ndata.head()","metadata":{"_cell_guid":"486c9c18-c473-4c79-b2b1-1b6b153cab68","_uuid":"b0bad53d78b0323676d5a1e85be2b8519af5bde1"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Let's have an overview of the dataset\ndata.info()","metadata":{"_cell_guid":"666c124c-f74f-4ca0-8940-c90b41a9686a","_uuid":"7e810837e3ccdde4dbe48f650763a597dd24493c"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Check for NaN or null values in the dataset\ndata.isnull().sum()","metadata":{"_cell_guid":"5ec81f6c-ee45-45bf-b6f2-b04206ef1ad2","_uuid":"f29deaf205be9d4d5d0cca2cb703f3b2851272c8"},"cell_type":"code"},{"source":"This dataset is a disaster in terms of quality. Too many null values along with some other things like - and ;","metadata":{"_cell_guid":"b68e9b7a-a2a4-4383-bfd6-0b9c11b1b95d","_uuid":"57f2f8b9c84874b09cc12c4c9db81b95e1c67309"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"# We will start with Airport code column first\ncodes = data['Airport Code'].value_counts()\nprint(\"Total number of unique airport codes : \", len(codes))\nprint(\"Maximun number of times an airport has been reported : \", codes.values.max())\nprint(\"Airport code where maximum number of incidents happened : \", codes.index[codes.values == codes.values.max()].tolist()[0])\nprint(\"Least number of incidents that has happened on any airport : \", codes.values.min())\nprint(\"Airport code where least number of incidents has happened: \", codes.index[codes.values == codes.values.min()].tolist()[0])\nprint(\"Average number of incidents that happened over the period of time : \", int(codes.values.mean()))","metadata":{"_cell_guid":"07d7cf63-1a9b-490e-bee4-b468d3471509","_uuid":"81658c2b44336b41eb781ab91412c78e75bb22de"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Get the names of the airports with minimum and maximum number of incidents happened\nprint(\"Airport name with maximum number of incidents : \", list(data['Airport Name'][data['Airport Code'] =='LAX'])[0])\nprint(\"Airport name with minimum number of incidents : \", list(data['Airport Name'][data['Airport Code'] =='ADK'])[0])","metadata":{"_cell_guid":"9ff6c5c0-5639-4bd9-a551-b4747573b63a","_uuid":"f7dbc7bbb3a1822e0527b64963959ec469675e96"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Let's move to the claim type column\nunique_claim_type = data['Claim Type'].value_counts()\nprint(\"Total number of different claims : \", len(unique_claim_type))","metadata":{"_cell_guid":"cc1aa756-6b08-4af8-b4e0-72a8595d303c","_uuid":"913a8cfb2a5bc70c10745cf0fd684f65de9751f0"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Let's visualize these claims along with the numbers they have been reported\nclaim_index = unique_claim_type.index\nclaim_values = unique_claim_type.values\n\nplt.figure(figsize=(20,10))\nsns.barplot(y=claim_index, x=claim_values, orient='horizontal')\nplt.xlabel('Claim type', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.show()","metadata":{"_cell_guid":"72a3ba9f-c13c-4468-a4eb-8e40736b246d","_uuid":"b2a99d674abcb2b5ffb586692633a9c0287effd6"},"cell_type":"code"},{"source":"So, property loss is the most common type of thing that happens on the airport. I don't get it how but it seems that there are too many propoerty damage cases too. ","metadata":{"_cell_guid":"fd11d591-85d8-4455-8537-fd6e863001ad","_uuid":"8457b8553af1a8789922e6b158cb2bf4107a8c9c"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"# Let's move to the claim site\nunique_claim_sites = data['Claim Site'].value_counts()\nprint(\"Total number of unique claim sites : \", len(unique_claim_sites))","metadata":{"_cell_guid":"9e91f841-d651-464c-aa37-ae0da3858b04","_uuid":"6e7714f1cb32b63e0749a7b853170625792e1279"},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Let's visualize the actual number of instances for each class of claim site\nx = unique_claim_sites.index\ny = unique_claim_sites.values\n\nf = plt.figure(figsize=(20,10))\nsns.barplot(x, y)\nplt.xlabel('Claim site')\nplt.ylabel('Count')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{"_cell_guid":"d97baef8-2bf3-40e5-9f65-03f6c38972fc","_uuid":"684c3cced5272e3b5f14ee69d89cea51dabda9e4"},"cell_type":"code"},{"source":"So checked baggage and checkpoint are the sites where maximum incidents happen","metadata":{},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"# Let's move to the date columns for now. There are too many null values. Converting the column to datetime without\n# removing them may produce undesirable results. Will be happy to see some good\n# solution to it\ndata['Date Received'] = pd.to_datetime(data['Date Received'])\ndata['Received day'] = data['Date Received'].dt.weekday\ndata['Received month'] = data['Date Received'].dt.month","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Check the number of claims on different days\nclaims_count = data['Received day'].value_counts()\n\nplt.figure(figsize=(20,10))\nsns.barplot(claims_count.index, claims_count.values)\nplt.xlabel('Day of the week')\nplt.ylabel('Claims count')\nplt.xticks(range(7), ['Sun', 'Mon', 'Tues', 'Wed', 'Thu', 'Fri', 'Sat'])\nplt.yticks(fontsize=14)\nplt.show()","metadata":{},"cell_type":"code"},{"source":"so, maximum claims happens to be on Monday followed by Sunday and Tuesday. Why is it so? Remeber the number of claims regarding property loss? Maybe because people travel on weekens a lot.","metadata":{},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"claims_count = data['Received month'].value_counts()\nmonths = ['Jan', 'Feb', 'March', 'April', 'May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nplt.figure(figsize=(20,10))\nsns.barplot(claims_count.index, claims_count.values)\nplt.xlabel('Month')\nplt.ylabel('Claims count')\nplt.xticks(range(12), months, fontsize=14)\nplt.yticks(fontsize=12)\nplt.show()","metadata":{},"cell_type":"code"},{"source":"So January and August are the months where maximum number of claim reports are received\n\nThe date columns are so much messed up. There is so much of noise that I can't think of something good right now to filter them. Let's move to the next feature","metadata":{},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"def split_amount(x):\n    try:\n        if x is not None:\n            a = x.split('$')[1]\n            if ';' in a:\n                b,c = a.split(';')\n                return eval(b + c)\n            return eval(a)\n    except:\n        return 0","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"data['Claim Amount'] = data['Claim Amount'].apply(split_amount)\nprint(\"Maximum amount claimed : \", data['Claim Amount'].max())\nprint(\"Average amount claimed : \", data['Claim Amount'].mean())","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"data['Close Amount'] = data['Close Amount'].apply(split_amount)\nprint(\"Maximum closed amount  : \", data['Close Amount'].max())\nprint(\"Average closed amount  : \", data['Close Amount'].mean())","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"# Let's move to the disposition column finally\ndispos = data['Disposition'].value_counts()\n\nplt.figure(figsize=(10,5))\nsns.barplot(dispos.index, dispos.values)\nplt.xlabel('Disposition')\nplt.ylabel('Count')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","metadata":{},"cell_type":"code"},{"source":"That's it for now. The data contains too much of noise which requires some time to come up with some good techniques to filter that. Will get back to it later. Please upvote if you find this kernel useful.","metadata":{},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"source":"","metadata":{"collapsed":true},"cell_type":"code"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}}}