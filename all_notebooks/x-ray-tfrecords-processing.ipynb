{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Processing TFRecords - NIH Chest XRays\nHere I load in a series of TFRecords files, each containing a set of images from the XRays Dataset.\n\nAs there are multiple labels, this is a multilabel classification task.\n\nThe aim is to run a CNN to get labels for each possible condition/state."},{"metadata":{},"cell_type":"markdown","source":"# Loading Required Packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport IPython.display as display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom functools import partial\nimport sys\nfrom numpy import load\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nimport time as timer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Starting timer for Code Execution"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = timer.time()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Count of TFRecords\nThere are 256 files in total."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '/kaggle/input/nih-chest-xrays-tfrecords/'\n\nimage_dir = data_dir + 'data/'\n\ntfrlist_suffix = os.listdir(image_dir)\n\nprint('TFRecord file count: ' + str(len(tfrlist_suffix)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Viewing data distribution\nA CSV with metadata relating to the data is provided. The data has a series of True-False columns relating to conditions (or lack thereof).\n\nFrom the graphs, it is clear the data is highly imbalanced per condition, although the No Finding vs Finding overall is fairly balanced.\n\nSince the No Finding feature is an immediate consequence of the other features, I will treat this as the null hypothesis. Therefore, this feature in all modelling will be excluded."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(data_dir + 'preprocessed_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='No Finding', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heads = list(df.columns)[2:]\ncols = int(np.ceil(len(heads)/2))\n\n_, axs = plt.subplots(cols,2, figsize=(15, 30))\n\nfor i, _ in enumerate(heads):\n    if i % 2 == 0:\n        sns.countplot(x=heads[i], data=df, ax=axs[int(i/2),0])\n    else:\n        sns.countplot(x=heads[i], data=df, ax=axs[int((i-1)/2),1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grabbing TFRecords\nThe list of TFRecords is stored in a TF glob object."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfrlist = [image_dir + x for x in tfrlist_suffix]\n\nFILENAMES = tf.io.gfile.glob(tfrlist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating indexes for test/valid/train\nI split all the files into train, validation and test sets by random sampling.\nI first randomly sample the entire list to a 80-20% split, then set aside 10% of the train sets randomly as a validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"ALL = list(range(len(FILENAMES)))\n\nTRAIN_AND_VALID_INDEX = random.sample(ALL, int(len(ALL) * 0.8))\nTEST_INDEX = list(set(ALL) - set(TRAIN_AND_VALID_INDEX))\n\nTRAIN_INDEX = random.sample(TRAIN_AND_VALID_INDEX, int(len(TRAIN_AND_VALID_INDEX) * 0.9))\nVALID_INDEX = list(set(TRAIN_AND_VALID_INDEX) - set(TRAIN_INDEX))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting lists for Training, Validation and Test\nI use the above indices to split the entire list of files into the respective categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING_FILENAMES, VALID_FILENAMES, TEST_FILENAMES = [FILENAMES[index] for index in TRAIN_INDEX], [FILENAMES[index] for index in VALID_INDEX], [FILENAMES[index] for index in TEST_INDEX]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Entire count of samples\nThis indicates the total number of files (256) split into each bucket."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\nprint(\"Validation TFRecord Files:\", len(VALID_FILENAMES))\nprint(\"Test TFRecord Files:\", len(TEST_FILENAMES))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature space for parsing\nI use the column headers from the dataframe to parse a feature dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_description = {}\n\nfor elem in list(df.columns)[2:]:\n    feature_description[elem] = tf.io.FixedLenFeature([], tf.int64)\n    \nfeature_description['image'] = tf.io.FixedLenFeature([], tf.string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting parameters\nI use a batch size of 64, to factor for the imbalanced dataset.\n\nI also set the image size to 100 x 100 to limit memory consumption per iteration (9 hour limit on Kaggle)."},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64\nIMAGE_ONE_AXIS = 100\nIMAGE_SIZE = [IMAGE_ONE_AXIS, IMAGE_ONE_AXIS]\nAUTOTUNE = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function for reading the file\nThis function uses the above defined feature description to decode the image and its label. With the loop, I extract the feature's labels as a one-hot encoded list.\n\nNo Findings would be a zero vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_tfrecord(example):\n    example = tf.io.parse_single_example(example, feature_description)\n    image = tf.io.decode_jpeg(example[\"image\"], channels=3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    label = []\n    \n    for val in heads: label.append(example[val])\n    \n    return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data\nThe TFRecordDataset weaves together the individiual TFRecords, essentially treating them as one dataset. Randomness is introduced (deterministic = False) as no real order exists in the images. This speeds up loads."},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(filenames):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_tfrecord)\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Batch and shuffle\nThis step takes the above loaded data, shuffles it with $N = 2048$, and defines the batch for feeding into the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dataset(filenames):\n    dataset = load_dataset(filenames)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating the data\nEach of the train, validation and test datasets are now put into Batch objects. These objects can be fed directly into a CNN, without any further manipulation needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = get_dataset(TRAINING_FILENAMES)\nvalid_dataset = get_dataset(VALID_FILENAMES)\ntest_dataset = get_dataset(TEST_FILENAMES)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising some examples\nThis step is just to get some view on the data. The label is a translation of the one-hot encoded labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"image_viz, label_viz = next(iter(train_dataset))\n\ndef show_batch(X, Y):\n    plt.figure(figsize=(20, 20))\n    for n in range(25):\n        ax = plt.subplot(5, 5, n + 1)\n        plt.imshow(X[n])\n        \n        result = [x for i, x in enumerate(heads) if Y[n][i]]\n        title = \"+\".join(result)\n        \n        if result == []: title = \"No Finding\"\n        \n        plt.title(title)\n        plt.axis(\"off\")\n\nshow_batch(image_viz.numpy(), label_viz.numpy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining learning rate + early stop parameters\nThis enables the learning rate to be adjusted through epochs. It also enables the model to stop if within 10 epochs, the weights do not change within a threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_learning_rate = 0.01\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=5, decay_rate=0.96, staircase=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define CNN model\nThe CNN is defined similar to an ImageNet model, as defined in the article: [How to Use The Pre-Trained VGG Model to Classify Objects in Photographs](https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model(in_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3), out_shape=len(heads)):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(out_shape, activation='sigmoid'))\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n                  loss='binary_crossentropy',\n                  metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating steps_per_epoch and validation_steps\nThese quantities are defined most straightforwardly as the number of images in the dataset divided by the batch size, i.e. $ \\textrm{steps for sample} = \\lceil \\frac{\\textrm{sample size}}{\\textrm{batch size}} \\rceil $.\n\nHowever, the sizes are not known, so I calculate them here."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = sum(1 for _ in tf.data.TFRecordDataset(TRAINING_FILENAMES))\nvalidation_size = sum(1 for _ in tf.data.TFRecordDataset(VALID_FILENAMES))\n\nepoch_steps = int(np.ceil(train_size/BATCH_SIZE))\nvalidation_steps = int(np.ceil(validation_size/BATCH_SIZE))\n\nepochs = 10\n\nprint(\"steps_per_epoch: \" + str(epoch_steps))\nprint(\"validation_steps: \" + str(validation_steps))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running the model\nDue to a glitch, I could only set validation_steps and not steps_per_epoch (it kept stopping at epoch 2!).\n\nIt just means in epoch 1, it shows /Unknown. However it is known from the frame above how many steps are to be taken."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = define_model()\n\nhistory = model.fit(\n    train_dataset,\n    epochs=epochs,\n    validation_data=valid_dataset,\n    validation_steps = validation_steps\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, test_auc = model.evaluate(test_dataset, verbose=0)\n\nprint('Test auc:', test_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot model loss and AUC by epochs\nThis gives an idea of model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot loss\nax = plt.subplot(211)\nplt.title('Cross Entropy Loss')\nplt.plot(history.history['loss'], color='blue', label='train')\nplt.plot(history.history['val_loss'], color='orange', label='validation')\nax.axes.xaxis.set_visible(False)\n\n# plot accuracy\nplt.subplot(212)\nplt.title('AUC')\nplt.plot(history.history['auc'], color='blue', label='train')\nplt.plot(history.history['val_auc'], color='orange', label='validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using model to predict on Test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_model = model.predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualising predictions\nThe predictions are plotted here. The output is an array with values in $[0,1]$.\n\nI have also added a \"My interpretation\" element, which is where I reverse the encoding by saying\n\n$y_i =\n\\left\\{\n\t\\begin{array}{ll}\n\t\t1  & \\mbox{if } \\textrm{Pr}(x_i) > 0.5 \\\\\n\t\t0 & \\mbox{if } \\textrm{Pr}(x_i) \\leq 0\n\t\\end{array}\n\\right.$\n\nwhere $y_i$ is the $i$th assigned feature prediction and $Pr(x_i)$ is the probability if the $i$th feature prediction from the CNN.\n\nIf the resultant vector is the 0 vector, then this is No Finding.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_viz, label_viz = next(iter(test_dataset))\n\ndef show_batch(X, Y_act):\n    plt.figure(figsize=(25, 30))\n    for n in range(9):\n        \n        ax = plt.subplot(3, 3, n + 1)\n        ax = plt.imshow(X[n])\n        \n        result = [x for i, x in enumerate(heads) if Y_act[n][i]]\n        \n        title = \"+\".join(result)\n        \n        if result == []: title = \"No Finding\"\n        \n        title = \"Actual:\\n\" + title\n        \n        title += \"\\n\\n Prediction:\\n\" + str(fitted_model[n]) + \"\\n\\n My interpretation:\\n\"\n        \n        threshold = 0.5\n        \n        result = []\n        for i, _ in enumerate(heads):\n            if fitted_model[n][i] > threshold:\n                result.append(1)\n            else:\n                result.append(0)\n        \n        result = np.asarray(result)\n\n        if np.linalg.norm(result) == 0:\n            title += \"No Finding\"\n        else:\n            result = [x for i, x in enumerate(heads) if result[i]]\n            additional_title = \"+\".join(result)\n            title += additional_title\n            \n        plt.title(title)\n        plt.axis(\"off\")\n\nshow_batch(image_viz.numpy(), label_viz.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_time = timer.time()\n\ntime = end_time - start_time\n\nday = time // (24 * 3600)\ntime = time % (24 * 3600)\nhour = time // 3600\ntime %= 3600\nminutes = time // 60\ntime %= 60\nseconds = np.round(time,0)\nprint(f\"Total code execution time: {day} days, {hour} hours, {minutes} minutes, {seconds} seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Future work\n*  Checking the effect of the size of the image on AUC.\n* Checking if other metrics (e.g. Fbeta) are better metrics.\n* Check whether applying Learning rate ($\\alpha$) decay improves the result.\n* Adjust the decay steps and decay rate to see influence.\n* Check if different learning scheduler for $\\alpha$ improves the result.\n* Apply transformations to enhance the training set (e.g. rotating and zooming/cropping images).\n* See if Transfer learning is beneficial for this use case.\n* See if there is any correlation between conditions present in dataset."},{"metadata":{},"cell_type":"markdown","source":"# References\n1. [How to train a Keras model on TFRecord files](https://keras.io/examples/keras_recipes/tfrecord/)\n2. [Basics of image classification with Keras](http://https://towardsdatascience.com/basics-of-image-classification-with-keras-43779a299c8b)\n3. [Multi-Label Classification of Satellite Photos of the Amazon Rainforest](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}