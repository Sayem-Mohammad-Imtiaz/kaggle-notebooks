{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndirname = 'C:/Users/mayan/Documents/Credit Card Dataset/'\nfilename = 'UCI_Credit_Card.csv'\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing relevant libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = pd.read_csv('../input/UCI_Credit_Card.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us first check for null values\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fortunately this dataset does not contain null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us study the dataset a little bit\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now describe the dataset\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per the above Dataset we can see most people in this dataset are female since the mean of sex is close to 2 which corresponds to female.\nAverage Limit Balance is 167484.322667 dollars with a standard deviation of 129747.661567 dollars. We shall find out the distribution later on.\nMost individuals in the dataset are either single or other category since the mean is more than 1.5 also. \n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# EDA Time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Limit Balance histogram\ndf['LIMIT_BAL'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot of Income \nsns.boxplot(x='LIMIT_BAL', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see most people have Limit Balance skewed to the right. with some outliers which we shall remove.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countplot of gender\nsns.countplot(x='SEX', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Marital Status-wise countplot\nsns.countplot(x='MARRIAGE', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['MARRIAGE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our dataset legend does not have any values in the legend for marriage in 0 category. So let us club values with 0 into others i.e 3.\ndf.loc[df['MARRIAGE']==0,'MARRIAGE']=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lET US CHECK IF OUR TRANSFORMATION WORKED\ndf['MARRIAGE'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us see the distribution of Age\ndf['AGE'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus we can see most people in the dataset are under 45 with some outliers above 62 or so. We shall handle them too while handling outliers before preparing our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot of Age column\nsns.boxplot(y='AGE', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us see the age distribution with respect to sex\nsns.boxplot(x='SEX',y='AGE', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above example we can clearly see the distribution of age with respected to each sex.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now check the payment statuses of each month\nsns.countplot(x='PAY_0', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our legend has no description for -2 and 0 in this. Let us now remove them and replace by -1 assuming that those customer paid their dues duly.\ndf.loc[df['PAY_0']==-2,\"PAY_0\"]=-1\ndf.loc[df['PAY_0']==0,\"PAY_0\"]=-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now check the dataset for the values\nsns.countplot(x='PAY_0', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us move to the next month\nsns.countplot(x='PAY_2', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our legend has no description for -2 and 0 in this. Let us now remove them and replace by -1 assuming that those customer paid their dues duly.\ndf.loc[df['PAY_2']==-2,\"PAY_2\"]=-1\ndf.loc[df['PAY_2']==0,\"PAY_2\"]=-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_2', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us do the same for the other such PAY_ columns\nsns.countplot(x='PAY_3', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our legend has no description for -2 and 0 in this. Let us now remove them and replace by -1 assuming that those customer paid their dues duly.\ndf.loc[df['PAY_3']==-2,\"PAY_3\"]=-1\ndf.loc[df['PAY_3']==0,\"PAY_3\"]=-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_3', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_4', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our legend has no description for -2 and 0 in this. Let us now remove them and replace by -1 assuming that those customer paid their dues duly.\ndf.loc[df['PAY_4']==-2,\"PAY_4\"]=-1\ndf.loc[df['PAY_4']==0,\"PAY_4\"]=-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_4', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_5', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our legend has no description for -2 and 0 in this. Let us now remove them and replace by -1 assuming that those customer paid their dues duly.\ndf.loc[df['PAY_5']==-2,\"PAY_5\"]=-1\ndf.loc[df['PAY_5']==0,\"PAY_5\"]=-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_5', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_6', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since our legend has no description for -2 and 0 in this. Let us now remove them and replace by -1 assuming that those customer paid their dues duly.\ndf.loc[df['PAY_6']==-2,\"PAY_6\"]=-1\ndf.loc[df['PAY_6']==0,\"PAY_6\"]=-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='PAY_6', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us do a bit of EDA With deeper insights to be derived\ndefault = df[df['default.payment.next.month']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram of age of defaulters\ndefault['AGE'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This seems a bit similar to the whole data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countplot of Age by defaulters\nsns.countplot(x='SEX', data=default)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most defaulters are women. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us now see the defaulters as per education qualifications\nsns.countplot(x='EDUCATION', data=default)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see graduate school people have less default than university people who have most default. Even high school people have less default. But that would be less informative as we would need to see the limit balance of the different categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot of different categories in terms of education\nsns.boxplot(x='EDUCATION', y='LIMIT_BAL', data=default)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the limit enjoyed by the defaulters in the graduate category are highest and that of high school are lower and thus a lower chance of default. But all these categories in he dataset have tons of outliers in terms of education and thus education might now have the best relation with limit balance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us see the number of defaulters with respect to education and their respective sex.\nsns.countplot(x='EDUCATION', hue='SEX', data=default)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that in most defaulting categories in each educational level the defaulters are women.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us see the whether more defaulters are married or single\nsns.countplot(x='MARRIAGE', data=default)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is just a slight difference in the single and married categories in terms of being defaulters in this particular sample represented by the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Removing the outliers now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nz = np.abs(stats.zscore(df))\nprint(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"threshold = 3\nprint(np.where(z>3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfout = df[(z<3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfout.to_pickle('dfpreprocessed.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us check if the dataset is balanced or not\nsns.countplot(x='default.payment.next.month', data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since the dataset is heavily imbalanced we will have to balance it first in order to train a model on the dataset\nfrom imblearn.combine import SMOTETomek","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfout.drop('default.payment.next.month', axis=1, inplace=False)\ny = dfout['default.payment.next.month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampler = SMOTETomek(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xout, yout = sampler.fit_resample(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yout.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have a balanced dataset with 19736 points each side. Let us now shuffle the dataset and create training, validation and testing sets.\nXout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us create dummies on the relevant columns i.e. SEX, MARRIAGE, EDUCATION.\nsex = pd.get_dummies(Xout['SEX'], prefix='SEX', drop_first=True)\nedu = pd.get_dummies(Xout['EDUCATION'], prefix='EDUCATION', drop_first=True)\nmarriage = pd.get_dummies(Xout['MARRIAGE'], prefix='MARRIAGE', drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xout = pd.concat([Xout, sex, edu, marriage], axis=1)\nXout.drop(['SEX', 'EDUCATION', 'MARRIAGE'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xout.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordercol = ['ID', 'LIMIT_BAL','SEX_2','EDUCATION_1',\n       'EDUCATION_2', 'EDUCATION_3', 'EDUCATION_4','MARRIAGE_2',\n       'MARRIAGE_3', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5',\n       'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n       'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n       'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6'   ]\nXout = Xout[ordercol]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us scale the given data before proceeding to ML Training\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nXscaled = scaler.fit_transform(Xout)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xscaled= pd.DataFrame(Xscaled, columns=Xout.columns)\nXscaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing train test split\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(Xscaled, yout, train_size=0.9, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2, X_valid, y_train2, y_valid = train_test_split(X_train, y_train, train_size=(8/9), random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we don't need ID Column in our classifier. Let us drop it from all our X columns.\nX_train2.drop('ID', axis=1, inplace=True)\nX_valid.drop('ID', axis=1, inplace=True)\nX_test.drop('ID', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let us start Training the model on different classifier starting from basic to deep neural nets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(X_train2, y_train2)\nmodel1 = reg.fit(X_train2, y_train2)\n# Now let us evaluate the model\nreg.score(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It has a training accuracy of 76.14 %. Let us check the validation and testing accuracy\nreg.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The accuracy of the model seems consistent but it seems low","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# K Neighbors Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us try K-Nearest Neighbours Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(5)\nmodel2 = knn.fit(X_train2, y_train2)\nmodel2.score(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model has an accuracy of 86% in training and 79-80% in testing and validation, which thought better than logistic regression is still not as good as it could be.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(100)\nrfc.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model using Random Forests heavily overfits with 99% train accuracy and 85-86% Validation and Testing Accuracy. So we would need to still look for other options.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Gaussian Naive Bayes Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.score(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training accuracy itself of the gaussian naive bayes classifier is very poor.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier, XGBRFClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\nxgb.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.score(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scores are quite good for a non-Neural Network Classifier. Let us check out XGBRFC now","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# XGBRFClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbrfc = XGBRFClassifier()\nxgbrfc.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbrfc.score(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This has a lower training accuracy itself so there wouldn't be much point checking the validation and testing accuracies.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier()\ngbm.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm.score(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here too the training accuracy was not as great. So till now the XGBClassifier seems most consistent and best model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Basic Deep Neural Network for Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let us try out neural networks\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\ninput_size = 27\n\nmodel.add(Dense(input_size, activation='relu'))\nmodel.add(Dense(27, activation='relu'))\n\nmodel.add(Dense(14, activation='relu'))\nmodel.add(Dense(7, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 10\nmax_epochs = 1000\nearly =EarlyStopping(patience=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train2, y_train2, batch_size=batch_size, epochs=max_epochs, callbacks=[early], validation_data=(X_valid, y_valid), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predmodel = model.predict_classes(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train2, predmodel))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predvalid = model.predict_classes(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_valid, predvalid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predtest = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predtest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so the xgboost model seems best till now in terms of accuracy. Lets check its classification report and confusion matrix as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predtrainxgb = xgb.predict(X_train2)\npredvalxgb = xgb.predict(X_valid)\npredtestxgb = xgb.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predtestxgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_valid, predvalxgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train2, predtrainxgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking rfc\npredtrainrfc = rfc.predict(X_train2)\npredvalrfc = rfc.predict(X_valid)\npredtestrfc = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_train2, predtrainrfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 100 % train accuracy seems rather wrong. but let's check validation and test\nprint(classification_report(y_valid, predvalrfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predtestrfc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though Random Forest fits the training data perfectly and provides an f1-score and accuracy both similar to xgboost classifier, but the XGBClassifier is more consistent with less overfitting being observed and hence can be considered here as the best available model in this scenario.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}