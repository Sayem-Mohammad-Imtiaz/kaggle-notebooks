{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Trying Different Models on the Fake News Data"},{"metadata":{},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# load the first dataset\nnews_dataset = pd.read_csv(\"../input/fake-news-dataset/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset['class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be a single wrong value in the class column"},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset[news_dataset['class'] == 'February 5, 2017']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset['Unnamed: 6'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The record seems to have been shifted to the right due to the id value being repeated at the beginning."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# shifting the column values in the respective places\nnews_dataset.iloc[504, 2] = news_dataset.iloc[504, 3]\nnews_dataset.iloc[504, 3] = news_dataset.iloc[504, 4]\nnews_dataset.iloc[504, 4] = news_dataset.iloc[504, 5]\nnews_dataset.iloc[504, 5] = news_dataset.iloc[504, 6]\nnews_dataset.iloc[504, 6] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset.iloc[504]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset.drop(columns=['index', 'Unnamed: 6'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the fixed dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset.to_csv('news_dataset.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sanity check\nnews_dataset = pd.read_csv('news_dataset.csv')\nnews_dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"news_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the Data for Machine Learning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# splitting the dataset into training and test sets\nfeatures, target = news_dataset[['title', 'text']], (news_dataset['class'] == 'Fake').astype(int)\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, stratify=target, random_state=42)\nx_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nx_test.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape, y_train.shape, x_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.iloc[0]['title'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardize the title and text by removing punctuation, links, mentions and converting to lowercase\ndef standardize_text(x):\n    \"\"\"\n    x: a Pandas Series of strings (texts)\n    \"\"\"\n    \n    x = x.str.replace(r\"http\\S+\", \" \")\n    x = x.str.replace(r\"http\", \" \")\n    x = x.str.replace(r\"[^A-Za-z0-9()\\ ]\", \" \")\n    x = x.str.replace(r\"@\", \"at\")\n    x = x.str.replace(r\"@\\S+\", \"\")\n    x = x.str.replace(r\"\\s+\", \" \")\n    x = x.str.lower()\n    return x\n\nprint(standardize_text(x_train.iloc[0:1]['title']).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\nenglish_stopwords = stopwords.words('english')\n\ndef remove_stopwords(x):\n    \"\"\"\n    x: a Pandas Series of strings (texts)\n    \"\"\"\n    \n    word_tokenizer = RegexpTokenizer(r\"\\w+\")\n    x = x.apply(lambda x: \" \".join([t for t in word_tokenizer.tokenize(x) if t not in english_stopwords]))\n    return x\n\nprint(remove_stopwords(standardize_text(x_train.iloc[0:1]['title'])).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nclass Vocab(object):\n    def __init__(self, token_series, vocab_size=None, special=[]):\n        \"\"\"\n        token_series: a Pandas Series of token lists\n\n        vocab_size is the maximum length of the vocabulary (before adding the special tokens)\n        \"\"\"\n        vocabulary = Counter()\n        for x in token_series:\n            vocabulary.update(x)\n\n        # truncate vocabulary\n        if vocab_size is None:\n            vocab_size = len(vocabulary)\n        vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]    \n\n        # add special tokens in the keys\n        vocabulary = ['<unk>', '<pad>'] + special + vocabulary\n        self.unk = 0\n        self.pad = 1\n        self.idx_to_token = vocabulary\n        self.token_to_idx = {tok: i for i, tok in enumerate(vocabulary)}\n    \n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[i] for i in indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.base import BaseEstimator\n    \nclass Tokenizer(BaseEstimator):\n    def __init__(self, preprocess=None, vocab_size=None, special=[]):\n        self.preprocess = lambda x: preprocess(pd.Series(x))\n        self.vocab_size = vocab_size\n        self.special = special\n        self.word_tokenizer = RegexpTokenizer(r\"\\w+\")\n    \n    def fit(self, X, y=None):\n        \"\"\"\n        X: a Pandas Series of strings\n        \"\"\"\n        \n        X = self.preprocess(X)\n        X = X.apply(self.word_tokenizer.tokenize)\n        self.vocab = Vocab(X, self.vocab_size, self.special)\n        return self\n    \n    def transform(self, X):\n        \"\"\"\n        Converts a Series of strings to a numpy array of lists of indices\n        \n        X: a Pandas Series of strings\n        \"\"\"\n        \n        X = self.preprocess(X)\n        X = X.apply(self.word_tokenizer.tokenize)\n        return np.array([self.vocab[x] for x in X])\n    \n    def fit_transform(self, X, y=None):\n        X = self.preprocess(X)\n        X = X.apply(self.word_tokenizer.tokenize)\n        self.vocab = Vocab(X, self.vocab_size, self.special)\n        return np.array([self.vocab[x] for x in X])\n    \n    def to_tokens(self, indices):\n        \"\"\"\n        Converts a numpy array of lists of indices to a numpy array of tokens\n        \"\"\"\n        \n        return np.array([nostop_tokenizer.vocab.to_tokens(list(x)) for x in example_indices])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nostop_tokenizer = Tokenizer(lambda x: remove_stopwords(standardize_text(x)), vocab_size=10000)\nnostop_tokenizer.fit(pd.concat([x_train['title'], x_train['text'][:5000]]))  # TODO: use all the text data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_indices = nostop_tokenizer.transform(x_train.iloc[0:2]['title'])\nprint(example_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for sent in (' '.join(x) for x in nostop_tokenizer.to_tokens(example_indices)):\n    print(sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: create a vocabulary and tokenizer for each of these cases:\n#  1. with stopwords [done]\n#  2. without stopwords\n#  3. with special tokens such as <UNK>, <PAD>, <SOS>, <EOS> for deep learning models\n\n# TODO: for deep learning models:\n#  batch the text by using the <PAD> tokens\n#  use word embeddings (pretrained as well as random)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating Different Machine Learning Models"},{"metadata":{},"cell_type":"markdown","source":"### Bag of Words + Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# TODO: create custom count vectorizer","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}