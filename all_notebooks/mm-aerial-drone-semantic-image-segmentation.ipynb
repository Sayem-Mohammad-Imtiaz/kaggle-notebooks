{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\n<center><h2> On How to train A neural network for image Segmentation using Fast.ai and Transfer Learning</h2></center>\n\n\n***\n\n<center><img src=\"https://github.com/shadab4150/Aerial_drone_image_segmentation/raw/master/image_drone/drone1.png\"></center>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><h3> Please Upvote if you like it. </h3></center>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What is semantic segmentation ?\n\n* Source: **https://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html**\n\n* **Semantic image segmentation is the task of classifying each pixel in an image from a predefined set of classes.**\n\n***\n\nIn the following example, different entities are classified.\n\n![kd](https://divamgupta.com/assets/images/posts/imgseg/image15.png?style=centerme)\n\n***\n\n\nIn the above example, the pixels belonging to the bed are classified in the class “bed”, the pixels corresponding to the walls are labeled as “wall”, etc.\n\nIn particular, our goal is to take an image of size W x H x 3 and generate a W x H matrix containing the predicted class ID’s corresponding to all the pixels.\n\n***\n![kd](https://divamgupta.com/assets/images/posts/imgseg/image14.png?style=centerme)\n\n***\n\nUsually, in an image with various entities, we want to know which pixel belongs to which entity, For example in an outdoor image, we can segment the sky, ground, trees, people, etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing useful libraries","execution_count":null},{"metadata":{"id":"sSFJKc7j69gg","outputId":"fa09cd37-d8a1-4458-878d-66f2bc3bc242","trusted":true},"cell_type":"code","source":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.callbacks import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.image as immg\nimport gc\nimport numpy as np\nimport random\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"open_image('../input/semantic-drone-dataset/semantic_drone_dataset/original_images/001.jpg').data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Path to the dataset\n* Images were too big from original dataset So, resized them in 2 size\n* > 1800x1200\n* > 600x400","execution_count":null},{"metadata":{"id":"NCHO_oKMoq8x","trusted":true},"cell_type":"code","source":"path = Path('../input/drone-images-mask-resized/drone_data_small')  ","execution_count":null,"outputs":[]},{"metadata":{"id":"JaN_NRVfoy-5","outputId":"83e18bbf-ae2b-496e-8fb2-5f0e63c47fd8","trusted":true},"cell_type":"code","source":"path.ls()","execution_count":null,"outputs":[]},{"metadata":{"id":"vUgB2LHlE-b3","trusted":true},"cell_type":"code","source":"fnames = get_files(path/'train_small')\nfnames_mask = get_files(path/'label_small')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"open_image(fnames[4]).data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data PreProcessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm,tnrange","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Since each pixel belongs to a diffrent class below function counts total number of such classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path_im = path/'train_small'\npath_lb = path/'label_small'\nget_y_fns = lambda x: path_lb/f'{x.stem}.png'       # Function to get masks for a image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fnames[30],get_y_fns(fnames[30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_classes(fnames):\n    class_codes=[]\n    for i in tqdm(range(400)):\n        class_codes += list(np.unique(np.asarray(Image.open(get_y_fns(fnames[i])))))\n    return np.array(list(set(class_codes)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run this once to get total classes if you want, other wise below cell gives total classes\ncodes = get_classes(fnames)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"codes = np.array(codes)\ncodes","execution_count":null,"outputs":[]},{"metadata":{"id":"vLde9-4lww-t","trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"id":"jACqgzJ-rKjz"},"cell_type":"markdown","source":"## Function to show Drone with Mask","execution_count":null},{"metadata":{"id":"Ff_KqyJVpjHA","trusted":true},"cell_type":"code","source":"def drone_mask(f):  # f = file_name\n  img_a = immg.imread(f)\n  img_a_mask = immg.imread(get_y_fns(f))\n  plt.figure(1,figsize=(20,8))\n  plt.subplot(121)\n  plt.imshow(img_a);plt.title('Raw Drone footage ');plt.axis('off')\n  plt.subplot(122)\n  plt.imshow(img_a,alpha=0.8);\n  plt.imshow(img_a_mask,alpha=0.8);plt.title('Drone with  mask');plt.axis('off')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A sample Drone with Mask","execution_count":null},{"metadata":{"id":"19s0Lf2qpjER","outputId":"938fa766-5bf6-4c93-a09c-43b6cb3b056d","trusted":true},"cell_type":"code","source":"for i in range(3):\n    img_num = random.randint(10,200)\n    drone_mask(fnames[img_num])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating A DatabLock for the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"src=np.array([400,600])\n#src=src//2\nsrc","execution_count":null,"outputs":[]},{"metadata":{"id":"FiDbvuFK7Iqv","trusted":true},"cell_type":"code","source":"data = (SegmentationItemList.from_folder(path=path_im)  # Location from path\n        .split_by_rand_pct(0.2)                          # Split for train and validation set\n        .label_from_func(get_y_fns, classes=codes)      # Label from a above defined function\n        .transform(get_transforms(), size=src//2, tfm_y=True)   # If you want to apply any image Transform\n        .databunch(bs=4)                                   # Batch size  please decrese batch size if cuda out of memory\n        .normalize(imagenet_stats))            # Normalise with imagenet stats","execution_count":null,"outputs":[]},{"metadata":{"id":"vDKbpsk77Ivv","outputId":"e6717aeb-2745-4060-db54-bbfcc8aca3df","trusted":true},"cell_type":"code","source":"data.show_batch(rows=2,figsize=(20,10));","execution_count":null,"outputs":[]},{"metadata":{"id":"Y5GGCGMOJahV","outputId":"44974e11-8fd0-4b05-de54-60ba14645a05","trusted":true},"cell_type":"code","source":"len(data.train_ds), len(data.valid_ds), data.c  ","execution_count":null,"outputs":[]},{"metadata":{"id":"OkBXglti_Evr"},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"id":"2THHYXWL_Gt0"},"cell_type":"markdown","source":"* **Metrics for Drone mask**\n","execution_count":null},{"metadata":{"id":"jQnVSMSC7j0o","trusted":true},"cell_type":"code","source":"name2id = {v:k for k,v in enumerate(codes)}\nvoid_code = -1\n\ndef drone_accuracy_mask(input, target):\n    target = target.squeeze(1)\n    mask = target != void_code\n    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"3PzyU3wd7jvA","trusted":true},"cell_type":"code","source":"metrics = drone_accuracy_mask\nwd=1e-2    # wd = weight decay","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fastai's unet_learner\n* Source [**Fast.ai**](www.fast.ai)\n\n* This module builds a dynamic U-Net from any backbone **pretrained on ImageNet**, automatically inferring the intermediate sizes.\n\n![kd](https://www.researchgate.net/profile/Alan_Jackson9/publication/323597886/figure/fig2/AS:601386504957959@1520393124691/Convolutional-neural-network-CNN-architecture-based-on-UNET-Ronneberger-et-al.png)\n\n* **This is the original U-Net. The difference here is that the left part is a pretrained model.**\n\n* **This U-Net will sit on top of an encoder ( that can be a pretrained model -- eg. resnet50 ) and with a final output of num_classes.**","execution_count":null},{"metadata":{"id":"nvecjSCE7I0w","trusted":true},"cell_type":"code","source":"arch = models.resnet34\nlearn = unet_learner(data, # DatBunch\n                     arch, # Backbone pretrained arch\n                     metrics = [metrics], # metrics\n                     wd = wd, bottle=True, # weight decay\n                     model_dir = '/kaggle/working/') # model directory to save","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Summary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Finding a suitable learning rate for our model\n\n* With help fast.ai **learning rate finder** function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"8dc0ItTs9CiC","outputId":"fa4afcc1-b9a8-446d-a4a9-390ca670c3f3","trusted":true},"cell_type":"code","source":"gc.collect() # to clear the cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = SaveModelCallback(learn, monitor = 'drone_accuracy_mask', every = 'improvement', mode='max', name = 'best_model' )","execution_count":null,"outputs":[]},{"metadata":{"id":"YXpUCV20xgSX","trusted":true},"cell_type":"code","source":"lr = 1e-3           # Learning Rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit(10, lr,callbacks = [callbacks] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze()\nlearn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('best_model');\ncallbacks2 = SaveModelCallback(learn, monitor = 'drone_accuracy_mask', every = 'improvement', mode='max', name = 'best_model_ft' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(10,max_lr= slice(1e-5,1e-3/2),callbacks = [callbacks2] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results \n* Intial dynamic unet on top of an encoder ( resnet34 pretrained = 'imagenet' ), trained for 30 epochs gave an **accuracy** of **80.00%** .","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## To check results of our trained model","execution_count":null},{"metadata":{"id":"6dxZIut9wH4y","outputId":"b25d6c11-122f-450c-f625-5b021f43c61f","trusted":true},"cell_type":"code","source":"learn.show_results(rows = 4, figsize=(16,18))","execution_count":null,"outputs":[]},{"metadata":{"id":"x_KLV55nxQ82","trusted":true},"cell_type":"code","source":"learn.save('stage-1-big')  # saving the model ","execution_count":null,"outputs":[]},{"metadata":{"id":"8nDpfirUyIyY"},"cell_type":"markdown","source":"## Export the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.export('/kaggle/working/drone_mask.pkl')","execution_count":null,"outputs":[]},{"metadata":{"id":"95sTKf5A6xwi"},"cell_type":"markdown","source":"### Load the model  and predict","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Function to make a prediction and Overlap the Drone Images with Predicted Drone Mask**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def drone_predict(f):\n    img = open_image(f).resize((3,200,300))\n    mask = learn.predict(img)[0]\n    _,axs = plt.subplots(1,3, figsize=(24,10))\n    img.show(ax=axs[0], title='no mask')\n    img.show(ax=axs[1], y=mask, title='masked')\n    mask.show(ax=axs[2], title='mask only', alpha=1.)","execution_count":null,"outputs":[]},{"metadata":{"id":"PSdBvBIO7xRJ"},"cell_type":"markdown","source":"## Prediction","execution_count":null},{"metadata":{"id":"Yx6MUf4n1Cfz","outputId":"fc805ecf-548e-441d-afe2-b006f7967aea","trusted":true},"cell_type":"code","source":"for i in range(3):\n    n = random.randint(20,200)\n    drone_predict(fnames[n])","execution_count":null,"outputs":[]},{"metadata":{"id":"PZXq9e8cwlXN","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}