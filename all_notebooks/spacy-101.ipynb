{"cells":[{"metadata":{},"cell_type":"markdown","source":"# S p a C y  1 0 1\n\n```\nAll features described in SpaCy 101 Course with the News DataSet\n```"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport spacy\n\nprint(f'spacy = {spacy.__version__}')\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/repository/ZNClub-PA-ML-AI-Sentiment-analysis-using-Business-News-82d860a/data\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"\\\n../input/repository/ZNClub-PA-ML-AI-Sentiment-analysis-using-Business-News-82d860a/data/processed/normalized.csv\")\n\n#limit df\ndf = df[:5]\nrow_index = 1\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nclean_body = lambda x: re.sub(\"[\\n\\t\\r]\", \"\", x) if isinstance(x, str) else \"\"\n\ndf['body'] = df['body'].apply(clean_body)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Language Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.lang.en import English\nnlp = English()\ndf['Doc'] = df['body'].apply(nlp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NLP Model creates Document object"},{"metadata":{"trusted":true},"cell_type":"code","source":"row_index = 1\ndoc = df['Doc'][row_index]\ndoc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Document creates Span object"},{"metadata":{"trusted":true},"cell_type":"code","source":"span = doc[0:8]\nspan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lexical Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Index:   ', [token.i for token in span])\nprint('Text:    ', [token.text for token in span])\n\nprint('is_alpha:', [token.is_alpha for token in span])\nprint('is_punct:', [token.is_punct for token in span])\nprint('like_num:', [token.like_num for token in span])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Statistical models"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nnlp = spacy.load('en_core_web_sm')\ndf['StatsDoc'] = df['body'].apply(nlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = df['StatsDoc'][row_index]\ndoc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### POS: Parts of Speech"},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_df = pd.DataFrame()\n\nfor i, token in enumerate(doc):\n    pos_df.loc[i, 'text'] = token.text\n    pos_df.loc[i, 'lemma'] = token.lemma_,\n    pos_df.loc[i, 'pos'] = spacy.explain(token.pos_)\n    pos_df.loc[i, 'tag'] = token.tag_\n    pos_df.loc[i, 'dep'] = token.dep_\n    pos_df.loc[i, 'shape'] = token.shape_\n    pos_df.loc[i, 'is_alpha'] = token.is_alpha\n    pos_df.loc[i, 'is_stop'] = token.is_stop\n    pos_df.loc[i, 'is_punctuation'] = token.is_punct\n    \npos_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = pos_df.groupby('pos')['text'].count().reset_index()\n\npos.plot(x='pos' , y='text', kind='bar' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NER: Named Entity Recognition"},{"metadata":{"trusted":true},"cell_type":"code","source":"ent_df = pd.DataFrame()\n\nfor i, token in enumerate(doc.ents):\n    ent_df.loc[i, 'entity'] = token.text\n    ent_df.loc[i, 'label'] = token.label_\n    ent_df.loc[i, 'recognition'] = spacy.explain(token.label_)\nent_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ent = ent_df.groupby('label')['entity'].count().reset_index()\n\nent.plot(x='label' , y='entity', kind='bar' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.displacy.render(doc, style='ent',jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Token Dependency"},{"metadata":{"trusted":true},"cell_type":"code","source":"dep_df = pd.DataFrame()\n\nfor i, token in enumerate(doc):\n    dep_df.loc[i, 'token'] = token.text\n    dep_df.loc[i, 'label'] = token.dep_\n    dep_df.loc[i, 'dependency'] = spacy.explain(token.dep_)\ndep_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dep = dep_df.groupby('label')['token'].count().reset_index()\n\ndep.nlargest(columns=['token'], n=10).plot(x='label' , y='token', kind='bar' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy.displacy.render(doc, style='dep', jupyter=True,options={'distance': 80})\nspacy.displacy.render(nlp('Kaggle is fun, keep kaggling'), style='dep', jupyter=True,options={'distance': 80})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors\n# df['Similarity'] = df['StatsDoc'].apply(lambda x: x.similarity(doc)) \n\nnlp = spacy.load('en_core_web_lg')\ndf['LargeStatsDoc'] = df['body'].apply(nlp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = df['LargeStatsDoc'][row_index]\ndoc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matcher"},{"metadata":{},"cell_type":"markdown","source":"#### RuleMatcher"},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import Matcher\nmatcher = Matcher(nlp.vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```ScratchPad```"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from IPython.core.interactiveshell import InteractiveShell\n# InteractiveShell.ast_node_interactivity = \"all\"\nname = 'Nevil'\nname = 'James'\ntest_docs = [\n    f'Hey this is {name}',\n    f'Hi I am {name} from Mumbai',\n    f'Hi this is {name} joining from Mumbai'\n]\n\nsingle_doc = ''\nfor each in test_docs:\n    single_doc = single_doc + '; ' + each\n\nprocessed_test_docs = [nlp(each) for each in test_docs]\n\ndoc = processed_test_docs[0]\ndoc = nlp(single_doc)\n\nent_df = pd.DataFrame()\n\nfor i, token in enumerate(doc.ents):\n    ent_df.loc[i, 'entity'] = token.text\n    ent_df.loc[i, 'label'] = token.label_\n    ent_df.loc[i, 'recognition'] = spacy.explain(token.label_)\nent_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npos_df = pd.DataFrame()\n\nfor i, token in enumerate(doc):\n    pos_df.loc[i, 'text'] = token.text\n    pos_df.loc[i, 'lemma'] = token.lemma_,\n    pos_df.loc[i, 'pos'] = spacy.explain(token.pos_)\n    pos_df.loc[i, 'pos_id'] = token.pos_\n    pos_df.loc[i, 'tag'] = token.tag_\n    pos_df.loc[i, 'dep'] = token.dep_\n    pos_df.loc[i, 'shape'] = token.shape_\n    pos_df.loc[i, 'is_alpha'] = token.is_alpha\n    pos_df.loc[i, 'is_stop'] = token.is_stop\n    pos_df.loc[i, 'is_punctuation'] = token.is_punct\n    \npos_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Example\n\n- [Parts of Speech Annotations](https://spacy.io/api/annotation)\n- [Operators](https://spacy.io/usage/rule-based-matching#quantifiers)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npattern1 = [\n    {\"POS\": \"INTJ\", \"OP\": \"+\"},\n    {\"POS\": \"DET\", \"OP\": \"?\"},\n    {\"POS\": \"PRON\", \"OP\": \"?\"},\n    {\"POS\": \"VERB\", \"OP\": \"+\"},\n    {\"POS\": \"PROPN\", \"OP\": \"+\"},\n]\n\n\nmatcher = Matcher(nlp.vocab)\nmatcher.add(\"MeetingGreeting\", None, pattern1)\n\nlist_of_matches = [matcher(each) for each in processed_test_docs]\nsentence_no_of_matched = []\nfor sentence_no, matches in enumerate(list_of_matches):\n    for id, start, end in matches:\n        print(f'Pattern={id} matched_at={processed_test_docs[sentence_no][start:end]}')\n        sentence_no_of_matched.append(sentence_no)\n\nfor sentence_no in sentence_no_of_matched:\n    doc = processed_test_docs[sentence_no]\n    persons = [token for token in doc.ents if token.label_ == 'PERSON']\n#     persons = [(token.text, token.label_) for token in doc.ents]\n    print(f'DOC={doc} with NER={persons}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PhraseMatcher"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}