{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,18))\nim=plt.imread(\"../input/king-county-map/king county.jpg\")\nplt.imshow(im) \n#This is the actual map of the county","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/housesalesprediction/kc_house_data.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()\n# Here we see that there is no missing data in this data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().transpose()\n#Here we get overall statistical description of our data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(10,8))\nsns.distplot(df[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the distribution plot above, we see that the house prices are mainy distributed between 0 and 1 million dolar, and there some extreme outliers that we can just skip in order to prevent their influence over our deep learning model"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2)  \nsns.countplot(df[\"bedrooms\"],  ax=ax[0])\nsns.countplot(df[\"floors\"],  ax=ax[1])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),linewidths=0.5,annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our target is house prices, we can also special correlation of prices with the other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()[\"price\"].sort_values(ascending=False)\n#Here we can clearly see that there is positive high correlation between house prices and sqft_living(Square footage of the apartments interior living space)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"sqft_living\", y=\"price\", data=df,color=\"red\")\n#Here we visualize the relation between house prices and the square of the living area","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"long\", y=\"price\", data=df, color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this plot above, we understanda that the longitude between -122.0 and -122.4 has the most expensive prices, and the lontitude -121.4 has the lowest house prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"lat\",y=\"price\", data=df, color=\"green\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this plot above, we understand that the latitude between 47.5 and 47.7 has the most expensive house prices, and the latitudes between 47.2 and 47.4 has the lowest house prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(x=\"long\",y=\"lat\",c=\"price\", kind=\"scatter\",alpha=0.5,figsize=(20,15), cmap=plt.get_cmap(\"jet\"), colorbar=True, s=df[\"grade\"])\n#here we visualize the longitude and latitude and get the actual ara of the county and their relation with the price\n#We can easily see that the prices between 47.7 and 47.5 latitude has the highes prices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,20))\nim=plt.imread(\"../input/king-county-map/king county.jpg\")\nplt.imshow(im) \n#This is the actual map of the county and it correlates with our langitude and latitude plot above","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"long\",y=\"lat\",data=df, hue=\"price\",palette=\"rocket\",alpha=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to get better distribution, we can drop some outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()[\"price\"]\n#Here we the overall statistical information about house pricess and the outliers begins from 3.2 millon dolars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[\"price\"]>3000000]\n#Here we can see that there are only 40 houses that have higher than 3 million dolar house price","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Therefore, I will just create a new data frame without these outliers and create geographical maps again"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_without_outliers=df.sort_values(\"price\",ascending=False).iloc[46:]\ndf_without_outliers.sort_values(\"price\",ascending=False).head()\n#Now we have created a new data frame that has house prices lower than 3 million dolars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=\"long\",y=\"lat\",data=df_without_outliers, hue=\"price\",palette=\"rocket\",alpha=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see better the price distribution according to the latitude and longitude"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_without_outliers.plot(x=\"long\",y=\"lat\",c=\"price\", kind=\"scatter\",alpha=0.5,figsize=(20,15), cmap=plt.get_cmap(\"jet\"), colorbar=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.boxplot(x=\"waterfront\", y=\"price\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boxplot above shows that the houses near waterfront have higher house prices"},{"metadata":{},"cell_type":"markdown","source":"# 2. Feature Engineering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can just drop the id column because it has nor a special mening for predicting house prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"id\",axis=1, inplace=True)\ndf.head(3) # Now we dropped the id column from the data frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"date\"]=pd.to_datetime(df[\"date\"])\ndf.head(3) #here we have changed the structure of the date column in order use it better and make some feature engineering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we will create two new columns by feature enginnering in order to analyze data according time properties\ndf[\"year\"]=df[\"date\"].apply(lambda date: date.year)\ndf[\"month\"]=df[\"date\"].apply(lambda date: date. month)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[[\"year\",\"month\"]].head()\n#Here we have added our new feature that were hidden in the date column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"month\").mean()[\"price\"]\n#here we cna see the average price per month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2) \ndf.groupby(\"month\").mean()[\"price\"].plot(ax=ax[0], figsize=(15,6),c=\"red\")\ndf.groupby(\"year\").mean()[\"price\"].plot(ax=ax[1], figsize=(15,6),c=\"red\")\n#In the plot below, we see that the prices tends to become higher from march to july\n#The housing prices rise up from 2014 to 2015","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"date\", axis=1, inplace=True)\n#There is not to do with the date column and we get all the useful data via feature engineering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"zipcode\"].value_counts()\n# we need to drop zipcode column because ml algorithm will treat this as continues value and then cause wrong predcitions\n# We can not make them dummy variables because there 70 ifferent zip codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(\"zipcode\", axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we will make some feature engineering for yr_renovated column because majority of the houses are not renovated"},{"metadata":{"trusted":true},"cell_type":"code","source":"def renovation(feature):\n    if feature > 0:\n        feature=1\n    return feature\n#here we create a function that will assign 1 for those that are renovated and 0 those that are not renovated\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"yr_renovated\"]=df[\"yr_renovated\"].apply(renovation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"yr_renovated\"]. value_counts(). head(70)\n#Now we have just two class as 0 for non-renovated ones and 1 for renovated ones","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our data is ready for machine learning algorithm"},{"metadata":{},"cell_type":"markdown","source":"# 3. Splitting Data and Training the Algorithm:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we assign features  to the X and price to the y \nX=df.drop(\"price\",axis=1).values\ny=df[\"price\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3)\n#Here we split our data as train and test set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to chech whether our data shapes in train and test set comply with each other"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential() #here we get an insance of our model\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel.add(Dense(1)) # here we add a dthe fina layer with 1 neurons because we have one output, that is the house price\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=\"adam\", loss=\"mse\")\n#Here assign adam optimizer as our optimizer and mean squared error as our loss function for our deep learning model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x= X_train, y= y_train, batch_size=128, epochs=300, validation_data=(X_test, y_test))\n#Here we fit our model into the training X and y set with batch_size 128 and 300 epaochs, and we use also test dataset as validation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Predicting and Evaluation of the Model's Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model.history.history)\n#Here we can see losses in both our loss function and validation loss in the test data ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npd.DataFrame(model.history.history).plot(figsize=(15,10))\n#There happens a decrease in both our training and validation loss to a certain point and become stable after 60.th epoch\n#Moreover there is no overfitting because both lines goes with a perfect harmony","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=model.predict(X_test) #here the trained algorithm makes predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The absolute mean error :\",mean_absolute_error(y_test, predictions))\nprint(\"The squared mean error :\",mean_squared_error(y_test, predictions))\nprint(\"The squared mean error :\",np.sqrt(mean_squared_error(y_test, predictions)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The mean of the real data: \",df[\"price\"].mean())\nprint(\"The absolute mean error :\",mean_absolute_error(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There is 149 000 dolar error and it means %20 procent error that our model makes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Variance Score :\", explained_variance_score(y_test, predictions))\n#The variance shows how many percent that our model can explain,so our model can explain %58 procent accurately","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot((y_test-predictions),color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.scatter(y_test, predictions)\nplt.scatter(y_test,y_test,color=\"red\")\n#In this we can see that our model predict lower and normal house prices very good, but the oulier not good\n#The outlier affects negatively the performance of our model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because  the predictions of our model is not good enough, we will standardize our features and retrain the model"},{"metadata":{},"cell_type":"markdown","source":"# 5. Retraining Our Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we rescaled our features and it is ready for ML algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2=Sequential() #here we get an insance of our model\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel2.add(Dense(1)) # here we add a dthe fina layer with 1 neurons because we have one output, that is the house price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile(optimizer=\"adam\", loss=\"mse\")\n#Here assign adam optimizer as our optimizer and mean squared error as our loss function for our deep learning model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.fit(x= X_train, y= y_train, batch_size=64, epochs=300, validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model2.history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(model2.history.history).plot(figsize=(15,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2=model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The absolute mean error :\",mean_absolute_error(y_test, predictions2))\nprint(\"The squared mean error :\",mean_squared_error(y_test, predictions2))\nprint(\"The squared mean error :\",np.sqrt(mean_squared_error(y_test, predictions2)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The mean of the real data: \",df[\"price\"].mean())\nprint(\"The absolute mean error :\",mean_absolute_error(y_test, predictions2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our model decreased mean error from 149 000 dolar to 135 000."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Variance Score :\", explained_variance_score(y_test, predictions2))\n#The variance shows how many percent that our model can explain,so our model can explain %58 procent accurately","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variance also increased from %58 to %65"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(10,15))\nplt.scatter(y_test,predictions2)\nplt.scatter(y_test,predictions, color=\"green\")\nplt.scatter(y_test,y_test, color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use linear regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(\"price\", axis=1)\ny=df[\"price\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3=LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions3=model3.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predictions of Linear Regression:\",mean_absolute_error(y_test,predictions3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(10,15))\nplt.scatter(y_test,predictions3, color=\"green\")\nplt.scatter(y_test,y_test, color=\"red\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Regression performs better than our deep learning model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model4=Sequential() #here we get an insance of our model\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(19, activation=\"relu\")) # here we add a dense layer with 19 neurons because we have 19 features\nmodel4.add(Dense(1)) # here we add a dthe fina layer with 1 neurons because we have one output, that is the house price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4.compile(optimizer=\"rmsprop\",loss=\"mse\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4.fit(x= X_train, y= y_train, batch_size=64, epochs=300, validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions4=model4.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mean_absolute_error(y_test,predictions4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nplt.scatter(y_test,predictions4, color=\"green\")\nplt.scatter(y_test,y_test, color=\"red\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We changed the optimizer from adams to rmsprop, but predictions are worse than before.\n\nThe next step is to drop the outliers and train the model again"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}