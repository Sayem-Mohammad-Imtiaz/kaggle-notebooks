{"cells":[{"metadata":{"_uuid":"f5381d6434a5b07e4bf03de42072c6f8c48d9dfd"},"cell_type":"markdown","source":"### <h1><center>Overview</center></h1>\n\n### <center>Sentiment analysis on airline tweets from February 2015 exploring text pre-processing, vectorization, Multinomial Naive Bayes classification. <br> Complementing analysis with data visualizations and feature engineering to get more insight on tweets characteristics. <br><br> Special thanks to Figure Eight for uploading the dataset on kaggle.</center> <br><br>\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Libraries to import\nimport numpy as np \nimport pandas as pd\nimport os\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nimport re\nimport string\nfrom nltk.stem.porter import *\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\npd.options.mode.chained_assignment = None\n\n%matplotlib inline\n\n# Read file\nAirline_Tweets = pd.read_csv('../input/Tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dec54800ce285850e3f26ad01a886ae5d186570b"},"cell_type":"markdown","source":"# Exploratory analysis and feature engineering"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"# Check first 5 rows\nAirline_Tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6a238b06d00b7e39fd24bd615b2aab4d1e7d657c"},"cell_type":"code","source":"# Count tweets\nprint(len(Airline_Tweets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87108ee28519c3cbed6735507c0ca2123e666f6b"},"cell_type":"code","source":"# Use lambda expression to check null values for all variables\nAirline_Tweets.apply(lambda x: sum(x.isnull()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96445aef9ddb6600fab6f93a2b45967871efbc43"},"cell_type":"code","source":"# Print the first 10 tweets and numerate them with enumerate\ntweets = list(Airline_Tweets['text'])\n\nfor message_no, tweets in enumerate(tweets[:10]):\n    print(message_no, tweets)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c3898ee3d767fc6f985802f026cc96671479ae7"},"cell_type":"code","source":"# Use group by to describe variables by airline sentiment label - could give some indication of what distinguishes each label\nAirline_Tweets.groupby('airline_sentiment').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"040114fc3a116c86db634f4a4bfc7e40bcde8f30"},"cell_type":"markdown","source":"* If airline_sentiment_confidence and negativereason_confidence refer to likelyhood of the label being correct it seems both variables present good probability average values <br>\n* Low average retweet rate, across all 3 sentiments"},{"metadata":{"trusted":true,"_uuid":"ff38d29801deb5273096ee8cbb63ea1f5461de47"},"cell_type":"code","source":"# Create a column with the lenght of each tweet\nAirline_Tweets['length'] = Airline_Tweets['text'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dae505b19595669dd789daac025416acbe92e97"},"cell_type":"code","source":"# Visualize tweets length\nplt.figure(figsize=(10, 7.5))\n# Remove the plot frame lines\nax1 = plt.subplot(111)  \nax1.spines[\"top\"].set_visible(False)  \nax1.spines[\"right\"].set_visible(False)  \n# Get axis only on the bottom and left of the plot\nax1.get_xaxis().tick_bottom()  \nax1.get_yaxis().tick_left() \n# Format xticks\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n# Format labels\nplt.xlabel(\"Tweets length\", fontsize=16)  \nplt.ylabel(\"Count\", fontsize=16)\n# Plot histogram with Tweets lenght\nplt.hist(list(Airline_Tweets['length'].values),  color=\"darkturquoise\", bins=20)\n# Change background colour to white\nax1 = plt.gca()\nax1.set_facecolor('w')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5861aed83340c52d3d92e4749ada2b970871ec2d"},"cell_type":"markdown","source":"* Distribution is slightly left skewed with mean tweet length (103) < median (114) <br>\n* Spike in tweets around the 140 mark which used to be the maximum length for each tweet at the time (increased to 180 recently)"},{"metadata":{"trusted":true,"_uuid":"545a4b7700b7c3653a4efc8be8f243c0dba56b3e"},"cell_type":"code","source":"# By running the describe method in tweets length was able to check the longest tweet is 186 characters\n# Use conditional selection and iloc to have a look at it\nAirline_Tweets[Airline_Tweets['length'] == 186]['text'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"553e80fdb46fc1795e070a1601d17e011c71cb8a"},"cell_type":"code","source":"# Create a count plot with seaborn to understand how many ocurrences we have for each airline sentiment\nplt.figure(figsize=(10, 7.5))\nsentiment_ocurrences = sns.countplot(x='airline_sentiment', data=Airline_Tweets, palette='GnBu_d')\n# Remove top and right axes\nsns.despine()\n# Set background colour\nsns.set_style(style='white')\n\nsentiment_ocurrences.set_xlabel(\"Airline sentiment\",fontsize=14)\nsentiment_ocurrences.set_ylabel(\"Number of Tweets\",fontsize=14)\nsentiment_ocurrences.tick_params(labelsize=17)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b36522e227e73f8e250f2e11e5466f21ef44f3c"},"cell_type":"markdown","source":"* More negative tweets than neutral or positive - something to have in mind for any machine learning tasks"},{"metadata":{"trusted":true,"_uuid":"af5f240787eba185641451059a9b1c3e17ad385b"},"cell_type":"code","source":"# Check if length is a distinguishing feature between a positive, neutral or negative tweet\nAirline_Tweets.hist(column='length', by='airline_sentiment', color=\"darkturquoise\", bins=20, figsize=(12,8))\n\n# Change axes and tick labels font size\nparams = {'axes.titlesize':'12',\n          'xtick.labelsize':'12',\n          'ytick.labelsize':'12'}\nplt.rcParams.update(params)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aee49513bd8459e43090d5b706469b8a0cec9e1f"},"cell_type":"markdown","source":"* Negative tweets seem to be left skewed <br>\n* Positive and neutral tweets do not seem to show much of a trend. There is a spike around the 140 mark, but they seem to have a more uniform trend"},{"metadata":{"trusted":true,"_uuid":"b2b0d87f1463dd20684875516c3c32ca40a4df0b"},"cell_type":"code","source":"# Use datetime to convert tweet_created to datetime\nAirline_Tweets['tweet_created'] = pd.to_datetime(Airline_Tweets['tweet_created'])\n# Extract hour from tweet_created\nAirline_Tweets['tweet_hour'] = Airline_Tweets['tweet_created'].dt.hour\n# Define a series of conditions to get part of the day based on the hour\nconditions = [\n    (Airline_Tweets['tweet_hour'] >= 5) & (Airline_Tweets['tweet_hour'] <= 11),\n    (Airline_Tweets['tweet_hour'] >= 12) & (Airline_Tweets['tweet_hour'] <= 18),\n    (Airline_Tweets['tweet_hour'] >= 19) & (Airline_Tweets['tweet_hour'] <= 24)]\n\nchoices = ['morning', 'afternoon', 'night']\n# Create column for part of day\nAirline_Tweets['part_of_day'] = np.select(conditions, choices, default='dawn')\n# Create iterator to be able to count number of tweets\nAirline_Tweets['count'] = 1\n# Group part of day and airline sentiment to count all combinations of both\ntable = Airline_Tweets.groupby(['part_of_day', 'airline_sentiment'])['count'].sum()\n# Convert into a dataframe\ntable = pd.DataFrame(table)\n# Reset all indexes\ntable.reset_index(inplace=True)\n# Calculate percentage values\ntable['Percentage'] = 0\ntable['Percentage'].loc[table['airline_sentiment'] == 'negative'] = table['count'] / 9178 # 9178 - total number negative tweets\ntable['Percentage'].loc[table['airline_sentiment'] == 'neutral'] = table['count'] / 3099 # 3099 - total number neutral tweets\ntable['Percentage'].loc[table['airline_sentiment'] == 'positive'] = table['count'] / 2363 # 2363 - total number positive tweets\ntable['Percentage'] = round(table['Percentage'] * 100,0) #round percentage\n# Specify colour for each group\nday_part_colour = {\"morning\": \"powderblue\", \"afternoon\": \"c\", \"night\":\"c\", \"dawn\":\"powderblue\"}\n# Use catplot from seaborn to plot the results from group by\nax = sns.catplot(x=\"part_of_day\", y=\"Percentage\", col=\"airline_sentiment\",data=table, saturation=.6, kind=\"bar\", ci=None, aspect=.9, palette=day_part_colour, order=['morning', 'afternoon', 'night', 'dawn'])\n# Make some adjustments to plot default settings\n(ax.set_axis_labels(\"\", \"Par of day by sentiment (%)\")\n  #.set_xticklabels(['morning', 'afternoon', 'night', 'dawn'])\n  .set_titles(\"{col_name} {col_var}\")\n  .set(ylim=(0, 40))\n  .despine(left=True))\n# Change the font size\nplt.rcParams[\"axes.labelsize\"] = 14\nfor ax in plt.gcf().axes:\n    l = ax.get_xlabel()\n    ax.set_xlabel(l, fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"012df2bc1fdafc592fdc9a607cb85e8c49dbb33f"},"cell_type":"markdown","source":"* Most people seem to have tweeted in the afternoon and night across all sentiment states - could related with frequency of flights at those times <br>\n* **Note**: Part of day defined according with following time (debatable as it could change with geographic location):<br>\n    * morning: 5am to 11am<br>\n    * afternoon: 12pm to 6pm<br>\n    * night: 6pm to midnight<br>\n    * dawn: midnight to 5am"},{"metadata":{"trusted":true,"_uuid":"fd471ea6b5368bd6994b7804f5e7543b42a589a4"},"cell_type":"code","source":"# Check airline sentiment for each airline\ntable = Airline_Tweets.groupby(['airline', 'airline_sentiment'])['count'].sum()\n# Convert into a dataframe\ntable = pd.DataFrame(table)\n# Reset all indexes\ntable.reset_index(inplace=True)\n# Calculate percentage values\ntable['Percentage'] = 0\ntable['Percentage'].loc[table['airline_sentiment'] == 'negative'] = table['count'] / 9178 # 9178 - total number negative tweets\ntable['Percentage'].loc[table['airline_sentiment'] == 'neutral'] = table['count'] / 3099 # 3099 - total number neutral tweets\ntable['Percentage'].loc[table['airline_sentiment'] == 'positive'] = table['count'] / 2363 # 2363 - total number positive tweets\ntable['Percentage'] = round(table['Percentage'] * 100,0) #round percentage\n# Use catplot from seaborn to plot the results from group by\nax = sns.catplot(x=\"airline\", y=\"Percentage\", col=\"airline_sentiment\",data=table, saturation=.6, kind=\"bar\", ci=None, aspect=1.5, palette='RdBu')\n# Make some adjustments to plot default settings\n(ax.set_axis_labels(\"\", \"Airline by sentiment (%)\")\n  .set_titles(\"{col_name} {col_var}\")\n  .set(ylim=(0, 40))\n  .despine(left=True))\n# Change the font size\nplt.rcParams[\"axes.labelsize\"] = 14\nfor ax in plt.gcf().axes:\n    l = ax.get_xlabel()\n    ax.set_xlabel(l, fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7444b604a8b079e3cbde30f15161f0266b418883"},"cell_type":"markdown","source":"* Delta, Southwest and Virgin America show good positive and neutral balance when compared to negative sentiment\n* US Airways and United seem to gather most of the negative comments - overall more than 65% of their tweets are negative\n* American and US Airways show bigger gaps between negative and positive or neutral sentiments\n\n* **Note**: data is slightly inbalanced. Virgin America has only 504 tweets compared to US Airways and United with 2.9k and 3.8k, respectively"},{"metadata":{"trusted":true,"_uuid":"334c1b7aac82340b8a05cfe817714ae1b539714a"},"cell_type":"code","source":"# Check the most common negative reason\nAirline_Tweets['negativereason'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c391c581c2b156fb05296a087557600d39390790"},"cell_type":"markdown","source":"## Text pre-processing"},{"metadata":{"_uuid":"32b8325f858b7b2b4c1fbd63d4e91acec9e4d85a"},"cell_type":"markdown","source":"* **Summary of pre-processing steps**: <br><br>\n    * remove tweet handles from the original text column (e.g. @VirginAirlines)\n    * remove special characters using [^a-zA-Z#]\n    * remove short words\n    * remove ponctuation signs using string.punctuation\n    * remove stopwords using stopwords.words('english')\n    * apply stemming by removing all the suffixes from words (e.g. 'cats', 'catlike', and 'catty' all should be the same as the string 'cat')\n\n* The 5 rows printed after each step will allow one to sense check how the data is changing"},{"metadata":{"trusted":true,"_uuid":"0bfb7e6bc640d4cba59a0a3c371128c230405efb"},"cell_type":"code","source":"# Create a new text column that removes tweet handles from text column (e.g. @VirginAirlines)\n# Create standard function that will remove twitter handles\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt\n# Apply the function above\nAirline_Tweets['tidy_tweet'] = np.vectorize(remove_pattern)(Airline_Tweets['text'], \"@[\\w]*\")\n# Check first 5 messages\nAirline_Tweets['tidy_tweet'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f82d28ae5427cc0b2c4d8fe51beb3e7e8b939a26"},"cell_type":"code","source":"# Remove special characters\nAirline_Tweets['tidy_tweet'] = Airline_Tweets['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n# Check first 5 messages\nAirline_Tweets['tidy_tweet'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dea6fc9d6a1fe17c51cf18066369aa8f59f1d63"},"cell_type":"code","source":"# Remove words that are very short and might not have any meaning\nAirline_Tweets['tidy_tweet'] = Airline_Tweets['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n# Check first 5 messages\nAirline_Tweets['tidy_tweet'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"819efc3cda2141565176c09c725ba4b859b0b772"},"cell_type":"code","source":"# To build a classification algorithm one has to transform each tweet text in a numerical feature vector\n# Use the bag of words approach to represent each word in the text by a number\ndef text_process(mess):\n    # Make use of string.punctuation to remove pontuation signs\n    nopunc = [char for char in mess if char not in string.punctuation]\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    # Use stopwords.words('english') to remove some of the most common words (e.g. 'the', 'of', 'a')\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n\nAirline_Tweets['tidy_tweet'] = Airline_Tweets['tidy_tweet'].apply(text_process)\n# Check first 5 messages\nAirline_Tweets['tidy_tweet'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05a59194883a747cdde96c67c8761930f5c96d5d"},"cell_type":"code","source":"# Create a word cloud at this step to understand what are the most common words used in each tweet\n# To speed up processing will split the data between positive and negative sentimet and plot a word cloud each\n\n# Take only positive tweets\npositive_tweets = Airline_Tweets.loc[Airline_Tweets['airline_sentiment'] == 'positive']['tidy_tweet']\n# Reset the index\npositive_tweets.reset_index(inplace=True, drop=True)\n# Join each element in a list together\nfor i in range(len(positive_tweets)):\n    positive_tweets[i] = ' '.join(positive_tweets[i])\n# Join all the words in a single string\nall_positive_words = ' '.join([text for text in positive_tweets])\n# Create the wordcloud object\nwordcloud = WordCloud(width=600, height=600, background_color=\"white\").generate(all_positive_words)\n# Plot graph\nplt.figure(figsize=(10, 7.5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a97d02935be63ea6e2a7d1487ba2be6ef563fcac"},"cell_type":"code","source":"# Take only negative tweets\nnegative_tweets = Airline_Tweets.loc[Airline_Tweets['airline_sentiment'] == 'negative']['tidy_tweet']\n# Reset the index\nnegative_tweets.reset_index(inplace=True, drop=True)\n# Join each element in a list together\nfor i in range(len(negative_tweets)):\n    negative_tweets[i] = ' '.join(negative_tweets[i])\n# Join all the words in a single string\nall_negative_words = ' '.join([text for text in negative_tweets])\n# Create the wordcloud object\nwordcloud = WordCloud(width=600, height=600, background_color=\"white\").generate(all_negative_words)\n# Plot graph\nplt.figure(figsize=(10, 7.5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77b3e53ce34ccb27cb7b754a3ea4c04cc7f96545"},"cell_type":"code","source":"# Use PorterStemmer() method to apply stemming to all the tweets\nstemmer = PorterStemmer()\n\nAirline_Tweets['tidy_tweet'] = Airline_Tweets['tidy_tweet'].apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n# Check first 5 messages\nAirline_Tweets['tidy_tweet'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d969e2266d113ead7ebb05331a7c65bca556a6c"},"cell_type":"code","source":"# Put back the column tidy_tweet on this original form with a string per row\nclean_tweets = Airline_Tweets['tidy_tweet']\n\nfor i in range(len(clean_tweets)):\n    clean_tweets[i] = ' '.join(clean_tweets[i])\n\nAirline_Tweets['tidy_tweet'] = clean_tweets\n\nAirline_Tweets['tidy_tweet'].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d3a6972f2de060d4c36a7a467926c8fffe53c4f"},"cell_type":"markdown","source":"## Vectorization\n\n* **Steps covered in vectorization**: <br><br>\n    * Use CountVectorizer to count the number of times each word occurs in every message following the [bag of words approach](https://en.wikipedia.org/wiki/Bag-of-words_model)\n    * Use TF-IDF to apply weighting and normalization using the following formulas\n        * TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n        * IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"},{"metadata":{"trusted":true,"_uuid":"f8318fdcde3bbb3ab170efa08da96c3bee11ab7d"},"cell_type":"code","source":"# Use CountVectorizer object to create a matrix will all the words in every tweet\n# For this analysis will use the default parameters for CountVectorizer\ntweet_transformer = CountVectorizer().fit(Airline_Tweets['tidy_tweet'])\n\n# Print total number of vocabulary words\nprint(len(tweet_transformer.vocabulary_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a85e205a0b815d643d53452c29a507ef705cad78"},"cell_type":"code","source":"# Check an example in detail and take the 4th tweet in the dataset and see its vector representation\n# For reference this is the text of the tweet: 'realli aggress blast obnoxi entertain guest face littl recours'\ntweet_3 = Airline_Tweets['tidy_tweet'][3]\n\nvector_3 = tweet_transformer.transform([tweet_3])\nprint(vector_3)\nprint(vector_3.shape)\n\n# There are 9 unique words in this message and the second number (e.g. 124) will allow one to see what word that is","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46c5b1bd20c2b2163505e405fb5726163d218b32"},"cell_type":"code","source":"# Apply the transformer in the entire tweets series\ntweet_bag_of_words = tweet_transformer.transform(Airline_Tweets['tidy_tweet'])\n# Check the shape and number of non-zero ocurrences\nprint('Shape of Matrix: ', tweet_bag_of_words.shape)\nprint('Amount of Non-Zero occurences: ', tweet_bag_of_words.nnz)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d38048339655f906869787b8646b254d6e031c3"},"cell_type":"code","source":"# Adjust the weights with TF-IDF\n# Each weight is calculated with the following formula:\n# Scenario: in one document with 100 words the word 'data' appears 5 times. There are 1,000 ducoments to classify and the word 'data' appears 90 times in all of them\n# TF = 5/100 = 0.05\n# IDF = log(1,000/90) = 1\n# Tf-idf weight = 0.05 * 1 = 0.05\nfrom sklearn.feature_extraction.text import TfidfTransformer\n# Apply the transformer to the bag of words\ntweet_tfidf_transformer = TfidfTransformer().fit(tweet_bag_of_words)\ntweet_tfidf = tweet_tfidf_transformer.transform(tweet_bag_of_words)\n# Check the shape\nprint(tweet_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aae97b15dab9f5d92d667ff9b5a35d093d65157"},"cell_type":"markdown","source":"## Build Multinomial Naive Bayes classification model"},{"metadata":{"_uuid":"1a6fd8c6f76f8078aa29d3a0023f612520dbc46a"},"cell_type":"markdown","source":"* **Objective**: this model will attempt to classify tweets between negative and not negative based on the text of each tweet <br><br>\n* **Model selection**: There is a variety of classification models that could suit this problem. Due to familiarity of approach and ease of implementation will use a Multinomial Naive Bayes classifier that usually [performs well on text classification](https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf) tasks like this one <br><br>\n* **Label simplification**: in line with the objective statement will create a variable **airline_sentiment_model** that will aggregate both positive or neutral sentiments in a single category making it a binary problem: 1 if negative 0 if not negative <br><br>\n* **Potential caveates**: simplifying labels will balance the ratio between negative tweets and not negative, but could come at the risk of losing detail as a postive coment is not exactly the same as a neutral one"},{"metadata":{"trusted":true,"_uuid":"572f5db8b1b6c21abfd20e6766214aec2f983601"},"cell_type":"code","source":"# Add new column for airline sentiment with binary outcome: 1 for negative comment 0 for not negative\n# Create dictionary to map\nsentiment_dictionary = {'negative': 1, 'neutral': 0, 'positive': 0}\n# Add new column mapping the dictionary\nAirline_Tweets['airline_sentiment_model'] = Airline_Tweets['airline_sentiment'].map(sentiment_dictionary)\n# Check first 5 rows\nAirline_Tweets.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee8428971b8cc5258281462e93ddc6afe4994167"},"cell_type":"code","source":"# Take X and y variables using the TF-IDF vectorization from the previous step\nX = tweet_tfidf\ny = Airline_Tweets['airline_sentiment_model']\n# Do a train test split - will choose to leave the default 30% of the data for testing\nX_train, X_test, y_train, y_test = \\\ntrain_test_split(X, y, test_size=0.3)\n# Check size of each sample\nprint(X_train.shape[0], X_test.shape[0], X_train.shape[0] + X_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"126e64242d9eb4856587068c4a48469563621e7b"},"cell_type":"code","source":"# Create the Multinomial Naives Bayes object\ntweet_sentiment_model = MultinomialNB()\n# Fit X_train and y_train to train the model\ntweet_sentiment_model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9954a0e6db56c34981f7a31df44fe98ace37786"},"cell_type":"code","source":"# Make one prediction \nprint('predicted:', tweet_sentiment_model.predict(X_test)[0])\nprint('expected:', y_test.iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8052f04398c57e109211ec20347646558bb9cbe7"},"cell_type":"code","source":"# Apply the model to predict X_test values\npredictions = tweet_sentiment_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5068474246cf73cb1ae220fe830e632cf20d34ce"},"cell_type":"markdown","source":"## Model evaluation"},{"metadata":{"trusted":true,"_uuid":"2bbfebe60751cab0b3e24b6e2965eb4d94ff2c1b"},"cell_type":"code","source":"# Print confusion matrix\nfrom sklearn.metrics import confusion_matrix\nprint (confusion_matrix(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17322403e99682146620dbbefd5317ebb314068c"},"cell_type":"code","source":"# Print classification report\nfrom sklearn.metrics import classification_report\nprint (classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ad7ba9c0351f5d20218054368737fbb224fa90d"},"cell_type":"markdown","source":"Performance will always be subjective to what metric is more relevant to the model's objective. <br> Will go through a few key points: <br><br>\n0 - tweet is not negative <br>\n1 - tweet is negative <br><br>\n* Model is showing encouraging recall rate, or probability of detection, by classifying correctly 97% of all negative tweets: recall = 2670/(90+2670) = 0.97 <br>\n* Precision indicates there is still potential for improvement with about 74% of negative preditions to be correctly classified: precision = 2670/(2670+924) = 0.74 <br>\n* There are still 924 positive tweets that were incorrectly classified as negative leading to False Positive Rate (FPR) of 55%: FPR = 924/(708+924) = 0.55 <br><br>\n\nIf an airline would be trying to understand why are people tweeting negatively about them, false positives would not be too problematic as this could be filtered out with a deep dive exploratory data analysis. Alternatively, if they would try to follow up on negative tweets by getting into contact with customers then false positives could be an issue, as they could potentially reach out to people that had no problem flying with them."},{"metadata":{"_uuid":"c6a9e2bf3f6f8073c199cee15e91302d67e404e7"},"cell_type":"markdown","source":"# Further topics to explore\n"},{"metadata":{"trusted":true,"_uuid":"b8639d6449dbc1c1de2e1731ae8b79532d04462e"},"cell_type":"markdown","source":"* Understand the difference between using a model only with CountVectorizer() excluding the TF-IDF step <br>\n* Explore hyper parameter tunning in vectorization step and with MultiNomialNB algorithm <br>\n* Test performance with different classification models <br>\n* Extend analysis for longer period and check how seasonality affects negative airline sentiment"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}