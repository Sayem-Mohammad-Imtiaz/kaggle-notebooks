{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"#01FB16ECS419 Tejas Prashanth\n#01FB16ECS416 Tanmaya Udupa\n#01FB16ECS426 VS Meghana\n#Assignment6\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns #For the correlation plot\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nimport math as m\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score\nimport graphviz\nfrom sklearn import svm\nfrom sklearn.preprocessing import MinMaxScaler\n#import sys\n#print(sys.path)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\".\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"70e9d590e7e7f17a8ebc59be29d3d9249ae48e7e"},"cell_type":"code","source":"df=pd.read_csv(\"../input/Absenteeism_at_work.csv\")\nprint(df.iloc[0,])\nprint(df.columns)\n#Summary stats\nprint(df.describe())\n\n\n\n#Split into training and testing\nX=df[df.columns.difference(['Absenteeism time in hours'])]\ny=df[['Absenteeism time in hours']]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)\n# for ele in X_train.columns:\n#     plt.hist(X_train.loc[:,ele])\n#     plt.title(ele)\n#     plt.show()\nplt.hist(y_train.iloc[:,0])\nplt.title(\"Absenteeism time in hours\")\nplt.show()   \n\n\nsns.boxplot(y_train.iloc[:,0])\nplt.show()\n\nprint(y_train)\ntraining_data=pd.concat([X_train,y_train],ignore_index=False,axis=1)\nprint(training_data.head())\nprint(len(training_data))\n\n#Since the time is positively skewed, all outliers should be removed. \n# q1=y_train.iloc[:,0].quantile(0.25)\n# q3=y_train.iloc[:,0].quantile(0.75)\n# iqr=q3-q1\n# upper_limit=q3+1.5*iqr\n# training_data=training_data.loc[training_data['Absenteeism time in hours']<upper_limit]\n# print(\"hello\")\n# print(training_data)\n# X_train=training_data.iloc[:,:-1]\n# print(X_train.columns)\n# y_train=training_data.iloc[:,-1]\n\nq1=df.loc[:,'Absenteeism time in hours'].quantile(0.25)\nq3=df.loc[:,'Absenteeism time in hours'].quantile(0.75)\niqr=q3-q1\nupper_limit=q3+1.5*iqr\ndf=df.loc[df['Absenteeism time in hours']<upper_limit]\nprint(\"hello\")\n\nX=df[df.columns.difference(['Absenteeism time in hours'])]\ny=df[['Absenteeism time in hours']]\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=1)\ntraining_data=pd.concat([X_train,y_train],ignore_index=False,axis=1)\nprint(training_data.head())\nprint(len(training_data))\n\nprint(training_data)\nX_train=training_data.iloc[:,:-1]\nprint(X_train.columns)\ny_train=training_data.iloc[:,-1]\n\n#Plot a correlation matrix\nattr_to_drop=[]\ncor_matrix=training_data.corr()\nsns.heatmap(cor_matrix,xticklabels=cor_matrix.columns.values,yticklabels=cor_matrix.columns.values)\nplt.show()\n#print(cor_matrix)\nprint(\"Age and Absenteeism in hours correlation\",cor_matrix.loc[\"Age\",\"Absenteeism time in hours\"])\nprint(\"Service time and absenteeism in hours correlation\",cor_matrix.loc[\"Service time\",\"Absenteeism time in hours\"])\n#Numerically, age has a stronger correlation with absenteeism in hours. As a result, service time can be removed\n#training_data=training_data[training_data.columns.difference(['Service time'])]\n#print(cor_matrix.loc[\"Body mass index\",\"Absenteeism time in hours\"])\n\n#No null values in the dataset\nprint(training_data.isnull().sum())\n\n# unique_dict=dict()\n# for i in training_data.columns:\n#     unique_dict[i]=training_data.loc[:,i].unique()\n# print(unique_dict)\n#print(max(y_train.iloc[:,0]))\nprint(training_data.columns)\n\n\n#From the correlation plot, age and body mass index depict a strong correlation\nprint(cor_matrix.loc['Age','Absenteeism time in hours']>cor_matrix.loc['Body mass index','Absenteeism time in hours'])\n#Since age depicts a strong correlation, age is considered\nattr_to_drop.append(\"ID\")\nattr_to_drop.append(\"Body mass index\")\nattr_to_drop.append(\"Service time\")\n#attr_to_drop.append(\"Month of absence\")\nattr_to_drop.append(\"Distance from Residence to Work\")\nX_train=X_train[X_train.columns.difference(attr_to_drop)]\nX_test=X_test[X_test.columns.difference(attr_to_drop)]\n\nprint(y_train)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":false},"cell_type":"raw","source":""},{"metadata":{"trusted":false,"_uuid":"a3aee426d43e4a980c0719d0d018b47ae9553c3d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9484e2c1b9586b905b7520097125d6217c27fcbd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d74a154a9863107a2589fe2c721e1d2e72cff6d5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6de44c4c6951bf04ba367da0fdd82b809f28ef34"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"7155938c34ecc110899283b0a2d97f602a3ab003"},"cell_type":"code","source":"#Decision Trees\nfrom sklearn import tree\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\nleaf_nodes=[8,9,10,11,12,13,14,15,16,17,18,19,20]\ntest_scores=[]\nfor i in leaf_nodes:\n    clf=DecisionTreeClassifier(criterion='entropy',random_state=42,min_samples_leaf=1,max_leaf_nodes=i)\n    clf.fit(X_train,y_train)\n    predict_y_test=clf.predict(X_test)\n    test_scores.append(accuracy_score(y_test,predict_y_test))\n\nplt.plot(leaf_nodes,test_scores)\nplt.show()\n\n#Since having a maximum of 14-16 leaf nodes yields the highest accuracy, the classifier chosen is as follows    \n    \nclf=DecisionTreeClassifier(criterion='entropy',random_state=42,min_samples_leaf=1,max_leaf_nodes=16)\nclf.fit(X_train,y_train)\npredict_y_test=clf.predict(X_test)\nprint(clf.feature_importances_)\n\nprint(confusion_matrix(y_test,predict_y_test))\nprint(classification_report(y_test,predict_y_test))\nprint(\"Testing accuracy\")\nprint(accuracy_score(y_test,predict_y_test))\nprint(\"Training accuracy\")\nprint(clf.score(X_train,y_train))\nprint(X_train.columns)\n\ndot_data = tree.export_graphviz(clf, out_file=\"Absenteeism.dot\",feature_names=X_train.columns.tolist(),class_names=y_train.unique().astype(str).tolist()) \ngraph = graphviz.Source(dot_data) \nos.system(\"dot -Tpng Absenteeism.dot -o absent.png\")\n\n# from sklearn.datasets import load_iris\n# iris=load_iris()\n# print(iris.target_names)\nprint(len(X_train.columns.tolist()))\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64d81dfac899ad25bf550623ce5b6d1a1d7cea65"},"cell_type":"code","source":"#using svc\n\nscaler = MinMaxScaler()\ntraining_data_scaled = scaler.fit_transform(X_train)\ntesting_data_scaled = scaler.fit_transform(X_test)\n#from mlxtend.plotting import plot_decision_regions\n\n#df1= pd.DataFrame(data=training_data_scaled[1:,1:],index=training_data_scaled[1:,0],columns=training_data_scaled[0,1:])\n#X=df1.drop(training_data_scaled.columns[[10]],axis=1)\n#print(training_data_scaled)\n#X1=np.asarray(training_data_scaled)\n#X=X1.drop(X1.columns[[10]],axis=1)\n#y=training_data_scaled[10]\n\nC=1\nclf = svm.SVC(kernel='rbf', C=C).fit(training_data_scaled,y_train)\n\n#clf = svm.SVC()\n#clf.fit(training_data_scaled,y_train)\npred=clf.predict(testing_data_scaled)\n\n#print(y_test)\nprint(pred)\n#y1=np.asarray(y_test)\n#print(y1)\nprint(\"Training accuracy\")\nprint(clf.score(training_data_scaled,y_train))\nprint(\"Testing accuracy\")\nprint(accuracy_score(y_test,pred))\n\nprint(confusion_matrix(y_test,pred))\nprint(classification_report(y_test,pred))\n#print(accuracy_score(y1,pred))\n# from sklearn.metrics import r2_score\n# print(y_test)\n# coef = r2_score(y_test,pred)\n# print(coef)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c148119df84171c4f65c631f7073ad736236e2b9"},"cell_type":"code","source":"#USING SVR\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVR\n\nscaler = MinMaxScaler()\ntraining_data_scaled = scaler.fit_transform(X_train)\ntesting_data_scaled = scaler.fit_transform(X_test)\n\nC=1.0\nclf=svm.SVR(kernel='linear', C=C).fit(training_data_scaled,y_train)\n\n#clf = svm.SVR()\n#clf.fit(training_data_scaled,y_train)\npred=clf.predict(testing_data_scaled)\n\n#print(y_test)\n#print(pred)\n#y1=np.asarray(y_test)\n#print(y1)\n\n#print(accuracy_score(y1,pred))\nfrom sklearn.metrics import r2_score\nprint(y_test)\ncoef = r2_score(y_test,pred)\nprint(coef)\n\n\n\n#clf.predict([[2., 2.]])\n#C = 1.0 \n#svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n#print(X)\n#clf = SVC(gamma='auto')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a422ee0400512258de37e53839367e362a4de43f"},"cell_type":"code","source":"#KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nscaler = MinMaxScaler()\ntraining_data_scaled = scaler.fit_transform(X_train)\ntesting_data_scaled = scaler.fit_transform(X_test)\n\nkmeans = KMeans(n_clusters=len(y_train.unique())-1,max_iter=200,random_state=1,n_init=10)\nkmeans.fit(training_data_scaled)\nprediction = kmeans.predict(testing_data_scaled)\n\n\n#In order to assign a label to each cluster(A label that represents absenteeism time in hours),the predict() in run against every sample in the trainign dataset and its KMeans cluster label is found\n#Then,the a dictionary is maintained to keep track of the absenteeism time obtain for each KMeans cluster label for every sample\n#The majority absenteeism time in each cluster is chosen as the custom cluster label, which is maintained in the dictionary cluster_label \nlabels_dict=dict()\ncluster_label=dict()\nprint(training_data_scaled[0])\nlabel=kmeans.predict(training_data_scaled[0].reshape(1,-1))\nprint(label)\nfor i in range(len(training_data_scaled)):\n    label=kmeans.predict(training_data_scaled[i].reshape(1,-1))\n    if(label[0] not in labels_dict.keys()):\n        labels_dict[label[0]]=list()\n    labels_dict[label[0]].append(y_train.iloc[i])\n#print(labels_dict)\nfor key in labels_dict:\n    cluster_label[key]=pd.Series(labels_dict[key]).value_counts().index[0]\nprint(cluster_label)\n#print(y_train.unique())\n#y_pred represents the cluster labels given by KMeans\n#y_actual represents the actual Absenteeism in hours\ndef cluster_score_validate(y_actual,y_pred):\n    if(len(y_actual)!=len(y_pred)):\n        print(\"Shape error with actual and predictions\")\n    count=0\n    #Iterate through y_pred\n    for i in range(len(y_pred)):\n        ele=y_pred[i]\n        if(y_actual.iloc[i]==cluster_label[ele]):\n            count=count+1\n    return(float(count/len(y_actual)))\nprint(\"Testing accuracy is\")\nprint(cluster_score_validate(y_test.iloc[:,0],prediction))\n\nvalues_k=list(range(5,len(y_train.unique())+1))\nscores=[]\nprint(values_k)\nfor j in range(5,len(y_train.unique())+1):\n    #print(\"hello\")\n    kmeans = KMeans(n_clusters=j,max_iter=200,random_state=1,n_init=20)\n    kmeans.fit(training_data_scaled)\n    prediction = kmeans.predict(testing_data_scaled)\n    labels_dict=dict()\n    cluster_label=dict()\n    #print(training_data_scaled[0])\n    label=kmeans.predict(training_data_scaled[0].reshape(1,-1))\n    #print(label)\n    for i in range(len(training_data_scaled)):\n        label=kmeans.predict(training_data_scaled[i].reshape(1,-1))\n        if(label[0] not in labels_dict.keys()):\n            labels_dict[label[0]]=list()\n        labels_dict[label[0]].append(y_train.iloc[i])\n    #print(labels_dict)\n    for key in labels_dict:\n        cluster_label[key]=pd.Series(labels_dict[key]).value_counts().index[0]\n    scores.append(cluster_score_validate(y_test.iloc[:,0],prediction))\n\nplt.plot(values_k,scores)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6c78573282e657aa6e0a423ce07c91e0acd2e59"},"cell_type":"raw","source":""},{"metadata":{"trusted":false,"_uuid":"88784f2f8b0554944f007b029bf2bc60af37872f"},"cell_type":"code","source":"#KNN\n#using knn\nfrom sklearn.neighbors import KNeighborsClassifier \nli=[]\nscaler = MinMaxScaler()\ntraining_data_scaled = scaler.fit_transform(X_train)\ntesting_data_scaled = scaler.fit_transform(X_test)\nx=[3,4,5,6,7,8,9,10,11,12]\nfor i in x :\n    classifier = KNeighborsClassifier(n_neighbors=i)  \n    classifier.fit(training_data_scaled, y_train)\n    y_pred = classifier.predict(testing_data_scaled)\n    #print(y_pred)\n    li.append(accuracy_score(y_test,y_pred))\n#print(li)\nplt.plot(x,li)\n#plt.xlim(0.4275,0.4276)\nplt.show()\n\nnp.random.seed(0)\nclassifier = KNeighborsClassifier(n_neighbors=6)  \nclassifier.fit(training_data_scaled, y_train)\ny_pred = classifier.predict(testing_data_scaled)\nprint(\"6 neighbours are chosen since it gives the highest accuracy\")\nprint(\"Testing accuracy\")\nprint(accuracy_score(y_test,y_pred))\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n#from sklearn.metrics import classification_report, confusion_matrix  \n#print(confusion_matrix(y_test, y_pred))  \n#print(classification_report(y_test, y_pred))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3975be718b28e367c3003a8df774b8bcca9532b3"},"cell_type":"code","source":"#Multiple regression model\n#There should be no multi collinearity\ntrain_multiple_regression=X_train\nprint(X_test.columns,train_multiple_regression.columns)\nprint(train_multiple_regression.shape,X_test.shape)\n#y_train=m.log(y_train.astype(np.float))\ntrain_multiple_regression = sm.add_constant(train_multiple_regression)\n#print(train_multiple_regression)\nmodel=sm.OLS(y_train,train_multiple_regression).fit()\nX_test=sm.add_constant(X_test)\npredict_y_test=model.predict(X_test)\n\nprint(model.summary())\nplot_actual=y_test.reset_index().loc[:,'Absenteeism time in hours']\nplot_fitted=predict_y_test.reset_index().iloc[:,1]\n#print(plot_x,plot_y)\n#plt.plot(,)\nplt.scatter(plot_actual,plot_fitted)\nplt.title(\"Fitted vs. Actual values\")\nplt.xlabel(\"Actual values\")\nplt.ylabel(\"Fitted values\")\n#print(len(plot_x)==len(plot_y))\n#plt.plot(np.arange(len(plot_x)),plot_x,type='line')\nplt.show()\n\n\nresiduals=plot_actual-plot_fitted\nplt.scatter(plot_fitted,residuals)\nplt.title(\"Residual plot\")\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ddf499f738b03e00d04e3c1bfae9c4c4bef930d3"},"cell_type":"code","source":"\n#Applying transformations\n#print(min(y_train_transformed))\ny_train_transformed=y_train\nX_train_transformed=X_train\nX_test_transformed=X_test\ny_train_transformed=pd.to_numeric(y_train_transformed).apply(lambda x:m.log(x) if x!=0 else 0)\nX_train_transformed = sm.add_constant(X_train_transformed)\n\nmodel=sm.OLS(y_train_transformed,X_train_transformed).fit()\nX_test_transformed = sm.add_constant(X_test_transformed)\n\npredict_y_test=model.predict(X_test_transformed)\n\nprint(model.summary())\n\nplot_actual=pd.to_numeric(y_test.reset_index().loc[:,'Absenteeism time in hours']).apply(lambda x:m.log(x) if x!=0 else 0)\nplot_fitted=predict_y_test.reset_index().iloc[:,1]\n#print(plot_x,plot_y)\n#plt.plot(,)\nplt.scatter(plot_actual,plot_fitted)\nplt.title(\"Fitted vs. Actual values\")\nplt.xlabel(\"Actual values\")\nplt.ylabel(\"Fitted values\")\n#print(len(plot_x)==len(plot_y))\n#plt.plot(np.arange(len(plot_x)),plot_x,type='line')\nplt.show()\n\n\nresiduals=plot_actual-plot_fitted\nplt.scatter(plot_fitted,residuals)\nplt.title(\"Residual plot\")\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\nprint(X_test.shape)\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c909a527dd3d48d231916a25d3127063b2be98e5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1379d8f14a20c79d5a1453604414ca091e332c73"},"cell_type":"code","source":"models={\"Multiple regression\":27.5,\"Decision Trees\":54,\"SVC\":46.5,\"SVR\":18.23,\"KMeans\":43.1,\"KNN\":44.25}\nplt.bar(np.arange(len(models.keys())),list(models.values()))\nplt.title(\"Comparing accuracy of various models\")\nplt.xticks(np.arange(len(models.keys())),models.keys(),rotation=45)\nplt.xlabel(\"Model\")\nplt.ylabel(\"Accuracy(in %)\")\nplt.show()\n\nf1_scores={\"SVC\":37,\"KNN\":44,\"Decision Trees\":45}\nplt.bar(np.arange(len(f1_scores.keys())),list(f1_scores.values()))\nplt.title(\"Comparing accuracy of various models\")\nplt.xticks(np.arange(len(f1_scores.keys())),f1_scores.keys(),rotation=45)\nplt.xlabel(\"Model\")\nplt.ylabel(\"F1 score(in %)\")\nplt.show()\n\nprint(\"Although the assignment involved finding classification models, a multiple regression model is also constructed since the dataset involves prediction of absenteeism time, which consists of discrete values. \")\nprint(\"1) Among the regression models that were constructed, a log transformed slightly improved the R-squared model, but the residual plots clearly depict a trend in the residual. Hence, linear regression is not applicable for this problem\")\nprint(\"2) Among the classifiers, Decision Trees and K Nearest Neighbours are the best models.\")\nprint(\"The performance measures taken into consideration are accuracy and F1 score. This is because accuracy cannot predict the usefulness of a model. F1 score provides a intermediary measure between recall, which depicts the the proportion correctly classified out of all the values that belong to a particular class, and precision, which depicts the proportion of correctly classified points out of all points that were classified as belonging to a particular class\")\nprint(\"3) Although SVC had a higher accuracy, KNN has a higher F1 score than SVC, due to which KNN is a better model than SVC.\")\nprint(\"Moreover, the difference in accuracy between KNN and SVC is small whereas the difference in their respective F1 scores is large. As a result, in this case,F1 score is considered more significant than the accuracy\")\nprint(\"4) Therefore, due to the above mentioned reasons, the best classifier is the Decision Tree, which is followed by the K Nearest Neighbours(KNN)\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9d6084417cee55218cd434d4b288dbd81a975d33"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":1}