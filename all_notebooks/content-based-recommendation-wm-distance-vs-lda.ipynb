{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Word Mover's Distance vs LDA\nThe goal of this kernel is to provide a contrast between two different methods to find document similarity.\n* The Word Mover's Distance methods finds the n nearest neighbors of the selected content based on the Word Mover's distance. It is based on a word-embedding representation of the document which means it takes into account the overall sentiment of the story and compares it to that of other stories.\n* Latent Dirichlet Allocation or LDA is a Gibbs Sampling method which models the documents as a mixture of distributions of k topics. Then we find the n nearest neighbors of the selected content based on similarity of topic distributions. The semantic similarities of the text are ignored because the probabilities are calculated based on the occurences of the word.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \n\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read and preprocess data\n\n* Convert to lowercase\n* Remove stopwords\n* Lemmatize (helps to reduce the size of word dictionary)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(stopwords.words('english'))\nstopwords_dict = Counter(stopwords)\n\ndef preprocess_text(text):\n    text = text.lower() # Convert to lowercase\n    review = re.sub('[^a-zA-Z]',' ', text) # Remove words with non-letter characters\n    words = text.split()\n    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords_dict] # Remove stop words\n    text = \" \".join(words)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['newDescription'] = data.description.apply(preprocess_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Mover's Distance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim.downloader as api\nmodel = api.load('word2vec-google-news-300')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recommend using word mover's distance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# user has watched a title\npick = \"Avengers: Infinity War\"\n\npick_row = data[data.title.str.lower() == pick.lower()]\npick_index = pick_row.index.values[0]\npick_description = pick_row.newDescription.values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get wm distance\ndef getWMD(newText):\n    return model.wmdistance(pick_description, newText)\n\n# select nearest 10\ndef getTopNByWmd(df, col, n):\n    return df.sort_values(by = col).head(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get 10 nearest titles (to our pick) based on Word Mover's Distance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute wm distances\nfilteredData = data[data.index != pick_index]\nfilteredData['wmd'] = filteredData.newDescription.apply(getWMD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getTopNByWmd(filteredData, 'wmd', 10).title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Latent Dirichlet Allocation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize the documents.\nfrom nltk.tokenize import RegexpTokenizer\n\ndocs = data.newDescription.copy()\n\n# Split the documents into tokens.\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in range(len(docs)):\n    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n    \n# Remove words that are only one character.\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute bigrams.\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\nbigram = Phrases(docs, min_count=5, threshold=10)\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.corpora import Dictionary\n\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in docs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train LDA model.\nfrom gensim.models import LdaModel, LdaMulticore\n\n# Set training parameters.\nnum_topics = 15\nchunksize = 2000\npasses = 20\niterations = 100\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimal number of topics","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from gensim.models import CoherenceModel\n\ntopic_size = [1,5,10,15,20,25,30,35,40]\ncoherence_score = []\n\ndef getModelCoherence(n_topics):\n    sample_model = LdaMulticore(corpus=corpus,\n                     id2word=id2word,\n                     num_topics=n_topics,\n                     chunksize=chunksize,\n                     passes=passes,\n                     iterations=10,\n                     per_word_topics=True)\n    \n    cm = CoherenceModel(model=sample_model, corpus=corpus, dictionary=dictionary, coherence=\"u_mass\")\n    return cm.get_coherence()\n\nfor i in topic_size:\n    coherence_score.append(getModelCoherence(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(16, 8))\nplt.plot(topic_size, coherence_score)\n\nplt.title(\"Optimal LDA Model\")\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Coherence Scores\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The elbow method tells us that it is optimal to choose the point where the curve hinges (like an elbow) and begins to flatten. This happens to be around (num of topics = 15).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Final LDA model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_topics = 10\n\nmodel = LdaMulticore(corpus=corpus,\n                     id2word=id2word,\n                     num_topics=num_topics, \n                     chunksize=chunksize,\n                     passes=passes,\n                     random_state=80,\n                     iterations=iterations,\n                     per_word_topics=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, row in data.iterrows():\n    for i in range(0,num_topics):\n        data.at[index,'topic_'+str(i)] = 0\n    for t in model.get_document_topics(corpus[index]):\n        data.at[index,'topic_'+str(t[0])] = t[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recommendation based on LDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# user has watched a title\npick = \"Avengers: Infinity War\"\n\npick_row = data[data.title.str.lower() == pick.lower()]\npick_index = pick_row.index.values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get wm distance\ndef Euclidean(row, n_topics):\n    pick_vec = []\n    row_vec = []\n    for i in range(0,n_topics):\n        pick_vec.append(pick_row.iloc[0]['topic_'+str(i)])\n        row_vec.append(row['topic_'+str(i)])\n        \n    # Get similarity based on top k topics of picked vector\n    k=10\n    \n    top_5_idx = np.argsort(pick_vec)[-k:]\n    pick_vec = np.array(pick_vec)[top_5_idx]\n    row_vec = np.array(row_vec)[top_5_idx]\n    \n    return np.linalg.norm(row_vec - pick_vec)\n\n# select nearest 10\ndef getTopNByLDA(df, col, n):\n    return df.sort_values(by = col).head(n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get 10 nearest titles (to our pick) based on Euclidean distance between the topic distributions. \nWe only select top 5 topics of a document for this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute lda distances\nfilteredData = data.copy()\nfor index, row in filteredData.iterrows():\n    filteredData.at[index,'lda'] = Euclidean(filteredData.iloc[index], num_topics)\n    \nfilteredData = filteredData[filteredData.index != pick_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getTopNByLDA(filteredData, 'lda', 10).title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nIt appears that LDA has recommended much more similar titles based on our pick when compared to a word-embeddings approach.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}