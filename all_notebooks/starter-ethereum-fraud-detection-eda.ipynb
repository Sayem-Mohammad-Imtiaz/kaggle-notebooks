{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1> Ethereum Fraud Detection EDA </h1>\n\n<h3>the purpose of this notebook is to gain a better understanding of the data. The following questions are going to be asked:</h3>\n<h4>Q1. Do we have any missing values?</h4>\n<h4>Q2. Is the data balanced?</h4>\n<h4>Q3. Is the data skewed?</h4>\n<h4>Q4. What feature values often belong to fraud accounts?</h4>\n<h4>Q5. Is our data random or does it follow a certain trend?</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom pandas_profiling import ProfileReport\nfrom pandas.plotting import lag_plot,autocorrelation_plot\n\nsns.set_style('ticks')\npd.set_option('display.max_columns',500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/transaction_dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the `Preprocessor` class being called. All I am doing for now is removing unneeded features"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Preprocessor:\n    \"\"\"\n    This is the base Preprocessor class that will be using for \n    any data preprocessing required\n    \"\"\"\n    def __init__(self,df):\n        self.df = df\n\n    def clean(self):\n        self.remove_features()\n        self.drop_duplicates()\n    \n    def add_columns(self,inference=0):\n        \"\"\"\n        This method adds columns to the data fetched via the REST API\n        Parameters:\n        filename = the name of the file, without the .csv extension\n        Returns:\n        df = A DataFrame of the dataset with columns\n        \"\"\"\n        # Define list of columns\n        cols = ['Index',\n                         'Address',\n                         'FLAG',\n                         'Avg min between sent tnx',\n                         'Avg min between received tnx',\n                         'Time Diff between first and last (Mins)',\n                         'Sent tnx',\n                         'Received Tnx',\n                         'Number of Created Contracts',\n                         'Unique Received From Addresses',\n                         'Unique Sent To Addresses',\n                         'min value received',\n                         'max value received ',\n                         'avg val received',\n                         'min val sent',\n                         'max val sent',\n                         'avg val sent',\n                         'min value sent to contract',\n                         'max val sent to contract',\n                         'avg value sent to contract',\n                         'total transactions (including tnx to create contract',\n                         'total Ether sent',\n                        #  'total ether received',\n                         'total ether sent contracts',\n                         'total ether balance',\n                         ' Total ERC20 tnxs',\n                         ' ERC20 total Ether received',\n                         ' ERC20 total ether sent',\n                         ' ERC20 total Ether sent contract',\n                         ' ERC20 uniq sent addr',\n                         ' ERC20 uniq rec addr',\n                         ' ERC20 uniq sent addr.1',\n                         ' ERC20 uniq rec contract addr',\n                         ' ERC20 avg time between sent tnx',\n                         ' ERC20 avg time between rec tnx',\n                         ' ERC20 avg time between rec 2 tnx',\n                         ' ERC20 avg time between contract tnx',\n                         ' ERC20 min val rec',\n                         ' ERC20 max val rec',\n                         ' ERC20 avg val rec',\n                         ' ERC20 min val sent',\n                         ' ERC20 max val sent',\n                        #  ' ERC20 avg val sent',\n                         ' ERC20 min val sent contract',\n                         ' ERC20 max val sent contract',\n                         ' ERC20 avg val sent contract',\n                         ' ERC20 uniq sent token name',\n                         ' ERC20 uniq rec token name',\n                         ' ERC20 most sent token type',\n                         ' ERC20_most_rec_token_type']\n\n        # Read file,assign cols\n        self.df.columns = cols\n\n    def remove_features(self,inference=False):\n        \"\"\"\n        This method removes unnecessary features\n        Returns:\n        \n        df = a DataFrame without unneeded features\n        \"\"\"\n        # Remove unnecessary fields\n        self.df.drop(['Index','Address', ' ERC20 uniq sent token name',\n ' ERC20 uniq rec token name',\n ' ERC20 most sent token type',\n ' ERC20_most_rec_token_type',' ERC20 min val sent contract',' ERC20 max val sent contract',' ERC20 avg val sent contract','min value sent to contract','max val sent to contract','avg value sent to contract',' ERC20 avg time between sent tnx',' ERC20 avg time between rec tnx',' ERC20 avg time between rec 2 tnx','total ether sent contracts',' ERC20 avg time between contract tnx',' ERC20 total Ether sent contract',' ERC20 uniq sent addr.1'],axis=1,inplace=True)\n    def drop_duplicates(self):\n        self.df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = Preprocessor(df)\npreprocessor.remove_features()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ProfileReport(df,minimal=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Straight away, we can see that there are several features with missing values. Either Imputation or Removal will be required"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also see here that the majority of our features are heavily skewed, so we will have to apply feature engineering and possibly some transformations to the features"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can seew that the features all lie in different ranges. Usually, we would normalise our features before training, however I am going to use a tree-based model, so normalisation is not needed here"},{"metadata":{},"cell_type":"markdown","source":"<h1>Q1. Do we have any missing values?</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 12 features, each missing 829 rows. In other words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of missing rows: ' + str(round(((829/len(df)) * 100),1)) + '%') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8.4% of our data is missing. Possible courses of action:\n\n1. Drop NaN value rows\n2. Impute NaN value rows"},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look the rows with missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.isnull().T.any()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we notice something; all the missing values seem to belong to fraudulent accounts. We can confirm this:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df[df.isnull().T.any()]['FLAG'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our theory is true; All the missing values are of the positive class!"},{"metadata":{},"cell_type":"markdown","source":"<h1>Q2: Is the data balanced?</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['FLAG'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['FLAG'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of non-fraudulent instances: ' + str(round(((7662/len(df)) * 100))) + '%') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Percentage of fraudulent instances: ' + str(round(((2179/len(df)) * 100))) + '%') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see here that the data is heavily imblanced, with only 22% of the accounts considered as fraudulent. Possible courses of action:\n\n1. Oversampling/Undersampling.\n2. Leaving it as it is for the model."},{"metadata":{},"cell_type":"markdown","source":"<h1>Q3. Is the data skewed?</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The answer is yes, and we see that some features, such as `ERC20 avg val sent`, are heavily skewed, with most of the weight being on the left tail. Except `total ether balance`, which is slightly skewed to the right"},{"metadata":{},"cell_type":"markdown","source":"If we plot a KDE plot of `ERC20 avg val sent`:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.kdeplot(df.dropna()[' ERC20 avg val sent'],bw=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get this plot, with some random distribution. However, when we perform a boxcox transformation of the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(boxcox1p(df.dropna()[' ERC20 avg val sent'],boxcox_normmax(df.dropna()[' ERC20 avg val sent'] + 1)), bw=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get data that is normally distributed!"},{"metadata":{},"cell_type":"markdown","source":"<h1>Q4. What feature values often belong to fraud accounts?</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=False,cmap='coolwarm',fmt='')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['FLAG'].sort_values(ascending=False)[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there seems to be no real correlations at all between features, with the highest correled feature being the `Time Diff between first and last (Mins)`, with a correlation of around -0.26. However, there could be some underlying correlations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.barplot(df['Number of Created Contracts'],df['FLAG'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the more contacts a user has created, the more likely they are to be of a fraudulent transaction"},{"metadata":{},"cell_type":"markdown","source":"<h1>Q5. Is our data random or does it follow a certain trend?</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nautocorrelation_plot(df['total ether balance'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nlag_plot(df['total ether balance'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly here we can see that the majority of the data points are random, with the autocorrelation plot showing us that most of the points are located in the 99% confidence band. \n\nThe lag plot shows a similar story, with many of the points clustered at the center, showing us that the data has a few non-zero values, but the points are mainly non-zero, and do not follow any trend"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}