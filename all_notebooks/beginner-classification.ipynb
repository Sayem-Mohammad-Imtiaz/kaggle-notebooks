{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Loan Acceptance Prediction \n\nMy personal goal for this notebook is to complete another EDA on a dataset, and create a guideline with all the steps for a ML Binary Classification Problem. Let me know if you have any questions, I'll try my best to solve them. "},{"metadata":{},"cell_type":"markdown","source":"TOC:\n1. [Problem Framing](#t1.)\n2. [Assess Data Quality & Missing Values](#t2.)\n3. [Exploratory Data Analysis](#t3.)\n4. [Prepare the Data](#t4.)\n5. [Shortlisting Promising ML Models](#t5.)\n6. [Fine-Tune the System](#t6.)\n7. [Presenting the Solution](#t7.)"},{"metadata":{},"cell_type":"markdown","source":"*References:*\n* https://www.kaggle.com/yaheaal/loan-status-with-different-models\n* https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"t1.\"></a>\n## 1. Problem Framing"},{"metadata":{},"cell_type":"markdown","source":"### Problem Definition"},{"metadata":{},"cell_type":"markdown","source":"Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan. Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers.\n\nThis is a standard supervised classification task.A classification problem where we have to predict whether a loan would be approved or not. Below is the dataset attributes with description."},{"metadata":{},"cell_type":"markdown","source":"### The Data"},{"metadata":{},"cell_type":"markdown","source":"This dataset was obtained from Kaggle and is named Loan Prediction Dataset. It contains a set of 613 records under 13 attributes:\n\n* **Loan_ID:** A uniques loan ID\n* **Gender:** Male/Female\n* **Married:** Married(Yes)/Not married(No)  \n* **Dependents:** Number of persons depending on the client.\n* **Education:** Application Education (Graduate / Undergraduate)\n* **Self_Employed:** Self employed (Yes/No)\n* **ApplicantIncome:** Applicant income\n* **CoapplicantIncome:** Coapplicant Income\n* **LoanAmount:** Loan amount in thousands\n* **Loan_Amount_Term:** Term of lean in months\n* **Credit_History:** Credit history meets guidelines\n* **Property_Area:** Urban/Semi and Rural\n* **Loan_Status:** Loan approved (Y,N)\n\nAs we said, the main objective of this probles is to use machine learning techniques to predict loan payments, therefore, our target value is Loan_Status."},{"metadata":{},"cell_type":"markdown","source":"### Preparing the tools"},{"metadata":{"trusted":true},"cell_type":"code","source":"#basic libraries\nimport sys\nimport numpy as np\nimport pandas as pd\nimport os\nimport warnings\n\n#seed the project\nnp.random.seed(64)\n\n#ploting libraries\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nsns.set(context='notebook', style='whitegrid', palette='pastel', font='sans-serif', font_scale=1, color_codes=False, rc=None)\n\n#warning hadle\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Set up completed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import train and test data\ntest_filepath = \"/kaggle/input/loan-prediction-problem-dataset/test_Y3wMUE5_7gLdaTN.csv\"\ntrain_filepath = \"/kaggle/input/loan-prediction-problem-dataset/train_u6lujuX_CVtuZ9i.csv\"\n\ntrain_df = pd.read_csv(train_filepath)\ntest_df = pd.read_csv(test_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the first rows of the training data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the first rows of the test data\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"t2.\"></a>\n## 2. Data Quality & Missing Values Assessment"},{"metadata":{},"cell_type":"markdown","source":"### Data Types\nWe can see our dataset consist on different types of data. As we continue to analyze it, we'll find features which are numerical and should actually be categorial."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Different data types in the dataset\ntrain_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Values\n\nAs you can see we have some missing data, let's have a look how many we have for each column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to see the amount of missing values present in each column.\ntrain_df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot graphic of missing values\nmissingno.matrix(train_df, figsize = (30,10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is important to check which columns contain empty values, as most ML models don't work when these are present. In this case, we can see that the columns Gender, Married, Dependents, Self_Employed, LoanAmount, Loan_Amount_Term and Credit_History have missing values. But as we can confirm, none of these features have enough missing attributes to discard it. But to be able to continue with our analysis we can't have any na value, so we will replace them by the most frequent value."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"t2.\"></a>\n## 3. Exploration Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Exploration of our Target Feature: Loan Status"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('Loan_Status', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Applicant Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='ApplicantIncome', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both groups, the one that had their loan approved and the ones that didn't, seem that have a very similar pattern, therefore, we wouldn't consider the applicant's income as an important feature for our analysis."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Coapplicant Income"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='CoapplicantIncome', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do actually see a different pattern between both groups, so we can say that the Coapplicant Income is a an important feature for this ML modern."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Loan Amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nsns.boxplot(x='Loan_Status', y='LoanAmount', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both groups seem to have a very similar pattern, therefore, we wouldn't consider the loan amount an important feature for our analysis."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Loan Amount Term"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.violinplot(x='Loan_Status', y='Loan_Amount_Term', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at this plots, we can't see a significant difference between the subjects that got the loan accepted and the ones that didn't."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Credit History"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.violinplot(x='Loan_Status', y='Credit_History', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that if you got a Credit History = 1, you have a better chance to get a loan. This is an important feature for our future analysis. But we can clearly see this is actually a categorical variable, as we can see that it is 1 or 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Credit_History'] = train_df['Credit_History'].astype('O')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Gender', hue='Loan_Status', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't see a pattern here, most males got a loan but most females also did. As of now we wouldn't classify it as an important feature for our model."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Married"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Married', hue='Loan_Status', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that if you are married you have a better chance of getting a loan, this seems like a good feature."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Dependents"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Dependents', hue='Loan_Status', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that if the applicant has zero dependent members, they have a higher chance ot get a loan, this also seems like a good feature."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Education"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Education', hue='Loan_Status', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that graduated students have a slight more chance of getting a loan accepted. This might be and important feature."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Self Employed"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Self_Employed', hue='Loan_Status', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you are self-employed or not, you will get almost the same chance to get a loan, we don't see a pattern, therefore, this doesn't seem like an important feature."},{"metadata":{},"cell_type":"markdown","source":"### Exploration of Property Area"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(x='Property_Area', hue='Loan_Status', data=train_df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that living in a semiurban are gives you the biggest chance to get a loan approved. This seems like a good feature."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"t4.\"></a>\n## 4. Preparing the Data"},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"As we saw earlier, there are some features that seem more useful for this project than others. Given this, one of our goals will be to drop the following features: ['Loan_ID','ApplicantIncome','LoanAmount', 'Loan_Amount_Term', 'Gender','Self_Employed']; that were declared not interesting during the previous discussion."},{"metadata":{"trusted":true},"cell_type":"code","source":"# safety copy of our current df\ntrain = train_df.copy()\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numerical Attributes Pipeline"},{"metadata":{},"cell_type":"markdown","source":"To get our dataset ready for our ML algorithms, we need to deal with missing data. In this case we'll opt for replacing NAs with the mean. For simplicity, we do this for all numerical variables."},{"metadata":{},"cell_type":"markdown","source":"A part from dropping the na values, it is also important to note that a few machine learning algorithms are highly sensitive to features that span varying degrees of magnitude, range, and units. This is why feature scaling is a crucial part of the data preprocessing stage."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attribs = ['CoapplicantIncome']\ntrain_num = train[num_attribs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Attributes Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the target column\n\ntarget_values = {'Y': 0 , 'N' : 1}\n\ntarget = train['Loan_Status']\ntrain.drop('Loan_Status', axis=1, inplace=True)\n\ntarget = target.map(target_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Machine Learning algorithms work better with numbers, so we will convert our categorical variables. We can transform our data to have a singular attribute per category. This process is called one-hot encoding (also known as dummy variables/attributes). "},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_attribs = ['Married', 'Dependents', 'Education', 'Property_Area', 'Credit_History']\ntrain_cat = train[cat_attribs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_pipeline = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy=\"most_frequent\", fill_value='missing')),\n        (\"onehot\", OneHotEncoder(handle_unknown='ignore')),\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't forget about our target feature; 'Loan_Status', we need to transform this column."},{"metadata":{},"cell_type":"markdown","source":"### Full Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\npreprocess_pipeline = ColumnTransformer(transformers=[\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = preprocess_pipeline.fit_transform(train)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"t5.\"></a>\n## 5. Shortlisting Promising ML Models"},{"metadata":{},"cell_type":"markdown","source":"We will use 5 of the most used models for training binary classifying ptoblems.\n\n* Logistic Regression\n* KNeighbors Classifier\n* SVC\n* Decision Tree Classifier\n* Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# classify function\nfrom sklearn.model_selection import cross_val_score\ndef classify(model, x, y):\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    model.fit(x_train, y_train)\n    print(\"Accuracy is\", model.score(x_test, y_test)*100)\n    # cross validation - it is used for better validation of model\n    # eg: cv-5, train-4, test-1\n    score = cross_val_score(model, x, y, cv=5)\n    print(\"Cross validation is\",np.mean(score)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we fit and train our models, we will be using cross-validation to have an idea of how good our model is."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(random_state=64)\nclassify(lr_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nkn_model = KNeighborsClassifier()\nclassify(kn_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc_model = SVC(random_state=64)\nclassify(svc_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt_model = DecisionTreeClassifier(random_state=64)\nclassify(dt_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(n_estimators=100, random_state=64)\nclassify(rf_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After seeing these results, we can see that there are 3 models that seem interesting enough to try some hyperparameter tuning to see if they can improve; LR, SVC and Random Forest."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"t6.\"></a>\n## 6. Fine-Tune the Model"},{"metadata":{},"cell_type":"markdown","source":"### Grid Search\nThis function will help us see which hyperparameters work better for our models, by using cross-validation ot evaluate all the possible combinations."},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {\"C\":np.logspace(-3,3,7),  \"penalty\":[\"l1\",\"l2\"]}\n  ]\n\ngrid_search = GridSearchCV(lr_model, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)\nlr_model = grid_search.best_estimator_\nclassify(lr_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SVC Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]},\n    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n]\n\ngrid_search = GridSearchCV(svc_model, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X, y)\nsvc_model  = grid_search.best_estimator_\nclassify(svc_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\ngrid_search = GridSearchCV(rf_model, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(X, y)\nrf_model = grid_search.best_estimator_\nclassify(rf_model, X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After tunning the hyperparameters, we can see that Logistic Regression seems to be offering the best fit."},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{},"cell_type":"markdown","source":"As a final test for our chosen model, it will be useful to see the nymber of correct and incorrect predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(C=10.0, random_state=64)\nmodel.fit(x_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = model.predict(x_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This actually doesn't seem like an ideal result, as we can see a great amount of False Negatives, but for the sake of the practice and learning, we'll leave it as it is."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"t7.\"></a>\n## 7. Presenting the Solution"},{"metadata":{},"cell_type":"markdown","source":"### Let's predict on our test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# safety copy of our the test df\ntest = test_df.copy()\n\nX_test = preprocess_pipeline.transform(test)\nX_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr_model.predict(X_test)\ny_pred","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}