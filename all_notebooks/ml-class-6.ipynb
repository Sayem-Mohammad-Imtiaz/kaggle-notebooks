{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# The topics we will cover in this class are as follows:\n\n* Introduction to <b>Theano</b>.\n* Introduction to <b>TensorFlow</b>.\n* Introduction to <b>Keras</b>.\n* Tensorflow with Keras."},{"metadata":{},"cell_type":"markdown","source":"# Theano\n\nTheano is a Python library for fast numerical computation to aid in the development of deep learning models.\n\nIt knows how to take your structures and turn them into very efficient code that uses NumPy and efficient native libraries to run as fast as possible on CPUs or GPUs.\n\nYou can read more about it here:  http://www.deeplearning.net/software/theano/\n\nSyntax of Theano is symbolic, as shown here:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import theano\nfrom theano import tensor\n\n# declare two symbolic floating-point scalars\na = tensor.dscalar()\nb = tensor.dscalar()\n\n# create a simple expression\nc = a + b\n\n# convert the expression into a callable object that takes (a,b)\n# values as input and computes a value for c\nf = theano.function([a,b], c)\n\n# bind 7.5 to 'a', 3.5 to 'b', and evaluate 'c'\nresult = f(7.5, 3.5)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensorflow\n\nTensorflow is an end to end machine learning platform released by Google.  Like Theano, Tensorflow is also a Python library for fast numerical computation. \n\nUnlike Theano, TensorFlow does have more of a production focus with a capability to run on CPUs, GPUs and even very large clusters.\n\nLearn more about Tensorflow here:  https://www.tensorflow.org/\n\n## tf.estimator API\nUsing tf.estimator dramatically lowers the number of lines of code.\n\ntf.estimator is compatible with the scikit-learn API. Scikit-learn is an extremely popular open-source ML library in Python, with over 100k users, including many at Google.\n\nLearn more about Tensorflow Estimator API here:  https://www.tensorflow.org/guide/estimators"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\n# declare two symbolic floating-point scalars\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\n\n# create a simple symbolic expression using the add function\nadd = tf.add(a, b)\n\n# bind 7.5 to ' a ' , 3.5 to ' b ' , and evaluate ' c '\nsess = tf.Session()\nbinding = {a: 7.5, b: 3.5}\nc = sess.run(add, feed_dict=binding)\nprint(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Minimizing loss in Tensorflow\nhttps://developers.google.com/machine-learning/crash-course/fitter/graph"},{"metadata":{},"cell_type":"markdown","source":"## Tensorflow setup\nLoading the necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nimport math\n\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the dataset\nCalifornia housing example data."},{"metadata":{"trusted":true},"cell_type":"code","source":"california_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Randomizing the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"california_housing_dataframe = california_housing_dataframe.reindex(\n        np.random.permutation(california_housing_dataframe.index))\ncalifornia_housing_dataframe[\"median_house_value\"] /= 1000.0\ncalifornia_housing_dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Describe the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"california_housing_dataframe.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the First Model\nIn this exercise, we'll try to predict median_house_value, which will be our label (sometimes also called a target). We'll use total_rooms as our input feature.\n\nWe'll use Tensorflow Estimators API. Learn more about it here: https://www.tensorflow.org/guide/estimators"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the input feature: total_rooms.\nmy_feature = california_housing_dataframe[[\"total_rooms\"]]\n\n# Configure a numeric feature column for total_rooms.\nfeature_columns = [tf.feature_column.numeric_column(\"total_rooms\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the target label"},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = california_housing_dataframe[\"median_house_value\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configure the LinearRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use gradient descent as the optimizer for training the model.\nmy_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n\n# Configure the linear regression model with our feature columns and optimizer.\n# Set a learning rate of 0.0000001 for Gradient Descent.\nlinear_regressor = tf.estimator.LinearRegressor(\n    feature_columns=feature_columns,\n    optimizer=my_optimizer\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define the Input Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    \"\"\"Trains a linear regression model of one feature.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    \"\"\"\n  \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(buffer_size=10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = linear_regressor.train(\n    input_fn = lambda:my_input_fn(my_feature, targets),\n    steps=100\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an input function for predictions.\n# Note: Since we're making just one prediction for each example, we don't \n# need to repeat or shuffle the data here.\nprediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n\n# Call predict() on the linear_regressor to make predictions.\npredictions = linear_regressor.predict(input_fn=prediction_input_fn)\n\n# Format predictions as a NumPy array, so we can calculate error metrics.\npredictions = np.array([item['predictions'][0] for item in predictions])\n\n# Print Mean Squared Error and Root Mean Squared Error.\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nroot_mean_squared_error = math.sqrt(mean_squared_error)\nprint(\"Mean Squared Error (on training data): %0.3f\" % mean_squared_error)\nprint(\"Root Mean Squared Error (on training data): %0.3f\" % root_mean_squared_error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"comparing the RMSE to the difference of the min and max of our targets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"min_house_value = california_housing_dataframe[\"median_house_value\"].min()\nmax_house_value = california_housing_dataframe[\"median_house_value\"].max()\nmin_max_difference = max_house_value - min_house_value\n\nprint(\"Min. Median House Value: %0.3f\" % min_house_value)\nprint(\"Max. Median House Value: %0.3f\" % max_house_value)\nprint(\"Difference between Min. and Max.: %0.3f\" % min_max_difference)\nprint(\"Root Mean Squared Error: %0.3f\" % root_mean_squared_error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looking at how well our predictions match our targets, in terms of overall summary statistics."},{"metadata":{"trusted":true},"cell_type":"code","source":"calibration_data = pd.DataFrame()\ncalibration_data[\"predictions\"] = pd.Series(predictions)\ncalibration_data[\"targets\"] = pd.Series(targets)\ncalibration_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll get a uniform random sample of the data so we can make a readable scatter plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = california_housing_dataframe.sample(n=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we'll plot the line we've learned, drawing from the model's bias term and feature weight, together with the scatter plot. The line will show up red."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the min and max total_rooms values.\nx_0 = sample[\"total_rooms\"].min()\nx_1 = sample[\"total_rooms\"].max()\n\n# Retrieve the final weight and bias generated during training.\nweight = linear_regressor.get_variable_value('linear/linear_model/total_rooms/weights')[0]\nbias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n\n# Get the predicted median_house_values for the min and max total_rooms values.\ny_0 = weight * x_0 + bias \ny_1 = weight * x_1 + bias\n\n# Plot our regression line from (x_0, y_0) to (x_1, y_1).\nplt.plot([x_0, x_1], [y_0, y_1], c='r')\n\n# Label the graph axes.\nplt.ylabel(\"median_house_value\")\nplt.xlabel(\"total_rooms\")\n\n# Plot a scatter plot from our data sample.\nplt.scatter(sample[\"total_rooms\"], sample[\"median_house_value\"])\n\n# Display graph.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This initial line looks way off."},{"metadata":{},"cell_type":"markdown","source":"## Tweak the Model Hyperparameters\nWe'll proceed in 10 evenly divided periods so that we can observe the model improvement at each period."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(learning_rate, steps, batch_size, input_feature=\"total_rooms\"):\n  \"\"\"Trains a linear regression model of one feature.\n  \n  Args:\n    learning_rate: A `float`, the learning rate.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    input_feature: A `string` specifying a column from `california_housing_dataframe`\n      to use as input feature.\n  \"\"\"\n  \n  periods = 10\n  steps_per_period = steps / periods\n\n  my_feature = input_feature\n  my_feature_data = california_housing_dataframe[[my_feature]]\n  my_label = \"median_house_value\"\n  targets = california_housing_dataframe[my_label]\n\n  # Create feature columns.\n  feature_columns = [tf.feature_column.numeric_column(my_feature)]\n  \n  # Create input functions.\n  training_input_fn = lambda:my_input_fn(my_feature_data, targets, batch_size=batch_size)\n  prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=False)\n  \n  # Create a linear regressor object.\n  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  linear_regressor = tf.estimator.LinearRegressor(\n      feature_columns=feature_columns,\n      optimizer=my_optimizer\n  )\n\n  # Set up to plot the state of our model's line each period.\n  plt.figure(figsize=(15, 6))\n  plt.subplot(1, 2, 1)\n  plt.title(\"Learned Line by Period\")\n  plt.ylabel(my_label)\n  plt.xlabel(my_feature)\n  sample = california_housing_dataframe.sample(n=300)\n  plt.scatter(sample[my_feature], sample[my_label])\n  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n\n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(\"Training model...\")\n  print(\"RMSE (on training data):\")\n  root_mean_squared_errors = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    linear_regressor.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n    # Take a break and compute predictions.\n    predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n    predictions = np.array([item['predictions'][0] for item in predictions])\n    \n    # Compute loss.\n    root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(predictions, targets))\n    # Occasionally print the current loss.\n    print(\"  period %02d : %0.2f\" % (period, root_mean_squared_error))\n    # Add the loss metrics from this period to our list.\n    root_mean_squared_errors.append(root_mean_squared_error)\n    # Finally, track the weights and biases over time.\n    # Apply some math to ensure that the data and line are plotted neatly.\n    y_extents = np.array([0, sample[my_label].max()])\n    \n    weight = linear_regressor.get_variable_value('linear/linear_model/%s/weights' % input_feature)[0]\n    bias = linear_regressor.get_variable_value('linear/linear_model/bias_weights')\n\n    x_extents = (y_extents - bias) / weight\n    x_extents = np.maximum(np.minimum(x_extents,\n                                      sample[my_feature].max()),\n                           sample[my_feature].min())\n    y_extents = weight * x_extents + bias\n    plt.plot(x_extents, y_extents, color=colors[period]) \n  print(\"Model training finished.\")\n\n  # Output a graph of loss metrics over periods.\n  plt.subplot(1, 2, 2)\n  plt.ylabel('RMSE')\n  plt.xlabel('Periods')\n  plt.title(\"Root Mean Squared Error vs. Periods\")\n  plt.tight_layout()\n  plt.plot(root_mean_squared_errors)\n\n  # Output a table with calibration data.\n  calibration_data = pd.DataFrame()\n  calibration_data[\"predictions\"] = pd.Series(predictions)\n  calibration_data[\"targets\"] = pd.Series(targets)\n  display.display(calibration_data.describe())\n\n  print(\"Final RMSE (on training data): %0.2f\" % root_mean_squared_error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(\n    learning_rate=0.00002,\n    steps=500,\n    batch_size=5\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Try a different feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(\n    learning_rate=0.00002,\n    steps=1000,\n    batch_size=5,\n    input_feature=\"population\"\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Synthetic features"},{"metadata":{"trusted":true},"cell_type":"code","source":"california_housing_dataframe[\"rooms_per_person\"] = (\n    california_housing_dataframe[\"total_rooms\"] / california_housing_dataframe[\"population\"])\n\ncalibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Identify Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplot(1, 2, 2)\n_ = california_housing_dataframe[\"rooms_per_person\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete the outliers\ncalifornia_housing_dataframe[\"rooms_per_person\"] = (\n    california_housing_dataframe[\"rooms_per_person\"]).apply(lambda x: min(x, 5))\n\n_ = california_housing_dataframe[\"rooms_per_person\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"to verify, we'll retrain and print the caliberation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"calibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=\"rooms_per_person\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Evaluation\nFor the next section of the code, we'll use multiple features, and not just one feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nimport math\n\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format\n\ncalifornia_housing_dataframe = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n\ndef preprocess_features(california_housing_dataframe):\n  \"\"\"Prepares input features from California housing data set.\n\n  Args:\n    california_housing_dataframe: A Pandas DataFrame expected to contain data\n      from the California housing data set.\n  Returns:\n    A DataFrame that contains the features to be used for the model, including\n    synthetic features.\n  \"\"\"\n  selected_features = california_housing_dataframe[\n    [\"latitude\",\n     \"longitude\",\n     \"housing_median_age\",\n     \"total_rooms\",\n     \"total_bedrooms\",\n     \"population\",\n     \"households\",\n     \"median_income\"]]\n  processed_features = selected_features.copy()\n  # Create a synthetic feature.\n  processed_features[\"rooms_per_person\"] = (\n    california_housing_dataframe[\"total_rooms\"] /\n    california_housing_dataframe[\"population\"])\n  return processed_features\n\ndef preprocess_targets(california_housing_dataframe):\n  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n\n  Args:\n    california_housing_dataframe: A Pandas DataFrame expected to contain data\n      from the California housing data set.\n  Returns:\n    A DataFrame that contains the target feature.\n  \"\"\"\n  output_targets = pd.DataFrame()\n  # Scale the target to be in units of thousands of dollars.\n  output_targets[\"median_house_value\"] = (\n    california_housing_dataframe[\"median_house_value\"] / 1000.0)\n  return output_targets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the training set, we'll choose the first 12000 examples, out of the total of 17000."},{"metadata":{"trusted":true},"cell_type":"code","source":"training_examples = preprocess_features(california_housing_dataframe.head(12000))\ntraining_examples.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_targets = preprocess_targets(california_housing_dataframe.head(12000))\ntraining_targets.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the validation set, we'll choose the last 5000 examples, out of the total of 17000."},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_examples = preprocess_features(california_housing_dataframe.tail(5000))\nvalidation_examples.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\nvalidation_targets.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate and train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    \"\"\"Trains a linear regression model of multiple features.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    \"\"\"\n    \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def construct_feature_columns(input_features):\n  \"\"\"Construct the TensorFlow Feature Columns.\n\n  Args:\n    input_features: The names of the numerical input features to use.\n  Returns:\n    A set of feature columns\n  \"\"\" \n  return set([tf.feature_column.numeric_column(my_feature)\n              for my_feature in input_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(\n    learning_rate,\n    steps,\n    batch_size,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  \"\"\"Trains a linear regression model of multiple features.\n  \n  In addition to training, this function also prints training progress information,\n  as well as a plot of the training and validation loss over time.\n  \n  Args:\n    learning_rate: A `float`, the learning rate.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    training_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for training.\n    training_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for training.\n    validation_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for validation.\n    validation_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for validation.\n      \n  Returns:\n    A `LinearRegressor` object trained on the training data.\n  \"\"\"\n\n  periods = 10\n  steps_per_period = steps / periods\n  \n  # Create a linear regressor object.\n  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  linear_regressor = tf.estimator.LinearRegressor(\n      feature_columns=construct_feature_columns(training_examples),\n      optimizer=my_optimizer\n  )\n  \n  # Create input functions.\n  training_input_fn = lambda: my_input_fn(\n      training_examples, \n      training_targets[\"median_house_value\"], \n      batch_size=batch_size)\n  predict_training_input_fn = lambda: my_input_fn(\n      training_examples, \n      training_targets[\"median_house_value\"], \n      num_epochs=1, \n      shuffle=False)\n  predict_validation_input_fn = lambda: my_input_fn(\n      validation_examples, validation_targets[\"median_house_value\"], \n      num_epochs=1, \n      shuffle=False)\n\n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(\"Training model...\")\n  print(\"RMSE (on training data):\")\n  training_rmse = []\n  validation_rmse = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    linear_regressor.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period,\n    )\n    # Take a break and compute predictions.\n    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n    training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n    \n    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n    \n    \n    # Compute training and validation loss.\n    training_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(training_predictions, training_targets))\n    validation_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(validation_predictions, validation_targets))\n    # Occasionally print the current loss.\n    print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n    # Add the loss metrics from this period to our list.\n    training_rmse.append(training_root_mean_squared_error)\n    validation_rmse.append(validation_root_mean_squared_error)\n  print(\"Model training finished.\")\n\n  # Output a graph of loss metrics over periods.\n  plt.ylabel(\"RMSE\")\n  plt.xlabel(\"Periods\")\n  plt.title(\"Root Mean Squared Error vs. Periods\")\n  plt.tight_layout()\n  plt.plot(training_rmse, label=\"training\")\n  plt.plot(validation_rmse, label=\"validation\")\n  plt.legend()\n\n  return linear_regressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_regressor = train_model(\n    learning_rate=0.00003,\n    steps=500,\n    batch_size=5,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate on Test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"california_housing_test_data = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\", sep=\",\")\n\ntest_examples = preprocess_features(california_housing_test_data)\ntest_targets = preprocess_targets(california_housing_test_data)\n\npredict_test_input_fn = lambda: my_input_fn(\n      test_examples, \n      test_targets[\"median_house_value\"], \n      num_epochs=1, \n      shuffle=False)\n\ntest_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)\ntest_predictions = np.array([item['predictions'][0] for item in test_predictions])\n\nroot_mean_squared_error = math.sqrt(\n    metrics.mean_squared_error(test_predictions, test_targets))\n\nprint(\"Final RMSE (on test data): %0.2f\" % root_mean_squared_error)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keras\n\nKeras is a high-level API to build and train deep learning models. The Keras library provides a wrapper for both Theano and TensorFlow.  It provides a clean and simple API that allows you to define and evaluate deep learning models in just a few lines of code.  It is easy to use and leverages the power of Theano and Tensorflow.\n\nIt's used for fast prototyping, advanced research, and production, with three key advantages:\n\n* **User friendly:** Keras has a simple, consistent interface optimized for common use cases. It provides clear and actionable feedback for user errors.\n* **Modular and composable:** Keras models are made by connecting configurable building blocks together, with few restrictions.\n* **Easy to extend:** Write custom building blocks to express new ideas for research. Create new layers, loss functions, and develop state-of-the-art models.\n\nMore information about Keras available on their website:  https://keras.io/"},{"metadata":{},"cell_type":"markdown","source":"## **HOMEWORK**"},{"metadata":{},"cell_type":"markdown","source":"# Tensorflow with Keras\n\n### Example: Handwritten Digit Recognition\nHandwriting digit recognition is a difficult computer vision classification problem.\n\nThe **MNIST dataset** is a standard problem for evaluating algorithms on the problem of handwriting digit recognition. It contains 60,000 images of digits that can be used to train a model, and 10,000 images that can be used to evaluate its performance.\n\n## Import tf.keras\ntf.keras is TensorFlow's implementation of the Keras API specification. \n> tf.keras makes TensorFlow easier to use without sacrificing flexibility and performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\nprint(tf.VERSION)\nprint(tf.keras.__version__)\n\nmnist = tf.keras.datasets.mnist\n\n# Load and pepare the MNIST data\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sequential model\nIn Keras, you assemble layers to build models. A model is (usually) a graph of layers. The most common type of model is a stack of layers: the **tf.keras.Sequential** model.\n\nTo build a simple, fully-connected network (i.e. multi-layer perceptron), we can do this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential()\n# Adds a densely-connected layer with 64 units to the model:\nmodel.add(layers.Dense(64, activation='relu'))\n# Add another:\nmodel.add(layers.Dense(64, activation='relu'))\n# Add a softmax layer with 10 output units:\nmodel.add(layers.Dense(10, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Configure the layers\nThere are many tf.keras.layers available with some common constructor parameters:\n\n* **activation**: Set the activation function for the layer. This parameter is specified by the name of a built-in function or as a callable object. By default, no activation is applied.\n* **kernel_initializer and bias_initializer:** The initialization schemes that create the layer's weights (kernel and bias). This parameter is a name or a callable object. This defaults to the \"Glorot uniform\" initializer.\n* **kernel_regularizer and bias_regularizer:** The regularization schemes that apply the layer's weights (kernel and bias), such as L1 or L2 regularization. By default, no regularization is applied.\n\nThe following instantiates tf.keras.layers.Dense layers using constructor arguments:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a sigmoid layer:\nlayers.Dense(64, activation='sigmoid')  \n# Or: layers.Dense(64, activation=tf.sigmoid)\n\n# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\nlayers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n\n# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\nlayers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))\n\n# A linear layer with a kernel initialized to a random orthogonal matrix:\nlayers.Dense(64, kernel_initializer='orthogonal')\n\n# A linear layer with a bias vector initialized to 2.0s:\nlayers.Dense(64, bias_initializer=tf.keras.initializers.constant(2.0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set up training\nAfter the model is constructed, configure its learning process by calling the compile method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n# Adds a densely-connected layer with 64 units to the model:\nlayers.Dense(64, activation='relu', input_shape=(32,)),\n# Add another:\nlayers.Dense(64, activation='relu'),\n# Add a softmax layer with 10 output units:\nlayers.Dense(10, activation='softmax')])\n\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"tf.keras.Model.compile takes three important arguments:\n\n* **optimizer:** This object specifies the training procedure. Pass it optimizer instances from the tf.train module, such as tf.train.AdamOptimizer, tf.train.RMSPropOptimizer, or tf.train.GradientDescentOptimizer.\n* **loss:** The function to minimize during optimization. Common choices include mean square error (mse), categorical_crossentropy, and binary_crossentropy. Loss functions are specified by name or by passing a callable object from the tf.keras.losses module.\n* **metrics:** Used to monitor training. These are string names or callables from the tf.keras.metrics module.\n\nThe following shows a few examples of configuring a model for training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure a model for mean-squared error regression.\nmodel.compile(optimizer=tf.train.AdamOptimizer(0.01),\n              loss='mse',       # mean squared error\n              metrics=['mae'])  # mean absolute error\n\n# Configure a model for categorical classification.\nmodel.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n              loss=tf.keras.losses.categorical_crossentropy,\n              metrics=[tf.keras.metrics.categorical_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Input NumPy data\nFor small datasets, use in-memory NumPy arrays to train and evaluate a model. The model is \"fit\" to the training data using the fit method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef random_one_hot_labels(shape):\n  n, n_class = shape\n  classes = np.random.randint(0, n_class, n)\n  labels = np.zeros((n, n_class))\n  labels[np.arange(n), classes] = 1\n  return labels\n\ndata = np.random.random((1000, 32))\nlabels = random_one_hot_labels((1000, 10))\n\nmodel.fit(data, labels, epochs=10, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**tf.keras.Model.fit** takes three important arguments:\n\n* **epochs:** Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).\n* **batch_size:** When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.\n* **validation_data:** When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument—a tuple of inputs and labels—allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch.\n\nHere's an example using validation_data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndata = np.random.random((1000, 32))\nlabels = random_one_hot_labels((1000, 10))\n\nval_data = np.random.random((100, 32))\nval_labels = random_one_hot_labels((100, 10))\n\nmodel.fit(data, labels, epochs=10, batch_size=32,\n          validation_data=(val_data, val_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Input tf.data datasets\nUse the Datasets API to scale to large datasets or multi-device training. Pass a tf.data.Dataset instance to the fit method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiates a toy dataset instance:\ndataset = tf.data.Dataset.from_tensor_slices((data, labels))\ndataset = dataset.batch(32)\ndataset = dataset.repeat()\n\n# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\nmodel.fit(dataset, epochs=10, steps_per_epoch=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, the fit method uses the steps_per_epoch argument—this is the number of training steps the model runs before it moves to the next epoch. Since the Dataset yields batches of data, this snippet does not require a batch_size.\n\nDatasets can also be used for validation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((data, labels))\ndataset = dataset.batch(32).repeat()\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\nval_dataset = val_dataset.batch(32).repeat()\n\nmodel.fit(dataset, epochs=10, steps_per_epoch=30,\n          validation_data=val_dataset,\n          validation_steps=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate and predict\nThe tf.keras.Model.evaluate and tf.keras.Model.predict methods can use NumPy data and a tf.data.Dataset.\n\nTo evaluate the inference-mode loss and metrics for the data provided:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.random.random((1000, 32))\nlabels = random_one_hot_labels((1000, 10))\n\nmodel.evaluate(data, labels, batch_size=32)\n\nmodel.evaluate(dataset, steps=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And to predict the output of the last layer in inference for the data provided, as a NumPy array:"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(data, batch_size=32)\nprint(result.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}