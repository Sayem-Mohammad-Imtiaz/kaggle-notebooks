{"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","file_extension":".py","name":"python","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"1bd2304a-c67d-4bf9-990b-384a09ce2e08","_uuid":"fd68d27cad97bcbf0183c305d6b5acbb9e672c77"},"cell_type":"markdown","source":"# Exploratory Analysis of Vehicle-for-Hire Data\n\nThis notebook contains an exploratory analysis of the [Uber Pickups in New York City](https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city) data set.\n\nThe analysis looks at a single month of pick-up data.  \n\nThe analysis is broken up into five sections:\n\n- [Data Prep](#data_prep): Load the data into pandas DataFrame and extract features.\n- [Initial Exploration](#initial): Calculate hourly averages for each weekday and plot with seaborn.\n- [Spatial Visualization](#spatial): Implement spatial binning and plot with Basemap.\n- [Detailed Interactive Plotting](#leaflet): Plot raw data on a Leaflet using mplleaflet.\n- [Clustering](#clustering): Cluster data with DBSCAN and plot hot spots on Leaflet.\n\nThe heat map was inspired by **[Dotman](https://www.kaggle.com/dotman/data-exploration-and-visualization)**.\n\nThe clustering was inspired by **[Geoff Boeing](http://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/)**.\n\n#### Package Management"},{"metadata":{"_cell_guid":"1f1b568c-1d61-497e-86c5-0e05ebe613c8","collapsed":true,"_uuid":"a8f56f8486b67114f52e81718eb8c4ac7ef6eebc"},"execution_count":null,"source":"import time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nfrom matplotlib import cm\nimport mplleaflet as mpl\n%matplotlib inline\n\nfrom sklearn.cluster import DBSCAN\nfrom geopy.distance import great_circle\nfrom shapely.geometry import MultiPoint","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"c4408671-d114-4192-a7f3-2a8cbcd4bbe8","_uuid":"d88db2a9a287c681b39c03cba24e5eaad78a9b87"},"cell_type":"markdown","source":"<a id=\"data_prep\"></a>\n## Data Prep\n\nLoad data into **[pandas](http://pandas.pydata.org/)** DataFrame and get a high level summary.  My data is saved in a directory `../input/` which is how I like it."},{"metadata":{"_cell_guid":"8237aa2c-df6c-45ec-a208-d4a03807c29b","_uuid":"e59e2609cb7c95830d719d87122468f5400e6c34"},"execution_count":null,"source":"rides = pd.read_csv('../input/uber-raw-data-aug14.csv')\nrides.info()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"7c26936f-f4a7-471a-ae4e-afa251c432d1","_uuid":"476e7a90dd198e496e096311ed65b269117d49e1"},"cell_type":"markdown","source":"We'll look at the data to gain more intuition."},{"metadata":{"_cell_guid":"5651f8bb-65c7-470d-ad71-bb9c2ebbdd28","_uuid":"a686a8ae3af3bebddcd8cf451c890367fa052b5a"},"execution_count":null,"source":"rides.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"3bfb49e3-c539-4ff0-96e3-3059ae027a1b","collapsed":true,"_uuid":"98d92554a7823e80982496fc1f192f1134617529"},"cell_type":"markdown","source":"If possible, I like to remove special characters and capital letters from the column names."},{"metadata":{"_cell_guid":"c7db0a40-99da-435d-bb2b-d73155e20b88","collapsed":true,"_uuid":"fcc8045e0ee875ec4a4b92f7d2a1567748460be3"},"execution_count":null,"source":"rides.columns = ['timestamp', 'lat', 'lon', 'base']","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cc960ee1-7787-4262-8595-8adfb6d20bdb","_uuid":"f24828db20aca2026bb787a8f0011d29a90e3a15"},"cell_type":"markdown","source":"Then we'll convert the \"timestamp\" entries to pandas Timestamp objects. "},{"metadata":{"_cell_guid":"ede68fef-3c45-46ab-9dea-32df553860bb","_uuid":"a624ee2267fe571e74bcc407a6b6540cab720281"},"execution_count":null,"source":"ti = time.time()\n\nrides['timestamp'] = pd.to_datetime(rides['timestamp'])\n\ntf = time.time()\nprint(tf-ti,' seconds.')","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"499de909-9dcd-4bfd-8245-d6c6482cf4dc","_uuid":"80cbd69cb1fc16b1e5cb92d491e41922294df7d2"},"cell_type":"markdown","source":"Goodness! That took a long time.  Let's go ahead and save the data in a convenient format so we can load it to a new/reset kernel without having to convert the timestamp again.\n\nThe top line in the cell below will \"pickle\" the data.  The bottom line will read the `.pkl` file to a pandas DataFrame. "},{"metadata":{"_cell_guid":"611440a1-e3d0-4ad4-a0d8-74ae8767a388","collapsed":true,"_uuid":"e58e6ae06c6e700dc4b18751e58a5b0b166a79c3"},"execution_count":null,"source":"rides.to_pickle('./test_data.pkl')\n# rides = pd.read_pickle('./test_data.pkl')","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"52fabf91-0a3b-4174-8ae8-e536170e759e","_uuid":"f6ecb17726dc68a592aebefc091d3f227e3c5ba3"},"cell_type":"markdown","source":"We *could* try to use the Timestamp objects for the sorting and grouping which is required to make sense of this data set, but it will be much simpler to extract some features up front.  We'll unpack the Timestamp into individual features using datetime methods.  We want the weekday name, the month number, day number, hour number, and minute number all in separate columns.  \n\nWe can also use `pandas.Series.dt.strftime` to extract additional features with customized formatting from the timestamp.  It's not necessary for this analysis but I'll demonstrate it below.  For now the lines will be commented out."},{"metadata":{"_cell_guid":"14eddb74-d5b9-4f52-a21e-97c97fb1d9a7","_uuid":"e45fdd926de4fb8c60d33f97fde1b3eafa36e3c8"},"execution_count":null,"source":"rides['weekday'] = rides.timestamp.dt.weekday_name\nrides['month'] = rides.timestamp.dt.month\nrides['day'] = rides.timestamp.dt.day\nrides['hour'] = rides.timestamp.dt.hour\nrides['minute'] = rides.timestamp.dt.minute\n\n## customized features\n# rides['month_name'] = rides.timestamp.dt.strftime('%B')\n# rides['day_hour'] = rides.timestamp.dt.strftime('%d-%H')\n\n## ocular analysis\nrides.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"6a1764be-1ef2-489f-8444-9cfcfd325cc6","_uuid":"5f94aa2ff1e30206c20c54d02d795a80c2366d82"},"cell_type":"markdown","source":"Pandas lets us define an ordered categorical index.  We'll choose the weekday; this will help with sorting later on."},{"metadata":{"_cell_guid":"0450d18d-b0b5-477a-b57b-968d4c23bffb","collapsed":true,"_uuid":"7f9db0783e3035618bee333f25e3544c62a24a57"},"execution_count":null,"source":"day_map = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday', 'Sunday']\n\nrides['weekday'] = pd.Categorical(rides['weekday'], categories=day_map, ordered=True)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"c52961da-64bb-45a4-80f5-2be2ece53a11","collapsed":true,"_uuid":"bf8fb8561dbf44e03e75585545866b343893c006"},"cell_type":"markdown","source":"<a id=\"initial\"></a>\n## Initial Exploration\n\nIn this section we'll aggregate the data into more useful metrics.  Specifically, we'll calculate the total number of rides per hour for every hour in the month, and then use that data to calculate the average number of rides for each hour on a given weekday.\n\n#### Hourly Ride Data\n\nWe'll count the rides for every hour in the month and retain the weekday feature.  The `groupby` operation will assign a hierarchical index which we can reset.  Also, we'll rename the column to indicate that it's a count of the total number of rides matching the criteria."},{"metadata":{"_cell_guid":"3a1aea06-d2d9-4d54-8622-8c6fc226ebf6","_uuid":"fceb98e93ce0bdff2523713a6658a293d3354009"},"execution_count":null,"source":"## groupby operation\nhourly_ride_data = rides.groupby(['day','hour','weekday'])['timestamp'].count()\n\n## reset index\nhourly_ride_data = hourly_ride_data.reset_index()\n\n## rename column\nhourly_ride_data = hourly_ride_data.rename(columns = {'timestamp':'ride_count'})\n\n## ocular analysis\nhourly_ride_data.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d09a9fe2-5687-4b40-9d95-a3db2428f306","collapsed":true,"_uuid":"8705745f07265adb6c6236b72a3613c534b88de8"},"cell_type":"markdown","source":"#### Weekday Hourly Averages\n\nNext, we'll use the hourly data to compute hourly averages for each weekday.  We'll sort the data on the previously defined weekday index for clean plotting."},{"metadata":{"_cell_guid":"44bd1557-e8e8-45b5-929e-18c261f8ee09","_uuid":"488c985bb7d07dd392df067405d472c523ff9c67"},"execution_count":null,"source":"## groupby operation\nweekday_hourly_avg = hourly_ride_data.groupby(['weekday','hour'])['ride_count'].mean()\n\n## reset index\nweekday_hourly_avg = weekday_hourly_avg.reset_index()\n\n## rename column\nweekday_hourly_avg = weekday_hourly_avg.rename(columns = {'ride_count':'average_rides'})\n\n## sort by categorical index\nweekday_hourly_avg = weekday_hourly_avg.sort_index()\n\n## ocular analysis\nweekday_hourly_avg.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"87f3a5ed-26c9-4902-b2d3-def26c42b00f","_uuid":"1facda70eca5d70affed4c261cc4b3d5ca70cc81"},"cell_type":"markdown","source":"#### Define Color Palette\n\nSeaborn's default color blind palette only has six colors, which will cause a repeat for our weekday categorical data. The default color scheme is too bright for my taste, and while you can manually adjust the brightness and saturation in seaborn, it's easy to create colors which are not distinguishable for people with some degree of color blindness when using this approach.\n\nInstead of wasting time hacking together a \"functional\" color palette, we'll use the *Color Blind 10* from __[Tableau](http://tableaufriction.blogspot.com/2012/11/finally-you-can-use-tableau-data-colors.html)__, with code from __[Randal Olson](http://www.randalolson.com/2014/06/28/how-to-make-beautiful-data-visualizations-in-python-with-matplotlib/)__ to convert from the published 0-255 format to the 0-1 format which matplotlib accepts."},{"metadata":{"_cell_guid":"b857024b-7780-4bb3-97f3-ba169ec1855b","collapsed":true,"_uuid":"bf5451c6cf7b07df80507ac046b4511feda07e33"},"execution_count":null,"source":"tableau_color_blind = [(0, 107, 164), (255, 128, 14), (171, 171, 171), (89, 89, 89),\n             (95, 158, 209), (200, 82, 0), (137, 137, 137), (163, 200, 236),\n             (255, 188, 121), (207, 207, 207)]\n\nfor i in range(len(tableau_color_blind)):  \n    r, g, b = tableau_color_blind[i]  \n    tableau_color_blind[i] = (r / 255., g / 255., b / 255.)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"0c26dd87-93ba-4acf-bcf0-ca811e730cd9","collapsed":true,"_uuid":"c227b8347bc3ca9b529fe2cf8384c105a4e6eee6"},"cell_type":"markdown","source":"#### seaborn\n\nThe [seaborn](https://seaborn.pydata.org/) package makes visualizing this data quick and convenient.  With a single line of code, we'll plot the average pickup rate vs. hour of the day, for each individual day of the week.  We can conveniently implement our custom *Color Blind 10* color palette and format the plot using familiar matplotlib syntax."},{"metadata":{"_cell_guid":"0c9eebb6-8b25-4d17-8139-a2a1e467963f","_uuid":"5b4da333a77d19089b4d9f7891a56bec1ffc6208"},"execution_count":null,"source":"## create figure\nfig = plt.figure(figsize=(12,6))\nax = fig.add_subplot(111)\n\n## set palette   \ncurrent_palette = sns.color_palette(tableau_color_blind)\n\n## plot data\nsns.pointplot(ax=ax, x='hour',y='average_rides',hue='weekday', \n              palette = current_palette, data = weekday_hourly_avg)\n\n## clean up the legend\nl = ax.legend()\nl.set_title('')\n\n## format plot labels\nax.set_title('Weekday Averages for August 2014', fontsize=30)\nax.set_ylabel('Rides per Hour', fontsize=20)\nax.set_xlabel('Hour', fontsize=20)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"3e766e8d-c5c2-4090-a4a6-051fba279d3f","_uuid":"572476dab2f976eb6f56a678ae4c70efb36537b9"},"cell_type":"markdown","source":"Doesn't that look nice?\n\nWeekdays see a skewed bimodal distribution, with a smaller peak during the morning commute and a larger peak beginning at the close of business and continuing through evening social hours.  Weekends do not experience a morning rush, but rise steadily throughout the day before diverging in the early evening.  Thursday shows the largest evening volume.  However, while Friday and Saturday nights plateau at lower levels than Thursday, the carryover into the early morning party hours (Sat/Sun morning 0-2 AM) is much more significant.\n\n<a id=\"spatial\"></a>\n## Spatial Visualization\n\nCan we say anything about the location of these rides?\n\nLet's build a function to help us plot the data on a map.  We'll utilize the **[Basemap](https://matplotlib.org/basemap/)** package to bin the results based on location to create a heat map.  Basemap will use the **[hexbin](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hexbin)** method from matplotlib for the spatial binning.  We'll set the `zorder` parameter so that the heat map is plotted on top of the continent fill."},{"metadata":{"_cell_guid":"04ab5b76-b1b7-458d-9d31-42a114dd2c73","collapsed":true,"_uuid":"396fceaeb721509b8512fb5e92c7b9a7169414ec"},"execution_count":null,"source":"def heat_map(ax_loc,title_str,rides_this_hour,nsew):\n    \n    ## get the axis\n    ax = fig.add_subplot(ax_loc)\n\n    ## make the basemap object\n    m = Basemap(projection='merc', urcrnrlat=nsew[0], llcrnrlat=nsew[1],\n                urcrnrlon=nsew[2], llcrnrlon=nsew[3], lat_ts=nsew[1], resolution='f')\n\n    ## draw the background features\n    m.drawmapboundary(fill_color = 'xkcd:light blue')\n    m.fillcontinents(color='xkcd:grey', zorder = 1)\n    m.drawcoastlines()\n    m.drawrivers()\n\n    ## project the GPS coordinates onto the x,y representation\n    x, y = m(rides_this_hour['lon'].values, rides_this_hour['lat'].values)\n\n    ## count the instances using the hexbin method and plot the results\n    m.hexbin(x, y, gridsize=1000, mincnt = 1, bins = 'log', cmap=cm.YlOrRd, zorder = 2);\n\n    ## set the title\n    ax.set_title(title_str, fontsize=24)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"687e6b68-568f-4a0f-9d72-3a866043947d","_uuid":"855de94f887effdd23e1e7cd72188c0a38f3f00e"},"cell_type":"markdown","source":"#### Ask the Question\n\nIn the weekday average plot above, we see simlar levels of rides at 5pm and 9pm on Thursday, but are they happening in the same location?  We'll call our `heat_map` function at different times to plot the respective spatially binned ride distributions on a map."},{"metadata":{"_cell_guid":"d9691bea-4b04-4d80-ba71-6e8728c762dc","_uuid":"8f2166860c022ae01be2be989b6701c49a3076b8"},"execution_count":null,"source":"## set weekday for analysis\ntarget_day = 'Thursday'\n\n## north,south,east,west lat/lon coordinates for bounding box\nnsew = [40.9, 40.6, -73.8, -74.1]\n\n## create figure\nfig = plt.figure(figsize=(14,8))\n\n## target hours\nhrs = [17, 21, 23]\n\n## axis subplot locations\nax_loc = [131, 132, 133] \n\n## title strings\ntitle_str = ['5 PM', '9 PM', '11 PM']\n\n## plot loop\nfor ii in range(len(ax_loc)):\n\n    ## get the ride data from the target hour\n    rides_this_hour = rides.loc[(rides['weekday'] == target_day) & (rides['hour'] == hrs[ii])]\n\n    ##  plot the heat map\n    heat_map(ax_loc[ii],title_str[ii],rides_this_hour,nsew)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"23496d5a-bb1e-4b0a-b4cc-1cef10746ac3","_uuid":"23d35e2d6cc90d694516adc45cc2e368757581ce"},"cell_type":"markdown","source":"First, note that the default shape file included with the Basemap package does not have adequate resolution for this scale.  The Hudson River is not visible, despite calling `drawrivers()`, and Manhatan appears as a peninsula.  We'll need to use a higher resolution shape file for detailed analysis, but we can make some general conclusions based on our knowledge of the city.  Central Park is a highly visible feature which we can use to orient ourselves.\n\nAs the workday is ending, the most concentrated traffic is in Midtown, slightly trending towards the Upper East Side.  By 9:00 pm, some of the pressure north of Midtown is relaxing and hot spots are developing in other parts of the city.  They appear close to Greenwich Village and Washington Square Park, with other small hot spots on the Hudson. By 11:00 pm, the total traffic has reduced and there's only one hot spot along the Hudson.\n\nStill, this is average data for all Thursdays in the month.  What if one particular Thursday skewed the results?  \n\n#### Daily Comparison\n\nLuckily, we still have the hourly ride data."},{"metadata":{"_cell_guid":"dcaacd2e-0058-4724-863c-d7560e63bf0a","_uuid":"b4362c22fc4d36d60bf53189f2cd913520a76b3b"},"execution_count":null,"source":"hourly_ride_data.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"bc39097d-bac3-42c9-88c4-421766516121","_uuid":"33f830d0c01dd23bfcf0fa4775b0afbe59ce8b2b"},"cell_type":"markdown","source":"So we can just grab all of the Thursday hourly data..."},{"metadata":{"_cell_guid":"d710cac2-524a-4693-83b3-14bf5923ac23","collapsed":true,"_uuid":"fed52cdf8c44277a78c8c3e6b26624105d8f1938"},"execution_count":null,"source":"thursday_hourly_data = hourly_ride_data[hourly_ride_data['weekday']=='Thursday']","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"9dfcaecc-a104-403b-a051-d005624e56af","_uuid":"ac2b392c28012d76af5c4bc33f30ed8e3fa1e47c"},"cell_type":"markdown","source":"... and plot it with seaborn, color coded by date!"},{"metadata":{"_cell_guid":"ffc3c056-49a3-4fc9-9b8e-dc14164eabdd","_uuid":"cce334db46f1bf9ba0a59d5d9172ef3ddf21ef71"},"execution_count":null,"source":"## create figure\nfig = plt.figure(figsize=(12,6))\nax = fig.add_subplot(111)\n\n## set palette   \ncurrent_palette = sns.color_palette(tableau_color_blind)\n\n## plot data\nsns.pointplot(ax=ax, x='hour',y='ride_count',hue='day', palette = current_palette, data = thursday_hourly_data)\n\n## clean up the legend\nl = ax.legend()\nl.set_title('')\n\n## format plot labels\nax.set_title('Hourly Ride Count for Thursdays in August 2014', fontsize=25)\nax.set_ylabel('Rides', fontsize=20)\nax.set_xlabel('Hour', fontsize=20)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"0df38464-2fa0-4cf9-9069-79477874f00a","collapsed":true,"_uuid":"73d913c474d2b2a72908d833433908373dc66a68"},"cell_type":"markdown","source":"There is some variation in the evening hours.  Let's compare the highest hourly ride count (10 PM on Aug 21) to the same time on a \"typical\" Thursday."},{"metadata":{"_cell_guid":"464babfd-cba5-45e8-b8c6-c9def310ae79","_uuid":"688c5cdb94e2b0027deec59d80969c98b16b41e2"},"execution_count":null,"source":"## set day for analysis\ntarget_day = [14, 21]\n\n## north,south,east,west lat/lon coordinates for bounding box\nnsew = [40.9,40.6,-73.8,-74.1]\n\n## create figure\nfig = plt.figure(figsize=(14,8))\n\n## hour \nhrs = 22\n\n## axis locations\nax_loc = [121, 122] \n\n## title strings\ntitle_str = ['Aug 14, 10 PM', 'Aug 21, 10 PM']\n\n## plot loop\nfor ii in range(len(ax_loc)):\n\n    ## get the ride data from the target hour\n    rides_this_hour = rides.loc[(rides['day'] == target_day[ii]) & (rides['hour'] == hrs)]\n\n    ## plot the heat map\n    heat_map(ax_loc[ii],title_str[ii],rides_this_hour,nsew)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"58b17979-bc9e-4d86-a604-9d9d4a2b6fe0","_uuid":"a8677605f63245ddaff4fde810b1e2182a99c834"},"cell_type":"markdown","source":"These results are ok.  We can see one large cluster on the 14th and several distributed clusters on the 21st, but it's difficult to pinpoint the location.  We really need a more detailed plot to keep asking questions.  One option is to track down the relevant shape files to draw the city in more resolution, but it takes a long time for Basemap to process the data and make the plots anyway.  \n\n<a id=\"leaflet\"></a>\n# Detailed Interactive Plotting\n\nWe'll utilize **[mplleaflet](https://github.com/jwass/mplleaflet)** to overlay a basic matplotlib plot onto a **[Leaflet](http://leafletjs.com/)**. It will automatically perform the required calculations to project our GPS coordiates onto the 2D representation. It's also significantly faster.\n\nThere are two display options for mplleaflet.  You can either open the Leaflet map in a new tab using `mplleaflet.show()`, or you can display the map inline using `mplleaflet.display()`.  The inline display doesn't seem to handle large data sets well, but I have not run into that problem when opening the Leaflet in the new tab.\n\nGo ahead and play around with both options.  You'll note that the marker size does not change as you zoom in. We just have to balance the marker size with the desired map resolution.  If you plan to zoom in a lot, make the marker size bigger.  Luckily, the plotting is very fast and the Leaflet is very responsive so it's not too painful to experiment.\n\nBelow I have selected data from Thursday, August 14 between 10:00 PM and 10:15 PM.  It plots inline for me on Kaggle, but it takes minute to load.  You can zoom in, pan around, and see the exact locations of each pickup."},{"metadata":{"_cell_guid":"b2ca385f-cdeb-4960-a5b4-0f597c36581c","_uuid":"2d7c984e1e6a1e059604951a593851375a635afd"},"execution_count":null,"source":"## make the figure\nfig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(111)\n\n## get ride data\nrides_this_hour = rides.loc[(rides['day']== 14) & (rides['hour'] == 22) & (rides['minute'] < 16)]\n\n## plot ride data\nplt.plot(rides_this_hour['lon'], rides_this_hour['lat'], 'bo', markersize=4)\n\n## display the Leaflet\n# mpl.show()     # opens in a new interactive tab\nmpl.display()  # shows interactive map inline in Jupyter but cannot handle large data sets","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cf6d5ba7-ab76-46b0-82a2-563b58b247b9","_uuid":"2f97bb8059f2390dd531da39a6f123d0af4d088e"},"cell_type":"markdown","source":"<a id=\"clustering\"></a>\n# Clustering\n\nWhile it can be useful to look at the raw data set and gain intuition, looking at snap shots doesn't give us enough information to make good conclusions, and plotting hours of individual pick-ups just makes a mess.  We want actionable conclusions from this data set, so we'll need to be able to make quantitative comparisons between different regions in the city.  \n\nBut how will we define the regions?  \n\nWe could manually define them and then bin the pickups accordingly, but the information we seek is already in our data set.  Rather than forcing our data into a framework built out of our own assumptions, let's just observe how the people move around and look for special cases.\n\nWe'll implement the **[DBSCAN clustering method from scikit-learn](http://scikit-learn.org/stable/modules/clustering.html#dbscan)** instead of structured spatial binning.  This will give us more control over the questions we ask.  It's also much faster than the Basemap implementation of `hexbin`.\n\nThe [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) algorithm will group points together that meet a specified density metric.  Basically, we'll define a maximum distance to make two individual points count as neighbors, as well as a minimum number of neighbors for a group of points to qualify as a cluster.  The algorithm will sort the points into groups which meet the criteria and discard all of the outliers.\n\nOnce DBSCAN has identified all applicable clusters, we can easily calculate the centroid using the `MultiPoint` class from **[Shapely](https://pypi.python.org/pypi/Shapely)** and plot the results.  \n\nThis allows us to precisely identify locations which experience a high volume of pick-ups during a specified time frame.  By using the total number of pickups in an individual cluster as a metric for coloring the hot spot locations, we can visualize the intensity of a given hot spot in addition to it's centroid.\n\nInspired by **[Geoff Boeing](http://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/)**.\n\n#### Clustering Function\n\nFirst, we'll write a function which runs the clustering algorithm and returns the \"hot spots.\"  We'll get the coordinates of the centroid and the number of pick-ups in each cluster."},{"metadata":{"_cell_guid":"54fb8d5e-b1a1-4610-b9d8-c235d8fb420d","collapsed":true,"_uuid":"ae5263d3857fa43e8e71b046204efce7c9a120bf"},"execution_count":null,"source":"def get_hot_spots(max_distance,min_cars,ride_data):\n    \n    ## get coordinates from ride data\n    coords = ride_data.as_matrix(columns=['lat', 'lon'])\n    \n    ## calculate epsilon parameter using\n    ## the user defined distance\n    kms_per_radian = 6371.0088\n    epsilon = max_distance / kms_per_radian\n    \n    ## perform clustering\n    db = DBSCAN(eps=epsilon, min_samples=min_cars,\n                algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n    \n    ## group the clusters\n    cluster_labels = db.labels_\n    num_clusters = len(set(cluster_labels))\n    clusters = pd.Series([coords[cluster_labels == n] for n in range(num_clusters)])\n    \n    ## report\n    print('Number of clusters: {}'.format(num_clusters))\n    \n    ## initialize lists for hot spots\n    lat = []\n    lon = []\n    num_members = []\n    \n    ## loop through clusters and get centroids, number of members\n    for ii in range(len(clusters)):\n\n        ## filter empty clusters\n        if clusters[ii].any():\n\n            ## get centroid and magnitude of cluster\n            lat.append(MultiPoint(clusters[ii]).centroid.x)\n            lon.append(MultiPoint(clusters[ii]).centroid.y)\n            num_members.append(len(clusters[ii]))\n            \n    hot_spots = [lon,lat,num_members]\n    \n    return hot_spots","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"8e28bd58-e33a-4c8b-b128-29e599ff06ab","_uuid":"3c9d3d2b5a6e239765f56019e535723a77d284c8"},"cell_type":"markdown","source":"#### Ask the Question\n\nWhere are the locations which experience more than 25 pickups that occur within 50 meters of each other after 4:00 PM on August 14, 2014?"},{"metadata":{"_cell_guid":"36839d06-ca6f-40fc-a965-d77950621b03","_uuid":"d4817ae833cec849bffeaa8c794d0bb6d1041747"},"execution_count":null,"source":"## get ride data\nride_data = rides.loc[(rides['day']== 21) & (rides['hour'] > 15)]\n\n## maximum distance between two cluster members in kilometers\nmax_distance = 0.05\n\n## minimum number of cluster members\nmin_pickups = 25\n\n## call the get_hot_spots function\nhot_spots = get_hot_spots(max_distance ,min_pickups, ride_data)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"7012d39d-f8c8-4387-a98e-f5f578246151","_uuid":"17a8d6e2608bcd15991c1ab4b4c7489b9bc9ae46"},"cell_type":"markdown","source":"Now plot it up!  This time we'll use a scatter plot so that we can set the marker color using the cluster size as a metric. If we use a logarithmic scale on the color map, then there is better visual distinction near the lower bound.  If we use a linear color scale, then values near the lower bound will all appear as a similar color. Go ahead and try them both."},{"metadata":{"_cell_guid":"02c52c3a-cc64-4cf1-aee8-d9313df9fc31","_uuid":"2df2851af3827016ffbc9e0987d1c980cc1381d0"},"execution_count":null,"source":"## make the figure\nfig = plt.figure(figsize=(14,8))\nax = fig.add_subplot(111)\n\n## set the color scale\ncolor_scale = np.log(hot_spots[2])\n# color_scale = hot_spots[2]\n\n## make the scatter plot\nplt.scatter(hot_spots[0], hot_spots[1],s=80,c=color_scale,cmap=cm.cool)\n\n## display the Leaflet\n# mpl.show()     # opens in a new interactive tab\nmpl.display()  # shows interactive map inline in Jupyter but cannot handle large data sets","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"fa4f5bd2-3d3d-4bde-acaf-70773dd5c5c4","_uuid":"e55ed0e8489ced0678581bf8eea8c283809cfc1d"},"cell_type":"markdown","source":"Now we can easily visualize the 56 locations which experienced at least 25 pick-ups during the hours of 4:00 PM and Midnight on August 21st, 2014. If you were a driver, where would you hang out on a Thursday night?"}]}