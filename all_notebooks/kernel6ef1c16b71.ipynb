{"cells":[{"metadata":{},"cell_type":"markdown","source":"![logo](https://v.dam-img.rfdcontent.com/cms/001/268/454/600x600_smart_fit.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[RedFlagDeals](https://forums.redflagdeals.com/hot-deals-f9/) is a forum where users can post sales or deals that they have come across. Two tables have been scraped from the website. The main table with a description of all columns is shown below. The second table stores comments that were made on each post. Comments can be linked back to their original posts in the main table through the `title` column.\n\nIn this project, the data from both datasets will be cleaned by converting columns to their most appropriate data type, removing unwanted characters from strings and by dealing with missing values. Some of the nan records can be substituted through information found in the title or url. \n\nIf you don't want to bother with data wrangling and start analyzing, I included the final cleaned version as a download (`rfd_main_cleaned`).\n\n|Column name|Description|\n|---|---|\n|'title'| Title of post|\n|'votes'| Sum of up-, and down-votes|\n|'source'| Name of retailer offering the sale|\n|'creation_date'| Date of initial post|\n|'last_reply'| Date of most recent reply|\n|'author'| User name of post author|\n|'replies'| Number of replies|\n|'views'| Number of views|\n|'price'| Price of product on sale|\n|'saving'| Associated saving|\n|'expiry'| Expiry date of sale|\n|'url'| Link to deal|\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Packages\nimport requests # Scraping\nfrom bs4 import BeautifulSoup # HTML parsing\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data and explore","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Main table","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_raw = pd.read_csv('../input/data-on-sales-posted-on-redflagdeals/rfd_main.csv').iloc[:,1:]\ndf_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_comments = pd.read_csv(\"../input/data-on-sales-posted-on-redflagdeals/rfd_comments.csv\").loc[:,\"title\":]\ndf_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_comments.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II. Data wrangling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Comment table","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the short exploration above we can see that we need to remove one row with missing values for the `comments` column. These probably correspond to comments without text. Further, comment strings will need to be cleaned.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete rows with empty comments\ndf_comments.dropna(axis=0, inplace=True)\ndf_comments.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print first 20 comments\nprint([x for x in df_comments['comments']][0:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only way to clean these strings up will be through regular expressions. \n\nThe following should be removed:  \n* ↑ symbols\n* \\n new line characters\n* urls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ↑ symbols\narrow_removed = [re.sub(\"↑+\",\"\", str(string)) for string in df_comments['comments']]\n# \\n characters\nnewline_removed = [re.sub(\"\\\\n+\",\" \",string) for string in arrow_removed]\n# urls\nurls_removed = [re.sub(r\"\\bhttp.+\",\" \",string) for string in newline_removed]\n# Assign cleaned comments back\ndf_comments['comments'] = pd.Series(urls_removed)\n\n# first 100 comments of cleaned table\nprint([x for x in df_comments['comments']][0:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save cleaned comment table as file\ndf_comments.to_csv('rfd_comments.csv')\n\ndf_comments.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Main table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy of raw data set\ndf = df_raw.copy()\n\n# List of tuples: (column name, column dtype)\ncol_dtypes = [(col, type(x)) for x,col in zip(df.iloc[0], df.columns)]\n\n# Print tuple for columns containing dates\nfor col in col_dtypes:\n    if col[0] in ['creation_date', 'last_reply', 'expiry']:\n        print(col[0], ': ', col[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the columns are formatted as datetime. To facilitate working with the dates, we will convert them to datetime. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Convert date columns to datetime dtype","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_datetime(column_name: str) -> pd.Series:\n    \"\"\"\n    Converts a column of either format \"%b %d, %Y %I:%M %p\"\n    or format \"%B %d, %Y\" from string to date-time\n    \n    Args:\n    date_column - name of column with dates encoded as strings\n    \n    Returns:\n    Column elements converted to datetime in a pandas.Series object\n    \"\"\"    \n    # Superfluous characters removed\n    column_clean = df[column_name].str.replace(\"st\",\"\").str.replace(\"nd\",\"\")\\\n                        .str.replace(\"rd\",\"\").str.replace(\"th\",\"\").str.strip()\n    \n    # Check for correct length of cleaned column\n    column_len = len(column_clean)\n    print(\"Cleaned and original column are of equal lenght: \", \n          column_len == len(df[column_name]), \"\\n\")\n    \n    # Convert each entry from format \"%b %d, %Y %I:%M %p\" to datetime\n    date_column = []\n    try:\n        date_column = column_clean.apply(lambda x :\\\n                        datetime.datetime.strptime(str(x), \"%b %d, %Y %I:%M %p\"))\n    except: \n        print(\"\\\"%b %d, %Y %I:%M %p\\\" is incorrect format\")\n        pass\n    \n    # Convert from format \"%B %d, %Y\" to datetime\n    for date in df[column_name]:\n        if date is not np.nan:\n            try:\n                date_column.append(datetime.datetime.strptime(date, \"%B %d, %Y\"))\n            except: \n                print(\"\\\"%B %d, %Y\\\" is incorrect format for\", date)\n                break\n        else: \n            date_column.append(None)\n    \n    if len(date_column) != column_len:\n        print(\"\\n\", \"Incorrect column length!\\n\")\n    else:\n        print(\"\\n\", \"Column has expected length!\\n\")\n    \n    return pd.Series(date_column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creation_date column converted to datetime\ncreation_date = to_datetime('creation_date')\n\n# Compare random slice of original and converted column\nprint(creation_date.iloc[99:105], \"\\n\")\nprint(df.loc[99:104, 'creation_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# last_reply column converted to datetime\nlast_reply = to_datetime('last_reply')\n\n# Print original and new column for comparison\nprint(last_reply.iloc[208:215], \"\\n\")\nprint(df.loc[208:214, 'last_reply'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"expiry = to_datetime('expiry')\nprint(expiry.iloc[150:157], \"\\n\")\nprint(df.loc[150:156, 'expiry'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The to_datetime() function appears to correctly convert each of the columns. The results can now be used in the DataFrame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign datetime columns to DataFrame\ndf.expiry = expiry\ndf.last_reply = last_reply\ndf.creation_date = creation_date\n\n# Verify dates\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with missing data: `source`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[:, ['source', 'title']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is possible, that users simply forgot to include the source of the deal. We will check if missing sources are mentioned in the corresponding title.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set of entries in 'source' column\nretailer_set = set(df['source'].dropna())\nprint(\"Number of unique sources: \", len(retailer_set))\nprint(df.source.isnull().sum(), \"missing values in source column\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The large number of unique sources is promising! \n\nNext we will use the set previously created to iterate through the titles and check if any of the unique source names are present. If a source name from the set is found in `title` and no value is found in the corresponding `source` column, then the index as well as the source name are saved in the `replace` dictionary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"replace_dict = {} # key: index; value: retailer name to replace missing source value at index\n\n# Iterate through set of unique values from source source column\nfor retailer in retailer_set:\n    \"\"\"Fill replace dictioray with indecies and source names. Entries are made\n    when a source name is found in the title column while the corresponding source entry\n    is empty.\"\"\"\n    \n    # Iterate through 'source' and 'title' columns row-by-row\n    # Generate boolean array: True if unique source name (retailer) found in \"title\" and \"source\" is np.nan\n    source_missing_and_in_title = np.array([retailer in title \n                                     if source is np.nan else False\n                                     for title,source in zip(df.title, df.source)])\n    \n    # Indecies for which source_missing_and_in_title is True\n    replacement_indicies = np.where(source_missing_and_in_title == True)[0]\n    # Fill \"replace\" dictionary\n    for index in replacement_indicies:\n        if index not in replace_dict.keys():\n            replace_dict[index] = retailer\n\nprint(\"Replacements found in 'title':\", len(replace_dict.values()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some missing sources can be replaced by information found in the title. We will use the indices and values stored in `replace_dict` to replace the appropriate values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"source_list = list(df.source) # copy of source column \nmissing_start = sum([x is np.nan for x in source_list]) # missing values before cleaning\nprint(\"Missing source values before replacement:\", missing_start)\n\nfor replace in replace_dict.items():\n    index = replace[0]\n    source_replacement = replace[1]\n    source_list[index] = source_replacement\n\nmissing_end = sum([x is np.nan for x in source_list]) # missing values after cleaning\nprint(\"Missing source values after replacement:\", missing_end)\nreplaced_count = missing_start-missing_end # number of replaced values\nprint(replaced_count, \"missing source records have been replaced!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All identified `source` records have been replaced with appropriate names of retailers found in the corresponding `title` column. The new `source` column can now replace the old one.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.source = source_list\nprint(\"Number of missing values as expected:\", (df.source.isnull().sum() == missing_end))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Further substitutions for missing `source` values may be found in the `url` column. The objective is to extract company names from the urls and use them to further replace missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'url' entries of rows with missing source values\nurl_replacement = df[df.source.isnull()].url\nprint(url_replacement.notnull().sum(), \"missing source values have corresponding urls\")\nurl_replacement.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The urls need to be split and cleaned to extract the name of the organization. The final cleaned values and their corresponding indices in the DataFrame will be stored in the `clean_urls` dictionary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_urls = {} # key: index in df, value: cleaned url\nindicies = url_replacement.index\n\nfor url in zip(indicies, url_replacement):\n    index = url[0]\n    replacement_url = url[1]\n    \n    # Clean if url value not missing\n    if replacement_url is not np.nan:\n        url_root = replacement_url.split(\"//\")[1].split(\"/\")[0].split(\"?\")[0].replace(\"www.\", \"\")\n        removed_domain = url_root.split(\".\")\n        clean_urls[index] = removed_domain\n    else:\n        clean_urls[index] = np.nan\n        \nprint(clean_urls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to identify the company names from the url splits observed in the print above.\nThe patterns shown in the table will facilitate this process. This is an oversimplification and will lead to some false extractions but the number of errors should be minimal.\n\n|Condition| Pattern|\n|---|---|\n|Lists length 2| company name is at index 0|\n|Lists length 3 and domain com, ca, or net| name is at index 1|\n|List length 3 and domain io| name is at index 0| \n|List length 4| no identifiable name|","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_url_final = clean_urls.copy()\n\nfor item in clean_url_final.items():\n    index = item[0]\n    url_split = item[1]\n    try:\n        if len(url_split) == 2:\n             # name at index 0\n            clean_url_final[index] = url_split[0].title()\n        \n        elif ((len(url_split) == 3) \n                        and ((url_split[-1] == \"com\") \n                                 or (url_split[-1] == \"ca\") \n                                 or (url_split[-1] == \"ca\"))):\n            # name at index 1\n            clean_url_final[index] = url_split[1].title()\n        \n        elif ((len(url_split) == 3) \n                        and (url_split[-1] == \"io\")):\n             # name at index 0\n            clean_url_final[index] = url_split[0].title()\n        else: \n              clean_url_final[index] = np.nan\n    except: value = np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add url-derived company names to DataFrame\ndf.loc[list(clean_url_final.keys()),'source'] = list(clean_url_final.values())\nprint(\"Missing source values remaining: \", df.source.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with missing data: `price`\n\nUsers may have forgotten  to tag prices associated with the deals they posted. We will verify if there are any `$` signs in the title for those rows that have missing price values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_prices_df = df[df.price.isnull()]\nprice_in_title = [\"$\" in title for title in missing_prices_df.title]\nprint(df.price.isnull().sum(), \"missing values in 'price' column\")\nprint(sum(price_in_title), \"missing prices have '$' signs in the title\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display first 10 title to evaluate if the missing price could be substituted\nreplacement_titles = missing_prices_df[price_in_title].title\n[title for title in replacement_titles][0:10]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"regex = \"[$]+[.,]*\\d+[.,]*\\d+\"\\\n        \"|[.,]*\\d+[.,]*\\d+[$]+\"\\\n        \"|[a-zA-Z]+[$]+[.,]*\\d+[.,]*\\d+\"\nprice_replacements = replacement_titles.str.findall(regex)\nprint(\"Number of possible replacements:\", len(price_replacements))\nprice_replacements","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will assume the first element in each list is most relevant and use it to replace missing price values. Some inaccuracies are likely to occur, but the estimates should be reasonable for the most part. Most often, user seem to not include prices in the summary if the product is available at different price categories. Picking one the prices is better than having no information at all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"replacement_dict = {} # key: index; value: price to replace missing value at index\n\n# Iterate through price lists found in price_replacements and corresonding indecies in DataFrame\nfor replacement in zip(price_replacements, list(price_replacements.index)):\n    price_list = replacement[0]\n    index = replacement[1]\n    if price_list != []:\n        price = price_list[0]\n        price_clean = (re.search(r\"\\d+[.,]*\\d+\", price)).group().replace(\",\",\"\")\n        replacement_dict[index] = price_clean\n        \nprint(len(replacement_dict), \"replacements found.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace missing values\ndf.loc[list(replacement_dict.keys()), 'price'] = list(replacement_dict.values())\nprint(\"Remaining missing values:\", df.price.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transforming `price` data to float","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"regex = \"\\d+\\.*\\d*\"\nmatches = [re.search(regex, str(x)) for x in df.price]\n\n# Append matches to new Series object\nnew_price = pd.Series()\nfor match in matches:\n    if match != None:\n        new = pd.Series(float(match.group()))\n    else:\n        new = pd.Series(np.nan)\n    new_price = new_price.append(new, ignore_index=True)\n    \n# Replace old price with new price column\ndf.price = new_price\n\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dealing with missing data: `saving`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Titles for which the saving entry is missing\nmissing_savings_df = df[df.saving.isnull()]\nprint([title for title in missing_savings_df.title.head(20)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, we will search \"%\" symbols in rows for which the \"saving\" column entry is empty, similar to what we have done for prices.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Titles containing the % symbol may contain information on savings\n# \"saving_in_title\" indicates the indicies for which there is no data\n# in the \"saving\" column and a \"%\" is found in the title.\nsaving_in_title = [\"%\" in title for title in missing_savings_df.title]\nprint(df.saving.isnull().sum(), \"missing values in 'saving' column\")\nprint(sum(saving_in_title), \"rows with missing 'saving' data have a '%' symbol in their title\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Titles containing the % symbol in rows with missing 'saving' entries  \nreplacement_titles = missing_savings_df[saving_in_title].title\n\n# Extract savings data\nregex = \"[.,]*\\d+[.,]*\\d+[%]+\"\nsaving_replacements = replacement_titles.str.findall(regex)\nprint(\"Number of possible replacements:\", len(replacement_titles))\nsaving_replacements","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we will assume the first occurrence to be most relevant. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"replacements = {}\nindex_saving_tuples = zip(saving_replacements.index, saving_replacements)\nfor index, saving in index_saving_tuples:\n    try:\n        replacements[index] = saving[0]\n    except:\n        print(\"Empty list found in 'saving_replacements'\")\n\nprint(\"=\"*50)\nprint(\"Replacements found for missing savings:\", len(replacements))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace missing values\ndf.loc[list(replacement_dict.keys()), 'price'] = list(replacement_dict.values())\nprint(\"Remaining missing values:\", df.saving.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will expand our search for percent symbols to the comments to see if we can increase the number of replacements.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# df slice with missing \"saving\" data and no \"%\" symbol in \"titel\"\nno_title_replacement = missing_savings_df[[(not replaceable) for replaceable in saving_in_title]]\n\n# titles will be used as ids for corresponding comments\ncomment_ids = set(no_title_replacement.title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert ids into indecies for comments_df\ncomment_indecies = [(x in comment_ids) for x in df_comments.title]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The DataFrame includes comments derived from initial posts as well the corresponding responses. To find information on savings, we are mainly interested in the initial posts. This is because we expect the authors to limit their comments to pertinent  information on the sales deal. Percent symbols may be present in the responses but are less likely to correspond to savings associated with the product. \n\nEach post will have a row with its title for every response that was made. To retrieve only the initial posts of the authors, we will group by title and then select the `first_valid_index()` from each group object. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Indecies for which titles appear for the first time\nindex_initial_posts = [x for x in df_comments[comment_indecies]\\\n                       .groupby('title').apply(pd.DataFrame.first_valid_index)]\n\nreplacement_comments = df_comments.iloc[index_initial_posts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Search for % symbol in comments\nsaving_found = [\"%\" in str(comment) for comment in replacement_comments.comments]\nprint(sum(saving_found), \"row(s) with missing 'saving' data have a '%' symbol in their title\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replacement_comments[saving_found].comments.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, only one replacement was found. Another option that could be explored in the future would be to search the titles for other indicators. For example, strings of the format \"50$ less\" could be explored.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Homogenizing the \"saving\" column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.saving.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, that some of the savings do not relate to percentages but rather dollars. We will convert all values first to percentages and lastly into numeric ratios from 0-1 to allow for mathematical operations. \n\nWe can only convert savings into percentages if data in the price section is available. We will fist verify that this is the case for most entries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Values in \"saving\" without % symbols\nno_missing_savings = df.iloc[df.saving.dropna().index]\ndollar_savings = no_missing_savings[[\"%\" not in str(saving) for saving in no_missing_savings.saving]]\nprint(\"Entries in 'saving' without % symbol:\", dollar_savings.shape[0])\nexisting_price = dollar_savings.price.notnull()\nprint(\"Corresponding entries with non-missing values in 'price':\", existing_price.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of the saving entries that are not missing, 189 values do not contain a % symbol. These most likely correspond  to \"\\\\$\" savings. Of those 189 entries, 178 entries contain values in the 'price' column. This implies that 182 \"\\\\$\" savings should be convertible to % savings. The remaining 11 will need to be deleted. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"$\" savings with missing 'price' data\nno_price_index = existing_price[existing_price == False].index\n\n# Verify data\ndf.iloc[no_price_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete \"$\" savings without price data\ndf.loc[no_price_index, \"saving\"] = np.nan\n\n# Verify changes\ndf.iloc[no_price_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regular expressions for \"%\" and \"$\" savings\nregex_percent = \"(\\d+\\.*\\d*\\s*%)|(%\\s*\\d+\\.*\\d*)\"\nregex_dollar = \"(\\d+\\.*\\d*\\s*\\$)|(\\$\\s*\\d+\\.*\\d*)\"\n\nprint(re.search(regex_percent, \"20%\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert savings to proportions from 0-1\nnew_savings = []\nfor index in range(df.shape[0]):\n    saving = str(df.iloc[index].saving)\n    # match objects\n    percent = re.search(regex_percent, saving)\n    dollar = re.search(regex_dollar, saving)\n    \n    if percent != None:\n        saving = percent.group()\n        saving_clean = float(saving.replace(\"%\",\"\").replace(\",\",\"\").strip())\n        new = float(saving_clean/100)\n    elif dollar != None:\n        saving = dollar.group()\n        saving_clean = float(saving.replace(\"$\",\"\").replace(\",\",\"\").strip())\n        price = df.iloc[index].price\n        if price > 0:\n            new = float(saving_clean/(price + saving_clean))\n        else:\n            new = 1.0\n    elif saving != \"nan\":\n        saving = re.search(\"\\d+\\.*\\d*\", saving)\n        if saving != None:\n            saving = float(saving.group())\n            price = df.iloc[index].price\n            new = float(saving/(price + saving))\n        else:\n            new = np.nan\n    else:\n        new = np.nan\n    new_savings.append(new)\n    \ndf[\"new_saving\"] = new_savings\ndf[df.new_saving.notnull()].loc[:,['price','saving','new_saving']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the most part it seems the conversion was a success. Some of the data in `saving` was deleted during the cleanup and conversion process. We will investigate those values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.saving.notnull() &  df.new_saving.isnull()].loc[:,['price','saving','new_saving']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The saving values that were not converted correspond to string values and should therefore be replaced by np.nan.\nIt seems that everything is in order, so we will replace the old with new saving column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.saving = df.new_saving.astype('float')\ndf.drop([\"new_saving\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Less than 0 replies\ndf[df.replies < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop([328], axis=0, inplace=True)\ndf[df.replies < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}