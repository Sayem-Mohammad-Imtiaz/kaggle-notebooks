{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic [EDA + Model Pipeline]\n\n#### This kernel provides exploratory analysis of the data to uncover the underlying structures  to better understand modeling strategies to use for machine learning. If, by any means, this notebook happens to stand out in anyway, please consider **upvoting** as it motivates me to create better notebooks.\n\n\n\n## Table of Contents:\n1. Introduction\n2. Importing Libraries\n3. Extracting of Basic Statistics\n4. Detailed Data Exploration\n5. Preprocessing and Data Cleaning\n6. Modelling\n7. Plotting Model's Performance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction (The Challange Description)\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Import Libraries:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport matplotlib as mpl\n\nfrom matplotlib import rcParams\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Dense\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nnp.random.seed(0)\n\n# Plots the figure in the kernel rather than opening a window or tab.\n%matplotlib inline\n\n# Set the universal size for figure\nrcParams['figure.figsize'] = (10, 8)\nplt.style.use(\"ggplot\")\nmpl.rc(\"savefig\", dpi = 200)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/titanic/train.csv\")\ntest_df  = pd.read_csv(\"../input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Extraction of Basic Statistics:\n\nHere we first acquire information on the dataset that tells us what Data Types do certain columns have, and which of them have Null Values that may require some cleaning at later stages. Then we use the describe method on both DataFrames that provides us with some basic information on how is the data distributed throughout the columns. This helps us determine as to which numerical columns would require normalization and which ones would require scaling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] Basic Information on Training Dataset: \\n\")\nprint(train_df.info())\n\nprint('')\nprint(\"[+] Basic Information on Testing Dataset: \\n\")\nprint(test_df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] Basic Statistics on Training DataFrame: \\n\")\nprint(train_df.describe())\n\nprint('')\nprint(\"[+] Basic Statistics on Testing DataFrame: \\n\")\nprint(test_df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] First Five Rows of Training DataFrame:\")\nprint(\"##########################################\\n\")\n\nprint(train_df.head(5))\nprint(\"\")\n\n\nprint(\"[+] First Five Rows of Testing DataFrame:\")\nprint(\"##########################################\\n\")\n\nprint(test_df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Extraction of Detailed Statistics\n\nHere, we will begin with tear down of each individual column and study how data is distributed in various categories by plotting their histograms and bar charts. Then in later stages we will study relationships between each of these columns. If there exists any correlation between any of the categories, we will decorrelate them through various techniques at our disposal. \n\n### [+] Study of Important Features\n\n1. The Study Names Column\n2. The Study Pclass Column\n3. The Study Sex Column\n4. The Study Age Column\n5. The Study of Fare Column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. The Study of the \"Name\" Column\n\nTaking a peek at this column reveals that there is a lot of information that can be extracted from this column. As this column contains names of various people aboard The Titanic, we will explore the distribution of these people according to their titles first. We will use the tools from Regular Expression library to loop over every single record, extract the text, segregate the titles into a seperate column named \"Title\", and count occurances of all unique titles in this dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] The Name Column of Training Dataset:\")\nprint(\"#######################################\\n\")\n\nprint(train_df['Name'].head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Title'] = train_df[\"Name\"].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\n\n\n_ = plt.figure(figsize = (12, 5))\n_ = plt.xlabel(\"Title\", fontsize = 16)\n_ = plt.ylabel(\"Count\", fontsize = 16)\n_ = plt.title(\"Occurances of Titles\", fontsize = 20)\n_ = plt.xticks(rotation = 90)\n\nsns.countplot(x = 'Title', data = train_df, palette = \"Blues_d\")\n\nplt.show()\n\n# Repeat the same procedure for testing dataset\ntest_df['Title'] = test_df[\"Name\"].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Title_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n                   }\n\ntrain_df['Title'] = train_df['Title'].map(Title_Dictionary)\ntest_df['Title'] = test_df['Title'].map(Title_Dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. The Study of the \"Pclass\" Column\n\nThe column \"Pclass\" represents categories of passengers aboard titanic, just as we would have economy class and first class in modern aviation. After exploration, we can see that there are three categories named 1, 2, 3. We can use seaborn's countplot method to visualize the distribution (An aid for quick glance).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] First 5 Rows of the  'Pclass' column: \")\nprint(\"########################################\\n\")\nprint(train_df['Pclass'].head())\nprint(\"\")\n\nprint(\"[+] Count of Categories in Tabular Format: \")\nprint(\"##########################################\\n\")\nprint(train_df['Pclass'].value_counts())\n\n_ = plt.figure(figsize = (12, 5))\n_ = plt.xlabel(\"Pclass\", fontsize = 15)\n_ = plt.ylabel(\"Count\", fontsize = 15)\n_ = plt.title(\"Occurances of Pclass\", fontsize = 15)\n_ = plt.xticks(rotation = 0)\n\nsns.countplot(x = 'Pclass', data = train_df, palette = \"Blues_d\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. The Study of the \"Sex\" Column\n\nJust like any categorical column of data we process the \"Sex\" column to find out that there happened to be more male than female, with a male count of 577 and 314 for the female. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting and basic EDA on the column\n\nprint(\"[+] First 5 Rows of the  'Sex' column: \")\nprint(\"########################################\\n\")\nprint(train_df['Sex'].head())\nprint(\"\")\n\nprint(\"[+] Count of Genders in Tabular Format: \")\nprint(\"##########################################\\n\")\nprint(train_df['Sex'].value_counts())\n\n_ = plt.figure(figsize = (12, 5))\n_ = plt.xlabel(\"Sex\", fontsize = 15)\n_ = plt.ylabel(\"Count\", fontsize = 15)\n_ = plt.title(\"Occurances of Sex\", fontsize = 15)\n_ = plt.xticks(rotation = 0)\n\nsns.countplot(x = 'Sex', data = train_df, palette = \"Blues_d\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. The Study of the \"Age\" and \"Fare\" Column\n\nAge, upon first glance using the head method, reveals that the data is continous, and it has a total of 177 null cells. Imputing them with median of the column can do the trick of cleaning these up. We then plot the distribution and observe that most of our candidates are centered between 20 and 40, and they are normally distributed. Though we will have to center this data to mean of zero and standard deviation of 1 so that our models can be trained on this data.\n\nWe then also explore the use of **Facet Grid** method of Seaborn that allows us to break the columns and adjust multiple plots into one figure. In this case, we segregate plots of age distributions according to whether the indivdual survived or preished. Faceting is the easiest way to make your plots multivariate. \n\nThen we also want to explore the distribution of age according to various age groups these individuals fall into, and whether these particular individuals survived or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic EDA and Imputation of Null Cells\n\nprint(\"[+] The first five rows of the 'Age' column: \")\nprint(\"##########################################\\n\")\nprint(train_df['Age'].head())\nprint(\"\")\n\nprint(\"[+] Total Number of Null Values : \", train_df[\"Age\"].isnull().sum())\n\n# Imputing the NaN values from the column\ntrain_df.loc[train_df.Age.isnull(), 'Age'] = train_df.groupby(['Sex','Pclass','Title'])['Age'].transform('median')\ntest_df.loc[test_df.Age.isnull(), 'Age']   = test_df.groupby(['Sex','Pclass','Title'])['Age'].transform('median')\n\nprint(\"[+] Total Null Values after Imputation: \", train_df[\"Age\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the data\n\n_ = plt.figure(figsize = (15,5))\n_ = plt.title(\"Distribuition and density by Age\")\n_ = plt.xlabel(\"Age\")\n_ = plt.xticks(rotation = 0)\n\nsns.distplot(train_df[\"Age\"], bins = 24,  color = 'black')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\nplot = sns.FacetGrid(train_df, col = \"Survived\", size = 6.2)\nplot = plot.map(sns.distplot, \"Age\", color = 'black')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_intervals = (0, 5, 12, 18, 25, 35, 60, 120)\ncategories    = ['Babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\ntrain_df[\"Age_Category\"] = pd.cut(train_df['Age'], age_intervals, labels = categories)\ntest_df[\"Age_Category\"]  = pd.cut(test_df['Age'], age_intervals, labels = categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.crosstab(train_df['Age_Category'], train_df['Survived']))\n\n_ = plt.figure(figsize = (15, 5))\n_ = plt.ylabel(\"Fare Distribution\", fontsize=18)\n_ = plt.xlabel(\"Age Categorys\", fontsize=18)\n_ = plt.title(\"Fare Distribution by Age Categorys \", fontsize=20)\n\nsns.swarmplot(x = 'Age_Category',y = \"Fare\", data = train_df, hue = \"Survived\", palette = \"PuBuGn_d\")\n\nplt.subplots_adjust(hspace = 0.5, top = 0.9)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Figure Setup\n_ = plt.figure(figsize=(15, 5))\n_ = plt.ylabel(\"Count\", fontsize = 18)\n_ = plt.xlabel(\"Age Categorys\", fontsize = 18)\n_ = plt.title(\"Age Distribution \", fontsize = 20)\n\nsns.countplot(\"Age_Category\",data = train_df, hue = \"Survived\", palette = \"PuBuGn_d\")\n\n# Plot the figure\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Relations Between the Important Features\n\nNow that we've looked at some variables that are important for prediction, we drop those that are irrelevent. So to do that we simply use drop method in pandas, and we do the same for test set. Looking at the relations between features we are left with, they unveil before us some interesting patterns, even the ones that might not be in your remote imagination. For example, it is likely that you may have survived the Titanic Tragedy if you had one or two kids. The very features that we may consider that might not correlate with survival, can often times be vital for prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the irrelevent columns\ntrain_dataset = train_df.drop(columns = ['Fare', 'Ticket', 'Age', 'Cabin', 'Name'])\ntest_dataset  = test_df.drop(columns = ['Fare', 'Ticket', 'Age', 'Cabin', 'Name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = plt.figure(figsize = (15, 5))\n_ = plt.title(\"Sex Distribution According to Survived or Not\")\n_ = plt.xlabel(\"Sex Distribution\")\n_ = plt.ylabel(\"Count\")\n\nsns.countplot(x = \"Sex\", data = train_dataset, hue = \"Survived\", palette = 'PuBuGn_d')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset[\"Embarked\"] = train_dataset[\"Embarked\"].fillna('S')\ntest_dataset[\"Embarked\"]  = test_dataset[\"Embarked\"].fillna('S')\n\n_ = plt.figure(figsize = (15, 5))\n_ = plt.title(\"Pclass Distribution According Survival\")\n_ = plt.xlabel(\"Embarked\")\n_ = plt.ylabel(\"Count\")\n\nsns.countplot(x = 'Embarked', data = train_dataset, hue = 'Survived', palette = 'PuBuGn_d')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.factorplot(x = 'SibSp', y = 'Survived', data = train_dataset, kind = 'bar', height = 5, aspect = 1.6, palette = \"PuBuGn_d\")\n_    = plot.set_ylabels(\"Probability of Survival\")\n_    = plot.set_xlabels(\"SibSp Number\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Preprocessing Data\n\nNow we encode all categorical variables so that we can process them through our machine learning model. Then we plot a correlation matrix to see correlations between various features, and we observe that there are only a few that show no correlation at all, so we need not worry about them.\n\nIn the next step we import Standard Scalar from sklearn.preprocessing and apply it to our datasets. The same scalar needs to be applied to both the training and testing datasets. This step is crucial as it transitions us into the modelling phase.\n\nBefore modelling make sure all the columns in the test and train test are of same number, otherwise our neural network will pose problems adapting to new shape of data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = pd.get_dummies(train_dataset, columns = [\"Sex\", \"Embarked\", \"Age_Category\",\"Title\"], prefix = [\"Sex\", \"Emb\", \"Age\", \"Prefix\"], drop_first = True)\ntest_dataset  = pd.get_dummies(test_dataset, columns = [\"Sex\", \"Embarked\", \"Age_Category\",\"Title\"], prefix = [\"Sex\", \"Emb\", \"Age\", \"Prefix\"], drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the correlation matrix\n\n_ = plt.figure(figsize = (18, 15))\n_ = plt.title(\"Correlation Matrix of Features in Training Dataset\")\n_ = sns.heatmap(train_dataset.astype(float).corr(), vmax = 1.0, annot = False, cmap = \"Blues\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = train_dataset.drop(['Survived', 'PassengerId'], axis = 1)\ntraining_target = train_dataset[\"Survived\"]\n\ntesting_data = test_dataset.drop([\"PassengerId\"], axis = 1)\n\nX_train, y_train = training_data.values, training_target.values \nX_test = (testing_data.values).astype(np.float64, copy = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] Shapes of Training and Testing Datasets: \")\nprint(\"############################################\\n\")\n\nprint('> Training Dataset = ', X_train.shape)\nprint('> Testing Dataset  = ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Modelling Using Keras\n\nSince this is a fairly basic dataset, it demands a basic solution. We can do this through various other techniques such as RandomForests and Gradient Boosting, but we will stick with neural networks as they tend to perform exceptionally well with basic datasets. Our model has two layers. One with 50 neurons, activation of relu, a dropout of 50%, and then we add the last layer with one unit in it, and we use sigmoid because it is binary classification that we are performing. \n\n**Callbacks** allow us to extract information from our model after each iteration of training. This is helpful as it liberates us from viewing the graphs of loss every time we train. We are using two callbacks in this example: Early Stopping and Model Checkpoint. Early Stopping automatically stops training our model as our training loss starts to ramp up for two consecutive iterations, whereas the Model Checkpoint keeps track of validation loss and saves the parameters at every iteration. After training, it saves the one that yielded the least validation loss while discarding the rest.\n\nThen loading the best weights that we acquired during the training phase, we predict on our test set and append those predictions into our sample submission file, saving it as **TitanicPreds.csv**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(50, input_shape = (17, ), activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.summary()\n\ncallbacks = [EarlyStopping(monitor='val_loss', patience = 2, mode = 'min'), \n             ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', save_best_only = True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nnetwork = model.fit(X_train, y_train, batch_size = 50, epochs = 100, verbose = True, callbacks = callbacks, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best_model.h5\")\ny_preds = model.predict(X_test)\n\nsubmission = pd.read_csv('../input/titanic/gender_submission.csv', index_col = 'PassengerId')\nsubmission['Survived'] = y_preds.astype(int)\nsubmission.to_csv('/kaggle/working/TitanicPreds.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Plotting Model's Performance\n\nPlotting the two metrics, we can see that our validation set gives a very good performance and achieves more than 80 percent accuracy. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[+] Available Parameters in Model's History: \")\nprint(\"#############################################\\n\")\n\nfor index, key in enumerate(network.history.keys()): print(str(index + 1) + \". \", key)\n\n_ = plt.figure(figsize = (15, 8))\n_ = plt.plot(network.history['val_accuracy'])\n_ = plt.plot(network.history['accuracy'])\n_ = plt.title('Training and Validation Accuracy')\n_ = plt.ylabel('Accuracy')\n_ = plt.xlabel('Epoch')\n_ = plt.legend(['train', 'validation'], loc = 'upper left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Conclusion\n\nThis was my first kernel and probably my first kaggle dataset. It is sure to have multitude of errors, but I hope to improve it and learn more about various Deep Learning techniques from top kagglers. Please consider taking a second and **UPVOTING** this notebook as it motivates me to create more.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}