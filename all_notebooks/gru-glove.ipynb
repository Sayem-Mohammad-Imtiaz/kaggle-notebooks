{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='cp437')\ndataset=dataset[['ctext','text']] #add headlines and see if it makes a difference\ndataset.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EOS_token = 1\nSOS_token=0\n\nclass Lang:\n    def __init__(self):\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"<sos>\", 1: \"<eos>\"}\n        self.n_words = 2  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        sentence = removeCont(sentence)\n        for word in sentence.split(' '):\n            if word not in stop_words:\n                self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions = {\"ain't\": \"is not\",\n                \"aren't\": \"are not\",\n                \"can't\": \"cannot\",\n                \"'cause\": \"because\",\n                \"could've\": \"could have\",\n                \"couldn't\": \"could not\",\n                \"didn't\": \"did not\",\n                \"doesn't\": \"does not\",\n                \"don't\": \"do not\",\n                \"hadn't\": \"had not\",\n                \"hasn't\": \"has not\",\n                \"haven't\": \"have not\",\n                \"he'd\": \"he would\",\n                \"he'll\": \"he will\",\n                \"he's\": \"he is\",\n                \"how'd\": \"how did\",\n                \"how'd'y\": \"how do you\",\n                \"how'll\": \"how will\",\n                \"how's\": \"how is\",\n                \"I'd\": \"I would\",\n                \"I'd've\": \"I would have\",\n                \"I'll\": \"I will\", \n                \"I'll've\": \"I will have\",\n                \"I'm\": \"I am\", \n                \"I've\": \"I have\", \n                \"i'd\": \"i would\",\n                \"i'd've\": \"i would have\", \n                \"i'll\": \"i will\",  \n                \"i'll've\": \"i will have\",\n                \"i'm\": \"i am\", \n                \"i've\": \"i have\", \n                \"isn't\": \"is not\", \n                \"it'd\": \"it would\",\n                \"it'd've\": \"it would have\", \n                \"it'll\": \"it will\", \n                \"it'll've\": \"it will have\",\n                \"it's\": \"it is\", \n                \"let's\": \"let us\", \n                \"ma'am\": \"madam\",\n                \"mayn't\": \"may not\", \n                \"might've\": \"might have\",\n                \"mightn't\": \"might not\",\n                \"mightn't've\": \"might not have\", \n                \"must've\": \"must have\",\n                \"mustn't\": \"must not\", \n                \"mustn't've\": \"must not have\", \n                \"needn't\": \"need not\", \n                \"needn't've\": \"need not have\",\n                \"o'clock\": \"of the clock\",\n                \"oughtn't\": \"ought not\", \n                \"oughtn't've\": \"ought not have\", \n                \"shan't\": \"shall not\", \n                \"sha'n't\": \"shall not\", \n                \"shan't've\": \"shall not have\",\n                \"she'd\": \"she would\", \n                \"she'd've\": \"she would have\", \n                \"she'll\": \"she will\", \n                \"she'll've\": \"she will have\", \n                \"she's\": \"she is\",\n                \"should've\": \"should have\", \n                \"shouldn't\": \"should not\", \n                \"shouldn't've\": \"should not have\", \n                \"so've\": \"so have\",\n                \"so's\": \"so as\",\n                \"this's\": \"this is\",\n                \"that'd\": \"that would\",\n                \"that'd've\": \"that would have\", \n                \"that's\": \"that is\", \n                \"there'd\": \"there would\",\n                \"there'd've\": \"there would have\", \n                \"there's\": \"there is\", \n                \"here's\": \"here is\",\n                \"they'd\": \"they would\", \n                \"they'd've\": \"they would have\",\n                \"they'll\": \"they will\", \n                \"they'll've\": \"they will have\", \n                \"they're\": \"they are\", \n                \"they've\": \"they have\", \n                \"to've\": \"to have\",\n                \"wasn't\": \"was not\", \n                \"we'd\": \"we would\", \n                \"we'd've\": \"we would have\", \n                \"we'll\": \"we will\", \n                \"we'll've\": \"we will have\", \n                \"we're\": \"we are\",\n                \"we've\": \"we have\", \n                \"weren't\": \"were not\", \n                \"what'll\": \"what will\", \n                \"what'll've\": \"what will have\", \n                \"what're\": \"what are\",\n                \"what's\": \"what is\", \n                \"what've\": \"what have\", \n                \"when's\": \"when is\", \n                \"when've\": \"when have\", \n                \"where'd\": \"where did\", \n                \"where's\": \"where is\",\n                \"where've\": \"where have\", \n                \"who'll\": \"who will\", \n                \"who'll've\": \"who will have\", \n                \"who's\": \"who is\", \n                \"who've\": \"who have\",\n                \"why's\": \"why is\", \n                \"why've\": \"why have\", \n                \"will've\": \"will have\", \n                \"won't\": \"will not\", \n                \"won't've\": \"will not have\",\n                \"would've\": \"would have\", \n                \"wouldn't\": \"would not\", \n                \"wouldn't've\": \"would not have\", \n                \"y'all\": \"you all\",\n                \"y'all'd\": \"you all would\",\n                \"y'all'd've\": \"you all would have\",\n                \"y'all're\": \"you all are\",\n                \"y'all've\": \"you all have\",\n                \"you'd\": \"you would\", \n                \"you'd've\": \"you would have\", \n                \"you'll\": \"you will\", \n                \"you'll've\": \"you will have\",\n                \"you're\": \"you are\", \n                \"you've\": \"you have\",\n                \"%\" :\" percent\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef removeCont(text):\n    if isinstance(text, float):\n        return str(text)\n    text=text.lower()\n    text=text.split()\n    for i in range(len(text)):\n        if isinstance(text[i],float):\n            text[i]=str(text)\n        if text[i] in contractions:\n            text[i]=contractions[text[i]]\n    text=' '.join(text)\n    text = text.replace(\"'s\",'') # convert your's -> your\n    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n    text = re.sub(r'\\.',' . ',text)\n    text = text.replace('.','')\n    text=text.split()\n    newtext=\"\"\n    for word in text:\n        if word not in stop_words:\n            newtext+=\" \"+word\n    return newtext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"removeCont(\"do...\").split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(dataset['ctext'])):\n    dataset['ctext'][i]=removeCont(dataset['ctext'][i])\n    dataset['text'][i]=removeCont(dataset['text'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['ctext'][100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang=Lang()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(dataset['ctext']))\nlen(dataset['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(dataset['text'])):\n    if isinstance(dataset['text'][i],float) or isinstance(dataset['ctext'][i],float):\n        continue\n    lang.addSentence(dataset['text'][i])\n    lang.addSentence(dataset['ctext'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang.word2index['daman']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils import data\nimport torch.nn.functional as F\nfrom torch.autograd import Variable as V\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom torchtext import data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in tqdm(f):\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors in the GloVe library' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 100\nEMBEDDING_DIM=100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix_len = len(lang.word2index)\nweights_matrix = np.zeros((matrix_len, 100))\nwords_found = 0\n\nfor i, word in enumerate(lang.word2index):\n    try: \n        weights_matrix[i] = embeddings_index[word]\n        words_found += 1\n    except KeyError:\n        weights_matrix[i] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_emb_layer(weights_matrix, non_trainable=False):\n    num_embeddings=len(lang.word2index)\n    emb_layer = nn.Embedding(num_embeddings, EMBEDDING_DIM)\n    emb_layer.load_state_dict({'weight': weights_matrix})\n    if non_trainable:\n        emb_layer.weight.requires_grad = False\n\n    return emb_layer, num_embeddings, embedding_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self,input_size, hidden_size):\n        super(Encoder,self).__init__()\n        self.hidden_size = hidden_size\n        num_embeddings=len(lang.word2index)\n        #self.embedding = create_emb_layer(weights_matrix)\n        self.embedding = nn.Embedding(num_embeddings, EMBEDDING_DIM)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n    \n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        num_embeddings=len(lang.word2index)\n        #self.embedding = create_emb_layer(weights_matrix)\n        self.embedding = nn.Embedding(num_embeddings, EMBEDDING_DIM)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def indexesFromSentence(sentence):\n    #print(sentence)\n    sent=sentence.split()\n    sent.pop(0)\n    l=[]\n    c=0\n    #print(sent)\n    for word in sent:\n        word=word.replace('.','')\n        try:\n            l.append(lang.word2index[word])\n        except:\n            c+=1\n    return l\n\ndef tensorFromSentence(sentence):\n    indexes = indexesFromSentence(sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(pair[0].to_string().lower())\n    target_tensor = tensorFromSentence(pair[1].to_string().lower())\n    return (input_tensor, target_tensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"teacher_forcing=0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_with_replacement = dataset.sample(n=1,replace=True)\nprint(sample_with_replacement['ctext'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getrandom():\n    l = []\n    sample_with_replacement = dataset.sample(n=1,replace=True)\n    l.append(sample_with_replacement['ctext'])\n    l.append(sample_with_replacement['text'])\n    return l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calcLoss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size)\n    loss = 0\n    encoder_hidden=encoder.initHidden()\n    for i in range(input_length):\n        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n        encoder_outputs[i] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]])\n    decoder_hidden = encoder_hidden\n    \n    #Teacher forcing\n    for i in range(target_length): #, decoder_attention  , encoder_outputs\n        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n        loss += criterion(decoder_output, target_tensor[i])\n        decoder_input = target_tensor[i]\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(encoder, decoder, n_iters, learning_rate=0.01):\n    epoch_loss,epoch_acc=0,0\n    plot_losses = []\n    print_every=1000\n    print_loss_total = 0  \n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    training_pairs = [tensorsFromPair(getrandom())\n                      for i in range(n_iters)]\n    criterion = nn.NLLLoss()\n    encoder.train()\n    decoder.train()\n    for iter in range(1, n_iters + 1):\n        training_pair = training_pairs[iter - 1]\n        input_tensor = training_pair[0]\n        target_tensor = training_pair[1]\n        #print(input_tensor)\n        #break\n        loss = calcLoss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        #plot_loss_total += loss\n        \n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total / print_every\n            print_loss_total = 0\n            print(' (%d %d%%) %.4f', iter, iter / n_iters * 100, print_loss_avg)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n        encoder.eval()\n        decoder.eval()\n        for i in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[i],encoder_hidden)\n            encoder_outputs[i] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]])  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        #decoder_attentions = torch.zeros(max_length, max_length)\n\n        for i in range(max_length):#, decoder_attention , encoder_outputs\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            #decoder_attentions[i] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n            #, decoder_attentions[:di + 1]\n        return decoded_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_size = 100\nencoder = Encoder(lang.n_words, hidden_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder=Decoder(hidden_size, lang.n_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(encoder,decoder, 7500)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}