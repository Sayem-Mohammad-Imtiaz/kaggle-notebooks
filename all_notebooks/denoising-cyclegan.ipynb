{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install git+https://www.github.com/keras-team/keras-contrib.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import scipy.misc\n\nfrom keras.datasets import mnist\n# from keras_contrib.layers.normalization import InstanceNormalization\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\n\nimport datetime\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport os,cv2\nfrom glob import glob\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def noisy(img):    \n    gauss = np.random.normal(0,1,img.size)\n    gauss = gauss.reshape(img.shape[0],img.shape[1],1).astype('uint8')\n    # Add the Gaussian noise to the image\n    img_gauss = cv2.add(img,gauss)\n    # Display the image\n    #cv2.imshow('a',img_gauss)\n    cv2.waitKey(0)\n    return img_gauss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataLoader():\n    def __init__(self, img_res=(128, 128)):\n        self.img_res = img_res\n        \n\n    \n    def load_data(self,domaineA=False,domaineB=False, batch_size=1):\n        path1=sorted(glob('../input/chest-xray-pneumonia/chest_xray/test/NORMAL/*'))\n        path2=sorted(glob('../input/covidct/COVID-CT/CT_COVID/*'))\n        \n        \n        i=np.random.randint(0,27)\n        \n        batch1=path1[i*batch_size:(i+1)*batch_size]\n        batch2=path2[i*batch_size:(i+1)*batch_size]\n        imgs=[]\n        if domaineA :\n            for filename1 in batch1:\n                img=cv2.imread(filename1,0)\n                img=img[...,::-1]\n                img=noisy(img)\n                img=cv2.resize(img,self.img_res,interpolation=cv2.INTER_AREA)\n                imgs.append(img)\n        if domaineB :\n            for filename2 in batch2:\n                img=cv2.imread(filename2,0)\n                img=img[...,::-1]\n                img=cv2.resize(img,self.img_res,interpolation=cv2.INTER_AREA)\n                imgs.append(img)\n\n        imgs = np.reshape(imgs, (len(imgs), 128, 128, 1))\n        imgs=np.array(imgs)/127.5-1\n\n    \n        return imgs\n\n    def load_batch(self, batch_size=1):\n        path_A = sorted(glob('../input/chest-xray-pneumonia/chest_xray/train/NORMAL/*'))\n        path_B = sorted(glob('../input/covidct/COVID-CT/CT_COVID/*'))\n\n        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n        total_samples = self.n_batches * batch_size\n\n        # Sample n_batches * batch_size from each path list so that model sees all\n        # samples from both domains\n        path_A = np.random.choice(path_A, total_samples, replace=False)\n        path_B = np.random.choice(path_B, total_samples, replace=False)\n\n        for i in range(self.n_batches-1):\n            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n            imgs_A, imgs_B = [], []\n            for file1, file2 in zip(batch_A, batch_B):\n                img1 = cv2.imread(file1,0)\n                img2 = cv2.imread(file2,0)\n                img1 = img1[...,::-1]\n                img2 = img2[...,::-1]\n                img1 = noisy(img1)\n                img1 = cv2.resize(img1,self.img_res,interpolation=cv2.INTER_AREA)\n                img2 = cv2.resize(img2,self.img_res,interpolation=cv2.INTER_AREA)\n                imgs_A.append(img1)\n                imgs_B.append(img2)\n                \n            imgs_A = np.reshape(imgs_A, (len(imgs_A), 128, 128, 1))\n            imgs_B = np.reshape(imgs_B, (len(imgs_B), 128, 128, 1))\n            imgs_A = np.array(imgs_A)/127.5 - 1.\n            imgs_B = np.array(imgs_B)/127.5 - 1.\n\n            yield imgs_A, imgs_B\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CycleGAN():\n    def __init__(self, img_rows=128, img_cols=128, channels=1):\n        self.img_rows = img_rows \n        self.img_cols = img_cols\n        self.channels = channels\n        self.img_shape = (img_rows, img_cols, channels)\n        \n        self.data_loader = DataLoader( img_res=(self.img_rows, self.img_cols))\n        \n\n        \n        # Calculate output shape of D (PatchGAN)\n        patch = int(self.img_rows / 2**4)\n        self.disc_patch = (patch, patch, 1)\n        \n        # Number of filters in the first layer of G and D\n        self.gf = 32\n        self.df = 64\n        \n        # controls how strictly the cycle-consistency loss is enforced. Setting this value higher will ensure that you your\n        # original and reconstructed image are as close together as possible.\n        self.lambda_cycle = 10.0\n        # this value influences how dramatic are the changes—especially early in the training process.\n        # Setting a lower value leads to unnecessary changes—e.g. completely inverting the colors early on\n        self.lambda_id = 0.9 * self.lambda_cycle\n        \n        optimizer = Adam(0.0002, 0.5)\n        \n        # Build and compile the discriminators\n        self.d_a = self.build_discriminator()\n        self.d_b = self.build_discriminator()\n\n        self.d_a.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n        self.d_b.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n\n        # Build the generators\n        self.g_ab = self.build_generator()\n        self.g_ba = self.build_generator()\n        \n        # Input images from both domains\n        img_a = Input(shape=self.img_shape)\n        img_b = Input(shape=self.img_shape)\n        \n        # Translate images to the other domain\n        fake_b = self.g_ab(img_a)\n        fake_a = self.g_ba(img_b)\n        \n        # Translate images back to original domain\n        recon_a = self.g_ba(fake_b)\n        recon_b = self.g_ab(fake_a)\n        \n        # Identity mapping of images\n        img_a_id = self.g_ba(img_a)\n        img_b_id = self.g_ab(img_b)\n        \n        # For the combined model we will only train the generators\n        self.d_a.trainable = False\n        self.d_b.trainable = False\n        \n        # Discriminators determines validity of translated images\n        valid_a = self.d_a(fake_a)\n        valid_b = self.d_b(fake_b)\n        \n        # Combined model trains generators to fool discriminators\n        self.combined = Model(inputs=[img_a, img_b], output=[valid_a, valid_b, recon_a, recon_b, img_a_id, img_b_id])\n        self.combined.compile(loss=['mse', 'mse', 'mae', 'mae', 'mae', 'mae'],\n                             loss_weights=[1, 1, self.lambda_cycle, self.lambda_cycle, self.lambda_id, self.lambda_id], optimizer=optimizer)\n        \n    @staticmethod\n    def conv2d(layer_input, filters, f_size=4, normalization=True):\n        \"\"\"Layers used during downsampling\"\"\"\n        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n        d = LeakyReLU(alpha=0.2)(d)\n        if normalization:\n            d = InstanceNormalization()(d)\n\n        return d\n\n    @staticmethod\n    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n        \"\"\"Layers used during upsampling\"\"\"\n        u = UpSampling2D(size=2)(layer_input)\n        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n        if dropout_rate:\n            u = Dropout(dropout_rate)(u)\n        u = InstanceNormalization()(u)\n        u = Concatenate()([u, skip_input])\n\n        return u\n        \n    # Next, we build the generator code, which uses the residual skip connections as we\n    # described earlier. This is a so-called “U-Net” architecture, which is simpler to write\n    # than the ResNet architecture, which some implementations use.\n    def build_generator(self):\n        \"\"\"U-net Generator\"\"\"\n        # Image input\n        d0 = Input(shape=self.img_shape)\n        \n        # Downsampling\n        d1 = self.conv2d(d0, self.gf)\n        d2 = self.conv2d(d1, self.gf * 2)\n        d3 = self.conv2d(d2, self.gf * 4)\n        d4 = self.conv2d(d3, self.gf * 8)\n        \n        # Upsampling\n        u1 = self.deconv2d(d4, d3, self.gf * 4)\n        u2 = self.deconv2d(u1, d2, self.gf * 2)\n        u3 = self.deconv2d(u2, d1, self.gf)\n        \n        u4 = UpSampling2D(size=2)(u3)\n        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n        \n        return Model(d0, output_img)\n        \n    def build_discriminator(self):\n        img = Input(shape=self.img_shape)\n        \n        d1 = self.conv2d(img, self.df, normalization=False)\n        d2 = self.conv2d(d1, self.df * 2)\n        d3 = self.conv2d(d2, self.df * 4)\n        d4 = self.conv2d(d3, self.df * 8)\n        \n        validity = Conv2D(1, kernel_size=4, strides=1, padding='same',activation='sigmoid')(d4)\n        \n        return Model(img, validity)\n    \n    def sample_images(self, epoch, batch_i):\n        r, c = 2, 3\n\n        imgs_a = self.data_loader.load_data(domaineA=True ,domaineB=False, batch_size=1)\n        imgs_b = self.data_loader.load_data(domaineA=False,domaineB=True, batch_size=1)\n        \n        # Translate images to the other domain\n        fake_b = self.g_ab.predict(imgs_a)\n        fake_a = self.g_ba.predict(imgs_b)\n        # Translate back to original domain\n        reconstr_a = self.g_ba.predict(fake_b)\n        reconstr_b = self.g_ab.predict(fake_a)\n\n        gen_imgs = np.concatenate([imgs_a, fake_b, reconstr_a, imgs_b, fake_a, reconstr_b])\n\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        titles = ['Original', 'Translated', 'Reconstructed']\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt].reshape(128,128), cmap=\"gray\")\n                #axs[i,j].imshow(gen_imgs[cnt])\n                axs[i, j].set_title(titles[j])\n                axs[i,j].axis('off')\n                cnt += 1\n        fig.savefig(\"./%d.png\" % (epoch))\n        plt.show()\n    \n    def train(self, epochs, batch_size=1, sample_interval=50):\n        start_time = datetime.datetime.now()\n        \n        valid = np.ones((batch_size,) + self.disc_patch)\n        fake = np.zeros((batch_size,) + self.disc_patch)\n        \n        for epoch in range(epochs):\n            for batch_i, (imgs_a, imgs_b) in enumerate(self.data_loader.load_batch(batch_size)):\n                # ----------------------\n                #  Train Discriminators\n                # ----------------------\n                \n                # Translate images to opposite domain\n                fake_b = self.g_ab.predict(imgs_a)\n                fake_a = self.g_ba.predict(imgs_b)\n                \n                # Train the discriminators (original images = real / translated = Fake)\n                da_loss_real = self.d_a.train_on_batch(imgs_a, valid)\n                da_loss_fake = self.d_a.train_on_batch(fake_a, fake)\n                da_loss = 0.5 * np.add(da_loss_real, da_loss_fake)\n\n                db_loss_real = self.d_b.train_on_batch(imgs_b, valid)\n                db_loss_fake = self.d_b.train_on_batch(fake_b, fake)\n                db_loss = 0.5 * np.add(db_loss_real, db_loss_fake)\n\n                # Total discriminator loss\n                d_loss = 0.5 * np.add(da_loss, db_loss)\n                \n                # ------------------\n                #  Train Generators\n                # ------------------\n                g_loss = self.combined.train_on_batch([imgs_a, imgs_b], [valid, valid, imgs_a, imgs_b, imgs_a, imgs_b])\n\n                if batch_i % sample_interval == 0:\n                    self.sample_images(epoch, batch_i)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan = CycleGAN()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan.train(epochs=100, batch_size=64, sample_interval=10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}