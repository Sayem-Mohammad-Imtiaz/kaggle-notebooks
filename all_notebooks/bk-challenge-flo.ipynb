{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Customer Churn at a Bank\n### Introduction to the case\nEvery month BankCo loses thousands of customers to it’s competitors. The customers who leave the bank are known commonly as ‘\nchurned customers’. BankCo has asked you, a data scientist, to help them predict which customers may churn in future so that they can ta ke steps to incentivise those customers to stay.\n### What is required and how will by submission be evaluated?\nTo predict which customers will churn you will need to build and train a classification model in Python or R using the data h\nost ed here:\nhttps:// www.kaggle.com adammaus /predicting churn for bank customers"},{"metadata":{},"cell_type":"markdown","source":"# Answers\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing librairis\nimport pandas as pd\nimport plotly.graph_objs as go\nimport plotly.io as pio\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np\n#importing machine learing models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the churn_modelling dataset\nTrain_data=pd.read_csv('../input/predicting-churn-for-bank-customers/Churn_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Manipulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the columns are not interesting to use right now, therefore lets remove them in our datasets for the moment\n#to have fewer columns.\nTrain_data=Train_data.drop(['RowNumber','CustomerId','Surname'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"Train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis\nThis is an approach that I will be using to analyse our dataset, to check their main characteristics,\nand to see if I can see partterns, to spot anomalies, and check assumptions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am going to check for unique values in the data attributes\nTrain_data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am going to check for how many rows and columns is my dataset\nTrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am going to check for the standard deviation, medium, mean to help me understand how spread out a data set is.\n# A high standard deviation will implies that, on average, data points will look spread out (far from the average)\n#, and a low standard of deviation, close from the average\n\nTrain_data_frame1=Train_data[['CreditScore','Geography','Age','Tenure','Balance','EstimatedSalary']]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the standard deviation\nTrain_data_frame1.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the mean\nTrain_data_frame1.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the median\nTrain_data_frame1.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentages of people per category (exited or stayed)\n\n#def Graph():\nperc_people_cat=round(100*Train_data['Exited'].value_counts(normalize=True),2)\ntext=[]\nfor i in range (0,2):\n    text.append(('{}%'.format(perc_people_cat[i])))\n    \ncolors = ['purple','green']\n\n\n# Building a graph to vizualize what has been found above\n\ndata=go.Bar(y=perc_people_cat,\n             text=text,\n             textposition='auto',\n\n            marker={'color': colors}\n\n\n\n            )\n\nlayout=  go.Layout(\n                        title = ' Data distribution ',\n                        xaxis={'title':'0 for stayed and 1 for exited'},\n                        yaxis={'title':'Customers in Percentage'}\n\n\n                            )\n\n                                \n\nfig = go.Figure(data=data, layout=layout)\n\npio.show(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this chart, we can see that the percentage of the customer who exited is less with only 20.37% whereas the percentage of the customer who stayed are the majority with 79.63%."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Geographical distribution\n#Exited\ncustomer_france_ex=Train_data[(Train_data['Geography']=='France') & (Train_data['Exited']==1)]\nnum_customer_france_ex=[customer_france_ex['Exited'].count()]\ncustomer_Spain_ex=Train_data[(Train_data['Geography']=='Spain') & (Train_data['Exited']==1)]\nnum_customer_Spain_ex=[customer_Spain_ex['Exited'].count()]\ncustomer_Germany_ex=Train_data[(Train_data['Geography']=='Germany') & (Train_data['Exited']==1)]\nnum_customer_Germany_ex=[customer_Germany_ex['Exited'].count()]\n#Stayed\ncustomer_france_st=Train_data[(Train_data['Geography']=='France') & (Train_data['Exited']==0)]\nnum_customer_france_st=[customer_france_st['Exited'].count()]\ncustomer_Spain_st=Train_data[(Train_data['Geography']=='Spain') & (Train_data['Exited']==0)]\nnum_customer_Spain_st=[customer_Spain_st['Exited'].count()]\ncustomer_Germany_st=Train_data[(Train_data['Geography']=='Germany') & (Train_data['Exited']==0)]\nnum_customer_Germany_st=[customer_Germany_st['Exited'].count()]\n#Building the graph for vizualization\ntrace1=go.Bar(x=['exited'],\n              y=num_customer_france_ex,\n              name='France',\n                  \n             text='France',\n             textposition='auto'\n\n            #marker={'color': colors}\n\n            )\ntrace2=go.Bar(x=['exited'],\n             y=num_customer_Spain_ex,\n              name='Spain',\n             text='Spain',\n             textposition='auto'\n\n            #marker={'color': colors}\n\n            )\ntrace3=go.Bar(x=['exited'],\n                y=num_customer_Germany_ex,\n              name='Germany',\n              \n             text='Germany',\n             textposition='auto'\n\n            #marker={'color': colors}\n\n            )\ntrace4=go.Bar(x=['Stayed'],\n              y=num_customer_france_st,\n              name='France',\n                  \n            text='France',\n             textposition='auto'\n\n            #marker={'color': colors}\n\n            )\ntrace5=go.Bar(x=['Stayed'],\n              y=num_customer_Spain_st,\n              name='Spain',\n            text='Spain',\n             textposition='auto'\n\n            #marker={'color': colors}\n\n            )\ntrace6=go.Bar(x=['Stayed'],\n              y=num_customer_Germany_st,\n              name='Germany',\n                  \n            text='Germany',\n             textposition='auto'\n\n            #marker={'color': colors}\n\n            )\ndata = [trace1, trace2,trace3,trace4,trace5,trace6]\n\nlayout=  go.Layout(\n                        title = 'Geographical distribution ',\n                        #xaxis={'title':'0 for stayed and 1 for exited'},\n                        yaxis={'title':'Customers'},\n                        barmode='stack'\n\n\n                            )                    \n\nfig = go.Figure(data=data, layout=layout)\n\npio.show(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is the geographical distribution. As you can see we have around 2000 clients who exited and around 8000 clients who stayed. For client who exited, Germany and France seems to have almost the same proportion, while Spain has a small proportion. On the other hand, on the clients who stayed, France seems to have a larger proportion, followed by spain then Germany. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gender Distribution\n#Exited\nExited_Male=Train_data[(Train_data['Gender']=='Male') & (Train_data['Exited']==1)].count()\nExited_Female=Train_data[(Train_data['Gender']=='Female') & (Train_data['Exited']==1)].count()\n#Stayed\nStayed_Male=Train_data[(Train_data['Gender']=='Male') & (Train_data['Exited']==0)].count()\nStayed_Female=Train_data[(Train_data['Gender']=='Female') & (Train_data['Exited']==0)].count()\n#Building the graph\ntrace1=go.Bar(x=['exited'],\n              y=Exited_Male,\n              name='Male',\n              text='Male',\n             textposition='auto'\n\n            )\ntrace2=go.Bar(x=['exited'],\n              y=Exited_Female,\n              name='Female',\n              text='Female',\n             textposition='auto'\n\n            )\ntrace3=go.Bar(x=['Stayed'],\n              y=Stayed_Male,\n              name='Male',\n                 text='Male',\n             textposition='auto'\n\n            )\ntrace4=go.Bar(x=['Stayed'],\n              y=Stayed_Female,\n              name='Female',\n                 text='Female',\n             textposition='auto'\n\n            )\ndata = [trace1, trace2,trace3,trace4]\nlayout=  go.Layout(\n                        title = 'Gender distribution ',\n                        #xaxis={'title':'0 for stayed and 1 for exited'},\n                        yaxis={'title':'Customers'},\n                        barmode='stack'\n\n\n                            )                    \n\nfig = go.Figure(data=data, layout=layout)\n\npio.show(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the gender distribution above, we can see that the number of Female that exited was a little bit higher that the Male, and that the number of Male who stayed or did not exit are a little bit more higher than the female."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Age distribution\n#Exited customers\nAge_Exited_dataframe=Train_data[['Age','Exited']][Train_data['Exited']==1].groupby('Age').count()\n#Stayed customers\nAge_Stayed_dataframe=Train_data[['Age','Exited']][Train_data['Exited']==0].groupby('Age').count()\nAge_Exited_dataframe['Age'] = Age_Exited_dataframe.index\nAge_Stayed_dataframe['Age'] = Age_Stayed_dataframe.index\nX1=Age_Exited_dataframe['Age']\nY1=Age_Exited_dataframe['Exited']\nX2=Age_Stayed_dataframe['Age']\nY2=Age_Stayed_dataframe['Exited']\n\n#BUilding the graph\ntrace1=go.Bar(x=X1,\n              y=Y1,\n             name='Exited',\n\n            )\ntrace2=go.Bar(x=X2,\n              y=Y2,\n             name='Stayed'\n\n            )\ndata = [trace1, trace2]\nlayout=  go.Layout(\n                        title = 'Age distribution ',\n                        xaxis={'title':'Age'},\n                        yaxis={'title':'Customers'},\n\n                            )                    \n\nfig = go.Figure(data=data, layout=layout)\n\npio.show(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can click on the legend on your upper right, to display only on graph at a time. And from the exited bars, you can easily notice that customers with the age between 40 years old and 50 years old are the one that exit most compare to the customers in between 18 years old and 39 years old but also from 51 years old to 70's. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am going to get the correlation matrix so that I can understand the relationship between all this variables\n#in our datasets. For me to make sure I strengthen my model, I am going to identify and try\n#reducing the features in our datasets that are highly correlated.\ncorrelation=Train_data[Train_data.columns[:10]].corr()\n\n# I am going to vizualize it by using  a  heatmap. \n\nsns.set()\nsns.set(font_scale = 1)\nsns.heatmap(correlation,cmap='coolwarm', annot = True,fmt = \".2f\",annot_kws={'size':15})\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for outliers\nIn this part, I am going to check if there is outliers in my data, and think of a way of handling them"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Below is the function which will be helping us to detect outliers\ntrace = []\ndef boxplot(df):\n    for element in df:\n        trace.append(\n            go.Box(\n                name = element,\n                y = df[element]\n            )\n        )\n#credit Score plot box\ncredit_Score_df=Train_data[Train_data.columns[:1]]\nboxplot(credit_Score_df)\ndata=trace\nlayout=  go.Layout(\n                        title = 'Credit Score plot box ')\n                        \n\n                                \nfig = go.Figure(data=data, layout=layout)\npio.show(fig)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph above shows that for Age there are outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Below is the function which will be helping us to detect outliers\ntrace = []\ndef boxplot(df):\n    for element in df:\n        trace.append(\n            go.Box(\n                name = element,\n                y = df[element]\n            )\n        )\n#Age plot box\nage_df=Train_data[Train_data.columns[3:4]]\nboxplot(age_df)\ndata=trace\nlayout=  go.Layout(\n                        title = 'Age plot box ')\n                        \n\n                                \nfig = go.Figure(data=data, layout=layout)\npio.show(fig)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph above shows that for Age there are outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Below is the function which will be helping us to detect outliers\ntrace = []\ndef boxplot(df):\n    for element in df:\n        trace.append(\n            go.Box(\n                name = element,\n                y = df[element]\n            )\n        )\n#Tenure plot box\nTenure_df=Train_data[Train_data.columns[4:5]]\nboxplot(Tenure_df)\ndata=trace\nlayout=  go.Layout(\n                        title = 'Tenure plot box ')\n                        \n\n                                \nfig = go.Figure(data=data, layout=layout)\npio.show(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph above shows that everything looks normal"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Below is the function which will be helping us to detect outliers\ntrace = []\ndef boxplot(df):\n    for element in df:\n        trace.append(\n            go.Box(\n                name = element,\n                y = df[element]\n            )\n        )\n#Balance plot box\nBalance_df=Train_data[Train_data.columns[5:6]]\nboxplot(Balance_df)\ndata=trace\nlayout=  go.Layout(\n                        title = 'Tenure plot box ')\n                        \n\n                                \nfig = go.Figure(data=data, layout=layout)\npio.show(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph above shows that everything looks normal"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Below is the function which will be helping us to detect outliers\ntrace = []\ndef boxplot(df):\n    for element in df:\n        trace.append(\n            go.Box(\n                name = element,\n                y = df[element]\n            )\n        )\n#Estimated Salary plot box\nEstimatedSalary_df=Train_data[Train_data.columns[9:10]]\nboxplot(EstimatedSalary_df)\ndata=trace\nlayout=  go.Layout(\n                        title = 'Estimated Salary plot box ')\n                        \n\n                                \nfig = go.Figure(data=data, layout=layout)\npio.show(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph above shows that everything looks normal\n"},{"metadata":{},"cell_type":"markdown","source":"# Converting Categorical columns to numeric columns\nAs machine learning algorthms work best with numerical data , yet we have got columns namely gender and geography that categorical, we are going to proceed to the conversion from categorical to numerical.  I am going to use one-hot encoding to easy the process as we have 3 different categories. So 1 will mean that a country is present on that row and 0 will mean abscence."},{"metadata":{"trusted":true},"cell_type":"code","source":"# One-Hot encoding our categorical attributes\ncategory_list = ['Geography', 'Gender']\nTrain_data = pd.get_dummies(Train_data, columns = category_list, prefix = category_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking Feature importance"},{"metadata":{},"cell_type":"markdown","source":"To have an idea of which features are more important than others, I am going to use Random Forest classifier. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = Train_data.drop('Exited', axis=1)\ny = Train_data.Exited\nlabels = X.columns\nforest = RandomForestClassifier (n_estimators = 5000, random_state = 0, n_jobs = -1)\nforest.fit(X, y)\nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\n# Vizualizing the graph\nplt.title('Importance of the features')\nplt.bar(range(X.shape[1]), importances[indices], color = \"blue\", align = \"center\")\nplt.xticks(range(X.shape[1]), labels, rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph above is showing how important are the features from the most important on the left which is creditscore to the least important which are genders in our case"},{"metadata":{},"cell_type":"markdown","source":"# Choosing Machine learning Algorithms"},{"metadata":{},"cell_type":"markdown","source":"In this section, I am going to use different machine learning algorthms so that We can have the one that performa better. I will be comparing the accuracy as well. As we are trying to classify, I have chosen to use Logistic Regression, K-Nearest Neighbor (KNN), and RandomForest as I believe there are one of the most use during the solution of classsification problems and it has been proved that they are also reliable."},{"metadata":{},"cell_type":"markdown","source":"#### Before I start building the models, let me first split my dataset into the training data that I will give 70%, and the testing data that I will give 30%"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting my dataset into 70% training and 30% testing\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training our algorithm and then start the prediction\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First Ransom Forest\nclassifier = RandomForestClassifier(n_estimators=5000, random_state=0)  \nclassifier.fit(X_train, y_train)  \npredictions = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#then we are going to evaluate how the algorithm we just trained perform by looking at different metrics as it will be printed below\nprint(classification_report(y_test,predictions ))  \nprint(accuracy_score(y_test, predictions ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Second: Logistic regression\n# Initialization of the Logistic Regression\nLogistic_model = LogisticRegression(penalty = 'l2', dual = False, tol = 0.0002, C = 1.0, fit_intercept = True,\n                            intercept_scaling = 1, class_weight = None, \n                            random_state = None, solver = 'liblinear', max_iter = 100,\n                            multi_class = 'ovr', verbose = 2)\n# Fitting the model with training data \nLogistic_model.fit(X_train, y_train)\npredict_logistic = Logistic_model.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#then we are going to evaluate how the algorithm we just trained perform by looking at different metrics as it will be printed below\nprint(classification_report(y_test,predict_logistic ))  \nprint(accuracy_score(y_test, predict_logistic ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Third: K-Nearest Neighbor (KNN)\n\nKnn_model = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform', algorithm = 'ball_tree', leaf_size = 30, p = 2,\n                             metric = 'minkowski', metric_params = None)\n# Fitting the model with training data \nKnn_model.fit(X_train, y_train)\npredict_KNN = Knn_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#then we are going to evaluate how the algorithm we just trained perform by looking at different metrics as it will be printed below\nprint(classification_report(y_test,predict_KNN ))  \nprint(accuracy_score(y_test, predict_KNN ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen above, between the three algorithm used, Random Forest has most high accuracy with almost 87%, followed by Logistic regression which has an accuracy of almost 80% and then finally the KNN with the accuracy of almost 76%. If I were to choose only one algorithm to use, I would choose the random Forest, as if you look at it F1-score, it shoes that it is high, therefore more balanced.\nPrecision and recall are two extremely important model evaluation metrics. Precision refers to the percentage of your results which are relevant, recall refers to the percentage of total relevant results correctly classified by the three chosen algorithm above.\nThe F1 score conveys the balance between the precision and the recall."},{"metadata":{},"cell_type":"markdown","source":"**Optimization***\nIf I had more time, I could use the cross-validation and Hyperparameter tuning technique in order to improve the accuracy of the classifiers \n\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"qwdqw"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}