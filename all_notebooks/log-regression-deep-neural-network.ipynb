{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Preparation**","metadata":{}},{"cell_type":"markdown","source":"# 1.1 Import Library & Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T15:25:31.318274Z","iopub.execute_input":"2021-06-23T15:25:31.318729Z","iopub.status.idle":"2021-06-23T15:25:31.330905Z","shell.execute_reply.started":"2021-06-23T15:25:31.318691Z","shell.execute_reply":"2021-06-23T15:25:31.329939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/star-categorization-giants-and-dwarfs/Star3642_balanced.csv\")\ndata","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:31.404494Z","iopub.execute_input":"2021-06-23T15:25:31.405222Z","iopub.status.idle":"2021-06-23T15:25:31.454476Z","shell.execute_reply.started":"2021-06-23T15:25:31.405179Z","shell.execute_reply":"2021-06-23T15:25:31.453041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.2 Exploratory Analysis","metadata":{}},{"cell_type":"markdown","source":"Check whether there are vacancies or outliers in the data set, so as to determine the work of data preprocess","metadata":{}},{"cell_type":"code","source":"print(data.isnull().sum())\nprint(data.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:31.456196Z","iopub.execute_input":"2021-06-23T15:25:31.456575Z","iopub.status.idle":"2021-06-23T15:25:31.471467Z","shell.execute_reply.started":"2021-06-23T15:25:31.456543Z","shell.execute_reply":"2021-06-23T15:25:31.469931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:31.474052Z","iopub.execute_input":"2021-06-23T15:25:31.474404Z","iopub.status.idle":"2021-06-23T15:25:31.525132Z","shell.execute_reply.started":"2021-06-23T15:25:31.474372Z","shell.execute_reply":"2021-06-23T15:25:31.523903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the result, there's no need to delete null or na value. But it's necessary to delete some outliers in Plx, causing the std of Plx is too high.\n","metadata":{}},{"cell_type":"markdown","source":"# 1.3 Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Delete the data in the top 5% of e_Plx\ndata = data.drop(data[ data['e_Plx'] > data['e_Plx'].quantile(q=0.95)].index)\ndata","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:31.52677Z","iopub.execute_input":"2021-06-23T15:25:31.527275Z","iopub.status.idle":"2021-06-23T15:25:31.555967Z","shell.execute_reply.started":"2021-06-23T15:25:31.527235Z","shell.execute_reply":"2021-06-23T15:25:31.554694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete irrelevant variables\ndel data['Vmag']\ndel data['e_Plx']\ndel data['SpType']\ndata","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:31.557882Z","iopub.execute_input":"2021-06-23T15:25:31.55822Z","iopub.status.idle":"2021-06-23T15:25:31.584345Z","shell.execute_reply.started":"2021-06-23T15:25:31.558188Z","shell.execute_reply":"2021-06-23T15:25:31.583021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw scatterplot matrix\nsns.pairplot(data,hue='TargetClass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:31.586214Z","iopub.execute_input":"2021-06-23T15:25:31.586552Z","iopub.status.idle":"2021-06-23T15:25:35.799462Z","shell.execute_reply.started":"2021-06-23T15:25:31.586521Z","shell.execute_reply":"2021-06-23T15:25:35.798075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Build algorithm**","metadata":{}},{"cell_type":"markdown","source":"# 2.1 foundation function","metadata":{}},{"cell_type":"code","source":"# Classification function: sigmoid function\n#    which is denoted: sigmoid(x)\n# Learning rate update algorithm: Exponential decay algorithm. \n#    which is denoted: e_decay(alpha,decay_rate,epoch)\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef e_decay(alpha,decay_rate,epoch):\n    #alpha: learning rate\n    #decay_rate: decay rate, which is given previously\n    #epoch: The current number of rounds of the algorithm loop\n        return alpha/(1+decay_rate*epoch)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.801341Z","iopub.execute_input":"2021-06-23T15:25:35.801837Z","iopub.status.idle":"2021-06-23T15:25:35.809307Z","shell.execute_reply.started":"2021-06-23T15:25:35.801787Z","shell.execute_reply":"2021-06-23T15:25:35.807943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2 logistic regression algorithm","metadata":{}},{"cell_type":"code","source":"# Classification function: sigmoid function\n# Cost function: maximum likelihood estimation function\n# Weight optimization algorithm: Gradient descent method\n# Learning rate update algorithm: Exponential decay algorithm. \nclass LogRegression():\n        def __init__(self,numFeatures):\n                self.weights=np.array([1]*numFeatures)\n        def train(self,x,y,alpha,steps):\n            # Gradient descent method\n            for step in range(steps):\n                output=sigmoid(x.dot(self.weights.T)).T\n                err=y-output\n                # Consider the Cost function is maximum likelihood estimation function,\n                # so we have: weights-=weights-alpha*err*x\n                self.weights=self.weights+alpha*err.dot(x)\n                # Update learning rate\n                alpha=e_decay(alpha,decay_rate=1/(10*steps),epoch=step)\n        def predict(self,x,y):\n                numSamples=np.shape(x)[0]\n                correct_num=0\n                y_pred=np.array([])\n                for i in range(numSamples):\n                    predict=sigmoid(x[i,:].dot(self.weights))\n                    if predict>=0.5:\n                        y_pred=np.append(y_pred,1)\n                    else:\n                        y_pred=np.append(y_pred,0)\n                    if (predict>=0.5)==bool(y[i]):\n                        correct_num+=1\n                accuracy=float(correct_num)/numSamples\n                #Return prediction results and accuracy\n                return y_pred,accuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.811609Z","iopub.execute_input":"2021-06-23T15:25:35.81201Z","iopub.status.idle":"2021-06-23T15:25:35.825694Z","shell.execute_reply.started":"2021-06-23T15:25:35.811963Z","shell.execute_reply":"2021-06-23T15:25:35.824013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3 deep neural network","metadata":{}},{"cell_type":"code","source":"class Neuron():\n    def __init__(self, weights, bias):\n        self.weights = weights\n        self.bias = bias","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.828189Z","iopub.execute_input":"2021-06-23T15:25:35.828561Z","iopub.status.idle":"2021-06-23T15:25:35.84266Z","shell.execute_reply.started":"2021-06-23T15:25:35.828514Z","shell.execute_reply":"2021-06-23T15:25:35.841628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use for calaulating partial derivative\ndef deriv_sigmoid(x):\n    fx = sigmoid(x)\n    return fx * (1 - fx)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.843941Z","iopub.execute_input":"2021-06-23T15:25:35.844263Z","iopub.status.idle":"2021-06-23T15:25:35.857839Z","shell.execute_reply.started":"2021-06-23T15:25:35.844231Z","shell.execute_reply":"2021-06-23T15:25:35.856623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification function: sigmoid function\n# Cost function: MSE --> use for calaulating partial derivative\n# Weight optimization algorithm: Gradient descent method\n# Learning rate update algorithm: Exponential decay algorithm. \nclass NeuralNetwork():\n    def __init__(self,numFeatures):\n        # Hidden layers neuron\n        # Which has \"numFeatures\" neurons, and every neuron has \"numFeatures\" weights and 1 bias\n        self.h_layers=[(Neuron(weights=np.array([1]*numFeatures,dtype='float64'),bias=0)) for _ in range(numFeatures)]\n        # Output neuron\n        # 1 neuron, which has \"numFeatures\" weights and 1 bias\n        self.o_node=Neuron(weights=np.array([1]*numFeatures,dtype='float64'),bias=0)\n    def feedforward(self, x):#feed forward network\n        h_lst=[]\n        for h_node in self.h_layers:\n            sum_h=h_node.weights.dot(x)+h_node.bias\n            h_lst.append(sigmoid(sum_h))\n        sum_o=self.o_node.weights.dot(np.array(h_lst))+self.o_node.bias\n        y_pred=sigmoid(sum_o)\n        return y_pred\n    def train(self,train_x,train_y,alpha,steps):\n        for epoch in range(steps):\n            for x, y in zip(train_x,train_y):\n                # Calculate prediction of train data set\n                sum_h_lst=[]\n                h_lst=[]\n                for h_node in self.h_layers:\n                    sum_h=h_node.weights.dot(x)+h_node.bias\n                    sum_h_lst.append(sum_h)\n                    h_lst.append(sigmoid(sum_h))\n                sum_o=self.o_node.weights.dot(np.array(h_lst))+self.o_node.bias\n                y_pred=sigmoid(sum_o)\n                # Calaulate partial derivative\n                d_L_d_ypred = -2 * (y - y_pred)\n                # Output node\n                d_ypred_d_w=[]\n                for h in h_lst:\n                    d_ypred_d_w.append(h*deriv_sigmoid(sum_o))\n                d_ypred_d_b=deriv_sigmoid(sum_o)\n                d_ypred_d_h=[]\n                for w in self.o_node.weights:\n                    d_ypred_d_h.append(w*deriv_sigmoid(sum_o))\n                # Hidden layers\n                d_h_d_w=[]\n                d_h_d_b=[]\n                for sum_h in sum_h_lst:\n                    temp=[]\n                    for i in range(len(x)):\n                        temp.append(x[i]*deriv_sigmoid(sum_h))\n                    d_h_d_w.append(temp)\n                    d_h_d_b.append(deriv_sigmoid(sum_h))\n                # Update weights and bias: using the result of partial derivative\n                # Output node\n                for w_index in range(len(self.o_node.weights)):\n                    self.o_node.weights[w_index] -= alpha * d_L_d_ypred * d_ypred_d_w[w_index]\n                self.o_node.bias -= alpha * d_L_d_ypred * d_ypred_d_b\n                # Hidden layers\n                for h_index in range(len(self.h_layers)):\n                    for w_index in range(len(h_node.weights)):\n                        self.h_layers[h_index].weights[w_index]-=alpha*d_L_d_ypred*d_ypred_d_h[h_index]*d_h_d_w[h_index][w_index]\n                    self.h_layers[h_index].bias -= alpha * d_L_d_ypred * d_ypred_d_h[h_index] * d_h_d_b[h_index]\n            # Update learning rate\n            alpha=e_decay(alpha,decay_rate=1/(10*steps),epoch=epoch)    \n    def predict(self,test_x,test_y):\n        numSamples=np.shape(test_x)[0]\n        correct_num=0\n        y_pred=np.array([])\n        for x, y in zip(test_x,test_y):\n            predict=self.feedforward(x)\n            if predict>=0.5:\n                y_pred=np.append(y_pred,1)\n            else:\n                y_pred=np.append(y_pred,0)\n            if (predict>=0.5)==bool(y):\n                correct_num+=1\n        accuracy=float(correct_num)/numSamples\n        return y_pred,accuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.859935Z","iopub.execute_input":"2021-06-23T15:25:35.86029Z","iopub.status.idle":"2021-06-23T15:25:35.883895Z","shell.execute_reply.started":"2021-06-23T15:25:35.860258Z","shell.execute_reply":"2021-06-23T15:25:35.882818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Train & Prediction**","metadata":{}},{"cell_type":"markdown","source":"# 3.1 Create Training Set & Test Set","metadata":{}},{"cell_type":"code","source":"# Train data set\ntrain_data=data.sample(frac=0.5,random_state=1999)\ntrain_x=np.array(train_data.loc[:,['Plx','B-V','Amag']])\ntrain_y=np.array(train_data['TargetClass'])\n# Test data set\ntest_data=data[~data.index.isin(train_data.index)]\ntest_x=np.array(test_data.loc[:,['Plx','B-V','Amag']])\ntest_y=np.array(test_data['TargetClass'])\n\ntrain_x,train_y","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.885388Z","iopub.execute_input":"2021-06-23T15:25:35.88571Z","iopub.status.idle":"2021-06-23T15:25:35.913239Z","shell.execute_reply.started":"2021-06-23T15:25:35.88568Z","shell.execute_reply":"2021-06-23T15:25:35.912099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numFeatures=np.shape(train_x)[1]\n# Data Standardization\nfor i in range(numFeatures):\n    maxx=train_x[:,i].max()\n    minn=train_x[:,i].min()\n    train_x[:,i]=(train_x[:,i]-minn)/(maxx-minn)\n    test_x[:,i]=(test_x[:,i]-minn)/(maxx-minn)\n\ntrain_x,train_y","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.915311Z","iopub.execute_input":"2021-06-23T15:25:35.915637Z","iopub.status.idle":"2021-06-23T15:25:35.927221Z","shell.execute_reply.started":"2021-06-23T15:25:35.915607Z","shell.execute_reply":"2021-06-23T15:25:35.926475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.2 Logistic Regression Prediction","metadata":{}},{"cell_type":"code","source":"log_reg=LogRegression(numFeatures)\nlog_reg.train(train_x,train_y,alpha=0.1,steps=10000)\ny_pred,accuracy=log_reg.predict(test_x,test_y)\nprint('accuracy=',accuracy)\nsns.set()\nc_m= confusion_matrix(test_y,y_pred,labels=[0,1])\nsns.heatmap(c_m,square=True,annot=True,cmap='Blues',fmt='.20g')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:35.928948Z","iopub.execute_input":"2021-06-23T15:25:35.929639Z","iopub.status.idle":"2021-06-23T15:25:36.888949Z","shell.execute_reply.started":"2021-06-23T15:25:35.92957Z","shell.execute_reply":"2021-06-23T15:25:36.887869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3 Deep Neural Network Prediction","metadata":{}},{"cell_type":"code","source":"network=NeuralNetwork(numFeatures)\nnetwork.train(train_x,train_y,alpha=0.1,steps=100)\ny_pred,accuracy=network.predict(test_x,test_y)\nprint('accuracy=',accuracy)\nsns.set()\nc_m= confusion_matrix(test_y,y_pred,labels=[0,1])\nsns.heatmap(c_m,square=True,annot=True,cmap='Blues',fmt='.20g')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T15:25:36.89025Z","iopub.execute_input":"2021-06-23T15:25:36.890595Z","iopub.status.idle":"2021-06-23T15:30:31.770113Z","shell.execute_reply.started":"2021-06-23T15:25:36.890551Z","shell.execute_reply":"2021-06-23T15:30:31.769052Z"},"trusted":true},"execution_count":null,"outputs":[]}]}