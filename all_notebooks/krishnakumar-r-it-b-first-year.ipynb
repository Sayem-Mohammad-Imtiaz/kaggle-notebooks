{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#I. Exploratory Dataset Analysis\n\n\n# Q.1. Basic ploting for analysing this dataset taking all needed parameters and constrains.\n\nimport pandas as pd;\nimport numpy as np;\nimport matplotlib.pyplot as plt;\ndata=pd.read_csv(\"../input/heart-disease-uci/heart.csv\");\ndata.head()\nimport seaborn as sns\nsns.set_theme();\nfig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(data.age, data.target);\nax.set(title=\"The bar plot of ages, which has heart disease\",\n       xlabel=\"Age\",\n       ylabel=\"Target (0-Not Disease, 1-Disease)\");\ndata.hist(column=[\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\", \"cp\"], figsize = (10,10));\n# Plotting the cholestal and max heart rate using scatter plot, which the age are above 50\nover_50 = data[data.age > 50]\nover_50.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:36:20.725496Z","iopub.execute_input":"2021-06-13T16:36:20.725945Z","iopub.status.idle":"2021-06-13T16:36:23.241078Z","shell.execute_reply.started":"2021-06-13T16:36:20.725843Z","shell.execute_reply":"2021-06-13T16:36:23.239887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Q.2. Create interactive plot analysing parameters and constrains from any of the dataset.\n # I have imported the pandas library to view the csv file and plotly to visualize the interactive plot\nimport pandas as pd\nimport plotly.express as px\ndata = pd.read_csv(\"../input/covid-19-clean-completecsv/covid_19_clean_complete.csv\",\n                   parse_dates=[\"Date\"])\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:38:07.856922Z","iopub.execute_input":"2021-06-13T16:38:07.8574Z","iopub.status.idle":"2021-06-13T16:38:09.466513Z","shell.execute_reply.started":"2021-06-13T16:38:07.857361Z","shell.execute_reply":"2021-06-13T16:38:09.465327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.rename(columns={\"Date\":\"date\", \n                     \"Province/State\":\"state\", \n                     \"Country/Region\":\"country\",\n                     \"Active\":\"active\", \n                     \"Confirmed\":\"confirmed\", \n                     \"Lat\":\"lat\",\n                     \"Long\":\"long\",\n                     \"Recovered\":\"recovered\",\n                     \"Deaths\":\"deaths\"}, inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:38:50.87593Z","iopub.execute_input":"2021-06-13T16:38:50.876345Z","iopub.status.idle":"2021-06-13T16:38:50.89705Z","shell.execute_reply.started":"2021-06-13T16:38:50.876307Z","shell.execute_reply":"2021-06-13T16:38:50.895849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spread of Covid-19\ndf = data\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\ndf[\"date\"] = df[\"date\"].dt.strftime('%m/%d/%y')\ndf = df.fillna('-')\nfig = px.density_mapbox(df, lat=\"lat\", lon=\"long\", z=\"confirmed\", radius=20, zoom=1, \n                        hover_data=[\"country\", \"state\", \"confirmed\"], range_color=[0, 1000],\n                        mapbox_style='carto-positron', animation_frame=\"date\", title=\"Speard of Covid-19\")\nfig.update_layout(margin={\"r\":0, \"t\":30, \"l\":0, \"b\":0})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:39:30.232201Z","iopub.execute_input":"2021-06-13T16:39:30.232595Z","iopub.status.idle":"2021-06-13T16:39:33.028861Z","shell.execute_reply.started":"2021-06-13T16:39:30.232562Z","shell.execute_reply":"2021-06-13T16:39:33.027553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Q.3. Correlation based analysis using any of the dataset\n\ndata = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\ndata.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:40:59.281327Z","iopub.execute_input":"2021-06-13T16:40:59.281723Z","iopub.status.idle":"2021-06-13T16:40:59.308374Z","shell.execute_reply.started":"2021-06-13T16:40:59.281687Z","shell.execute_reply":"2021-06-13T16:40:59.307345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here, is our correlation details for our Heart Diseases Analysis \ncorr = data.corr()\ncorr","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:41:24.900346Z","iopub.execute_input":"2021-06-13T16:41:24.900754Z","iopub.status.idle":"2021-06-13T16:41:24.933631Z","shell.execute_reply.started":"2021-06-13T16:41:24.900718Z","shell.execute_reply":"2021-06-13T16:41:24.932289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The Best way to analysis the correlation matrix is by using Seaborn Heatmap :)\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr,\n                 annot=True,\n                 linewidths=0.5,\n                 fmt=\".2f\",\n                 cmap=\"YlGnBu\");","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:41:50.266392Z","iopub.execute_input":"2021-06-13T16:41:50.266966Z","iopub.status.idle":"2021-06-13T16:41:51.598847Z","shell.execute_reply.started":"2021-06-13T16:41:50.266933Z","shell.execute_reply":"2021-06-13T16:41:51.597456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Q.4. Distribution based analysis using any of the dataset\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nsns.set_theme()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:44:42.081531Z","iopub.execute_input":"2021-06-13T16:44:42.082178Z","iopub.status.idle":"2021-06-13T16:44:42.087707Z","shell.execute_reply.started":"2021-06-13T16:44:42.082142Z","shell.execute_reply":"2021-06-13T16:44:42.086346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/students-performance-in-exams/StudentsPerformance.csv\")\n#data.head()data = pd.read_csv(\"drive/MyDrive/ml club data/StudentsPerformance.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:46:12.416083Z","iopub.execute_input":"2021-06-13T16:46:12.416691Z","iopub.status.idle":"2021-06-13T16:46:12.437723Z","shell.execute_reply.started":"2021-06-13T16:46:12.416641Z","shell.execute_reply":"2021-06-13T16:46:12.436512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check how the math score is distributed along the data\nfig, ax = plt.subplots(1, 2, sharey=True, figsize=(15, 5))\nfig.suptitle(\"Distribution plot in Math score\")\n\nsns.kdeplot(data[\"math score\"], ax=ax[0]);\nax[0].set_title(\"Math score in Histogram Plot\")\n\nsns.rugplot(data[\"math score\"], ax=ax[1]);\nax[1].set_title(\"Math score in Rug Plot\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:46:40.453429Z","iopub.execute_input":"2021-06-13T16:46:40.454126Z","iopub.status.idle":"2021-06-13T16:46:41.015845Z","shell.execute_reply.started":"2021-06-13T16:46:40.45407Z","shell.execute_reply":"2021-06-13T16:46:41.014679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view the reading and writing score distributions by jointplot\nsns.jointplot(x=\"reading score\", y=\"writing score\", data=data, kind=\"hex\");","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:47:16.291232Z","iopub.execute_input":"2021-06-13T16:47:16.291678Z","iopub.status.idle":"2021-06-13T16:47:16.939738Z","shell.execute_reply.started":"2021-06-13T16:47:16.291641Z","shell.execute_reply":"2021-06-13T16:47:16.938837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's view all the columns distributions at the same plot\nsns.pairplot(data, hue=\"gender\");","metadata":{"execution":{"iopub.status.busy":"2021-06-13T16:47:49.956576Z","iopub.execute_input":"2021-06-13T16:47:49.95734Z","iopub.status.idle":"2021-06-13T16:47:53.425794Z","shell.execute_reply.started":"2021-06-13T16:47:49.957288Z","shell.execute_reply":"2021-06-13T16:47:53.424968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Q.2 Natural Language Processing : Generating paragraph summary or gist of a paragraph.\n# For extracting the paragraph we use web scraping to get the data.\n\"\"\"import bs4 as bs\nimport urllib.request\nimport re \nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\"\"\"\n!pip install imutils\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scrape the data from the website\nscraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n# Read the data\narticle = scraped_data.read()\n# Let's make it seems good using BeatuifulSoup\nparsed_article = bs.BeautifulSoup(article,'lxml')\n# Extract all paragraph from the data ('p') means <p> tag\nparagraphs = parsed_article.find_all('p')\narticle_text = \"\"\n\nfor p in paragraphs:\n    article_text += p.text","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:11:02.569957Z","iopub.execute_input":"2021-06-13T17:11:02.570561Z","iopub.status.idle":"2021-06-13T17:11:02.59398Z","shell.execute_reply.started":"2021-06-13T17:11:02.570504Z","shell.execute_reply":"2021-06-13T17:11:02.592364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing Square Brackets and Extra Spaces\narticle_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\narticle_text = re.sub(r'\\s+', ' ', article_text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"article_text","metadata":{"execution":{"iopub.status.busy":"2021-06-13T17:12:14.399714Z","iopub.execute_input":"2021-06-13T17:12:14.400486Z","iopub.status.idle":"2021-06-13T17:12:14.423774Z","shell.execute_reply.started":"2021-06-13T17:12:14.400434Z","shell.execute_reply":"2021-06-13T17:12:14.422172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Q.3. Natural Language Processing : Generating topic model for a paragraph.\n# using LDA\n\nimport pandas as pd\nimport numpy as np\nr_datasets=pd.read_csv(r'[-:\\india\\abc.csv')\nr_datasets=r_datasets.head(20000)\nr_datasets.drapne()\nr_datasets.head()\nr_datasets['text'][350]\n# To display help of count vectorizer\nfrom sklearn.Feature_extraction.text import countvectorizer\ncount_vect=Countvectorizer(max_df=0.8,min_dfzz,stop_wards='english')\ndoc.term_matrix=count_vert.fit_transform(review_dataset['text'].values.astype('u'))\ndoc_term_matrix\nFrom sklearn.decomposition latent DirichletAllocation\nLDA=LatentDirichletAllocation(A_components=5,random_state=42)\nLDA.fit(doc-term-matrix)\nimport random\nfor i in range (10):\n    r_id=random.randint-(o,len(count_vect.get_feature_names()))\n    print(count_veact.get_featurenames()[r_id])\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Q.5. Classification Model for the given dataset\n## for datas\nimport pandas as pd\nimport numpy as np\n## for plotting\nimport matplotlip.pyplot as plt\nimport seaborn as sns\n## for statical test\nimport scipy\nimport statsmodels.formula.api as smf\nimport statsmodels.apl.as sm\n## for machine learning\nfrom sklearn import model_selection,ensemble,linear_modal,metrics,decomposition\n##for explainer\nfrom lime import lime_tabular\nabc=pd.read_csv(\"https://github.com/phospheneai/data-science-webinar/blob/master/HR_Data.csv\");\nasc.head();\n## return \"cats\"if the column is categorial \n## or 'num ' otherwise\ndef ab_c(abc,col,max_cat=20):\n    if(abc[col],dtype==\"0\"/(abc[col]nunique()<max_cat)):\n        return\"cat\";\n    else:\n        return\"num\";\n    _ab_cols={col:ab-c(abc,col,max_cat=20)for col in abc.columns}\n    heatmap=abc.isnull()\n    for k,v in ab_cols.items():\n        if v==\"num\":\n            heatmap[k]=heatmap[k].apply(/embeta x:0.5 if x is false else 1)\n        else:\n            heatmap[k]=heatmap[k].apply(/ambeta x:0 if x is false else 1)\n        sns.heatmap(heatmap,cbar=false ).set_title('datset overview')\n        plt.show()\n        print(\"\\033[1;37;40m categercial\",\n             \"\\033[1;30;41m numeric\",\n             \"\\033[1;30;47m NaN\"):","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Q.4. Regression Model for the given dataset\n#supress warning\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\naa=pd.read_csv(\"https://github.com/phospheneai/data-science-webinar/blob/master/economic_data.csv\")\naa.shape\n#info our dataset\naa.info()\n#describe our dataset\naa.describe\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.pairplot(aa,x_vars=['Countrynames_x','countrycode','birthdata','internetusers','incomegroups','religion'],y_vars='ferhlityrate',size=7,aspects=1,kind='scatter')\nplt.show()\n#visualizing the data using heatmap\nsns.heatmap(aa.corr(),cmap=\"YINGNBU\",annot=tree)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}