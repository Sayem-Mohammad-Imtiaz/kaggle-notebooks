{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pylab\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport seaborn as sns\nimport math \n%matplotlib inline\n\n\n# Input data files are available in the \".ut./inp/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/census-data-set/Census Income Dataset.csv',na_values = [' ?'])\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = pd.read_csv('../input/census-test-dataset/Census Income Testset.csv', na_values =[' ?'])\ntest_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Apply feature discretization to Income_bracket column\ndef Income_bracket_binarization(feat_val):\n    if feat_val == '<=50K':\n        return 0\n    else:\n        return 1\n    \ndataset['Income_bracket'] = dataset['Income_bracket'].apply(Income_bracket_binarization)\ntest_dataset['Income_bracket'] = test_dataset['Income_bracket'].apply(Income_bracket_binarization)\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CategoricalImputer():\n  \n    def __init__(self, columns = None, strategy='most_frequent'):\n        self.columns = columns\n        self.strategy = strategy\n    \n    \n    def fit(self,X, y=None):\n        if self.columns is None:\n            self.columns = X.columns\n            print(self.columns)\n    \n        if self.strategy is 'most_frequent':\n            self.fill = {column: X[column].value_counts().index[0] for \n        column in self.columns}\n            \n        else:\n              self.fill ={column: '0' for column in self.columns}\n\n        return self\n    \n    def transform(self,X):\n        for column in self.columns:\n            X[column] = X[column].fillna(self.fill[column])\n        return X\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"obj = CategoricalImputer(columns=\n          ['workclass','Occupation', 'Native_Country'])\ntrain_result = obj.fit(dataset[['workclass','Occupation', 'Native_Country']])\n\ndataset[['workclass','Occupation', 'Native_Country']] = train_result.transform(dataset[['workclass','Occupation', 'Native_Country']])\n\ntest_obj = CategoricalImputer(columns=\n          ['workclass','Occupation', 'Native_Country'])\ntest_result = test_obj.fit(test_dataset[['workclass','Occupation', 'Native_Country']])\n\ntest_dataset[['workclass','Occupation', 'Native_Country']] = test_result.transform(test_dataset[['workclass','Occupation', 'Native_Country']])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Plot():\n    #Find Indices where Income is >50K and <=50K\n    fig = plt.figure(figsize=(15,15))\n    fig.subplots_adjust(hspace=0.7, wspace=0.7)\n    pylab.suptitle(\"Analyzing the dataset\", fontsize=\"xx-large\")\n    plt.subplot(3,2,1)\n    ax = sns.countplot(x='Age', hue='Income_bracket', data=dataset)\n    plt.subplot(3,2,2)\n    ax =sns.countplot(x='workclass', hue='Income_bracket', data=dataset)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n    plt.subplot(3,2,3)\n    ax =sns.countplot(x='Education', hue='Income_bracket', data=dataset)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n    plt.subplot(3,2,4)\n    ax = sns.countplot(x='Occupation', hue='Income_bracket', data=dataset)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n    plt.subplot(3,2,5)\n    ax = sns.countplot(x='Gender', hue='Income_bracket', data=dataset)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n    plt.subplot(3,2,6)\n    ax = sns.countplot(x='hours_per_week', hue='Income_bracket', data=dataset)\n    \n    return None\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analyzing distribution for the dataset\ndataset.hist(\n    column=[\"Age\",\"Education\", \"hours_per_week\"],figsize=(6, 5))\n\npylab.suptitle(\"Analyzing distribution for the dataset\", fontsize=\"xx-large\")\n\nPlot()\n\nX = dataset.drop('Income_bracket',axis =1)\ny = dataset['Income_bracket']\n\n#Split data set into training set and test set\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Categorical_Encoder(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns  = columns\n        self.encoders = None\n    def fit(self, data, target=None):\n        \"\"\"\n        Expects a data frame with named columns to encode.\n        \"\"\"\n        # Encode all columns if columns is None\n        if self.columns is None:\n            self.columns = data.columns\n        # Fit a label encoder for each column in the data frame\n        self.encoders = {\n            column: LabelEncoder().fit(data[column])\n            for column in self.columns\n        }\n        return self\n    def transform(self, data):\n        \"\"\"\n        Uses the encoders to transform a data frame.\n        \"\"\"\n        output = data.copy()\n        for column, encoder in self.encoders.items():\n            output[column] = encoder.transform(data[column])\n        return output\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = {\n        column: list(dataset[column].unique())\n        for column in dataset.columns\n        if dataset[column].dtype == 'object'\n    }\nencoder = Categorical_Encoder(categorical_features.keys())\ndataset = encoder.fit_transform(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dataset.values\nX_train = np.float32(data[:,[0,1,2,3,5,6,7,8,9,10,11,12]])\nY_train = data[:,[13]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = {\n        column: list(test_dataset[column].unique())\n        for column in test_dataset.columns\n        if test_dataset[column].dtype == 'object'\n    }\nencoder = Categorical_Encoder(categorical_features.keys())\ntest_dataset = encoder.fit_transform(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeCost(A, y):\n\n    \"\"\"\n    Computes the cost using the sigmoid cross entropy\n    \n    Arguments:\n    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n    labels -- vector of labels y (1 or 0) \n    \n    Note: What we've been calling \"z\" and \"y\" in this class are respectively called \"logits\" and \"labels\" \n    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n    \n    Returns:\n    cost -- runs the session of the cost (formula (2))\n    \"\"\"\n    \n    A= tf.cast(A,tf.float32)\n    y = tf.cast(y,tf.float32)\n    \n    cross_entropy_cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = A, labels = y)\n    \n    return  cross_entropy_cost\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Xavier_Intializer(dim):\n    \n    tf.set_random_seed(1)\n    w = tf.get_variable(\"w\", [dim, 1], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b = 0\n\n    assert(w.shape == (dim,1))\n    assert(isinstance(b, float) or isinstance(b, int))\n        \n    return w,b\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def propagate(X,y,w,b):\n    \n    #Implements the forward propagation for the model: LINEAR -> SIGMOID\n    # Retrieve the parameters from the dictionary \"parameters\" \n    Z = tf.add(tf.matmul(X,w), b)                      # Z = np.dot(W, X) + b\n    A =  tf.sigmoid(Z)                             # A1 = sigmoid(Z1)\n    \n    return A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef random_mini_batches(X, Y, mini_batch_size, seed = 0):\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X_train, Y_train,minibatch_size,learning_rate = 0.005,num_iterations = 1500,num_epochs = 1500, print_cost = True):\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    (m, n_x) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    print(m)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    seed = 3\n    \n    # Initialize parameters\n    w,b = Xavier_Intializer(n_x)\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    A = propagate(X_train, Y_train,w,b)\n    # Cost function: Add cost function to tensorflow graph\n    cost = computeCost(A,Y_train)    \n    \n    # Backpropagation: Define the tensorflow optimizer. Use a Gradient Descent\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train.T, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                ### END CODE HERE ###\n                \n                epoch_cost += minibatch_cost / num_minibatches\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n        \n        # lets save the parameters in a variable\n        w = sess.run(w)\n        b = sess.run(b)\n        print(\"Parameters have been trained!\")\n        \n        return w,b\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w,b = model(X_train, Y_train,minibatch_size = 256)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}