{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport pickle\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support as prfs\n\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, load_model, save_model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Dense, Flatten\n\n%matplotlib inline\nprint(\"[Done] Libraries imported...\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to load data from path\ndef load_data(data_path):\n    X = []\n    y = []\n    labels = os.listdir(data_path)\n    img_path_per_label = {labels[i]: [os.path.join(data_path, labels[i], img_path) for img_path in os.listdir(data_path + '/' + labels[i])] for i in range(len(labels))}\n    for key in list(img_path_per_label.keys()):\n        for img_path in img_path_per_label[key]:\n            X.append(cv2.resize(cv2.imread(img_path), (30, 30), interpolation=cv2.INTER_BITS2))\n            y.append(key)\n\n    return np.array(X), np.array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to increase brightness of images\ndef increase_brightness(img, value=20):\n    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h, s, v = cv2.split(hsv_img)\n\n    limit = 255 - value\n    v[v <= limit] += value\n    v[v > limit] = 255\n\n    final_hsv = cv2.merge((h, s, v))\n    return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to display random set of images from dataset\ndef display_random_set(data, labels):\n    for i in range(10):\n        random_val = np.random.randint(low=0, high=len(data))\n        plt.subplot(2, 5, (i + 1))\n        plt.imshow(data[random_val])\n        plt.title(labels[random_val])\n        plt.axis(False)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convolutional Neural Network model\ndef build_model(num_classes, img_dim):\n    model = Sequential()\n\n    model.add(Conv2D(filters=64, kernel_size=(2, 2), padding='same', activation='relu', input_shape=img_dim))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(filters=64, kernel_size=(2, 2), padding='same', activation='relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(filters=128, kernel_size=(2, 2), padding='same', activation='relu'))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(filters=128, kernel_size=(2, 2), padding='same', activation='relu'))\n    model.add(BatchNormalization())\n\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.1))\n\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    sgd = SGD(learning_rate=0.001, nesterov=True, name='SGD_Optimizer')\n    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['categorical_accuracy', 'mse'])\n\n    print(model.summary())\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Model\ndef train_model(x, y, x_val, y_val, model, train=False):\n    batch_size = 64\n    num_epochs = 25\n    if train:\n        checkpoint = ModelCheckpoint(filepath='traffic_sign_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n        history = model.fit(x=x, y=y, validation_data=(x_val, y_val), shuffle=True, batch_size=batch_size, epochs=num_epochs, callbacks=[checkpoint], verbose=1)\n        save_history_file(file_name='traffic_sign.pickle', history=history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_history_file(file_name, history):\n    pickle_out = open(file_name, 'wb')\n    pickle.dump(history.history, pickle_out)\n    pickle_out.close()\n\n\ndef load_history(file_name):\n    pickle_in = open(file_name, 'rb')\n    saved_hist = pickle.load(pickle_in)\n    return saved_hist","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions to plot loss curves and accuracy per class\ndef plot_curves(history):\n\n    plt.figure(figsize=(10, 5))\n    sns.set_style(style='dark')\n    plt.subplot(1, 2, 1)\n    plt.plot(history['loss'])\n    plt.plot(history['val_loss'])\n    plt.xlabel('Iterations')\n    plt.ylabel('Error')\n    plt.title('Training & Validation Loss')\n    plt.legend(['Train loss', 'Validation loss'])\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history['mse'])\n    plt.plot(history['val_mse'])\n    plt.xlabel('Iterations')\n    plt.ylabel('Error')\n    plt.title('Training & Validation MSE')\n    plt.legend(['Train mse', 'Validation mse'])\n\n    plt.show()\n\n\ndef accuracy_per_class(labels, precision, recall, f1):\n    # plt.subplots(figsize=(18, 30))\n\n    x = range(len(labels))\n    plt.subplot(3, 1, 1)\n    plt.title(\"Precision per class\")\n    plt.ylim(0, 1.00)\n    plt.bar(x, precision, color='Red')\n    plt.xticks(x, rotation=90)\n\n    plt.subplot(312)\n    plt.title('Recall per class')\n    plt.ylim(0, 1.00)\n    plt.bar(x, recall, color='Green')\n    plt.xticks(x, rotation=90)\n\n    plt.subplot(313)\n    plt.title('F1 score per class')\n    plt.ylim(0, 1.00)\n    plt.bar(x, f1, color='Blue')\n    plt.xticks(x, rotation=90)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading Test data\ndef load_test_data(test_data_dir, test_data_labels_dir):\n    # reading csv file\n    data = np.loadtxt(test_data_labels_dir, delimiter=',', skiprows=1, dtype=str)\n    x_test = np.array([os.path.join(test_data_dir, img_name) for img_name in data[:, 0]])\n    x_test = np.array([cv2.resize(cv2.imread(img_path), (30, 30), interpolation=cv2.INTER_BITS2) for img_path in x_test])\n    y_test = np.array(data[:, 1]).astype(np.int)\n\n    return x_test, y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    # Reading Data from folders\n    X, y = load_data(data_path='../input/traffic-sign-cropped/crop_dataset/crop_dataset/')\n    print(f\"Data shape: {X.shape},   Labels: {y.shape}\\n\")\n\n    # Displaying random set of images from data\n    display_random_set(data=X, labels=y)\n\n    # Splitting data into training and testing data, training will consist of 70% of the data and 30% of the remaining\n    # will be testing data.\n    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n    print(f\"Training Data: {x_train.shape},   Training labels: {y_train.shape}\\nValidation Data: {x_val.shape},   \"\n          f\"Validation labels: {y_val.shape}\\n\")\n\n    # Adjusting labels to be represented as categorical data.\n    y_train = to_categorical(y=y_train, num_classes=len(np.unique(y)))\n    y_val = to_categorical(y=y_val, num_classes=len(np.unique(y)))\n\n    # Creating Neural network model.\n    model = build_model(num_classes=len(np.unique(y)), img_dim=x_train[0].shape)\n\n    # To train the model again change train value to True, change to False to not train.\n    train_model(x=x_train, y=y_train, x_val=x_val, y_val=y_val, model=model, train=True)\n\n    print(\"[In progress] Loading H5 model and history file...\")\n    classifier = load_model(filepath='traffic_sign_model.h5')\n    hist_loaded = load_history(file_name='traffic_sign.pickle')\n    print(\"[Done] Loading H5 model and history file...\")\n\n    # Loading data for testing model.\n    x_test, y_test = load_test_data(test_data_dir='../input/traffic-sign-cropped/test_data/test_data', test_data_labels_dir='../input/traffic-sign-cropped/test_labels.csv')\n    predictions = classifier.predict_classes(x_test)\n    accuracy = np.array([1 if predictions[i] == int(y_test[i]) else 0 for i in range(len(predictions))])\n    print(f\"Accuracy on test data: {np.mean(accuracy) * 100} %.\")\n\n    # plotting loss and mse curves for training and validation steps\n    plot_curves(hist_loaded)\n\n    # plotting accuracy bar graph per class\n    labels = np.unique(y)\n    precision, recall, f1, support = prfs(y_true=y_test, y_pred=predictions, average=None)\n    accuracy_per_class(labels, precision, recall, f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}