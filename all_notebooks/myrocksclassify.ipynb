{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 数据预处理","metadata":{}},{"cell_type":"code","source":"# \"\"\"\n# # !/usr/bin/env python\n# # # -*- coding: utf-8 -*-\n# @@@ NAME: DataProcesser.PY\n# @@@ AUTHER: YuCong Wang\n# ----------------------document---------------------\n# ****** kaggle notebook only ************\n# ****** 源数据图片大小： 3000 * 4096 *******\n# ****** 分割尺寸 ************************** \n# ****** x = 3000 / num1 ******************\n# ****** y = 4096 / num2 ******************\n# \"\"\"\n# import random\n# import shutil\n# import pandas as pd\n# import cv2\n# import os \n\n# random.seed(1)\n# def train_test_split(train_ratio=.6, test_ratio=.2, valdation_ratio=.2): # 输入训练集，测试集，验证集占比, 自动将数据分到不同的文件夹\n#     # 声明全局变量\n#     global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n#     if 10 * train_ratio + 10 * test_ratio + 10 * valdation_ratio != 10: # 转化为整数再进行数值相等判断操作。\n#         print(train_ratio + test_ratio + valdation_ratio, \"比例不满足\")\n#     else:\n#         # 获得每个类别的列表\n#         data = pd.read_csv(label_path, encoding=\"gbk\")\n#         data_g = data.groupby('样本类别')['样本编号']\n#         data_g = list(data_g)\n#         for j in range(len(data_g)):\n#             data_gl = data_g[j][1].to_list()\n#             # 打乱列表\n#             random.shuffle(data_gl)\n#             # 按比例划分\n#             train_len = round(train_ratio * len(data_gl))\n#             test_len = round(test_ratio * len(data_gl))\n#             valdation_len = round(valdation_ratio * len(data_gl))\n            \n#             train_lis = data_gl[:train_len]\n#             valdation_lis = data_gl[train_len : train_len + valdation_len]\n#             test_lis = data_gl[train_len + valdation_len : ]\n#             # 将样本分类\n#             file_lis = os.listdir(to_path)\n#             print(\"groupby data done\")\n#             i = 0\n#             while i < len(file_lis):\n#                 img = file_lis[i]\n#                 img_name = img.split(\"-\")[1]\n                \n#                 img_name = eval(img_name)\n#                 if img_name in train_lis:\n#                     # 复制文件\n#                     for k in range(2,splited_num + 1):\n#                         gname = img.split(\".\")[0]\n#                         gtype = img.split(\".\")[1]\n#                         gname = gname.split(\"-\")\n#                         gname[-1] = str(k)\n#                         gname = \"-\".join(gname)\n#                         img = gname + \".\" + gtype\n#                         shutil.copyfile(to_path + img, train_path + img)\n#                 elif img_name in valdation_lis:\n#                     for k in range(2,splited_num + 1):\n#                         gname = img.split(\".\")[0]\n#                         gtype = img.split(\".\")[1]\n#                         gname = gname.split(\"-\")\n#                         gname[-1] = str(k)\n#                         gname = \"-\".join(gname)\n#                         img = gname + \".\" + gtype\n#                         shutil.copyfile(to_path + img, val_path + img)\n#                 elif img_name in test_lis:\n#                     for k in range(2,splited_num + 1):\n#                         gname = img.split(\".\")[0]\n#                         gtype = img.split(\".\")[1]\n#                         gname = gname.split(\"-\")\n#                         gname[-1] = str(k)\n#                         gname = \"-\".join(gname)\n#                         img = gname + \".\" + gtype\n#                         shutil.copyfile(to_path + img, test_path + img)\n#                 i += splited_num\n#             print((train_lis,valdation_lis,test_lis))\n\n# def classify(name):\n#     # 声明全局变量\n#     global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n#     data = pd.read_csv(label_path, encoding=\"gbk\")\n#     data_g = data.groupby('样本类别')['样本编号']\n#     data_g = list(data_g)\n#     name = eval(name.split('-')[0]) # 133\n#     dic = {}\n#     for j in range(len(data_g)): # 7\n#         dic[str(j)] = data_g[j][1].to_list()\n#     lis = []\n#     for k,v in dic.items():\n#         if name in v:\n#             lis.append(k)\n#     if len(lis) == 1:\n#         return lis[0]\n#     elif len(lis) >= 1:\n#         return \"-\".join(lis)\n#     else:\n#         return \"unknow\"\n\n# def split_save_imgs(file_path, name, classes):\n#      # 声明全局变量\n#     global to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n# #     path = \"E:\\\\kaggledatabase\\\\2021taidi\\\\rock\\\\rocks\\\\133-1.bmp\"\n#     img = cv2.imread(file_path) \n#     # spilt\n#     count = 0\n#     for i in range(int(3000 / num1)):\n#         block1 = img[i * num1 : (i + 1) * num1, :]\n#         for j in range(int(4096 / num2)):\n#             count += 1\n#             block2 = block1[:, j * num2 : (j + 1) * num2]\n#     #         print(block2.shape)\n#             # 缩放到224 224\n#             block2 = cv2.resize(block2,(to_shape1,to_shape2))\n#             cv2.imwrite(to_path + \"{}-{}-{}.png\".format(classes, name, count),block2,[int(cv2.IMWRITE_PNG_COMPRESSION), 0]) # 不压缩保存\n\n# # 0-133-1-1 ，第0类的113-1号彩色照片的第一个分割\n\n# def do_spilt(): # 源文件的目录\n#     # 声明全局变量\n#     global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n#     for i in os.listdir(from_path):\n#         name = i.split(\".\")[0]\n#         if name.split(\"-\")[1] == \"1\":    # 只分割彩色图片\n#             classes = classify(name)\n#             split_save_imgs(from_path + i, name, classes)\n\n# def check_paths():\n#     # 声明全局变量\n#     global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n#     # 检查输入目录\n#     if os.path.exists(from_path) is not True:\n#         print(\"源文件目录不存在\")\n#         return False\n#     if os.path.exists(to_path) is not True:\n#         os.makedirs(to_path)\n#     if os.path.exists(label_path) is not True:\n#         print(\"label文件不存在\")\n#         return False\n#      # 创建文件夹\n#     if os.path.exists(train_path)is not True:\n#         os.makedirs(\"./new/train_data/\")\n#     if os.path.exists(test_path)is not True:\n#         os.makedirs(\"./new/test_data/\")\n#     if os.path.exists(val_path)is not True:\n#         os.makedirs(\"./new/val_data/\")\n# def check_split_num(n1, n2):\n# #     lis = []\n#     for i in range(1, 4096 + 1):\n#         for j in range(1, 3000 + 1):\n#             if 4096 % i == 0 and 3000 % j == 0 and j == n1 and i == n2:\n#                 return (n1,n2)\n#     print(\"无效的整数分割，请参考以下数对\\n\")\n#     for i in range(1, 4096 + 1):\n#         for j in range(1, 3000 + 1):\n#             if 4096 % i == 0 and 3000 % j == 0:\n#                 print((j, i))\n\n                \n# # 创建全局变量                \n# def modifyGlobal():\n#     # 声明全局变量\n#     global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n#     ### 定义路径\n#     from_path = r\"../input/my-rocks/only-colors/only-color/\" # 源数据\n#     to_path = r\"./new_sets/\" # 分割好的图片的存放位置\n#     label_path = r\"../input/my-rocks/rock_label_1.csv\" # 标签文件路径\n#     train_path = r\"./new/train_data/\" # 新的训练数据集\n#     test_path = r\"./new/test_data/\"# 新的测试数据集\n#     val_path = r\"./new/val_data/\"# 新的验证数据集\n    \n# def main(n1, n2, shape1 = 224, shape2 = 224):\n#     # 声明全局变量\n#     global num1, num2, splited_num, to_shape1, to_shape2\n#     # 先检查路径是否正确\n#     check_paths()\n#     print(\"check paths done\")\n#     # 检查分割大小是否正确\n#     num1, num2 = check_split_num(n1, n2) # 一张图片分成四份。\n#     print(\"check split done\")\n#     # 定义压缩形状大小, 默认224 224\n#     to_shape1, to_shape2 = shape1, shape2\n#     # 计算分割后图片份数：\n#     splited_num = int((3000 / num1) * (4096 / num2))\n#     # 先分割，后整理\n#     do_spilt()\n#     print(\"do split done\")\n#     # 整理成训练集与测试集 ， 思想是每个类别都按比例分割， 建议比例6 2 2 否则可能出现 test_data 某个类别无数据的情况\n#     train_test_split()\n#     print(\"train test split done\")\n        \n# if __name__ == \"__main__\":\n#     ### 引入全局变量\n#     modifyGlobal()\n# #     print(from_path)\n#     ### 主函数 单个图片的宽， 高， 压缩后宽， 压缩后高\n#     main(1500, 2048, 224, 224)\n    ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# os.listdir(\"./\")[:10]","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ### 删除文件以及文件夹\n# import shutil\n# try:\n#     shutil.rmtree(\"./\")\n# except:\n#     print(\"done\")\n#     pass","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 针对demov1数据集","metadata":{}},{"cell_type":"markdown","source":"# 删除数据","metadata":{}},{"cell_type":"code","source":"import shutil\ntry:\n    shutil.rmtree(\"./\")\nexcept:\n    print(\"done\")\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# !/usr/bin/env python\n# # -*- coding: utf-8 -*-\n@@@ NAME: DataProcesser.PY\n@@@ AUTHER: YuCong Wang\n----------------------document---------------------\n****** kaggle notebook only ************\n****** 源数据图片大小： 3000 * 4096 *******\n****** 分割尺寸 ************************** \n****** x = 3000 / num1 ******************\n****** y = 4096 / num2 ******************\n\"\"\"\nimport random\nimport shutil\nimport pandas as pd\nimport cv2\nimport os \n\nrandom.seed(1)\ndef train_test_split(train_ratio=.6, test_ratio=.2, valdation_ratio=.2): # 输入训练集，测试集，验证集占比, 自动将数据分到不同的文件夹\n    # 声明全局变量\n    global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n    if 10 * train_ratio + 10 * test_ratio + 10 * valdation_ratio != 10: # 转化为整数再进行数值相等判断操作。\n        print(train_ratio + test_ratio + valdation_ratio, \"比例不满足\")\n    else:\n        # 获得每个类别的列表\n        data = pd.read_csv(label_path, encoding=\"gbk\")\n        data_g = data.groupby('样本类别')['样本编号']\n        data_g = list(data_g)\n        for j in range(len(data_g)):\n            data_gl = data_g[j][1].to_list()\n            # 打乱列表\n            random.shuffle(data_gl)\n            # 按比例划分\n            train_len = round(train_ratio * len(data_gl))\n            test_len = round(test_ratio * len(data_gl))\n            valdation_len = round(valdation_ratio * len(data_gl))\n            \n            train_lis = data_gl[:train_len]\n            valdation_lis = data_gl[train_len : train_len + valdation_len]\n            test_lis = data_gl[train_len + valdation_len : ]\n            # 将样本分类\n            file_lis = os.listdir(to_path)\n            print(\"groupby data done\")\n            i = 0\n            while i < len(file_lis):\n                img = file_lis[i]\n                img_name = img.split(\"-\")[1]\n                \n                img_name = eval(img_name)\n                if img_name in train_lis:\n                    # 复制文件\n                    for k in range(2,splited_num + 1):\n                        gname = img.split(\".\")[0]\n                        gtype = img.split(\".\")[1]\n                        gname = gname.split(\"-\")\n                        gname[-1] = str(k)\n                        gname = \"-\".join(gname)\n                        img = gname + \".\" + gtype\n                        shutil.copyfile(to_path + img, train_path + img)\n                elif img_name in valdation_lis:\n                    for k in range(2,splited_num + 1):\n                        gname = img.split(\".\")[0]\n                        gtype = img.split(\".\")[1]\n                        gname = gname.split(\"-\")\n                        gname[-1] = str(k)\n                        gname = \"-\".join(gname)\n                        img = gname + \".\" + gtype\n                        shutil.copyfile(to_path + img, val_path + img)\n                elif img_name in test_lis:\n                    for k in range(2,splited_num + 1):\n                        gname = img.split(\".\")[0]\n                        gtype = img.split(\".\")[1]\n                        gname = gname.split(\"-\")\n                        gname[-1] = str(k)\n                        gname = \"-\".join(gname)\n                        img = gname + \".\" + gtype\n                        shutil.copyfile(to_path + img, test_path + img)\n                i += splited_num\n            print((train_lis,valdation_lis,test_lis))\n\ndef classify(name):\n    # 声明全局变量\n    global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n    data = pd.read_csv(label_path, encoding=\"gbk\")\n    data_g = data.groupby('样本类别')['样本编号']\n    data_g = list(data_g)\n    name = eval(name.split('-')[0]) # 133\n    dic = {}\n    for j in range(len(data_g)): # 7\n        dic[str(j)] = data_g[j][1].to_list()\n    lis = []\n    for k,v in dic.items():\n        if name in v:\n            lis.append(k)\n    if len(lis) == 1:\n        return lis[0]\n    elif len(lis) >= 1:\n        return \"-\".join(lis)\n    else:\n        return \"unknow\"\n\ndef split_save_imgs(file_path, name, classes):\n     # 声明全局变量\n    global to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n#     path = \"E:\\\\kaggledatabase\\\\2021taidi\\\\rock\\\\rocks\\\\133-1.bmp\"\n    img = cv2.imread(file_path) \n    # spilt\n    count = 0\n    for i in range(int(3000 / num1)):\n        block1 = img[i * num1 : (i + 1) * num1, :]\n        for j in range(int(4096 / num2)):\n            count += 1\n            block2 = block1[:, j * num2 : (j + 1) * num2]\n    #         print(block2.shape)\n            # 缩放到224 224\n            block2 = cv2.resize(block2,(to_shape1,to_shape2))\n            cv2.imwrite(to_path + \"{}-{}-{}.png\".format(classes, name, count),block2,[int(cv2.IMWRITE_PNG_COMPRESSION), 0]) # 不压缩保存\n\n# 0-133-1-1 ，第0类的113-1号彩色照片的第一个分割\n\ndef do_spilt(): # 源文件的目录\n    # 声明全局变量\n    global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n    for i in os.listdir(from_path):\n        name, suffix= i.split(\".\")[0],i.split(\".\")[1]\n        if name.split(\"-\")[1] == \"1\" and suffix == \"bmp\":    # 只分割彩色图片和bmp\n            classes = classify(name)\n            split_save_imgs(from_path + i, name, classes)\n\ndef check_paths():\n    # 声明全局变量\n    global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n    # 检查输入目录\n    if os.path.exists(from_path) is not True:\n        print(\"源文件目录不存在\")\n        return False\n    if os.path.exists(to_path) is not True:\n        os.makedirs(to_path)\n    if os.path.exists(label_path) is not True:\n        print(\"label文件不存在\")\n        return False\n     # 创建文件夹\n    if os.path.exists(train_path)is not True:\n        os.makedirs(\"./new/train_data/\")\n    if os.path.exists(test_path)is not True:\n        os.makedirs(\"./new/test_data/\")\n    if os.path.exists(val_path)is not True:\n        os.makedirs(\"./new/val_data/\")\ndef check_split_num(n1, n2):\n#     lis = []\n    for i in range(1, 4096 + 1):\n        for j in range(1, 3000 + 1):\n            if 4096 % i == 0 and 3000 % j == 0 and j == n1 and i == n2:\n                return (n1,n2)\n    print(\"无效的整数分割，请参考以下数对\\n\")\n    for i in range(1, 4096 + 1):\n        for j in range(1, 3000 + 1):\n            if 4096 % i == 0 and 3000 % j == 0:\n                print((j, i))\n\n                \n# 创建全局变量                \ndef modifyGlobal(path1):\n    # 声明全局变量\n    global from_path, to_path, label_path, train_path, test_path, val_path, num1, num2, splited_num\n    ### 定义路径\n#     from_path = r\"../input/demov1/Rock01-btach128/\" # 源数据\n    from_path = path1\n    to_path = r\"./new_sets/\" # 分割好的图片的存放位置\n    label_path = r\"../input/demov1/rock_label.csv\" # 标签文件路径\n    train_path = r\"./new/train_data/\" # 新的训练数据集\n    test_path = r\"./new/test_data/\"# 新的测试数据集\n    val_path = r\"./new/val_data/\"# 新的验证数据集\n    \ndef main(n1, n2, shape1 = 224, shape2 = 224):\n    # 声明全局变量\n    global num1, num2, splited_num, to_shape1, to_shape2\n    # 先检查路径是否正确\n    check_paths()\n    print(\"check paths done\")\n    # 检查分割大小是否正确\n    num1, num2 = check_split_num(n1, n2) # 一张图片分成四份。\n    print(\"check split done\")\n    # 定义压缩形状大小, 默认224 224\n    to_shape1, to_shape2 = shape1, shape2\n    # 计算分割后图片份数：\n    splited_num = int((3000 / num1) * (4096 / num2))\n    # 先分割，后整理\n    do_spilt()\n    print(\"do split done\")\n    # 整理成训练集与测试集 ， 思想是每个类别都按比例分割， 建议比例6 2 2 否则可能出现 test_data 某个类别无数据的情况\n    train_test_split()\n    print(\"train test split done\")\n        \nif __name__ == \"__main__\":\n    pass\n    ### 引入全局变量\n    # 处理全部文件夹\n    for i in os.listdir(\"../input/demov1/\"):\n        if os.path.isdir(\"../input/demov1/\" + i):\n            print(i)\n            modifyGlobal(\"../input/demov1/\" + i + \"/\")\n            main(750, 1024, 224, 224)### 主函数 单个图片的宽， 高， 压缩后宽， 压缩后高\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 离线数据增强","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n### 自由组合数据增强手段 ### \ndef add_enhance_train(method, data):\n        # 图片编码\n        data = tf.image.encode_png(data)\n        # 获取名称\n        name = path.split(\"/\")[-1]\n        # 保存\n        with tf.io.gfile.GFile(train_path + name +\"-\" + method, 'wb') as file:\n            file.write(data.numpy())\n            \n\n# data_path = load_data(\"train\")[0]\nfor path in os.listdir(train_path):\n    # 获取图片编号\n    # 根据路径读取图片\n    data = tf.io.read_file(train_path + path)\n    # 图片解码。忽略透明层\n    data = tf.image.decode_png(data, channels=3)\n    # 增强手段1 # \n    add_enhance_train(\"adjust\", tf.image.adjust_saturation(data,3))\n    # 增强手段2 # \n    add_enhance_train(\"fup\", tf.image.flip_up_down(data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"./new/train_data/\")[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def a_new_decorator(a_func):\n \n    def wrapTheFunction():\n        print(\"I am doing some boring work before executing a_func()\")\n \n        a_func()\n \n        print(\"I am doing some boring work after executing a_func()\")\n \n    return wrapTheFunction\n \ndef a_function_requiring_decoration():\n    print(\"I am the function which needs some decoration to remove my foul smell\")\na_function_requiring_decoration()\n\n \na_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)\n\na_function_requiring_decoration()\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 生成数据集v1","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.data.experimental import AUTOTUNE\n# 返回tensor\ndef load_data(model):\n    file_path_list = []\n    label_list = []\n    if model == \"train\":\n        for name in os.listdir(train_path):\n            file_path_list.append(train_path + name)\n            label_list.append(eval(name.split(\"-\")[0]))\n    if model == \"test\":\n        for name in os.listdir(test_path):\n            file_path_list.append(test_path + name)\n            label_list.append(eval(name.split(\"-\")[0]))\n    if model == \"val\":\n        for name in os.listdir(val_path):\n            file_path_list.append(val_path + name)\n            label_list.append(eval(name.split(\"-\")[0]))\n    return (file_path_list, label_list)\n\ndef normalize(x):\n    x_mean, x_std = tf.nn.moments(x, axes=[0,1])\n    x = (x - x_mean) / x_std\n    print(x)\n    return x\n\ndef denormalize(x):\n    x_mean, x_std = tf.nn.moments(x, axes=[0,1])\n    x = x * x_std + x_mean\n    return x\n\ndef preprocess(x, y):\n    # 根据路径读取图片\n    x = tf.io.read_file(x)\n    # 图片解码。忽略透明层\n    x = tf.image.decode_png(x, channels=3)\n    # 转换成张量，并归一化, 浮点数除以浮点数\n    x = tf.cast(x, dtype = tf.float32) / 255.\n    # 标准化\n    x = normalize(x)\n    # 标签转化为张量\n    y = tf.convert_to_tensor(y)\n    # 独热编码\n    y = tf.one_hot(y, depth=7)\n    return x,y\n    \n\nbatchsz = 64\n# 创建训练集dataset对象,使用了该函数之后， fit的时候是不支持 validation_split 这个参数提供的功能的\nimages, labels = load_data(model = \"train\")\ntrain_data = tf.data.Dataset.from_tensor_slices((images, labels))\n# \n# 打乱并且生成batch\ntrain_data = train_data.shuffle(1000).map(preprocess, num_parallel_calls=AUTOTUNE).batch(batchsz)\n\n# 将两个数据集合并\n# train_data = tf.data.Dataset.zip((train_data_origain, train_data_enhanced))\n\n# 创建验证集Dataset对象\nimages1, labels1 = load_data(model = \"val\")\nval_data = tf.data.Dataset.from_tensor_slices((images1, labels1))\n# 生成batch\nval_data = val_data.map(preprocess).batch(batchsz)\n\n# 创建测试集Dataset对象\nimages2, labels2 = load_data(model = \"test\")\ntest_data = tf.data.Dataset.from_tensor_slices((images2, labels2))\n# 生成batch\ntest_data = test_data.map(preprocess).batch(batchsz)\n\n# 实例化迭代器\ntrdata_iter = iter(train_data)\ntedata_iter = iter(test_data)\nvaldata_iter = iter(val_data)\nprint(\"tr batch: \", next(trdata_iter)[0].shape, next(trdata_iter)[1].shape)\nprint(\"te batch: \", next(tedata_iter)[0].shape, next(tedata_iter)[1].shape)\nprint(\"val batch: \", next(valdata_iter)[0].shape, next(valdata_iter)[1].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#interleave:将每个数据进行处理，利用interleave将处理的结果合并形成新的数据集\n#case:文件dataset -> 具体数据集\ndataset2 = val_data.interleave(\n    #map_fn:对数据进行怎样的变换\n    lambda v: tf.data.Dataset.from_tensor_slices(v),\n    #cycle_lenth:并行程度，即并行的去同时处理dataset中的多少个元素\n    cycle_length=5,\n    #block_length:从上面变换的结果中，每次取多少个结果出来\n    block_length=5\n)\nfor item in dataset2:\n    print(item)","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用desnet做测试","metadata":{}},{"cell_type":"code","source":"#基本包导入\nimport numpy as np\nimport time\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.optimizers as optimizers\nimport tensorflow.keras.losses as losses\n#实时数据增强功能 \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# 构建densenet\nnet = tf.keras.applications.DenseNet121(include_top=False, pooling=\"max\")\nnet.trainable = True\nnewnet = tf.keras.Sequential([\n#     data_augmentation,\n    net,\n    layers.Dense(1024, activation=\"relu\"),\n    layers.BatchNormalization(),\n    layers.Dropout(rate = 0.5),\n    layers.Dense(7)\n])\nnewnet.build(input_shape=(224,224,3))\nnewnet.summary()\n\n# 装配\nnewnet.compile(optimizer=optimizers.Adam(lr=1e-3),\n              loss = losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练\nhistory = newnet.fit(train_data, \n                     validation_data=val_data, \n                     validation_freq=3,\n                    epochs=100)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 保存模型","metadata":{}},{"cell_type":"code","source":"tf.saved_model.save(newnet, \"./model-saveall\")\nprint(\"save all done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}