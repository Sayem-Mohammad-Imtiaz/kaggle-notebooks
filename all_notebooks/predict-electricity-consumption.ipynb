{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predict Electricity Consumption using Time Series Analysis"},{"metadata":{},"cell_type":"markdown","source":"## What is Time Series analysis?\n\nTime series forecasting is a technique for the prediction of events through a sequence of time. The technique is used across many fields of study, from geology to behaviour to economics. The techniques predict future events by analyzing the trends of the past, on the assumption that future trends will hold similar to historical trends.\n\n\n**Time series forecasting is performed in a variety of applications including:**\n\n1. Weather forecasting\n\n2. Earthquake prediction\n\n3. Astronomy\n\n4. Statistics\n\n5. Mathematical finance\n\n6. Econometrics\n\n7. Pattern recognition\n\n8. Signal processing\n\n9. Control engineering\n\nTime series forecasting is sometimes just the analysis of experts studying a field and offering their predictions. In many modern applications, however, time series forecasting uses computer technologies, including:\n\n1. Machine learning\n\n2. Artificial neural networks\n\n3. Support vector machines\n\n4. Fuzzy logic\n\n5. Gaussian processes\n\n6. Hidden Markov models\n\nThere are two main goals of time series analysis: (a) identifying the nature of the phenomenon represented by the sequence of observations, and (b) forecasting (predicting future values of the time series variable). Both of these goals require that the pattern of observed time series data is identified and more or less formally described. Once the pattern is established, we can interpret and integrate it with other data (i.e., use it in our theory of the investigated phenomenon, e.g., seasonal commodity prices). Regardless of the depth of our understanding and the validity of our interpretation (theory) of the phenomenon, we can extrapolate the identified pattern to predict future events.\n\n\n## Stages in Time Series Forecasting\n\nSolving a time series problem is a little different as compared to a regular modeling task. A simple/basic journey of solving a time series problem can be demonstrated through the following processes. We will understand about tasks which one needs to perform in every stage. \n\n\nWe will also look at the python implementation of each stage of our problem-solving journey.\n\nSteps are –\n\n\n**1. Visualizing time series**\n\nIn this step, we try to visualize the series. We try to identify all the underlying patterns related to the series like trend and seasonality. Do not worry about these terms right now, as we will discuss them during implementation. You can say that this is more a type of exploratory analysis of time series data.\n\n\n**2. Stationarising time series**\n\nA stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., “stationarised”) through the use of mathematical transformations. A stationarised series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past! Another reason for trying to stationarise a time series is to be able to obtain meaningful sample statistics such as means, variances, and correlations with other variables. Such statistics are useful as descriptors of future behavior only if the series is stationary. For example, if the series is consistently increasing over time, the sample mean and variance will grow with the size of the sample, and they will always underestimate the mean and variance in future periods. And if the mean and variance of a series are not well-defined, then neither are its correlations with other variables\n\n\n**3. Finding the best parameters for our model**\n\nWe need to find optimal parameters for forecasting models one’s we have a stationary series. These parameters come from the ACF and PACF plots. Hence, this stage is more about plotting above two graphs and extracting optimal model parameters based on them. Do not worry, we will cover on how to determine these parameters during the implementation part below!\n\n\n**4. Fitting model**\n\nOnce we have our optimal model parameters, we can fit an ARIMA model to learn the pattern of the series. Always remember that time series algorithms work on stationary data only hence making a series stationary is an important aspect\n\n\n**5. Predictions**\n\nAfter fitting our model, we will be predicting the future in this stage. Since we are now familiar with a basic flow of solving a time series problem, let us get to the implementation.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement\n\nThe dataset can be downloaded from here. It contains only 2 columns, one column is Date and the other column relates to the consumption percentage.\n\nIt shows the consumption of electricity from 1985 till 2018. The goal is to predict electricity consumption for the next 6 years i.e. till 2024."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') # special style template for matplotlib, highly useful for visualizing time series data\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 7\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/electric-production/Electric_Production.csv')\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, Define column names, drop nulls, convert Date to DateTime format and make Date as an index column because it is not possible to plot the graph without index."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns=['Date', 'Consumption']\ndf=df.dropna()\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.set_index('Date', inplace=True) #set date as index\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let us start with our predefined steps:"},{"metadata":{},"cell_type":"markdown","source":"### 1.Visualizing the time series."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.xlabel(\"Date\")\nplt.ylabel(\"Consumption\")\nplt.title(\"production graph\")\nplt.plot(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remember that for time series forecasting, a series needs to be stationary. The series should have a constant mean, variance, and covariance.\n\n\nThere are few points to note here, the mean is not constant in this case as we can clearly see an upward trend.\n\n\nHence, we have identified that our series is not stationary. We need to have a stationary series to do time series forecasting. In the next stage, we will try to convert this into a stationary series.\n\n\nLets us plot the scatterplot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(style='k.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also visualize the data in our series through a distribution too."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot(kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe a near-normal distribution(bell-curve) over consumption values.\n\n\nAlso, a given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.\n\nThese components are defined as follows:\n\n**Level**: The average value in the series.\n\n**Trend**: The increasing or decreasing value in the series.\n\n**Seasonality**: The repeating short-term cycle in the series.\n\n**Noise**: The random variation in the series.\n\nIn order to perform a time series analysis, we may need to separate seasonality and trend from our series. The resultant series will become stationary through this process.\n\n\nSo let us separate Trend and Seasonality from the time series."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(df, model='multiplicative')\nresult.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gives us more insight into our data and real-world actions. Clearly, there is an upward trend and a recurring event where electricity consumption shoots maximum every year."},{"metadata":{},"cell_type":"markdown","source":"### 2. Stationarising the time series.\n\n\nFirst, we need to check if a series is stationary or not.\n\n\n#### ADF (Augmented Dickey-Fuller) Test\n\nThe Dickey-Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test is:\n\n\n**Null Hypothesis:** The series has a unit root (value of a =1)\n\n\n**Alternate Hypothesis:** The series has no unit root.\n\n\nIf we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary (we will understand more about difference stationary in the next section).\n\n\nIf both mean and standard deviation are flat lines(constant mean and constant variance), the series becomes stationary.\n\n\nThe following function is one that can plot a series with its rolling mean and standard deviation."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(12).mean()\n    rolstd = timeseries.rolling(12).std()\n    #Plot rolling statistics:\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(rolmean, color='red', label='Rolling Mean')\n    plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean and Standard Deviation')\n    plt.show(block=False)\n\n    #perform dickey fuller test  \n    print(\"Results of dickey fuller test\")\n    adft = adfuller(timeseries['Consumption'],autolag='AIC')\n    # output for dft will give us without defining what the values are.\n    #hence we manually write what values does it explains using a for loop\n    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\n    for key,values in adft[4].items():\n        output['critical value (%s)'%key] =  values\n    print(output)\n\ntest_stationarity(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through the above graph, we can see the increasing mean and standard deviation and hence our series is not stationary."},{"metadata":{},"cell_type":"markdown","source":"We see that the p-value is greater than 0.05 so we cannot reject the Null hypothesis. Also, the test statistics is greater than the critical values. so the data is non-stationary.\n\n\nTo get a stationary series, we need to eliminate the trend and seasonality from the series.\n\n\nWe start by taking a log of the series to reduce the magnitude of the values and reduce the rising trend in the series. Then after getting the log of the series, we find the rolling average of the series. A rolling average is calculated by taking input for the past 12 months and giving a mean consumption value at every point further ahead in series."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log = np.log(df)\nmoving_avg = df_log.rolling(12).mean()\nstd_dev = df_log.rolling(12).std()\nplt.plot(df_log)\nplt.plot(moving_avg, color=\"red\")\nplt.plot(std_dev, color =\"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After finding the mean, we take the difference of the series and the mean at every point in the series.\n\nThis way, we eliminate trends out of a series and obtain a more stationary series."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_moving_avg_diff = df_log-moving_avg\ndf_log_moving_avg_diff.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perform the Dickey-Fuller test (ADFT) once again. We have to perform this function every time to check whether the data is stationary or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(df_log_moving_avg_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we observed that the data attained stationarity.\n\nOne of the modules is completed as we came to a conclusion. We need to check the weighted average, to understand the trend of the data in time series. Take the previous log data and to perform the following operation."},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_average = df_log.ewm(halflife=12, min_periods=0,adjust=True).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The exponential moving average (EMA) is a weighted average of the last n prices, where the weighting decreases exponentially with each previous price/period. In other words, the formula gives recent prices more weight than past prices."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df_log)\nplt.plot(weighted_average, color='red')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Consumption\")\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\nplt.legend()\nplt.show(block =False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Previously we subtracted df_log with moving average, now take the same df_log and subtract with weighted_average and perform the Dickey-Fuller test (ADFT) once again."},{"metadata":{"trusted":true},"cell_type":"code","source":"logScale_weightedMean = df_log-weighted_average\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10,6\ntest_stationarity(logScale_weightedMean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we observed that the data attained stationarity. We also see that the test statistics and critical value is relatively equal.\n\nThere can be cases when there is a high seasonality in the data. In those cases, just removing the trend will not help much. We need to also take care of the seasonality in the series. One such method for this task is differencing.\n\n\nDifferencing is a method of transforming a time series dataset.\nIt can be used to remove the series dependence on time, so-called temporal dependence. This includes structures like trends and seasonality. Differencing can help stabilize the mean of the time series by removing changes in the level of a time series, and so eliminating (or reducing) trend and seasonality.\n\n\nDifferencing is performed by subtracting the previous observation from the current observation.\n\nPerform the Dickey-Fuller test (ADFT) once again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_log_diff = df_log - df_log.shift()\nplt.title(\"Shifted timeseries\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Consumption\")\nplt.plot(df_log_diff)\n\n#Let us test the stationarity of our resultant series\ndf_log_diff.dropna(inplace=True)\ntest_stationarity(df_log_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to perform decomposition which provides a structured way of thinking about a time series forecasting problem, both generally in terms of modeling complexity and specifically in terms of how to best capture each of these components in a given model."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install chart_studio","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from chart_studio.plotly import plot_mpl\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(df_log, model='additive', freq = 12)\nresult.plot()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, perform the Dickey-Fuller test (ADFT) once again."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def test_stationarity_final(timeseries):\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(12).mean()\n    rolstd = timeseries.rolling(12).std()\n    #Plot rolling statistics:\n    plt.plot(timeseries, color='blue',label='Original')\n    plt.plot(rolmean, color='red', label='Rolling Mean')\n    plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean and Standard Deviation')\n    plt.show(block=False)\n\n\n\ntrend = result.trend\ntrend.dropna(inplace=True)\nseasonality = result.seasonal\nseasonality.dropna(inplace=True)\nresidual = result.resid\nresidual.dropna(inplace=True)\ntest_stationarity_final(residual)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the decomposition, if we look at the residual then we have clearly a flat line for both mean and standard deviation. We have got our stationary series and now we can move to find the best parameters for our model."},{"metadata":{},"cell_type":"markdown","source":"### 3. Finding the best parameters for our model\n\nBefore we go on to build our forecasting model, we need to determine optimal parameters for our model. For those optimal parameters, we need ACF and PACF plots.\n\n\nA nonseasonal ARIMA model is classified as an “ARIMA(p,d,q)” model, where:\n\np → Number of autoregressive terms,\nd → Number of nonseasonal differences needed for stationarity, and\nq → Number of lagged forecast errors in the prediction equation.\n\n\nValues of p and q come through ACF and PACF plots. So let us understand both ACF and PACF!"},{"metadata":{},"cell_type":"markdown","source":"#### Autocorrelation Function(ACF)\n\nStatistical correlation summarizes the strength of the relationship between two variables. Pearson’s correlation coefficient is a number between -1 and 1 that describes a negative or positive correlation respectively. A value of zero indicates no correlation.\n\n\nWe can calculate the correlation for time series observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an autocorrelation.\n\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a correlogram or an autocorrelation plot.\n\n\n#### Partial Autocorrelation Function(PACF)\n\nA partial autocorrelation is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n\n\nThe partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\n\n\nThe autocorrelation for observation and observation at a prior time step is comprised of both the direct correlation and indirect correlations. It is these indirect correlations that the partial autocorrelation function seeks to remove.\n\n\nBelow code plots, both ACF and PACF plots for us:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import acf,pacf\n# we use d value here(data_log_shift)\nacf = acf(df_log_diff, nlags=15)\npacf= pacf(df_log_diff, nlags=15,method='ols')#plot PACF\nplt.subplot(121)\nplt.plot(acf) \nplt.axhline(y=0,linestyle='-',color='blue')\nplt.axhline(y=-1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='black')\nplt.axhline(y=1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='black')\nplt.title('Auto corellation function')\nplt.tight_layout()#plot ACF\nplt.subplot(122)\nplt.plot(pacf) \nplt.axhline(y=0,linestyle='-',color='blue')\nplt.axhline(y=-1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='black')\nplt.axhline(y=1.96/np.sqrt(len(df_log_diff)),linestyle='--',color='black')\nplt.title('Partially auto corellation function')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Fitting model\n\nIn order to find the p and q values from the above graphs, we need to check, where the graph cuts off the origin or drops to zero for the first time from the above graphs the p and q values are merely close to 3 where the graph cuts off the origin ( draw the line to x-axis) now we have p,d,q values. So now we can substitute in the ARIMA model and let's see the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(df_log, order=(3,1,3))\nresult_AR = model.fit(disp = 0)\nplt.plot(df_log_diff)\nplt.plot(result_AR.fittedvalues, color='red')\nplt.title(\"sum of squares of residuals\")\nprint('RSS : %f' %sum((result_AR.fittedvalues-df_log_diff[\"Consumption\"])**2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Less the RSS value, the more effective the model is. You check with (2,1,0),(3,1,1), etc to look for the smallest values of RSS."},{"metadata":{},"cell_type":"markdown","source":"### 5. Predictions\n\nThe following code helps us to forecast shampoo sales for the next 6 years."},{"metadata":{"trusted":true},"cell_type":"code","source":"result_AR.plot_predict(1,500)\nx=result_AR.forecast(steps=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, we calculated the future predictions till 2024 the greyed out area is the confidence interval that means the predictions will not cross that area."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}