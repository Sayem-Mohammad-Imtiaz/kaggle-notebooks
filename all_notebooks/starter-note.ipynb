{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Reading data","metadata":{}},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-19T06:20:48.415287Z","iopub.execute_input":"2021-08-19T06:20:48.415704Z","iopub.status.idle":"2021-08-19T06:20:49.566626Z","shell.execute_reply.started":"2021-08-19T06:20:48.415608Z","shell.execute_reply":"2021-08-19T06:20:49.565751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/emotion-detection-from-text/tweet_emotions.csv', delimiter=',')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:07:43.158938Z","iopub.execute_input":"2021-08-19T03:07:43.159285Z","iopub.status.idle":"2021-08-19T03:07:43.335441Z","shell.execute_reply.started":"2021-08-19T03:07:43.159255Z","shell.execute_reply":"2021-08-19T03:07:43.334497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#some modifications (comment out based on need)\n# df = df[df.sentiment != 'anger'] #& 'boredom' & 'enthusiasm' & 'empty'\n# df = df[df.sentiment != 'boredom']\n# df = df[df.sentiment != 'enthusiasm']\n# df = df[df.sentiment != 'empty']\n# df = df[df.sentiment != 'neutral']\n# df = df[df.sentiment != 'other']\n# df = df[df.sentiment != 'sentiment'] #there is sentiment in sentiments!","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:48:10.700684Z","iopub.execute_input":"2021-08-19T03:48:10.701082Z","iopub.status.idle":"2021-08-19T03:48:10.748438Z","shell.execute_reply.started":"2021-08-19T03:48:10.701051Z","shell.execute_reply":"2021-08-19T03:48:10.747179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">EDA</p>","metadata":{}},{"cell_type":"code","source":"def basic_eda(df, row_limit=5, list_elements_limit=10):\n    ### rows and columns\n    print('Info : There are {} columns in the dataset'.format(df.shape[1]))\n    print('Info : There are {} rows in the dataset'.format(df.shape[0]))\n    \n    print(\"==================================================\")\n    \n    ## data types\n    print(\"\\nData type information of different columns\")\n    dtypes_df = pd.DataFrame(df.dtypes).reset_index().rename(columns={0:'dtype', 'index':'column_name'})\n    cat_df = dtypes_df[dtypes_df['dtype']=='object']\n    num_df = dtypes_df[dtypes_df['dtype']!='object']\n    print('Info : There are {} categorical columns'.format(len(cat_df)))\n    print('Info : There are {} numerical columns'.format(len(dtypes_df)-len(cat_df)))\n    \n    if list_elements_limit >= len(cat_df):\n        print(\"Categorical columns : \", list(cat_df['column_name']))\n    else:\n        print(\"Categorical columns : \", list(cat_df['column_name'])[:list_elements_limit])\n        \n    if list_elements_limit >= len(num_df):\n        print(\"Numerical columns : \", list(num_df['column_name']))\n    else:\n        print(\"Numerical columns : \", list(num_df['column_name'])[:list_elements_limit])\n    \n    #dtypes_df['dtype'].value_counts().plot.bar()\n    display(dtypes_df.head(row_limit))\n    \n    print(\"==================================================\")\n    print(\"\\nDescription of numerical variables\")\n    \n    #### Describibg numerical columns\n    desc_df_num = df[list(num_df['column_name'])].describe().T.reset_index().rename(columns={'index':'column_name'})\n    display(desc_df_num.head(row_limit))\n    \n    print(\"==================================================\")\n    print(\"\\nDescription of categorical variables\")\n    \n    desc_df_cat = df[list(cat_df['column_name'])].describe().T.reset_index().rename(columns={'index':'column_name'})\n    display(desc_df_cat.head(row_limit))\n    \n    return","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T03:10:09.322326Z","iopub.execute_input":"2021-08-19T03:10:09.322675Z","iopub.status.idle":"2021-08-19T03:10:09.335379Z","shell.execute_reply.started":"2021-08-19T03:10:09.322645Z","shell.execute_reply":"2021-08-19T03:10:09.333825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_eda(df)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:13:05.027922Z","iopub.execute_input":"2021-08-19T03:13:05.028327Z","iopub.status.idle":"2021-08-19T03:13:05.117992Z","shell.execute_reply.started":"2021-08-19T03:13:05.028278Z","shell.execute_reply":"2021-08-19T03:13:05.116659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quickly check for mising values\ntotal = df.isnull().sum()\ntotal","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:13:08.680861Z","iopub.execute_input":"2021-08-19T03:13:08.681268Z","iopub.status.idle":"2021-08-19T03:13:08.696827Z","shell.execute_reply.started":"2021-08-19T03:13:08.681224Z","shell.execute_reply":"2021-08-19T03:13:08.695539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of sentiments in the data","metadata":{}},{"cell_type":"code","source":"col = 'sentiment'\nfig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\nexplode = list((np.array(list(df[col].dropna().value_counts()))/sum(list(df[col].dropna().value_counts())))[::-1])[:10]\nlabels = list(df[col].dropna().unique())[:10]\nsizes = df[col].value_counts()[:10]\n#ax.pie(sizes, explode=explode, colors=bo, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.pie(sizes,  explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.add_artist(plt.Circle((0,0),0.6,fc='white'))\nsns.countplot(y =col, data = df, ax=ax1)\nax1.set_title(\"Count of each emotion\")\nax2.set_title(\"Percentage of each emotion\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T03:48:15.904686Z","iopub.execute_input":"2021-08-19T03:48:15.905038Z","iopub.status.idle":"2021-08-19T03:48:16.239161Z","shell.execute_reply.started":"2021-08-19T03:48:15.905007Z","shell.execute_reply":"2021-08-19T03:48:16.237697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see that there are 13 different classes and some of the are having very few examples. (i.e. Anger, Boredom, Empty etc...). This is a very imbalanced dataset and it will not allow the model to converge. We'll reduce the number of classes.\nlike this","metadata":{}},{"cell_type":"code","source":"df['sentiment'] = df['sentiment'].apply(lambda x : x if x in ['happiness', 'sadness', 'worry', 'neutral', 'love'] else \"other\") ","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:48:04.685736Z","iopub.execute_input":"2021-08-19T03:48:04.686149Z","iopub.status.idle":"2021-08-19T03:48:04.704876Z","shell.execute_reply.started":"2021-08-19T03:48:04.686102Z","shell.execute_reply":"2021-08-19T03:48:04.702822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col = 'sentiment'\nfig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12,8))\nexplode = list((np.array(list(df[col].dropna().value_counts()))/sum(list(df[col].dropna().value_counts())))[::-1])[:10]\nlabels = list(df[col].dropna().unique())[:10]\nsizes = df[col].value_counts()[:10]\n#ax.pie(sizes, explode=explode, colors=bo, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.pie(sizes,  explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\nax2.add_artist(plt.Circle((0,0),0.6,fc='white'))\nsns.countplot(y =col, data = df, ax=ax1)\nax1.set_title(\"Count of each emotion\")\nax2.set_title(\"Percentage of each emotion\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T03:13:24.715621Z","iopub.execute_input":"2021-08-19T03:13:24.716037Z","iopub.status.idle":"2021-08-19T03:13:25.052356Z","shell.execute_reply.started":"2021-08-19T03:13:24.716007Z","shell.execute_reply":"2021-08-19T03:13:25.050922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are good to go!","metadata":{}},{"cell_type":"code","source":"df['char_length'] = df['content'].apply(lambda x : len(x))\ndf['token_length'] = df['content'].apply(lambda x : len(x.split(\" \")))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:13:55.764293Z","iopub.execute_input":"2021-08-19T03:13:55.76468Z","iopub.status.idle":"2021-08-19T03:13:55.822633Z","shell.execute_reply.started":"2021-08-19T03:13:55.764647Z","shell.execute_reply":"2021-08-19T03:13:55.821565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of character length and token length overall","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nsns.distplot(df['char_length'], ax=ax1)\nsns.distplot(df['token_length'], ax=ax2)\nax1.set_title('Number of characters in the tweet')\nax2.set_title('Number of token(words) in the tweet')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T03:13:57.131263Z","iopub.execute_input":"2021-08-19T03:13:57.131648Z","iopub.status.idle":"2021-08-19T03:13:57.932339Z","shell.execute_reply.started":"2021-08-19T03:13:57.131613Z","shell.execute_reply":"2021-08-19T03:13:57.931206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of character length sentiment-wise [Top 5 sentiments]","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,8))\nfor sentiment in df['sentiment'].value_counts().sort_values()[-5:].index.tolist():\n    #print(sentiment)\n    sns.kdeplot(df[df['sentiment']==sentiment]['char_length'],ax=ax, label=sentiment)\nax.legend()\nax.set_title(\"Distribution of character length sentiment-wise [Top 5 sentiments]\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T03:14:00.716653Z","iopub.execute_input":"2021-08-19T03:14:00.717013Z","iopub.status.idle":"2021-08-19T03:14:01.183247Z","shell.execute_reply.started":"2021-08-19T03:14:00.71698Z","shell.execute_reply":"2021-08-19T03:14:01.181934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution of token length sentiment-wise [Top 5 sentiments]","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\nfor sentiment in df['sentiment'].value_counts().sort_values()[-5:].index.tolist():\n    #print(sentiment)\n    sns.kdeplot(df[df['sentiment']==sentiment]['token_length'],ax=ax, label=sentiment)\nax.legend()\nax.set_title(\"Distribution of token length sentiment-wise [Top 5 sentiments]\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T03:14:17.657318Z","iopub.execute_input":"2021-08-19T03:14:17.657704Z","iopub.status.idle":"2021-08-19T03:14:18.085704Z","shell.execute_reply.started":"2021-08-19T03:14:17.657671Z","shell.execute_reply":"2021-08-19T03:14:18.084591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's look at the most common character and token length","metadata":{}},{"cell_type":"code","source":"avg_df = df.groupby('sentiment').agg({'char_length':'mean', 'token_length':'mean'})","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:14:18.711351Z","iopub.execute_input":"2021-08-19T03:14:18.711745Z","iopub.status.idle":"2021-08-19T03:14:18.726161Z","shell.execute_reply.started":"2021-08-19T03:14:18.711712Z","shell.execute_reply":"2021-08-19T03:14:18.72489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2)  = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\nax1.bar(avg_df.index, avg_df['char_length'])\nax2.bar(avg_df.index, avg_df['token_length'], color='green')\nax1.set_title('Avg number of characters')\nax2.set_title('Avg number of token(words)')\nax1.set_xticklabels(avg_df.index, rotation = 45)\nax2.set_xticklabels(avg_df.index, rotation = 45)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T03:14:24.647222Z","iopub.execute_input":"2021-08-19T03:14:24.647608Z","iopub.status.idle":"2021-08-19T03:14:25.028269Z","shell.execute_reply.started":"2021-08-19T03:14:24.647577Z","shell.execute_reply":"2021-08-19T03:14:25.026943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations \n- There are 13 different emotions.\n- \"Neurtal\" and \"Worry\" are the most frequent emotions in the dataset.\n- Most of the tweets have around 45 characters.\n- The most frequent token length is around 10.\n- people having \"empty\" and \"neutral\" emotion write smaller tweet.","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Text Processing</p>\n\nWe'll do all text pre-processing in a single step using this generic function. Details of each step provided in the function details","metadata":{}},{"cell_type":"code","source":"!pip install inflect","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T06:21:05.933617Z","iopub.execute_input":"2021-08-19T06:21:05.93415Z","iopub.status.idle":"2021-08-19T06:21:15.063936Z","shell.execute_reply.started":"2021-08-19T06:21:05.934104Z","shell.execute_reply":"2021-08-19T06:21:15.062911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T06:21:15.065683Z","iopub.execute_input":"2021-08-19T06:21:15.066023Z","iopub.status.idle":"2021-08-19T06:21:27.997243Z","shell.execute_reply.started":"2021-08-19T06:21:15.065979Z","shell.execute_reply":"2021-08-19T06:21:27.995462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bs4","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T06:21:27.999846Z","iopub.execute_input":"2021-08-19T06:21:28.000375Z","iopub.status.idle":"2021-08-19T06:21:38.130464Z","shell.execute_reply.started":"2021-08-19T06:21:28.000322Z","shell.execute_reply":"2021-08-19T06:21:38.129229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data preparation and text-preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport nltk\nimport inflect\nimport contractions\nfrom bs4 import BeautifulSoup\nimport re, string, unicodedata\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:21:38.133165Z","iopub.execute_input":"2021-08-19T06:21:38.133558Z","iopub.status.idle":"2021-08-19T06:21:39.163349Z","shell.execute_reply.started":"2021-08-19T06:21:38.133518Z","shell.execute_reply":"2021-08-19T06:21:39.162385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### text preprocessing\n\ndef text_preprocessing_platform(df, text_col, remove_stopwords=True):\n    \n    ## Define functions for individual steps\n    # First function is used to denoise text\n    def denoise_text(text):\n        # Strip html if any. For ex. removing <html>, <p> tags\n        soup = BeautifulSoup(text, \"html.parser\")\n        text = soup.get_text()\n        # Replace contractions in the text. For ex. didn't -> did not\n        text = contractions.fix(text)\n        return text\n    \n    ## Next step is text-normalization\n    \n    # Text normalization includes many steps.\n    \n    # Each function below serves a step.\n    \n    \n    def remove_non_ascii(words):\n        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n            new_words.append(new_word)\n        return new_words\n    \n    \n    def to_lowercase(words):\n        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = word.lower()\n            new_words.append(new_word)\n        return new_words\n    \n    \n    def remove_punctuation(words):\n        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            new_word = re.sub(r'[^\\w\\s]', '', word)\n            if new_word != '':\n                new_words.append(new_word)\n        return new_words\n    \n    \n    def replace_numbers(words):\n        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n        p = inflect.engine()\n        new_words = []\n        for word in words:\n            if word.isdigit():\n                new_word = p.number_to_words(word)\n                new_words.append(new_word)\n            else:\n                new_words.append(word)\n        return new_words\n    \n    \n    def remove_numbers(words):\n        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n        p = inflect.engine()\n        new_words = []\n        for word in words:\n            if word.isdigit():\n                pass\n#                 new_word = p.number_to_words(word)\n#                 new_words.append(new_word)\n            else:\n                new_words.append(word)\n        return new_words\n    \n    \n    def remove_stopwords(words):\n        \"\"\"Remove stop words from list of tokenized words\"\"\"\n        new_words = []\n        for word in words:\n            if word not in stopwords.words('english'):\n                new_words.append(word)\n        return new_words\n    \n    \n    def stem_words(words):\n        \"\"\"Stem words in list of tokenized words\"\"\"\n        stemmer = LancasterStemmer()\n        stems = []\n        for word in words:\n            stem = stemmer.stem(word)\n            stems.append(stem)\n        return stems\n    \n    \n    def lemmatize_verbs(words):\n        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n        lemmatizer = WordNetLemmatizer()\n        lemmas = []\n        for word in words:\n            lemma = lemmatizer.lemmatize(word, pos='v')\n            lemmas.append(lemma)\n        return lemmas\n    \n    \n    ### A wrap-up function for normalization\n    def normalize_text(words, remove_stopwords):\n        words = remove_non_ascii(words)\n        words = to_lowercase(words)\n        words = remove_punctuation(words)\n        words = replace_numbers(words)\n        if remove_stopwords:\n            words = remove_stopwords(words)\n        #words = stem_words(words)\n        words = lemmatize_verbs(words)\n        return words\n    \n    def normalize_text2(words, remove_stopwords):\n      \n        words = remove_punctuation(words)\n        words = remove_numbers(words)\n        if remove_stopwords:\n            words = remove_stopwords(words)\n        #words = stem_words(words)\n        words = lemmatize_verbs(words)\n        return words\n    \n    # All above functions work on word tokens we need a tokenizer\n    \n    # Tokenize tweet into words\n    def tokenize(text):\n        return nltk.word_tokenize(text)\n    \n    \n    # A overall wrap-up function\n    def text_prepare(text):\n        text = denoise_text(text)\n        text = ' '.join([x for x in normalize_text(tokenize(text), remove_stopwords)])\n        return text\n    \n    def text_prepare2(text): # Hadi's version for applying on top of the previous preprocessed data!!\n        text = denoise_text(text)\n        text = ' '.join([x for x in normalize_text2(tokenize(text), remove_stopwords)])\n        return text\n    \n    # run every-step\n    df[text_col] = [text_prepare(x) for x in df[text_col]] # BEWARE TO CHANGE BACK\n#     df[text_col] = [text_prepare2(x) for x in df[text_col]] #uncomment for my version of preprocessing  BEWARE TO CHANGE BACK\n\n    \n    # return processed df\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:21:39.165136Z","iopub.execute_input":"2021-08-19T06:21:39.165587Z","iopub.status.idle":"2021-08-19T06:21:39.195017Z","shell.execute_reply.started":"2021-08-19T06:21:39.165555Z","shell.execute_reply":"2021-08-19T06:21:39.193714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Before Text Preprocessing\")\ndisplay(df.head()[['content']])\nprocessed_df = text_preprocessing_platform(df, 'content', remove_stopwords=False)\nprint(\"After Text Preprocessing\")\ndisplay(processed_df.head()[['content']])","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:15:15.599388Z","iopub.execute_input":"2021-08-19T03:15:15.599922Z","iopub.status.idle":"2021-08-19T03:16:24.470314Z","shell.execute_reply.started":"2021-08-19T03:15:15.599864Z","shell.execute_reply":"2021-08-19T03:16:24.468879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Uni-gram Analysis\n\nLet's find out the most frequent words in the corpus.","metadata":{}},{"cell_type":"code","source":"def print_word_cloud(df, sentiment):\n\n    print(\"Word cloud of most frequent words for the sentiment : {}\".format(sentiment))\n\n    temp_df = df[df['sentiment']==sentiment]\n    print(\"Number of Rows : \", len(temp_df))\n\n    corpus = ''\n    for text in temp_df.content:\n        text = str(text)\n        corpus += text\n        \n    total = 0\n    count = defaultdict(lambda: 0)\n    for word in corpus.split(\" \"):\n        total += 1\n        count[word] += 1\n        \n    top20pairs = sorted(count.items(), key=lambda kv: kv[1], reverse=True)[:20]\n    top20words = [i[0] for i in top20pairs]\n    top20freq = [i[1] for i in top20pairs]\n    \n    xs = np.arange(len(top20words))\n    width = 0.5\n\n    fig = plt.figure(figsize=(10,6))                                                               \n    ax = fig.gca()  #get current axes\n    ax.bar(xs, top20freq, width, align='center')\n\n    ax.set_xticks(xs)\n    ax.set_xticklabels(top20words)\n    plt.xticks(rotation=45)\n    \n    \n    stopwords = set(STOPWORDS)\n    # lower max_font_size, change the maximum number of word and lighten the background:\n    wordcloud = WordCloud(max_font_size=50, max_words=50,stopwords=stopwords, background_color=\"white\").generate(corpus)\n    plt.figure(figsize = (12, 12), facecolor = None)\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:16:24.472821Z","iopub.execute_input":"2021-08-19T03:16:24.473406Z","iopub.status.idle":"2021-08-19T03:16:24.487158Z","shell.execute_reply.started":"2021-08-19T03:16:24.473331Z","shell.execute_reply":"2021-08-19T03:16:24.485637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_word_cloud(df, 'sadness')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:16:24.48908Z","iopub.execute_input":"2021-08-19T03:16:24.489604Z","iopub.status.idle":"2021-08-19T03:16:25.472389Z","shell.execute_reply.started":"2021-08-19T03:16:24.489545Z","shell.execute_reply":"2021-08-19T03:16:25.471139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_word_cloud(df, 'happiness')","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:16:25.473999Z","iopub.execute_input":"2021-08-19T03:16:25.474423Z","iopub.status.idle":"2021-08-19T03:16:26.602587Z","shell.execute_reply.started":"2021-08-19T03:16:25.474382Z","shell.execute_reply":"2021-08-19T03:16:26.601249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The word cloud makes sense as per the emotion","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Model Development</p>\n\n### Bidirectional LSTM (using GloVe Embedding)","metadata":{}},{"cell_type":"code","source":"# Model Building\nfrom keras.layers import Dropout, Dense, GRU, Embedding, LSTM, Bidirectional, TimeDistributed, Flatten\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import matthews_corrcoef, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n\n# Logging\nimport logging\nlogging.basicConfig(level=logging.INFO)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:21:39.196749Z","iopub.execute_input":"2021-08-19T06:21:39.197128Z","iopub.status.idle":"2021-08-19T06:21:45.631306Z","shell.execute_reply.started":"2021-08-19T06:21:39.197085Z","shell.execute_reply":"2021-08-19T06:21:45.629789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Embedding textual data using GloVe Embeddings","metadata":{}},{"cell_type":"code","source":"def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n    np.random.seed(7)\n    text = np.concatenate((X_train, X_test), axis=0)\n    text = np.array(text)\n    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n    tokenizer.fit_on_texts(text)\n    sequences = tokenizer.texts_to_sequences(text)\n    word_index = tokenizer.word_index\n    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n    print('Found %s unique tokens.' % len(word_index))\n    indices = np.arange(text.shape[0])\n    # np.random.shuffle(indices)\n    text = text[indices]\n    print(text.shape)\n    X_train = text[0:len(X_train), ]\n    X_test = text[len(X_train):, ]\n    embeddings_index = {}\n    f = open(\"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\", encoding='utf-8')\n    for line in f:\n        try:\n            values = line.split()\n            word = values[0]\n            try:\n                coefs = np.asarray(values[1:], dtype='float32')\n            except:\n                pass\n            embeddings_index[word] = coefs\n        except UnicodeDecodeError:\n            pass\n    f.close()\n    print('Total %s word vectors.' % len(embeddings_index))\n    return (X_train, X_test, word_index,embeddings_index, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:21:59.429798Z","iopub.execute_input":"2021-08-19T06:21:59.430193Z","iopub.status.idle":"2021-08-19T06:21:59.442309Z","shell.execute_reply.started":"2021-08-19T06:21:59.430151Z","shell.execute_reply":"2021-08-19T06:21:59.440531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building Bidirectional LSTM Model","metadata":{}},{"cell_type":"code","source":"def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n    # Model building\n    model = Sequential()\n    hidden_layer = 2\n    lstm_node = 32\n    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            if len(embedding_matrix[i]) != len(embedding_vector):\n                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n                exit(1)\n            embedding_matrix[i] = embedding_vector\n    model.add(Embedding(len(word_index) + 1,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True))\n    print(lstm_node)\n    for i in range(0,hidden_layer):\n        model.add(Bidirectional(LSTM(lstm_node,return_sequences=True, recurrent_dropout=0.5)))\n        model.add(Dropout(dropout))\n    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.5)))\n    model.add(Dropout(dropout))\n    #model.add(TimeDistributed(Dense(256)))\n    #model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(nclasses, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-19T06:22:03.092653Z","iopub.execute_input":"2021-08-19T06:22:03.093064Z","iopub.status.idle":"2021-08-19T06:22:03.105976Z","shell.execute_reply.started":"2021-08-19T06:22:03.09303Z","shell.execute_reply":"2021-08-19T06:22:03.104469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation Utilities","metadata":{}},{"cell_type":"code","source":"###Utility\n\ndef get_eval_report(labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    precision = (tp)/(tp+fp)\n    recall = (tp)/(tp+fn)\n    f1 = (2*(precision*recall))/(precision+recall)\n    return {\n        \"mcc\": mcc,\n        \"tp\": tp,\n        \"tn\": tn,\n        \"fp\": fp,\n        \"fn\": fn,\n        \"pricision\" : precision,\n        \"recall\" : recall,\n        \"F1\" : f1,\n        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n    }\n\ndef compute_metrics(labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(labels, preds)\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\ndef class_balance(df, target):\n  cls = df[target].value_counts()\n  cls.plot(kind='bar')\n  plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-19T06:22:06.815216Z","iopub.execute_input":"2021-08-19T06:22:06.815605Z","iopub.status.idle":"2021-08-19T06:22:06.827084Z","shell.execute_reply.started":"2021-08-19T06:22:06.815564Z","shell.execute_reply":"2021-08-19T06:22:06.825931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Running Experiment","metadata":{}},{"cell_type":"code","source":"preprocess = True\ntext = 'content'\ntarget = 'sentiment'\nMAX_SEQUENCE_LENGTH = 60","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:12:29.518373Z","iopub.execute_input":"2021-08-19T07:12:29.519014Z","iopub.status.idle":"2021-08-19T07:12:29.523008Z","shell.execute_reply.started":"2021-08-19T07:12:29.518965Z","shell.execute_reply":"2021-08-19T07:12:29.522223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# traindf = pd.read_csv('/kaggle/input/emotion-detection-from-text/tweet_emotions.csv', delimiter=',')\ndf = pd.read_csv('/kaggle/input/english-contractions/df_processed.csv', delimiter=',')\n#some modifications (comment out based on need)\n# df = df[df.sentiment != 'neutral']\ndf['sentiment'] = df['sentiment'].apply(lambda x : x if x in ['happiness', 'sadness', 'worry', 'neutral', 'love','surprise'] else \"other\") \ndf = df[df.sentiment != 'other']\n# run preprocessing with uncommenting text prepare2\nprint(\"Before Text Preprocessing\")\ndisplay(df.head()[['content']])\n# processed_df = text_preprocessing_platform(df, 'content', remove_stopwords=False)\nprint(\"After Text Preprocessing\")\ndisplay(df.head()[['content']]) #BEWARE\n\n\n\n\n# if preprocess:\n#     traindf = text_preprocessing_platform(traindf, text)\n    \n# train_final = processed_df[['content', 'sentiment']]\ntrain_final = df[['content', 'sentiment']]\nprint(\"Train DataFrame\")\ndisplay(train_final.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:13:27.856505Z","iopub.execute_input":"2021-08-19T07:13:27.856952Z","iopub.status.idle":"2021-08-19T07:13:27.999964Z","shell.execute_reply.started":"2021-08-19T07:13:27.856907Z","shell.execute_reply":"2021-08-19T07:13:27.998514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.to_csv('df_processed_6labels.csv',index=False)\n# le = LabelEncoder()\n# df['labels'] = le.fit_transform(df['sentiment'])\n# df = df[['tweet_id','content', 'sentiment' , 'labels']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:17:10.87134Z","iopub.execute_input":"2021-08-19T07:17:10.871755Z","iopub.status.idle":"2021-08-19T07:17:11.076633Z","shell.execute_reply.started":"2021-08-19T07:17:10.871721Z","shell.execute_reply":"2021-08-19T07:17:11.075609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Label encoding target column\nle = LabelEncoder()\ntrain_final['sentiment'] = le.fit_transform(train_final['sentiment'])\n\n## df for training and prediction\ndf = train_final","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:31:32.379835Z","iopub.execute_input":"2021-08-19T07:31:32.380274Z","iopub.status.idle":"2021-08-19T07:31:32.398906Z","shell.execute_reply.started":"2021-08-19T07:31:32.380237Z","shell.execute_reply":"2021-08-19T07:31:32.397709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:31:33.926516Z","iopub.execute_input":"2021-08-19T07:31:33.926881Z","iopub.status.idle":"2021-08-19T07:31:33.936857Z","shell.execute_reply.started":"2021-08-19T07:31:33.926852Z","shell.execute_reply":"2021-08-19T07:31:33.936052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train - test split","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"X = df[text]\ny = df[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nprint(\"Generating Glove Embeddings...\")\nX_train_Glove,X_test_Glove, word_index,embeddings_index, tokenizer = loadData_Tokenizer(X_train,X_test, MAX_SEQUENCE_LENGTH=MAX_SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:31:37.644691Z","iopub.execute_input":"2021-08-19T07:31:37.645238Z","iopub.status.idle":"2021-08-19T07:31:50.725615Z","shell.execute_reply.started":"2021-08-19T07:31:37.645203Z","shell.execute_reply":"2021-08-19T07:31:50.724493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:31:50.728337Z","iopub.execute_input":"2021-08-19T07:31:50.728779Z","iopub.status.idle":"2021-08-19T07:31:50.73573Z","shell.execute_reply.started":"2021-08-19T07:31:50.728733Z","shell.execute_reply":"2021-08-19T07:31:50.734565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check previous value! this should be imported into the next cell for the number of classes","metadata":{}},{"cell_type":"code","source":"# Model Training\nwith warnings.catch_warnings():\n    print(\"Building Model ...\")\n    model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, 6)\n    model_RNN.summary()\n    print(\"\\n Starting Training ... \\n\")\n    history = model_RNN.fit(X_train_Glove, y_train,\n                              validation_data=(X_test_Glove, y_test),\n                              epochs=6,\n                              batch_size=128,\n                              verbose=1)\n    warnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:33:02.116332Z","iopub.execute_input":"2021-08-19T07:33:02.117106Z","iopub.status.idle":"2021-08-19T07:39:28.669809Z","shell.execute_reply.started":"2021-08-19T07:33:02.117052Z","shell.execute_reply":"2021-08-19T07:39:28.668477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n Plotting results ... \\n\")\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')\nprint(\"\\n Evaluating Model ... \\n\")\npredicted = model_RNN.predict_classes(X_test_Glove)\n#print(predicted)\nprint(metrics.classification_report(y_test, predicted))\n# print(\"\\n\")\n# logger = logging.getLogger(\"logger\")\n# result = compute_metrics(y_test, predicted)\n# for key in (result.keys()):\n#   logger.info(\"  %s = %s\", key, str(result[key]))","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:51:03.248856Z","iopub.execute_input":"2021-08-19T07:51:03.249307Z","iopub.status.idle":"2021-08-19T07:51:13.080731Z","shell.execute_reply.started":"2021-08-19T07:51:03.249275Z","shell.execute_reply":"2021-08-19T07:51:13.079941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T07:32:03.84979Z","iopub.status.idle":"2021-08-19T07:32:03.850443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The model is performing really bad.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-19T04:19:51.706568Z","iopub.execute_input":"2021-08-19T04:19:51.706984Z","iopub.status.idle":"2021-08-19T04:19:51.719765Z","shell.execute_reply.started":"2021-08-19T04:19:51.706954Z","shell.execute_reply":"2021-08-19T04:19:51.718268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-19T03:43:11.392245Z","iopub.execute_input":"2021-08-19T03:43:11.392844Z","iopub.status.idle":"2021-08-19T03:43:11.4465Z","shell.execute_reply.started":"2021-08-19T03:43:11.392797Z","shell.execute_reply":"2021-08-19T03:43:11.444444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}