{"cells":[{"metadata":{},"cell_type":"markdown","source":"The purpose of this notebook is to explore several different options for model and feature selection in order to classify mushrooms as either edible or poisonous. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Metadata: \n\nAttribute Information: (classes: edible=e, poisonous=p)\n\ncap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n\ncap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n\ncap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n\nbruises: bruises=t,no=f\n\nodor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\ngill-attachment: attached=a,descending=d,free=f,notched=n\n\ngill-spacing: close=c,crowded=w,distant=d\n\ngill-size: broad=b,narrow=n\n\ngill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n\nstalk-shape: enlarging=e,tapering=t\n\nstalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n\nstalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\nstalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\nstalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\nstalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\nveil-type: partial=p,universal=u\n\nveil-color: brown=n,orange=o,white=w,yellow=y\n\nring-number: none=n,one=o,two=t\n\nring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n\nspore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n\npopulation: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n\nhabitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost\nfrom xgboost import XGBClassifier, plot_importance, plot_tree\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import SelectKBest, chi2, RFE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we need to load the data into a DataFrame and check the head to make sure that our data is loaded correctly. ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"address = '/kaggle/input/mushroom-classification/mushrooms.csv'\nmushroom_data = pd.read_csv(address) \nmushroom_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should also check for NaN or missing vlaues in our DataFrame ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mushroom_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we need to select our target column, as well as our features. \nWe use the \"get_dummies\" approach to encode the strings as binary columns, dropping the first of any encoding to ensure that we avoid overparameterisation with redundant information.  \nFinally we use sklearn's \"test_train_split\" to split our data into a 2/3 training batch and a 1/3 testing batch.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = mushroom_data.drop('class',axis=1)\ny = mushroom_data['class'].values\nX = pd.get_dummies(X, drop_first=True)\ny = pd.get_dummies(y, drop_first=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check our training DataFrame.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a number of different models that can be used to solve classification problems with supervised learning, and we can compare a variety of these to see which is the most effective with this dataset. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint(accuracy_score(preds, np.ravel(y_test)))\nprint(confusion_matrix(preds, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can test the permuation importance of the features used in the prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBClassifier()\nxgb_model.fit(X_train, y_train,\n             early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)],\n             verbose=True)\nxgb_preds=xgb_model.predict(X_test)\nconfusion_matrix(xgb_preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(xgb_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is also possible to make use of the inbuilt plotting functions of XGBoost to plot the tree structure","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_importance(xgb_model, importance_type='weight')\nfig, ax = plt.subplots(figsize=(30, 30))\nxgboost.plot_tree(xgb_model, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_model = GaussianNB()\nnb_model.fit(X_train, np.ravel(y_train))\npreds = nb_model.predict(X_test)\nconfusion_matrix(preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(nb_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier()\ntree_model.fit(X_train, y_train)\npreds = tree_model.predict(X_test)\nconfusion_matrix(preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(tree_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplot_tree(tree_model, feature_names=X_test.columns.tolist())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_model = SVC()\nsvm_model.fit(X_train, np.ravel(y_train))\npreds = svm_model.predict(X_test)\nconfusion_matrix(preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#perm = PermutationImportance(svm_model, random_state=1).fit(X_test, y_test)\n#eli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"per_model = Perceptron()\nper_model.fit(X_train, np.ravel(y_train))\npreds = per_model.predict(X_test)\nconfusion_matrix(preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(per_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model = RidgeClassifier()\nridge_model.fit(X_train, np.ravel(y_train))\npreds = ridge_model.predict(X_test)\nconfusion_matrix(preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_model = SGDClassifier()\nsgd_model.fit(X_train, np.ravel(y_train))\npreds = sgd_model.predict(X_test)\nconfusion_matrix(preds, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvs = cross_val_score(sgd_model, X_test, np.ravel(y_test), cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (cvs.mean(), cvs.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we want to select a subset of the features in order to be able to make predictions with fewer inputs, we could try PCA, or we can look at methods of feature selection within SKLearn, which can either be performed based on a particular model, or in a model agnostinc way.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca_fitted = pca.fit_transform(X_train)\npca_fitted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(pca_fitted[:,0],pca_fitted[:,1], c=np.ravel(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier()\nselector = RFE(tree_model, n_features_to_select=10, step=1)\nselector = selector.fit(X_train, np.ravel(y_train))\nselector.support_\nprint('Tree model columns: ',X_train.columns[[i for i in selector.support_==True]])\nsgd_model = SGDClassifier()\nselector = RFE(sgd_model, n_features_to_select=10, step=1)\nselector = selector.fit(X_train, np.ravel(y_train))\nselector.support_\nprint('SGD model columns: ',X_train.columns[[i for i in selector.support_==True]])\nper_model = Perceptron()\nselector = RFE(per_model, n_features_to_select=10, step=1)\nselector = selector.fit(X_train, np.ravel(y_train))\nprint('Perceptron model columns: ',X_train.columns[[i for i in selector.support_==True]])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_X_train = X_train.drop(X_train.columns[[i for i in selector.support_==False]], axis=1)\nreduced_X_test = X_test.drop(X_train.columns[[i for i in selector.support_==False]], axis=1)\nreduced_X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_model = SGDClassifier()\nsgd_model.fit(reduced_X_train, np.ravel(y_train))\npreds = sgd_model.predict(reduced_X_test)\nprint(confusion_matrix(preds, y_test))\ncvs = cross_val_score(sgd_model, reduced_X_test, np.ravel(y_test), cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (cvs.mean(), cvs.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier()\ntree_model.fit(reduced_X_train, np.ravel(y_train))\npreds = tree_model.predict(reduced_X_test)\nprint(confusion_matrix(preds, y_test))\ncvs = cross_val_score(tree_model, reduced_X_test, np.ravel(y_test), cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (cvs.mean(), cvs.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"red_pca = PCA(n_components=3)\nred_pca_fitted = red_pca.fit_transform(reduced_X_train)\nplt.scatter(red_pca_fitted[:,0],red_pca_fitted[:,1],c=np.ravel(y_train))\nplt.scatter(red_pca_fitted[:,0],red_pca_fitted[:,2],c=np.ravel(y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we use the 'SelectKBest' function in order to try a pre-fitting approach to feature selection, by choosing the five best features based on a chi^2 statistical test. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = mushroom_data.drop('class',axis=1)\ny = mushroom_data['class'].values\nX = pd.get_dummies(X, drop_first=True)\ny = pd.get_dummies(y, drop_first=True)\nX_new = SelectKBest(chi2, k=5).fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.33, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeClassifier()\ntree_model.fit(X_train, np.ravel(y_train))\npreds = tree_model.predict(X_test)\nprint(confusion_matrix(preds, y_test))\ncvs = cross_val_score(tree_model, X_test, np.ravel(y_test), cv=10)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (cvs.mean(), cvs.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}