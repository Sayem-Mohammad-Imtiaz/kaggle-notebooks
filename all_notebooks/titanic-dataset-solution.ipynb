{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Shubham Paliwal","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf_titanic_test = pd.read_csv(\"../input/titanic-dataset-from-kaggle/test.csv\")\ndf_titanic_train = pd.read_csv(\"../input/titanic-dataset-from-kaggle/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting options to display all rows and columns\n\npd.options.display.max_columns=None\npd.options.display.max_rows=None\npd.options.display.width=None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train['Pclass'].value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here survived counts shows 0 - dead, and 1 = Alive, after the titanic incedent.\ndf_titanic_train['Survived'].value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train['Sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df_titanic_train['Survived'],df_titanic_train['Sex']).plot.bar(figsize = (10,6))\nprint(\"Total male and female in titanic\",df_titanic_train['Sex'].value_counts())\nprint(\"_________________________________\")\nprint(\"Total Survived male and female in titanic\")\nprint(pd.crosstab(df_titanic_train['Survived'],df_titanic_train['Sex']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train['Embarked'].value_counts().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df_titanic_train['Survived'],df_titanic_train['Embarked']).plot.bar(figsize = (10,6))\nprint(\"Total Port of Embarkation in titanic\")\nprint(df_titanic_train['Embarked'].value_counts())\nprint(\"_________________________________\")\nprint(\"Total Survived as per the port of Embarkation in titanic\")\nprint(pd.crosstab(df_titanic_train['Survived'],df_titanic_train['Embarked']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Port of Embarkation in titanic\")\nprint(df_titanic_train['Embarked'].value_counts())\nprint(\"_________________________________\")\nprint(\"Total Survived as per the port of Embarkation in titanic (In percent)\")\nprint(pd.crosstab(df_titanic_train['Survived'],df_titanic_train['Embarked'])/df_titanic_train['Embarked'].value_counts()*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train['Embarked'].fillna('S', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train['Embarked'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The Survived column is the target variable. If Suvival = 1 the passenger survived, otherwise he's dead. This is the variable we're going to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.boxplot(df_titanic_train['Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we'll fill the NA values of age from their respective Median. we can also use mean instead, but median is more robust for outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [df_titanic_train, df_titanic_test]\n\nfor dataset in data:\n    mean = df_titanic_train[\"Age\"].mean()\n    std = df_titanic_test[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n# compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = df_titanic_train[\"Age\"].astype(int)\ndf_titanic_train[\"Age\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train['Died'] = 1 - df_titanic_train['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', \n                                                                      figsize=(15,8),\n                                                          stacked=True, colors=['g', 'r']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It looks like male passengers are more likely to succumb or died."},{"metadata":{},"cell_type":"markdown","source":"As we saw in the barchart above and validate by the following:\n\nWomen survive more than men, as depicted by the larger female green in bar chart."},{"metadata":{},"cell_type":"markdown","source":"### Now lets have an look on cross tab between survived and fare. how fare ticket of each passenger and see how it could impact the survival."},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize=(20, 9))\nplt.hist([df_titanic_train[df_titanic_train['Survived'] == 1]['Fare'], df_titanic_train[df_titanic_train['Survived'] == 0]['Fare']], \n         stacked=True, color = ['g','b'],\n         bins = 50, label = ['Survived','Dead'])\nplt.xlabel('Fare')\nplt.ylabel('Number of passengers')\nplt.legend();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that clearly, on blue hist is high in between 0 to 100, Passengers with cheaper ticket fares are more likely to die. or rather say, passengers with more expensive tickets, and therefore a more important social status, seem to be rescued first and have priority."},{"metadata":{"trusted":true},"cell_type":"code","source":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20, 8))\nwomen = df_titanic_train[df_titanic_train['Sex']=='female']\nmen = df_titanic_train[df_titanic_train['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that men have a high probability of survival when they are between 18 and 40 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 12 and 42."},{"metadata":{},"cell_type":"markdown","source":"For men the probability of survival is very low between the age of 5 and 28, but that isn’t true for women."},{"metadata":{},"cell_type":"markdown","source":"### Now we have done with the data visualization. \n## Now we going to fill the missing values or its time to treat missing value or convert some important categorical variable into numeric or we can say dummy variable for our further statistical analysis or tests. and to build a effective model to predict our target variable effectively."},{"metadata":{},"cell_type":"markdown","source":"1. Cabin: A cabin number looks like ‘C123’ and the letter refers to the deck. Therefore we’re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [df_titanic_train, df_titanic_test]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can now drop the cabin feature\ndf_titanic_train = df_titanic_train.drop(['Cabin'], axis=1)\ndf_titanic_test = df_titanic_test.drop(['Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.we've converted the \"Cabin\" variable into Deck as per their letter initial. (we can see below)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. We've already fill missing value of age from their respective median because this is something robust to fill Na's of age with their median value."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [df_titanic_train, df_titanic_test]\nfor dataset in data:\n    dataset['Age'] = df_titanic_train[\"Age\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Embarked - we've only 2 missing value so we can fill that too on the basis of mode.\n### As we know that pessenger id is not important as of now for the statistical calculation, we can drop that."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train = df_titanic_train.drop(['PassengerId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also drop the Died column as we've created that for some viualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train = df_titanic_train.drop(['Died'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is one more important thing if we've notice the column of relatives shows that their are some family with some of their relatives and some of them are alone. so can club the sibsp & Parch and then will able to see how many of them can actually survived with their relatives and how many of them are alone with their family survived.¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [df_titanic_train, df_titanic_test]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\ndf_titanic_train['not_alone'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 537 families on the ship with more than 1 relative along with them. and 354 with their own family only."},{"metadata":{},"cell_type":"markdown","source":"### We can also convert gender (Male, Female) into numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [df_titanic_train, df_titanic_test]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we all know that ticket, pessenger id , and class of the pessenger shows the same insight towards the pessenger. so for that purpose we can drop ticket column. and also because ticket column has 691 unique value which will be very difficult to convert into meaningfull category."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train = df_titanic_train.drop(['Ticket'], axis=1)\ndf_titanic_test = df_titanic_test.drop(['Ticket'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embarked:\nConvert ‘Embarked’ feature into numeric.\nS = 0\nC = 1\nQ = 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"Ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [df_titanic_train, df_titanic_test]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(Ports)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we're going to create the categories:"},{"metadata":{},"cell_type":"markdown","source":"#### Now we need to convert the ‘age’ feature. we will create the new ‘AgeGroup” variable, by categorizing every age into a group. It is important to divide age group so that it will cover whole data in a effective way. so we'll divide in group accooding to our describe function of age, that 0 to 11, 11-18, 18-22,22-27,27-33,33-40, 40-65, 65 and above."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [df_titanic_train, df_titanic_test]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let see the distribution of our created category:¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we can see that there is biasness in data distribution of age group because we''ve fill the missing values with their respective median which is now results us vague numbers, and will definelty misslead our model. so now we're now filling the age missing values with random numbers.¶"},{"metadata":{},"cell_type":"markdown","source":"We've done with the changes in previous fillna in age field."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train = df_titanic_train.drop(['Name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have only fare variable with us which not completely categorized for our statistical calculation, going to divide this fare into reasoable bins so that it will cover at least 80% of data effectively.¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train[\"Fare\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [df_titanic_train, df_titanic_test]\n\nfor dataset in data: \n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"genders = {\"male\": 0, \"female\": 1}\ndata = [df_titanic_train, df_titanic_test]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Age times class\n# data = [df_titanic_train, df_titanic_train]\n#for dataset in data:\n#    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Fare per person \n# for dataset in data:\n#    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n #   dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\n# df_titanic_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FUNCTIONS TAB\ndef set_plot_sizes(sml, med, big):\n    plt.rc('font', size=sml)          # controls default text sizes\n    plt.rc('axes', titlesize=sml)     # fontsize of the axes title\n    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels\n    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels\n    plt.rc('legend', fontsize=sml)    # legend fontsize\n    plt.rc('figure', titlesize=big)  # fontsize of the figure title\ndef add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):\t\n    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n    the information from the date. This applies changes inplace.\n    Parameters:\n    -----------\n    df: A pandas data frame. df gain several new columns.\n    fldname: A string or list of strings that is the name of the date column you wish to expand.\n        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n    drop: If true then the original date column will be removed.\n    time: If true time features: Hour, Minute, Second will be added.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n    >>> df\n        A\n    0   2000-03-11\n    1   2000-03-12\n    2   2000-03-13\n    >>> add_datepart(df, 'A')\n    >>> df\n        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n    >>>df2 = pd.DataFrame({'start_date' : pd.to_datetime(['3/11/2000','3/13/2000','3/15/2000']),\n                            'end_date':pd.to_datetime(['3/17/2000','3/18/2000','4/1/2000'],infer_datetime_format=True)})\n    >>>df2\n        start_date\tend_date    \n    0\t2000-03-11\t2000-03-17\n    1\t2000-03-13\t2000-03-18\n    2\t2000-03-15\t2000-04-01\n    >>>add_datepart(df2,['start_date','end_date'])\n    >>>df2\n    \tstart_Year\tstart_Month\tstart_Week\tstart_Day\tstart_Dayofweek\tstart_Dayofyear\tstart_Is_month_end\tstart_Is_month_start\tstart_Is_quarter_end\tstart_Is_quarter_start\tstart_Is_year_end\tstart_Is_year_start\tstart_Elapsed\tend_Year\tend_Month\tend_Week\tend_Day\tend_Dayofweek\tend_Dayofyear\tend_Is_month_end\tend_Is_month_start\tend_Is_quarter_end\tend_Is_quarter_start\tend_Is_year_end\tend_Is_year_start\tend_Elapsed\n    0\t2000\t    3\t        10\t        11\t        5\t            71\t            False\t            False\t                False\t                False\t                False\t            False\t            952732800\t    2000\t    3\t        11\t        17\t    4\t            77\t            False\t            False\t            False\t            False\t                False\t        False\t            953251200\n    1\t2000\t    3\t        11\t        13\t        0\t            73\t            False\t            False\t                False\t                False               \tFalse           \tFalse           \t952905600     \t2000       \t3\t        11      \t18  \t5           \t78          \tFalse\t            False           \tFalse           \tFalse               \tFalse          \tFalse           \t953337600\n    2\t2000\t    3\t        11\t        15\t        2           \t75          \tFalse           \tFalse               \tFalse               \tFalse               \tFalse               False           \t953078400      \t2000    \t4          \t13      \t1   \t5           \t92          \tFalse           \tTrue            \tFalse           \tTrue                \tFalse          \tFalse           \t954547200\n    \"\"\"\n    if isinstance(fldnames,str): \n        fldnames = [fldnames]\n    for fldname in fldnames:\n        fld = df[fldname]\n        fld_dtype = fld.dtype\n        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n            fld_dtype = np.datetime64\n\n        if not np.issubdtype(fld_dtype, np.datetime64):\n            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n        targ_pre = re.sub('[Dd]ate$', '', fldname)\n        attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n        if time: attr = attr + ['Hour', 'Minute', 'Second']\n        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n        df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n        if drop: df.drop(fldname, axis=1, inplace=True)\n            \n            \n            \n            \ndef train_cats(df):\n    \"\"\"Change any columns of strings in a panda's dataframe to a column of\n    categorical values. This applies the changes inplace.\n    Parameters:\n    -----------\n    df: A pandas dataframe. Any columns of strings will be changed to\n        categorical values.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    note the type of col2 is string\n    >>> train_cats(df)\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    now the type of col2 is category\n    \"\"\"\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)\n\n\n\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n  \n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = pd.Categorical(df[y_fld]).codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef fix_missing(df, col, name, na_dict):\n    \n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\ndef numericalize(df, col, name, max_n_cat):\n\n    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):\n        df[name] = pd.Categorical(col).codes+1\n        \ndef get_sample(df,n):\n    \"\"\" Gets a random sample of n rows from df, without replacement.\n    Parameters:\n    -----------\n    df: A pandas data frame, that you wish to sample from.\n    n: The number of rows you wish to sample.\n    Returns:\n    --------\n    return value: A random sample of n rows of df.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})\n    >>> df\n       col1 col2\n    0     1    a\n    1     2    b\n    2     3    a\n    >>> get_sample(df, 2)\n       col1 col2\n    1     2    b\n    2     3    a\n    \"\"\"\n    idxs = sorted(np.random.permutation(len(df))[:n])\n    return df.iloc[idxs].copy()\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))\n\ndef set_rf_samples(n):\n    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n    n random rows.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n))\n    \ndef reset_rf_samples():\n    \"\"\" Undoes the changes produced by set_rf_samples.\n    \"\"\"\n    forest._generate_sample_indices = (lambda rs, n_samples:\n        forest.check_random_state(rs).randint(0, n_samples, n_samples))\n    \ndef parallel_trees(m, fn, n_jobs=8):\n        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From now onwards we'll start working on building Machine Learning Models which Includes:\n### 1. Stochastic Gradient Descent (SGD):\n### 2. Random Forest:\n### 3. Logistic Regression:\n### 4. K Nearest Neighbor:\n### 5. Gaussian Naive Bayes:\n### 6. Linear Support Vector Machine:\n### 7. Decision Tree:"},{"metadata":{},"cell_type":"markdown","source":"Now we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other. Later on, we will use cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_titanic_test = df_titanic_test.drop(['PassengerId'], axis=1)\ndf_titanic_test = df_titanic_test.drop(['Name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_titanic_train.drop(\"Survived\", axis=1)\nY_train = df_titanic_train[\"Survived\"]\nX_test = df_titanic_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stochastic Gradient Descent (SGD):¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD = linear_model.SGDClassifier(max_iter=5,tol=None)      # max_inter = Max number of iteration]\nSGD.fit(X_train, Y_train)\nY_Prediction = SGD.predict(X_test)\nSGD.score(X_train, Y_train)\n\nAccuracy_SGD = round(SGD.score(X_train, Y_train) * 100, 2)\nAccuracy_SGD","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_predict = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Nearest Neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN \nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive Bayes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussian = GaussianNB() \ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Support Vector Machine:****"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we can compare the accuracy of models and will see which is the model on the basis of their accuracy and then will proceed further."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression','Random Forest', 'Naive Bayes', \n                                  'Stochastic Gradient Decent','Decision Tree'],\n                        'Score': [acc_linear_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, \n                                  Accuracy_SGD, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now from the above results we can see that Random Forest and Decision tree would give us the best accuracy than else and prove our model more reliable whereas, on the other hand KNN & SVM also give decent accuracy. if we do Hyperperameter Tunning then there is high probability to get High accuracy than now. but as of now we're proceeding further and completing our Dataset Solution with Random Forest Model."},{"metadata":{},"cell_type":"markdown","source":"## Thank You"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}