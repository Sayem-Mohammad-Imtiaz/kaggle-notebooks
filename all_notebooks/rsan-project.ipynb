{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#IMPORTING REQUIRED LIBRARIS\n#SOME FUNCTIONS ARE IMPORTED LATER WHEN THEY WERE NEEDED\nimport numpy as np\nimport pandas as pd\nimport random\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nnp.random.seed(seed = 0)  #giving a seed to random functions\n\ndef decide_kopma(row):    #defined function to custom label encode our 'Kopma' column\n    if row == 'Sorunsuz':\n        return 0          #0 is no ticket\n    else:\n        return 1          #1 is ticket\nprint('Done!')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T18:28:42.996465Z","iopub.execute_input":"2021-06-29T18:28:42.996841Z","iopub.status.idle":"2021-06-29T18:28:43.005231Z","shell.execute_reply.started":"2021-06-29T18:28:42.996811Z","shell.execute_reply":"2021-06-29T18:28:43.00373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc_df = pd.read_csv(\"../input/connectionlossdata/newdata.csv\", header=0, index_col=0,low_memory=False)#reading our data\ndc_df.Kopma.fillna(value=\"Sorunsuz\", inplace=True) #filling 'no ticket' rows of 'kopma' column so they don't appear NA\n\ndc_df['POSSEHIR'].fillna(value='A', inplace=True)   # filling all empty rows of categorical columns\ndc_df['POSSEMT'].fillna(value='A', inplace=True)    # to label encode them later\ndc_df['SANTRALADI'].fillna(value='A', inplace=True) #\ndc_df['Atlak'].fillna(value='A', inplace=True)      #\n\ndc_df['Kopma'] = dc_df.apply(lambda x: decide_kopma(x['Kopma']),axis=1) #applying our custom encoding function on 'Kopma' column\n\ndc_df.iloc[:,5:] = dc_df.iloc[:,5:].apply(LabelEncoder().fit_transform) #Label encoding rest of our data\n\ndc_df.to_csv(\"./newdata.csv\")  #saving our modified .csv file since this process takes a long time\n                               #and this will save us a lot of time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dc_df = pd.read_csv(\"../input/connectionlossdata/newdata.csv\", header=0, index_col=0,low_memory=False) #reading our new saved .csv file\nprint(Counter(dc_df['Kopma'])) #checking how our 'Kopma' column is distributed\ny = dc_df.Kopma    #creating our output row\nX = dc_df.copy()   #creating our input matrix, copying since we will modify it thus preserving original\nX.drop(columns='Kopma', inplace=True) #dropping output column from input matrix \ncols = list(X.columns)    # getting a list of our column names\nselected_features = cols  # these cols are our selected features\nprint('Done!')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:28:57.20497Z","iopub.execute_input":"2021-06-29T18:28:57.205341Z","iopub.status.idle":"2021-06-29T18:29:04.353642Z","shell.execute_reply.started":"2021-06-29T18:28:57.205311Z","shell.execute_reply":"2021-06-29T18:29:04.352759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = list(X.columns)\nprint('All columns:')\nprint(cols)\npmax = 1                                                         # this block of code is the\nall_cols = cols.copy()                                           # feature selection algorithm\nwhile(len(cols)>0):                                              # how it exactly works is\n    p=[]                                                         # beyond this course and us\n    X_l = X[cols]                                                # thus we will not explain it\n    X_l = sm.add_constant(X_l)\n    model = sm.OLS(y,X_l).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)\n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\n\nprint('Selected columns:')\nselected_features = cols    #showing the columns that were chosen by our algorithm\nprint(selected_features)\nprint('Eliminated columns:')#showing the eliminated columns\nprint([x for x in list(X.columns) if x not in selected_features])\nX = X[selected_features] #our input matrix is now only our selected_features","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:29:08.612073Z","iopub.execute_input":"2021-06-29T18:29:08.612409Z","iopub.status.idle":"2021-06-29T18:29:24.199256Z","shell.execute_reply.started":"2021-06-29T18:29:08.612381Z","shell.execute_reply":"2021-06-29T18:29:24.198402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0) #splitting our data to so we have test material for later\nprint('Data split!')\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators = 200, random_state = 0) #creation of our classifier model\nclassifier.fit(X_train[:20000], y_train[:20000])      #fitting our data, we only used first 20K rows to save time\nprint('Model made!')\n\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom matplotlib import pyplot as plt\n\nfig, axs = plt.subplots(ncols = 2, figsize=(10,5))           #this block of code is formatting the plots\nfig.tight_layout(pad=5.0)                                    #thus won't be explained\nplt.rcParams.update({'font.size': 15})\naxs[0].tick_params(axis='both', which='major', labelsize=15)\naxs[1].tick_params(axis='both', which='major', labelsize=15)\ndisp = plot_confusion_matrix(classifier, X_test, y_test,     #creating confusion matrices to check how our model did\n                             cmap=plt.cm.Blues,\n                             ax=axs[0])\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             normalize='true',\n                             ax=axs[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:29:47.812224Z","iopub.execute_input":"2021-06-29T18:29:47.812772Z","iopub.status.idle":"2021-06-29T18:30:38.04222Z","shell.execute_reply.started":"2021-06-29T18:29:47.812742Z","shell.execute_reply":"2021-06-29T18:30:38.041146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.concat([X_train, y_train], axis=1)           #prepating a dataframe to undersample our data\n\ngood_connections = train_df[train_df.Kopma == 0].index     #getting the indeces of our good_connection rows\nsample_size = sum(train_df.Kopma == 1)  #getting how many bad_connections we have to gather only that many good_connection rows\n\nrandom_indices = np.random.choice(good_connections, sample_size, replace=False) #choosing from our good_connection indices randomly\nundersampled_df = train_df.loc[random_indices]  #creating a data frame from our randomly chosen good_connections\nbad_connections = train_df.loc[dc_df.Kopma == 1].index  #getting the indices of our bad_connections\nbad_connections_sample = train_df.loc[bad_connections]  #creating a data frame from our bad_connections\nundersampled_df = undersampled_df.append(bad_connections_sample) #combining good/bad_connections data frames\nprint(Counter(undersampled_df['Kopma']))  #looking if they are bad/good are equally represented\n\ny_train = undersampled_df.Kopma               #creaing new X,y_trains\nX_train = undersampled_df[selected_features]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:30:58.043847Z","iopub.execute_input":"2021-06-29T18:30:58.044247Z","iopub.status.idle":"2021-06-29T18:30:59.567366Z","shell.execute_reply.started":"2021-06-29T18:30:58.044216Z","shell.execute_reply":"2021-06-29T18:30:59.565912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = RandomForestClassifier(n_estimators = 200, random_state = 0) #creating new classifier\nclassifier.fit(X_train, y_train)  #fitting our undersampled data\nprint('Model made!')\n\nfig, axs = plt.subplots(ncols = 2, figsize=(10,5))               #graph formatting\nfig.tight_layout(pad=5.0)\nplt.rcParams.update({'font.size': 15})\naxs[0].tick_params(axis='both', which='major', labelsize=15)\naxs[1].tick_params(axis='both', which='major', labelsize=15)\ndisp = plot_confusion_matrix(classifier, X_test, y_test,      #creating confusion matrix to check our model\n                             cmap=plt.cm.Blues,\n                             ax=axs[0])\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             normalize='true',\n                             ax=axs[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:31:02.359266Z","iopub.execute_input":"2021-06-29T18:31:02.359679Z","iopub.status.idle":"2021-06-29T18:32:29.280842Z","shell.execute_reply.started":"2021-06-29T18:31:02.359643Z","shell.execute_reply":"2021-06-29T18:32:29.28006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier  #trying a new method of classifying\n\nclassifier = KNeighborsClassifier(n_neighbors=7, weights='distance') #we explained why we chose n=7 later\n                                                                     #weights='Distance' performed better then other choices\nclassifier.fit(X_train, y_train)  #fitting data\nprint('Model done!')\n\nfig, axs = plt.subplots(ncols = 2, figsize=(10,5))                #graph formatting\nfig.tight_layout(pad=5.0)\nplt.rcParams.update({'font.size': 15})\naxs[0].tick_params(axis='both', which='major', labelsize=15)\naxs[1].tick_params(axis='both', which='major', labelsize=15)\ndisp = plot_confusion_matrix(classifier, X_test, y_test,    #creating confusion matrix to check our model\n                             cmap=plt.cm.Blues,\n                             ax=axs[0])\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             normalize='true',\n                             ax=axs[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:32:38.197186Z","iopub.execute_input":"2021-06-29T18:32:38.197537Z","iopub.status.idle":"2021-06-29T18:33:11.633253Z","shell.execute_reply.started":"2021-06-29T18:32:38.197505Z","shell.execute_reply":"2021-06-29T18:33:11.632288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nprint('Data split!')\n\ntrain_df = pd.concat([X_train, y_train], axis=1)     #this block of code works exactly the same as\n                                                     #the prior undersampling, we just gathered\n                                                     #600 more good_connections than bad_connections\n                                                     #to represent the fact that good_connections\n                                                     #DO appear more often than bad_connections\ngood_connections = train_df[train_df.Kopma == 0].index\nsample_size = sum(train_df.Kopma == 1)\nrandom_indices = np.random.choice(good_connections, sample_size+600, replace=False) #only difference between the prior code is in this line\nundersampled_df = train_df.loc[random_indices]\nbad_connections = train_df.loc[dc_df.Kopma == 1].index\nbad_connections_sample = train_df.loc[bad_connections]\nundersampled_df = undersampled_df.append(bad_connections_sample)\nprint(Counter(undersampled_df['Kopma']))\ny_train = undersampled_df.Kopma\nX_train = undersampled_df[selected_features]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:43:39.733733Z","iopub.execute_input":"2021-06-29T18:43:39.734086Z","iopub.status.idle":"2021-06-29T18:43:44.399104Z","shell.execute_reply.started":"2021-06-29T18:43:39.734056Z","shell.execute_reply":"2021-06-29T18:43:44.397922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knc_classifiers = list()\n\nfor i in range(4,10): #creaing many KNC models with different n values\n    classifier = KNeighborsClassifier(n_neighbors= i, weights='distance')\n    classifier.fit(X_train, y_train)\n    knc_classifiers.append(classifier)\n    \n    accs = list()\n\nfor clf in knc_classifiers:  #testing every model we created\n    y_pred = clf.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred, normalize = \"true\")\n    acc = (cm[0,0] + cm[1,1]) / cm.sum() #accuracy is based on normalized confusion matrix\n                                         #as that's what is important for us\n    print(\"Knn classifier with n = {}, acc = {}\".format(clf.n_neighbors, acc))\n    accs.append(acc)\n\nfig, ax = plt.subplots(figsize = (8,8))  #creating a plot of accuracies\n\nknc_ns = [clf.n_neighbors for clf in knc_classifiers]  #graphing lines won't be explained\n\nax.plot(knc_ns, accs, linewidth = 3.0)\nax.set_ylim([0,1])\nax.set_title('K-Neighbors Algorithm')\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy\")\nax.grid()\nax.minorticks_on()\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:46:56.061636Z","iopub.execute_input":"2021-06-29T18:46:56.061907Z","iopub.status.idle":"2021-06-29T18:49:17.661995Z","shell.execute_reply.started":"2021-06-29T18:46:56.061883Z","shell.execute_reply":"2021-06-29T18:49:17.661389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = np.argmax(accs) #getting the index our highest precision n value\n\nclassifier = KNeighborsClassifier(n_neighbors=n+4, weights='distance') #our n value is index+4\nclassifier.fit(X_train, y_train) #fitting data\nprint('Model done!')\n\nfig, axs = plt.subplots(ncols = 2, figsize=(10,5))         #graphing function, same as prior ones\nfig.tight_layout(pad=5.0)\nplt.rcParams.update({'font.size': 15})\naxs[0].tick_params(axis='both', which='major', labelsize=15)\naxs[1].tick_params(axis='both', which='major', labelsize=15)\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             ax=axs[0])\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             normalize='true',\n                             ax=axs[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T18:49:31.686648Z","iopub.execute_input":"2021-06-29T18:49:31.686918Z","iopub.status.idle":"2021-06-29T18:50:01.789356Z","shell.execute_reply.started":"2021-06-29T18:49:31.686894Z","shell.execute_reply":"2021-06-29T18:50:01.788462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RANDOM FOREST OPTIMUM DEPTH","metadata":{}},{"cell_type":"code","source":"#this block does for Random Forest what the prior one did for K Neighbors,\n#basically we tried different depth values and chose the best one.\nrfc_classifiers = list()\n\nfor max_depth in range(10,30):\n    classifier = RandomForestClassifier(max_depth= max_depth, n_estimators = 200, random_state = 0)\n    classifier.fit(X_train, y_train)\n    rfc_classifiers.append(classifier)\n    \n    accs = list()\n\nfor clf in rfc_classifiers:\n    y_pred = clf.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred, normalize = \"true\")\n    acc = (cm[0,0] + cm[1,1]) / cm.sum()\n    print(\"RF classifier with n = {}, acc = {}\".format(clf.max_depth, acc))\n    accs.append(acc)\n\nfig, ax = plt.subplots(figsize = (8,8))\n\nrfc_ns = [clf.max_depth for clf in rfc_classifiers]\n\nax.plot(rfc_ns, accs, linewidth = 3.0)\nax.set_ylim([0,1])\nax.set_title('Random Forest Algorithm')\nax.set_xlabel(\"Depth\")\nax.set_ylabel(\"Accuracy\")\nax.grid()\nax.minorticks_on()\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:25:12.954729Z","iopub.execute_input":"2021-06-29T19:25:12.955104Z","iopub.status.idle":"2021-06-29T19:36:13.42013Z","shell.execute_reply.started":"2021-06-29T19:25:12.955071Z","shell.execute_reply":"2021-06-29T19:36:13.419123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_depth = np.argmax(accs) #getting the index our highest precision depth value\n\nclassifier = RandomForestClassifier(max_depth=best_depth+10,  n_estimators = 200, random_state = 0) #our depth value is index+10\nclassifier.fit(X_train, y_train) #fitting data\nprint('Model done!')\n\nfig, axs = plt.subplots(ncols = 2, figsize=(10,5))         #graphing function, same as prior ones\nfig.tight_layout(pad=5.0)\nplt.rcParams.update({'font.size': 15})\naxs[0].tick_params(axis='both', which='major', labelsize=15)\naxs[1].tick_params(axis='both', which='major', labelsize=15)\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             ax=axs[0])\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             normalize='true',\n                             ax=axs[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:36:42.96146Z","iopub.execute_input":"2021-06-29T19:36:42.96174Z","iopub.status.idle":"2021-06-29T19:38:04.648131Z","shell.execute_reply.started":"2021-06-29T19:36:42.961717Z","shell.execute_reply":"2021-06-29T19:38:04.646789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking how our data performs without feature selection\n\ndc_df = pd.read_csv(\"../input/connectionlossdata/newdata.csv\", header=0, index_col=0,low_memory=False) #reading our new saved .csv file\nprint(Counter(dc_df['Kopma'])) #checking how our 'Kopma' column is distributed\ny = dc_df.Kopma    #creating our output row\nX = dc_df.copy()   #creating our input matrix, copying since we will modify it thus preserving original\nX.drop(columns='Kopma', inplace=True) #dropping output column from input matrix \ncols = list(X.columns)    # getting a list of our column names\nselected_features = cols  # these cols are our selected features\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nprint('Data split!')\n\ntrain_df = pd.concat([X_train, y_train], axis=1)     #this block of code works exactly the same as\n                                                     #the prior undersampling, we just gathered\n                                                     #600 more good_connections than bad_connections\n                                                     #to represent the fact that good_connections\n                                                     #DO appear more often than bad_connections\ngood_connections = train_df[train_df.Kopma == 0].index\nsample_size = sum(train_df.Kopma == 1)\nrandom_indices = np.random.choice(good_connections, sample_size+2500, replace=False) #only difference between the prior code is in this line\nundersampled_df = train_df.loc[random_indices]\nbad_connections = train_df.loc[dc_df.Kopma == 1].index\nbad_connections_sample = train_df.loc[bad_connections]\nundersampled_df = undersampled_df.append(bad_connections_sample)\nprint(Counter(undersampled_df['Kopma']))\ny_train = undersampled_df.Kopma\nX_train = undersampled_df[selected_features]\n\nclassifier = KNeighborsClassifier(n_neighbors=4, weights='distance') #our n value is index+4\nclassifier.fit(X_train, y_train) #fitting data\nprint('Model done!')\n\nfig, axs = plt.subplots(ncols = 2, figsize=(10,5))         #graphing function, same as prior ones\nfig.tight_layout(pad=5.0)\nplt.rcParams.update({'font.size': 15})\naxs[0].tick_params(axis='both', which='major', labelsize=15)\naxs[1].tick_params(axis='both', which='major', labelsize=15)\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             ax=axs[0])\ndisp = plot_confusion_matrix(classifier, X_test, y_test,\n                             cmap=plt.cm.Blues,\n                             normalize='true',\n                             ax=axs[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T19:48:05.825548Z","iopub.execute_input":"2021-06-29T19:48:05.825887Z","iopub.status.idle":"2021-06-29T19:48:43.626Z","shell.execute_reply.started":"2021-06-29T19:48:05.825855Z","shell.execute_reply":"2021-06-29T19:48:43.625022Z"},"trusted":true},"execution_count":null,"outputs":[]}]}