{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first attempt at a kernel and I'd be very grateful for any feedback.\n\nI have no data science training so this is just a little bit of experimentation on my part. I'd love to get any constructive feedback, or even pointers toward resources that might help me to improve.\n\nFor my very first kernel I decided to have a look at how we encode the non-numerical data can affect the results of our analysis."},{"metadata":{"collapsed":true},"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\nimport itertools as it\n%matplotlib inline","cell_type":"code","outputs":[],"execution_count":1},{"metadata":{},"cell_type":"markdown","source":"First use Pandas to read in the csv file and view the description to check that this looks right."},{"metadata":{"scrolled":true},"source":"mushrooms_full = pd.read_csv('../input/mushrooms.csv')\nmushrooms_full.describe()","cell_type":"code","outputs":[],"execution_count":2},{"metadata":{},"cell_type":"markdown","source":"The class tells us whether the mushroom is poisonous or ebible. All of the other variables tell us things about the properties. We want to see if we can classify the mushrooms accurately using their properties. First we will divide the full dataset into two to give us a train and test set."},{"metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(mushrooms_full, test_size=0.2)","cell_type":"code","outputs":[],"execution_count":3},{"metadata":{},"cell_type":"markdown","source":"Now we can use machine learning on the train data to see if we can successfully predict which mushrooms in the test data will be safe to eat.\nFirst we will plot a heat map to see if there is any stong correlation between any of the features. Before this we will use a simple replace to convert all of the letters to numbers. We will also try reversing the allocation of the numbers to see if this skews the results in the correlation by \"weighting\" the variables."},{"metadata":{},"source":"# we add in a step to capture any values we have that are not just standard letters\n# using the union of the alphabet and the values in the table\nallin = set(np.unique(train.values.ravel()))\nletters = 'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'\nletters = set(letters)\nadditional_letters = allin - letters\nletters = list(letters) + list(additional_letters)\n\ntrain1 = train.copy()\nfor i in range(len(letters)):\n    train1 = train1.replace({letters[i]: i},regex=False)\n\ntrain1_corr = train1.corr()\nprint(train1_corr['class'])\nsns.heatmap(train1_corr, square=True)","cell_type":"code","outputs":[],"execution_count":4},{"metadata":{},"source":"letters.reverse()\ntrain2 = train.copy()\nfor i in range(26):\n   train2=train2.replace({letters[i]: i}, regex=False)\n\ntrain2_corr = train2.corr()\nprint(train2_corr['class'])\nsns.heatmap(train2_corr, square=True)","cell_type":"code","outputs":[],"execution_count":5},{"metadata":{},"cell_type":"markdown","source":"From the heat map we can see straight away that Veil-Type doesn't seem to have any correlation. If we go back to the data table we can see that in fact there is only one entry in this column so it will not be useful to us in trying to predict whether a mushroom is poisonous. Hence we can ignore this column."},{"metadata":{},"source":"train['veil-type'].unique()","cell_type":"code","outputs":[],"execution_count":6},{"metadata":{"collapsed":true},"source":"train = train.drop('veil-type',1)","cell_type":"code","outputs":[],"execution_count":7},{"metadata":{},"cell_type":"markdown","source":"We can also see from the heat map that there is strong correlation between veil-colour and gill-attachment, so if we want to reduce the number of variables we could drop one of these as the other is strongly correlated.\n\nWe can now look at the variables that are most strongly correlated with class:"},{"metadata":{},"source":"train1_corr = train1_corr.abs()\ntrain2_corr = train2_corr.abs()\ntrain1_corr.sort_values('class', ascending=0).head(5)","cell_type":"code","outputs":[],"execution_count":8},{"metadata":{},"cell_type":"markdown","source":"What if we numbered each column independently, will this make a difference to the correlation?"},{"metadata":{"collapsed":true},"source":"train3 = train.copy()\nnum_cols = len(train.columns)\nencode_dictionary = []\nfor i in range(num_cols):\n    train3_unique = train3.iloc[:,i].unique()\n    encode_dictionary.append({'Variable Order' : list(train3_unique) })\n    for j in range(len(train3_unique)):\n        train3.iloc[:,i] = train3.iloc[:,i].replace({train3_unique[j]: j})","cell_type":"code","outputs":[],"execution_count":9},{"metadata":{},"source":"train3.describe()","cell_type":"code","outputs":[],"execution_count":10},{"metadata":{},"source":"train3_corr = train3.corr()\ntrain3_corr = train3_corr.abs()\nprint(train3_corr.sort_values('class', ascending=0)['class'])\nsns.heatmap(train3_corr, square=True)","cell_type":"code","outputs":[],"execution_count":11},{"metadata":{},"cell_type":"markdown","source":"If we compare the three ways we have numbered the data by simply comparing the sum of the correlation in the class column we can see that the way we assign numerical data to the categorical data affects its correlation. So can we improve the correlation by change the order that we assign numbers to the data?"},{"metadata":{},"source":"print(train1_corr['class'].sum()) #a=0, b=1 ...\nprint(train2_corr['class'].sum()) #a=25, b=24, ...\nprint(train3_corr['class'].sum()) #each unique value assigned to 0,1,2,3, ...","cell_type":"code","outputs":[],"execution_count":12},{"metadata":{},"cell_type":"markdown","source":"We can use permutations to find every possible ordering, for example if we look at the cap-surface column which has 4 unique values we can have 24 different permutations. We can then look at this to see if we can see any difference in the correlation when we vary the order that we number the categories."},{"metadata":{"collapsed":true},"source":"train_unique = train['cap-surface'].unique()\nmush_perm = list(it.permutations(train_unique))","cell_type":"code","outputs":[],"execution_count":13},{"metadata":{"collapsed":true},"source":"train5 = train[['class','cap-surface']].copy()\ntrain5.iloc[:,0] = train5.iloc[:,0].replace({'p': 1})\ntrain5.iloc[:,0] = train5.iloc[:,0].replace({'e': 0})","cell_type":"code","outputs":[],"execution_count":14},{"metadata":{"collapsed":true},"source":"corr_compare = []\nfor i in range(24):\n    perm = list(mush_perm[i])\n    for j in range(4):\n        train5.iloc[:,1] = train5.iloc[:,1].replace({perm[j]: j})\n    corr_compare.append({'List': perm, 'Corr': np.absolute(train5.corr().iloc[1,0])})\n    train5 = train[['class','cap-surface']].copy()\n    train5.iloc[:,0] = train5.iloc[:,0].replace({'p': 1})\n    train5.iloc[:,0] = train5.iloc[:,0].replace({'e': 0})","cell_type":"code","outputs":[],"execution_count":15},{"metadata":{},"source":"pd.DataFrame(corr_compare).sort_values('Corr',ascending=0).head()","cell_type":"code","outputs":[],"execution_count":16},{"metadata":{},"cell_type":"markdown","source":"So this shows that the way in which we order the categories when we assign them numerical values can have an effect on the correlation that's achieved. Also we can see that reversing the order of the assignment does not affect the correlation, which is why we can see the information in pairs, which means that we only need to consider the ordering where we have more than two unique values in a column. So now we will do this for the full table."},{"metadata":{},"source":"train6 = train.copy()\nencoding_index = []\nencoding_index.append({'Order': ['e','p'] , 'Corr to Class': 1 })\nnum_cols = len(train6.columns)\ntrain6.iloc[:,0] = train6.iloc[:,0].replace({'p': 1})\ntrain6.iloc[:,0] = train6.iloc[:,0].replace({'e': 0})\nfor i in range(1,num_cols):\n    train6_unique = train6.iloc[:,i].unique()\n    if len(train6_unique) < 3 or len(train6_unique) > 6:\n        for j in range(len(train6_unique)):\n            train6.iloc[:,i] = train6.iloc[:,i].replace({train6_unique[j] : j})\n        encoding_index.append({'Order': list(train6_unique) , 'Corr to Class': np.absolute(train6.iloc[:,[0,i]].corr().iloc[1,0]) })\n    else:\n            corr_max = 0\n            mush_perm = list(it.permutations(train6_unique))\n            for s in range(int(np.floor(len(mush_perm)/2))):\n                a = list(mush_perm[s])\n                a.reverse()\n                a = tuple(a)\n                mush_perm.remove(a)\n            for k in range(len(mush_perm)):\n                train_temp = train6.iloc[:,[0,i]].copy()\n                perm = list(mush_perm[k])\n                for j in range(len(train6_unique)):\n                    train_temp.iloc[:,1] = train_temp.iloc[:,1].replace({perm[j]: j})\n                if corr_max < np.absolute(train_temp.corr().iloc[1,0]):\n                    corr_max = np.absolute(train_temp.corr().iloc[1,0])\n                    best_perm = perm\n            encoding_index.append({'Order': list(best_perm) , 'Corr to Class': corr_max })\n            for q in range(len(train6_unique)):\n                train6.iloc[:,i] = train6.iloc[:,i].replace({best_perm[q] : q})   \ntrain6.corr().abs()['class'].sum()","cell_type":"code","outputs":[],"execution_count":17},{"metadata":{},"cell_type":"markdown","source":"So we can see that we have increased the total of the correlation between the variables and the outcome, i.e. if the mushroom is poisonous or not.\n\nNow we will take two versions of the data, train3 - this was where we numbered the data simply by assigning to each unique value and train6 where we ordered the assignments to maximise the correlation to class.\n\nWe will try using a simple nearest neighbours analysis to see how both perform with the training data:"},{"metadata":{},"source":"sorted(train1['class'].unique())","cell_type":"code","outputs":[],"execution_count":18},{"metadata":{"collapsed":true},"source":"#scale so that the values in each column are between 0 and 1\nt1_dividers = []\nt1_dividers.append(train1.iloc[:,0].max())\nfor i in range(1,len(train1.columns)):\n    t1_dividers.append(train1.iloc[:,i].max())\n    train1.iloc[:,i] = train1.iloc[:,i]/train1.iloc[:,i].max()\ntrain1 = train1.drop('veil-type',1) #to match 3 and 6 we remove the veil type column\ntrain1['class'] = train1['class'].replace({min(train1['class'].unique()) : 0}, regex = False) #to make the class column zeroes and ones\ntrain1['class'] = train1['class'].replace({max(train1['class'].unique()) : 1}, regex = False) #to make the class column zeroes and ones\nt3_dividers = []\nfor i in range(len(train3.columns)):\n    t3_dividers.append(train3.iloc[:,i].max())\n    train3.iloc[:,i] = train3.iloc[:,i]/train3.iloc[:,i].max()\n\nt6_dividers = []    \nfor i in range(len(train6.columns)):\n    t6_dividers.append(train6.iloc[:,i].max())\n    train6.iloc[:,i] = train6.iloc[:,i]/train6.iloc[:,i].max()","cell_type":"code","outputs":[],"execution_count":19},{"metadata":{},"source":"from sklearn import neighbors\nx1 = train1.drop('class',1).values\ny1 = train1['class'].values\nknn1 = neighbors.KNeighborsClassifier(n_neighbors=9)\n#Train the classifier\nknn1.fit(x1,y1)\n#Compute the prediction according to the model\nknn1.score(x1,y1)","cell_type":"code","outputs":[],"execution_count":20},{"metadata":{},"source":"x3 = train3.drop('class',1).values\ny3 = train3['class'].values\nknn3 = neighbors.KNeighborsClassifier(n_neighbors=9)\n#Train the classifier\nknn3.fit(x3,y3)\n#Compute the prediction according to the model\nknn3.score(x3,y3)","cell_type":"code","outputs":[],"execution_count":21},{"metadata":{},"source":"x6 = train6.drop('class',1).values\ny6 = train6['class'].values\nknn6 = neighbors.KNeighborsClassifier(n_neighbors=9)\n#Train the classifier\nknn6.fit(x6,y6)\n#Compute the prediction according to the model\nknn6.score(x6,y6)","cell_type":"code","outputs":[],"execution_count":22},{"metadata":{},"cell_type":"markdown","source":"Both performed very well on the training data, now how about when we use the test data. We will need to encode it in the same way as the training data, i.e. if a=1 in the training data then a=1 in the test data."},{"metadata":{"collapsed":true},"source":"encode_dictionary = pd.DataFrame(encode_dictionary) #this contains the lists with the order we assigned the variables to train3\nencoding_index = pd.DataFrame(encoding_index)   #this contains the lists from when we used the optimum correlation\nletters.reverse()","cell_type":"code","outputs":[],"execution_count":23},{"metadata":{},"source":"test1 = test.copy()\ntest1 = test1.drop('veil-type',1)\nnum_cols = len(test1.columns)\nfor i in range(len(letters)):\n    test1 = test1.replace({letters[i]: i},regex=False)\nnp.unique(test1.values.ravel())  #to check we had no letters in test that we didn't have in train","cell_type":"code","outputs":[],"execution_count":24},{"metadata":{},"source":"test3 = test.copy()\ntest3 = test3.drop('veil-type',1)\nnum_cols = len(test3.columns)\nfor i in range(num_cols):\n    test3_unique = encode_dictionary.iloc[i,0]\n    test3_unique2 = test3.iloc[:,i].unique()\n    for j in range(len(test3_unique)):\n        if test3_unique[j] in test3_unique2:\n            test3.iloc[:,i] = test3.iloc[:,i].replace({test3_unique[j]: j})\nnp.unique(test3.values.ravel())  #to check we had no letters in test that we didn't have in train","cell_type":"code","outputs":[],"execution_count":25},{"metadata":{},"source":"test6 = test.copy()\ntest6 = test6.drop('veil-type',1)\nnum_cols = len(test6.columns)\nfor i in range(num_cols):\n    test6_unique = encoding_index.iloc[i,1]\n    test6_unique2 = test6.iloc[:,i].unique()\n    for j in range(len(test6_unique)):\n        if test6_unique[j] in test6_unique2:\n            test6.iloc[:,i] = test6.iloc[:,i].replace({test6_unique[j]: j})\nnp.unique(test6.values.ravel())  #to check we had no letters in test that we didn't have in train","cell_type":"code","outputs":[],"execution_count":26},{"metadata":{},"source":"#scale the test values in the same way as train\nfor i in range(len(test1.columns)):\n    test1.iloc[:,i] = test1.iloc[:,i]/t1_dividers[i]\ntest1['class'] = test1['class'].replace({min(test1['class'].unique()) : 0}, regex = False)\ntest1['class'] = test1['class'].replace({max(test1['class'].unique()) : 1}, regex = False)#to make the class column zeroes and ones\nfor i in range(len(test3.columns)):\n    test3.iloc[:,i] = test3.iloc[:,i]/t3_dividers[i]\nfor i in range(len(test6.columns)):\n    test6.iloc[:,i] = test6.iloc[:,i]/t6_dividers[i]\nx1test = test1.drop('class',1).values\ny1test = test1['class'].values\nx3test = test3.drop('class',1).values\ny3test = test3['class'].values  \nx6test = test6.drop('class',1).values\ny6test = test6['class'].values","cell_type":"code","outputs":[],"execution_count":27},{"metadata":{},"source":"from sklearn import metrics\ny1hat=knn1.predict(x1test)\nprint (\"TESTING STATS - Alphabetic:\")\nprint (\"classification accuracy:\", metrics.accuracy_score(y1hat.astype(int), y1test.astype(int)))\nprint (\"confusion matrix: \\n\"+ str(metrics.confusion_matrix(y1hat,y1test)))\ny3hat=knn3.predict(x3test)\nprint (\"\\n TESTING STATS - Unique:\")\nprint (\"classification accuracy:\", metrics.accuracy_score(y3hat, y3test))\nprint (\"confusion matrix: \\n\"+ str(metrics.confusion_matrix(y3hat,y3test)))\ny6hat=knn6.predict(x6test)\nprint (\"\\n TESTING STATS - Correlation:\")\nprint (\"classification accuracy:\", metrics.accuracy_score(y6hat, y6test))\nprint (\"confusion matrix: \\n\"+ str(metrics.confusion_matrix(y6hat,y6test)))","cell_type":"code","outputs":[],"execution_count":28},{"metadata":{},"cell_type":"markdown","source":"So this shows that the way we number the data could make a small difference to the accuracy of the model."}],"metadata":{"language_info":{"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4}