{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stroke Prediction EDA and Prediction with Tree-based methods\nWe explore, analyse and process the given dataset followed by model design, training and tuning using Tree-based machine learning algorithms.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial Exploration of Dataset","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 5110 samples and 12 dimensions. Among these 12, all except `id` and `stroke` are feature columns (10 in total) and `stroke` is our label column (response). The column `id` can be removed and stored as `Series` object just in case we require it later.","metadata":{}},{"cell_type":"code","source":"patient_id = df.pop('id')\ndf.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Among our predictors, we have 3 numerical features; `age, avg_glucose_level, bmi`. The remaining features along with the label are categorical and should be converted to `category` datatype, as done below.","metadata":{}},{"cell_type":"code","source":"cat_cols = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status', 'stroke']\nnum_cols = ['age', 'avg_glucose_level', 'bmi']\n\ndf[cat_cols] = df[cat_cols].astype('category')\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in df.columns:\n    if df[column].dtype.name == 'category':\n        print(column,\": \")\n        print(df[column].value_counts())\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observations:\n* `age` feature looks pretty evenly distributed but `avg_glucose_level` and `bmi` shows signs of right tail heavy distributions due to large differences between their third quartile and maximum value.\n* The minimum age value of 0.08 and maximum bmi of 97 are suspicious.\n* There is only one sample with `gender=Other`. This may result in our final model not performing adequately for similar samples when used later with unseen data. It might create further issues depending on which set (training or test) is gets split into. For our case, we will keep it but in real world applications, such an event requires discussion with the data source or domain expert.\n* Our binary categorical medical feature columns have more `0`s than `1`s","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis\nFirst, we plot a Pairs Grid as well as the correlation matrix with the numerical columns along with the label.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(x_vars=num_cols, y_vars=num_cols, hue='stroke', data=df, palette='bright')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[num_cols].corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we plot individual histograms for all the numeric feature columns.","metadata":{}},{"cell_type":"code","source":"for column in df.columns:\n    if df[column].dtype.name is not 'category':\n        plt.hist(df[column], bins=30, edgecolor='black')\n        plt.title(column)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As stated in the observations of the previous section, we can clearly see that `bmi` and `avg_glucose_level` are tail heavy distribtuions while `age` approximates to a Gassian distribution. If we were using parametric classification algorithms like logarithmic regression, we would have to normalize these using Z-score normalization or any other similar method. But, we have chosen to use Tree based method which are non-parametric and hence are uneffected.  \nFor the categorical features, we will graph categorical plots.","metadata":{}},{"cell_type":"code","source":"for column in df.columns:\n    if (df[column].dtype.name is 'category') and (column != 'stroke'):\n        sns.catplot(y=column, hue=\"stroke\", kind=\"count\", edgecolor=\"black\", data=df)\n        plt.title(column)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is important to interpret each graph keeping in mind the number of samples per category. For example, we can say married people have a higher chance of stroke than unmarried people looking at the `ever_married` plot but this ignores the fact that we have more samples of married people, 3353 vs just 1757 `ever_married=No` samples.","metadata":{}},{"cell_type":"markdown","source":"## Handling Erraneuos Data\nFirst, we look at all samples that have an age less than 1","metadata":{}},{"cell_type":"code","source":"df[ df['age'] < 1 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen from the `work_type` column, we can safely state that none of these error. We could round them off to zero but it might not be necessary in our case as we plan on using Tree-based methods.  \nNow, we look at all samples with a bmi greater than 50. We could look below a lower threshold of 40 as that is considered the maximum usually observed.","metadata":{}},{"cell_type":"code","source":"df[ df['bmi'] > 60 ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A look at the above samples and their features does not provide a clearer understanding of whether these are erraneous entries or extreme cases. The only thing common between all of them is that they neither had heart disease or stroke. For our case, we will let the observations be but in the field, this might once again involve discussions and clarification from the data source or domain expert.","metadata":{}},{"cell_type":"markdown","source":"## Handling Missing Data","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 201 missing values in the `bmi` column. If the missing value was in a categorical column, we could ignore it as it would be handled in the encoding of the dataset later.  \nLet us have a look at the features of the corresponding samples.","metadata":{}},{"cell_type":"code","source":"bmifilt = df['bmi'].isna() \nfor column in df.columns:\n    if df[column].dtype.name is 'category':\n        print(column,': ')\n        print( df.loc[bmifilt, column].value_counts())\n        print()\n    else:\n        print(column,': ')\n        print( df.loc[bmifilt, column].describe())\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compared to our complete dataset, these samples have a higher average glucose level. The remaining features look usual with nothing particularly standing out. To fill the missing values, we will use the average value of `bmi` column but according to their stroke value.","metadata":{}},{"cell_type":"code","source":"strokefilt = (df['stroke'] == 1)\n\ns0_mean = df.loc[~strokefilt,'bmi'].mean()\ns1_mean = df.loc[strokefilt,'bmi'].mean()\n\ndf.loc[(bmifilt) & (~strokefilt), 'bmi'] = s0_mean\ndf.loc[(bmifilt) & (strokefilt), 'bmi'] = s1_mean\n\ndf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the Dataset\nWe need to split the dataset into training and test set before we start modelling. We will split the dataset into a ratio of 7:3. Validation set is not required because we will be using cross-validation for hyperparameter tuning.  \nBefore we split, we need to one-hot encode our categorical columns, except the binary ones. The columns with binary string data will be label encoded. This has to be done due to the limitation in Scikit-Learn where categorical variables cannot be passed despite working directly on categorical data being a main advantage of Tree-based methods.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle_gender = LabelEncoder()\nle_marr = LabelEncoder()\nle_residence = LabelEncoder()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod_df = df.copy()\n\nmod_df['gender'] = le_gender.fit_transform(df['gender'])\nmod_df['ever_married'] = le_marr.fit_transform(df['ever_married'])\nmod_df['Residence_type'] = le_residence.fit_transform(df['Residence_type'])\n\n#display the labels to act as reference later\nprint(le_gender.classes_)\nprint(le_marr.classes_)\nprint(le_residence.classes_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also label encoded the `gender` column despite it being on-binary because `Other` can be conceptually ignored as there is only one sample with it.  \nNow, we one-hot encode using the `.get_dummies()` function in the *Pandas* library.","metadata":{}},{"cell_type":"code","source":"encdf = pd.get_dummies( df[['work_type', 'smoking_status']] )\nmod_df.drop(['work_type', 'smoking_status'], axis=1, inplace=True)\nmod_df = mod_df.join(encdf)\nmod_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we split, let us have one look at the correlation matrix with the fully processed dataset","metadata":{}},{"cell_type":"code","source":"sns.heatmap(mod_df.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`age` and `ever_married` look to have a relation but nothing significant that might require feature engineering.  \nNow, we split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = mod_df.pop('stroke')\nX = mod_df\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nprint(X_train.shape, '\\t', X_test.shape)\nprint(y_train.shape, '\\t', y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Design and Training\nWe will use 3 machine learning algorithms; **Decision Tree** and **Random Forests**. The models with default parameters are run first and compared based on training accuracy, F-score and AUC (Area under ROC Curve) to get an initial understanding. Then, we tune both models using cross-validiation to improve our results. Nowhere during this step should the test dataset be used.","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndtree = DecisionTreeClassifier(random_state=0)\ndtree.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\nprint(accuracy_score(y_train, dtree.predict(X_train)))\nprint(f1_score(y_train, dtree.predict(X_train)))\nprint(roc_auc_score(y_train, dtree.predict(X_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Surprisingly, we get 100% for all of our three metrics. This might seem ideal but might be a case of our model overfitting the training data. If that is the case, our Tree will perform horribly on unseen samples making it useless for any real world applications. To go a little further, let us look at the cross-validation score for our model as well. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ndt_cvscore = cross_val_score(dtree, X_train, y_train, cv=10)\nprint(dt_cvscore)\nprint(dt_cvscore.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we get a more clear metric eual to 91% that can be used to compare our models.  \nWe can now try to visualise our tree.","metadata":{}},{"cell_type":"code","source":"from sklearn import tree\n\nplt.figure(figsize = (20,14))\ntree.plot_tree(dtree, rounded=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to large number of predictors, the visualization does not provide fine details. A better approach would be to save a tree as an image file using the instructions given [here](https://chrisalbon.com/machine_learning/trees_and_forests/visualize_a_decision_tree/) and analyze it using a standard GUI image viewer. We recommend this step because interpretabiliy of the model as provided my the visualized tree is one of the main advantages of using a Decision Tree.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(accuracy_score(y_train,rf.predict(X_train)))\nprint(f1_score(y_train, rf.predict(X_train)))\nprint(roc_auc_score(y_train, rf.predict(X_train)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_cvscore = cross_val_score(rf, X_train, y_train, cv=10)\nprint(rf_cvscore)\nprint(rf_cvscore.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we see that the original three metrics are still 100% but we get a much better cross-validation score when compared to a Decision Tree.","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning\nWe will be using `GridSearchCV` for this.\n### Decision Tree tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndtree = DecisionTreeClassifier(random_state=0)\ndtparam = {\"max_depth\":[3,5,10,None], 'max_features':['auto',8,16,None]}\ndtcv = GridSearchCV(dtree, param_grid=dtparam, cv=10, scoring=['f1','accuracy'], refit='f1')\ndtcv.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dtcv.best_estimator_)\nprint(dtcv.best_score_)\nprint(dtcv.cv_results_['mean_test_accuracy'][dtcv.best_index_])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our accuracy remained the same. To get a better understanding, we plot a heatmap with all parameters to see how our accuracy improves.","metadata":{}},{"cell_type":"code","source":"x1 = ['3','5','10','None']\nx2 = ['auto','8','16','None']\ny = dtcv.cv_results_['mean_test_accuracy']\n\ntempdf = pd.DataFrame(data=np.reshape(y,(4,4)), index=x1, columns=x2)\nsns.heatmap(tempdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Limiting the depth of our tree to either 3 or even 5 provides the most significant impact to our model. Further minute improvement is by limiting the number of features selected to 16.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest tuning","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state=0)\nrfparam = {\"n_estimators\":[5,10,50,100,200], \"max_depth\":[3,5,10,None]}\nrfcv = GridSearchCV(rf, param_grid=rfparam, cv=10,  scoring=['f1', 'accuracy'], refit='f1')\nrfcv.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rfcv.best_estimator_)\nprint(rfcv.best_score_)\nprint(rfcv.cv_results_['mean_test_accuracy'][rfcv.best_index_])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x1 = ['3','5','10','None']\nx2 = ['5','10','50','100','200']\ny = rfcv.cv_results_['mean_test_accuracy']\n\ntempdf = pd.DataFrame(data=np.reshape(y,(4,5)), index=x1, columns=x2)\nsns.heatmap(tempdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once again, limiting our depth provides the majority of the improvement in our accuracy.","metadata":{}},{"cell_type":"markdown","source":"## Test Accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve, plot_confusion_matrix\n\ndef final_report(model):\n    print(accuracy_score(y_test, model.predict(X_test)))\n    plot_confusion_matrix(model, X_test, y_test)\n    plot_roc_curve(model, X_test, y_test) \n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_report(dtcv.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_report(rfcv.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Observations:\nEven though we get high accuracy values, we get both a low AUC and low F-score with our models. The high accuracy is due to the imbalance in the number of samples with `stroke=0` vs `stroke=1`. This is why the F-score is a better metric for comparison as it somewhat negates this imabalance due to its mathematical formula.  ","metadata":{}},{"cell_type":"markdown","source":"#### Please do provide suggestions for areas of improvement, both technical and general, in this notebook as this is my first try on Kaggle. It will be greatly appreciated.\n","metadata":{}}]}