{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 id=\"heading\">\n\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/deb009/predict-customer-churn/notebook#heading\">¶</a>\n</h1>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../input/churn-risk-rate-hackerearth-ml/\"\ntrain = pd.read_csv(f\"{PATH}train.csv\",index_col='customer_id')\ntest = pd.read_csv(f\"{PATH}test.csv\",index_col='customer_id')\nsubmission= pd.read_csv(f\"{PATH}sample_submission.csv\",index_col='customer_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NULL VALUES**\n\nIt looks like there are null values in some of the columns in test and train set.\nWe will now try to the find the amount of null values in each dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train data:\n\n36992 rows, 25 columns\n\nTypes of columns:\n    1. numerical : 6\n    2. categorical : 19 \n    \nMissing values in 3 columns, they are:\n    1. region_category\n    2. preferred_offer_types\n    3. points_in_wallet\n    \n# Test data\n\n19919 rows, 24 columns\n\nTypes of columns:\n    1. numerical : 5\n    2. categorical : 19 \n \nMissing values in 3 columns, they are:\n    1. region_category\n    2. preferred_offer_types\n    3. points_in_wallet\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we can see that Name and security_no are tottaly unique in test and train data.\nWe can remove them as of now."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop([\"security_no\",\"Name\"], axis=1)\ntest = test.drop([\"security_no\",\"Name\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding test and train set together\n\ndf =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#profile = ProfileReport(df)\n#profile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target Variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['churn_risk_score'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('churn_risk_score', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According the compeition, the churn score is between 1 to 5 i.e, good to worse but here we can see that -1.\nAfter a little searching over the internet, I found that the negative churn is actually good.\n\n**What is negative churn?**\n\nIt is achieved when the total additional revenue generated from existing customers is greater than the revenue lost from cancellations and downgrades. When your recurring revenue grows without the addition of new customers, you’re achieving positive net revenue retention.\n\nSimply, net negative churn is when current customers are spending so much additional money (services, upgrades, and add-ons) that your churn is offset by it. \n\nfor more information : [https://www.profitwell.com/recur/all/negative-churn](http://)"},{"metadata":{},"cell_type":"markdown","source":"There is a huge difference between some of the classes in the target(churn_risk_score) column.(-1,1 and 2 are quite less as compared 3,4,5).\nWe will handle it when creating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the joining_date type datetime format\ntrain['joining_date'] = train['joining_date'].astype('datetime64[ns]')\ntest['joining_date'] = test['joining_date'].astype('datetime64[ns]')\ntest['last_visit_time'] = test['last_visit_time'].astype('datetime64[ns]')\ntrain['last_visit_time'] = train['last_visit_time'].astype('datetime64[ns]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = train.columns\n\n## Getting all the data that are not of \"object\" type. \nnumerical_columns = train[feature_cols].select_dtypes(include=['int64','float64']).columns\ncategorical_columns = train[feature_cols].select_dtypes(exclude=['int64','float64','datetime64[ns]']).columns\n\nprint(len(numerical_columns), len(categorical_columns))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorical variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ntrain_0_df = train.loc[train['churn_risk_score'] == -1]\ntrain_1_df = train.loc[train['churn_risk_score'] == 1]\ntrain_2_df = train.loc[train['churn_risk_score'] == 2]\ntrain_3_df = train.loc[train['churn_risk_score'] == 3]\ntrain_4_df = train.loc[train['churn_risk_score'] == 4]\ntrain_5_df = train.loc[train['churn_risk_score'] == 5]\n\nnum_rows, num_cols = 4,5\nfig = make_subplots(rows=num_rows, cols=num_cols)\n\nfor index, column in enumerate(df[categorical_columns].columns):\n    i,j = ((index // num_cols)+1, (index % num_cols)+1)\n    data = train_0_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: -1',\n    ), row=i, col=j)\n\n    data = train_1_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 1'\n    ), row=i, col=j)\n    data = train_2_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 2'\n    ), row=i, col=j)\n    data = train_3_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 3'\n    ), row=i, col=j)\n    data = train_4_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 4'\n    ), row=i, col=j)\n    data = train_5_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 5'\n    ), row=i, col=j)\n    \n    fig.update_xaxes(title=column, row=i, col=j)\n    fig.update_layout(barmode='stack')\n    \nfig.update_layout(\n    autosize=False,\n    width= 1600,\n    height=1600,\n    showlegend=False,\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # avg_frequency_login_days(Represents the no. of times a customer has logged in to the website)"},{"metadata":{},"cell_type":"markdown","source":"It looks like avg_freq_login_days(Represents the no. of times a customer has logged in to the website) column is a numeric type column \nbut it is showing up in the cat columns because we weren't able to convert it as it had a value = ERROR (count-3500 values).\n\nERROR value means the website was unable to register the avg_freq_login_days due to internal problem may be software glitch etc.\nIt will be NaNs inplace of the ERROR values.\nSo, we will replace it with NaN value "},{"metadata":{"trusted":true},"cell_type":"code","source":"#argument errors='coerce' converts invalid values  into NaN and the data type is float.\n#We will deal with the feature engineering of this column in numeric section\ntrain['avg_frequency_login_days'] = pd.to_numeric(train['avg_frequency_login_days'], errors='coerce')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}