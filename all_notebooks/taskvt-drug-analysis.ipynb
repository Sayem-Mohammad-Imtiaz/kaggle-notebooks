{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Table of Content\n1. [Preamble](#preamble)\n2. [Executive Summary](#exec_summary)\n3. [Executable Code](#code)\n4. [Citations](#citations)\n5. [Appendix: Consortium Organization](#org_and_process)\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Preamble <a id=\"preamble\"></a>\nThis is a notebook created by a collaborative effort of CoronaWhy.org. \n- Visit our [website](https://www.coronawhy.org) to learn more.\n- Read our [story](https://medium.com/@arturkiulian/im-an-ai-researcher-and-here-s-how-i-fight-corona-1e0aa8f3e714).\n- Visit our [main notebook](https://www.kaggle.com/arturkiulian/coronawhy-org-global-collaboration-join-slack) for historical context on how this community started."},{"metadata":{},"cell_type":"markdown","source":"# 2. Executive Summary <a id=\"exec_summary\"></a>"},{"metadata":{},"cell_type":"markdown","source":"This notebook is targetted at the Task: \"What do we know about vaccines and therapeutics?\", and in particular the sub-task concerning \"Effectiveness of drugs being developed and tried to treat COVID-19 patients\".  \n\nTo address this question we sought to identify all papers in the dataset which indicated that a named drug had been used or considered as a treatment / therapy for COVID-19.  We consider every named drug that is prescribable in the United States, as catalogued in the publicly available RxNorm database.  \n\nSentences are extracted from each paper indicating what is stated about this drug as a treatment.  These statements are classified by relevance to COVID-19: whether mentioned in the same paragraph or paper as the drug, for example.  The statements can also be sorted by drug name, section of paper where it is mentioned (title/abstract/body).  \n\nEach sentence is presented alongside a link to the paper, with title, abstract and year of publication displayed.  A graphical user interface is provided in order to navigate the list of statements about each drug.  The GUI is presented inline below, or can be accessed in fullscreen here:\nhttps://app.powerbi.com/view?r=eyJrIjoiYWUyZjJhZjQtMDIwNi00OWQ0LTliNDctZmNiM2Q5YTkzNzJhIiwidCI6ImRjMWYwNGY1LWMxZTUtNDQyOS1hODEyLTU3OTNiZTQ1YmY5ZCIsImMiOjEwfQ%3D%3D\n\nThe code in later sections of this notebook generates the data that feeds the above visualization. It uses only the raw CORD-19 data release and some cited publically available data tables, and has a run-time of a few hours.  The above GUI can be used without re-running this notebook to view the results.\n\nWe hope that this tool may prove to be a valuable resource for anyone who is trying to navigate the wide and rapidly evolving literature on treatments for COVID-19, collecting into one place all statements that have been made about a large array of drug candidates that could be of relevance.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import IFrame\nIFrame('https://app.powerbi.com/view?r=eyJrIjoiYWUyZjJhZjQtMDIwNi00OWQ0LTliNDctZmNiM2Q5YTkzNzJhIiwidCI6ImRjMWYwNGY1LWMxZTUtNDQyOS1hODEyLTU3OTNiZTQ1YmY5ZCIsImMiOjEwfQ%3D%3D', width=800, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Executable Code <a id=\"code\"></a>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In the following sections, the code that produces the visualizible output will be\npresented. Running the full notebook will produce the output files used above."},{"metadata":{},"cell_type":"markdown","source":"### Imports and Installs"},{"metadata":{"trusted":true},"cell_type":"code","source":"UseSciSpacy=True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pylab\nimport pandas as pd\nimport json\nimport os\nimport re\nimport spacy\nimport numpy as np  \nimport pandas as pd \nimport spacy\n\nif(UseSciSpacy):\n   #Instal SciSpacy\n    !pip install -U scispacy\n    !pip install -U https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\n    import scispacy\n    nlp=spacy.load(\"/opt/conda/lib/python3.6/site-packages/en_core_sci_lg/en_core_sci_lg-0.2.3/\", disable=[\"tagger\"])\nelse:\n    !python -m spacy download en_core_web_lg\n    nlp = spacy.load('en_core_web_lg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Drug Search Term List Construction \n\nThis section uses publically source-able data to construct list of drug terms to search.\n\nThe starting point is the list of all prescribable drugs from the RxNorm database:\n   https://download.nlm.nih.gov/rxnorm/RxNorm_full_prescribe_03022020.zip\n\nFrom this list we then remove the following un-useful names:\n1. names of chemical elements, from this periodic table:\n  https://gist.github.com/GoodmanSciences/c2dd862cd38f21b0ad36b8f96b4bf1ee\n2. names of animals, from this list: \n  https://gist.github.com/atduskgreg/3cf8ef48cb0d29cf151bedad81553a54   \n3. names of fruits and vegetables, from this list: \n  https://alphabetizer.flap.tv/lists/list-of-fruits-and-vegetables.php\n\nWe also restrict to single-word entries, of more than 5 characters.\nFor convenience these publically available files have been collected in the kaggle\n directory rxnorm_inputdata."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This flags determines whether to re-run the text matching code.\n# Warning, if it takes along time! Intermediate data \n# from this step is provided with the notebook.\nRunMatching=False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Open the RxNorm file and extract drug names list\nfile=open(\"/kaggle/input/rxnorm-inputdata/RxNorm_full_prescribe_03022020/rrf/RXNCONSO.RRF\",'r').readlines()\nnames=[]\nfor line in file:\n    names.append(line.split(\"|\")[14].lower())\nnames=np.unique(names)\n\n# restrict to single-word drug names with >5 characters\nsinglenames=[]\nfor name in names:\n    if ((not \" \" in name) and (len(name)>5)):\n        singlenames.append(name)\n\n# Load up elements\nElements=pd.read_csv(\"/kaggle/input/rxnorm-inputdata/Elements.csv\")\nElementNames = Elements.Element.str.lower()\n\n# Load up animals, \nAnimalsRaw=open(\"/kaggle/input/rxnorm-inputdata/animals.txt\",'r').readlines()\nAnimalNames=[]\nfor a in AnimalsRaw:\n    if not \" \" in a:\n        AnimalNames.append(a[:-1].lower())\n\n# Load up fruit and veg\nFruitVegRaw=open(\"/kaggle/input/rxnorm-inputdata/FruitAndVeg.txt\",'r').readlines()\nFruitVegNames=[]\nfor a in FruitVegRaw:\n    if not \" \" in a:\n        FruitVegNames.append(nlp(a[:-1].lower())[0].lemma_)\n\n# Apply the filter        \nfilterednames=[]\nfor name in singlenames:\n    if (not name  in AnimalNames) and (not name  in FruitVegNames)  and (not name  in ElementNames.values):\n        filterednames.append(name)\n        \nnp.savetxt(\"/kaggle/working/DrugNames.txt\",filterednames,fmt=\"%s\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text matching in the CORD-19 dataset\n\nAll content extraction in this notebook is based on string matching. We use two methods:\n1.  Lemmatized matching - useful for terms that may be used in different contexts like \"treatment / treats / treat\", etc. However, its slower, especially for full-text search\n2.  Direct matching - match string directly when its a proper name, e.g. drug names or coronavirus synonyms\n\nThese are implemented in the two functions below, and then applied to drug names, virus names, treatment words and study types \n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# These are helper functions for extracting word matches from the text\n# both lemmatized and non-lemmatized versions are possible.\n\nPaths=[\"/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset/pdf_json/\",\"/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset/pdf_json/\",\"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/\",\"/kaggle/input/CORD-19-research-challenge/custom_license/custom_license/pdf_json/\"]\n\n\n# These functions determine what blocks are pulled from the paper for matching\ndef TitleBlocks(paper):\n    return([{'text':paper['metadata']['title']}])\n\ndef AbstractBlocks(paper):\n    return(paper['abstract'])\n\ndef BodyBlocks(paper):\n    return(paper['body_text'])\n\n\n\n# This function finds matching lemmas and notes positions of\n# occurence in the relevant json block. This function uses\n# the lemmatized text.\ndef PullMentionsLemmatized(Paths, BlockSelector,SecName, Words):\n\n    Positions=[]\n    FoundWords=[]\n    Section=[]\n    BlockID=[]\n    BlockText=[]\n    PaperID=[]\n    \n    tokenized_words=[]\n    for w in Words:\n        tokenized_words.append(nlp(w.lower())[0].lemma_)\n    for Path in Paths:\n        print(Path)\n\n        Files=os.listdir(Path)\n        for p in Files:\n\n            readfile=open(Path+p,'r')\n            paper=json.load(readfile)\n            Blocks=BlockSelector(paper)\n\n            for b in range(0,len(Blocks)):\n                text=nlp(Blocks[b]['text'].lower())\n\n                for t in text:\n                    for w in tokenized_words:\n                        if(w == t.lemma_):\n                            Section.append(SecName)\n                            FoundWords.append(w)\n                            Positions.append(t.idx)\n                            BlockText.append(Blocks[b]['text'])\n                            BlockID.append(b)\n                            PaperID.append(p[:-5])\n    return {'sha':PaperID,'blockid':BlockID,'word':FoundWords,'sec':Section,'pos':Positions,'block':BlockText}\n\n\n# This function finds matching words and notes positions of\n# occurence in the relevant json block. This function uses\n# direct text matching (not lemmatized)\ndef PullMentionsDirect(Paths, BlockSelector,SecName, Words):\n    Positions=[]\n    FoundWords=[]\n    Section=[]\n    BlockID=[]\n    BlockText=[]\n    PaperID=[]\n    for wi in range(0,len(Words)):\n        Words[wi]=Words[wi].lower()\n    for Path in Paths:\n        print(Path)\n\n        Files=os.listdir(Path)\n        for p in Files:\n\n            readfile=open(Path+p,'r')\n            paper=json.load(readfile)\n            Blocks=BlockSelector(paper)\n\n            for b in range(0,len(Blocks)):\n                text=Blocks[b]['text'].lower()\n                for w in Words:\n                    if(w in text):\n                        pos=text.find(w)\n                   \n                        #check we're not in the middle of another word\n                        if(text[pos-1]==\" \" and ( (pos+len(w))>=len(text) or not text[pos+len(w)].isalpha())):\n                            Section.append(SecName)\n                            FoundWords.append(w)\n                            Positions.append(text.find(w))\n                            BlockText.append(Blocks[b]['text'])\n                            BlockID.append(b)\n                            PaperID.append(p[:-5])\n    return {'sha':PaperID,'blockid':BlockID,'word':FoundWords,'sec':Section,'pos':Positions,'block':BlockText}\n\n\n# Run to get treatment words\ndef ExtractToCSV(Words,Filename,Lemmatized=True, RunTitle=True, RunAbstract=True, RunBody=False):\n\n    if(Lemmatized):\n        PullMentions = PullMentionsLemmatized\n    else:\n        PullMentions = PullMentionsDirect\n    \n    DataDicts=[]\n    if(RunTitle): \n        DataDicts.append(PullMentions(Paths, TitleBlocks,    \"title\",    Words))\n    if(RunAbstract):\n        DataDicts.append(PullMentions(Paths, AbstractBlocks, \"abstract\", Words))\n    if(RunBody):\n        DataDicts.append(PullMentions(Paths, BodyBlocks,     \"body\",     Words))\n\n    SummedDictionary=DataDicts[0]\n    for k in DataDicts[0].keys():\n        for d in DataDicts:\n            SummedDictionary[k]=SummedDictionary[k]+d[k]\n\n    dat=pd.DataFrame(SummedDictionary)\n    dat.to_csv(Filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Switch this off to run over only title and abstract -\n#  go faster for debugging, but less complete info.\nIncludeBodyText=True\n\n# These lines of code will run the extraction\n\nif(RunMatching):\n    Words=[\"COVID-19\", \"Coronavirus\", \"Corona\", \"2019-nCoV\", \"SARS-CoV\",]\n    ExtractToCSV(Words, \"/kaggle/working/TitleAbstractBodyMatches_virusnames.csv\", Lemmatized=False,RunBody=IncludeBodyText)\n\n    Words=np.loadtxt(\"DrugNames.txt\",dtype='str')\n    ExtractToCSV(Words, \"/kaggle/working/TitleAbstractBodyMatches_drugs.csv\", Lemmatized=False,RunBody=IncludeBodyText)\n\n    Words=['treat','treatment' 'alleviate', 'manage', 'suppress','suppression', 'prescribe','therapy','cure','remedy', 'therapeutic','administer']\n    ExtractToCSV(Words, \"/kaggle/working/TitleAbstractBodyMatches_therapies.csv\", Lemmatized=True,RunBody=IncludeBodyText)\n\n    Words=[\"vitro\", \"vivo\", \"in-vitro\", \"in-vivo\", \"mouse\",\"mice\",\"clinial\",\"human\",\"computational\",\"vertical\",\"horizontal\",\"theoretical\",\"simulation\"]\n    ExtractToCSV(Words, \"/kaggle/working/TitleAbstractBodyMatches_exptypes.csv\", Lemmatized=True,RunBody=IncludeBodyText)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overlap Extraction\n\nHaving found the search terms, we now seek co-occurences of pairs and triplets in the same sentence or paragraph.  Counts of co-occurences are plotted on 2D matrix plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"dat_therapies=pd.read_csv(\"/kaggle/input/textmatchesvt/TitleAbstractBodyMatches_therapies.csv\")\ndat_drugs= pd.read_csv(\"/kaggle/input/textmatchesvt/TitleAbstractBodyMatches_drugs.csv\")\ndat_viruses= pd.read_csv(\"/kaggle/input/textmatchesvt/TitleAbstractBodyMatches_virusnames.csv\")\ndat_exps= pd.read_csv(\"/kaggle/input/textmatchesvt/TitleAbstractBodyMatches_exptypes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop unnecessary columns\ndat_drugs=dat_drugs.drop('Unnamed: 0',axis=1).set_index('block')\ndat_therapies=dat_therapies.drop('Unnamed: 0',axis=1).set_index('block')\ndat_viruses=dat_viruses.drop('Unnamed: 0',axis=1).set_index('block')\ndat_exps=dat_exps.drop('Unnamed: 0',axis=1).set_index('block')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll use this function later to see if two words are in the same sentence\n#  within the block\n\ndef SameSentenceCheck(block,pos1,pos2):\n    if(pos1<pos2):\n        Interstring=block[int(pos1):int(pos2)]\n    else:\n        Interstring=block[int(pos2):int(pos1)]\n    SentenceEnders=[\".\",\";\",\"?\",\"!\"]\n    for s in SentenceEnders:\n        if s in Interstring:\n            return 0\n    return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function makes the 2D quilt plot for showing co-occurences at block\n#   or sentence level of various classes of search terms\n#\ndef Make2DPlot(dat_joined, factor1, factor2, single_sentence_plots=False):\n    if(single_sentence_plots):\n        grouped = dat_joined[dat_joined.same_sentence==True].groupby(['word_'+factor1,'word_'+factor2])\n    else:\n        grouped = dat_joined.groupby(['word_'+factor1,'word_'+factor2])\n\n    Values    = grouped.count().values[:,0]\n\n    Index=grouped.count().index\n    Index1=[]\n    Index2=[]\n    for i in Index:\n        Index1.append(i[0])\n        Index2.append(i[1])\n\n    Uniq1=np.unique(Index1)\n    Uniq2=np.unique(Index2)\n\n    for i in range(0,len(Index1)):\n        Index1[i]=np.where(Index1[i]==Uniq1)[0][0]\n        Index2[i]=np.where(Index2[i]==Uniq2)[0][0]\n\n    pylab.figure(figsize=(5,5),dpi=200)\n    hist=pylab.hist2d(Index1,Index2, (range(0,len(Uniq1)+1),range(0,len(Uniq2)+1)), weights=Values,cmap='Blues')\n    pylab.xticks(np.arange(0,len(Uniq1))+0.5, Uniq1,rotation=90)\n    pylab.yticks(np.arange(0,len(Uniq2))+0.5, Uniq2)\n    pylab.clim(0,np.max(hist[0])*1.5)\n    for i in range(0,len(Uniq1)):\n        for j in range(0,len(Uniq2)):\n            pylab.text(i+0.5,j+0.5,int(hist[0][i][j]),ha='center',va='center')\n\n    pylab.colorbar()\n    if(single_sentence_plots):\n        pylab.title(factor1+\" and \" +factor2+\" in One Sentence\")\n        pylab.tight_layout()\n        pylab.savefig(\"Overlap\"+factor1+\"_Vs_\"+factor2+\"_2D_sentence.png\",bbox_inches='tight',dpi=200)\n    else:\n        pylab.title(factor1+\" and \" +factor2+\" in One Block\")\n        pylab.tight_layout()\n        pylab.savefig(\"Overlap\"+factor1+\"_Vs_\"+factor2+\"_2D_block.png\",bbox_inches='tight',dpi=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Virus / Therapy coincidences **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prune and join, and extract overlap counts\ndat_joined_vt=dat_therapies.join(dat_viruses, rsuffix='_virus',lsuffix=\"_therapy\")\ndat_joined_vt=dat_joined_vt[dat_joined_vt.notna().word_therapy & dat_joined_vt.notna().word_virus]\n\n\n#Make single sentence index\ndat_joined_vt=dat_joined_vt.drop([\"sha_therapy\",\"blockid_therapy\",\"sec_therapy\"],axis=1).reset_index().rename(columns={\"sha_virus\":\"sha\",\"blockid_virus\":\"blockid\",\"sec_virus\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_vt.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_vt.block[i],dat_joined_vt.pos_virus[i],dat_joined_vt.pos_therapy[i]))\ndat_joined_vt.insert(len(dat_joined_vt.columns),'same_sentence',SingleSentence)\ndat_joined_vt.to_csv(\"Overlaps_Virus_Therapy.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Make2DPlot(dat_joined_vt,\"virus\",\"therapy\")\nMake2DPlot(dat_joined_vt,\"virus\",\"therapy\",single_sentence_plots=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Virus / Drug coincidences"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prune and join, and extract overlap counts\ndat_joined_vd=dat_drugs.join(dat_viruses, rsuffix='_virus',lsuffix=\"_drug\")\ndat_joined_vd=dat_joined_vd[dat_joined_vd.notna().word_drug & dat_joined_vd.notna().word_virus]\n\ndat_joined_vd=dat_joined_vd.drop([\"sha_drug\",\"blockid_drug\",\"sec_drug\"],axis=1).reset_index().rename(columns={\"sha_virus\":\"sha\",\"blockid_virus\":\"blockid\",\"sec_virus\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_vd.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_vd.block[i],dat_joined_vd.pos_drug[i],dat_joined_vd.pos_drug[i]))\ndat_joined_vd.insert(len(dat_joined_vd.columns),'same_sentence',SingleSentence)\ndat_joined_vd.to_csv(\"Overlaps_Virus_Drug.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drugsubset=[\"naproxen\",\"clarithromycin\",\"chloroquine\",\"kaletra\",\"Favipiravir\",\"Avigan\",'hydroxychloroquine','baricitinib']\nMake2DPlot(dat_joined_vd[dat_joined_vd.word_drug.isin(drugsubset)],\"virus\",\"drug\")\nMake2DPlot(dat_joined_vd[dat_joined_vd.word_drug.isin(drugsubset)],\"virus\",\"drug\",single_sentence_plots=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drug / Therapy Coincidences\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prune and join, and extract overlap counts\ndat_joined_dt=dat_drugs.join(dat_therapies, rsuffix='_therapy',lsuffix=\"_drug\")\ndat_joined_dt=dat_joined_dt[dat_joined_dt.notna().word_drug & dat_joined_dt.notna().word_therapy]\n\ndat_joined_dt=dat_joined_dt.drop([\"sha_drug\",\"blockid_drug\",\"sec_drug\"],axis=1).reset_index().rename(columns={\"sha_therapy\":\"sha\",\"blockid_therapy\":\"blockid\",\"sec_therapy\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_dt.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_dt.block[i],dat_joined_dt.pos_drug[i],dat_joined_dt.pos_therapy[i]))\ndat_joined_dt.insert(len(dat_joined_dt.columns),'same_sentence',SingleSentence)\ndat_joined_dt.to_csv(\"Overlaps_Drug_Therapy.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Make2DPlot(dat_joined_dt[dat_joined_dt.word_drug.isin(drugsubset)],\"drug\",\"therapy\")\nMake2DPlot(dat_joined_dt[dat_joined_dt.word_drug.isin(drugsubset)],\"drug\",\"therapy\",single_sentence_plots=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Study type / drug coincidences"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prune and join, and extract overlap counts\ndat_joined_de=dat_drugs.join(dat_exps, rsuffix='_exp',lsuffix=\"_drug\")\ndat_joined_de=dat_joined_de[dat_joined_de.notna().word_drug & dat_joined_de.notna().word_exp]\n\ndat_joined_de=dat_joined_de.drop([\"sha_drug\",\"blockid_drug\",\"sec_drug\"],axis=1).reset_index().rename(columns={\"sha_exp\":\"sha\",\"blockid_exp\":\"blockid\",\"sec_exp\":\"sec\"})\nSingleSentence=[]\nfor i in dat_joined_de.index:\n    SingleSentence.append(SameSentenceCheck(dat_joined_de.block[i],dat_joined_de.pos_drug[i],dat_joined_de.pos_exp[i]))\ndat_joined_de.insert(len(dat_joined_de.columns),'same_sentence',SingleSentence)\ndat_joined_de.to_csv(\"Overlaps_Drug_Experiment.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Make2DPlot(dat_joined_de[dat_joined_de.word_drug.isin(drugsubset)],\"drug\",\"exp\")\nMake2DPlot(dat_joined_de[dat_joined_de.word_drug.isin(drugsubset)],\"drug\",\"exp\",single_sentence_plots=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tripartite Coincidences"},{"metadata":{"trusted":true},"cell_type":"code","source":"dat_joined_vtd=dat_therapies.join(dat_viruses, rsuffix='_virus',lsuffix=\"_therapy\").join(dat_drugs)\ndat_joined_vtd=dat_joined_vtd[dat_joined_vtd.notna().word_therapy & dat_joined_vtd.notna().word_virus & dat_joined_vtd.notna().word]\ngrouped_vtd=dat_joined_vtd.groupby(['word_therapy','word_virus','word'])\ngrouped_vtd.count().sha_therapy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat_joined_vtd=dat_joined_vtd.reset_index().drop(['sha_therapy','blockid_therapy','sec_therapy','sha_virus','blockid_virus','sec_virus'],axis=1).rename(columns={'word':'word_drug','pos':'pos_drug'}).set_index('sha')\ndat_joined_vtd=dat_joined_vtd[[\"block\",\"sec\",\"blockid\",\"word_therapy\",\"pos_therapy\",\"word_virus\", \"pos_virus\",\"word_drug\",\"pos_drug\"]]\ndat_joined_vtd.to_csv(\"Overlaps_Drug_Therapy_Virus.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparation for Visualization\nThis section implements some data manipulation and cleaning to prepare for visualization."},{"metadata":{"trusted":true},"cell_type":"code","source":"OverlapsVirus=pd.read_csv(\"./Overlaps_Virus_Drug.csv\")\nOverlapsTherapy=pd.read_csv(\"./Overlaps_Drug_Therapy.csv\")\n\nPapersWithVirusDrugOverlap=OverlapsVirus.sha.unique()\nPapersWithVirusMention=dat_viruses.sha.unique()\nOverlapsTherapy=OverlapsTherapy[OverlapsTherapy.same_sentence==1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This two helper function does its best to extract the year from the \n#  inconsistently formatted metadata\n\ndef ConvertDateToYear(datestring):\n    import dateutil.parser as parser\n\n    if(pd.notna(datestring)):\n        try:\n            date=parser.parse(str(datestring),fuzzy=True)\n            return date.year\n        except ValueError:\n            return 0\n    else:\n        return 0\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the elements we need out of the paper metadata\nmeta=pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\")\nyears=meta.publish_time.apply(ConvertDateToYear)\nmeta.insert(len(meta.columns),'year',years)\nmeta_to_use=meta.set_index('sha')[['doi','title','year','abstract']]\n\n# And mix it in\nOverlapsTherapy=OverlapsTherapy.set_index('sha').join(meta_to_use).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the relevant sentences where matches were found\nSentenceEnders=\"\\. |; \\! \\? \"\nExtractedSentences=[]\nfor i in OverlapsTherapy.index:\n    sentences = re.split(SentenceEnders,OverlapsTherapy.block.loc[i])\n    RunningCount=0\n    ExtractedSentences.append(np.NaN)\n\n    for s in range(0,len(sentences)):\n        RunningCount=RunningCount+len(sentences[s])\n        if(OverlapsTherapy.pos_drug.loc[i]<RunningCount):\n            ExtractedSentences[-1]=sentences[s]\n            break\n            \nOverlapsTherapy.insert(len(OverlapsTherapy.columns),'sentence',ExtractedSentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for coincidences in block and paper\nCoronaInPaper=OverlapsTherapy.sha.isin(PapersWithVirusMention)\nCoronaInBlock=OverlapsTherapy.sha.isin(PapersWithVirusDrugOverlap)\nOverlapsTherapy.insert(len(OverlapsTherapy.columns),'corona_paper',CoronaInPaper)\nOverlapsTherapy.insert(len(OverlapsTherapy.columns),'corona_block',CoronaInBlock)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tidy and trim\nOverlapsTherapy=OverlapsTherapy.rename(columns={'word_drug':'drug','sec':'section','block':'paragraph'}).drop(['blockid','pos_drug','pos_therapy','Unnamed: 0','same_sentence','word_therapy'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove a few obvious fakes\nFakes=['injection','glucose','perform','ethanol','methanol','paraffin','soybean','horseradish','ginger','mouthwash','oregano','formaldehyde','alcohol']\nOverlapsTherapy=OverlapsTherapy[ OverlapsTherapy.drug.isin(Fakes)==False]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is the final file that is used as input to the visualization stage\nOverlapsTherapy.to_csv(\"DrugVisData.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Negation Detection\nA trigger term list *(citation1)* is used to identify negation for each extracted drug-treatment co-occurence.\n\n#### Pros:\n- Quick and easy approach\n\n#### Cons:\n- Sentence hierarchical structure is not accounted for. Contrastive conjunctions (but, however, etc.) and long sentences with each part talking about different drugs could cause false positives."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = OverlapsTherapy\noutput_data = data\nout_dir = './'\n\n\n# negated term list (use the human annotated version)\nneg_list = pd.read_csv('/kaggle/input/neg-list-complete/neg_list_complete.txt', sep='\\t', header=0)\nneg = neg_list['ITEM'].values\nneg_term = [' ' + item + ' ' for item in neg]\nneg_term.extend(item + ' ' for item in neg)\n\n\nfor i in range(0,len(data)):\n    if pd.isnull(data.loc[i,'sentence']):\n        output_data.loc[i,'Is_Negated'] = 0\n    else:\n        # tag negated or affirmed based on string matching --- negation term list\n        # add one space to prevent loss of 'no ', 'not ', ... etc.\n        if any(substring in ' ' + data.loc[i,'sentence'].lower() for substring in neg_term):\n            output_data.loc[i,'Is_Negated'] = 1\n        else:\n            output_data.loc[i,'Is_Negated'] = 0\n\n# save results in a output file\noutput_data.to_csv('DrugVisData_Negated_Output.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the most negated and most asserted drugs based on number of mentions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"negated_drug_mentions = output_data.loc[output_data.Is_Negated==1,'drug']\\\n                                    .groupby(output_data['drug'])\\\n                                    .value_counts()\\\n                                    .droplevel(level=0)\nprint('Top 20 most negated drugs:\\n')\nprint(negated_drug_mentions.nlargest(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"asserted_drug_mentions = output_data.loc[output_data.Is_Negated==0,'drug']\\\n                                    .groupby(output_data['drug'])\\\n                                    .value_counts()\\\n                                    .droplevel(level=0)\nprint('Top 20 most asserted drugs:\\n')\nprint(asserted_drug_mentions.nlargest(20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's look at the distribution of percentage negated mentions for drugs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"drug_mentions = output_data.groupby([output_data['drug'],output_data.Is_Negated])\\\n                            .size().to_frame(name = 'size').reset_index()\\\n                            .pivot(index='drug',columns='Is_Negated',values='size').fillna(0).reset_index()\n\ndrug_mentions['Percentage Negations'] = (drug_mentions[1]*100)/(drug_mentions[0]+drug_mentions[1])\n\ndrug_mentions.hist(column='Percentage Negations')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drugs with 100% negation\ndrug_mentions.nlargest(n=1,columns='Percentage Negations',keep='all').plot.bar('drug',[1,0],figsize=(15,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drugs with 50% negation\ndrug_mentions.loc[drug_mentions['Percentage Negations']==50,:].plot.bar('drug',[1,0],figsize=(15,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Citations <a id=\"citations\"></a>\n1. @article{weng2020clinical,\n  title={Clinical Text Summarization with Syntax-Based Negation and Semantic Concept Identification},\n  author={Weng, Wei-Hung and Chung, Yu-An and Schrasing Tong},\n  journal={arXiv preprint arXiv:2003.00353},\n  year={2020}"},{"metadata":{},"cell_type":"markdown","source":"# 5. Appendix: Consortium Organization <a id=\"org_and_process\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}