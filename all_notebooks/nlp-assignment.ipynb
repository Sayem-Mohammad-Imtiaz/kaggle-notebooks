{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install indic-nlp-library","metadata":{"papermill":{"duration":8.511947,"end_time":"2021-04-02T05:53:26.540524","exception":false,"start_time":"2021-04-02T05:53:18.028577","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport torch\n\nfrom torch.utils import data\nfrom torch import nn\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.88717,"end_time":"2021-04-02T05:53:28.447231","exception":false,"start_time":"2021-04-02T05:53:26.560061","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport csv\n\n# This function takes csv file and split in train, validation set and save in appropriate file\ndef train_validation_split(csv_file):\n\n    Hindi_sts = [] \n    Eng_sts = []\n    with open(csv_file, 'r') as f:\n        reader = csv.reader(f)\n        for row in reader:\n            if(row[0] == ''):\n                continue\n            Hindi_sts.append(row[1])\n            Eng_sts.append(row[2])\n\n    trn_len = int(102322*0.8)\n    tst_len = 102322 - trn_len    \n    tst_idxs = random.sample(range(102322), tst_len)\n\n    X_validate = []\n    y_validate = []\n    for idx in tst_idxs:\n        X_validate.append(Hindi_sts[idx])\n        y_validate.append(Eng_sts[idx])\n\n    set1 = set(range(102322))\n    set2 = set(tst_idxs)\n    trn_idxs = list(set1 - set2)\n\n    X_train = []\n    y_train = []\n    for idx in trn_idxs:\n        X_train.append(Hindi_sts[idx])\n        y_train.append(Eng_sts[idx])\n    \n    with open('X_train.txt', 'w') as f, open('y_train.txt', 'w') as f1:\n        for row1,row2 in zip(X_train, y_train):\n            f.write(row1 + '\\n')\n            f1.write(row2 + '\\n')\n\n    with open('X_validate.txt', 'w') as f, open('y_validate.txt', 'w') as f1:\n        for row1,row2 in zip(X_validate, y_validate):\n            f.write(row1 + '\\n')\n            f1.write(row2 + '\\n')\n    \n    print(len(X_train))\n    \ncsv_file = '../input/hineng/train/train.csv'\ntrain_validation_split(csv_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References for below code:\nhttp://d2l.ai/","metadata":{}},{"cell_type":"code","source":"from operator import is_not\nfrom functools import partial\n\ndef tokenize_hin(X_train):\n    source_tokenize = []\n    for s in X_train:\n        s = re.sub(r'[^\\u0970-\\u097f\\u0900-\\u0963]+', ' ', s).split(' ')\n        s = [x for x in s if x is not '']\n        source_tokenize.append(s)    \n    return source_tokenize\n        \ndef tokenize_eng(y_train):\n    target_tokenize = []\n    for s in y_train:\n        s = s.lower()\n        s = re.sub(r'[^a-zA-Z]+', ' ', s).split(' ')\n        s = [x for x in s if x is not '']\n        target_tokenize.append(s)\n    return target_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Accumulator:\n    \"\"\"For accumulating sums over `n` variables.\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Vocab:  #@save\n    \"\"\"Vocabulary for text.\"\"\"\n    def __init__(self, tokens=None, min_freq=0 , reserved_tokens=None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        \n        # Sort according to frequencies\n        counter = count_corpus(tokens)\n        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n        # The index for the unknown token is 0\n        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n        uniq_tokens += [\n            token for token, freq in self.token_freqs\n            if freq >= min_freq and token not in uniq_tokens]\n        self.idx_to_token, self.token_to_idx = [], dict()\n        for token in uniq_tokens:\n            self.idx_to_token.append(token)\n            self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\ndef count_corpus(tokens):\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)","metadata":{"papermill":{"duration":0.033152,"end_time":"2021-04-02T05:53:28.583698","exception":false,"start_time":"2021-04-02T05:53:28.550546","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def try_gpu(i=0):\n    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n    if torch.cuda.device_count() >= i + 1:\n        return torch.device(f'cuda:{i}')\n    return torch.device('cpu')","metadata":{"papermill":{"duration":0.024541,"end_time":"2021-04-02T05:53:28.625064","exception":false,"start_time":"2021-04-02T05:53:28.600523","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def truncate_pad(line, num_steps, padding_token):\n    \"\"\"Truncate or pad sequences.\"\"\"\n    if len(line) > num_steps:\n        return line[:num_steps]  # Truncate\n    return line + [padding_token] * (num_steps - len(line))  # Pad\n\ndef load_array(data_arrays, batch_size, is_train=True):\n    \"\"\"Construct a PyTorch data iterator.\"\"\"\n    dataset = data.TensorDataset(*data_arrays)\n    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n\ndef build_array_nmt(lines, vocab, num_steps):\n    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n    lines = [vocab[l] for l in lines]\n    lines = [l + [vocab['<eos>']] for l in lines]\n    array = torch.tensor([truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n    return array, valid_len","metadata":{"papermill":{"duration":0.026609,"end_time":"2021-04-02T05:53:28.711864","exception":false,"start_time":"2021-04-02T05:53:28.685255","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_nmt(X_train, y_train, batch_size, num_steps, num_examples=600):\n    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n    \n    source = tokenize_hin(X_train)\n    target = tokenize_eng(y_train)\n    \n    src_vocab = Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n    tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n    \n    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n    \n    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n    data_iter = load_array(data_arrays, batch_size)\n    \n    return data_iter, src_vocab, tgt_vocab","metadata":{"papermill":{"duration":0.02785,"end_time":"2021-04-02T05:53:28.756706","exception":false,"start_time":"2021-04-02T05:53:28.728856","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdditiveAttention(nn.Module):\n    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n        super(AdditiveAttention, self).__init__(**kwargs)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, queries, keys, values, valid_lens):\n        queries, keys = self.W_q(queries), self.W_k(keys)\n        # After dimension expansion, shape of `queries`: (`batch_size`, no. of\n        # queries, 1, `num_hiddens`) and shape of `keys`: (`batch_size`, 1,\n        # no. of key-value pairs, `num_hiddens`). Sum them up with\n        # broadcasting\n        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n        features = torch.tanh(features)\n        # There is only one output of `self.w_v`, so we remove the last\n        # one-dimensional entry from the shape. Shape of `scores`:\n        # (`batch_size`, no. of queries, no. of key-value pairs)\n        scores = self.w_v(features).squeeze(-1)\n        \n        #PAD_IDX = tgt_vocab.token_to_idx['<pad>']\n        #loss = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n        #self.attention_weights = loss(scores)\n        \n        self.attention_weights = masked_softmax(scores, valid_lens)\n        \n        # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n        # dimension)\n        return torch.bmm(self.dropout(self.attention_weights), values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@save\nclass DotProductAttention(nn.Module):\n    \"\"\"Scaled dot product attention.\"\"\"\n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # Shape of `queries`: (`batch_size`, no. of queries, `d`)\n    # Shape of `keys`: (`batch_size`, no. of key-value pairs, `d`)\n    # Shape of `values`: (`batch_size`, no. of key-value pairs, value\n    # dimension)\n    # Shape of `valid_lens`: (`batch_size`,) or (`batch_size`, no. of queries)\n    def forward(self, queries, keys, values, valid_lens=None):\n        d = queries.shape[-1]\n        # Set `transpose_b=True` to swap the last two dimensions of `keys`\n        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        return torch.bmm(self.dropout(self.attention_weights), values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 num_heads, dropout, bias=False, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.attention = DotProductAttention(dropout)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n\n    def forward(self, queries, keys, values, valid_lens):\n        # Shape of `queries`, `keys`, or `values`:\n        # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`)\n        # Shape of `valid_lens`:\n        # (`batch_size`,) or (`batch_size`, no. of queries)\n        # After transposing, shape of output `queries`, `keys`, or `values`:\n        # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n        # `num_hiddens` / `num_heads`)\n        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n        values = transpose_qkv(self.W_v(values), self.num_heads)\n\n        if valid_lens is not None:\n            # On axis 0, copy the first item (scalar or vector) for\n            # `num_heads` times, then copy the next item, and so on\n            valid_lens = torch.repeat_interleave(valid_lens,\n                                                 repeats=self.num_heads,\n                                                 dim=0)\n\n        # Shape of `output`: (`batch_size` * `num_heads`, no. of queries,\n        # `num_hiddens` / `num_heads`)\n        output = self.attention(queries, keys, values, valid_lens)\n\n        # Shape of `output_concat`:\n        # (`batch_size`, no. of queries, `num_hiddens`)\n        output_concat = transpose_output(output, self.num_heads)\n        return self.W_o(output_concat)\n\ndef transpose_qkv(X, num_heads):\n    # Shape of input `X`:\n    # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`).\n    # Shape of output `X`:\n    # (`batch_size`, no. of queries or key-value pairs, `num_heads`,\n    # `num_hiddens` / `num_heads`)\n    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n\n    # Shape of output `X`:\n    # (`batch_size`, `num_heads`, no. of queries or key-value pairs,\n    # `num_hiddens` / `num_heads`)\n    X = X.permute(0, 2, 1, 3)\n\n    # Shape of `output`:\n    # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n    # `num_hiddens` / `num_heads`)\n    return X.reshape(-1, X.shape[2], X.shape[3])\n\ndef transpose_output(X, num_heads):\n    \"\"\"Reverse the operation of `transpose_qkv`\"\"\"\n    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n    X = X.permute(0, 2, 1, 3)\n    return X.reshape(X.shape[0], X.shape[1], -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n    def __init__(self, encoder, decoder, **kwargs):\n        super(EncoderDecoder, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, enc_X, dec_X, *args):\n        enc_outputs = self.encoder(enc_X, *args)\n        (outputs, dec_hidden_st, dec_cell, enc_valid_lens) = self.decoder.init_state(enc_outputs, *args)\n        return self.decoder(dec_X, (outputs, dec_hidden_st, dec_cell, enc_valid_lens))\n\nclass Seq2SeqEncoder(nn.Module):\n    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers,\n                          dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, X, *args):\n        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n        X = self.dropout(self.embedding(X))\n        # In RNN models, the first axis corresponds to time steps\n        X = X.permute(1, 0, 2)\n        # When state is not mentioned, it defaults to zeros\n        output, (hidden_st, cell) = self.rnn(X)\n        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n        return output, hidden_st, cell\n\nclass Seq2SeqAttentionDecoder(nn.Module):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        #self.attention = AdditiveAttention(num_hiddens, num_hiddens, num_hiddens, dropout)\n        #self.attention = DotProductAttention(dropout)\n        self.attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, 4, dropout)\n        \n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size + num_hiddens, num_hiddens, num_layers,\n                          dropout=dropout)\n        self.dense = nn.Linear(num_hiddens, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_lens, *args):\n        # Shape of `outputs`: (`num_steps`, `batch_size`, `num_hiddens`).\n        # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n        # `num_hiddens`)\n        outputs, hidden_st, cell = enc_outputs\n        return (outputs.permute(1, 0, 2), hidden_st, cell, enc_valid_lens)\n\n    def forward(self, X, state):\n        # Shape of `enc_outputs`: (`batch_size`, `num_steps`, `num_hiddens`).\n        # Shape of `hidden_state[0]`: (`num_layers`, `batch_size`,\n        # `num_hiddens`)\n        enc_outputs, hidden_st, cell, enc_valid_lens = state\n        # Shape of the output `X`: (`num_steps`, `batch_size`, `embed_size`)\n        X = self.embedding(X).permute(1, 0, 2)\n        outputs, self._attention_weights = [], []\n        for x in X:\n            # Shape of `query`: (`batch_size`, 1, `num_hiddens`)\n            query = torch.unsqueeze(hidden_st[-1], dim=1)\n            # Shape of `context`: (`batch_size`, 1, `num_hiddens`)\n            context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens)\n            # Concatenate on the feature dimension\n            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n            # Reshape `x` as (1, `batch_size`, `embed_size` + `num_hiddens`)\n            out, (hidden_st, cell) = self.rnn(x.permute(1, 0, 2), (hidden_st, cell))\n            outputs.append(out)\n            #self._attention_weights.append(self.attention.attention_weights)\n        \n        # After fully-connected layer transformation, shape of `outputs`:\n        # (`num_steps`, `batch_size`, `vocab_size`)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.permute(1, 0, 2), [\n            enc_outputs, hidden_st, cell, enc_valid_lens]\n\n    @property\n    def attention_weights(self):\n        return self._attention_weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sequence_mask(X, valid_len, value=0):\n    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n    maxlen = X.size(1)\n    mask = torch.arange((maxlen), dtype=torch.float32,\n                        device=X.device)[None, :] < valid_len[:, None]\n    X[~mask] = value\n    return X\n\nclass MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n\n    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n    # `label` shape: (`batch_size`, `num_steps`)\n    # `valid_len` shape: (`batch_size`,)\n    def forward(self, pred, label, valid_len):\n        weights = torch.ones_like(label)\n        weights = sequence_mask(weights, valid_len)\n        self.reduction = 'none'\n        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n        return weighted_loss","metadata":{"papermill":{"duration":0.026876,"end_time":"2021-04-02T05:53:28.893535","exception":false,"start_time":"2021-04-02T05:53:28.866659","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def masked_softmax(X, valid_lens):\n    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n    # `X`: 3D tensor, `valid_lens`: 1D or 2D tensor\n    if valid_lens is None:\n        return nn.functional.softmax(X, dim=-1)\n    else:\n        shape = X.shape\n        if valid_lens.dim() == 1:\n            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n        else:\n            valid_lens = valid_lens.reshape(-1)\n        # On the last axis, replace masked elements with a very large negative\n        # value, whose exponentiation outputs 0\n        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n        return nn.functional.softmax(X.reshape(shape), dim=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n    \"\"\"Train a model for sequence to sequence.\"\"\"\n    def xavier_init_weights(m):\n        if type(m) == nn.Linear:\n            nn.init.xavier_uniform_(m.weight)\n        if type(m) == nn.LSTM:\n            for param in m._flat_weights_names:\n                if \"weight\" in param:\n                    nn.init.xavier_uniform_(m._parameters[param])\n\n    net.apply(xavier_init_weights)\n    net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    loss = MaskedSoftmaxCELoss()\n    net.train()\n    \n    for epoch in range(num_epochs):\n        metric = Accumulator(2)  # Sum of training loss, no. of tokens\n        \n        for batch in data_iter:\n            \n            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n            \n            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n            Y_hat, _ = net(X, dec_input, X_valid_len)\n            \n            l = loss(Y_hat, Y, Y_valid_len)\n            l.sum().backward()  # Make the loss scalar for `backward`\n            \n            torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n            num_tokens = Y_valid_len.sum()\n            optimizer.step()\n            with torch.no_grad():\n                metric.add(l.sum(), num_tokens)\n        \n        if (epoch + 1) % 10 == 0:\n            print(epoch, f'loss {metric[0] / metric[1]:.3f}')\n    print(f'final loss {metric[0] / metric[1]:.3f}')","metadata":{"papermill":{"duration":0.032843,"end_time":"2021-04-02T05:53:28.988319","exception":false,"start_time":"2021-04-02T05:53:28.955476","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_size, num_hiddens, num_layers, dropout = 64, 128, 2, 0.25\nbatch_size, num_steps = 64, 15\nlr, num_epochs, device = 0.001, 300, try_gpu()\n\nwith open('../input/train-validate/Data/X_train.txt', 'r') as f, open('../input/train-validate/Data/y_train.txt', 'r') as f1:\n    X_train = f.read().splitlines()\n    y_train = f1.read().splitlines()\n\ntrain_iter, src_vocab, tgt_vocab = load_data_nmt(X_train, y_train, batch_size, num_steps)\n\nencoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder = Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nnet = EncoderDecoder(encoder, decoder)\nprint(\"Hello\")\ntrain_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(net.state_dict(), './train_model_31.pt')","metadata":{"papermill":{"duration":0.19713,"end_time":"2021-04-02T08:35:18.763152","exception":false,"start_time":"2021-04-02T08:35:18.566022","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#net.load_state_dict(torch.load('./train_model_11.pt'))","metadata":{"papermill":{"duration":0.029102,"end_time":"2021-04-02T08:35:18.812765","exception":false,"start_time":"2021-04-02T08:35:18.783663","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=False):\n    \"\"\"Predict for sequence to sequence.\"\"\"\n    \n    # Set `net` to eval mode for inference\n    net.eval()\n    #tkns = indic_tokenize.trivial_tokenize(normalizer.normalize(src_sentence))\n    tkns = tokenize_hin([src_sentence])\n    tkns = tkns[0]\n    src_tokens = src_vocab[tkns] + [src_vocab['<eos>']]\n    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n    \n    # Add the batch axis\n    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n    enc_outputs = net.encoder(enc_X, enc_valid_len)\n    dec_st = net.decoder.init_state(enc_outputs, enc_valid_len)\n    \n    # Add the batch axis\n    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n    output_seq, attention_weight_seq = [], []\n    \n    #Y, dec_st = net.decoder(dec_X, dec_st)\n        \n    # We use the token with the highest prediction likelihood as the input\n    # of the decoder at the next time step\n    \n    for _ in range(num_steps):\n        Y, dec_st = net.decoder(dec_X, dec_st)\n        \n        # We use the token with the highest prediction likelihood as the input\n        # of the decoder at the next time step\n        dec_X = Y.argmax(dim=2)\n        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n        \n        if save_attention_weights:\n            attention_weight_seq.append(net.decoder.attention_weights)\n        \n        # Once the end-of-sequence token is predicted, the generation of the\n        # output sequence is complete\n        if pred == tgt_vocab['<eos>']:\n            break\n        output_seq.append(pred)\n    \n    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq\n","metadata":{"papermill":{"duration":0.034782,"end_time":"2021-04-02T08:35:18.868933","exception":false,"start_time":"2021-04-02T08:35:18.834151","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_validate = []\ny_validate = []\nwith open('../input/train-validate/Data/X_validate.txt', 'r') as f, open('../input/train-validate/Data/y_validate.txt', 'r') as f1:\n    X_validate = f.read().splitlines()\n    y_validate = f1.read().splitlines()\n\nlen(X_validate), len(y_validate)","metadata":{"papermill":{"duration":0.14895,"end_time":"2021-04-02T08:35:19.088227","exception":false,"start_time":"2021-04-02T08:35:18.939277","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = []\nfor hind, eng in zip(X_validate,y_validate):\n    translation, attention_weight_seq = predict_seq2seq(net, hind, src_vocab, tgt_vocab, num_steps, device)\n    predict.append(translation)","metadata":{"papermill":{"duration":167.345995,"end_time":"2021-04-02T08:38:06.456207","exception":false,"start_time":"2021-04-02T08:35:19.110212","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('predict.txt', 'w') as f:\n    for row1 in predict:\n        f.write(row1 + '\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer = []\n\ntest_csv = pd.read_csv('../input/tst-data/hindistatements.csv')\nX_test = test_csv.iloc[:,2]\n    \nfor hind in X_test:\n    translation, attention_weight_seq = predict_seq2seq(net, hind, src_vocab, tgt_vocab, num_steps, device)\n    answer.append(translation)\n\nwith open('answer.txt', 'w') as f:\n    for row1 in answer:\n        f.write(row1 + '\\n')","metadata":{"papermill":{"duration":31.60771,"end_time":"2021-04-02T08:39:11.624396","exception":false,"start_time":"2021-04-02T08:38:40.016686","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}