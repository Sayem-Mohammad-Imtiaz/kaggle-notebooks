{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# installing haystack\n! pip install git+https://github.com/deepset-ai/haystack.git\n    \n# Installing Elasticsearch\n! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n! chown -R daemon:daemon elasticsearch-7.6.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# General libraries\nimport re, os, string, random, requests\nimport pandas as pd\nfrom subprocess import Popen, PIPE, STDOUT\nfrom tqdm import tqdm\n\n# Haystack importings\nfrom haystack import Finder\nfrom haystack.reader.farm import FARMReader\nfrom haystack.utils import print_answers\nfrom haystack.document_store.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.retriever.sparse import ElasticsearchRetriever","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Starting ElasticSearch server as daemon\nes_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n                   stdout=PIPE, stderr=STDOUT,\n                   preexec_fn=lambda: os.setuid(1)  # as daemon\n                  )\n\n# wait until ElasticSearch has started\n! sleep 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_index(n):\n    \"\"\"Return a random string of length n\"\"\"\n    letters = string.ascii_lowercase\n    result_str = ''.join(random.choice(letters) for i in range(n))\n    return result_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trim_doc(doc):\n    \"\"\"Trim doc with respect to the boundary of a sentence.\"\"\"\n    \n    trimmedText = []\n    charCount = 0\n    for sentence in doc.split('.'):\n        if charCount < DOC_THRESHOLD:\n            charCount+=len(sentence.strip())\n            trimmedText.append(sentence)\n\n    finalText = \".\".join(trimmedText)\n    \n    return finalText","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \"\"\"Doc cleaning\"\"\"\n    \n    # Lowering text\n    text = text.lower()\n    \n    # Removing punctuation\n    text = \"\".join([c for c in text if c not in PUNCTUATION])\n    \n    # Removing whitespace and newlines\n    text = re.sub('\\s+',' ',text)\n    \n    # Trimming doc\n    text = trim_doc(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Constants\nES_INDEX = get_index(10) # Elastic Search DB index name\nPUNCTUATION = \"\"\"!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~\"\"\" # excluding . (full-stop) from the set of punctuations\nDOC_THRESHOLD = 10000 # character limit for a doc\nTOP_K_RETRIEVER = 10 # top k documents to analyze further for a given query\nTOP_K_READER = 5 # top k number of answers to return\nBASE_URL = \"http://localhost:9200/\"+ES_INDEX+\"/_doc/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/nips-papers-1987-2019-updated/papers.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(subset=['full_text'], inplace=True)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Structuring data to haystack required format\n# Format: [{'text': 'paper_content', 'meta':{'name':'title'}}]\ndocs = []\ncorpora = []\ndoc_len = []\n\nfor index, row in tqdm(data.iterrows()):\n    dicts = {}\n    dicts['text'] = clean_text(row['full_text'])\n    doc_len.append(len(dicts['text']))\n    corpora.append(dicts['text'])\n    dicts['meta'] = {}\n    dicts['meta']['name'] = clean_text(row['title'])\n    docs.append(dicts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average characters in a document after trimming\nsum(doc_len)/len(docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Be careful while overwriting data on the same ES index\ndocument_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=ES_INDEX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, let's write the dicts containing documents to our DB.\ndocument_store.write_documents(docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiating ES retriever \nretriever = ElasticsearchRetriever(document_store=document_store)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Initializing reader on the top of roberta-base-squad2 pre-trained model, which will be downloaded on the first run\n# Here, we can set the size of context window for our answers and use the GPU if available\n\nreader = FARMReader(model_name_or_path=\"ahotrod/albert_xxlargev1_squad2_512\",use_gpu=True, context_window_size=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting reader and retriever to Finder\nfinder = Finder(reader, retriever)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Question prediction with TOP_K_RETRIEVER and TOP_K_READER\nquestion = \"What is the use of CNN?\"\nprediction = finder.get_answers(question=question, top_k_retriever=TOP_K_RETRIEVER, top_k_reader=TOP_K_READER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing answers with minimal detail\n# details = minimal | medium | all\n\nprint_answers(prediction, details=\"minimal\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(prediction['answers'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Amazing ðŸ”¥\nThe question was, **What is the use of CNN?**.\n\nAs we all know, the CNN (ConvNet/Convolutional Neural Network) algorithm deals with the image data. It looks like our QA system has answered our query very well. Yayyy!! ðŸ¥³ ðŸŽ‰ðŸŽ‰ðŸŽ‰\n![](https://media.tenor.com/images/5a2d3ba3504d3f48da005d9fe6b52110/tenor.gif)\n\nDon't forget to upvote the notebook, if you like my work. Let me know your feedback in the comment section below. ðŸ˜Š\n\n### #StaySafe"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}