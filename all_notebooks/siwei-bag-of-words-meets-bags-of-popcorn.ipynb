{"cells":[{"metadata":{"trusted":true,"_uuid":"f3d79c4adbfb86e19e5b583ba2f71c13372e83aa","collapsed":true},"cell_type":"code","source":"# help()\n# import platform  \n# platform.architecture()  \n\n# I met \"Memory error so that I want to see the version and the bit imfo\"","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"a2bc7ffe2c3db69582ac5135c39302247ddb9da8"},"cell_type":"markdown","source":"# import data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfile = \"../input/bag-of-words-meets-bags-of-popcorn-/labeledTrainData.tsv\"\ndata_train = pd.read_csv(file, sep=\"\\t\")\nfile = \"../input/bag-of-words-meets-bags-of-popcorn-/testData.tsv\"\ndata_test = pd.read_csv(file, sep=\"\\t\")\nfile = \"../input/bag-of-words-meets-bags-of-popcorn-/unlabeledTrainData.tsv\"\ndata_unlabeled = pd.read_csv(file, sep=\"\\t\", error_bad_lines=False)\n# the difference between .csv and .tsv is taht tsv file use \"\\t\" as sep rather than \",\"\n\ny_train = np.array(data_train.iloc[:,1])\ndata_train.drop([\"sentiment\"], axis = 1, inplace = True)\nx_train = np.array(data_train)\nx_test = np.array(data_test)\nx_unlabeled = np.array(data_unlabeled)\n\nn_samples_train = x_train.shape[0]\nn_samples_test = x_test.shape[0]\nn_samples_unlabeled = x_unlabeled.shape[0]\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(x_unlabeled.shape)\nprint(x_train[0])\n# Any results you write to the current directory are saved as output.","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"ebbc83e3529d687b986a533da376b732f8c6c535"},"cell_type":"markdown","source":"# pre-process\n## scan the text and pick out html symbols\nWe can see that there're so many `\\` and `<br/>`. we need to pick them out.\n\n## word stemming\nTo lower case and stemming"},{"metadata":{"trusted":true,"_uuid":"47106b1a48f0526503c19dd111c555f1dfd65312","collapsed":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nfrom nltk.stem.porter import PorterStemmer  \nimport re\ndef scanner_stemmer(review):\n    text = BeautifulSoup(review, \"html.parser\").get_text()\n    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n    text = text.split()\n    porter_stemmer = PorterStemmer()\n    for index,item in enumerate(text):\n        text[index] = porter_stemmer.stem(item)\n    text = \" \".join(text)\n    return text","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"0257f412b9e5e16eed60d03a53fad5508786fb55"},"cell_type":"markdown","source":"### split reviews into sentences\nIf we want to train a Word2Vec model, it needs the input to be sentences."},{"metadata":{"trusted":true,"_uuid":"cc322e8461b09bad4032b397c096aec38087f5f4"},"cell_type":"code","source":"import nltk.data\n# nltk.download()\n\ndef review2sentences( review, tokenizer, remove_stopwords=False ):\n    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    raw_sentences = tokenizer.tokenize(review.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n    return sentences\n\ndef reviews2sentences(x_1, x_2):\n    sentences = []  \n    for review in x_1[\"review\"]:\n        sentences += review_to_sentences(review, tokenizer)\n    for review in x_2[\"review\"]:\n        sentences += review_to_sentences(review, tokenizer)\n    print(len(sentences))","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"markdown","source":"## reviews to vector\nchange words into a format that a computer can understand, meanswhile, don't lose so much information."},{"metadata":{"_uuid":"1b96c43697ae60c4d47112d79e3399934acd4dfd"},"cell_type":"markdown","source":"### TF-IDF"},{"metadata":{"trusted":true,"_uuid":"16d4715505ac3b72e01c192a40c1eb1e8554ceaf","collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n# TF-IDF will convert all characters to lowercase by default\n\ndef tfidf(x_1, x_2):\n    n_samples_1 = len(x_1)\n    n_samples_2 = len(x_2)\n    \n    # vectorizer = CV(analyzer='word', ngram_range=(1, 2))\n    vectorizer = TFIDF(max_features=40000,sublinear_tf=1,analyzer='word', ngram_range=(1, 3)) #sublinear_tf=1\n\n    x_all = x_1 + x_2\n    x_all = vectorizer.fit_transform(x_all)\n    x_1 = x_all[:n_samples_1]\n    x_2 = x_all[n_samples_1:]\n    print(x_1[0].shape, \"\\n\")\n    return x_1, x_2","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"d5c6ae4059cfb5427e718d63117782fef424bac8"},"cell_type":"markdown","source":"### Word2Vec\nFirstly, we have to train the model.  \nSecondly, get the vector format of each word.  \nAt the last, we transform reviews to vectors by:\n- Vector Averagin\n- Clustering\n\nNote: I use the pre-train model by Google, so that I take the original reviews as input rather than the stemmed."},{"metadata":{"trusted":true,"_uuid":"7972243b75056a5306e345cac7aa73a0063d8bca"},"cell_type":"code","source":"# x_sentence_all = reviews2sentences(x_train, x_unlabeled)\n\n# num_features = 300    # Word vector dimensionality                      \n# min_word_count = 40   # Minimum word count                        \n# num_workers = 4       # Number of threads to run in parallel\n# context = 10          # Context window size                                                                                    \n# downsampling = 1e-3   # Downsample setting for frequent words\n\n# from gensim.models import word2vec\n# print \"Training model...\"\n# model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, \n#                           window = context, sample = downsampling)\n\n\n# model.init_sims(replace=True)\n# model_name = \"300features_40minwords_10context\"\n# model.save(model_name)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62cea3d11a4efe11e50d88b4e61ee7139a88dc1b"},"cell_type":"code","source":"import gensim  \nmodel = gensim.models.KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin',binary=True)  \nlen_vec_review = 300\ndef reviews2vec(reviews):\n    n_reviews = reviews.shape[0]\n    vec_review = np.zeros((n_reviews, len_vec_review))\n    index2word_set = set(model.index2word)\n    for i in range(n_reviews):\n        cnt = 0\n        text = BeautifulSoup(reviews[i][1], \"html.parser\").get_text()\n        text = re.sub(\"[^a-zA-Z]\",\" \", text)\n        text = text.split()\n        for word in text:\n            if word in index2word_set:\n                cnt += 1\n                vec_review[i] += model[word]\n        vec_review[i] /= cnt\n    return vec_review","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"c63a318f82cf3fc955c9dc59d0e30bb468378974"},"cell_type":"markdown","source":"## do the pre-process part"},{"metadata":{"trusted":true,"_uuid":"ef7a247159ec5b156de4635f091a9b03d2f1b3e1"},"cell_type":"code","source":"# reviews to text after being lowered case and stemmed.\n# x_text_train is a list, each obj is a string.\nx_text_train = []\nfor i in range(n_samples_train):\n    x_text_train.append(scanner_stemmer(x_train[i][1]))\nprint(x_text_train[0])\n\nx_text_test = []\nfor i in range(n_samples_test):\n    x_text_test.append(scanner_stemmer(x_test[i][1]))\n\n## text to vector\nx_vec_train_tfidf, x_vec_train_tfidf = tfidf(x_text_train, x_text_test)\n    \n# reviews to vector \nx_vec_train_w2v = reviews2vec(x_train)\nx_vec_test_w2v = reviews2vec(x_test)\nprint(x_vec_train_w2v[0])\nprint(model[\"moment\"])","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"b0d0a6007012ec16c7e1dd78dca11934d47ad5ca"},"cell_type":"markdown","source":"# General function"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8e64540dae3e9299be38c11e99c3d124df636184"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\ndef general_function(mod_name, model_name, x_train, y_train, x_test):\n    y_pred = model_train_predict(mod_name, model_name, x_train, y_train, x_test)\n    output_prediction(y_pred, model_name)\n\ndef get_score(clf, x_train, y_train, use_acc=True):\n    if use_acc:\n        y_pred = clf.predict(x_train)\n        right_num = (y_train == y_pred).sum()\n        print(\"acc: \", right_num/n_samples_train)\n        scores = cross_val_score(clf, x_train, y_train, cv=5)\n        print(\"K-fold Accuracy: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    else:\n        y_pred = clf.predict_proba(x_train)[:,1]\n        score_auc = roc_auc_score(y_train, y_pred)\n        print(\"auc: \", score_auc)\n        scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='roc_auc')\n        print(\"K-fold AUC: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n    \n\ndef model_train_predict(mod_name, model_name, x_train, y_train, x_test, use_acc=True):\n    import_mod = __import__(mod_name, fromlist = str(True))\n    if hasattr(import_mod, model_name):\n         f = getattr(import_mod, model_name)\n    else:\n        print(\"404\")\n        return []\n    clf = f()\n    clf.fit(x_train, y_train)\n    get_score(clf, x_train, y_train, use_acc = False)\n    if use_acc:\n        y_pred = clf.predict(x_test)\n    else:\n        y_pred = clf.predict_proba(x_test)[:,1]\n    return y_pred\n\ndef output_prediction(y_pred, model_name):\n    print(y_pred)\n    data_predict = {\"id\":x_test[:,0], \"sentiment\":y_pred}\n    data_predict = pd.DataFrame(data_predict)\n    data_predict.to_csv(\"bwmbp output %s.csv\" %model_name, index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06a8677d91573c5d68d310c34357c3a8d67db123"},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"dfa1e8cdcd92c9be9b8b6966c644b8a42872d2fc","collapsed":true},"cell_type":"code","source":"# from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nmod_name = \"sklearn.naive_bayes\"\n# model_name = \"GaussianNB\"\nmodel_name = \"MultinomialNB\"\n# model_name = \"BernoulliNB\"\ngeneral_function(mod_name, model_name, x_vec_train, y_train, x_vec_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2024c0f346680a2f93173606860bff0b0c463f1"},"cell_type":"markdown","source":"# SVM"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c0f91d6ff4aea1bf2ab818f4e03a8d844b6494fa"},"cell_type":"code","source":"# from sklearn.svm import SVC\nmod_name = \"sklearn.svm\"\nmodel_name = \"SVC\"\n# general_function(mod_name, model_name, x_vec_train, y_train, x_vec_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbb2667af79140a846265b9a6473c1da2861ac4d"},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true,"_uuid":"d0c7079fa1e52ef932b9fb8d98b541b959cf92d7","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmod_name = \"sklearn.ensemble\"\nmodel_name = \"RandomForestClassifier\"\n# general_function(mod_name, model_name, x_vec_train, y_train, x_vec_test)\n\nclf = RandomForestClassifier(n_estimators=100, min_samples_split=50)\nclf.fit(x_vec_train_w2v, y_train)\nget_score(clf, x_vec_train_w2v, y_train, use_acc = False)\ny_pred = clf.predict_proba(x_vec_test_w2v)[:,1]\noutput_prediction(y_pred, model_name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a29b78a58c1b7390e2a0fd5025d0f28de25c484e"},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"3072eb72b6c7779a47360e426a8066a8d8bbb50f","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmod_name = \"sklearn.linear_model\"\nmodel_name = \"LogisticRegression\"\ngeneral_function(mod_name, model_name, x_vec_train_w2v, y_train, x_vec_test_w2v)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}