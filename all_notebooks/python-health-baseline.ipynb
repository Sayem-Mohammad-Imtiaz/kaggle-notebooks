{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ncount_2017 = pd.read_csv('../input/health-data/count_2017.csv')\ncount_2018 = pd.read_csv('../input/health-data/count_2018.csv')\ncount_2019 = pd.read_csv('../input/health-data/count_2019.csv')\ncount_2020 = pd.read_csv('../input/health-data/count_2020.csv')\nto_predict = pd.read_csv('../input/health-data/to_predict.csv')\nsubmit = pd.read_csv('../input/health-data/submit.csv')\ncount_df = pd.concat([count_2017,count_2018,count_2019,count_2020],axis=0)\nimport  os\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold,train_test_split\nfrom sklearn.metrics import mean_squared_error \nimport time, datetime\nimport matplotlib.pyplot as plt\nfrom pandas import Series,DataFrame\nimport seaborn as sns\ntest=to_predict\ndef time_process(df):\n    df.date=pd.to_datetime(df.date)\n    df['year']=df.date.dt.year     \n    df['month']=df.date.dt.month\n    df['day']=df.date.dt.day\n    df['weekofyear']=df['date'].apply(lambda x: x.weekofyear)\n    df['week_day']=df['date'].dt.weekday\n    df=pd.get_dummies(df,columns=['week_day'])\n    return df\ncount_df['date'] = count_df['date'].apply(str).apply(lambda x:datetime.datetime.strptime(x,'%Y%m%d')).astype('datetime64[D]')\ntest['date']=test['date'].apply(str).apply(lambda x:datetime.datetime.strptime(x,'%Y/%m/%d')).astype('datetime64[D]')\ncount_df = time_process(count_df)\ntest = time_process(to_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p1 = pd.read_csv('../input/fix-health-data/p1.csv')\np2 = pd.read_csv('../input/fix-health-data/p2.csv')\np3 = pd.read_csv('../input/fix-health-data/p3.csv')\np4 = pd.read_csv('../input/fix-health-data/p4.csv')\np5 = pd.read_csv('../input/fix-health-data/p5.csv')\np6 = pd.read_csv('../input/fix-health-data/p6.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c18=count_2018['count'][count_2018['admin_illness_name']=='上呼吸道感染']\nc19=p1['count'][p1['admin_illness_name']=='上呼吸道感染']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c19_3=pd.DataFrame(np.zeros(shape=(14,1))).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctest=pd.concat([c18,c19,c19_3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input, Conv1D, Dense, Dropout, Lambda, Concatenate\nfrom keras.optimizers import Adam\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport pandas as pd\nimport pathlib\nimport gc\nfrom datetime import timedelta\nimport os\n\n\npred_steps = 14\n#数据处理部分\ndef preprocessing():\n\n    df = ctest.values\n\n    val_pred_start = len(df) - pred_steps\n    val_pred_end = len(df)\n\n    train_pred_start = val_pred_start - pred_steps - 1\n    train_pred_end = val_pred_start - 1\n    enc_length = train_pred_start\n\n    train_enc_start = 0\n    train_enc_end = train_enc_start + enc_length - 1\n\n    val_enc_start = train_enc_start + pred_steps\n    val_enc_end = val_enc_start + enc_length - 1\n\n    print('Train encoding:', train_enc_start, '-', train_enc_end)\n    print('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\n    print('Val encoding:', val_enc_start, '-', val_enc_end)\n    print('Val prediction:', val_pred_start, '-', val_pred_end)\n    series_array = np.array(df)\n    series_array = series_array.reshape(len(series_array))\n    return train_enc_start, train_enc_end, train_pred_start, train_pred_end, val_enc_start, val_enc_end, val_pred_start, val_pred_end, series_array\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#根据起始日期来划分数据\ndef get_time_block_series(series_array, start_date, end_date):\n    return series_array[start_date:end_date]\n\n#数据归一化，返回归一化后的数据，平均值，方差\ndef transform_series_encode(data):\n    data_mean = data.mean(axis=0).reshape(-1, 1)\n    data_std = data.std(axis=0).reshape(-1,1)\n    epsilon = 1e-6\n    data = (data - data_mean)/(data_std+epsilon)\n    data = data.reshape((data.shape[0], data.shape[1], 1))\n    return data, data_mean, data_std\n\n#数据反归一化\ndef untransform_series_decode(data, data_mean, data_std):\n    # data = data.reshape(data.shape[0], data.shape[1])\n    data_std = data_std[0][0]\n    data_mean = data_mean[0][0]\n    data = data*data_std + data_mean\n    data = data.reshape(len(data))\n    # np.clip(np.power(10., data) - 1.0, 0.0, None)\n    return  data\n\n#数据归一化\ndef transform_series_decode(data, data_mean, data_std):\n    epsilon = 1e-6 # prevent numerical error in the case std = 0\n    data = (data - data_mean)/(data_std+epsilon)\n    data = data.reshape((data.shape[0], data.shape[1], 1))\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#数据预测\ndef predict_sequences(input_sequences, batch_size):\n    history_sequences = input_sequences.copy()\n    print(history_sequences.shape)\n    pred_sequences = np.zeros((history_sequences.shape[0], pred_steps, 1))  # initialize output (pred_steps time steps)\n    print(pred_sequences.shape)\n    for i in range(pred_steps):\n        # record next time step prediction (last time step of model output)\n        last_step_pred = model.predict(history_sequences,batch_size)[:, -1, 0]\n        print(\"last step prediction first 10 channels\")\n        print(last_step_pred[0:10])\n        print(i,last_step_pred.shape)\n        pred_sequences[:, i, 0] = last_step_pred\n\n        # add the next time step prediction to the history sequence\n        history_sequences = np.concatenate([history_sequences,\n                                           last_step_pred.reshape(-1, 1, 1)], axis=1)\n\n    return pred_sequences\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#数据预测和画图\ndef predict_and_plot(encoder_input_data, pre_target_data, encode_series_mean, encode_series_std,batch_size, enc_tail_len=50, decoder_target_data=1):\n    encode_series = encoder_input_data\n    pred_series = predict_sequences(encode_series, batch_size)\n\n    encode_series = encode_series.reshape(-1, 1)\n    pred_series = pred_series.reshape(-1, 1)\n    encode_series = encode_series.reshape(len(encode_series))\n    pred_series = pred_series.reshape(len(pred_series))\n    pre_target_data = pre_target_data.reshape(len(pre_target_data))\n\n    encode_series = untransform_series_decode(encode_series,encode_series_mean, encode_series_std)\n    pred_series = untransform_series_decode(pred_series,encode_series_mean, encode_series_std)\n    # pre_target_data = untransform_series_decode(pre_target_data,encode_series_mean, encode_series_std)\n    print(encode_series[-100:])\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(0,100), encode_series[-100:],label = 'Encoding Series')\n    plt.plot(range(100, 100 + len(pred_series)), pred_series, color='red', label='predict')\n    plt.plot(range(100, 100 + len(pre_target_data)), pre_target_data, color='orange', label='target')\n\n    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n    plt.legend()\n    plt.show()\n\nmodel_name = 'Wavenet'\n\n# load existing model\n# load_previous_models = True\n# if os.path.exists('{}.h5'.format(model_name)):\n#     print('Load Previous Models')\n#     model = load_model(model_name+'.h5')\n\n\n#根据数据处理，得到数据的划分\ntrain_enc_start, train_enc_end, train_pred_start, train_pred_end, val_enc_start, val_enc_end, val_pred_start, val_pred_end, series_array = preprocessing()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Build neural networks ####\nif not os.path.exists('{}.h5'.format(model_name)):\n    n_filters = 128\n    filter_width = 5\n    dilation_rates = [2**i for i in range(12)]\n    history_seq = Input(shape=(None, 1))\n    x = history_seq\n\n    for dilation_rate in dilation_rates:\n        x = Conv1D(filters = n_filters,\n                   kernel_size=filter_width,\n                   padding='causal',\n                   dilation_rate=dilation_rate)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(.2)(x)\n    x = Dense(64)(x)\n    x = Dense(1)(x)\n    def slice(x, seq_length):\n        return x[:, -seq_length:, :]\n\n    pred_seq_train = Lambda(slice, arguments={'seq_length':pred_steps})(x)\n\n    model = Model(history_seq, pred_seq_train)\n\nmodel.summary()\n\n#### Train neural networks ####\nbatch_size = 2**10\nepochs = 100\nencoder_input_data = get_time_block_series(series_array,train_enc_start, train_enc_end)\nencoder_input_data, encode_series_mean, encode_series_std = transform_series_encode(encoder_input_data)\ndecoder_target_data = get_time_block_series(series_array,train_pred_start, train_pred_end)\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean, encode_series_std)\n# we append a lagged history of the target series to the input data,\n# so that we can train with teacher forcing\nlagged_target_history = decoder_target_data[:, :-1, :1]\nencoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\nmodel.compile(Adam(), loss='mean_absolute_error')\nhistory = model.fit(encoder_input_data, decoder_target_data,\n                    batch_size=batch_size,\n                    epochs=epochs)\n# # save the model\n# model.save(model_name + '.h5')\n\nplt.figure()\nplt.plot(history.history['loss'])\n\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error Loss')\nplt.title('Loss Over Time')\nplt.show()\n\npre_input_data = get_time_block_series(series_array, val_enc_start, val_enc_end)\npre_input_data, encode_series_mean, encode_series_std = transform_series_encode(pre_input_data)\npre_target_data = get_time_block_series(series_array,val_pred_start, val_pred_end)\n\npredict_and_plot(pre_input_data, pre_target_data,encode_series_mean, encode_series_std, 2**10,decoder_target_data=pre_target_data)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}