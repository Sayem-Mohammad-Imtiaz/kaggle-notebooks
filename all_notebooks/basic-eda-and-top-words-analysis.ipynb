{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Analyzing the monkeys' songs.\nFirst, import a csv containing all of the lyrics","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nlyrics_df = pd.read_csv('../input/arctic-monkeys-lyrics/lyrics.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:02:38.445117Z","iopub.execute_input":"2021-08-24T02:02:38.445488Z","iopub.status.idle":"2021-08-24T02:02:38.472246Z","shell.execute_reply.started":"2021-08-24T02:02:38.445451Z","shell.execute_reply":"2021-08-24T02:02:38.471373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the text, removing some words that will get in the way of the analysis.","metadata":{}},{"cell_type":"code","source":"import string, re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\n\ndef remove_punc(lyrics):\n    return \"\".join([c for c in lyrics if c not in string.punctuation])\n\ndef remove_stopwords(lyrics):\n    return [w for w in lyrics if w not in stopwords.words('english')]\n\nmarkers = ['[', ']','Verse','1','2','3','Chorus','Spoken Intro','Intro','Bridge','PreChorus', 'and','And','Outro']\ndef remove_markers(lyrics):\n    return [w for w in lyrics if w not in markers]\n\nlyrics = []\ntokenizer = RegexpTokenizer(r'\\w+')\n\nlyrics_df['lyrics'] = lyrics_df['lyrics'].apply(lambda x: remove_punc(x))\nlyrics_df['lyrics'] = lyrics_df['lyrics'].apply(lambda x: tokenizer.tokenize(x))\nlyrics_df['lyrics'] = lyrics_df['lyrics'].apply(lambda x: remove_markers(x))\nlyrics_df['lyrics'] = lyrics_df['lyrics'].apply(lambda x: remove_stopwords(x))\nlyrics_df['lyrics'].head(20)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:02:40.917073Z","iopub.execute_input":"2021-08-24T02:02:40.917622Z","iopub.status.idle":"2021-08-24T02:02:46.939634Z","shell.execute_reply.started":"2021-08-24T02:02:40.917587Z","shell.execute_reply":"2021-08-24T02:02:46.938864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using spacy, we'll see the most frequent words in the lyrics.","metadata":{}},{"cell_type":"code","source":"lyrics_df['lyrics'] = lyrics_df['lyrics'].apply(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:02:46.940947Z","iopub.execute_input":"2021-08-24T02:02:46.941414Z","iopub.status.idle":"2021-08-24T02:02:46.947891Z","shell.execute_reply.started":"2021-08-24T02:02:46.941369Z","shell.execute_reply":"2021-08-24T02:02:46.947125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nfrom collections import Counter\n\ndef most_freq_words(df, number):\n    sp = spacy.load('en_core_web_sm')\n    complete_doc = sp(' '.join([i for i in df['lyrics']]))\n    words = [token.text for token in complete_doc\n             if not token.is_stop and not token.is_punct]\n    word_freq = Counter(words)\n    common_words = word_freq.most_common(number)\n    print (common_words)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:02:46.949538Z","iopub.execute_input":"2021-08-24T02:02:46.950073Z","iopub.status.idle":"2021-08-24T02:02:47.845044Z","shell.execute_reply.started":"2021-08-24T02:02:46.950035Z","shell.execute_reply":"2021-08-24T02:02:47.844093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we'll generate a wordcloud","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef word_cloud(df):\n    v = TfidfVectorizer()\n    x = v.fit_transform(df['lyrics'])\n    text = df.lyrics.values\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'white',\n        stopwords = STOPWORDS).generate(str(text))\n    fig = plt.figure(\n        figsize = (40, 30),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:02:47.846866Z","iopub.execute_input":"2021-08-24T02:02:47.847516Z","iopub.status.idle":"2021-08-24T02:02:47.899597Z","shell.execute_reply.started":"2021-08-24T02:02:47.84747Z","shell.execute_reply":"2021-08-24T02:02:47.89814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using all discography, we get the following:","metadata":{}},{"cell_type":"code","source":"most_freq_words(lyrics_df, 20)\nword_cloud(lyrics_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:02:47.901404Z","iopub.execute_input":"2021-08-24T02:02:47.901891Z","iopub.status.idle":"2021-08-24T02:03:10.556527Z","shell.execute_reply.started":"2021-08-24T02:02:47.901831Z","shell.execute_reply":"2021-08-24T02:03:10.554265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, only the 'Suck it and see' album.","metadata":{}},{"cell_type":"code","source":"sias_df = lyrics_df[lyrics_df['album'] == 'Suck It and See']\nmost_freq_words(sias_df, 20)\nword_cloud(sias_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:03:10.558044Z","iopub.execute_input":"2021-08-24T02:03:10.558362Z","iopub.status.idle":"2021-08-24T02:03:24.578448Z","shell.execute_reply.started":"2021-08-24T02:03:10.558332Z","shell.execute_reply":"2021-08-24T02:03:24.57724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, only the 'TBH&C' album.","metadata":{}},{"cell_type":"code","source":"tranqulity_df = lyrics_df[lyrics_df['album'] == 'Tranquility Base Hotel & Casino']\nmost_freq_words(tranqulity_df, 20)\nword_cloud(tranqulity_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:03:24.580033Z","iopub.execute_input":"2021-08-24T02:03:24.58064Z","iopub.status.idle":"2021-08-24T02:03:40.624078Z","shell.execute_reply.started":"2021-08-24T02:03:24.580593Z","shell.execute_reply":"2021-08-24T02:03:40.623256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What topics are more frequent in the lyrics? For what we'll use Latent Dirichlet Allocation","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef print_top_words(model, feature_names, n_top_words):\n    topics=[]\n    for topic_idx, topic in enumerate(model.components_):\n        topics.append([feature_names[i]\n        for i in topic.argsort()[:-n_top_words - 1:-1]])    \n    for t in topics:\n        print(t)\n\ndef lda(tags):\n    number_topics = 10\n    no_top_words = 5\n    no_features = 5000\n\n    ct_vectorizer = CountVectorizer(max_features=no_features)\n    tags_ct = ct_vectorizer.fit_transform(tags)\n\n    lda = LatentDirichletAllocation(n_components=number_topics, n_jobs=-1)\n    lda.fit(tags_ct)\n    \n    ct_features_names = ct_vectorizer.get_feature_names()\n    print_top_words(lda, ct_features_names, no_top_words)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:03:40.626693Z","iopub.execute_input":"2021-08-24T02:03:40.627294Z","iopub.status.idle":"2021-08-24T02:03:40.667416Z","shell.execute_reply.started":"2021-08-24T02:03:40.627249Z","shell.execute_reply":"2021-08-24T02:03:40.666119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running it on the whole discography, in the suck it and see and in the tbh&c album","metadata":{}},{"cell_type":"code","source":"lda(lyrics_df['lyrics'])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:03:40.668941Z","iopub.execute_input":"2021-08-24T02:03:40.669266Z","iopub.status.idle":"2021-08-24T02:03:43.153942Z","shell.execute_reply.started":"2021-08-24T02:03:40.669235Z","shell.execute_reply":"2021-08-24T02:03:43.152784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda(sias_df['lyrics'])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:03:43.155566Z","iopub.execute_input":"2021-08-24T02:03:43.155891Z","iopub.status.idle":"2021-08-24T02:03:43.315326Z","shell.execute_reply.started":"2021-08-24T02:03:43.155856Z","shell.execute_reply":"2021-08-24T02:03:43.314184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda(tranqulity_df['lyrics'])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T02:03:43.317112Z","iopub.execute_input":"2021-08-24T02:03:43.317538Z","iopub.status.idle":"2021-08-24T02:03:43.497472Z","shell.execute_reply.started":"2021-08-24T02:03:43.317489Z","shell.execute_reply":"2021-08-24T02:03:43.496424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}