{"cells":[{"metadata":{"id":"NOVecSOGZ6r3"},"cell_type":"markdown","source":"# **Feature Extraction and Dimensionality Reduction with Principal Component Analysis (PCA) and Comparison Accuracy 6 Machine Learning Classification Models: before-after PCA.**\n\nStep 1:Collect Data: UCI Parkinson's Disease Classification Data Set\n\nStep 2: Eigendecomposition - Eigenvalues, Eigenvectors and Eigenspace\n\nStep 3: Primary Component Selection\n\nStep 4: Projection New Feature Space\n\nStep 5: Principal Component Analysis (PCA)\n\nStep 6:  Comparison Accurancy 6 Machine Learning Models : before-after PCA\n\n1. Model : Logistic Regression\n2. Model : Support Vector Machines (SVM)\n3. Model : Decision Tree Classifier\n4. Model : KNN(k-nearest neighbors algorithm)\n5. Model : Random Forest Classifier\n6. Model: Gaussian Naive Bayes\n"},{"metadata":{"id":"MkYnaBWaVX8X","trusted":true},"cell_type":"code","source":"import os \nos.chdir(\"../input/parkinsons-disease-speech-signal-features/\")\n!ls","execution_count":null,"outputs":[]},{"metadata":{"id":"xFlZ9T1Vcvn0"},"cell_type":"markdown","source":"# **Step 1:Collect Data: UCI Parkinson's Disease Classification Data Set**\nhttps://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification"},{"metadata":{"id":"obgaP1PIV0H8","outputId":"55f53dfc-5fde-429b-a3c8-11b7ab5622be","trusted":true},"cell_type":"code","source":"# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00470/pd_speech_features.rar\n","execution_count":null,"outputs":[]},{"metadata":{"id":"l38LN_evWejk","outputId":"ca1bb24a-6f43-466f-f0eb-2d213313ff43","trusted":true},"cell_type":"code","source":"# !unrar x pd_speech_features.rar","execution_count":null,"outputs":[]},{"metadata":{"id":"891kf5AVdjFl"},"cell_type":"markdown","source":"**Data Set Information:**\n\nThe data used in this study were gathered from 188 patients with PD (107 men and 81 women) with ages ranging from 33 to 87 (65.1Â±10.9) at the Department of Neurology in CerrahpaÅŸa Faculty of Medicine, Istanbul University. The control group consists of 64 healthy individuals (23 men and 41 women) with ages varying between 41 and 82 (61.1Â±8.9). During the data collection process, the microphone is set to 44.1 KHz and following the physicianâ€™s examination, the sustained phonation of the vowel /a/ was collected from each subject with three repetitions.\n\n\n**Attribute Information:**\n\nVarious speech signal processing algorithms including Time Frequency Features, Mel Frequency Cepstral Coefficients (MFCCs), Wavelet Transform based Features, Vocal Fold Features and TWQT features have been applied to the speech recordings of Parkinson's Disease (PD) patients to extract clinically useful information for PD assessment.\n\n\n**Relevant Papers:**\n\nProvide references to papers that have cited this data set in the past (if any)."},{"metadata":{"id":"YIQcSz4SXQEM","outputId":"aa35f462-8f7d-4ac4-f1d6-b4812904b3eb","trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"pd_speech_features.csv\") # import dataset \n\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"OJU4DbpfODFO"},"cell_type":"markdown","source":" Data Cleaning and Data Manipulation**"},{"metadata":{"id":"GcPQNdGdOA8L","outputId":"82d00e45-fa1f-4bdd-cdae-adfc7fca1373","trusted":true},"cell_type":"code","source":"# df.columns = df.iloc[0]\n# df = df.iloc[1:,].reindex()\n# df","execution_count":null,"outputs":[]},{"metadata":{"id":"CNwfdldTN6VG","outputId":"5d8ecd02-a017-4e68-fef4-68022c09fa06","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"5tyI_eeHXWlM","outputId":"0b8273e2-03e8-478f-8688-f58f6a32c95d","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"KXm-iYvgTY_z"},"cell_type":"markdown","source":"**Determining dependent and independent variables of the dataset**"},{"metadata":{"id":"Sd8fSpt9Y67p","trusted":true},"cell_type":"code","source":"X = df.iloc[:, 0:754].values  # select the independent variables\ny = df.iloc[:, 754].values    # select the dependent variable and target column","execution_count":null,"outputs":[]},{"metadata":{"id":"4qFtCulY9aIv"},"cell_type":"markdown","source":"**Data Standardization**\n"},{"metadata":{"id":"-kzOamil7LOc","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"id":"gqjEyzZor0ap"},"cell_type":"markdown","source":"# Step 2: Eigendecomposition - Eigenvalues, Eigenvectors and Eigenspace \nThe eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (elementary components) determine the directions of the new feature space, and the eigenvalues determine their size. In other words, eigenvalues describe the variance of the data along the new feature axes. Covariance Matrix The classical approach to PCA is to perform eigende composition on the covariance matrix, which is a matrix in which each element represents the covariance between two features. The covariance between two properties is calculated as follows:\n\nCov(X,Y)=∑(xi−x¯)(yi−y¯)N−1\n"},{"metadata":{"id":"1r8c2uBwPk8v"},"cell_type":"markdown","source":"**Compute the covariance matrix**"},{"metadata":{"id":"dcCaEEa17LUD","outputId":"aaf8fee9-ed87-45b1-f054-b3d45e9cf3b6","trusted":true},"cell_type":"code","source":"import numpy as np\n\nX_mean = np.mean(X, axis=0)\n# cov_mat = np.cov(X) # another method \ncov_mat = (X - X_mean).T.dot((X - X_mean)) / (X.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","execution_count":null,"outputs":[]},{"metadata":{"id":"oBg9FovJyim1"},"cell_type":"markdown","source":"**The second method for compute the covariance matrix**"},{"metadata":{"id":"qqCCG6kWyXlW","outputId":"9422e4c7-c90e-4baf-f1de-2afcd58b5a1a","trusted":true},"cell_type":"code","source":"print('NumPy covariance matrix: \\n%s' %np.cov(X.T))","execution_count":null,"outputs":[]},{"metadata":{"id":"l5PE7jXmsIP7"},"cell_type":"markdown","source":"**Compute the Eigenvalues and Eigenvectors**\nWe make an identification on the covariance matrix: All three approaches yield the same eigenvectors and eigenvalue pairs: Identification of the covariance matrix after standardizing the data. Essence composition of the correlation matrix."},{"metadata":{"id":"wILOik2p8Yco","outputId":"e1bbf199-6ea4-425c-ce8f-e6e54eff3c5f","trusted":true},"cell_type":"code","source":"eigenvalues, eigenvectors = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eigenvectors[:5])\nprint('\\nEigenvalues \\n%s' %eigenvalues[:5])","execution_count":null,"outputs":[]},{"metadata":{"id":"LejpVHeulZvl","outputId":"58f11486-c649-46e3-8356-916dfc8e6a1d","trusted":true},"cell_type":"code","source":"len(eigenvalues)","execution_count":null,"outputs":[]},{"metadata":{"id":"UHYw1Mj_sYGv"},"cell_type":"markdown","source":"# **Step 3: Primary Component Selection**\nSorting Eigenpairs (Sorting of self-pairs)\n  The purpose of PCA is to reduce the dimensionality of the original feature space by projecting it into a smaller subspace where the eigenvectors will form the axes. However, the eigenvectors only describe the directions of the new axis, because they all have the same unit length 1.To decide which eigenvector (s) can be omitted without losing too much information, we need to examine the corresponding eigenvalues: Eigenvectors with the lowest eigenvalues carries little information; these can fall. The common approach is to order the eigenvalues from highest to lowest."},{"metadata":{"id":"mKfTYjODNg4m"},"cell_type":"markdown","source":"**Compute the variance of eigen values**\nWe select only first 6 features for this project"},{"metadata":{"id":"a23cf_sENmLI","outputId":"9c69f8c3-414f-4fb5-c904-a1403647319f","trusted":true},"cell_type":"code","source":"total_of_eigenvalues = sum(eigenvalues)\nvarariance = [(i / total_of_eigenvalues)*100 for i in sorted(eigenvalues, reverse=True)]\n\nvarariance[:50]","execution_count":null,"outputs":[]},{"metadata":{"id":"Nu5Wsa-MQtiR"},"cell_type":"markdown","source":"**As seen in the figure, the properties after 350 affect the target column by 0. These do not have any effect on the functioning of the model.**"},{"metadata":{"id":"th7S14NAgxyY","outputId":"6fa59754-3a15-4ec3-df80-416520f5dbd9","trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(15, 10))\n\n    plt.bar(range(len(eigenvalues)), varariance, alpha=0.8, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"_Co0s4XWgxSY","outputId":"c1b756df-32ea-4712-ee6d-8fe5dae70050","trusted":true},"cell_type":"code","source":"varariance[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"yNAvZ9BotZU5"},"cell_type":"markdown","source":"**Projection Matrix** \nThe projection matrix is used to transform the Input data (X) into the new property subspace. The Projection Matrix is a matrix of combined upper k eigenvectors. Here, we reduce the 4-dimensional feature space to a 2-dimensional feature subspace by selecting the \"first 2\" eigenvectors with the highest eigenvalues to construct our 2 dimensional eigenvector matrix."},{"metadata":{"id":"l1j3YniHqpnp","outputId":"1c0d6387-1a87-433b-f1c9-adcde6ae305e","trusted":true},"cell_type":"code","source":"eigenpairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\n\n# Sorting eigenvalues and eigenvectors from higher values to lower values\neigenpairs.sort(key=lambda x: x[0], reverse=True)\n\neigenpairs[0][0]","execution_count":null,"outputs":[]},{"metadata":{"id":"BNjvElOIsC-d","outputId":"4e46b206-5b92-49e0-9046-8b066cf3b90d","trusted":true},"cell_type":"code","source":"eigenpairs[5][1].shape","execution_count":null,"outputs":[]},{"metadata":{"id":"GTfgCF3Xs3uD","outputId":"875b4e34-1aa0-4578-bdc2-8e032c610c98","trusted":true},"cell_type":"code","source":"# only for 6 features \nmatrix_weighing = np.hstack((eigenpairs[0][1].reshape(754,1),\n                      eigenpairs[1][1].reshape(754,1),\n                      eigenpairs[2][1].reshape(754,1),\n                      eigenpairs[3][1].reshape(754,1),\n                      eigenpairs[4][1].reshape(754,1),\n                      eigenpairs[5][1].reshape(754,1)))\nmatrix_weighing","execution_count":null,"outputs":[]},{"metadata":{"id":"CHfhl3KJvbe-"},"cell_type":"markdown","source":"# **Step 4: Projection in a New Feature Space**"},{"metadata":{"id":"crk46hKxwHQD"},"cell_type":"markdown","source":"Projection into the New Feature Space In this last step, we will use the 754 × 6 dimensional projection matrix W to transform our samples into the new hexahedron through the equation Y = X × W."},{"metadata":{"id":"s2DqmqYKsDaY","outputId":"d9277f11-4b9d-483f-e3a7-ac99f1efbe39","trusted":true},"cell_type":"code","source":"Y = X.dot(matrix_weighing)\nY.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"dxzAktlSwuJz","outputId":"81b07603-9245-4dd5-fae3-b8ac9353ddf1","trusted":true},"cell_type":"code","source":"df[\"class\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"id":"nA3IHy8BwN0W","outputId":"57a0603d-61e1-4f74-a35e-92a1aa1d22b5","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('0', '1'), ('red', 'green')):\n        plt.scatter(Y[y==lab, 0], Y[y==lab, 1], label=lab, c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout();\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"id":"rdj9uFR40Msn"},"cell_type":"markdown","source":"# **Step 5: Principal Component Analysis (PCA)**"},{"metadata":{"id":"JcJXe2KSwOJ4","outputId":"bcb1a65f-8d16-41a4-e031-659945fef3a9","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,754,1)\nplt.grid()\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","execution_count":null,"outputs":[]},{"metadata":{"id":"-ThzaNZrR781"},"cell_type":"markdown","source":"**Division of training and test data**"},{"metadata":{"id":"FbpSjZtKZH-8","trusted":true},"cell_type":"code","source":"# eğitim ve test kümelerinin bölünmesi\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"sE-KZ82ASDeJ"},"cell_type":"markdown","source":"**Implementing Standard scaling data**"},{"metadata":{"id":"LqJ63b7lZRwa","outputId":"d571d475-b818-4467-c605-151ce03449dd","trusted":true},"cell_type":"code","source":"# Standard scaler haline getirme verileri\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nprint(\"X_train shape = \",X_train.shape)\nprint(\"X_test shape = \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"YZReZwvJSchE"},"cell_type":"markdown","source":"**Preparation of new data set to be used in training models. Principal Component Analysis(PCA) implementation. Feature extraction of the data set. And the size reduction has been done.**"},{"metadata":{},"cell_type":"markdown","source":"#**Note: ** I chose the (n_components)top 6 components with the highest variance. anyone can give a different number. It is an optional choice. Decide to process only 6 of the 754 features with the highest variance. It reduces the size very much and enables fast processing and only the most effective features will be processed. \n\n**PCA enabled only 6 variables to be processed instead of 754 variables.**"},{"metadata":{"id":"jDsJzwXFZT56","outputId":"3e467c32-23c8-450c-f965-9eb7c353f445","trusted":true},"cell_type":"code","source":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 6)\n\nX_train2 = pca.fit_transform(X_train) # sadece bir tane PCA ile çalışıyor aynı uzayda olması için\nX_test2 = pca.transform(X_test)       # test verisini eğitmiyoruz sadece transform uyguluyoruz\n\nprint(\"X_train2 shape = \",X_train2.shape)\nprint(\"X_test2 shape = \",X_test2.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"ZWLVtKtfBRwK"},"cell_type":"markdown","source":"# **Step 6: Comparison Accurancy 6 Machine Learning Models : before-after PCA**"},{"metadata":{"id":"peZlJJ0T4H0l"},"cell_type":"markdown","source":"## **1. Model : Logistic Regression**"},{"metadata":{"id":"UlsN_cugVVKD"},"cell_type":"markdown","source":"**Before PCA**"},{"metadata":{"id":"HSZ3fFjVZYef","outputId":"d841efd0-d451-4fa2-b5de-ec8c224fd285","trusted":true},"cell_type":"code","source":"#pca dönüşümünden önce gelen Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"OkPV1YP_V_CI"},"cell_type":"markdown","source":"**After PCA**"},{"metadata":{"id":"AGn2nFt6-mrJ","outputId":"f7155335-1292-44b8-9b66-a997abf09fab","trusted":true},"cell_type":"code","source":"\n#pca dönüşümünden sonra gelen LR\nclassifier2 = LogisticRegression(random_state=0)\nclassifier2.fit(X_train2,y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"mBDfSD6JWPZy"},"cell_type":"markdown","source":"**Success comparison of PCA and non-PCA models**"},{"metadata":{"id":"bvnuA4-w-mvO","trusted":true},"cell_type":"code","source":"#Predictions : tahminler\ny_pred = classifier.predict(X_test)    # without PCA\ny_pred2 = classifier2.predict(X_test2) # after PCA","execution_count":null,"outputs":[]},{"metadata":{"id":"fSbzxH2OXWdJ"},"cell_type":"markdown","source":"**Comparison between real and before PCA**"},{"metadata":{"id":"QhzJbte0-xfx","outputId":"92f0c637-28c0-4da1-a907-55f0e11d91fc","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, datasets, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n#actual / PCA olmadan çıkan sonuç\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"eSuMGS_pXYLn"},"cell_type":"markdown","source":"**Comparison between real and before PCA**"},{"metadata":{"id":"fxnCozeR-0Ed","outputId":"e01e3e7b-629f-4087-d029-9f357c9cf221","trusted":true},"cell_type":"code","source":"#actual / PCA sonrası çıkan sonuç\nprint(\"Comparison between real and after PCA \")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"lsUczHlcHVAI"},"cell_type":"markdown","source":"## **2. Model : Support Vector Machines (SVM)**"},{"metadata":{"id":"ZESzlGlpXIbf"},"cell_type":"markdown","source":"**Comparison between real and before PCA**"},{"metadata":{"id":"EG9JLOFFHXaf","outputId":"f4718bef-25c7-4f30-a686-562c0f961514","trusted":true},"cell_type":"code","source":"\n#Support Vector Machine\nfrom sklearn.svm import SVC\n \n\nclassifier = SVC()\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"Support Vector Machine:\")\n\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"EduFtCjtXLhp"},"cell_type":"markdown","source":"**Comparison between real and after PCA**"},{"metadata":{"id":"m9yzE_DCJzuW","outputId":"47296c7e-23e9-4e70-b988-93b7b5ee694b","trusted":true},"cell_type":"code","source":"\n#Support Vector Machine\nfrom sklearn.svm import SVC\n \n\nclassifier = SVC()\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\ncm = confusion_matrix(y_test,y_pred2)\naccuracy = accuracy_score(y_test,y_pred2)\nprint(\"Support Vector Machine:\")\n\nprint(\"Comparison between real and after PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"id":"7VAlJxArKkO5"},"cell_type":"markdown","source":"## **3. Model : Decision Tree Classifier**"},{"metadata":{"id":"caWyHZMKXibq"},"cell_type":"markdown","source":"**Comparison between real and before PCA**"},{"metadata":{"id":"PmDpH-o1KGBL","outputId":"2f5daa44-5d96-48ba-9fbf-428251abe0f6","trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.tree import DecisionTreeClassifier as DT\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nclassifier = DT(criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\n\nprint(\"Decision Tree Classifier :\")\n\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"qMRaCvPAXk3A"},"cell_type":"markdown","source":"**Comparison between real and after PCA**"},{"metadata":{"id":"rOe9fUVUKF6E","outputId":"5478ff51-0f41-4d0a-f789-5e2f7eb60f15","trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n\n\nfrom sklearn.tree import DecisionTreeClassifier as DT\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nclassifier = DT(criterion='entropy', random_state=0)\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\n\n\nprint(\"Comparison between real and after PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"id":"4xC8iNs9FlM8"},"cell_type":"markdown","source":"## **4. Model : KNN(k-nearest neighbors algorithm)**"},{"metadata":{"id":"OOJxW622XwxS"},"cell_type":"markdown","source":"**Comparison between real and before PCA**"},{"metadata":{"id":"DhnDh0QUFBhA","outputId":"037b82e2-955a-471a-b41c-deea53454938","trusted":true},"cell_type":"code","source":"\nfrom sklearn import neighbors, datasets, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nknn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(\"K-Neighbors Classifier :\")\n\nprint(\"Comparison between real and before PCA\")\n\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"U_y8g84WYbQ-"},"cell_type":"markdown","source":"**Comparison between real and after PCA**"},{"metadata":{"id":"J6u2QnsHGGbG","outputId":"8f450d25-371c-457c-8b36-fabe2c8d78d2","trusted":true},"cell_type":"code","source":"knn = neighbors.KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train2, y_train)\ny_pred2 = knn.predict(X_test2)\n\nprint(\"Comparison between real and after PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"id":"dsO6zU0YKdjY"},"cell_type":"markdown","source":"## **5. Model : Random Forest Classifier**"},{"metadata":{"id":"zfZtXSgcYYnj"},"cell_type":"markdown","source":"**Comparison between real and before PCA**"},{"metadata":{"id":"8dh7T0lPKd0U","outputId":"27d1c0f0-a41c-4f72-c810-8b40dd6a3b41","trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestClassifier as RF\n\nclassifier = RF(n_estimators=10, criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\n\nprint(\"Random Forest Classifier :\")\n\nprint(\"Comparison between real and before PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"zsLIzBblY4Bn"},"cell_type":"markdown","source":"**Comparison between real and after PCA**"},{"metadata":{"id":"1tDexc6zLkcW","outputId":"c3f483d3-d37e-47e9-f7ee-5b8c27a60e1d","trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestClassifier as RF\n\nclassifier = RF(n_estimators=10, criterion='entropy', random_state=0)\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\n\nprint(\"Comparison between real and after PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"id":"9r_Va04aKC5J"},"cell_type":"markdown","source":"## ** 6. Model:  Gaussian Naive Bayes**"},{"metadata":{"id":"Z45wq-KMZAoA"},"cell_type":"markdown","source":"**Comparison between real and before PCA**"},{"metadata":{"id":"3tFR2UamKF0_","outputId":"37426a1a-0364-49f3-b0ea-b2e4ad9d01ed","trusted":true},"cell_type":"code","source":"\n#Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\n\nprint(\"Gaussian Naive Bayes :\")\n\nprint(\"Comparison between real and before PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred))\nprint('Classification \\n', classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"LGJCyLXtZDYz"},"cell_type":"markdown","source":"**Comparison between real and after PCA**"},{"metadata":{"id":"GpoFhG3oKKi0","outputId":"537afb29-b267-4e53-872b-77bb36215120","trusted":true},"cell_type":"code","source":"\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train2,y_train)\ny_pred2 = classifier.predict(X_test2)\n\nprint(\"Comparison between real and after PCA\")\nprint('Accuracy Score:', accuracy_score(y_test, y_pred2))\nprint('Confusion matrix \\n',  confusion_matrix(y_test, y_pred2))\nprint('Classification \\n', classification_report(y_test, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It is observed that reducing the size with PCA, that is, reducing the number of variables, has a positive effect on the success score of some machine learning classification models. It is possible to produce more effective and faster solutions by taking a small amount of data loss. Reducing dimensions with PCA will provide us with great convenience, especially in studies related to Big Data.**"},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}