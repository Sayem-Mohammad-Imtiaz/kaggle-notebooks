{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\nimport pandas as pd\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk \nimport string\nimport re\nimport numpy as np\nfrom wordcloud import WordCloud \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # function for splitting data to train and test sets\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\n\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\ntrain= pd.read_csv(\"../input/train.csv\", sep=',', error_bad_lines=False , encoding ='ISO-8859-1')\ntest = pd.read_csv(\"../input/test.csv\", sep=',', error_bad_lines=False , encoding ='ISO-8859-1')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.rename(columns={\"SentimentText;;;;;;;;;;;;\": \"SentimentText\"}) \nprint(train.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.rename(columns={\"SentimentText;;;;;;;;;;;;;;;;;;;\": \"SentimentText\"}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('ItemID')['SentimentText'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('ItemID')['SentimentText'].count().mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.ItemID.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.ItemID.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SentimentText'] = train['SentimentText'].astype(str) \ntest['SentimentText'] = test['SentimentText'].astype(str) \nprint('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['SentimentText'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['SentimentText'].apply(lambda x: len(x.split())))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['SentimentText'].notna()]\ntrain = train[train['Sentiment'].notna()]\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value = train['Sentiment'].value_counts()\nvalue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1 = train[~train['Sentiment'].isin([\"0\",\"1\"])]\nvalue_counts_sentiment  = train_1['Sentiment'].value_counts()\ntrainG = pd.merge(train,train_1, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\ntrainG","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_counts_sentiment  = trainG['Sentiment'].value_counts()\nvalue_counts_sentiment ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test[test['SentimentText'].notna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"patterns= ['[^!.?]+']\n\nfor tweet in trainG['SentimentText'] :\n    #Convert to lower case\n         tweet = tweet.lower()\n         #Convert www.* or https?://* to URL\n         tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n         #Convert @username to AT_USER\n         tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n         #Remove additional white spaces\n         tweet = re.sub('[\\s]+', ' ', tweet)\n         #Replace #word with word\n         tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n         #trim\n         tweet = tweet.strip('\\'\"')\n         \n         \ntest['SentimentText'] = test['SentimentText'].astype(str)\nfor tweet in test['SentimentText'] :\n    #Convert to lower case\n         tweet = tweet.lower()\n         #Convert www.* or https?://* to URL\n         tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n         #Convert @username to AT_USER\n         tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n         #Remove additional white spaces\n         tweet = re.sub('[\\s]+', ' ', tweet)\n         #Replace #word with word\n         tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n         #trim\n         tweet = tweet.strip('\\'\"')       \nfor tweet in trainG['SentimentText'] :\n    for p in patterns:\n        tweet = re.findall(p, tweet)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainG","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nstrip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n\nfor string in trainG['SentimentText']:\n    \n    string = string.lower().replace(\"<br />\", \" \")\n    string = re.sub(strip_special_chars, \"\", string.lower())    \n    \ntrainG","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainG.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainG['SentimentText'] = trainG['SentimentText'].astype(str)\ntrainG = trainG[trainG['SentimentText'].notna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean text from noise\ndef clean_text(text):\n    #Filter to allow only alphabets\n    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    \n    #Remove Unicode characters\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    \n    #Convert to lowercase to maintain consistency\n    text = text.lower()\n       \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrainG['SentimentText'] = trainG['SentimentText'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(text): \n    tknzr = TweetTokenizer()\n    return tknzr.tokenize(text)\n\ndef stem(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n\nen_stopwords = set(stopwords.words(\"english\")) \n\nvectorizer = CountVectorizer(\n    analyzer = 'word',\n    tokenizer = tokenize,\n    lowercase = True,\n    ngram_range=(1, 1),\n    stop_words = en_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#data=pd.concat([trainG,test],sort=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data['SentimentText'] = data['SentimentText'].astype(str) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Clean text from noise\ndef clean_text(text):\n    #Filter to allow only alphabets\n    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    \n    #Remove Unicode characters\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    \n    #Convert to lowercase to maintain consistency\n    text = text.lower()\n       \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data['SentimentText']  =data['SentimentText'] .apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = trainG.copy()\ndata.drop('ItemID', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.isnull().any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Sentiment'] = data['Sentiment'].astype('category')\nprint(type(data['Sentiment'][0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['label_id'] = data['Sentiment'].cat.codes\ndata['label_id'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainG.to_excel(\"trainG_clean.xlsx\", sheet_name='global', index=False)\ntest.to_excel(\"test_clean.xlsx\",sheet_name = \"gloabl\" , index = False) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}