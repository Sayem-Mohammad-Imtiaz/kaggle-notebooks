{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ESERCIZIO 2: \nImplement polynomial regression using the Ridge Regression method available in scikit-learn, see sklearn.linear model.Ridge() and look at the behavior of the solution when changing the parameter alpha (ùõº)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plot\nfrom sklearn.linear_model import Ridge # ridge\nfrom sklearn.preprocessing import PolynomialFeatures # PolynomialFeatures, per costruire equazioni di grado superiore al primo\nfrom sklearn.pipeline import make_pipeline # make_pipeline, per costruire equazioni di grado superiore al primo\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creazione del dataframe iris"},{"metadata":{},"cell_type":"markdown","source":"Caricamento dei dati dal dataset iris:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"iris = pd.read_csv('../input/iris/Iris.csv', nrows=1500, usecols=[0, 3], encoding='latin-1') # creazione del dataframe con campi identifivo del fiore (Id) e lunghezza del petalo in cm (PetalLengthCm)\niris.head(10) # stampa delle prime 10 righe del dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analisi dei dati"},{"metadata":{},"cell_type":"markdown","source":"Rappresento le relazioni tra l'identificativo del fiore e la lunghezza del suo petalo (in cm)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [15, 10]\n\nplt.xlabel('Id flowers')\nplt.ylabel('Petal length (cm)')\nplt.scatter(iris.Id, iris.PetalLengthCm, marker='o', color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Il grafico mette in evidenza, visuale, come i fiori si posizionano su 3 categorie:\n- da id 0 a circa 50, con lunghezza del petalo che varia da 1 a 1.8 cm circa;\n- da id 51 a circa 100, con lunghezza del petalo che varia da 3 a 5 cm circa;\n- da id 101 a circa 160, con lunghezza del petalo che varia da 5.1 a 6.8 cm circa.\n\nMi sembra, dunque, corretto poter classificare i fiori in piccoli, medi o grandi."},{"metadata":{},"cell_type":"markdown","source":"# Applicazione della Ridge Regression Linear"},{"metadata":{},"cell_type":"markdown","source":"La Ridge Regression e' una tecnicha di restringimento (regolarizzazione), che utilizza parametri e valori diversi, per ridurre o penalizzare i coefficienti; in modo da poter descrivere, nel modo migliore, l'andamento dei dati.\n\nLo scopo e' quello di ridurre al minimo la funzione obiettivo:\n\n||y - Xw||^2_2 + alpha * ||w||^2_2\n\nin modo da risolvere un modello di regressione in cui:\n1. la funzione di perdita' e' la funzione dei minimi quadrati lineari;\n2. la regolarizzazione viene fornita dalla norma L2. La cui, contiene una penalita', che permette:\n    *  che le variabili con un contributo inferiore al risultato non siano annullate, come accadrebbe invece utilizzando un calcolo di frequenza. Ma che gli venga attribuito un valore vicino allo 0. \n    * di evitare il problema del overfitting (apprendimento esclusivo su una sola parte del dominio a causa, per esempio, di un numero di variabili superiore ai fenomeni osservati), attribuendo un peso inferiore a variabili che descrivono la stessa proprieta'.\n    \nIn questo modo si riduce la complessita' del modello, senza l'eliminazione di variabili, non memorizzando esclusivamente i dati di addestramento, ma dando la possibilita' di generalizzare su nuovi dati."},{"metadata":{},"cell_type":"markdown","source":"**Il valore alpha**\n\nLa penalita', che chiamero', alpha e' fondamentale per stimare la retta di regressione (ridge) adeguatamente.\n\nQuando alpha e' uguale a 0, nessuna penalita' ha effetto sulla regressione, che produrra' dei minimi quadrati classici, non rilevando variabili con un basso contributo e tendendo all'overfitting. Invece quando alpha cresce, avvicinandosi a infinito, l'impatto che la penalita' ha sulla regressione aumenta, portando tutti i coefficienti di regressione prossimi allo 0. Dunque la soluzione migliore, sembra quella di trovare un'alpha medio del modello di dati in esame.\n\nDi seguito mostro alcuni casi di variazione di alpha."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applicazione della Ridge Regression con alpha uguale a 0, su equazione di primo grado\n\nlength_petal = list(iris.PetalLengthCm)\n\nx_train = []\nfor y in iris.index:\n    x_train.append([y])\n    \nridg = Ridge(alpha=0) # riduce al minimo la funzione obiettivo: ||y - Xw||^2_2 + alpha * ||w||^2_2 con alpha a 0\nridg.fit(x_train, length_petal) # linearizza il modello\nprediction = ridg.predict(x_train) # effettua la previsione utilizzando il modello lineare \n\n# disegno del plot\nplt.xlabel('Id flowers')\nplt.ylabel('Petal Length (cm)')\nplt.scatter(iris.Id, iris.PetalLengthCm, marker='o', color='black')\nplt.plot(iris.Id, prediction, color='orange', linewidth='1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'andamento dei dati, descritti dalla retta di regressione, si presenta crescere in modo costante, di pari passo con l'aumentare dell'identificativo del fiore.\nQuesto prova in modo formale l'esistenza delle 3 cateogire dimensionali (piccoli, medi, grandi). Inoltre, come si nota dalla figura, i punti che hanno posizioni differenti da quelli medi, non vengono presi in considerazione, prediligendo situazioni di overfitting (come mi aspettavo)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# alpha uguale a 1\nridg1 = Ridge(alpha=1) # riduce al minimo la funzione obiettivo: ||y - Xw||^2_2 + alpha * ||w||^2_2 con alpha a 1\nridg1.fit(x_train, length_petal) # linearizza il modello\nprediction1 = ridg1.predict(x_train) # effettua la previsione utilizzando il modello lineare\n\n# alpha uguale a 100\nridg2 = Ridge(alpha=100) # riduce al minimo la funzione obiettivo: ||y - Xw||^2_2 + alpha * ||w||^2_2 con alpha a 100\nridg2.fit(x_train, length_petal) # linearizza il modello\nprediction2 = ridg2.predict(x_train) # effettua la previsione utilizzando il modello lineare\n\n\n# disegno plot\nplt.xlabel('Id flowers')\nplt.ylabel('Petal Length (cm)')\nplt.scatter(iris.Id, iris.PetalLengthCm, marker='o', color='black')\nplt.plot(iris.Id, prediction, color='orange', linewidth='1')\nplt.plot(iris.Id, prediction1, 'r--', color='purple')\nplt.plot(iris.Id, prediction2, '.', color='green')\nplt.legend(['alpha = 0','alpha = 1','alpha = 100'], numpoints=1)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Come si vede dalla figura sopra, i valori di alpha assegnati (1 e 100) sono troppi piccoli, per poter rendere evidente delle oscillazioni, nella descrizione dell'andamento dei dati, rispetto ad alpha uguale a 0 (la cui linea viene addirittura coperta in figura).\n\nPer rilevare delle variazione nella rappresentazione, devo aumentare notevolmente il valore della panalita' (prendendo due casi limiti, ad es. alpha = 50 000 e 1 000 000)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# alpha uguale a 50000\nridg3 = Ridge(alpha=50000) # riduce al minimo la funzione obiettivo: ||y - Xw||^2_2 + alpha * ||w||^2_2 con alpha a 50000\nridg3.fit(x_train, length_petal) # linearizza il modello\nprediction3 = ridg3.predict(x_train) # effettua la previsione utilizzando il modello lineare\n\n# alpha uguale a 1000000\nridg4 = Ridge(alpha=1000000) # riduce al minimo la funzione obiettivo: ||y - Xw||^2_2 + alpha * ||w||^2_2 con alpha a 1000000\nridg4.fit(x_train, length_petal) # linearizza il modello\nprediction4 = ridg4.predict(x_train) # effettua la previsione utilizzando il modello lineare\n\n\n\n# disegno plot\nplt.xlabel('Id flowers')\nplt.ylabel('Petal Length (cm)')\nplt.scatter(iris.Id, iris.PetalLengthCm, marker='o', color='black')\nplt.plot(iris.Id, prediction, color='orange', linewidth='1')\nplt.plot(iris.Id, prediction3, 'r--', color='blue')\nplt.plot(iris.Id, prediction4, 'r--', color='pink')\nplt.legend(['alpha = 0','alpha = 50000','alpha = 1000000'], numpoints=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La figura sopra, mostra un aspetto interessante.\n\nNel caso di alpha 50 000 (linea tratteggiata blu) le variabili o punti, che hanno valore differente della media, dei punti, cominciano a incidere sulla posizione della retta di regressione. Questo appare evidente nei primi identificativi.\nPer, invece, alpha uguale a 1 000 000 (linea tratteggiata rosa), i coefficienti delle variabili iniziano a tendere a 0, sinomino di valore di alpha eccessivo per permettere una corretta rappresentazione dei dati."},{"metadata":{},"cell_type":"markdown","source":"**Polinomi di grado superiore al primo**\n\n\nDi seguito studio cosa accade incrementando il grado della retta di regressione con alpha a 0, 50 000 e 1 000 000."},{"metadata":{"trusted":true},"cell_type":"code","source":"# polinomio di secondo grado\n\nalphas = [0, 50000, 1000000] # valore di alpha, mi baso sui valori provati con la retta\nprediction = []\nfor n in range(0, 3): # per i 3 valori di alpha\n    # PolynomialFeatures: genera caratteristiche polinomiali e di interazione (combinazione dei coefficienti da grado 0 a 2)\n    # make_pipeline: costruisce una pipeline dagli stimatori dati\n    ridg = make_pipeline(PolynomialFeatures(2), Ridge(alpha=alphas[n])) \n    ridg.fit(x_train, length_petal)\n    prediction.append(ridg.predict(x_train))\n\n# disegno plot\nplt.xlabel('Id flowers')\nplt.ylabel('Petal Length (cm)')\nplt.scatter(iris.Id, iris.PetalLengthCm, marker='o', color='black')\nplt.plot(iris.Id, prediction[0], color='orange', linewidth='1')\nplt.plot(iris.Id, prediction[1], 'r--', color='blue')\nplt.plot(iris.Id, prediction[2], 'r--', color='pink')\nplt.legend(['alpha = 0','alpha = 50000','alpha = 1000000'], numpoints=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inoltre metto a confronto le funzioni di primo, secondo e quarto grado con parametro alpha 50 000 (che dal mio studio risulta in grado di fornire una buona approssimazione)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confronto fra le equazioni polinomiali di 1, 2 e 4 grado con alfa 50 000\n\n\nalpha = 50000\nprediction = []\ndegree_eq = [1,2,4]\nfor n in degree_eq: # iterazione sui gradi dell'equazione\n    ridg = make_pipeline(PolynomialFeatures(n), Ridge(alpha)) \n    ridg.fit(x_train, length_petal)\n    prediction.append(ridg.predict(x_train))\n\n# disegno plot\nplt.xlabel('Id flowers')\nplt.ylabel('Petal Length (cm)')\nplt.scatter(iris.Id, iris.PetalLengthCm, marker='o', color='black')\nplt.plot(iris.Id, prediction[0], color='green', linewidth='1')\nplt.plot(iris.Id, prediction[1], 'r--', color='orange')\nplt.plot(iris.Id, prediction[2], color='purple', linewidth='1')\nplt.legend(['equazione di grado 1','equazione di grado 2', 'equazione di grado 4'], numpoints=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I due grafici, appena plottati, rendono evidente come con l'aumentare del grado, che chiamo p, del polinomio di regressione, aumenta anche l'accuratezza con il quale la funzione approssima i valori del dataset.\n\nGia', con un polinomio di secondo grado e con alpha = 0 (linea continua arancio del primo grafico) si ottiene una buona rappresentazione dei dati.\nTuttavia dalla teoria so, che un aumento eccessivo di p causa complessita' nello spazio delle ipotesi H, che non risulta piu' in grado di generalizzare bene, e quindi inefficace nel prevedere il risultato per coppie di valori mai viste prima.\n\n\n\nInoltre, per concludere questa trattazione, nel caso specifico del dataset iris, quando il grado del polinomio supera il quarto grado, il risultato perde di accuratezza. Questo mi fa pensare, a causa di una problema mal condizionato. Difatti a mano a mano che il grado del polinomio aumenta, le tre funzioni, anche se con alpha differenti, si avvicinano fino a collassare; come si puo' vedere dal grafico seguente."},{"metadata":{"trusted":true},"cell_type":"code","source":"# polinomio di quarto grado\n\nalphas = [0, 50000, 1000000] # valore di alpha, mi baso sui valori provati con la retta\nprediction = []\nfor n in range(0, 3): # per i 3 valori di alpha\n    ridg = make_pipeline(PolynomialFeatures(4), Ridge(alpha=alphas[n])) \n    ridg.fit(x_train, length_petal)\n    prediction.append(ridg.predict(x_train))\n\n# disegno plot\nplt.xlabel('Id flowers')\nplt.ylabel('Petal Length (cm)')\nplt.scatter(iris.Id, iris.PetalLengthCm, marker='o', color='black')\nplt.plot(iris.Id, prediction[0], color='orange', linewidth='1')\nplt.plot(iris.Id, prediction[1], 'r--', color='blue')\nplt.plot(iris.Id, prediction[2], 'r--', color='pink')\nplt.legend(['alpha = 0','alpha = 50000','alpha = 1000000'], numpoints=1)\nplt.show()\n\n# dal quinto grado in poi il risultato non e' piu' accurato. Fa pensare che il problema sia mal condizionato","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}