{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Kaggle practise\n#### Pima dibetes india dataset\n     using Artifical neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd  #import pandas and EDA\ndf = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().any() # isnull is used to analysis the anymore NAN is occured and how many null value is here to be analysised","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df,hue = 'Outcome')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('Outcome',axis=1).values  ####independent features\ny= df['Outcome'].values  #### dependent features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create the tensor\n#Create independent values, input values used only for float \n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\n\n# create dependent value, output values never required for float so that using integer\n\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape  #identify the rows and columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Creating model with pytorch\n#input is 8, hidden1,2 set if u want and lossfunction and optimizer is used \n\nclass ANN_Model(nn.Module):\n    def __init__(self,input_features = 8,hidden1=20,hidden2=60,out_features=2):\n        super().__init__()\n        self.f_connected1 = nn.Linear(input_features,hidden1)\n        self.f_connected2 = nn.Linear(hidden1,hidden2)\n        self.out = nn.Linear(hidden2,out_features)\n    \n    def forward(self,x):\n        x = F.relu(self.f_connected1(x))\n        x=F.relu(self.f_connected2(x))\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Instantiate my ANN model\ntorch.manual_seed(20)     #seed is permanently fixed the values\nmodel = ANN_Model()  #assign the previous function stored variabe name model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.parameters  #what are the parameters assign the function in the neural network","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####backward propagation-----> define the loss function\n\n#lossfunction --> multifeature using crossentropy\nloss_function = nn.CrossEntropyLoss()\n#Reduce the loss using optimizer best one adam and minibatch (using learningrate(lr)= 0.01,adam optimizer is used)\noptimizer = torch.optim.Adam(model.parameters(),lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####Assume the epochs and finalloss\n###epochs and finalloss ---> Its eveytime flow the all layer and apply the optimizer to reduce the loss in the process of backpropagation\n\nepochs = 500\nfinalloss = []\nfor i in range(epochs):    #execute the back_propagation \n    i=i+1\n    y_predict = model.forward(X_train)  #forwardly moved\n    loss = loss_function(y_predict,y_train)\n    finalloss.append(loss)\n    if i%10 == 1:\n        print(\"Epoch number:{} and the loss : {}\".format(i,loss.item()))\n    optimizer.zero_grad()    ###Clears the gradients of all optimized \n    loss.backward()  #assign to backward for the reduce the loss and optimizer to assign the increase the accuracy\n    optimizer.step()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###plot the loss function and different the visualize the method like matplot and seaborn\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(range(epochs),finalloss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### prediction in x_test data\n\npredictions = []\nwith torch.no_grad(): #nograd is the assign the 0 and 1\n    for i,data in enumerate(X_test):\n        y_predict =(model(data))\n        predictions.append(y_predict.argmax().item()) #argmax is assign in 0 and 1\n        print(y_predict.argmax().item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions #predictions is used for binary values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix ----> easily identify the prediction visualize   "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix \nCon_Mat= confusion_matrix(y_test,predictions)\nCon_Mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(15,8))\nsns.heatmap(Con_Mat,annot = True,cmap=\"cubehelix\")\nplt.xlabel('Actualvalues')\nplt.ylabel('Predictedvalues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score as Acs\nscore = Acs(y_test,predictions)\nscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####save the model\ntorch.save(model,'pimadiabetes.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Modeldevelop =torch.load('pimadiabetes.pt')  #load the prepare model\nModeldevelop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Modeldevelop.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### load and predictions of new data\n###How to predict\nlist(df.iloc[0,:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdata=[[6.0, 150.0, 80.0, 38.0, 0.0, 80.6, 0.907, 75.0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"newdatatensor = torch.tensor(newdata)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### prediction in assign new data and predict x_test\n##identify the prediction using model\n\nwith torch.no_grad(): #nograd is the assign the 0 and 1\n    #print(Modeldevelop(newdatatensor))\n    print(Modeldevelop(newdatatensor).argmax().item()) #output is recognised","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}