{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:13:55.726749Z","iopub.execute_input":"2021-06-03T13:13:55.727092Z","iopub.status.idle":"2021-06-03T13:13:55.859596Z","shell.execute_reply.started":"2021-06-03T13:13:55.727058Z","shell.execute_reply":"2021-06-03T13:13:55.858821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('mask_binary')\nread_img = '/kaggle/input/makeup-lips-segmentation-28k-samples/set-lipstick-original/720p/'\nread_mask = '/kaggle/input/makeup-lips-segmentation-28k-samples/set-lipstick-original/mask/'\nsave_path = '/kaggle/working/mask_binary'","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:15:20.818441Z","iopub.execute_input":"2021-06-03T13:15:20.818767Z","iopub.status.idle":"2021-06-03T13:15:20.822564Z","shell.execute_reply.started":"2021-06-03T13:15:20.818731Z","shell.execute_reply":"2021-06-03T13:15:20.821752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_list = []\nfor dirname, _, filenames in os.walk(read_mask):\n    for filename in filenames:\n        mask_list.append(filename)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:14:01.397552Z","iopub.execute_input":"2021-06-03T13:14:01.397868Z","iopub.status.idle":"2021-06-03T13:14:51.667886Z","shell.execute_reply.started":"2021-06-03T13:14:01.397838Z","shell.execute_reply":"2021-06-03T13:14:51.667049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/makeup-lips-segmentation-28k-samples/set-lipstick-original/list.csv', delimiter=',', nrows = 2000)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:14:51.66925Z","iopub.execute_input":"2021-06-03T13:14:51.669576Z","iopub.status.idle":"2021-06-03T13:14:51.697538Z","shell.execute_reply.started":"2021-06-03T13:14:51.669541Z","shell.execute_reply":"2021-06-03T13:14:51.696827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:14:56.86853Z","iopub.execute_input":"2021-06-03T13:14:56.869209Z","iopub.status.idle":"2021-06-03T13:14:56.902856Z","shell.execute_reply.started":"2021-06-03T13:14:56.869149Z","shell.execute_reply":"2021-06-03T13:14:56.901809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Преобразование маски в монохромное изображение\ndef mask_b(x):\n    mask = cv2.imread(os.path.join(read_mask, x), cv2.IMREAD_GRAYSCALE)\n    (thresh, mask_binary) = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n    \n    cv2.imwrite(os.path.join(save_path, x), mask_binary)\n    return os.path.join(save_path, x)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:15:00.947188Z","iopub.execute_input":"2021-06-03T13:15:00.947527Z","iopub.status.idle":"2021-06-03T13:15:00.952611Z","shell.execute_reply.started":"2021-06-03T13:15:00.947497Z","shell.execute_reply":"2021-06-03T13:15:00.951847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = []\nnew_mask = []\nfor index, row in df.iterrows():\n    if row['mask'] not in mask_list:\n        img_path = os.path.join(read_img, row['filename'])\n        new_mask_path = 'not found'\n        img.append(img_path)\n        new_mask.append(new_mask_path)\n    else:    \n        img_path = os.path.join(read_img, row['filename'])\n        new_mask_path = mask_b(row['mask'])\n        img.append(img_path)\n        new_mask.append(new_mask_path)\ndf['filename'] = img\ndf['mask'] = new_mask","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:15:25.787352Z","iopub.execute_input":"2021-06-03T13:15:25.787675Z","iopub.status.idle":"2021-06-03T13:15:57.414086Z","shell.execute_reply.started":"2021-06-03T13:15:25.787645Z","shell.execute_reply":"2021-06-03T13:15:57.413244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(index = df.loc[(df['mask']=='not found'),].index)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:16:02.677248Z","iopub.execute_input":"2021-06-03T13:16:02.67756Z","iopub.status.idle":"2021-06-03T13:16:02.684715Z","shell.execute_reply.started":"2021-06-03T13:16:02.677531Z","shell.execute_reply":"2021-06-03T13:16:02.683768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torch\nfrom torch.nn import functional as F\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:16:04.797655Z","iopub.execute_input":"2021-06-03T13:16:04.798041Z","iopub.status.idle":"2021-06-03T13:16:05.871871Z","shell.execute_reply.started":"2021-06-03T13:16:04.797983Z","shell.execute_reply":"2021-06-03T13:16:05.871024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[0,8]\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:16:06.287536Z","iopub.execute_input":"2021-06-03T13:16:06.28788Z","iopub.status.idle":"2021-06-03T13:16:06.312861Z","shell.execute_reply.started":"2021-06-03T13:16:06.287848Z","shell.execute_reply":"2021-06-03T13:16:06.312115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-03T11:13:48.541776Z","iopub.execute_input":"2021-06-03T11:13:48.542096Z","iopub.status.idle":"2021-06-03T11:13:48.545882Z","shell.execute_reply.started":"2021-06-03T11:13:48.542067Z","shell.execute_reply":"2021-06-03T11:13:48.54498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDatasetFromImages(Dataset):\n    def __init__(self, data_info):\n        # Подаем наш подготовленный датафрейм\n        self.data_info = data_info\n        \n        # Разделяем датафрейм на rgb картинки \n        self.image_arr = self.data_info.iloc[:,0]\n        # и на сегментированные картинки\n        self.label_arr = self.data_info.iloc[:,8]\n        \n        # Количество пар картинка-сегментация\n        self.data_len = len(self.data_info.index)\n        \n        # convert str names to class values on masks\n        # Конвертируем стринговые имена в значения класса на маске\n        self.class_values = [labels.index(cls) for cls in labels]\n        \n    def __getitem__(self, index):\n        # Читаем картинку и сразу же представляем ее в виде numpy-массива \n        # размера 600х800 float-значний\n        img = Image.open(self.image_arr[index])\n        # Преобразовываем к размеру 256х256\n        img = img.resize((256,256))\n        img = np.asarray(img).astype('float')\n        # Нормализуем изображение в значениях [0,1]\n        img = torch.as_tensor(img)/255    \n        # Количество каналов ставим на первый план - этого хочет pytorch\n        img = img.permute(2,0,1).float()\n        \n        # Считываем нашу маску\n        mask = np.asarray(plt.imread(self.label_arr[index]))[:,:]*255\n\n        # Здесь мы создаем список бинарных масок из нашей одной общей маски \n        masks = [(mask == v) for v in self.class_values]\n        # Стекаем все вместе в один многомерный тензор масок\n        mask = np.stack(masks, axis=-1).astype('float')\n        #  Приводим к типу тензора\n        mask = torch.as_tensor(mask)\n        # Размерность каналов на передний план\n        mask = mask.permute(2,0,1)\n        \n        # делаем ресайз маски на 256х256\n        # Для этого используем функцию interpolate\n        ### Что бы ресайзить и высоту и ширину картинки, нужно перед interpolate\n        ### пороизвести unsqueeze над тензором, и squeeze после.\n        # unsqueeze - меняет размерность img c (256, 256, 3) -> (1, 256, 256, 3),\n        mask = mask.unsqueeze(0)\n        mask = F.interpolate(input=mask, size=256, mode='nearest')\n        mask=mask.squeeze(0).squeeze(0)\n        \n        \n        return (img, mask)\n\n    def __len__(self):\n        return self.data_len","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:34:03.162101Z","iopub.execute_input":"2021-06-03T13:34:03.162442Z","iopub.status.idle":"2021-06-03T13:34:03.173404Z","shell.execute_reply.started":"2021-06-03T13:34:03.16241Z","shell.execute_reply":"2021-06-03T13:34:03.172264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super(UNet, self).__init__()\n        self.num_classes = num_classes\n\n        # Левая сторона (Путь уменьшения размерности картинки)\n        self.down_conv_11 = self.conv_block(in_channels=3, out_channels=64)\n        self.down_conv_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_21 = self.conv_block(in_channels=64, out_channels=128)\n        self.down_conv_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_31 = self.conv_block(in_channels=128, out_channels=256)\n        self.down_conv_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.down_conv_41 = self.conv_block(in_channels=256, out_channels=512)\n        self.down_conv_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n        \n        # Правая сторона (Путь увеличения размерности картинки)\n        self.up_conv_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_12 = self.conv_block(in_channels=1024, out_channels=512)\n        self.up_conv_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_22 = self.conv_block(in_channels=512, out_channels=256)\n        self.up_conv_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_32 = self.conv_block(in_channels=256, out_channels=128)\n        self.up_conv_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.up_conv_42 = self.conv_block(in_channels=128, out_channels=64)\n        \n        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        self.softmax = nn.Softmax()\n    \n    @staticmethod\n    def conv_block(in_channels, out_channels):\n        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels),\n                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(),\n                                    nn.BatchNorm2d(num_features=out_channels))\n        return block\n    \n    @staticmethod\n    def crop_tensor(target_tensor, tensor):\n        target_size = target_tensor.size()[2]\n        tensor_size = tensor.size()[2]\n        delta = tensor_size - target_size\n        delta = delta // 2\n\n        return tensor[:,:, delta:tensor_size-delta, delta:tensor_size-delta]\n\n\n    def forward(self, X):\n        # Проход по левой стороне\n        x1 = self.down_conv_11(X) # [-1, 64, 256, 256]\n        x2 = self.down_conv_12(x1) # [-1, 64, 128, 128]\n        x3 = self.down_conv_21(x2) # [-1, 128, 128, 128]\n        x4 = self.down_conv_22(x3) # [-1, 128, 64, 64]\n        x5 = self.down_conv_31(x4) # [-1, 256, 64, 64]\n        x6 = self.down_conv_32(x5) # [-1, 256, 32, 32]\n        x7 = self.down_conv_41(x6) # [-1, 512, 32, 32]\n        x8 = self.down_conv_42(x7) # [-1, 512, 16, 16]\n        \n        middle_out = self.middle(x8) # [-1, 1024, 16, 16]\n\n        # Проход по правой стороне\n        x = self.up_conv_11(middle_out) # [-1, 512, 32, 32]\n        y = self.crop_tensor(x, x7)\n        x = self.up_conv_12(torch.cat((x, y), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n        \n        x = self.up_conv_21(x) # [-1, 256, 64, 64]\n        y = self.crop_tensor(x, x5)\n        x = self.up_conv_22(torch.cat((x, y), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n        \n        x = self.up_conv_31(x) # [-1, 128, 128, 128]\n        y = self.crop_tensor(x, x3)\n        x = self.up_conv_32(torch.cat((x, y), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n        \n        x = self.up_conv_41(x) # [-1, 64, 256, 256]\n        y = self.crop_tensor(x, x1)\n        x = self.up_conv_42(torch.cat((x, y), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n        \n        output = self.output(x) # [-1, num_classes, 256, 256]\n        output = self.softmax(output)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:36:53.59293Z","iopub.execute_input":"2021-06-03T13:36:53.593294Z","iopub.status.idle":"2021-06-03T13:36:53.612725Z","shell.execute_reply.started":"2021-06-03T13:36:53.593263Z","shell.execute_reply":"2021-06-03T13:36:53.611788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nlabels = [ 'Unlabeled','Lips']\n\n# 70 % в тренировочную выборку, 30 - в тестовую\nX_train, X_test = train_test_split(df,test_size=0.3)\n\n# Упорядочиваем индексацию\nX_train.reset_index(drop=True,inplace=True)\nX_test.reset_index(drop=True,inplace=True)\n\n# Оборачиваем каждую выборку в наш кастомный датасет\ntrain_data = CustomDatasetFromImages(X_train)\ntest_data = CustomDatasetFromImages(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:56:59.267927Z","iopub.execute_input":"2021-06-03T13:56:59.268267Z","iopub.status.idle":"2021-06-03T13:56:59.276272Z","shell.execute_reply.started":"2021-06-03T13:56:59.268235Z","shell.execute_reply":"2021-06-03T13:56:59.275458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_loader = DataLoader(train_data,batch_size=8,shuffle=True)\ntest_data_loader = DataLoader(test_data,batch_size=5,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:57:01.767472Z","iopub.execute_input":"2021-06-03T13:57:01.767807Z","iopub.status.idle":"2021-06-03T13:57:01.772522Z","shell.execute_reply.started":"2021-06-03T13:57:01.767771Z","shell.execute_reply":"2021-06-03T13:57:01.771379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nepochs = 1","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:55:14.400546Z","iopub.execute_input":"2021-06-03T13:55:14.400856Z","iopub.status.idle":"2021-06-03T13:55:14.405676Z","shell.execute_reply.started":"2021-06-03T13:55:14.400826Z","shell.execute_reply":"2021-06-03T13:55:14.404727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:55:15.339412Z","iopub.execute_input":"2021-06-03T13:55:15.339711Z","iopub.status.idle":"2021-06-03T13:55:15.344926Z","shell.execute_reply.started":"2021-06-03T13:55:15.339682Z","shell.execute_reply":"2021-06-03T13:55:15.343932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nUmodel = UNet(num_classes=2).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:57:20.675685Z","iopub.execute_input":"2021-06-03T13:57:20.676009Z","iopub.status.idle":"2021-06-03T13:57:20.939742Z","shell.execute_reply.started":"2021-06-03T13:57:20.675976Z","shell.execute_reply":"2021-06-03T13:57:20.938902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(Umodel.parameters())","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:57:22.079755Z","iopub.execute_input":"2021-06-03T13:57:22.080104Z","iopub.status.idle":"2021-06-03T13:57:22.086252Z","shell.execute_reply.started":"2021-06-03T13:57:22.08007Z","shell.execute_reply":"2021-06-03T13:57:22.084957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install segmentation-models-pytorch","metadata":{"execution":{"iopub.status.busy":"2021-06-03T13:55:16.979467Z","iopub.execute_input":"2021-06-03T13:55:16.979886Z","iopub.status.idle":"2021-06-03T13:55:22.541835Z","shell.execute_reply.started":"2021-06-03T13:55:16.979845Z","shell.execute_reply":"2021-06-03T13:55:22.540916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Импортируем библиотеку time для расчета, сколько времени у нас уходит на одну эпоху\nimport time\n\nimport segmentation_models_pytorch as smp\nimport time\ncriterion = smp.utils.losses.DiceLoss()\n\n# Полезная функция для детектирования аномалий в процессе обучения\n#torch.autograd.set_detect_anomaly(True)\n\n# запускаем главный тренировочный цикл\nepoch_losses = []\nUmodel.train()\nfor epoch in range(epochs):\n    time1 = time.time()\n    epoch_loss = []\n    for batch_idx, (data, labels) in enumerate(train_data_loader):\n        \n        data, labels = data.to(device), labels.to(device)        \n        \n        optimizer.zero_grad()\n\n        outputs = Umodel(data)\n        \n        loss = criterion(outputs, labels)\n        \n                \n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss.append(loss.item())\n        \n        if batch_idx%50==0:\n            print(f'batch index : {batch_idx} | loss : {loss.item()}')\n\n    print(f'Epoch {epoch+1}, loss: ',np.mean(epoch_loss))\n    time2 = time.time()\n    print(f'Spend time for 1 epoch: {time2-time1} sec')\n    \n    \n    \n    epoch_losses.append(epoch_loss)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:01:11.688645Z","iopub.execute_input":"2021-06-03T14:01:11.689045Z","iopub.status.idle":"2021-06-03T14:03:19.760229Z","shell.execute_reply.started":"2021-06-03T14:01:11.688992Z","shell.execute_reply":"2021-06-03T14:03:19.75929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_orig(image):\n    #image = images[0,:,:,:]\n    image = image.permute(1, 2, 0)\n    image = image.numpy()\n    image = np.clip(image, 0, 1)\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:03:19.761602Z","iopub.execute_input":"2021-06-03T14:03:19.762116Z","iopub.status.idle":"2021-06-03T14:03:19.767187Z","shell.execute_reply.started":"2021-06-03T14:03:19.762075Z","shell.execute_reply":"2021-06-03T14:03:19.766317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, data in enumerate(test_data_loader):\n    images, labels = data\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = Umodel(images)\n    f, axarr = plt.subplots(1,3)\n\n    for j in range(0,5):\n        axarr[0].imshow(outputs.squeeze().detach().cpu().numpy()[j,1,:,:])\n        axarr[0].set_title('Guessed labels')\n\n        axarr[1].imshow(labels.detach().cpu().numpy()[j,0,:,:])\n        axarr[1].set_title('Ground truth labels')\n\n        original = get_orig(images[j].cpu())\n        axarr[2].imshow(original)\n        axarr[2].set_title('Original Images')\n        plt.show()\n        plt.gcf().show()\n    if i>5:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-06-03T14:04:18.675491Z","iopub.execute_input":"2021-06-03T14:04:18.675813Z","iopub.status.idle":"2021-06-03T14:04:22.341324Z","shell.execute_reply.started":"2021-06-03T14:04:18.675784Z","shell.execute_reply":"2021-06-03T14:04:22.340416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}