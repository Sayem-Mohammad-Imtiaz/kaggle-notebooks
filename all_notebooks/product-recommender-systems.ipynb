{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Recommender Systems "},{"metadata":{},"cell_type":"markdown","source":"##### A recommender system, or a recommendation system, is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item.They are primarily used in commercial applications."},{"metadata":{},"cell_type":"markdown","source":"Everyday a million products are being recommended to users based on popularity and other metrics on e-commerce websites.\n\nLets make our own recommendation system that recommends 5 new products based on the user's habits.\n\nData Source - Amazon Reviews data (http://jmcauley.ucsd.edu/data/amazon/) The repository has several datasets. For this case study, we are using the Electronics dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import the basic libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data=pd.read_csv('../input/amazon-product-reviews/ratings_Electronics (1).csv',names=('userId','productId','ratings','timestamp'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of rows: {} & Columns : {} in our Dataset\".format(Data.shape[0],Data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop the timestamp column as it is not relevent for our analysis\nData=Data.drop(['timestamp'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing timestamp column, our dataset has three columns where two are of object type and rating being a neumeric(float)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dp=Data.duplicated().sum()\n#Check for duplicates \nprint(\"Number of Duplicates in our dataset :{}\".format(dp))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Check for Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you could see there are no null values either in our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"uq=len(Data['userId'].unique())\npq=len(Data['productId'].unique())\nprint(\"The number of Unique Users:{} and number of unique products:{} in our ecommerce site\".format(uq,pq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Data.describe(exclude=[np.object]).T)\nq1=Data['ratings'].quantile(.25)\nq3=Data['ratings'].quantile(.50)\nIQR=q3-q1\nprint(\"#################################################################\")\nprint(\"IQR for ratings in our data is :{}\".format(IQR))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Statistical analysis of numeric column:\n\nSince userid and productid columns are objects, we shall do Statistical analysis of rating column alone. \n\n1. Every user in the dataset has rated atleast one product.\n2. The Minimum rating that a product has received in 1.0 and the max rating the product has received is 5.0.\n3. The range of dispersion for rating is 1-5.\n4. The average/mean rating by all users to our products is 4.01, with standard deviation 0f 1.3. Our data points are quite widely spread from the mean.\n\n5. Our First Qaurtile 25% is 3 which means 25% of data points fall at or below it.\n6. Our median second Qaurtile at 50% is 5.\n7. Our Third quartile 75% is 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"rt_gp=Data.groupby('ratings')['ratings'].count()\nprint(rt_gp)\nplt.figure(figsize=(15,5))\nsns.distplot(Data['ratings'],norm_hist=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.countplot(Data['ratings'],palette=\"Set3\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Let's explore the rating groups:\n1. From the histogram, we could see the five groups of ratings.\n2. Looks like our users are more generous and have given the top rating 5 for good products.\n3. Ratings 1,2,3 have a similiar trend among users, whereas rating 4 is slighly higher.\n##### Distribution Analysis:\n1. User Group 1,2,3 is normally distributed with a smooth peak and are platykurtic , whereas for groups 4 & 5 the peaks are sharp and are leptokurtic.\n\nTip: kurtosis values are compared with that of the normal distribution as values less than 3 are said to be platykurtic, or \"flat-topped.\" Alternatively, kurtosis values higher than 3 are said to be leptokurtic, usually appearing sharp at their peak value. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Data['ratings'].skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ratings in our data is a negatively skewed with the tail being extended towrdas left from the median."},{"metadata":{"trusted":true},"cell_type":"code","source":"rt_gp_user=Data.groupby('userId')['ratings'].count()\nrt_gp_product=Data.groupby('productId')['ratings'].count()\nMost_occured_procuct=Data['productId'].value_counts().idxmax()\nMost_freq_user=Data['userId'].value_counts().idxmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"###########################################################################################################################\")\nprint(\"The Max number of ratings we have received for a single product is :{} & the product ID that has received is :{}\".format(rt_gp_product.max(),Most_occured_procuct))\nprint(\"The User :{} has given max number of ratings across products with Number of ratings being:{}\".format(Most_freq_user,rt_gp_user.max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Take a subset of the dataset to make it less sparse/ denser"},{"metadata":{},"cell_type":"markdown","source":"#### Identifying number of ratings provided by each user.\n1. I am using pandas join and groupby to get the count of number of ratings given by each user. \n\nThe column ratings_user_count will give the number of ratings provided by the user."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=Data.join(Data.groupby('userId')['ratings'].count(),on='userId',rsuffix='_user_count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now, Lets try to make a subset of data. Though we have a large dataset, lets consider ratings provided by user who have rated more than 50 products. The reason being\n1. We may not be able to understand or rely on a user rating with fewer number of representation from the user. For example, if a product has received only a single rating or a user has rated only one product it doesn't give us any variety.\n2. Memory consideration, for techinques like Matrix factorization,SVD, collaborative filtering methods it becomes compuationally complex with the local machines on a high volume dataset."},{"metadata":{},"cell_type":"markdown","source":"I am creating a function subset , which will count the number of ratings provided by each user and will calssify a user as if he has rated more than 50 ratings or rated less than 50 ratings in a column called Group"},{"metadata":{"trusted":true},"cell_type":"code","source":"def subset(row):\n    if row['ratings_user_count']> 50:\n        return \"Rated more than 50\"\n    else:\n        return \"Rated Less than 50\"\ndf['Group']=df.apply(subset,axis=1)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_more_than_50=df[df['Group']=='Rated more than 50']\nuser_less_than_50=df[df['Group']=='Rated Less than 50']\nA=user_more_than_50['Group'].count()\nB=user_less_than_50['Group'].count()\nprint(\"Number of users who have more than 50 ratings:{}\".format(user_more_than_50['Group'].count()))\nprint(\"Number of users who have less than 50 ratings:{}\".format(user_less_than_50['Group'].count()))\n\nsub_per=(A/(A+B)) * 100\nprint(\"Our subset is just :{} % of our total data, However it gives us the data density required with total number of records:{}\".format(sub_per,A))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Group'].value_counts().plot.pie(shadow=True, startangle=120,autopct='%.2f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Keep the users only who has given 50 or more number of ratings\n\nNow, Let's extract the details of users and products who have given more than 50 ratings and store it is a seperate dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"subset=df[df['Group']=='Rated more than 50']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the subset dataset contains 122171 rows of unique users who have given morethan 50 ratings.\n\nThe below distplot shows the distribution of our subset.\n\nIf we compare the distribution of original data vs subset, we could see that the distributions are similar for user groups who have rated  5,4. whereas the distribution for ratings 1,2,3 have slightly changed and the peaks looks flat and smooth.\n\nThis is due to change in the data both in volume and values."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.distplot(subset['ratings']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#exporting the subset to csv \nsubset.to_csv('subset_morethan_50ratings.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset_with_number_of_ratings=subset.join(subset.groupby('productId')['ratings'].count(),on='productId',rsuffix='_product_count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset_with_number_of_ratings.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now , Iam adding one more column \"ratings_product_count\" this will give the count of how many ratings the specific product ID has received.\n\nIn popularity based recommendation , we will use this detail to give more insights to the user about the product.\n\nHow to read the above dataframe?\n\n\"The product 0594481813 has a rating of 3.0 and has received only one rating by a user AT09WGFUM934H who has rated 110 other products in our subset\"."},{"metadata":{},"cell_type":"markdown","source":"### Popularity Recommender model\n\n#### Having built the dataset with our required columns, we can now build our popularity based recommender system.\n"},{"metadata":{},"cell_type":"markdown","source":"### What is popularity based recommender system?\n\nIt is the simplest recommendation model that works on principle of popularity that identifies the products that are popular among users. This will give the users recommendation of products that are in trend, high in demand and are bought by users."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets drop the columns userId and group \nPopularity_based_Recommendadtion=subset_with_number_of_ratings.drop(['userId','Group'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Popularity_based_Recommendadtion.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Popularity based on user rating and number of ratings received by each product."},{"metadata":{"trusted":true},"cell_type":"code","source":"PRS_Product_rated_count=pd.DataFrame(Popularity_based_Recommendadtion.groupby('productId')['ratings','ratings_product_count'].mean().sort_values(by='ratings_product_count',ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have now created a dataframe that lists the products, its ratings and how many ratings the product has received and sorted based on number of ratings received."},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Top 10 products based on number of ratings the product has received \nPRS_Product_rated_count.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Popularity based on user rating"},{"metadata":{"trusted":true},"cell_type":"code","source":"PRS_by_Rating=pd.DataFrame(Popularity_based_Recommendadtion.groupby('productId')['ratings','ratings_product_count'].mean().sort_values(by='ratings',ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have now created a dataframe that lists the products, sorted based on rating and how many ratings the product has received and sorted based on user ratings"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Top 10 products based on top rating received by the product  \nPRS_by_Rating.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Both the above systems give the user top rated products. The table 2 though shows the products which have been rated the best but the number  of ratings it has received is just 1.\n#### However the table 1 gives more credibility as it recommends the products based on number of users who have rated the product and not just products that have just been rated high"},{"metadata":{},"cell_type":"markdown","source":"Now, We will get the number of ratings for each scale between 1 to 5 for each product and make it as a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"S=pd.DataFrame(subset.groupby('productId')['ratings'].value_counts().unstack().fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Rating count for each scale \nNow lets further try to enhance the Table 1 with  recommendations to users with how many ratings have been given by users for each product on a scale of 1 to 5 along with product rating & product rating count."},{"metadata":{"trusted":true},"cell_type":"code","source":"Popularity_Final=pd.merge(PRS_Product_rated_count,S,on='productId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Popularity_Final.nlargest(15,'ratings_product_count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### So, above is the popularity based recommendation that we have created and I am highlighting the top 15 products that are popular among users along with average rating, number of ratings the product has received and its splitup on number of rating for each scale between 1 to 5.\n\n#### Advantages of Popularity based recommendation:\n1. Computationally easy, Less complex.\n2. No user charecterstics is required , hence does not suffer from cold start problem.\n3. Can be made available to the user from day 1 of starting business.\n\n#### Disadvantages:\n1. There is no variety in the recommendadtion.\n2. No personalization in the recommendation either. Irrespective of what the user might be interested the model recommends the same set of prodcut to every user."},{"metadata":{},"cell_type":"markdown","source":"### Collaborative Filtering model\n\n##### This method of recommendadtion overcomes the shortfall of popularity based recommendation systems.\n\nCollaborative filtering works on the similarity between different users or similarity between items.The similarity could be used to recommend the products based on user or item behaviour.\n\nCouple of methods to implement Collaborative filtering :\n\n1. Matrix Factorization using Singular value decomposition\n2. Nearest Neighbour collaborative filtering (User based and Item based)"},{"metadata":{},"cell_type":"markdown","source":"### Matrix Factorization using Singular value decomposition"},{"metadata":{},"cell_type":"markdown","source":"##### Matrix Factorization is used to identify/predict what  rating will a specific user give for a given product  based on the previous ratings he has provided to other items.\n\n#### How is this done ?\n\n1. The user & item data is first made avaiable as a matrix. Product and user charecterstics is computed for each user and each item.\n2. Dot product of these charectersics will give the predictions for each user for each item in the matrix.\n3. Now since we have the actual and the predicted data the algorithm will further use gradient descent to find the minimal error to predict the closest possible rating for the user.\n\n#### We will use singular value decomposition from Surprise Library to implement this in python.\n\nThe name surprise  stands for \"Simple Python RecommendatIon System Engine\""},{"metadata":{},"cell_type":"markdown","source":"Lets import SVD , Dataset and reader from surprise to read the dataset in surprise format."},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import SVD\nfrom surprise import Dataset\nfrom surprise import Reader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reader=Reader(rating_scale=(1.0, 5.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This line will create the matrix from our pandas dataframe\ndata_for_mf=Dataset.load_from_df(subset[['userId','productId','ratings']],reader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_for_mf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split the data randomly into a train and test dataset :\n\nNote that I am using train_test_split from surprise package and not from sklearn , so there will be a slight change in syntax"},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection.split import train_test_split\nX,Y=train_test_split(data_for_mf,test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets name the model M1 and call the SVD algorithm, This single line of code does all the magic\nM1=SVD()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets train our model M1 with the training dats\nM1.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets do the prediction for the testset\npredictions=M1.test(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each user (uid), for the product (iid) our model has given the actual rating(r_ui) and the predicted rating (est) the user will give for the product.\n\nwas_impossible= False denotes that the algorithm was able predict the user rating , If was_impossible = True then it means that the algorithm was not able to predict the rating.This may generally happen if the user does not has any rating provided to any of the product which is also known as the cold start problem."},{"metadata":{},"cell_type":"markdown","source":"Now using the below function we will map the procuct id's(iid and est rating) for each user and also will sort the first 5 products recommended for each user"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\ndef get_top_n(predictions, n=5):\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n = get_top_n(predictions,n=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Root Mean Square Error for the Matrix Factorization using SVD:{}\".format(accuracy.rmse(predictions,verbose=False)))\nprint(\"The Mean Absolute Error for the Matrix Factorization using SVD:{}\".format(accuracy.mae(predictions,verbose=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Collaborative Filtering Using KNNwithMeans \n\nKNNWith means is a nearest neighbour mapping method , that is used to recommend users the products based on the what your closest neighbour has bought/liked.\n\nMore often than not, we do see \"People who liked this also liked this\", \"People who bought this also bought these\" when we do online shopping or watch movies etc. These are classic examples of recommendation systems that work on the concept of nearest neighbour colloborative filtering.\n\nHow is this done ?\n\nThe similarity between user/item is calculated for each user either based on cosine similiarity or using pearson correlation coefficient. Depending on the values of these parameters the products are suggested to the user.\n\nNearest Neighbour recommendation is not just limited to users , it can also be applied to understand the similarity between items."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Agin from surprise library , I am importing KNNwithmeans to implement this\nfrom surprise import KNNWithMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Due to memory issue I am using just a small subset of data , \n#I tried with higher values but I ran out of memory allocation and 50000 seems to be working fine\nsubset2=subset.head(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subset2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's load the dataset in surprise format from the pandas dataframe and split the data into test and train\ndata_for_collab=Dataset.load_from_df(subset2[['userId','productId','ratings']],reader)\ntrainset,testset=train_test_split(data_for_collab,test_size=.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### I am using Grid.search from surprise to implement KNN for both user-user colloboration and item-item colloboration , we will also use both cosine and pearson similirtity and find the best parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"parm_grid={'k':[50,60,70],'name':[\"cosine\",\"pearson_baseline\"],'user_based':[True,False]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection import GridSearchCV\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import cross_validate\nGrid_1=GridSearchCV(KNNWithMeans,parm_grid,measures=[\"rmse\", \"mae\"],cv=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_1.fit(data_for_collab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_1.best_params['rmse']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_1.best_estimator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grid_1.best_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNNwithMeans (user - user similarity)\nNow, since we have obtained the best parameters for the KNNwithmeans using Gridsearch, we shall train the model M2 with these parameters.\n\nThough the best results are obtained through user user similarity in gridsearch, we shall also perform KNN with item -item similarity."},{"metadata":{},"cell_type":"markdown","source":"#### What is user-user similarity?\n\nThe recommendations are provided to a user based on how similar the given user is with his neighbour within a set of cluster. The similarity is calculated either through cosine distance or pearson correlation.\n\n#### What is item-item  similarity?\n\nThe recommendations are provided to a user based on how similar the given item is with it's neighbours within a set of cluster. The similarity is calculated either through cosine distance or pearson correlation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"M2=KNNWithMeans(k=50,sim_options={'name': 'cosine', 'min_support': 5, 'user_based': True,'k':5},verbose= True,c=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets fit our training data to our model\nM2.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred2=M2.test(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Root Mean Square Error for KNNwithMeans using user user similarity:{}\".format(accuracy.rmse(pred2,verbose=False)))\nprint(\"The Mean Absolute Error for KNNwithMeans using user user similarity:{}\".format(accuracy.mae(pred2,verbose=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNNwithMeans Item-Item similarity \n\nWhen we implement colloborative filtering with surprise package, the method that we will invoke remains the same and just the boolean value for User_based parameter decides if we are going to implement user-user similarity or item-item similarity.\n\nUser_based= False will perform item , item similarity\nUser_based= True  will perform user, user similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"M3=KNNWithMeans(k=50,sim_options={'name': 'cosine', 'min_support': 5, 'user_based': False,'k':5},verbose= True,c=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"M3.fit(trainset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred3=M3.test(testset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Root Mean Square Error for KNNwithMeans using item item similarity:{}\".format(accuracy.rmse(pred3,verbose=False)))\nprint(\"The Mean Absolute Error for KNNwithMeans using item item similarity:{}\".format(accuracy.mae(pred3,verbose=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TOP 5 Recommendations from each model"},{"metadata":{},"cell_type":"markdown","source":"### Popularity Based Recommendation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The top 5 products that we recommend using popularity based recommendation:\")\nprint(\"##########################################################################\")\nprint(Popularity_Final.nlargest(5,'ratings_product_count'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matrix Factorization using Singular Value Decomposition "},{"metadata":{"trusted":true},"cell_type":"code","source":"top_n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNNWithMeans user - user similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_user(pred2,n=5):\n  top_n=defaultdict(list)\n  for uid,iid,true_r,est,_ in pred2:\n    top_n[uid].append((iid,est))\n  for uid,user_ratings in top_n.items():\n    user_ratings.sort(key=lambda x: x[1],reverse=True)\n    top_n[uid]=user_ratings[:n]\n  return top_n\nTop_CF_user=get_top_user(pred2,n=5)\n\n# For each user Print the recommended items\nfor uid, user_ratings in Top_CF_user.items():\n    print(uid, [iid for (iid, _) in user_ratings])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNNWithMeans item-item similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_item(pred3,n=5):\n  top_n=defaultdict(list)\n  for uid,iid,true_r,est,_ in pred3:\n    top_n[uid].append((iid,est))\n  for uid,user_ratings in top_n.items():\n    user_ratings.sort(key=lambda x: x[1],reverse=True)\n    top_n[uid]=user_ratings[:n]\n  return top_n\nTop_CF_item=get_top_item(pred3)\n\n# For each user Print the recommended items\ndf_item=pd.DataFrame()\nfor uid, user_ratings in Top_CF_item.items():\n    print(uid, [iid for (iid, _) in user_ratings])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### User-User VS Item-Item similarity\nThough user based and item based looks similar in implementation the results & how they work is entirely different.\n\nBelow I am highlighting the predictions of our model M2 (user based) & M3 (Item based)\n\nProduct recommendation Prediction of user-user similarity for user A250AXLRBVYKB4\n\nA250AXLRBVYKB4 ['B00004Z5M1', 'B000WGR3VG', 'B001ISK6FW', 'B00154MCKQ', 'B001GPVGZ6']\n\nProduct recommendation Prediction of item-item similarity for user A3P1508PZ0UADD\n\nA250AXLRBVYKB4 ['B001G04VJO', 'B000P0CTSQ', 'B00194101O', 'B001TH7GSW', 'B00081A2KY']\n\nAs you could see, the list of products are entirely differnt."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The Root Mean Square Error for the Matrix Factorization using SVD:{}\".format(accuracy.rmse(predictions,verbose=False)))\nprint(\"The Mean Absolute Error for the Matrix Factorization using SVD:{}\".format(accuracy.mae(predictions,verbose=False)))\nprint(\"The Root Mean Square Error for KNNwithMeans using user user similarity:{}\".format(accuracy.rmse(pred2,verbose=False)))\nprint(\"The Mean Absolute Error for KNNwithMeans using user user similarity:{}\".format(accuracy.mae(pred2,verbose=False)))\nprint(\"The Root Mean Square Error for KNNwithMeans using item item similarity:{}\".format(accuracy.rmse(pred3,verbose=False)))\nprint(\"The Mean Absolute Error for KNNwithMeans using item item similarity:{}\".format(accuracy.mae(pred3,verbose=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\n\n#### Recommendation systems built:\n1. Popularity based \n2. Collaborative filtering using Singular value decomposition\n3. Collaborative filtering using KNNWithmeans (user-user similarity)\n4. Collaborative filtering using KNNWithmeans (item-item similarity)\n\nPopularity based system was useful in providing the users with products those have received good ratings and were on high demand. However, there was no variety and personalization to the user. \n\nWhereas, collaborative filtering with KNNwithmeans was good in providing the recommendations with personalization by selecting products those were purchased by similar users or products that were similar to other products the user has bought/liked.\n\nWith SVD we were able to predict what rating a user will provide if a product is recommended to a user depending on other ratings he had provided, And recommendation can be done by setting a threshold value and recommend products that have high rating. \n\nOn compairing all the recommendations we have built, each has its own advantages and disadvantages. \n\nInitially, when we do not have any user details we can prefer the popularity based recommendation , As the business grows and when we have more details about user's behaviour etc we should definetely go with collaborative filtering techniques to make precise , personalised recommendation to be useful for the user. "},{"metadata":{},"cell_type":"markdown","source":"##### Lets do 5 fold cross validation for our best model M2 with user_based = True and see the RMSE and MAE "},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_validate(M2,data_for_mf , measures=['RMSE', 'MAE'], cv=5, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To conclude, for this problem statement I suggest to go with Collaborative filtering using KNNwith means(User-user similarity) as the RMSE and MAE for this system is least in error compared to others.\n\nThis means that , the recommendation done with our model will be precise , customized for each user providing personalization."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}