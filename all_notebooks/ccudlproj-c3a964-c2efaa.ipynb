{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\n\nimport functools\nimport tensorflow as tf\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, BatchNormalization, Activation, Dropout, Lambda, SpatialDropout2D\nfrom keras.layers.convolutional import Conv2D, UpSampling2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers.merge import concatenate\n\nfrom keras.losses import BinaryCrossentropy\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:42.231553Z","iopub.execute_input":"2021-06-29T11:51:42.23193Z","iopub.status.idle":"2021-06-29T11:51:42.23997Z","shell.execute_reply.started":"2021-06-29T11:51:42.231897Z","shell.execute_reply":"2021-06-29T11:51:42.238527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input/imagedata/","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:42.297456Z","iopub.execute_input":"2021-06-29T11:51:42.29804Z","iopub.status.idle":"2021-06-29T11:51:43.189035Z","shell.execute_reply.started":"2021-06-29T11:51:42.297966Z","shell.execute_reply":"2021-06-29T11:51:43.187222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nibabel as nib\nimport math\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:43.193401Z","iopub.execute_input":"2021-06-29T11:51:43.196502Z","iopub.status.idle":"2021-06-29T11:51:43.210794Z","shell.execute_reply.started":"2021-06-29T11:51:43.195907Z","shell.execute_reply":"2021-06-29T11:51:43.209282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport keras\n#from datagenerator import SequenceData\nfrom tensorflow.keras.utils import Sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:43.214093Z","iopub.execute_input":"2021-06-29T11:51:43.216192Z","iopub.status.idle":"2021-06-29T11:51:43.223912Z","shell.execute_reply.started":"2021-06-29T11:51:43.216142Z","shell.execute_reply":"2021-06-29T11:51:43.222193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coding=Big5\n#trainset_path = 'trainset/C2_TrainDev/Train/'\nprint(\"***csv_dev :2***\")\nclass SequenceData(keras.utils.Sequence):\n\n    def __init__(self, model, im_dir, label_dir, im_list ,target_size, batch_size, shuffle=True):\n        self.model = model\n        self.datasets = []\n        self.im_dataset_path = im_dir\n        self.label_dataset_path = label_dir\n        self.datasets = im_list\n        self.image_size = target_size[0:2]\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.datasets))\n        self.shuffle = shuffle\n        \n\n\n\n    def __len__(self):\n        \n        num_imgs = len(self.datasets)\n        return math.ceil(num_imgs / float(self.batch_size))\n\n    def __getitem__(self, idx):\n       \n        batch_indexs = self.indexes[idx *\n                                    self.batch_size:(idx + 1) * self.batch_size]\n        \n        batch = [self.datasets[k] for k in batch_indexs]\n        \n        X, y = self.data_generation(batch)\n        return X, y\n\n    def on_epoch_end(self):\n     \n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n   \n\n    def data_generation(self, batch_datasets):\n        images = []\n        labels = []\n\n        X, y = self.get_data(batch_datasets)\n        # images.append(image)\n        # labels.append(label)\n\n        # X = np.array(images)\n        # y = np.array(labels)\n\n        return X, y\n\n    def get_data(self, imgs):\n\n        # Ground Truth for entire data (num_data, 7, 7, class+5)\n        \n        Gt_list = np.zeros((len(imgs), 240, 240, 1))\n        im_index = 0\n        #imgs = ['02176.jpg']\n        Img_list = np.zeros(shape=(len(imgs), 240, 240, 1))\n        \n        for img in imgs:\n            gray_img = cv2.imread(self.im_dataset_path + img, cv2.IMREAD_GRAYSCALE)\n            label = cv2.imread(self.label_dataset_path + img, cv2.IMREAD_GRAYSCALE)\n            \"\"\"\"\n            img_resize = cv2.resize(gray_img, (240, 240),\n                                    interpolation=cv2.INTER_LINEAR)\n                                    \n            label_resize = cv2.resize(label, (240, 240),\n                                    interpolation=cv2.INTER_LINEAR)\n            \"\"\"\n            img_resize = gray_img / 255.0\n            \n            label_resize = label / 255.0\n            label_resize = np.ceil(label_resize)\n            \n            img_resize = img_resize.reshape((img_resize.shape[0], img_resize.shape[1], 1))\n            label_resize = label_resize.reshape((label_resize.shape[0], label_resize.shape[1], 1))\n           \n            Gt_list[im_index] = label_resize\n            Img_list[im_index] =  img_resize\n            im_index += 1\n       \n        return Img_list, Gt_list\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:43.227493Z","iopub.execute_input":"2021-06-29T11:51:43.229381Z","iopub.status.idle":"2021-06-29T11:51:43.264751Z","shell.execute_reply.started":"2021-06-29T11:51:43.229311Z","shell.execute_reply":"2021-06-29T11:51:43.263321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw(history):\n    print(history.history.keys())\n    fig1 = plt.figure()\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.savefig(\"./Loss_dice_s.png\")\n    \n    fig2 = plt.figure()\n    plt.plot(history.history['dice_coef'])\n    plt.plot(history.history['val_dice_coef'])\n    plt.title('Model Dice_coef')\n    plt.ylabel('Dice_coef')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.savefig(\"./Dice_dice_s.png\")\n    \n    fig3 = plt.figure()\n    plt.plot(history.history['precision'])\n    plt.plot(history.history['val_precision'])\n    plt.title('Model precision')\n    plt.ylabel('precision')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.savefig(\"./precision_dice_s.png\")\n    \n    fig4 = plt.figure()\n    plt.plot(history.history['recall'])\n    plt.plot(history.history['val_recall'])\n    plt.title('Model recall')\n    plt.ylabel('recall')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.savefig(\"./recall_dice_s.png\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:43.270009Z","iopub.execute_input":"2021-06-29T11:51:43.271852Z","iopub.status.idle":"2021-06-29T11:51:43.289721Z","shell.execute_reply.started":"2021-06-29T11:51:43.271806Z","shell.execute_reply":"2021-06-29T11:51:43.287449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Convolution(input_tensor, filters, drop = 0.0):\n    \n    x = Conv2D(filters=filters, kernel_size=(3,3), padding='same', strides=(1,1))(input_tensor)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout2D(drop)(x)\n    return x\n\ndef unet(input_shape):\n    \n    inputs = Input((input_shape))\n    \n    conv_1 = Convolution(inputs, 16 * 1)  #Origin 32\n    conv_1 = Convolution(conv_1, 16 * 1) \n    maxp_1 = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv_1)\n    \n    conv_2 = Convolution(maxp_1, 32 * 1)  #Origin 64\n    conv_2 = Convolution(conv_2, 32 * 1)\n    maxp_2 = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv_2)\n    \n    conv_3 = Convolution(maxp_2, 64 * 1, 0.5)  #Origin 128\n    conv_3 = Convolution(conv_3, 64 * 1, 0.5)\n    maxp_3 = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv_3)\n    #maxp_3 = Dropout(0.5)(maxp_3)\n    \n    conv_4 = Convolution(maxp_3, 128 * 1, 0.5)  #Origin 256\n    conv_4 = Convolution(conv_4, 128 * 1, 0.5)\n    maxp_4 = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(conv_4)\n    #3maxp_4 = Dropout(0.5)(maxp_4)\n    \n    conv_5 = Convolution(maxp_4, 256 * 1, 0.5)   #Origin 512\n    conv_5 = Convolution(conv_5, 256 * 1, 0.5)\n    \n    # deconv1\n    upsample_6 = UpSampling2D((2,2))(conv_5)\n    upsample_6 = Convolution(upsample_6, 128 * 1, 0.5)  #CH:128 #Origin 256\n    \n    # Merge 1\n    conv_6 = concatenate([upsample_6, conv_4])  #128 + 128\n    #conv_6 = Dropout(0.5)(conv_6)\n    conv_6 = Convolution(conv_6, 128 * 1, 0.5)\n    conv_6 = Convolution(conv_6, 128 * 1, 0.5)\n    \n    # deconv 2\n    upsample_7 = UpSampling2D((2,2))(conv_6)\n    upsample_7 = Convolution(upsample_7, 64 * 1, 0.5) #64\n    \n    # Merge 2\n    upsample_7 = concatenate([upsample_7, conv_3]) #64 + 64\n    #upsample_7 = Dropout(0.5)(upsample_7)\n    conv_7 = Convolution(upsample_7, 64 * 1, 0.5)  #Origin 128\n    conv_7 = Convolution(conv_7, 64 * 1, 0.5)  #Origin 128\n    \n    # deconv 3\n    upsample_8 = UpSampling2D((2,2))(conv_7)\n    upsample_8 = Convolution(upsample_8, 32 * 1, 0.5)\n    \n    # Merge 3\n    upsample_8 = concatenate([upsample_8, conv_2]) #32 + 32\n    #upsample_8  = Dropout(0.5)(upsample_8)\n    conv_8 = Convolution(upsample_8, 32 * 1, 0.5)  #CH = 32 #Origin 64\n    conv_8 = Convolution(conv_8, 32 * 1, 0.5)\n    \n    # deconv 4\n    upsample_9 = UpSampling2D((2,2))(conv_8)\n    upsample_9 = Convolution(upsample_9, 16 * 1, 0.5)\n    \n    # Merge 4\n    upsample_9 = concatenate([upsample_9, conv_1])\n    #upsample_9 = Dropout(0.5)(upsample_9)\n    conv_9 = Convolution(upsample_9, 16 * 1, 0.5)  #Origin 16\n    \n    conv_10 = Convolution(conv_9, 16 * 1, 0.5)\n    \n    outputs = Conv2D(2, (1,1), activation='softmax')(conv_10)\n    #outputs = outputs[:, :, :, 0]\n    \n    \n    model = Model(inputs=[inputs], outputs=[outputs])\n    \n    return model\n\n\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    y_true = y_true[:, :, :, 0]\n    y_pred = y_pred[:, :, :, 0]\n    \n    y_true_f = K.round(K.flatten(y_true)) # K.round(y_true) #\n    y_pred_f= K.round(K.flatten(y_pred)) # K.round(y_pred) #\n    intersection = K.sum(y_true_f * y_pred_f)\n    \n    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n\n# Computing Precision\ndef precision(y_true, y_pred):\n    \n    y_true = y_true[:, :, :, 0]\n    y_pred = y_pred[:, :, :, 0]\n    \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = (true_positives ) / (predicted_positives + K.epsilon())\n    \n    return precision\n\n# Computing Sensitivity\ndef recall(y_true, y_pred):\n    \n    y_true = y_true[:, :, :, 0]\n    y_pred = y_pred[:, :, :, 0]\n    \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    \n    return (true_positives ) / (actual_positives + K.epsilon())    \n    \ndef dice_loss(y_true, y_pred, smooth=1.0):\n    \n    y_true = y_true[:, :, :, 0]\n    y_pred = y_pred[:, :, :, 0]\n    \n    y_true_f = K.round(K.flatten(y_true))\n    y_pred_f = K.flatten(y_pred)\n    \n    \n    intersection = K.sum(y_true_f * y_pred_f)\n    dice = (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n    \n    return 1 - dice\n\n    \ndef dice_bin_loss(y_true, y_pred):\n\n    bce = BinaryCrossentropy(from_logits=True)(y_true, y_pred)\n\n    return bce + dice_loss(y_true, y_pred)\n\ndef focal_loss(y_true, y_pred, gamma=2., alpha=.25):\n\n  y_true = K.flatten(y_true)\n  y_pred = K.flatten(y_pred)\n\n  pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n  pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n  focal_loss_fixed =  -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1+K.epsilon())) - K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon())) / 200000\n  return focal_loss_fixed\n\ndef gdice(y_true, y_pred):\n  \n  y_true_f = K.flatten(y_true)\n  y_pred_f = K.flatten(y_pred)\n  weights = 1./K.square(K.sum(y_true_f) + 1e-6)\n  #weights = weights/K.sum(weights)\n  num = weights*K.sum(y_true_f * y_pred_f)\n  den = weights*K.sum(y_true_f + y_pred_f)\n  return 1 - 2.*(num + K.epsilon()) / (den + K.epsilon())\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:43.29488Z","iopub.execute_input":"2021-06-29T11:51:43.296776Z","iopub.status.idle":"2021-06-29T11:51:43.389641Z","shell.execute_reply.started":"2021-06-29T11:51:43.29673Z","shell.execute_reply":"2021-06-29T11:51:43.387288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \n    EPOCH = 40\n    im_path =  '../input/imdata/im/'\n    label_path = '../input/segdata/seg/'\n    pretrained_weight = '../input/smoothdice40/Agdice100_s.h5'\n    \n    BATCH_SIZE = 16\n\n    input_shape = (240, 240, 1)\n   \n    model = unet(input_shape=(240,240,1))\n    \n    \n    lr_schedule = optimizers.schedules.ExponentialDecay(\n                                                        initial_learning_rate=1e-4,\n                                                        decay_steps=2496 * 15,\n                                                        decay_rate=0.2)\n    \n    \n    \n    Adam = optimizers.Adam(learning_rate = lr_schedule)\n                           \n    alpha = 0.0001  # weight decay coefficient\n    \n    \n    for layer in model.layers:\n        if isinstance(layer, keras.layers.Conv2D) :\n            #layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.kernel))\n            setattr(layer, 'kernel_regularizer', keras.regularizers.l2(alpha))\n        if hasattr(layer, 'bias_regularizer') and layer.use_bias:\n            #layer.add_loss(lambda: keras.regularizers.l2(alpha)(layer.bias))\n            setattr(layer, 'bias_regularizer', keras.regularizers.l2(alpha))\n                                   \n                           \n                           \n                           \n    model.compile(optimizer=Adam, loss=dice_loss, metrics=[dice_coef, precision, recall])\n\n\n    # Spilt validation from dataset\n    \n    dataset = sorted(os.listdir(im_path))\n    \n    #dataset = dataset[0: 1009]\n    val_len = int(len(dataset) * 0.2)\n    test_len = int(len(dataset) * 0.1)\n    train_len = len(dataset) - val_len - test_len\n    \n    val_dataset = dataset[: val_len]\n    train_dataset = dataset[val_len: val_len + train_len]\n    random.shuffle(train_dataset)\n    \n    \n    \n    print(len(train_dataset), len(val_dataset))\n\n    train_generator = SequenceData('train', im_path, label_path, train_dataset, input_shape, BATCH_SIZE)\n    val_generator = SequenceData('val', im_path, label_path, val_dataset, input_shape, BATCH_SIZE)\n\n\n    #model.load_weights(pretrained_weight)\n    model.summary()\n\n\n\n    es = EarlyStopping(monitor='val_loss',\n                       mode='min',\n                       patience=20,\n                       restore_best_weights=True)\n\n    history = model.fit(train_generator,\n                        epochs=EPOCH,\n                        steps_per_epoch=len(train_generator),\n                        validation_data=val_generator,\n                        validation_steps=len(val_generator),\n                        callbacks = [es]\n                        )\n    weight_name = 'final_s.h5'\n    model.save(weight_name)   \n    print(f'Model saved! {weight_name}')\n    print(history.history['loss'])\n    draw(history) \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:43.3923Z","iopub.execute_input":"2021-06-29T11:51:43.39277Z","iopub.status.idle":"2021-06-29T11:51:43.411968Z","shell.execute_reply.started":"2021-06-29T11:51:43.392711Z","shell.execute_reply":"2021-06-29T11:51:43.410752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nm_dataset_path = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData/'\n\noutput_path = './output/Test/'\n\ndef resize_img(img):\n    \n    img = np.rint(img).astype(np.uint8)\n    #img = cv2.resize(img, (240, 240), interpolation=cv2.INTER_LINEAR)\n    img = img / 255.0\n    #print(img.dtype, img[100][100])\n    \n    img = img.reshape((1, img.shape[0], img.shape[1], 1))\n    \n    return img\n\ndef test_img(img_name, model, im_path, label_path):\n    \n    \n    #print(\"Predict \", img_name)\n    #####   Read image #####\n    img = cv2.imread(im_path + img_name, cv2.IMREAD_GRAYSCALE)\n    label = cv2.imread(label_path + img_name, cv2.IMREAD_GRAYSCALE)\n    img = img / 255.0\n    label = np.ceil(label / 255)\n    \n    \n    img = img.reshape((1, img.shape[0], img.shape[1], 1))\n    print(img.shape)\n    label = label.reshape((1, label.shape[0], label.shape[1], 1))\n    #label = np.ceil(label)\n    #model.evaluate(img) \n    mask = model.predict(img)\n    \n    mask = np.round(mask)\n    \n    TP = np.sum(mask * label)\n    AP = np.sum(label)\n    PP = np.sum(mask)\n    \n    print('Predict ', img_name, ' dice: ', TP,' ', AP,' ', PP,' ', 2 * (TP + 1e-8) / (AP + PP + 1e-8))\n   \n    return TP, AP, PP\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:51:43.415191Z","iopub.execute_input":"2021-06-29T11:51:43.416062Z","iopub.status.idle":"2021-06-29T11:51:43.446596Z","shell.execute_reply.started":"2021-06-29T11:51:43.41597Z","shell.execute_reply":"2021-06-29T11:51:43.444687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \"\"\"\"\n    im_path =  '../input/imdata/im/'\n    label_path = '../input/segdata/seg/'\n    \n    \n    dataset = sorted(os.listdir(im_path))\n    \n    #dataset = dataset[0: 1009]\n    val_len = int(len(dataset) * 0.2)\n    test_len = int(len(dataset) * 0.1)\n    train_len = len(dataset) - val_len - test_len\n    print(val_len, train_len, test_len)\n    \n    #test_dataset = dataset[74*155: 332*155]\n    test_dataset = dataset[332*155: ]\n    print(len(test_dataset))\n    \n    val_len = int(len(dataset) * 0.2)\n    test_len = int(len(dataset) * 0.1)\n    train_len = len(dataset) - val_len - test_len\n    \n    val_dataset = dataset[: val_len]\n    train_dataset = dataset[val_len: val_len + train_len]\n    \n    weights_path = '../input/smoothdice40/Agdice100_s.h5'\n    \n    model = unet(input_shape=(240, 240, 1))\n    \"\"\"\"\n    Adam = optimizers.Adam(lr=1e-4)\n    model.compile(optimizer=Adam, loss=dice_loss, metrics=[dice_coef, precision, recall])\n    \"\"\"\n    model.load_weights(weights_path)\n    \n    \n    TP3D = 0\n    AP3D = 0\n    PP3D = 0\n    count = 1\n    dice3D = 0.0\n    precision3D = 0.0\n    recall3D = 0.0\n    \n    \n    meanprecision = 0.0\n    meanrecall = 0.0\n    meandice = 0.0\n    \n    for name in test_dataset:\n        \n        TP, AP, PP =test_img(name, model, im_path, label_path)\n        \n        if count < 155:\n            TP3D += TP\n            AP3D += AP\n            PP3D += PP\n            \n        else:\n            dice3D = ( 2 * TP3D + 1e-8) / (AP3D + PP3D + 1e-8)\n            \n            precision3D = (TP3D) / (PP3D + 1e-8)\n            recall3D = (TP3D) / (AP3D + 1e-8)\n            print('dice3D: ', dice3D, ' precision3D: ', precision3D, ' recall3D: ', recall3D)\n            \n            \n            meandice += dice3D\n            meanprecision += precision3D\n            meanrecall += recall3D\n            \n            TP3D = 0\n            AP3D = 0\n            PP3D = 0\n            count = 0\n        \n        count += 1\n    \n    meandice /= 36\n    print(\"result: \", meandice, ' meanprecision: ', meanprecision / 36, ' meanrecall: ', meanrecall / 36)\n    \"\"\"\n    ","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-29T11:51:43.449131Z","iopub.execute_input":"2021-06-29T11:51:43.449579Z","iopub.status.idle":"2021-06-29T11:56:57.361561Z","shell.execute_reply.started":"2021-06-29T11:51:43.449535Z","shell.execute_reply":"2021-06-29T11:56:57.360161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    \"\"\"\"\n    weights_path = '../input/dice40/Abin_s2.h5'\n    \n    model = unet(input_shape=(240, 240, 1))\n    \n    model.load_weights(weights_path)\n    \n    #print(\"Read \", img_name) #057011\n    #####   Read image #####\n    gray_img = cv2.imread('../input/imdata/im/057011.jpg', cv2.IMREAD_GRAYSCALE)\n    print(gray_img.shape)\n    #origin_shape = gray_img.shape\n    label = cv2.imread('../input/segdata/seg/057011.jpg', cv2.IMREAD_GRAYSCALE)\n          \n    #gray_img_resize = cv2.resize(gray_img, (240, 240), interpolation=cv2.INTER_LINEAR)\n                                    \n    #label = cv2.resize(label, (240, 240), interpolation=cv2.INTER_LINEAR)\n          \n    gray_img = gray_img / 255.0\n    \n            \n    gray_img_resize = gray_img.reshape((1, gray_img.shape[0], gray_img.shape[1], 1))\n    print(gray_img_resize.shape)\n    \n    predict_mask = model.predict(gray_img_resize)\n    #print(predict_mask)\n    gray_img = gray_img * 255.0\n   \n    predict_mask = np.round(predict_mask)\n    predict_mask = predict_mask * 255.0  \n    predict_mask = np.squeeze(predict_mask) \n    #predict_mask = cv2.resize(predict_mask, gray_img.shape, interpolation=cv2.INTER_LINEAR)\n    predict_mask = np.ceil(predict_mask)\n    \n    \n    fig1 = plt.figure()\n    plt.subplot(2, 2, 1)\n    plt.title('Origin')\n    plt.axis('off')\n    plt.imshow(np.squeeze(gray_img),cmap='gray')\n    \n    plt.subplot(2, 2, 2)\n    plt.title('Predicted Mask')\n    plt.axis('off')\n    plt.imshow(np.squeeze(gray_img),cmap='gray')\n    plt.imshow(np.squeeze(predict_mask),alpha=0.3,cmap='Reds')\n    #plt.imshow(np.squeeze(label),alpha=0.3,cmap='Greens')\n    \n    plt.subplot(2, 2, 3)\n    plt.title('Ground Truth')\n    plt.axis('off')\n    plt.imshow(np.squeeze(gray_img),cmap='gray')\n    #plt.imshow(np.squeeze(predict_mask),alpha=0.3,cmap='Reds')\n    plt.imshow(label,alpha=0.3,cmap='Greens')\n    \n    \n    plt.subplot(2, 2, 4)\n    plt.title('Overlap')\n    plt.axis('off')\n    plt.imshow(np.squeeze(gray_img),cmap='gray')\n    plt.imshow(np.squeeze(predict_mask),alpha=0.3,cmap='Reds')\n    plt.imshow(np.squeeze(label),alpha=0.3,cmap='Greens')\n    file = './TEST' + img_name + '.jpg' \n    plt.savefig(file)\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-29T11:56:57.363436Z","iopub.execute_input":"2021-06-29T11:56:57.364059Z","iopub.status.idle":"2021-06-29T11:56:57.377036Z","shell.execute_reply.started":"2021-06-29T11:56:57.36401Z","shell.execute_reply":"2021-06-29T11:56:57.375484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}