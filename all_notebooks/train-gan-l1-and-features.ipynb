{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Training GAN with pretrained models"},{"metadata":{},"cell_type":"markdown","source":"A DCGAN model is trained for image enhancement(superres, decrappify). <br>The dataset used is Flicker Image Dataset, availabe on Kaggle.\nFor training the model synthetic data is generated as in kernel: https://www.kaggle.com/greenahn/crappify-imgs<br>and saved to disk, which in conjunction with high resolution images are used to train the model.<br><br>\nThis model is trained by incorporating feature generated from vgg16 model in loss function, as in paper on neural art transfer.<br>\nFor more details, find the github repository at: https://github.com/nupam/GANs-for-Image-enhancement"},{"metadata":{},"cell_type":"markdown","source":"Both pretrained generator and discriminator models are loaded from disk, output file of kernel,<br> https://www.kaggle.com/greenahn/pretrain-gan-feature-loss.<br>\nThey are then put together as a GAN, and trained."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\nfrom fastai.vision.gan import *\nimport gc\nfrom torchvision.models import vgg16_bn","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## These folders contain crappy images in different resolution with differnt crappafication logic (randomly selected)\norig_path = Path('../input/flickrproc/hr/hr')\nfnames_df = pd.read_csv('../input/flickrproc/files.csv')\nbs = 16\nFOLDERS = {256:Path('../input/flickrproc/crappy_256/crappy/'), 320:Path('../input/flickrproc/crappy_320/crappy/'), }\nFOLDERS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"##loading training data\n## if dummy=True is provided, then dataset of ony 32 images is retured\ndef get_data(size=None, bs=None, folder=320, split=0.9, dummy=False):\n    if dummy:\n        if bs is None: bs = 1\n            \n        if size is None: \n            data = ImageImageList.from_df(fnames_df.iloc[:32], path = FOLDERS[320], cols='name').split_by_rand_pct(0.2, seed=34).label_from_func(lambda x: orig_path/Path(x).name).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n        else:\n            data = ImageImageList.from_df(fnames_df.iloc[:32], path = FOLDERS[320], cols='name').split_by_rand_pct(0.2, seed=34).label_from_func(lambda x: orig_path/Path(x).name).transform([], size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n        data.c = 3\n        return data\n    \n    if bs is None: \n        raise ValueError('Batchsize is not provided')\n    if size is None:\n        raise ValueError('Size of image is not provided')\n    \n    folder = FOLDERS[folder]\n    src = ImageImageList.from_df(fnames_df, \n                           path = folder, cols='name')\n    src = src.split_by_idx(np.arange(int(src.items.shape[0]*split), src.items.shape[0]))\n    \n    data = src.label_from_func(lambda x: orig_path/Path(x).name).transform(get_transforms(max_zoom=1.2), size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n    data.c = 3\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_loss = F.l1_loss\narch = models.resnet34\n\nvgg_m = vgg16_bn(True).features.cuda().eval()\nrequires_grad(vgg_m, False)\nblocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n\ndef gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(n, c, -1)\n    return (x @ x.transpose(1,2))/(c*h*w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature loss\nAs in paper on neural art tranfer, https://arxiv.org/abs/1508.06576.\nL1 pixel distance is also added to loss.<br>\nPrevents mode collapse and supervises for stabe and faster training."},{"metadata":{"trusted":true},"cell_type":"code","source":"class FeatureLoss(nn.Module):\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        super().__init__()\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target):\n        out_feat = self.make_features(target, clone=True)\n        in_feat = self.make_features(input)\n        self.feat_losses = [base_loss(input,target)]\n        self.feat_losses += [base_loss(f_in, f_out)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_data = get_data(bs=bs, size=128, folder=320)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wd = 1e-3\ny_range = (-3.,3.)\n\nlearn_gen = unet_learner(gen_data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics, blur=True, norm_type=NormType.Weight, model_dir=\"/kaggle/working\", y_range=y_range)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.load('../input/train-gan-l1-and-features/gen-256')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How good is pretrained model?"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.load('../input/train-gan-l1-and-features/gen-256')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gen = load_learner('../input/train-gan-l1-and-features/')\n# gen.data = get_data(dummy=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\n\ndef enhance(link):\n    !wget {link}\n    !mv download pic.jpg\n    img = open_image('pic.jpg')\n    img.resize((3, *resize_to(img, 720, use_min=True)))\n    img.refresh()\n    print(img.size)\n    img.save('original.jpg')\n    out0 = gen.predict(img)\n    img.flip_lr()\n    out1 = gen.predict(img)\n    out1[0].flip_lr()\n    img.flip_lr()\n    out0[0].save('0.jpg')\n    out1[0].save('1.jpg')\n    temp = (out0[0].data + out1[0].data)/2\n\n    temp = fastai.vision.image2np(temp)\n    plt.figure(figsize = (20,20))\n    plt.imshow(temp)\n    plt.imsave( 'avg.jpg', temp)\n    !tar -czf images.tar *.jpg\n    return FileLink('images.tar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#enhance('https://cloud.anupam.gq/index.php/s/ToQas9LjEConSHw/download')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results(rows=10, figsize=(24, 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading critic"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_critic_data(bs, size=256, split=0.9):\n    \n    def labeler(x):\n        ret = 'generated' if Path(x).parent.name == 'crappy' else 'original'\n        return ret\n    \n    df = fnames_df\n    valid_names = list(df['name'].iloc[int(split*len(df)):])\n    \n    src1 = ImageList.from_df(df, path = Path('../input/flickrproc/crappy_320')/'crappy', cols='name')\n    src2 = ImageList.from_df(df, path = orig_path, cols='name')\n    src1.add(items=src2)\n    \n    src = src1.split_by_valid_func(lambda x : Path(x).name in valid_names)\n    data = src.label_from_func(labeler)\n    data = data.transform(get_transforms(), size=size).databunch(bs=bs).normalize(imagenet_stats)\n    \n    data.c = 3\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_critic = get_critic_data(bs, 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_critic.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())\ndef create_critic_learner(data, metrics):\n    return   Learner(data_critic, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd, model_dir=\"/kaggle/working\")\nlearn_critic = create_critic_learner(data_critic, accuracy_thresh_expand)\nlearn_critic.load('../input/train-gan-l1-and-features/critic-256')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GAN\n**Putting both models together as a GAN**"},{"metadata":{},"cell_type":"markdown","source":"Training is done by adaptiveliy switching between discriminator and generator.<br>Discriminator is trained whenever discriminator loss drops below 0.65."},{"metadata":{"trusted":true},"cell_type":"code","source":"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\nlearn = GANLearner.from_learners(learn_gen, learn_critic, weights_gen=(1.,50.), show_img=True, switcher=switcher,\n                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd, model_dir=\"/kaggle/working\", gen_first=True)\nlearn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del learn.data\nlearn.data = get_data(256,bs//2, 320)\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.show_results(rows=20, figsize=(30, 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit(16,1e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.show_results(rows=20, figsize=(30, 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving models"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.save('gen-256')\nlearn_critic.save('critic-256')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn_gen.export(\"/kaggle/working/export.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}