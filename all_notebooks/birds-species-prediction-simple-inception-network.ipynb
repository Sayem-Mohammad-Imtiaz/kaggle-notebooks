{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow_datasets as tfds\nimport pathlib\nimport os","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:42.580778Z","iopub.execute_input":"2021-07-29T09:59:42.581275Z","iopub.status.idle":"2021-07-29T09:59:42.589224Z","shell.execute_reply.started":"2021-07-29T09:59:42.581242Z","shell.execute_reply":"2021-07-29T09:59:42.587774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1- input pipeline using tf.data","metadata":{}},{"cell_type":"code","source":"#training_data_path = pathlib.Path(r\"H:\\AI\\DataSets\\275 Bird Species also see 73 Sports Dataset\\birds_rev2\\train\")\n#validatoin_data_path = pathlib.Path(r\"H:\\AI\\DataSets\\275 Bird Species also see 73 Sports Dataset\\birds_rev2\\valid\")\n#testing_data_path = pathlib.Path(r\"H:\\AI\\DataSets\\275 Bird Species also see 73 Sports Dataset\\birds_rev2\\test\")\n\n#batch_size = 64\n#img_height = 224\n#img_width = 224\n\n#train_image_count = len(list(training_data_path.glob(r'*\\\\*.jpg')))\n\n#train_list_ds = tf.data.Dataset.list_files([str(training_data_path/'*/*'), \n#                                           str(validatoin_data_path/'*/*'),\n#                                           str(testing_data_path/'*/*')], shuffle=False)\n#train_list_ds = train_list_ds.shuffle(train_image_count, reshuffle_each_iteration=False)\n#------------------------------------------------------------------------------------------------------\n#val_image_count = len(list(validatoin_data_path.glob(r'*\\\\*.jpg')))\n\n#val_list_ds = tf.data.Dataset.list_files(str(validatoin_data_path/'*/*'), shuffle=False)\n#val_list_ds = val_list_ds.shuffle(val_image_count, reshuffle_each_iteration=False)\n#------------------------------------------------------------------------------------------------------\n#test_image_count = len(list(training_data_path.glob(r'*\\\\*.jpg')))\n\n#test_list_ds = tf.data.Dataset.list_files(str(testing_data_path/'*/*'), shuffle=False)\n#test_list_ds = test_list_ds.shuffle(test_image_count, reshuffle_each_iteration=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:58:19.602857Z","iopub.execute_input":"2021-07-29T09:58:19.603373Z","iopub.status.idle":"2021-07-29T09:58:19.609698Z","shell.execute_reply.started":"2021-07-29T09:58:19.603315Z","shell.execute_reply":"2021-07-29T09:58:19.608022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data_path = pathlib.Path('../input/100-bird-species/birds_rev2/train')\nvalidatoin_data_path = pathlib.Path('../input/100-bird-species/birds_rev2/valid')\ntesting_data_path = pathlib.Path('../input/100-bird-species/birds_rev2/test')\n\nbatch_size = 64\nimg_height = 150\nimg_width = 150\n\n\nimage_count = len(list(training_data_path.glob(r'*/*.jpg')))\n\nall_list_ds = tf.data.Dataset.list_files([str(training_data_path/'*/*'), \n                                          str(validatoin_data_path/'*/*'),\n                                          str(testing_data_path/'*/*')], shuffle=False)\nall_list_ds = all_list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n\n\nval_size = int(image_count * 0.2)\n#reduce the training set size fue to the small memory allocated for this notebook\nskip_train = int(image_count * 0.6)\ntrain_ds = all_list_ds.skip(skip_train)\nval_ds = all_list_ds.take(val_size)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:44.566792Z","iopub.execute_input":"2021-07-29T09:59:44.567231Z","iopub.status.idle":"2021-07-29T09:59:50.028468Z","shell.execute_reply.started":"2021-07-29T09:59:44.5672Z","shell.execute_reply":"2021-07-29T09:59:50.027143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = np.array(sorted([item.name for item in training_data_path.glob('*')]))\nnum_classes = len([item for item in training_data_path.glob('*')])\nprint(\"The number of classes is: {}\".format(num_classes))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:50.032489Z","iopub.execute_input":"2021-07-29T09:59:50.032793Z","iopub.status.idle":"2021-07-29T09:59:50.047428Z","shell.execute_reply.started":"2021-07-29T09:59:50.032755Z","shell.execute_reply":"2021-07-29T09:59:50.0462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.data.experimental.cardinality(train_ds).numpy())\nprint(tf.data.experimental.cardinality(val_ds).numpy())","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:50.051556Z","iopub.execute_input":"2021-07-29T09:59:50.051954Z","iopub.status.idle":"2021-07-29T09:59:50.070008Z","shell.execute_reply.started":"2021-07-29T09:59:50.051925Z","shell.execute_reply":"2021-07-29T09:59:50.068871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Write a short function that converts a file path to an (img, label) pair:**\n\nFor more details:\n\nhttps://www.tensorflow.org/tutorials/load_data/images#using_tfdata_for_finer_control","metadata":{}},{"cell_type":"code","source":"def process_path(file_path):\n    \n    parts = tf.strings.split(file_path, os.path.sep)\n    one_hot = parts[-2] == class_names\n    label = tf.argmax(one_hot)\n    \n    img = tf.io.read_file(file_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [img_height, img_width])\n\n    return img, label","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:50.074166Z","iopub.execute_input":"2021-07-29T09:59:50.074716Z","iopub.status.idle":"2021-07-29T09:59:50.084136Z","shell.execute_reply.started":"2021-07-29T09:59:50.074663Z","shell.execute_reply":"2021-07-29T09:59:50.082885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Use `Dataset.map` to create a dataset of image, label pairs.**\n\nFor more details:\n\nhttps://www.tensorflow.org/tutorials/load_data/images#using_tfdata_for_finer_control","metadata":{}},{"cell_type":"markdown","source":"Set `num_parallel_calls` so multiple images are loaded/processed in parallel.","metadata":{}},{"cell_type":"code","source":"train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\nval_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:50.086172Z","iopub.execute_input":"2021-07-29T09:59:50.086665Z","iopub.status.idle":"2021-07-29T09:59:50.286413Z","shell.execute_reply.started":"2021-07-29T09:59:50.086593Z","shell.execute_reply":"2021-07-29T09:59:50.285254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dead code\n\n#train_ds = train_list_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n#val_ds = val_list_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n#test_ds = test_list_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:50.288974Z","iopub.execute_input":"2021-07-29T09:59:50.289393Z","iopub.status.idle":"2021-07-29T09:59:50.295945Z","shell.execute_reply.started":"2021-07-29T09:59:50.289353Z","shell.execute_reply":"2021-07-29T09:59:50.293269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Configure dataset for performance** <br>\n\nTo train a model with this dataset you will want the data:<br>\n\n- To be well shuffled.<br>\n- To be batched.<br>\n- Batches to be available as soon as possible.<br>\n\nThese features can be added using the tf.data API. For more details:\n\nhttps://www.tensorflow.org/tutorials/load_data/images#using_tfdata_for_finer_control\n\n\n`.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\nThis will ensure the dataset does not become a bottleneck while training your model.\nIf your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n\n`.prefetch()` overlaps data preprocessing and model execution while training.\n\nsource: https://www.tensorflow.org/tutorials/load_data/images#configure_the_dataset_for_perfor","metadata":{}},{"cell_type":"code","source":"def configure_for_performance(ds):\n    ds = ds.cache()\n    ds = ds.shuffle(buffer_size=1024)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds\n\n#dead code\n#train_ds = configure_for_performance(train_ds)\n#val_ds = configure_for_performance(val_ds)\n#test_ds = configure_for_performance(test_ds)\n\ntrain_ds = configure_for_performance(train_ds)\nval_ds = configure_for_performance(val_ds)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:50.298177Z","iopub.execute_input":"2021-07-29T09:59:50.298711Z","iopub.status.idle":"2021-07-29T09:59:50.311325Z","shell.execute_reply.started":"2021-07-29T09:59:50.29865Z","shell.execute_reply":"2021-07-29T09:59:50.310092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize the data**\n\nFor more details:\n\nhttps://www.tensorflow.org/tutorials/load_data/images#using_tfdata_for_finer_control","metadata":{}},{"cell_type":"code","source":"image_batch, label_batch = next(iter(train_ds))\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n    label = label_batch[i]\n    plt.title(class_names[label])\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:50.315228Z","iopub.execute_input":"2021-07-29T09:59:50.3158Z","iopub.status.idle":"2021-07-29T09:59:54.328298Z","shell.execute_reply.started":"2021-07-29T09:59:50.315754Z","shell.execute_reply":"2021-07-29T09:59:54.327041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2- Data augmentation Using tf.image","metadata":{}},{"cell_type":"markdown","source":"A technique to increase the diversity of your training set by applying random (but realistic) transformations such as image rotation\n\nThere are a variety of preprocessing layers you can use for data augmentation including RandomContrast, RandomCrop, RandomZoom, RandomFlip, RandomRotation and others.\n\nTere are two ways to implement data augmentation usin tensorflow. the first approch is Make the preprocessing layers part of the model, here When you export your model using model.save, the preprocessing layers will be saved along with the rest of your model. this will help us when we deploy the model it will automatically standardize images.The second approch is Apply the preprocessing layers to your dataset\n\nsource: https://www.tensorflow.org/tutorials/images/data_augmentation\n\nI will use the second option in this notebook.","metadata":{}},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\", seed=0),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2, seed=0),\n    tf.keras.layers.experimental.preprocessing.RandomCrop(height=150, width=150, seed=0),\n    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.3, width_factor=0.3, seed=0)])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:54.33086Z","iopub.execute_input":"2021-07-29T09:59:54.331256Z","iopub.status.idle":"2021-07-29T09:59:54.363195Z","shell.execute_reply.started":"2021-07-29T09:59:54.331217Z","shell.execute_reply":"2021-07-29T09:59:54.362003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), \n#                        num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:54.364913Z","iopub.execute_input":"2021-07-29T09:59:54.365446Z","iopub.status.idle":"2021-07-29T09:59:54.370265Z","shell.execute_reply.started":"2021-07-29T09:59:54.3654Z","shell.execute_reply":"2021-07-29T09:59:54.369005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3- creating a simple model as a baseline","metadata":{}},{"cell_type":"markdown","source":"**define the convolutional base using a common pattern: a stack of `Conv2D` and `MaxPooling2D` layers.**\n\nThe CNN takes a tensor of shape (height, width, channels) 3D tensor (R, G, B) without givving it the batch size.\nW will give the input shape to your first layer (180, 180, 3) by passing the argument `input_shape`.\n\nsource: https://www.tensorflow.org/tutorials/images/cnn","metadata":{}},{"cell_type":"code","source":"baseline_model = tf.keras.models.Sequential()\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(150, 150, 3) ))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\nbaseline_model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))\nbaseline_model.add(tf.keras.layers.Conv2D(128, (1, 1), padding='same', activation='relu'))\nbaseline_model.add(tf.keras.layers.MaxPool2D(pool_size=(3, 3)))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:54.372204Z","iopub.execute_input":"2021-07-29T09:59:54.372964Z","iopub.status.idle":"2021-07-29T09:59:54.455594Z","shell.execute_reply.started":"2021-07-29T09:59:54.372919Z","shell.execute_reply":"2021-07-29T09:59:54.45449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:54.457462Z","iopub.execute_input":"2021-07-29T09:59:54.457951Z","iopub.status.idle":"2021-07-29T09:59:54.473938Z","shell.execute_reply.started":"2021-07-29T09:59:54.457905Z","shell.execute_reply":"2021-07-29T09:59:54.47242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). as we can see\nthe height and the width shrink as we go depper in the network, we can control the number of output channels \nby for each Conv2D by the first argument (e.g., filters=32)\n\nsource: https://www.tensorflow.org/tutorials/images/cnn","metadata":{}},{"cell_type":"markdown","source":"**Add Dense layers on top**\n\nFinally we will feed the last output tensor from the the convolutional base (of shape (20, 20, 64)) into Dense layers\nthat will perform the classification. First we need to Flatten the 3D tensor to 1D a `Flatten` layer, then we will add \none or more dense layers on top of the flatten layer, we have 275 classes so the final dense layer \nwill have 275 units.\n\n\nsource: https://www.tensorflow.org/tutorials/images/cnn\n\n\nThe final layer I will use the softmax activations that used for multicalss classification, \nfor the  layer that preceding the final layer I will use 512 units to avoid information bottleneck.\n\n\nAfter the first two layers I will add `BatchNormalization` layer to reduce the danger of **Vanishing/Exploding Gradient** \nproblems ","metadata":{}},{"cell_type":"code","source":"baseline_model.add(tf.keras.layers.Flatten())\n\n#dead code\n#baseline_model.add(tf.keras.layers.Dense(1024, activation='relu',\n#                                        kernel_initializer='he_normal'))\n#baseline_model.add(tf.keras.layers.BatchNormalization())\n#baseline_model.add(tf.keras.layers.Dropout(0.4))\n\nbaseline_model.add(tf.keras.layers.Dense(512, activation='relu',\n                                        kernel_initializer='he_normal'))\nbaseline_model.add(tf.keras.layers.BatchNormalization())\nbaseline_model.add(tf.keras.layers.Dropout(0.4))\n\nbaseline_model.add(tf.keras.layers.Dense(275, activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:54.475753Z","iopub.execute_input":"2021-07-29T09:59:54.476347Z","iopub.status.idle":"2021-07-29T09:59:54.53216Z","shell.execute_reply.started":"2021-07-29T09:59:54.476299Z","shell.execute_reply":"2021-07-29T09:59:54.531028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T09:59:54.533765Z","iopub.execute_input":"2021-07-29T09:59:54.534222Z","iopub.status.idle":"2021-07-29T09:59:54.553278Z","shell.execute_reply.started":"2021-07-29T09:59:54.534178Z","shell.execute_reply":"2021-07-29T09:59:54.551868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compile and train the model**","metadata":{}},{"cell_type":"code","source":"baseline_model.compile(optimizer='adam',\n                       loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                       metrics=['accuracy'])\n\nhistory = baseline_model.fit(train_ds, epochs=15, \n                             validation_data=val_ds)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-29T09:59:54.555856Z","iopub.execute_input":"2021-07-29T09:59:54.556197Z","iopub.status.idle":"2021-07-29T10:03:38.804699Z","shell.execute_reply.started":"2021-07-29T09:59:54.556164Z","shell.execute_reply":"2021-07-29T10:03:38.798963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting the training and the validation losses and accuracies as a function of the number of iterations**","metadata":{}},{"cell_type":"code","source":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nplt.plot(history.epoch, train_loss, label='Training Loss')\nplt.plot(history.epoch, val_loss, label='Validation Loss')\nplt.grid(True)\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(history.epoch, train_acc, label='Training Accuracy')\nplt.plot(history.epoch, val_acc, label='Validation Accuracy')\nplt.grid(True)\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dead code\n\n#testset_loss, testest_acc = baseline_model.evaluate(test_ds)\n#print(\"The test set loss: {}, Test set Accuracy: {}\".format(testset_loss, testest_acc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4- Simple Inception Network","metadata":{}},{"cell_type":"code","source":"def InceptionBlock(previous_output):\n    \n    kernel_init = tf.keras.initializers.glorot_uniform()\n    \n    conv_1x1_1 = tf.keras.layers.Conv2D(32, (1, 1), padding='same', activation='relu',\n                                        kernel_initializer=kernel_init)(previous_output)\n    \n    conv_3x3 = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu',\n                                      kernel_initializer=kernel_init)(previous_output)\n    conv_1x1_2 = tf.keras.layers.Conv2D(48, (1, 1), padding='same', activation='relu',\n                                        kernel_initializer=kernel_init)(conv_3x3)\n    \n    conv_5x5 = tf.keras.layers.Conv2D(32, (5, 5), padding='same', activation='relu',\n                                      kernel_initializer=kernel_init)(previous_output)\n    conv_1x1_3 = tf.keras.layers.Conv2D(16, (1, 1), padding='same', activation='relu',\n                                        kernel_initializer=kernel_init)(conv_5x5)\n    \n    conv_1x1_4 = tf.keras.layers.Conv2D(32, (1, 1), padding='same', activation='relu',\n                                        kernel_initializer=kernel_init)(previous_output)\n    maxpool = tf.keras.layers.MaxPool2D((3, 3), strides=(1, 1), padding='same')(conv_1x1_4)\n    \n    concat = tf.keras.layers.concatenate([conv_1x1_1, conv_1x1_2, conv_1x1_3, maxpool], axis=-1)\n    \n    return concat","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:12:11.01973Z","iopub.execute_input":"2021-07-29T10:12:11.020182Z","iopub.status.idle":"2021-07-29T10:12:11.033298Z","shell.execute_reply.started":"2021-07-29T10:12:11.02015Z","shell.execute_reply":"2021-07-29T10:12:11.0315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adding an Auxiliary output for fitting gradient vanishing and for regularization**","metadata":{}},{"cell_type":"code","source":"def aux_output(previous_output):\n    \n    \n    conv1 = tf.keras.layers.Conv2D(128, kernel_size=(1, 1), padding='same',\n                                     activation='relu')(previous_output)\n    \n    avg_pooling = tf.keras.layers.AveragePooling2D(5, strides=3)(conv1)\n    \n    x = tf.keras.layers.Flatten()(avg_pooling)\n        \n    x = tf.keras.layers.Dense(512, activation='relu',\n                              kernel_initializer='he_normal')(x)\n    \n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    aux_out = tf.keras.layers.Dense(275, activation='softmax', name='auxiliary_output')(x)\n    \n    return aux_out","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:12:11.31209Z","iopub.execute_input":"2021-07-29T10:12:11.312432Z","iopub.status.idle":"2021-07-29T10:12:11.321384Z","shell.execute_reply.started":"2021-07-29T10:12:11.312398Z","shell.execute_reply":"2021-07-29T10:12:11.31965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_input = tf.keras.Input(shape=(150, 150, 3))\n\nx = tf.keras.layers.Conv2D(64, (7, 7), strides=(2, 2), activation='relu')(model_input)\nx = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n\nx = tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1))(x)\n\nx = InceptionBlock(x)\nx = InceptionBlock(x)\naux_out_1 = aux_output(x)\nx = tf.keras.layers.MaxPool2D((3, 3), padding='same', strides=(2, 2))(x)\n\nx = InceptionBlock(x)\nx = InceptionBlock(x)\n\nx = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(x)\nx = tf.keras.layers.MaxPool2D(pool_size=5)(x)\n\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(512, activation='relu',\n                                        kernel_initializer='he_normal')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.5)(x)\nmodel_output = tf.keras.layers.Dense(275, activation='softmax', name='main_output')(x)\n\ninc_model = tf.keras.models.Model(inputs=model_input, outputs=[model_output, aux_out_1])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:12:11.470602Z","iopub.execute_input":"2021-07-29T10:12:11.470971Z","iopub.status.idle":"2021-07-29T10:12:11.873781Z","shell.execute_reply.started":"2021-07-29T10:12:11.470942Z","shell.execute_reply":"2021-07-29T10:12:11.872605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(inc_model,\n                          show_shapes=True,\n                          show_dtype=True,\n                          show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:12:11.875749Z","iopub.execute_input":"2021-07-29T10:12:11.87626Z","iopub.status.idle":"2021-07-29T10:12:13.07722Z","shell.execute_reply.started":"2021-07-29T10:12:11.876201Z","shell.execute_reply":"2021-07-29T10:12:13.075945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inc_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:12:13.079876Z","iopub.execute_input":"2021-07-29T10:12:13.080562Z","iopub.status.idle":"2021-07-29T10:12:13.133419Z","shell.execute_reply.started":"2021-07-29T10:12:13.080499Z","shell.execute_reply":"2021-07-29T10:12:13.132393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inc_model.compile(optimizer='adam',\n                  loss=[tf.keras.losses.SparseCategoricalCrossentropy(),\n                       tf.keras.losses.SparseCategoricalCrossentropy()],\n                  metrics=['accuracy'])\n\n\n\nhistory = inc_model.fit(train_ds, epochs=25, \n                        validation_data=val_ds)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-29T10:12:26.009847Z","iopub.execute_input":"2021-07-29T10:12:26.010831Z","iopub.status.idle":"2021-07-29T10:22:38.418341Z","shell.execute_reply.started":"2021-07-29T10:12:26.010778Z","shell.execute_reply":"2021-07-29T10:22:38.416141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = history.history['main_output_loss']\nval_loss = history.history['val_main_output_loss']\n\n\nplt.plot(history.epoch, train_loss, label='Training Loss')\nplt.plot(history.epoch, val_loss, label='Validation Loss')\nplt.grid(True)\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:22:44.548655Z","iopub.execute_input":"2021-07-29T10:22:44.549033Z","iopub.status.idle":"2021-07-29T10:22:44.799975Z","shell.execute_reply.started":"2021-07-29T10:22:44.548986Z","shell.execute_reply":"2021-07-29T10:22:44.798798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_acc = history.history['main_output_accuracy']\nval_acc = history.history['val_main_output_accuracy']\n\nplt.plot(history.epoch, train_acc, label='Training Accuracy')\nplt.plot(history.epoch, val_acc, label='Validation Accuracy')\nplt.grid(True)\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:22:47.338543Z","iopub.execute_input":"2021-07-29T10:22:47.338962Z","iopub.status.idle":"2021-07-29T10:22:47.576978Z","shell.execute_reply.started":"2021-07-29T10:22:47.33893Z","shell.execute_reply":"2021-07-29T10:22:47.575826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}