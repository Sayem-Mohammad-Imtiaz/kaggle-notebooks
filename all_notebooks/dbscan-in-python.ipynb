{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is DBSCAN?\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise and is one of clustering algorithms.\nAs the name of paper suggests the core idea of DBSCAN is around concept of dense regions. The assumption is that natural clusters are composed of densely located points. This requires definition of “dense region”. To do these two parameters are required for DBSCAN algorithm.\n\n* Eps, ε - distance\n* MinPts – Minimum number of points within distance Eps\n\nA “dense region” is therefore created by a minimum number of points within distance between all of them, Eps. Points which are within this distance but not close to minimum number of other points are treated as “border points”. Remaining ones are noise or outliers. This is shown in the picture below (for MinPts=3). Red points (D) are in a “dense region” – each one has minimum of 3 neighbours within distance Eps. Green points (B) are border ones – they have a neighbour within distance Eps but less than 3. Blue point (O) is an outlier – no neighbours within distance Eps.\n\n### Advantages of this approach:\n\n* it finds number of clusters itself, based on eps and MinPts parameters\n* It it able to differentiate elongated clusters or clusters surrounded by other clusters in contrary to e.g. K-Means where clusters are always convex.\n* It is also able to find points not fitting into any cluster – it detects outliers.\n\n### The biggest drawback of DBSCAN:\n\n* High computational expense of average O(n log(n)) coming from a need to execute a neighbourhood query for each point.\n* Poorly identifies clusters with various densities","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## If this Kernel helped you in any way, UPVOTES would be very much appreciated","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\n# import required libraries for clustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/online-retail-customer-clustering/OnlineRetail.csv', sep=\",\", encoding=\"ISO-8859-1\", header=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first five row\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# size of datset\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# statistical summary of numerical variables\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary about dataset\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\ndf.isna().sum() / df.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping rows having missing values\n\ndf = df.dropna()\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation\n\nWe are going to analysis the Customers based on below 3 factors:\n* R (Recency): Number of days since last purchase\n* F (Frequency): Number of tracsactions\n* M (Monetary): Total amount of transactions (revenue contributed)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# New Attribute : Monetary\n\ndf['Amount'] = df['Quantity']*df['UnitPrice']\n\nrfm_m = df.groupby('CustomerID')['Amount'].sum()\nrfm_m = rfm_m.reset_index()\nrfm_m.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New Attribute : Frequency\n\nrfm_f = df.groupby('CustomerID')['InvoiceNo'].count()\n\nrfm_f = rfm_f.reset_index()\nrfm_f.columns = ['CustomerID', 'Frequency']\nrfm_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the two dfs\n\nrfm = pd.merge(rfm_m, rfm_f, on='CustomerID', how='inner')\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New Attribute : Recency\n\n# Convert to datetime to proper datatype\n\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'],format='%d-%m-%Y %H:%M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the maximum date to know the last transaction date\n\nmax_date = max(df['InvoiceDate'])\nmax_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the difference between max date and transaction date\n\ndf['Diff'] = max_date - df['InvoiceDate']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute last transaction date to get the recency of customers\n\nrfm_p = df.groupby('CustomerID')['Diff'].min()\n\nrfm_p = rfm_p.reset_index()\nrfm_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract number of days only\n\nrfm_p['Diff'] = rfm_p['Diff'].dt.days\nrfm_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge tha dataframes to get the final RFM dataframe\n\nrfm = pd.merge(rfm, rfm_p, on='CustomerID', how='inner')\n\nrfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2 types of outliers and we will treat outliers as it can skew our dataset¶\n* Statistical\n* Domain specific","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier Analysis of Amount Frequency and Recency\n\nattributes = ['Amount','Frequency','Recency']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"Attributes\", fontweight = 'bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing (statistical) outliers for Amount\nQ1 = rfm.Amount.quantile(0.05)\nQ3 = rfm.Amount.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Recency\nQ1 = rfm.Recency.quantile(0.05)\nQ3 = rfm.Recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Frequency\nQ1 = rfm.Frequency.quantile(0.05)\nQ3 = rfm.Frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the Attributes\nIt is extremely important to rescale the variables so that they have a comparable scale.| There are two common ways of rescaling:\n\n* Min-Max scaling\n* Standardisation\n\nHere, we will use Standardisation Scaling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rescaling the attributes\n\nrfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n\n# Instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']\n\nrfm_df_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an object\ndb = DBSCAN(eps=0.8, min_samples=7, metric='euclidean')\n\n# fit the model\ndb.fit(rfm_df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cluster labled\ndb.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation\n\n### Silhouette","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\ncluster_labels = db.labels_   \n\n# silhouette score\nsilhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\nprint(\"The silhouette score is\", format(silhouette_avg))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_df_scaled['label']=db.labels_\n\nrfm_df_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in rfm_df_scaled.columns[:-1]:\n    plt.figure(figsize=(6,4))\n    sns.boxplot(data=rfm_df_scaled, y=c, x='label')\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}