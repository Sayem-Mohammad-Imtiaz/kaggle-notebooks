{"cells":[{"metadata":{"_uuid":"760bc17b637810d75808f09f6e5b775ddb664410","_cell_guid":"0a9cdd1c-2b3c-4ccc-9011-096d49538145"},"cell_type":"markdown","source":"<span style=\"font-size:larger;\">Introduction</span>\n<tab></tab>\nHello there! This is my very first attempt at writing a kernel. And I hope that it is up to expectation.\nI always welcome criticism, so please let me know what I can do to improve my skills.\n\nAfter taking the fourth class of deeplearning.ai - Convolutional Neural Network, I wanted test it out on some other dataset. The reason I picked this dataset is because I have written a logistic regression using TensorFlow on the original and popular MNIST dataset, and I thought this dataset is pretty interesting. \n\nMy approach to this problem is to first perform a simple CNN(just because I am excited for it) and then perform a logistic regression and compare them both. The reason behind picking logistic regression as a comparison is that in the previous MNIST problem(original), logistic regression could already perform pretty accurately, I managed to achieve 98% accuracy with just logistic regression. So if CNN is more powerful, it should be able to yield a much better accuracy.\n\nBefore diving into all that, let's take a look at the data and its distribution!\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f102c798d773a016b494c7063bbc97db82500079","_cell_guid":"e3e9775c-d740-4921-9d5f-be49f3bbc30b","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n# Import data\ndata = pd.read_csv(\"../input/handwritten_data_785.csv\", encoding = 'utf8')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a5b0edc33f20282f88e12215e92370bbd5edc95","_cell_guid":"c86dd8af-c83d-48ec-b3c7-6c32b39f479e"},"cell_type":"markdown","source":"The DataFrame as 378,037 Rows, and 785 Columns.\n\nThe first columns is the target values. Since we are predicting alphabets, it has 26 values ranging from 0 to 25. Each of the number relates to its corresponding alphabets. For example, 0 would be A, 1 would be B, and so on. \n\nEach row corresponds to an image, since the image pixel is 28 x 28, we have 28*28 = 784 numbers.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3abf10b6ab2bbe24128cf4be14ea886345e87ce7","_cell_guid":"50ec84d8-27b5-4bee-be89-5f147c663e54","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"DataFrame shape:\" + str(data.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"298ce01f34d3ab8812738260bcb8c281210bf753","_cell_guid":"11e70104-dca2-45ab-9309-2f0df0a89237"},"cell_type":"markdown","source":"The code below is just to make life simple and set a variable to X and Y.\n\ntarget : target variables\n\nfeatues: features variables","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1272514f841aa9c5cb301896e7330643842b7450","_cell_guid":"fe7b48ab-5937-49a1-8390-3b556fdf55b3","trusted":true},"cell_type":"code","source":"target = data.iloc[:,0].values.reshape(-1,1)\nfeatures = data.iloc[:, 1:]\nprint(\"Target shape:\" + str(target.shape))\nprint(\"Features shape:\" + str(features.shape))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"8e1f85ec8a9b2652a3aba1fa23ac03c07d675a2b","_cell_guid":"dc21d4dc-3c39-4d94-9452-fdf9c40c9e85","trusted":false,"collapsed":true},"cell_type":"code","source":"pd.DataFrame(target)[0].value_counts(normalize=True).sort_values(ascending=False).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ed8a95cd6858bb0ca43e98a04052df40fce56be","_cell_guid":"adf87278-e030-4f6f-bf08-abce870a86aa"},"cell_type":"markdown","source":"The graph above is the distribution of the target alphabets. As we can see, the alphabvet 'O'  has the highet precentage with almost 16% where as 'V' has the least percentage. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f2af4d64026c9707f475c53fe9f67835014ae7ba","_cell_guid":"6f2031b8-318b-4761-b7ef-40dae1f25f46","trusted":true},"cell_type":"code","source":"# Import pacakges\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Input, Activation\nfrom keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d74a9d46ee2674013fcc4af0548c1463128409c","_cell_guid":"e6489cbc-a882-4ef8-8bc9-404f73918d28"},"cell_type":"markdown","source":"We have to do a little data preprocessing.\n\nFirst we have to set an input_shape variable, so that it will be easier to feed it into our model later.\n\nWe also have to convert our target variables into categorical variables and split the data into trainng and test set. \n\nI understand that Professor Andrew Ng said that we should have a training set, validation set and test set. But just for simplicity, and to do a quick dirty prototyping, I shall ignore the validation set for now. \n\nNext, we convert our image data from a row to a mx28x28x1 tensor and normalize it by performing an element-wise division by 255. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"844e9a84efa8b471e8b79afb947b5e248f0d6b95","_cell_guid":"75c68c46-ecc1-483e-b93b-1708f97f7822","trusted":true},"cell_type":"code","source":"img_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)\n\ntarget2 = to_categorical(target.copy())\n\nX_train, X_test, y_train, y_test = train_test_split(features, target2, test_size=0.3, random_state=42)\n\nX_train = X_train.values.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.values.reshape(X_test.shape[0], img_rows, img_cols, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train /= 255\nX_test /= 255\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95c5527d3f4211294e417a9986421bb8a6628a33","_cell_guid":"2e49b2b3-b710-4121-b699-29b4d9a90668"},"cell_type":"markdown","source":"Time to create a model!\n\nIn this simple model, first, I added a (3x3) padding to the original image and then convolve it using a (3 x 3) window with 32 filtes and (1x1) stride. Next, we perform batch normalization on the output and then feed it into a ReLU activation function. After that, we will perform a maxpooling to downsample the image. Finally, we flatten the data and feed it into a fully connected layer with a Softmax activation function. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"de77aebec83c5013ce325a3e78436ea55d76882e","_cell_guid":"a4451219-7168-4354-baf7-e541f5a550e9","collapsed":true,"trusted":false},"cell_type":"code","source":"def model(input_shape):\n    X_input = Input(input_shape)\n    X = ZeroPadding2D((3,3))(X_input)\n    X = Conv2D(32, (3,3), strides=(1,1), name = 'conv0')(X)\n    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((2,2), name='maxpool1')(X)\n    X = Flatten()(X)\n    X = Dense(26, activation='softmax', name = 'fc')(X)\n    model = Model(inputs = X_input, outputs=X, name = 'ConvModel')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d34d0e68a1e21128bd6ec7fd38f097f02227f957","_cell_guid":"17401a16-a105-4c4c-bc15-3ca82e5fd8ad","collapsed":true,"trusted":false},"cell_type":"code","source":"firstModel = model(X_train.shape[1:])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d73326581c32dcbcaccc2c1a8e92fb3b63a7260","_cell_guid":"924eb636-3d9e-4718-81ad-5c2f0b371b75","collapsed":true,"trusted":false},"cell_type":"code","source":"firstModel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a4ff1fee82da0f59e94c3b6fb5a878b19ae46d4","_cell_guid":"6efadc9a-7e0c-4e7a-a995-10e2764302d6","collapsed":true,"trusted":false},"cell_type":"code","source":"firstModel.fit(X_train, y_train, epochs = 5, batch_size = 256)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acb0efa39e33a5f84f8c163644779438ca5c4d33","_cell_guid":"a1704ac9-3526-4a18-bd82-a119a2e74054","collapsed":true,"trusted":false},"cell_type":"code","source":"firstModel.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11925bbce4aec466712bd1694b261d72207181cf","_cell_guid":"c1b43fd4-de6f-49db-99ef-edc81cfa283d","collapsed":true,"trusted":false},"cell_type":"code","source":"firstModel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54adc0227910167e4e0eae691d3844d678f6f88e","_cell_guid":"f56e1219-8034-4ea3-8f3a-318c08c36631"},"cell_type":"markdown","source":"As shown above, since the data is pretty huge, it takes approximately 10 minutes to finish running an epoch. The good thing is that the loss reduces steadily after each epoch and the accuracy rate increases. Seems like a pretty good model. \nYou can glance through the model summary above. \n\nFor this simple model, we manage to produce a 96.91% accuracy rate on the test set. Perhaps running on a longer epoch will increase the accuracy. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dc2cfda64f462ee4d1344f956002d179411be7d0","_cell_guid":"8904437e-c85c-4f88-aea2-76c8bc0cf76b"},"cell_type":"markdown","source":"The model below will explore a simple neural network to perform a 3 hidden layers softmax regression . Before doing that, let's split the train test set again and not reshaping them into a 3 dimensional array. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"6202d98ec5d9fa102d2d1cbe3ecc6dd972152dc3","_cell_guid":"ae28645c-e381-4e73-bb39-7f2cc252cf4f","trusted":true},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\ninputs = X_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3de87c5cf9e0847954c53b8b41c2cc7d3355dbb6","_cell_guid":"de433a94-86a5-4933-9e1a-1a4bd14980d7","collapsed":true,"trusted":true},"cell_type":"code","source":"def logit_model(inputs):\n    model = Sequential()\n    model.add(Dense(512, activation='relu', input_dim=inputs))\n    model.add(Dense(256, activation='relu', input_dim=512))\n    model.add(Dense(128, activation='relu', input_dim=256))\n    model.add(Dense(26, activation='softmax', input_dim=128))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"944a0738292e7e87cc1f1866609b62bef0d05210","_cell_guid":"54617b95-1ef8-4672-8c19-461141525fcb","trusted":true,"collapsed":true},"cell_type":"code","source":"log_reg = logit_model(inputs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc5cd2a3acace4b9261e8012a4f6a3659dc94409","_cell_guid":"b02341ce-d3a8-46b1-b43b-d9d602dce054","collapsed":true,"trusted":true},"cell_type":"code","source":"log_reg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0126ed48090d435970c0a1bbff3875f01d1c7ee2","_cell_guid":"94be65f6-d9ca-452a-92f2-6bbe485b5af3","trusted":true},"cell_type":"code","source":"model = log_reg.fit(X_train, y_train, epochs = 5, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3883381405f1f07d9cf0aaddbbd4b67b5547fba","_cell_guid":"03927481-7dda-4655-8c04-df20cc9813a7","trusted":true},"cell_type":"code","source":"test_score = log_reg.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34344027cf96fa4b5163d41c1cd20be064fb5129"},"cell_type":"code","source":"print('Test cost: ' + str(test_score[0]))\nprint('Test cost: ' + str(test_score[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66fe580c026b82e2589e5052ee9e12dd719cc740","_cell_guid":"cf1c3c75-06e2-4bec-9803-6f622ed90129"},"cell_type":"markdown","source":"This resulting is interesting. A 3 hidden layers softmax regression performs better than a CNN network.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"08d388c2f6f3808ea58f8134cb0f7f20814886ba"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5e976efb25ae11c51bc60db1b251886d5dc46fd"},"cell_type":"code","source":"plt.plot(range(len(model.history['loss'])), model.history['loss'], label='Training cost')\nplt.title('Training Cost')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2220dcd8c1af8a73c2dd1c705559b62bc610a6dc"},"cell_type":"code","source":"plt.plot(range(len(model.history['acc'])), model.history['acc'], 'r', label='Training Accuracy')\nplt.title('Training Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9417f8972d5b3ea6735cf999dda48cec8dd7a3d4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}