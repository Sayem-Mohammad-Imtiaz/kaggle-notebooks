{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(r\"../input/kc-house-data/kc_house_data.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dateTime=pd.DatetimeIndex(df.date)\ndf['year']=dateTime.year\ndf['month']=dateTime.month\ndf['day']=dateTime.day\ndf.drop(columns=['date'],inplace=True)\ndf.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(columns=['id']).plot(kind='box',figsize=(50,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.hist(figsize=(30,20))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(),cmap='Reds')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"priceTransform=np.log(df.price)\npriceTransform.plot(kind='hist')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RegressionAlgorithms(df,outputVariable,transformationY=np.array):\n    class color:\n        PURPLE = '\\033[95m'\n        CYAN = '\\033[96m'\n        DARKCYAN = '\\033[36m'\n        BLUE = '\\033[94m'\n        GREEN = '\\033[92m'\n        YELLOW = '\\033[93m'\n        RED = '\\033[91m'\n        BOLD = '\\033[1m'\n        UNDERLINE = '\\033[4m'\n        END = '\\033[0m'\n        \n    from sklearn.model_selection import train_test_split\n    X_train,X_test,y_train,y_test=train_test_split(df[df.columns.difference([outputVariable])],transformationY(df[outputVariable]),random_state=0)\n    \n    #linear Regression\n    print(color.PURPLE + color.BOLD + color.UNDERLINE + 'LINEAR REGRESSION\\n' + color.END)\n    from sklearn.metrics import mean_absolute_error\n    import statsmodels.api as sm\n    X_train = sm.add_constant(X_train)\n    X_test = sm.add_constant(X_test)\n    model = sm.OLS(y_train,X_train).fit()\n    print(model.summary())\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train: {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #Decision Tree Regression\n    print(color.CYAN + color.BOLD + color.UNDERLINE + 'DECISION TREE REGRESSION\\n' + color.END)\n    from sklearn.tree import DecisionTreeRegressor\n    model=DecisionTreeRegressor()\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #Random Forest Regression\n    print(color.DARKCYAN + color.BOLD + color.UNDERLINE + 'RANDOM FOREST REGRESSION\\n' + color.END)\n    from sklearn.ensemble import RandomForestRegressor\n    model=RandomForestRegressor()\n    model.fit(X_train,y_train)\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #StandardScaler\n    from sklearn.preprocessing import StandardScaler\n    ss=StandardScaler()\n    ssTrain=ss.fit_transform(X_train)\n    ssTest=ss.transform(X_test)\n    \n    #Support Vector Regression\n    print(color.BLUE + color.BOLD + color.UNDERLINE + 'SUPPORT VECTOR REGRESSION\\n' + color.END)\n    from sklearn.svm import SVR\n    model=SVR()\n    model.fit(ssTrain,y_train)\n    pred=model.predict(ssTest)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(ssTrain)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n    \n    #KNN\n    print(color.GREEN + color.BOLD + color.UNDERLINE + 'K NEAREST NEIGHBOUR\\n' + color.END)\n    from sklearn.neighbors import KNeighborsRegressor\n    model=KNeighborsRegressor()\n    model.fit(ssTrain,y_train)\n    pred=model.predict(ssTest)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(ssTrain)\n    print(f\"\\nMean Absolute Error For Train : {mean_absolute_error(y_train,pred)}\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RegressionAlgorithms(df,'price',np.log)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Going with **LINEAR REGRESSION** because it doesnt overfit the data as most of the algorithms are doing. And gives not bad accuracy","metadata":{}},{"cell_type":"code","source":"def LinearRegressionFunction(df,y=['price']): \n    from sklearn.model_selection import train_test_split\n    X_train,X_test,y_train,y_test=train_test_split(df[df.columns.difference(y)],priceTransform,random_state=0)\n    from sklearn.metrics import mean_absolute_error\n    import statsmodels.api as sm\n    X_train = sm.add_constant(X_train)\n    X_test = sm.add_constant(X_test)\n    model = sm.OLS(y_train,X_train).fit()\n    print(model.summary())\n    pred=model.predict(X_test)\n    print(f\"\\nMean Absolute Error For Test : {mean_absolute_error(y_test,pred)}\\n\")\n    pred=model.predict(X_train)\n    print(f\"\\nMean Absolute Error For Train: {mean_absolute_error(y_train,pred)}\\n\\n\\n\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LinearRegressionFunction(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LinearRegressionFunction(df,['price','id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">There Is Something Wrong With Kaggle Kernel, Because The Same Thing Run Perfectly On My Jupyter Notebook.","metadata":{}}]}