{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Collabarative filtering based Recommender System in Python Using Amazon Dataset\n\n\nA recommender system is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item.\n\nThere are two variations of this system: content based and collaborative filtering based approach.\n\n**Collaborative filtering** is a method of making automatic predictions about the interests of a user by collecting information regarding his preference by collecting data from many users (collaborating).\n\n**Content-based filtering** methods work on a description of the item and a profile of the user's preferences.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Collection\n\nThe dataset is available on Kaggle by the name of 'Amazon and Best Buy electronics'and we have linked it here with our code. It is a list of over 7,000 online reviews for 50 electronic products from websites like Amazon and Best Buy provided by Datafiniti's Product Database. The dataset includes the review date, source, rating, title, reviewer metadata, and more. It is a pre-crawled dataset, taken as subset of a bigger dataset which is availabe through Datafiniti.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing the necessary libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.sparse\nimport nltk\nimport os\nimport json\nimport math\nimport csv\nimport re\nimport time\nfrom IPython.core.interactiveshell import InteractiveShell \nInteractiveShell.ast_node_interactivity = \"all\"\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.externals import joblib\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import svds\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom collections import Counter\nfrom surprise import KNNWithMeans\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accessing dataset\nHere we are accessing dataset by given link of reading files in input directory. We are renaming the columns so that we can access those columns which are needed by their names.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_read=pd.read_csv(\"/kaggle/input/amazon-and-best-buy-electronics/DatafinitiElectronicsProductData.csv\",delimiter=',', \n                           names = ['id1', 'asins1', 'brand1', 'categories1', 'colors1', 'dateAdded1'\n                                    , 'dateUpdated1', 'dimensions1', 'ean1', 'imageURL1', 'keys1', 'manufacturer1'\n                                    , 'manufacturerno1', 'name1', 'primarycategory1', 'reviewDate1', 'reviewDateSeen1'\n                                    , 'reviewsDoRecom1','reviewsNumHelp1','reviewsRating1','reviewsSourceURL1',\n                                   'reviewsText1','reviewsTitle1','reviewsUsername1','sourceURL1','upc1','weight1',])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying the first 5 rows of dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_read.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Accessing only the needed columns in an order\nThe variable saves only these 4 columns in this particular order.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Aceessing 4 columns of choice \\n\")\ndata4read = data_read[['reviewsUsername1', 'id1','reviewsRating1', 'reviewsText1']]\nprint(\"Done!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying the first 5 rows of chosen columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data4read.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying the shape of chosen columns\nThe shape of data represents the 7300 rows of data and 4 different columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of chosen columns:\\n\", data4read.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying the datatypes of chosen columns\n\nThe datatype of chosen columns is Python object as shown below in the output cell.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Type of chosen columns:\\n\",data4read.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Displaying the information of dataset\nHere a concise summary of the dataset is displayed which shows that usernames and product IDs are unique and are 7300 in number while 164 ratings are not given and 5 reviwes have not been given.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Information about data: \\n')\ndata4read.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling missing information\nHere the missing values in columns is shown which as we had calculated previoulsy is  164 for ratings and 5 for reviews.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_cols=data4read.isnull()\nmissing_cols=missing_cols.sum()\nprint('Number of missing values across columns: \\n')\nprint(missing_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing distribution of ratings given to products","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with sns.axes_style('white'):\n    g = sns.factorplot(\"reviewsRating1\", data=data4read, aspect=2.0,kind='count')\n    g.set_ylabels(\"Total number of ratings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final data for use\nHere we are printing count of unique items in each column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The whole data: \")\n\nun_reviews=np.unique(data4read.reviewsUsername1)\nlen_un_reviews=len(np.unique(data4read.reviewsUsername1))\nprint(\"Total number of Users:\\n\", len_un_reviews)\nun_products=np.unique(data4read.id1)\nlen_un_products=len(un_products)\nprint(\"Total number of products:\\n\", len_un_products)\nun_ratings=data4read.shape[0]\nprint(\"Total number of ratings:\\n\",un_ratings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing on reviews\nHere we are performing steps of text preprocessing. These steps are needed for transferring text from human language to machine-readable format for further processing. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_read=data4read[['reviewsText1']]\nprint(reviews_read)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove columns and now we have only one column 'reviews.txt'\nimport csv\n\ninput_file = '/kaggle/input/amazon-and-best-buy-electronics/DatafinitiElectronicsProductData.csv'\noutput_file = 'output.csv'\ncols_to_remove = [0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,22,23,24,25,26] # Column indexes to be removed (starts at 0)\n\ncols_to_remove = sorted(cols_to_remove, reverse=True) # Reverse so we remove from the end first\nrow_count = 0 # Current amount of rows processed\n\nwith open(input_file, \"r\") as source:\n    reader = csv.reader(source)\n    with open(output_file, \"w\", newline='') as result:\n        writer = csv.writer(result)\n        for row in reader:\n            row_count += 1\n            print('\\r{0}'.format(row_count), end='') # Print rows processed\n            for col_index in cols_to_remove:\n                del row[col_index]\n            writer.writerow(row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Converting csv to txt \nHere we are converting csv to text file for ease of access during preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"output_file = 'output.csv'\ncsv_file = 'output.csv'\ntxt_file = 'reviews.txt'\nwith open(txt_file, \"w\") as my_output_file:\n    with open(csv_file, \"r\") as my_input_file:\n        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n    my_output_file.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Counting stopwords\nHere we are counting number of stopwords in reviews text file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n \n# Open and read in a text file.\nwith open('output.csv', \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file, delimiter=',')\n    for lines in csv_reader: \n        txt_line = csv_file.read()\n        txt_words = txt_line.split()\n\n# stopwords found counter.\nsw_found = 0\n \n# If each word checked is not in stopwords list,\n# then append word to a new text file.\nfor check_word in txt_words:\n    if not check_word.lower() in stop_words:\n        # Not found on stopword list, so append.\n        appendFile = open('stopwords-removed.csv','a')\n        appendFile.write(\" \"+check_word)\n        appendFile.close()\n    else:\n        # It's on the stopword list\n        sw_found +=1\n        print(check_word)\n \nprint(sw_found,\"stop words found and removed\")\nprint(\"Saved as 'stopwords-removed.csv' \")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# POS tagging\nHere we are doing POS tagging of reviews text file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('output.csv', \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file, delimiter=',')\n    for lines in csv_reader: \n        txt_line = csv_file.read()\n        #txt_words = txt_line.split()\n        print(\"POS::\")\n        for x in txt_line:\n            print(nltk.pos_tag(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming tokens\nHere we are stemming tokens in reviews text file using the Porter Stemmer.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"d=0\nporter = PorterStemmer()\nwith open('/kaggle/input/amazon-and-best-buy-electronics/DatafinitiElectronicsProductData.csv', \"r\") as csv_file:\n    csv_reader = csv.reader(csv_file, delimiter=',')\n    for lines in csv_reader: \n        txt_line = csv_file.read()\n        txt_words = txt_line.split()\nprint(\"Porter Stemmer::\")\nfor check_word in txt_words:\n    print(check_word,porter.stem(check_word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping the reviews column as it is of no use in collabarative filtering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data4read.drop(['reviewsText1'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of ratings given according to Username","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a=data4read.groupby(by='reviewsUsername1')['reviewsRating1'].count()\nrated_products_per_user = a.sort_values(ascending=False)\nrated_products_per_user.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Describing the statistcal info of ratings data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rated_products_per_user.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing recommendations based on popularity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_rated_products_per_user=sum(rated_products_per_user >= 10)\nprint('No of rated product more than 10 per user : {}\\n'.format(sum_rated_products_per_user) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting itemID per their ratings\nHere we are making a plot of itemID versus its ratings to get a deeper insight of the data. Here we are making a new dataframe which contains users who has given 20 or more ratings\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df=data4read.groupby(\"id1\").filter(lambda x:x['reviewsRating1'].count() >=20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_of_ratings_per_product = new_df.groupby(by='id1')['reviewsRating1'].count().sort_values(ascending=True)\n\nfig = plt.figure(figsize=plt.figaspect(.25))\nax = plt.gca()\nplt.plot(no_of_ratings_per_product.values)\nplt.xlabel('Item')\nplt.ylabel('No of ratings per item')\nplt.title('Ratings per item')\nplt.yticks(np.arange( 0.005))\nax.set_xticklabels([])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Popular products\nThe number of ratings for a product are listed to understand the popular products as shown in the plot\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"c=new_df.groupby('id1')['reviewsRating1'].count().sort_values(ascending=False)\nc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pop_products = pd.DataFrame(new_df.groupby('id1')['reviewsRating1'].count())\nmost_popular = pop_products.sort_values('reviewsRating1', ascending=True)\n\nmost_popular.head(25).plot(kind = \"barh\", title=\"Popular items ratings\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Collaberative filtering \n\nCollaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Reading dataset\nGetting the new dataframe which contains users who have given 10 or more ratings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df=data4read.groupby(\"id1\").filter(lambda x:x['reviewsRating1'].count() >=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = Reader(rating_scale=(1, 5))\nnew_data = Dataset.load_from_df(new_df,reader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting dataset \nHere we have splitted the dataset as such that the test size is 25% of data while training data size is 70% of data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = train_test_split(new_data, test_size=0.25,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using collabarative filtering (Pearson similarity, item based)\nUsing k=5 for KNN with means and pearson based similarity to predict item-based collaborative filtering.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"algo4data = KNNWithMeans(k=5, sim_options={'name': 'pearson_baseline', 'user_based': False})\nalgo4data.fit(train_set)\ntest_prediction1 = algo4data.test(test_set)\ntest_prediction1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Using collabarative filtering (Cosine similarity, item based)\n\nUsing k=5 for KNN with means and cosine based similarity to predict item-based collaborative filtering.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"algo4data = KNNWithMeans(k=5, sim_options={'name': 'cosine', 'user_based': False})\nalgo4data.fit(train_set)\ntest_prediction2 = algo4data.test(test_set)\ntest_prediction2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using collabarative filtering (Pearson similarity, item based)\n\nUsing k=3 for KNN with means and pearson based similarity to predict item-based collaborative filtering.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"algo4data = KNNWithMeans(k=3, sim_options={'name': 'pearson_baseline', 'user_based': False})\nalgo4data.fit(train_set)\ntest_prediction3 = algo4data.test(test_set)\ntest_prediction3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making predictions for sparse matrices\nHere after running the below script we get sparse matrix so that is solved by correlation matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df1=new_df.head(1000)\nmatrix1 = new_df1.pivot_table(values='reviewsRating1', index='reviewsUsername1', columns='id1', fill_value=0, aggfunc='first')\nmatrix1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transpose of matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"transposed_matrix1 = matrix1.T\ntransposed_matrix1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decomposing the transposed matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SVD = TruncatedSVD(n_components=8)\ndecomposed_matrix1 = SVD.fit_transform(transposed_matrix1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making correlation matrix\nHere correlation matrix is made out of the transposed matrix by using numpy library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_matrix1 = np.corrcoef(decomposed_matrix1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=transposed_matrix1.index[5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Index no of item ID purchased by user","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"iid = \"AWIm0C3TYSSHbkXwx3S6\"\n\nproduct_names1 = list(transposed_matrix1.index)\npid1 = product_names1.index(iid)\nprint(\"Index no of item id purchased by user: \\n\")\nprint(pid1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation for all items\nCorrelation for all items with the item purchased by this user based on items rated by other users who bought the same item","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_pid = correlation_matrix1[pid1]\ncorrelation_pid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recommending top 3 correlated products","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r=transposed_matrix1.index[correlation_pid > -0.20858196]\nrecommend1 = list(r)\n\nrecommend1.remove(iid) #removing itself from recommendations\n\nrecommend1[0:3]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}