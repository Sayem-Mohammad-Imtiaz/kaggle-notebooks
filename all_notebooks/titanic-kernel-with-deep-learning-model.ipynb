{"cells":[{"metadata":{},"cell_type":"markdown","source":"Before building a model, I will do some analysis on the training data(transforming, fulfilling missings, manipulating, preprocessing...)\n\n# Data processing\n\nThere are about $2/3$ for training and $1/3$ for testing. And there are 12 columns in train_csv and 11 columns in test_csv.\n\n### Dictionary\n\n**survival** 0 = No, 1 = Yes\n\n**pclass** 1 = Upper class, 2 = Middle class, 3 = Lower class\n\n**sibsp** siblings/spouses on the ship\n\n**parch** parents/children on the ship\n\n**embarked** C = Cherbourg, Q = Queenstown, S = Southampton"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import library\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras as kr\nfrom keras.optimizers import SGD\nimport graphviz\n\nfrom matplotlib import rcParams #deal with customizing plot parameters (fontsize, colorscheme, ...)\nimport re #use for string manipulation\n\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import dataset\ndf_train = pd.read_csv(\"../input/train.csv\") #891*12\ndf_test = pd.read_csv(\"../input/test.csv\") #418*11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take a look to the estate/position of the passengers\ndf_train['Title'] = df_train.Name.apply(lambda x: re.search('([A-Z][a-z]+)\\.', x).group(1))\n#Do the same for the df_test\ndf_test['Title'] = df_test.Name.apply(lambda x: re.search('([A-Z][a-z]+)\\.', x).group(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grouping some titles then visualizing\ntit_dict = {\"Capt\": \"Intelligentsia\",\n            \"Col\": \"Intelligentsia\",\n            \"Major\": \"Intelligentsia\",\n            \"Dr\": \"Intelligentsia\",\n            \"Rev\": \"Intelligentsia\",\n            \"Jonkheer\": \"Royalty\",\n            \"Don\": \"Royalty\",\n            \"Sir\": \"Royalty\",\n            \"the Countess\": \"Royalty\",\n            \"Dona\": \"Royalty\",\n            \"Lady\": \"Royalty\",\n            \"Mme\": \"Mrs\",\n            \"Mrs\": \"Mrs\",\n            \"Mlle\": \"Miss\",\n            \"Miss\": \"Miss\",\n            \"Mr\": \"Mr\",\n            \"Master\": \"Master\"}\n\ndf_train['Title'] = df_train.Title.map(tit_dict)\ndf_test['Title'] = df_test.Title.map(tit_dict)\n\n#Printing the chance to be survived by position\nprint(\"Title - Chances to be survived\")\nprint(df_train.groupby(\"Title\")[\"Survived\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Visualization\nplt.figure(figsize=(12,5))\nsns.countplot(x='Title', data=df_train, palette='Set2', hue='Survived')\nplt.xlabel('titles')\nplt.ylabel('count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Age Distribution and Density\n#Use the median to fulfill the NaN with age\n\ndied_age = df_train[(df_train[\"Age\"]>=0) & (df_train[\"Survived\"]==0)]\nsurv_age = df_train[(df_train[\"Age\"]>=0) & (df_train[\"Survived\"]==1)]\n\nplt.figure(figsize=(12,5))\n\nsns.distplot(died_age[\"Age\"], color='g')\nsns.distplot(surv_age[\"Age\"], color='r')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Distribution and Density by Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Group the median age w.r.t sex, pclass, title\nage_grp = df_train.groupby([\"Sex\", \"Pclass\", \"Title\"])[\"Age\"]\nprint(age_grp.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Input to the Age NaN\ndf_train.loc[df_train.Age.isnull(), 'Age'] = df_train.groupby(['Sex', 'Pclass', 'Title']).Age.transform('median') #REMIND: pd.loc: access the cell with row & col\n\nplt.figure(figsize=(12,5))\nsns.distplot(df_train[\"Age\"], color='r')\nplt.xlabel(\"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperate by survivor\nplt.figure(figsize=(12,5))\n\n#use facetgrid\ng = sns.FacetGrid(df_train, col='Survived')\n#mapping a dataset onto multiple axes arrayed in a grid of rows and columns that correspond to levels of variables in the dataset\ng = g.map(sns.distplot, \"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Age intervals\n\nintv = (0, 5, 12, 18, 25, 35, 60, 120)\ncat = ['babies', 'children', 'teen', 'student', 'adult', 'elder', 'senior']\ndf_train[\"age_cat\"] = pd.cut(df_train.Age, intv, labels=cat) #segment and sort data values into bins\n\n#Do the same on the test\nintv = (0, 5, 12, 18, 25, 35, 60, 120)\ncat = ['babies', 'children', 'teen', 'student', 'adult', 'elder', 'senior']\ndf_test[\"age_cat\"] = pd.cut(df_test.Age, intv, labels=cat) #segment and sort data values into bins","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Survived by age category\nplt.figure(figsize=(12,5))\n\nplt.subplot(211)\nsns.countplot(\"age_cat\", data=df_train, hue=\"Survived\", palette=\"Set2\")\nplt.ylabel(\"count\")\nplt.xlabel(\"age_cat\")\nplt.title(\"Age Distribution\")\n\nplt.subplot(212)\nsns.swarmplot(x=\"age_cat\", y=\"Fare\", data=df_train, hue=\"Survived\", palette=\"Set2\")\nplt.ylabel(\"Fare Distribution\")\nplt.xlabel(\"age_cat\")\nplt.title(\"Fare Distribution\")\n\nplt.subplots_adjust(hspace=0.5) #amount of height reserved for space between subplots, expressed as a fraction of the average axis height\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fare distribution to Survived or not\nplt.figure(figsize=(12,5))\nsns.distplot(df_train[df_train.Survived==0][\"Fare\"], color='r')\nsns.distplot(df_train[df_train.Survived==1][\"Fare\"], color='g')\nplt.title(\"Fare Distribution by Survived\")\nplt.xlabel(\"Fare\")\nplt.ylabel(\"Density\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Treat the fare expend\n\n#Fill NA with -0.5\ndf_train.Fare = df_train.Fare.fillna(-0.5)\n#interval to categorize\nquant = (-1, 0, 8, 15, 31, 600)\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\ndf_train[\"Fare_cat\"] = pd.cut(df_train.Fare, quant, labels=label_quants)\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Fare_cat\", hue=\"Survived\", data=df_train, palette='Set2')\nplt.title(\"Count of survived & Fare expend\")\nplt.xlabel(\"fare_cat\")\nplt.ylabel(\"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Do it with df_test\n\n#Fill NA with -0.5\ndf_test.Fare = df_test.Fare.fillna(-0.5)\n#interval to categorize\nquant = (-1, 0, 8, 15, 31, 600)\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\ndf_test[\"Fare_cat\"] = pd.cut(df_test.Fare, quant, labels=label_quants)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Work on name\n\n#Drop some irrelevant\ndel df_train[\"Fare\"]\ndel df_train[\"Age\"]\ndel df_train[\"Ticket\"]\ndel df_train[\"Cabin\"]\ndel df_train[\"Name\"]\n\n#In df_test\ndel df_test[\"Fare\"]\ndel df_test[\"Age\"]\ndel df_test[\"Ticket\"]\ndel df_test[\"Cabin\"]\ndel df_test[\"Name\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total survived or not\nprint(df_train.groupby(\"Survived\")[\"PassengerId\"].count())\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Survived\", data=df_train, palette=\"Set2\")\nplt.title(\"Total Distribution by survived or died\")\nplt.xlabel('Target Distribution')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.crosstab(df_train.Survived, df_train.Sex))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Sex\", data=df_train, hue=\"Survived\", palette='Set2')\nplt.title('Sex Distribution by survived or not')\nplt.xlabel('Sex Distribution')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pclass vs. Embarked\nprint(pd.crosstab(df_train.Pclass, df_train.Embarked))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Pclass\", palette='Set2')\nplt.title(\"Embarked & Pclass\")\nplt.xlabel(\"Embarked with Pclass\")\nplt.ylabel(\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')\n\nprint(pd.crosstab(df_train.Survived, df_train.Embarked))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Survived\", palette='Set2')\nplt.title(\"Class Distribution by survived or died\")\nplt.xlabel(\"Embarked\")\nplt.ylabel(\"Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.crosstab(df_train.Survived, df_train.Pclass))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Pclass\", data=df_train, hue=\"Survived\", palette=\"Set2\")\nplt.xlabel(\"Pclass\")\nplt.ylabel(\"Count\")\nplt.title(\"Class Distribution by Survived or died\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SibSp & Parch\ng = sns.catplot(x=\"SibSp\", y=\"Survived\", data=df_train, kind='bar', height=5, aspect=1.6, palette=\"Set2\")\ng.set_ylabels('Survived Probability')\ng.set_xlabels('SibSp Number')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Parch\", y=\"Survived\", data=df_train, kind=\"bar\", size=6, palette='Set2')\ng = g.set_ylabels(\"Survival Probability\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train[\"SibSp\"]\ndel df_train[\"Parch\"]\n\ndel df_test[\"SibSp\"]\ndel df_test[\"Parch\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns=['Sex', 'Embarked', 'age_cat', 'Fare_cat', 'Title'], prefix=['Sex', 'Emb', 'Age', 'Fare', 'Prefix'], drop_first=True)\ndf_test = pd.get_dummies(df_test, columns=['Sex', 'Embarked', 'age_cat', 'Fare_cat', 'Title'], prefix=['Sex', 'Emb', 'Age', 'Fare', 'Prefix'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set')\nsns.heatmap(df_train.astype(float).corr(), vmax=1.0, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_train.drop([\"Survived\", \"PassengerId\"], axis=1) #Drop specified labels from rows or columns\ntrain0 = df_train[\"Survived\"]\n\ntest = df_test.drop([\"PassengerId\"], axis=1)\n\nX_train = train.values\ny_train = train0.values\n\nX_test = test.values\nX_test = X_test.astype(np.float64, copy=False)\n\n#Feature Scaling\nscaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\nX_train = scaler.fit_transform(X_train) #Fit to data, then transform it\nX_test = scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential() \n#The Sequential model is a linear stack of layers\nmodel.add(Dense(18,\n                activation='relu',\n                input_dim=19,\n                kernel_initializer='uniform'))\n#input_layer_neurons=20, hidden_layer_neurons/output=18\n'''\nThere is no known way to determine a good network structure evaluating the number of inputs or outputs. \nIt relies on the number of training examples, batch size, number of epochs, basically, in every significant parameter of the network.\nMoreover, a high number of units can introduce problems like overfitting and exploding gradient problems. \nOn the other side, a lower number of units can cause a model to have high bias and low accuracy values. \nOnce again, it depends on the size of data used for training.\n\nThe term kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights. \nIn case of statistical distribution, the library will generate numbers from that statistical distribution and use as starting weights.\n'''\nmodel.add(Dropout(0.5))\n'''\nDropout is a technique where randomly selected neurons are ignored during training. \nThey are “dropped-out” randomly. \nThis means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n\nGenerally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. \nA probability too low has minimal effect and a value too high results in under-learning by the network.\n'''\nmodel.add(Dense(60,\n                kernel_initializer='uniform',\n                activation='relu'))\n#hidden_layer_neurons=60\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,\n                kernel_initializer='uniform',\n                activation='sigmoid'))\n#output_layers=1\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGD(lr=0.01, momentum=0.9)\nmodel.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, batch_size=60, epochs=30, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)\n\nsubmission = pd.read_csv(\"../input/gender_submission.csv\", index_col='PassengerId')\nsubmission['Survived'] = y_pred.astype(int)\nsubmission.to_csv('TitanicKNN.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}