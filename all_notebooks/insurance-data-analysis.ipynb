{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Insurance Data Analysis\n\nThis is my first data analysis work following the working through the steps in the book Hands-on machine learning.\nI am a novice, I need feedback please."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np  # Linear Algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # visualization\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_rows', 10)\n\nfile_path = '../input/insurance/insurance.csv'\n\ninsurance = pd.read_csv(file_path)\ninsurance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.hist(bins = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(insurance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting bmi vs charges to find correlation\n\nplt.figure( figsize = (5,5))\nplt.title('Plot of BMI vs Age')\nsns.scatterplot(x ='bmi', y ='charges', data  = insurance, hue ='sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph shows a strong relationship between '<b> bmi </b> and insurance price"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (5,5))\nplt.title('Relation between Sex and Insurance Price')\nsns.barplot( x ='sex', y = 'charges', data = insurance, hue = 'smoker')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot( x = 'children', y = 'charges', data = insurance)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(insurance['charges'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the dataset into Test_Set and Train_set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import thr train_test_split function from scikit learn\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(insurance, test_size = 0.2, random_state = 42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#distribution of age in the data set\nsns.distplot(insurance['age'], kde = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the above distribution of age, I am going to stratify the age "},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance['age_cat'] = pd.cut(insurance['age'],\n                              bins =[0,20,40,60, np.inf],\n                              labels = [1,2,3,4])\n\n                        \n                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(insurance['age_cat'], kde = False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I am ready to do stratified sampling based on the age category. For this task, I am going to use scikit-learn <b>StratifiedShuffleSplit class:</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits =1, test_size = 0.2,random_state =42)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for train_index, test_index in split.split(insurance, insurance[\"age_cat\"]):\n    strat_train_set = insurance.loc[train_index]\n    strat_test_set = insurance.loc[test_index]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s see if this worked as expected. You can start by looking at the income category\nproportions in the test set:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":" strat_test_set[\"age_cat\"].value_counts() / len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you should remove the income_cat attribute so the data is back to its original\nstate:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop('age_cat', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking for Correlation\nSince the dataset is not too large, you can easily compute the standard correlation\ncoefficient (also called Pearson’s r) between every pair of attributes using the corr()\nmethod"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = insurance.corr()\n\ncorr_matrix['charges'].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let’s look at how much each attribute correlates with the median house value:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use pandas scatter_matrix for more correlation\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = ['charges','age','bmi','children']\nscatter_matrix(insurance[attributes], figsize =(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x ='age', y = 'charges', data = insurance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Experimenting with Attribute Combinations\nOne last thing you may want to do before actually preparing the data for Machine\nLearning algorithms is to try out various attribute combinations. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance['age_per_bmi'] = insurance['age']/ insurance['bmi']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = insurance.corr()\ncorr_matrix[\"charges\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the data for machine learning\nBut first let’s revert to a clean training set (by copying strat_train_set once again),\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\nthe same transformations to the predictors and the target values (note that drop()\ncreates a copy of the data and does not affect strat_train_set):\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance = strat_train_set.drop('charges', axis = 1) # axis = 'columns'\ninsurance_labels = strat_train_set['charges'].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling Text and Categorical attributes\nEarlier we left out the categorical attribute ocean_proximity because it is a text\nattribute so we cannot compute its median:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_cat = insurance[['region']]\ninsurance_cat.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\nvert these categories from text to numbers. For this, we can use  <b>Scikit-Learn’s Ordina\nlEncoder class:</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder =OrdinalEncoder()\n\ninsurance_cat_encoded = ordinal_encoder.fit_transform(insurance_cat)\ninsurance_cat_encoded[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One issue with this representation is that ML algorithms will assume that two nearby\nvalues are more similar than two distant values. This may be fine in some cases (e.g.,\nfor ordered categories such as “bad”, “average”, “good”, “excellent”), but it is obviously\nnot the case"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\ninsurance_cat_1hot = cat_encoder.fit_transform(insurance_cat)\ninsurance_cat_1hot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_cat_1hot.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sex data transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_sex_cat = insurance[['sex']]\n\ninsurance_sex_cat .head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_encoder =OrdinalEncoder()\n\ninsurance_sex_encoded = sex_encoder.fit_transform(insurance_sex_cat)\ninsurance_sex_encoded[:10]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# smoker categorical data transformation\nsmoker_cat = insurance[['smoker']]\nsmoker_encoder = OrdinalEncoder()\nsmoker_encoded = smoker_encoder.fit_transform(smoker_cat)\nsmoker_encoded\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoker_encoder.categories_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# insurance_num contains all the numerical attributes o\ninsurance_num = insurance.drop([\"sex\",'region','smoker'], axis=1)\n\ninsurance_num.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Future Scaling\nOne of the most important transformations you need to apply to your data is feature\nscaling. With few exceptions, Machine Learning algorithms don’t perform well when\nthe input numerical attributes have very different scales.\nThere are two common ways to get all attributes to have the same scale: min-max\nscaling and standardization.\n"},{"metadata":{},"cell_type":"markdown","source":"### Transformation Pipelines\nAs you can see, there are many data transformation steps that need to be executed in\nthe right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\nsuch sequences of transformations. Here is a small pipeline for the numerical\nattributes:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n    ('std_scaler', StandardScaler()),\n])\n\ninsurance_num_tr = num_pipeline.fit_transform(insurance_num)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Pipeline constructor takes a list of name/estimator pairs defining a sequence of\nsteps. All but the last estimator must be transformers (i.e., they must have a\nfit_transform() method). The names can be anything you like (as long as they are\nunique and don’t contain double underscores “__”): they will come in handy later for\nhyperparameter tuning.\nWhen you call the pipeline’s fit() method, it calls fit_transform() sequentially on\nall transformers, passing the output of each call as the parameter to the next call, until\nit reaches the final estimator, for which it just calls the fit() method."},{"metadata":{},"cell_type":"markdown","source":"So far, we have handled the categorical columns and the numerical columns sepa‐\nrately. It would be more convenient to have a single transformer able to handle all col‐\numns, applying the appropriate transformations to each column. In version 0.20,\nScikit-Learn introduced the ColumnTransformer for this purpose, and the good news\nis that it works great with Pandas DataFrames. Let’s use it to apply all the transforma‐\ntions to the housing data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(insurance_num)\ncat_attribs = ['sex', 'smoker','region']\n\n\nfull_pipeline = ColumnTransformer([\n     (\"num\", num_pipeline, num_attribs),\n    ('cat', OneHotEncoder(), cat_attribs),\n                                  ])\ninsurance_prepared = full_pipeline.fit_transform(insurance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Select and train a model\nAt last! You framed the problem, you got the data and explored it, you sampled a\ntraining set and a test set, and you wrote transformation pipelines to clean up and\nprepare your data for Machine Learning algorithms automatically. You are now ready\nto select and train a Machine Learning model."},{"metadata":{},"cell_type":"markdown","source":"#### Training and evaluating on the training set\nLet’s first train a Linear Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(insurance_prepared, insurance_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done! You now have a working Linear Regression model. Let’s try it out on a few\ninstances from the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"some_data = insurance.iloc[:5]\nsome_labels = insurance_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))\nprint(\"Labels\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It works, although the predictions are not exactly accurate. Let’s measure this regression model’s RMSE on the whole train‐\ning set using Scikit-Learn’s mean_squared_error function:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ninsurance_predictions = lin_reg.predict(insurance_prepared)\nlin_mse = mean_squared_error(insurance_labels, insurance_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is an example of a model underfitting\nthe training data. When this happens it can mean that the features do not provide\nenough information to make good predictions, or that the model is not powerful\nenough. "},{"metadata":{},"cell_type":"markdown","source":"Let’s train a <b>DecisionTreeRegressor</b>. This is a powerful model, capable of finding\ncomplex nonlinear relationships in the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(insurance_prepared,insurance_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the model is trained, let’s evaluate it on the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"insurance_predictions = tree_reg.predict(insurance_prepared)\ntree_mse = mean_squared_error(insurance_labels, insurance_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wait, what!? The error is too small. Could this model really be absolutely perfect? Of course,\nit is much more likely that the model has badly overfit the data."},{"metadata":{},"cell_type":"markdown","source":"### Better Evaluation Using Cross-Validation\nOne way to evaluate the Decision Tree model would be to use the train_test_split\nfunction to split the training set into a smaller training set and a validation set, the train your models against the smaller training set and evaluate them against the vali‐\ndation set. It’s a bit of work, but nothing too difficult and it would work fairly well.\nA great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The follow‐\ning code randomly splits the training set into 10 distinct subsets called folds, then it\ntrains and evaluates the Decision Tree model 10 times, picking a different fold for\nevaluation every time and training on the other 9 folds. The result is an array con‐\ntaining the 10 evaluation scores:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, insurance_prepared, insurance_labels,\n                        scoring = 'neg_mean_squared_error', cv =10)\n\ntree_rmse_scores = np.sqrt(-scores)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's write a function to display the results \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard Deviation: \", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to per‐\nform worse than the Linear Regression model! Notice that cross-validation allows\nyou to get not only an estimate of the performance of your model, but also a measure\nof how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\nscore of approximately 6723.76, generally ±596.628."},{"metadata":{},"cell_type":"markdown","source":"Let’s compute the same scores for the Linear Regression model just to be sure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_scores = cross_val_score(lin_reg, insurance_prepared,\n                             insurance_labels,\n                             scoring = 'neg_mean_squared_error', cv =10)\n\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse-scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model above is the worse of the model, it performs so bad on the train_set"},{"metadata":{},"cell_type":"markdown","source":"Let’s try one last model now: the <b>RandomForestRegressor</b>\n Random Forests work by training many Decision Trees on random subsets of\nthe features, then averaging out their predictions. Building a model on top of many\nother models is called Ensemble Learning, and it is often a great way to push ML algo‐\nrithms even further. We will skip most of the code since it is essentially the same as\nfor the other models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(insurance_prepared, insurance_labels)\n\n\n\ninsurance_predictions = forest_reg.predict(insurance_prepared)\nforest_mse = mean_squared_error(insurance_labels, insurance_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_scores = cross_val_score(forest_reg, insurance_prepared,\n                               insurance_labels,\n                               scoring = \"neg_mean_squared_error\",\n                               cv = 10)\n\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fine-Tune Your Model\nLet’s assume that you now have a shortlist of promising models. You now need to\nfine-tune them. Let’s look at a few ways you can do that.\n"},{"metadata":{},"cell_type":"markdown","source":"#### Grid Search\nOne way to do that would be to fiddle with the hyperparameters manually, until you\nfind a great combination of hyperparameter values. This would be very tedious work,\nand you may not have time to explore many combinations.\nInstead you should get Scikit-Learn’s<b> GridSearchCV</b> to search for you. All you need to\ndo is tell it which hyperparameters you want it to experiment with, and what values to\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\nusing cross-validation. For example, the following code searches for the best combi‐\nnation of hyperparameter values for the RandomForestRegressor:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n ]\n\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n scoring='neg_mean_squared_error',\nreturn_train_score=True)\n\ngrid_search.fit(insurance_prepared, insurance_labels)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search.best_estimator_\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And of course the evaluation scores are also available:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Randomized Search\nThe grid search approach is fine when you are exploring relatively few combinations,\nlike in the previous example, but when the hyperparameter search space is large, it is\noften preferable to use RandomizedSearchCV instead. This class can be used in much\nthe same way as the GridSearchCV class, but instead of trying out all possible combi‐\nnations, it evaluates a given number of random combinations by selecting a random\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\nfits:\n1. If you let the randomized search run for, say, 1,000 iterations, this approach will\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\nues per hyperparameter with the grid search approach).\n2.  You have more control over the computing budget you want to allocate to hyper‐\nparameter search, simply by setting the number of iterations.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(insurance_prepared, insurance_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyze the Best Models and Their Errors\nYou will often gain good insights on the problem by inspecting the best models. For\nexample, the RandomForestRegressor can indicate the relative importance of each\nattribute for making accurate predictions:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs  + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Your System on the Test Set\nAfter tweaking your models for a while, you eventually have a system that performs\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\nnothing special about this process; just get the predictors and the labels from your\ntest set, run your full_pipeline to transform the data (call transform(), not\nfit_transform(), you do not want to fit the test set!), and evaluate the final model\non the test set:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"charges\", axis=1)\ny_test = strat_test_set[\"charges\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nfinal_rmse\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In some cases, such a point estimate of the generalization error will not be quite\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\nrently in production? You might want to have an idea of how precise this estimate is.\nFor this, you can compute a 95% confidence interval for the generalization error using\nscipy.stats.t.interval():\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n loc=squared_errors.mean(),\n  scale=stats.sem(squared_errors)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}