{"cells":[{"metadata":{"_uuid":"c753d634-ae5b-4bdc-9d0b-9a48a4e4bcbd","_cell_guid":"948e2a98-9ccd-409d-aef4-7c234946fd08","trusted":true},"cell_type":"markdown","source":"**Pytorch Reference -**\n\n\ntorch.nn.Linear(in_features, out_features, bias=True)\n\ntorch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n\ntorch.nn.LeakyReLU(negative_slope=0.01, inplace=False) \n\ntorch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n\ntorch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None)\n\ntorch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)"},{"metadata":{"_uuid":"6e1edfef-e43c-4f79-adf4-7ac700164e51","_cell_guid":"6f63f5cf-ed4d-4d73-9994-dade6c6f9156","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/img_align_celeba\"))\n\n# Any results you write to the current directory are saved as output","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25e2724f-7428-49a6-b680-77ab78f6db19","_cell_guid":"71946bca-36c7-4624-a85d-9cb53ea6aa50","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nimport torchvision.utils as vutils\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport copy\nimport time\nimport cv2 as cv\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.image as mpimg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfb97e05-ad60-402f-95a4-325422fb0ecf","_cell_guid":"24e5d091-c3bc-4a58-b5d3-d587ac3598e7","trusted":true},"cell_type":"code","source":"datapath = \"../input/img_align_celeba/img_align_celeba/\"\nimages_path = os.listdir(\"../input/img_align_celeba/img_align_celeba/\")\nprint(len(images_path))\nimages_path = images_path[:30000]\nvalid_ratio = 0.8","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aad9ed47-2525-4195-8919-d95ec86697b4","_cell_guid":"c8d44ed6-5a6c-46ac-a984-d3768c39369e","trusted":true},"cell_type":"code","source":"class ImageData(Dataset):\n    def __init__(self,is_train=True):\n        self.is_train = is_train\n        self.transform = transforms.Compose([transforms.ToTensor(),])\n        self.train_index = int(valid_ratio * len(images_path))\n    def __len__(self):\n        if self.is_train:\n            return self.train_index\n        else:\n            return len(images_path) - self.train_index -1\n    def __getitem__(self, index):\n        if not self.is_train:\n            index = self.train_index + index\n        img = mpimg.imread(datapath+str(images_path[index]))\n        if self.transform is not None:\n            img = self.transform(img)\n        img = (img - 0.5) / 0.5\n        return img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b7fc061-b1bf-4194-b48f-bb4f4b414f2b","_cell_guid":"f7988f3b-7c8e-4167-8965-a529b7d09c5c","trusted":true},"cell_type":"code","source":"batch_size=20\ndataset = ImageData()\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a971789-d6dd-42bf-93dd-1c04da8019ab","_cell_guid":"0633462c-26c2-444d-8d9c-2e00f1bdbf27","trusted":true},"cell_type":"code","source":"a = next(iter(dataloader))\nprint(a[0].shape)\nimg = a[15]\nimg = img *0.5 + 0.5\nplt.imshow(img.permute(1,2,0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fa1b769-1563-4c63-8382-3d1fa155b792","_cell_guid":"573567ea-2c7f-42c4-974d-03bd6d1d7221","trusted":true},"cell_type":"code","source":"# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51923b25-aa9e-4f6b-80bd-ed6210e6c13e","_cell_guid":"1be83c18-e274-4863-98a0-33967b466063","trusted":true},"cell_type":"code","source":"IMG_WIDTH = 178\nIMG_HEIGHT = 218\nencode_size = [30,30]\nlatent_size = 200","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab029e2-e775-44dd-bba1-774db4fd05bd","_cell_guid":"948d19d6-ae0c-401e-ba68-667ed6063410","trusted":true},"cell_type":"code","source":"# Encoder Model\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5,stride = 2,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3,stride = 2,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3,stride = 1,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Tanh(),\n        )\n#         self.fc = nn.Sequential(\n#             nn.Linear(2688,encode_size),\n#             nn.Sigmoid(),\n#         )\n        \n    def forward(self, x):\n#         print(x.shape)\n        x = self.layer1(x)\n#         print(x.shape)\n        x = self.layer2(x)\n#         print(x.shape)\n        x = self.layer3(x)\n#         print(x.shape)\n        x = self.layer4(x)\n#         print(x.shape)\n#         x = x.view(x.shape[0],-1)\n#         print(x.shape)\n#         x = self.fc(x)\n#         print(x.shape)\n        x = self.layer5(x)\n#         print(x.shape)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60ccbf64-7bee-4927-8fef-cb30aad497c8","_cell_guid":"abb02f33-891d-4fe1-add4-42f1a557091a","trusted":true},"cell_type":"code","source":"device","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b2dfdbf-1f5f-43a8-881d-b2a2df34e8ba","_cell_guid":"826eb0f5-91bd-4f56-82b9-da1a98e1dbff","trusted":true},"cell_type":"code","source":"netE = Encoder().to(device)\nnetE.apply(weights_init)\ninp = torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 100)\ninp = inp.view((-1,3,IMG_HEIGHT,IMG_WIDTH))\noutput = netE(inp.to(device))\nprint(output.shape)\nprint((output.shape[0]*output.shape[1]*output.shape[2])/(IMG_WIDTH*IMG_HEIGHT*3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5713b81b-56f8-4443-bb0c-dc4883c22d78","_cell_guid":"98c9585b-7482-4492-9a7e-622b684ff261","trusted":true},"cell_type":"code","source":"# Generator / Decoder Model\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        \n        self.latent_fc1 = nn.Sequential(\n            nn.Linear(latent_size,1000),\n            nn.Sigmoid(),\n        )\n        self.latent_fc2 = nn.Sequential(\n            nn.Linear(1000,54*44),\n            nn.Sigmoid(),\n        )\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(9, 16, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(16, 32, (3,3), stride=1, padding=2, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(32, 128, (3,3), stride=2, padding=2, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, (5,5), stride=2, padding=0, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.layer5 = nn.Sequential(\n            nn.ConvTranspose2d(64, 3, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Sigmoid(),\n        )\n#         self.bn1 = nn.BatchNorm1d(hidden_feature_sizes[0]*filter_hidden_length*filter_hidden_width)\n\n        \n        \n    def forward(self, x):\n        y = x['noise'].to(device)\n        y = self.latent_fc1(y)\n        y = self.latent_fc2(y)\n        y = y.view(-1,1,54,44)\n#         print(y.shape)\n        x = x['encoded'].to(device)\n        x = torch.cat((x,y),1)\n#         print(x.shape)\n#         print(x.shape)\n        x = self.layer1(x)\n#         print(x.shape)\n        x = self.layer2(x)\n#         print(x.shape)\n        x = self.layer3(x)\n#         print(x.shape)\n        x = self.layer4(x)\n#         print(x.shape)\n        x = self.layer5(x)\n#         print(x.shape)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6276c1c-a06e-4219-bc85-1b9d25d69a9a","_cell_guid":"9a212328-587c-4b5f-ae96-b4c2f8f36687","trusted":true},"cell_type":"code","source":"netG = Generator().to(device)\nnetG.apply(weights_init)\ninp = {}\ninp['encoded'] = torch.randn(100*8*54*44).view((-1,8,54,44))\ninp['noise'] = torch.randn(100*latent_size).view((-1,latent_size))\noutput = netG(inp)\nprint(output.shape)\n#218 * 178","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"057ebdce-ff77-4b5a-90c2-724983d39bb0","_cell_guid":"2675e3d6-c32c-4637-bfc3-df350b8f4f9d","trusted":true},"cell_type":"markdown","source":"torch.Size([100, 3, 218, 178])\n\ntorch.Size([100, 64, 216, 176])\n\ntorch.Size([100, 128, 106, 86])\n\ntorch.Size([100, 32, 54, 44])\n\ntorch.Size([100, 16, 56, 46])\n\ntorch.Size([100, 8, 54, 44])\n\ntorch.Size([100, 8, 54, 44])"},{"metadata":{"_uuid":"a17c24c8-9437-4a20-80d9-26cf45d6084b","_cell_guid":"fb0e162f-0eaf-4c10-8089-53ee6d1b92c3","trusted":true},"cell_type":"code","source":"# Discriminator Model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.latent_layer1 = nn.Sequential(\n            nn.ConvTranspose2d(8, 12, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer2 = nn.Sequential(\n            nn.ConvTranspose2d(12, 16, (3,3), stride=1, padding=2, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer3 = nn.Sequential(\n            nn.ConvTranspose2d(16, 24, (3,3), stride=2, padding=2, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer4 = nn.Sequential(\n            nn.ConvTranspose2d(24, 36, (5,5), stride=2, padding=0, output_padding=1, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n        )\n        self.latent_layer5 = nn.Sequential(\n            nn.ConvTranspose2d(36, 3, (3,3), stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Tanh(),\n        )\n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=6, out_channels=64, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5,stride = 2,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3,stride = 2,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3,stride = 1,padding=2),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n        )\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3,stride = 1,padding=0),\n            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Tanh(),\n        )\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        self.fc1 = nn.Sequential(\n            nn.Linear(8*54*44,2000),\n            nn.Sigmoid(),\n        )\n        \n        self.fc2 = nn.Sequential(\n            nn.Linear(2000,100),\n            nn.Sigmoid(),\n        )\n        self.fc3 = nn.Sequential(\n            nn.Linear(100,1),\n            nn.Sigmoid(),\n        )\n        \n        \n        \n        \n        \n    def forward(self, x):\n        y = x['encoded'].to(device)\n        y = self.latent_layer1(y)\n        y = self.latent_layer2(y)\n        y = self.latent_layer3(y)\n        y = self.latent_layer4(y)\n        y = self.latent_layer5(y)\n#         print(y.shape)\n        x = x['img'].to(device)\n#         print(x.shape)\n        x = torch.cat((x,y),1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n#         print(x.shape)\n        x= x.reshape((x.shape[0],-1))\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bdbddcf-7ac3-494c-b6e3-7e62073340e8","_cell_guid":"fbbe0124-8740-421a-8b8e-6e4107118f79","trusted":true},"cell_type":"code","source":"netD = Discriminator().to(device)\nnetD.apply(weights_init)\ninp_x = {}\ninp_x['img']=torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 100).view((-1,3,IMG_HEIGHT,IMG_WIDTH))\ninp_x['encoded'] = torch.randn(100*8*54*44).view((-1,8,54,44))\noutput = netD(inp_x)\noutput.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"057c5e08-9933-4670-8921-6b7e1864705d","_cell_guid":"6c66b5e6-df39-4c91-9a85-6389a2c224a2","trusted":true},"cell_type":"code","source":"lr = 0.0002\n# Initialize BCELoss function\ncriterion = nn.BCELoss()\nmsecriterion = nn.MSELoss()\n\n# Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(64, 200, device=device)\n\n# Establish convention for real and fake labels during training\nreal_label = 1\nfake_label = 0\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizerE = optim.Adam(netE.parameters(), lr=lr, betas=(0.5, 0.999))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84c46811-4ed7-48d7-a5b9-e7346bc56890","_cell_guid":"c5cb4d04-0cff-4133-82b6-bfa46ed2f067","trusted":true},"cell_type":"code","source":"# Training Loop\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\nE_losses = []\niters = 0\nnum_epochs = 4\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, (images) in enumerate(dataloader, 0):\n        netG.train()\n        netD.train()\n        netE.train()\n        netD.zero_grad()\n        \n        images = images.to(device)\n        \n        x_inp_G = {}\n        x_inp_G['encoded'] = netE(images)\n        x_inp_G['noise'] = torch.randn(images.size(0)*latent_size).view((-1,latent_size))\n        fake_images = netG(x_inp_G)\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        \n        ## Create a fake pair batch --\n\n        inp_x = {}\n        inp_x['img']=images\n        inp_x['encoded'] = netE(images)\n        \n#         label = torch.full((images.size(0),), real_label, device=device)\n        label = torch.FloatTensor(np.random.uniform(low=0.855, high=0.999, size=(images.size(0)))).to(device)\n        output = netD(inp_x).view(-1)\n        errD_real = criterion(output, label)\n        errD_real.backward(retain_graph=True)\n        D_x = output.mean().item()\n        \n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        label = torch.FloatTensor(np.random.uniform(low=0.005, high=0.155, size=(images.size(0)))).to(device)\n#         label.fill_(fake_label)\n        output = netD(inp_x_fake).view(-1)\n        errD_fake = criterion(output, label)\n        errD_fake.backward(retain_graph=True)\n        D_G_z1 = output.mean().item()\n        \n        errD = errD_real + errD_fake\n        \n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        \n        label = torch.FloatTensor(np.random.uniform(low=0.895, high=0.999, size=(images.size(0)))).to(device)\n#         label.fill_(real_label)\n        output = netD(inp_x_fake).view(-1)\n        \n        errG = criterion(output, label) + msecriterion(images,fake_images)\n        errG.backward(retain_graph=True)\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        \n        netG.zero_grad()\n        inp_x_fake = {}\n        inp_x_fake['img']=fake_images\n        inp_x_fake['encoded'] = netE(images)\n        \n        label = torch.FloatTensor(np.random.uniform(low=0.895, high=0.999, size=(images.size(0)))).to(device)\n        output = netD(inp_x_fake).view(-1)\n        \n        errE = criterion(output, label) + msecriterion(images,fake_images)\n        errE.backward(retain_graph=True)\n        E_G_z2 = output.mean().item()\n        optimizerE.step()\n        \n        #################################_______STATS________###########################################\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        E_losses.append(errE.item())\n        \n        # Check how the generator is doing by saving G's output on fixed_noise\n#         if (iters % 50 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n#             netG.eval()\n#             with torch.no_grad():\n#                 fake = netG(fixed_noise).detach().cpu()\n#                 fake[:] = fake[:]*0.5 + 0.5\n#             img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n        del images\n        del inp_x_fake\n        del inp_x\n        del label\n        del output\n        del x_inp_G\n        torch.cuda.empty_cache()\n        iters += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3859dc9c-6edd-42e3-a0ed-d4691ac143bc","_cell_guid":"f0234d60-5cc3-44e6-a103-6d00a2eab928","trusted":true},"cell_type":"code","source":"# images_path = os.listdir(\"../input/img_align_celeba/img_align_celeba/\")\n# img = mpimg.imread(datapath+str(images_path[30001]))\n# transform=transform=transforms.Compose([transforms.ToTensor(),])\n# img = transform(img)\n# img = (img - 0.5) / 0.5\n\n# img = img.view(-1,img.shape[0],img.shape[1],img.shape[2]).to(device)\n# print(img.shape)\n# netE.eval()\n# netG.eval()\n\n# encoded_img = netE(img)\n# decoded_img = netG(encoded_img)\n# plt.imshow(img[0].cpu().detach().permute(1,2,0))\n\n# decoded_img = decoded_img *0.5 + 0.5\n# print(decoded_img.shape)\n# decoded_img = decoded_img[0].cpu().detach()\n# print(decoded_img.shape)\n# plt.imshow(decoded_img.permute(1,2,0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10110800-7667-4847-94e3-95fd1202dab9","_cell_guid":"147a0021-6141-4502-b0ad-856e9c293ab0","trusted":true},"cell_type":"code","source":"print(\"Evaluating the model ...\")\nnetE.eval()\nnetG.eval()\ntot_img_size = IMG_WIDTH * IMG_HEIGHT * 3\n# print(\"Size reduction is : \"+ str(float(encode_size/tot_img_size)*100.0)+\" percent\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c1a5491-0d62-40d3-904c-97ad4ce4bc8f","_cell_guid":"e2b7b6dd-a0c3-4d2a-92d3-9cf575136609","trusted":true},"cell_type":"code","source":"valid_dataset = ImageData(is_train=False)\nbatch_size=20\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\nvalid_batch = next(iter(valid_dataloader)).to(device)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"100eb825-1a35-42fd-ad8b-607e164eb3e2","_cell_guid":"12ab241b-a6e2-4486-9015-582afbf253ec","trusted":true},"cell_type":"code","source":"print(valid_batch.shape)\nencoded_img = netE(valid_batch)\nprint(encoded_img.shape)\n\n\nx_inp_G = {}\nx_inp_G['encoded'] = encoded_img\nx_inp_G['noise'] = torch.randn(valid_batch.shape[0]*latent_size).view((-1,latent_size))\n\n\n\n\n\nreconstructed_img = netG(x_inp_G)\nprint(reconstructed_img.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c8b3ef7-8881-49dc-8c00-33622a7d162c","_cell_guid":"ad7303ae-aed6-44ea-b70c-ac182a4d610b","trusted":true},"cell_type":"code","source":"num_images_to_show = 5\nf, axarr = plt.subplots(num_images_to_show,2)\nfor i in range(num_images_to_show):\n    axarr[i,0].imshow(valid_batch[i].cpu().detach().permute(1, 2, 0))\n    axarr[i,1].imshow(reconstructed_img[i].cpu().detach().permute(1, 2, 0))\n    f.set_figheight(20)\n    f.set_figwidth(20)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}