{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Task for Today  \n\n***\n\n## Children's Book Age Group Prediction  \n\nGiven *the names and descriptions of highly-rated children's books*, let's try to predict the **age group** for a given book.  \n  \nWe will use a TensorFlow neural network with an RNN to make our predictions."},{"metadata":{},"cell_type":"markdown","source":"# Getting Started"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nimport re\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/highly-rated-children-books-and-stories/children_stories.Csv', encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Labels"},{"metadata":{},"cell_type":"markdown","source":"We would like to create labels from the \"cats\" column. Let us divide the values in the column into two categories: *younger* and *older*.  \n  \nWe can start by sorting the indices of the column's value_counts() to see where we should make the split."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cats'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like **5 and up** is a good way to classify *older* books.  \n  \nLet's just sort the unique values as a list so we can see any leading or trailing whitespace in the entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(list(data['cats'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there is one blank age (\"Age \"), so let us remove any rows with this value."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(data.query(\"cats == 'Age '\").index, axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a list of all the values that count as *younger*."},{"metadata":{"trusted":true},"cell_type":"code","source":"young_ages = [\n    'Age 6months+',\n    'Age  0-3',\n    'Age 0+',\n    'Age 0-2',\n    'Age 0-3',\n    'Age 0-4',\n    'Age 0-5',\n    'Age 0-6',\n    'Age 1+',\n    'Age 1-2',\n    'Age 1-3',\n    'Age 1-4',\n    'Age 1-5',\n    'Age 1-6',\n    'Age 2+',\n    'Age 2-4',\n    'Age 2-5',\n    'Age 2-6',\n    'Age 2-7',\n    'Age 2-9',\n    'Age 3+',\n    'Age 3-4',\n    'Age 3-5',\n    'Age 3-6',\n    'Age 3-7',\n    'Age 4+',\n    'Age 4-11',\n    'Age 4-5',\n    'Age 4-6',\n    'Age 4-7',\n    'Age 4-8'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We apply a lambda function to change the value for a given book in the \"cats\" column to 0 if the book is for *younger* children and 1 if the book is for *older* children."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cats'] = data['cats'].apply(lambda age: 0 if age in young_ages else 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now check the value_counts() divided by the total number of examples to see if our split is good.  \n  \n51/49 is a decent split."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cats'].value_counts() / len(data['cats'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing Text  \n  \nNow that we have the labels properly assigned, let's prepare to create a dense encoding of each word in the name and the description."},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can start by defining a function to remove any digits and stop words from the texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_text(text):\n    \n    # Remove digits\n    text = re.sub(r'\\d+', ' ', text)\n    \n    # Split on whitespace\n    text = text.split()\n    \n    # Join on whitespace, but only the words that are not stop words\n    text = ' '.join([word for word in text if word not in stopwords.words('english')])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create three variables: **names** and **descriptions**, which will be the processed text columns, and **labels** which will just be a copy of the \"cats\" column."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = data['names'].copy().apply(process_text)\ndescriptions = data['desc'].copy().apply(process_text)\n\nlabels = data['cats'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"descriptions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us tokenize the texts to give us a word-to-integer mapping for all the words in all the texts.  \n  \nKeras' Tokenizer will automatically apply filtering for punctuation/special characters and split the strings (into words) on whitespace.  \n*Note:* We are fitting the tokenizer on the concatenation of the **names** and **descriptions**, so that all words are accounted for."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(pd.concat([names, descriptions]))\n\nnames = tokenizer.texts_to_sequences(names)\ndescriptions = tokenizer.texts_to_sequences(descriptions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our texts look like sequences of integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"names[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get the size of the vocabulary (the number of all unique words across all texts).  \n  \nWe can get this from the length of the tokenizer's word_index, and add +1 for the 0 character (which is not assigned to any word, but instead used for padding)."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_length = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary length:\", vocab_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should also get the lengths of the longest sequences in **names** and **descriptions**."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_name_length = np.max(list(map(lambda name: len(name), names)))\nmax_desc_length = np.max(list(map(lambda desc: len(desc), descriptions)))\n\nprint(\"Max name length:\", max_name_length)\nprint(\"Max description length:\", max_desc_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now pad the sequences according to their longest sequence (any sequences shorter than the max will have zeros added to the end)."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = pad_sequences(names, maxlen=max_name_length, padding='post')\ndescriptions = pad_sequences(descriptions, maxlen=max_desc_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now **names** and **descriptions** are proper NumPy arrays that have sequences of uniform length."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape:\", names.shape)\nnames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape:\", descriptions.shape)\ndescriptions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the Data (Train/Test)  \n  \nWe can split into train and test sets using sklearn's train_test_split() function.  \n  \nLet's use a train size of 70% and include a random state of 100."},{"metadata":{"trusted":true},"cell_type":"code","source":"names_train, names_test, descriptions_train, descriptions_test, labels_train, labels_test = train_test_split(names, descriptions, labels, train_size=0.7, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling  \n  \nWe are going to feed our feature data in through two inputs (one for the names and one for the descriptions)."},{"metadata":{},"cell_type":"markdown","source":"First, let's focus on the names.\n\nWe can embed the names in a high-dimensional vector space using a Keras Embedding layer.  \n  \nThis can allow us to learn representations for words, rather than manually creating the representations.  \nIt also allows us to have smaller inputs, as we are using a dense encoding.  \n  \nWe will then flatten the output from the embedding and prepare to send it to the final output."},{"metadata":{"trusted":true},"cell_type":"code","source":"name_dim = 64\n\nname_input = tf.keras.Input(shape=(max_name_length,), name=\"name_input\")\n\nname_embedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=name_dim,\n    input_length=max_name_length,\n    name=\"name_embedding\"\n)(name_input)\n\nname_flatten = tf.keras.layers.Flatten(name=\"name_flatten\")(name_embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's focus on the descriptions.  \n  \nFor the descriptions, we will also perform an embedding, but we will then feed it through a Gated Recurrent Unit (GRU) in order to capture time-dependent information in the data.  \n  \nWe will set return_sequences=True in the GRU and flatten the output."},{"metadata":{"trusted":true},"cell_type":"code","source":"desc_dim = 64\n\ndesc_input = tf.keras.Input(shape=(max_desc_length,), name=\"desc_input\")\n\ndesc_embedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=desc_dim,\n    input_length=max_desc_length,\n    name=\"desc_embedding\"\n)(desc_input)\n\ngru_layer = tf.keras.layers.GRU(\n    units=256,\n    return_sequences=True,\n    name=\"gru_layer\"\n)(desc_embedding)\n\ndesc_flatten = tf.keras.layers.Flatten(name=\"desc_flatten\")(gru_layer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will finalize our model by concatenating the outputs from the two sub-models and creating a final prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"concat = tf.keras.layers.concatenate([name_flatten, desc_flatten], name=\"concatenate\")\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(concat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at what the model looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Model(inputs=[name_input, desc_input], outputs=output)\n\nprint(model.summary())\ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training  \n  \nNow we just have to compile and fit our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nepochs = 14\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\nhistory = model.fit(\n    [names_train, descriptions_train],\n    labels_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau()\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(\n    history.history,\n    y=['loss', 'val_loss'],\n    labels={'x': \"epoch\", 'y': \"loss\"},\n    title=\"Loss Over Time\"\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.line(\n    history.history,\n    y=['accuracy', 'val_accuracy'],\n    labels={'x': \"epoch\", 'y': \"accuracy\"},\n    title=\"Accuracy Over Time\"\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate([names_test, descriptions_test], labels_test)\n\nprint(\"Accuracy:\", results[1])\nprint(\" ROC AUC:\", results[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps://youtu.be/hhsz2FKGsuQ"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}