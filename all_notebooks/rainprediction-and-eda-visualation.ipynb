{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict next day rain in Australia..."},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">0. Introduction</p>\n"},{"metadata":{},"cell_type":"markdown","source":"This notebook purpose is clearly understand to whetherAUS(predicition to whether rainy for  AUS ) dataset. In here mostly used visualation for this. For prediction used deep learning. In conclusion our ACC: %85.88 found.\n\nDescription About Columns:\n\n* Date: The date of observation\n* Location: The common name of the location of the weather station\n* MinTemp: The minimum temperature in degrees celsius\n* MaxTemp: The maximum temperature in degrees celsius\n* Rainfall: The amount of rainfall recorded for the day in mm\n* Evaporation: The so-called Class A pan evaporation (mm) in the 24 hours to 9am\n* Sunshine: The number of hours of bright sunshine in the day\n* WindGustDir: The direction of the strongest wind gust in the 24 hours to midnight\n* WindGustSpeed: The speed (km/h) of the strongest wind gust in the 24 hours to midnight\n* WindDir9am: Direction of the wind at 9am\n* WindDir3pm: Direction of the wind at 3pm\n* WindSpeed9am: Wind speed (km/hr) averaged over 10 minutes prior to 9am\n* WindSpeed3pm: Wind speed (km/hr) averaged over 10 minutes prior to 3pm\n* Humadity9am: Humidity (percent) at 9am\n* Humadity3pm: Humidity (percent) at 3pm\n* Pressure9am: Atmospheric pressure (hpa) reduced to mean sea level at 9am\n* Pressure3pm: Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n* Cloud9am: Fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast\n* Cloud3pm: Fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm\n* Temp9am: Temperature (degrees C) at 9am\n* Temp3pm: Temperature (degrees C) at 3pm\n* RainToday: Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n* RainTomorrow: The amount of next day rain in mm. Used to create response variable RainTomorrow. A kind of measure of the \"risk\"."},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Imports</p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sbn\nimport plotly.express as px\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dropout,BatchNormalization,Dense\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Read dataset and show first 5 rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Information About Dataset</p>"},{"metadata":{},"cell_type":"markdown","source":"This step is we will learn to some information about dataset.\n* Info is show us non-null(not empty) and dtype(object,float64 etc.)\n* Describe is show us the data description(count,min,max,std,mean etc.)\n* Isnull is show us the null values in data.\n* Unique is show us the each column have how many different values.\n* Corr is show us the each column corelation with target(\"RainTomorrow\") column and we showed on the graph(Only numeric data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shows the numbers of unique values\nfor col in data.columns:\n    n_uniq = data[col].nunique()\n    print(f\"Column Name:{col}, Unique Count: {n_uniq}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the values of  RainToday column\ndata[\"RainToday\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation amongst numeric attributes\ncorrmat = data.corr()\nplt.subplots(figsize=(16,16))\nsbn.heatmap(corrmat,annot=True, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Visualation The Dataset</p>"},{"metadata":{},"cell_type":"markdown","source":"This section we will try to show on graph some of situation. We will create lots of scenario and try to show on the graph so we can understand to dataset clearly."},{"metadata":{},"cell_type":"markdown","source":"* Firstly we will look RainToday column values count on graph. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.countplot(data = data,x = \"RainToday\",palette=\"Dark2\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step, we will look relation MinTemp and Location column. MinTemp column is minimum temperature. So our purpose  has find to minimum temperature cities and we can show on the graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[[\"Location\",\"MinTemp\"]].groupby(\"Location\").mean().sort_values(by=\"MinTemp\").iloc[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The next step is show us the above scenario on the graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(data[[\"Location\",\"MinTemp\"]].groupby(\"Location\").mean().sort_values(by=\"MinTemp\").iloc[:20],x=\"MinTemp\",\n            labels={\"MinTemp\":\"Minimum Temperature °C\",\"Location\":\"Location Names\"})\nfig.update_layout(\n    title={\n        \"text\":\"Top 20 Location has minimum temperature\",\n        \"x\":0.5,\n        \"y\":0.95,\n        \"xanchor\":\"center\",\n        \"yanchor\":\"top\"\n})\nfig.show()\n#Top 20 location has minimum temperature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Firstly, we will look relation MaxTemp and Location column. MaxTemp column is maximum temperature. So our purpose has find to maximum temperature cities and we can show on the graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[[\"Location\",\"MaxTemp\"]].groupby(\"Location\").mean().sort_values(ascending=False,by=\"MaxTemp\").iloc[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The next step is show us the above scenario on the graph."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(data[[\"Location\",\"MaxTemp\"]].groupby(\"Location\").mean().sort_values(ascending=False,by=\"MaxTemp\").iloc[:20],x = \"MaxTemp\",\n            labels = {\"MaxTemp\":\"Maximum Temperature °C\",\"Location\":\"Location Names\"})\nfig.update_layout(\ntitle={\n    \"x\":0.5,\n    \"y\":0.95,\n    \"xanchor\":\"center\",\n    \"yanchor\":\"top\",\n    \"text\": \"Top 20 Location has maximium temperature\"\n})\nfig.show()\n# Top 20 location temp high ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step, we will look Max Temp relation by Location on the year.Thus we will learn which year has  most high temperature  value and most low temperature value.\n* We just will show 12 location other wise they pick  a lot of take up space. For this we take 12 max temperature location.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show of 12 cities with the max temp column by years\ndata[\"year\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[0])\ndata[\"months\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[1])\ndata[\"day\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[2])\n\ndata_location = [\"Katherine\",\"Darwin\",\"Uluru\",\"Cairns\",\"Townsville\",\"AliceSprings\",\"Moree\",\"Brisbane\",\"PearceRAAF\",\"Cobar\",\"GoldCoast\",\"PerthAirport\"]\n\ngrp = data.groupby(\"Location\")\n\nnum_rows, num_cols = 6,2\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12, 12))\n#f.suptitle('Distribution of Features', fontsize=16)\n\nfor index, city in enumerate(data_location):\n    i,j = (index // num_cols, index % num_cols)\n    city_group = grp.get_group(city)\n    df=city_group[[\"MaxTemp\",\"year\"]].groupby(\"year\").mean()\n    sbn.lineplot(x=\"year\",y=\"MaxTemp\", data=df, ax=axes[i,j],marker=\"o\").set_title(city)\n\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step, we will look Min Temp relation by Location on the year.Thus we will learn which year has  most low temperature  value and most low temperature value.\n* We just will show 12 location other wise they pick  a lot of take up space. For this we take 12 min temperature location.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"year\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[0])\ndata[\"months\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[1])\ndata[\"day\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[2])\n\ndata_location = [\"MountGinini\",\"Canberra\",\"Tuggeranong\",\"Ballarat\",\"Launceston\",\"Sale\",\"Bendigo\",\"Dartmoor\",\"MountGambier\",\"Nhil\",\"Hobart\",\"SalmonGums\"]\n\ngrp = data.groupby(\"Location\")\n\nnum_rows,num_cols = 6,2\nf ,axes = plt.subplots(nrows = num_rows,ncols=num_cols,figsize=(12,12))\nfor index,city in enumerate(data_location):\n    i,j = (index // num_cols , index % num_cols)\n    city_group = grp.get_group(city)\n    df = city_group[[\"MinTemp\",\"year\"]].groupby(\"year\").mean()\n    sbn.lineplot(data = df,x = \"year\",y=\"MinTemp\",ax = axes[i,j],marker=\"o\").set_title(city)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step, we will look which Location most rainfall take."},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean rainfall all data \ndata[\"Rainfall\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 most rainfall on location \ndata[[\"Location\",\"Rainfall\"]].groupby(\"Location\").mean().sort_values(by=\"Rainfall\",ascending=False).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(data[[\"Location\",\"Rainfall\"]].groupby(\"Location\").mean().sort_values(by=\"Rainfall\",ascending=False).iloc[:10],x=\"Rainfall\",labels={\"Location\":\"Location Names\",\"Rainfall\":\"Rainfall Amount in day(mm)\"})\nfig.update_layout(\ntitle={\n    \"x\":0.5,\n    \"y\":0.95,\n    \"xanchor\":\"center\",\n    \"yanchor\":\"top\",\n    \"text\": \"Top 10 Location with maximium rainfall\"\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step, we will look at the amount of rainfall in Location by years. In here we just selected twelve cities but these Locations are the ones with the highest rainfall."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"year\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[0])\ndata[\"months\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[1])\ndata[\"day\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[2])\n\ngrp = data.groupby(\"Location\")\ndata_location = [\"Newcastle\",\"Katherine\",\"MountGinini\",\"Sydney\",\"NorahHead\",\"Townsville\",\"Williamtown\",\"Wollongong\",\"GoldCoast\",\"CoffsHarbour\",\"Darwin\",\"Cairns\"]\n\nnum_cols,num_rows = 2,6\nf,axes = plt.subplots(nrows=num_rows,ncols=num_cols,figsize=(12,12))\n\nfor index,city in enumerate(data_location):\n    i,j = (index//num_cols,index % num_cols)\n    city_group = grp.get_group(city)\n    df = city_group[[\"Rainfall\",\"year\"]].groupby(\"year\").mean()\n    sbn.lineplot(data=df,ax = axes[i,j],marker=\"o\").set_title(city)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step the amount of sunlight received in a day by Location (hours)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(data.groupby(\"Location\").mean().sort_values(ascending = False,by =\"Sunshine\").iloc[:10],x=\"Sunshine\",labels={\"Sunshine\":\"Sunshine (hours)\",\"Location\":\"Location Names\"})\nfig.update_layout(\ntitle={\n    \"x\":0.5,\n    \"y\":0.95,\n    \"xanchor\":\"center\",\n    \"yanchor\":\"top\",\n    \"text\": \"Top 10 Location with maximium Sunlight in a day\"\n})\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step the amount of sunlight received in a per months by Location (hours) "},{"metadata":{"trusted":true},"cell_type":"code","source":"data[[\"Location\",\"Sunshine\"]].groupby(\"Location\").mean().sort_values(ascending = False,by =\"Sunshine\").iloc[:12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"year\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[0])\ndata[\"months\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[1])\ndata[\"day\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[2])\ngrp = data.groupby(\"Location\")\ndata_location = [\"AliceSprings\",\"Woomera\",\"Moree\",\"PerthAirport\",\"PearceRAAF\",\"Perth\",\"Cobar\",\"Darwin\",\"Mildura\",\"Townsville\",\"WaggaWagga\",\"Brisbane\"]\n\nnum_cols ,num_rows = 2,6\n\nf,axes = plt.subplots(nrows = num_rows,ncols =num_cols,figsize=(12,12))\n\nfor index,city in enumerate(data_location):\n    i,j = (index//num_cols,index % num_cols)\n    city_group = grp.get_group(city)\n    df = city_group[[\"Sunshine\",\"months\"]].groupby(\"months\").mean()\n    sbn.lineplot(data=df,ax = axes[i,j],marker=\"o\").set_title(city)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step is will be show us the most windy Location."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[[\"Location\",\"WindGustSpeed\"]].groupby(\"Location\").mean().sort_values(ascending=False,by=\"WindGustSpeed\").iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(data[[\"Location\",\"WindGustSpeed\"]].groupby(\"Location\").mean().sort_values(ascending=False,by=\"WindGustSpeed\").iloc[:10],x=\"WindGustSpeed\",\n                labels={\"Location\":\"Location Names\",\"WindGustSpeed\":\"Wind Speed (km/h)\"})\nfig.update_layout(\ntitle={\n    \"x\":0.5,\n    \"y\":0.95,\n    \"xanchor\":\"center\",\n    \"yanchor\":\"top\",\n    \"text\": \"Top 10 Location with maximum Windy\"\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Next step, we will show top 12 most windy Location by months mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"year\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[0])\ndata[\"months\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[1])\ndata[\"day\"] = data[\"Date\"].apply(lambda x: x.split(\"-\")[2])\n\ngrp = data.groupby(\"Location\")\ndata_location = [\"Hobart\",\"MelbourneAirport\",\"Wollongong\",\"Ballarat\",\"Woomera\",\"MountGambier\",\"Nhil\",\"GoldCoast\",\"Portland\",\"Williamtown\",\"Nuriootpa\",\"AliceSprings\"]\n\nnum_cols, num_rows = 2,6\nf,axes = plt.subplots(nrows= num_rows,ncols = num_cols,figsize=(12,12))\n\nfor index,city in enumerate(data_location):\n    i,j = (index // num_cols,index % num_cols)\n    city_group =grp.get_group(city)\n    df=city_group[[\"WindGustSpeed\",\"months\"]].groupby(\"months\").mean()\n    sbn.lineplot(data=df,ax = axes[i,j],marker=\"o\").set_title(city)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Preprocessing</p>"},{"metadata":{},"cell_type":"markdown","source":"This section we will preprocessing for training. Firstly we will delete some columns, actually we don't need these columns. \nSome of them (year,day,months) we added  we added these for visualation but know we need to delete them. Also we will delete Date column because it's not meaning for the training every row it has different value and it's can effect badly our learning algorithm(deep learning). After all these we need to clean the data, our dataset has lots of NaN value and we need to decide what can we do for these. For RainToday and RainTomorrow columns i decided the delete NaN values because RainTomorrow is our target column and it's can't be NaN. F For the others columns NaN values i decided to fill each with column mean. Some columns had categorical values and our deep learning algorithm can't understand these so we encode these. For this  i used the LabelEncoder these columns (WindDir3pm,RainToday,Windir9am,WindGustDir,Location). After all of them dataset ready for training. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"RainTomorrow\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Data Cleaning to some NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop([\"year\",\"day\",\"months\"],axis=1)\ndata.drop([\"Date\"],axis=1,inplace =True)\ndata = data.dropna(how =\"all\")\ndata = data.dropna(subset = [\"RainToday\",\"RainTomorrow\"],how=\"any\")\ndata = data.dropna(subset= [\"WindDir9am\",\"WindGustDir\",\"WindDir3pm\"],how=\"any\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fill NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_mean(column):\n    for col in column:\n        data[col] = data[col].fillna(data[col].mean())\ncolumns_fill = data.columns.drop([\"RainToday\",\"RainTomorrow\",\"WindDir9am\",\"WindDir3pm\",\"WindGustDir\",\"Location\"])\nfill_mean(columns_fill)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Encode the categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(column):\n    lbl = LabelEncoder()\n    for col in column:\n        lbl.fit(data[col])\n        data[col] = lbl.transform(data[col].values.reshape(-1,))\ncolumns_process = [\"WindDir3pm\",\"RainToday\",\"WindDir9am\",\"WindGustDir\",\"Location\"]\npreprocessing(columns_process)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Decide train and test data columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_col = data.columns.drop(\"RainTomorrow\")\ntrain = data[feature_col]\ntarget = data[\"RainTomorrow\"].map({\"Yes\":1,\"No\":0}) #In here encode(0,1) target because i will use regression. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">5. Train And Test Split </p>"},{"metadata":{},"cell_type":"markdown","source":"In this section we will split the dataset into training and testing also we will split the training data set that we reserved for training so we to obtain validation dataset. Thus we can know,  what it's can do for real dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_tr,x_test,y_tr,y_test = train_test_split(train,target,test_size=0.2,random_state=1)\nx_train,x_val,y_train,y_val = train_test_split(x_tr,y_tr,test_size=0.1,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X train shape\",x_train.shape)\nprint(\"Y train shape\",y_train.shape)\nprint(\"X Val shape\",x_val.shape)\nprint(\"Y Val shape\",y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">6. Scale Data </p>"},{"metadata":{},"cell_type":"markdown","source":"In this section we will scale our training and testing dataset thus deep learning algorithm can be better. This is be happen: standart scaler is standardize features by removing the mean and scaling to unit variance, so deep learning algorithm can calculate easyly and it's can be learn better."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)\nX_val = sc.transform(x_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">7. Create Deep Learning Model </p>"},{"metadata":{},"cell_type":"markdown","source":"In this section, we will create our deep learning model. Firstly we need to know input_dim this is our feature columns(feature_col) count. After we will decide to how much Dense we need time we try for this mostly. In here isn't have special number, like you can start with this number. Mostly people start with input_dim number. They divide input_dim number to 2 and this keep going to 1. For regression last step just can 1 Dense."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(25,activation=\"relu\",kernel_initializer=\"normal\",input_dim=21))\nmodel.add(Dense(15,activation='relu'))\nmodel.add(Dense(15,activation='relu'))\nmodel.add(Dense(8,activation='relu'))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Compile and train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=\"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\ncall_backs = EarlyStopping(monitor = \"val_loss\",patience = 10, mode = min,restore_best_weights=True )\nhist = model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=32,verbose = 1, epochs = 30,callbacks = [call_backs])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">8. Evaluate The Model </p>"},{"metadata":{},"cell_type":"markdown","source":"In this section we will evaluate the model. For this we will use classification report,confusion matrix and accuracy score. All of this will show us succes the model."},{"metadata":{},"cell_type":"markdown","source":"* Next step is show us training loss,validation loss and training accuracy, validation accuracy values for model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,3))\nplt.subplot(1, 2, 1)\nplt.suptitle('Train', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(hist.history['loss'], color ='r', label='Training Loss')\nplt.plot(hist.history['val_loss'], color ='b', label='Validation Loss')\nplt.legend(loc='upper right')\n\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(hist.history['accuracy'], color ='g', label='Training Accuracy')\nplt.plot(hist.history['val_accuracy'], color ='m', label='Validation Accuracy')\nplt.legend(loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate Test Data\n* Next step, predict test  and compare with test. For this use X_test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)\ncm = confusion_matrix(y_pred,y_test)\nprint(\"Confusion Matrix:\")\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy Score\nacc = accuracy_score(y_test,y_pred)\nprint(\"Accuracy Score: \",acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Show on the graph to confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.heatmap(cm, annot=True,fmt='.5g',linewidths=5,cmap=\"Blues\",xticklabels='01',\n    yticklabels='01')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate Validation Data\n* Next step, predict validation  and compare with y_val. For this use X_val data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix\ny_pred_val = model.predict(X_val)\ny_pred_val = (y_pred_val > 0.5)\ncm_val = confusion_matrix(y_pred_val,y_val)\nprint(cm_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy Score\nacc_val = accuracy_score(y_pred_val,y_val)\nprint(\"Accuracy Validation: \",acc_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Show on the graph to confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"sbn.heatmap(cm_val, annot=True,fmt='.5g',linewidths=5,cmap=\"Blues\",xticklabels='01',\n    yticklabels='01')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}