{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Basic Feature Extraction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can use the text data to extract a number of features even if we do not have sufficient knowledge of Natural Language Processing.\n\nBefore starting, lets quickly read the training file from the dataset in order to perform different tasks on it.\n\nI am using the Twitter Sentiment Dataset.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(dirname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/twitter-sentiment-analysis-hatred-speech/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: Here we are only working with Textual data, bt we can also use the same methods to numerical features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1.1 Number of words\nOne of the most basic features we can extract is the number of words in each tweet.  \n\nTo do this we simply use the split function in python.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_count'] = df['tweet'].apply(lambda x:len(str(x).split(\" \")))\n# df.head()\ndf[['tweet', 'word_count']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.2 Number of Characters\nHere we calculate the number of characters in each tweet. This is done by calculating the length of the tweet.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['char_count'] = df['tweet'].str.len() # This will include spaces / white space.\n# df.head()\ndf[['tweet', 'char_count']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.3 Average Word Length\nWe will also extract anothe feature which will calculate the average word length of each tweet. This can also potentially help us in improving our model.\n\nHere we simply take the sum of the length of all the words and divide it by the total length of the tweet.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words) / len(words))\n\ndf['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n\n# df.head()\ndf[['tweet', 'avg_word']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.4 Number of Stopwords\nGenerally while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n\nHere we have importing stopwords from NLTK which is a basic NLP library in python.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n\n# df.head()\ndf[['tweet', 'stopwords']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.5 Number of Special Char\nOne more interesting feature which we can extract from a tweet is caluclating the number of hastags or methines present in it. This also helps in extracting extra information from our text data.\n\nHere we make use of the `starts with` function because hashtags always appear at the beginning of a word.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hashtags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n\n# df.head()\ndf[['tweet', 'hashtags']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.6 Number of numerics\nJust like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful that should be run while doing similar exercises.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n\n# df.head()\ndf[['tweet', 'numerics']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.7 Number of UpperCase words.\nAnger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identiy those words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['upper'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n\n# df.head()\ndf[['tweet', 'upper']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Basic Pre-Processing\nSo far we have learned hwo to extract basic features from the text data. Before diving into text and feature extraction, our first step should b cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our training data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.1 Lower Case\nThe first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words.\n\nFor instance, while calculating the word count \"Analytics' and 'analytics' will be taken as different words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet_lower'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n# df.head()\ndf[['tweet', 'tweet_lower']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2 Removing Punctuation\nThe next step is to remove punctuation, as it doesn't add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet_lower'] = df['tweet_lower'].str.replace('[^\\w\\s]','')\n# df.head()\ndf['tweet_lower'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: As you can see in the above output, all the punctuation including `#` and `@` has been removed from the `df`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.3 Removal of Stop Words\nAs we discussed earlier, stop words (or commonly occuring words) should be removed from the textdat. \nFor this purpose we can either create a list of stopwords ourselves or we can use predefined libraries.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\ndf.tweet_lower.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.4 Common word removal\nPrevoously, we just removed commonly occuring words in a general sense. We can also remove commonly occuring words from our text data first, lets check the 10 most frequently occuring words in our text data then take a call to remove or retain.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_words = pd.Series(' '.join(df['tweet_lower']).split()).value_counts()[:10]\nfreq_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets remove these words as their presence will not of any use in classification of our text data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_words = list(freq_words.index)\n\ndf['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_words))\n\ndf['tweet_lower'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.5 Rare words removal\nSimilar to the most common words, this time lets remove rarely occuring words from the text. As they are so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rare_words = pd.Series(' '.join(df['tweet_lower']).split()).value_counts()[-10:]\nrare_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove rare words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rare_words = list(rare_words.index)\n\ndf['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_words))\n\ndf['tweet_lower'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All these pre-processing steps are essential and help  us in reducing our vocabulary clutter so that the featues produced in the end are more effective.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.6 Spelling Correction\nWe have all seen tweets with a plethora of spelling mistakes. Out timeline are often filled with hastly sent tweets that are barely legible at times.\n\nIn that regards, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words.\n\nTo achieve this we will use the textblob library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet_lower'][:5].apply(lambda x: str(TextBlob(x).correct()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that it will actually take a lot of time to make these corrections. Therefore just for the purpose of learning, i have shown this technique by applying it on only the first 5 rows.\n\nMoreover we cannot always expect it to be accurate so some care should be taken before applying it.\n\nWe should also keep in mind that words are often used in their abbreviated form. For instance, `your` is used as `ur`. We should treat this before the spelling correction step, other wise these words might be transformed into any other word like above `model take or ðððð ððð` initially it was `model take urð ðððð ððð`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.7 Tokenization\nTokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TextBlob(df['tweet_lower'][1]).words ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.8 Stemming\nStemming refers to the removal of sufficies, like `ing`, `ly`, `s` etc by a simple rule-based approach. For this purpose we will use PorterStemmer from the NLTK library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet_lower'][:5].apply(lambda x:\" \".join([st.stem(word) for word in x.split()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above output, we can see that the word `dysfunctional` has been transformed into `dysfunct`, and `kids` transformed into `kid`, amoong other changes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.9 Lemmatization\nLemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore we usually perfer using lemmatization over stemming.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import Word\ndf['tweet_lower'] = df['tweet_lower'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\ndf.tweet_lower.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Advance Text Processing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Upto this point we have done all the basic pre-processing steps in order to clean our data.\n\nNow we can finally move on to extracting features using NLP technique.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.1 N-grams\nN-grams are the combination of multiple words used together.\nN-grams with N = 1 are called unigrams. Similarly, bigrams when N = 2; trigrams when N = 3 and so on.\n\nUnigrams do not usually contain as much information as compared to bigrams and trigrams.\n\nThe basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with.\n\nOptimum length really depends on the application - if yoyr n-grams are too short, you may fail to capture important differences. On the other hand, they are too long, you may fail to capture the \"general knwoeldge\" and only stick to a particular cases.\n\nSo lets quickly extract bigrams from our tweet using the ngrams function of the textblob library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TextBlob(df['tweet_lower'][0]).ngrams(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TextBlob(df['tweet'][0]).ngrams(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2 Term frequency\nTerm Frequency is simply the ratio of the count of word present in a sentence, to the length of the sentence.\n\nTherefore we can generalize term frequency as\n\n`TF = (Number of times term T appears in the particular row) / (Number of terms in that row`\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1 = (df['tweet_lower'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n\ntf1.columns = ['words', 'tf']\n\ntf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.3 Inverse Document Frequency\nThe intution behind inverse document frequency (IDF) is that a word is not of much use to us if it's appearing in all the documents.\n\nTherefore the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n\n`IDF = log(N/n)` where N is the total number of rows and n is thenumber of rows in which the word was present.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, word in enumerate(tf1['words']):\n    tf1.loc[i, 'idf'] = np.log(df.shape[0] / (len(df[df['tweet_lower'].str.contains(word)])))\n    \ntf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The more the value of IDF, the more unique is the word.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.4 Term Frequency - IDF (TF-IDF)\nTF-ID is the multiplication of the TF and IDF which we calculated above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tf1['tf_idf'] = tf1['tf'] * tf1['idf']\ntf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alternatively we can use sklearn's `TfidfVectorizer` as below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(max_features = 1000, lowercase = True, analyzer = 'word', stop_words = 'english', ngram_range = (1,1))\n\ndf_vect = tfidf.fit_transform(df['tweet'])\n\ndf_vect\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.5 Bag of Words\nBag of Words (BoW) referes to the representation of text which describes the presence of words within the text data. The intution behind this is that two similar text fields will contain similar kind of words, and will therefor have s similar bag of words.\n\nFurther that form the text alone we can learn somethins about the meaning of the document.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow = CountVectorizer(max_features=1000, lowercase = True, ngram_range= (1,1), analyzer = 'word')\n\ndf_bow = bow.fit_transform(df['tweet'])\n\ndf_bow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.6 Sentiment Analysis\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see that it returns a tuple representing polarity and subjectivity of each tweet.\n\nHere we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive and nearer to -1 means a negative sentiment. This can also work as a feature for building a ML Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0])\n\ndf[['tweet','sentiment']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment2'] = df['tweet_lower'].apply(lambda x: TextBlob(x).sentiment[0])\n\ndf[['tweet_lower','sentiment2']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.7 Word Embeddings\nWord Embedding is the representation of text in the form ofo vectors. Th eunderlying idea here is tha similar words will have a minimum distance between their vectors.\n\nWord2Vec models require a lot of text, so either we can train it on our training data or we can use the pre-trained word vectors developed by Google, Wiki etc.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.scripts.glove2word2vec import glove2word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}