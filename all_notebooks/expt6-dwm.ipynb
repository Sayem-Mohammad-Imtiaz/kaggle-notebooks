{"cells":[{"metadata":{},"cell_type":"markdown","source":"## NAME : JAYNIL GAGLANI\n## ROLL NO : 13\n## DATA WAREHOUSING AND MINING\n## EXPERIMENT NO 6"},{"metadata":{"_uuid":"4235fb9a93a98ed1f36bb5c23c42f5fdd0531e6e"},"cell_type":"markdown","source":"**BigMart Sales Prediction practice problem**\n\nWe have train (8523) and test (5681) data set, train data set has both input and output variable(s). We need to predict the sales for test data set.\n\n\n* Item_Identifier: Unique product ID\n\n* Item_Weight: Weight of product\n\n* Item_Fat_Content: Whether the product is low fat or not\n\n* Item_Visibility: The % of total display area of all products in a store allocated to the particular product\n\n* Item_Type: The category to which the product belongs\n\n* Item_MRP: Maximum Retail Price (list price) of the product\n\n* Outlet_Identifier: Unique store ID\n\n* Outlet_Establishment_Year: The year in which store was established\n\n* Outlet_Size: The size of the store in terms of ground area covered\n\n* Outlet_Location_Type: The type of city in which the store is located\n\n* Outlet_Type: Whether the outlet is just a grocery store or some sort of supermarket\n\n* Item_Outlet_Sales: Sales of the product in the particulat store. This is the outcome variable to be predicted."},{"metadata":{"_uuid":"517e5d8c5d0679b9fdfe580924e2d7d5f13d426a"},"cell_type":"markdown","source":"**Load Libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53eb780b85ad11a4982547dd5b7674e60a0e8b9c"},"cell_type":"markdown","source":"**Data Mining**"},{"metadata":{"trusted":true,"_uuid":"dda1ef778ba3c976a5e9acae8f49a04f218f2d2b"},"cell_type":"code","source":"train = pd.read_csv('../input/Train.csv')\ntest = pd.read_csv('../input/Test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7eb80d5e993494ead6a3b2c6d4e46875f892da4"},"cell_type":"markdown","source":"**Exploratory Data Analysis**"},{"metadata":{"trusted":true,"_uuid":"ba5c08031cb977f55eb3672c9c4ac8a961ee801f"},"cell_type":"code","source":"train['source'] = 'train'\ntest['source'] = 'test'\ntest['Item_Outlet_Sales'] = 0\ndata = pd.concat([train, test], sort = False)\nprint(train.shape, test.shape, data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54d252a73b579c3fbd431df4061a3888be362502"},"cell_type":"code","source":"data['Item_Outlet_Sales'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e722fa138f537cfd7ac63403f17df9731f3215ba"},"cell_type":"code","source":"sns.distplot(data['Item_Outlet_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be3b10ad2ad49cf355460cf59a384492c7924187"},"cell_type":"markdown","source":"1. Deviate from the normal distribution.\n1. Have appreciable positive skewness.\n1. Show peakedness."},{"metadata":{"trusted":true,"_uuid":"cd3c64fc99ef3955f6b82d97adfe4c479d025f4b"},"cell_type":"code","source":"print('Skewness: %f' % data['Item_Outlet_Sales'].skew())\nprint('Kurtsis: %f' %data['Item_Outlet_Sales'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorial_features = data.select_dtypes(include=[np.object])\ncategorial_features.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b37b8f20c7fb42ab0a15b4993a6836dda33ae9e8"},"cell_type":"code","source":"numerical_features = data.select_dtypes(include=[np.number])\nnumerical_features.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af4d995f889786fd4e9e5b4ad7f8246859063f07"},"cell_type":"markdown","source":"**Finding Missing values**"},{"metadata":{"_uuid":"454aa80c5eac39669879be6e38c4d5815447694f"},"cell_type":"markdown","source":"**Data Cleaning and Imputing Missing Values**\n\nWe found two variables with missing values – Item_Weight and Outlet_Size. Lets impute the former by the average weight of the particular item. This can be done as:"},{"metadata":{"trusted":true,"_uuid":"007ec6b20d62d581ba20d438f0e2f640fea82ebd"},"cell_type":"code","source":"item_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')\n\nmissing_values = data['Item_Weight'].isnull()\nprint('Missing values: %d' %sum(missing_values))\n\ndata.loc[missing_values,'Item_Weight']  = data.loc[missing_values,'Item_Identifier'].apply(lambda x: item_avg_weight.at[x,'Item_Weight'])\nprint('Missing values after immputation %d' %sum(data['Item_Weight'].isnull()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8d479dd34ebf44f7662fb052a02b1f1634cd3c0"},"cell_type":"markdown","source":"This confirms that the column has no missing values now. Lets impute Outlet_Size with the mode of the Outlet_Size for the particular type of outlet."},{"metadata":{"trusted":true,"_uuid":"211dffbdacaa27175cf56373ab965c99b7830eee"},"cell_type":"code","source":"#Import mode function:\nfrom scipy.stats import mode\n\n#Determing the mode for each\noutlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x.astype('str')).mode[0]))\nprint ('Mode for each Outlet_Type:')\nprint (outlet_size_mode)\n\n#Get a boolean variable specifying missing Item_Weight values\nmissing_values = data['Outlet_Size'].isnull() \n\n#Impute data and check #missing values before and after imputation to confirm\nprint ('\\nOrignal #missing: %d'% sum(missing_values))\ndata.loc[missing_values,'Outlet_Size'] = data.loc[missing_values,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\nprint (sum(data['Outlet_Size'].isnull()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae2fb9edd5f8fdb6ba4f8587e97ea348b5f416af"},"cell_type":"markdown","source":"**Modify Item_Visibility**\n\nWe noticed that the minimum value here is 0, which makes no practical sense. Lets consider it like missing information and impute it with mean visibility of that product."},{"metadata":{"trusted":true,"_uuid":"5b36399204051b4a6ee5ed1b81230c6f37e12387"},"cell_type":"code","source":"#Determine average visibility of a product\nvisibility_avg = data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n\n#Impute 0 values with mean visibility of that product:\nmissing_values = (data['Item_Visibility'] == 0)\n\nprint ('Number of 0 values initially: %d'%sum(missing_values))\ndata.loc[missing_values,'Item_Visibility'] = data.loc[missing_values,'Item_Identifier'].apply(lambda x: visibility_avg.at[x, 'Item_Visibility'])\nprint ('Number of 0 values after modification: %d'%sum(data['Item_Visibility'] == 0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f279f4f8ad4329fa1efe2a89d96b7354b554d42b"},"cell_type":"markdown","source":"**Create a broad category of Type of Item**\n\nEarlier we saw that the Item_Type variable has 16 categories which might prove to be very useful in analysis. So its a good idea to combine them. One way could be to manually assign a new category to each. But there’s a catch here. If you look at the Item_Identifier, i.e. the unique ID of each item, it starts with either FD, DR or NC. If you see the categories, these look like being Food, Drinks and Non-Consumables. So I’ve used the Item_Identifier variable to create a new column:"},{"metadata":{"trusted":true,"_uuid":"caae923caa1225a6888d7a678241c3a8e6c4cf3a"},"cell_type":"code","source":"#Get the first two characters of ID:\ndata['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\n#Rename them to more intuitive categories:\ndata['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food',\n                                                             'NC':'Non-Consumable',\n                                                             'DR':'Drinks'})\ndata['Item_Type_Combined'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e070d3be002edb07c9885a4d836e7d79627b33ba"},"cell_type":"markdown","source":"**Modify categories of Item_Fat_Content**\n\nWe found typos and difference in representation in categories of Item_Fat_Content variable. This can be corrected as:"},{"metadata":{"trusted":true,"_uuid":"781d1913bf6b207283ef9d098e128b9dcfc7be9b"},"cell_type":"code","source":"#Change categories of low fat:\nprint('Original Categories:')\nprint(data['Item_Fat_Content'].value_counts())\n\nprint('\\nModified Categories:')\ndata['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat',\n                                                             'reg':'Regular',\n                                                             'low fat':'Low Fat'})\nprint(data['Item_Fat_Content'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb02fd15225cebdabe6f6aebd17d713f67df75fe"},"cell_type":"code","source":"df = data.loc[:,['Item_Outlet_Sales']]\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9680033a4483072ccc2d3f538328adfa39fc4201"},"cell_type":"markdown","source":"**Determine the years of operation of a store**\nWe wanted to make a new column depicting the years of operation of a store. This can be done as:"},{"metadata":{"trusted":true,"_uuid":"d8bd53d863d8a85a23acfbc412a230830ace1cc2"},"cell_type":"code","source":"data['Outlet_Years'] = 2009 - data['Outlet_Establishment_Year']\ndata['Outlet_Years'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dd7c0d26c3ec533dcf1b138acb181cc06f3301f"},"cell_type":"code","source":"data.index = data['Outlet_Establishment_Year']\ndf = data.loc[:,['Item_Outlet_Sales']]\nts = df['Item_Outlet_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"262d6534bf4bdde3bcb0c0eb6752b00e2903a516"},"cell_type":"code","source":"temp_data = data.loc[data['Outlet_Establishment_Year'] == 1998]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f883ec73ea860fa839787d9137aad1d97645679"},"cell_type":"code","source":"temp_data['Outlet_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e4b711b1222e21af055a2a67238ec2acd228bab"},"cell_type":"code","source":"test_temp_data = test.loc[test['Outlet_Establishment_Year'] == 1998]\ntest_temp_data['Outlet_Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edeedb22d1dd67343a0b6f46896004fa7ccfc1a1"},"cell_type":"markdown","source":"**Numerical and One-Hot Coding of Categorical variables**"},{"metadata":{"trusted":true,"_uuid":"3d3b6a906c0b380c00798e63b735f0fe1714baf6"},"cell_type":"code","source":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#New variable for outlet\ndata['Outlet'] = le.fit_transform(data['Outlet_Identifier'])\nvar_mod = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet','Item_Identifier']\nle = LabelEncoder()\nfor i in var_mod:\n    data[i] = le.fit_transform(data[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"679f00d12da306e94fd2c5aed531200ee18f6abf"},"cell_type":"code","source":"#One Hot Coding:\ndata = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type',\n                              'Item_Type_Combined','Outlet'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72ffe0cc14e341da24fda6fda63b79f0b42b68ab"},"cell_type":"code","source":"#Drop the columns which have been converted to different types:\ndata.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)\n# data.to_csv(\"big_mart_sales.csv\")\n# #Divide into test and train:\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n# #Drop unnecessary columns:\ntest.drop(['source'],axis=1,inplace=True)\ntrain.drop(['source','Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = MinMaxScaler()\nfor col in train.columns:\n    train[col] = ss.fit_transform(train[[col]])\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nax = sns.heatmap(train.corr(), square=True, annot=True, fmt='.1f', linecolor='white',cmap='viridis')\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30)           \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations :\n>> #### The above correlation matrix shows that Item_Outlet_Sales is highly dependent on Item_MRP with correlation coefficient of 0.6"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.Item_Outlet_Sales\nX = train.drop('Item_Outlet_Sales',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multiple Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear Regression Score : \",lr.score(X_test,y_test))\nprint(\"R2 Score : \",r2_score(y_test,y_pred))\nprint(\"Mean Squared Error : \",mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(y_test,y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lr.intercept_)\nprint(lr.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train['Item_MRP']\ny = train.Item_Outlet_Sales\nX = pd.DataFrame(df,columns = ['Item_MRP'])\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1,shuffle=True)\nlr = LinearRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear Regression Score : \",lr.score(X_test,y_test))\nprint(\"R2 Score : \",r2_score(y_test,y_pred))\nprint(\"Mean Squared Error : \",mean_squared_error(y_test,y_pred))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(y_test,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion :\n>> #### The Multiple Linear Regression has average performance in case of multivariate data\n>> #### The Simple Linear Regression on one attribute has very low score of 0.32 "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}