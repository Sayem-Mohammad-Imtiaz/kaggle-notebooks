{"cells":[{"metadata":{"_uuid":"ea76a0856412f5aaffc78c7e2229e544f269c16f"},"cell_type":"markdown","source":"### Sections\n\n* Problem Definition\n* Analyze Data\n* Validation Dataset\n* Evaluate Algortithms: Baseline\n* Evaluate Algorithms: Standardize Data\n* Algorithm Tuning\n* Ensemble Methods\n* Finalize Model\n* Summary"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"\n### Problem Definition\n\nThe focus of this project will be the [Connectionist Bench (Sonar, Mines vs. Rocks) Data Set ](https://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)) avaialble from the UCI Machine Learning Repository. The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.\n\nEach pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.\n\nLet's get started."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d36e4c8be50eeddb1f6188c214f0129a369d46c"},"cell_type":"code","source":"# Load dataset\ndataset = read_csv('../input/sonar.all-data.csv', header=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"021ea6291b4da1134934a323eff8374300178914"},"cell_type":"markdown","source":"### Analyze data\n\nWe are going to take a closer look at our loaded data. Let's begin with some descriptive stats."},{"metadata":{"trusted":true,"_uuid":"0cb7a3c69c4f752b13b522902676e448b04653ff"},"cell_type":"code","source":"# shape\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19c6c21e0689a7128d60f87ea4233a6603327e49"},"cell_type":"code","source":"# types\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"123cca4aa3db4e06dc291cea37b798ab2d9dfbc9"},"cell_type":"code","source":"# head\ndataset.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b3a6a14905284af179f7cb7b1d2696594b4e3b5"},"cell_type":"code","source":"# describe\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd32d1f615c3dea60d43fabcdef965e63b526ad1"},"cell_type":"code","source":"# class distribution\ndataset.groupby(60).size()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0546eed5006993b4b260e1c28c9851566870c5e7"},"cell_type":"markdown","source":"### Validation Dataset\n\nIt is a good idea to use a validation hold-out set. This is a sample of the data that we hold back from our analysis and modeling. We use it right at the end of our project to confirm the accuracy of our final model. It is a smoke test that we can use to see if we messed up and to give us confidence on our estimates of accuracy on unseen data. We will use 80% of the dataset for modeling and hold back 20% for validation."},{"metadata":{"trusted":true,"_uuid":"1c46b8274fbc373b21e3c734b86f3a51aadc8907"},"cell_type":"code","source":"# Split-out validation dataset\narray = dataset.values\nX = array[:,0:60].astype(float)\nY = array[:,60]\nvalidation_size = 0.20\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y,\ntest_size=validation_size, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"885a8ff63af8de7c5889d576bee1c12061737c19"},"cell_type":"markdown","source":"### Evaluate Algorithms: Baseline\n\nLet’s create a baseline of performance on this problem and spot-check a number of different algorithms. We will select a suite of different algorithms capable of working on this classification problem."},{"metadata":{"trusted":true,"_uuid":"ff5a787a5d82d9965131fdd416f79adbd58c1df0"},"cell_type":"code","source":"# Test options and evaluation metric\nnum_folds = 10\nseed = 2019\nscoring = 'accuracy'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d017cf2350094b00d2834c10ae4258bb2f212e11"},"cell_type":"code","source":"# Spot-Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa23370dab5bf9f320742f44fb99f4c023927eb8"},"cell_type":"markdown","source":"The algorithms all use default tuning parameters. Let’s compare the algorithms. We will display the mean and standard deviation of accuracy for each algorithm as we calculate it and collect the results for use later."},{"metadata":{"trusted":true,"_uuid":"01a60c12c253ff34aa8510b5bb7c489d3f08ae67"},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"568c9b414b8f9bf8a19f1273f2456df61df63c70"},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"661e15138d21cf6ea0493027f13fa0b8a12a3f1b"},"cell_type":"markdown","source":"### Evaluate Algorithms: Standardize Data\n\nLet’s evaluate the same algorithms with a standardized copy of the dataset. This is where the data is transformed such that each attribute has a mean value of zero and a standard deviation of one. We also need to avoid data leakage when we transform the data. A good way to avoid leakage is to use pipelines that standardize the data and build the model for each fold in the cross validation test harness. That way we can get a fair estimation of how each model with standardized data might perform on unseen data."},{"metadata":{"trusted":true,"_uuid":"24dd7ecac35e780e692012ef5b02000b090e71ea"},"cell_type":"code","source":"# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',\nLogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA',\nLinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',\nKNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',\nDecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',\nGaussianNB())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36aeaba02adaa4cceae1bc54793f47185dbc84e1"},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"063b752aca4c76e8ef689ca2bfca1048e737459b"},"cell_type":"markdown","source":"### Algorithm Tuning\n\nThe results from above suggest digging deeper into the SVM and KNN algorithms. It is very likely that finetuning configuration beyond the default for these alogorithms may yield even more accurate models.\n\nWe can start off by tuning the number of neighbors for KNN. The default number of neighbors is 7. Below we try all odd values of k from 1 to 21, covering the default value of 7. Each k value is evaluated using 10-fold cross validation on the training standardized dataset."},{"metadata":{"trusted":true,"_uuid":"202ed8acab275f99f95d4a3ec83c18cb4bf7542e"},"cell_type":"code","source":"# Tune scaled KNN\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n# try all odd values of k from 1 to 21, covering the default value of 7.\nneighbors = [1,3,5,7,9,11,13,15,17,19,21]\nparam_grid = dict(n_neighbors=neighbors)\nmodel = KNeighborsClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"434d340381577a48c327b3a71976838e76328373"},"cell_type":"markdown","source":"We can tune two key parameters of the SVM algorithm, the value of C (how much to relax the margin) and the type of kernel. The default for SVM (the SVC class) is to use the Radial Basis Function (RBF) kernel with a C value set to 1.0. We will try a number of simpler kernel types and C values with less bias and more bias (less than and more than 1.0 respectively). Like with KNN, we will perform a grid search using 10-fold cross validation with a standardized copy of the training dataset. "},{"metadata":{"trusted":true,"_uuid":"af0e946f943c6008538f61233a34524f44f2f626"},"cell_type":"code","source":"# Tune scaled SVM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41d35ade11330140ab6a0a4d2473233300b9bd22"},"cell_type":"markdown","source":"### Ensemble Methods\n\nAnother way that we can improve the performance of algorithms on this problem is by using ensemble methods. In this section we will evaluate four different ensemble machine learning algorithms, two boosting and two bagging methods:\n\n* Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).\n* Bagging Methods: Random Forests (RF) and Extra Trees (ET).\n\nWe will use the same test harness as before, 10-fold cross validation. No data standardization is used in this case because all four ensemble algorithms are based on decision trees that are less sensitive to data distributions."},{"metadata":{"trusted":true,"_uuid":"ad6aa4d8d6fce4fcc56140594f8533edd91fde52"},"cell_type":"code","source":"# ensembles\nensembles = []\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\nensembles.append(('RF', RandomForestClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"971281ec631ff1d480afa5525594d80856c74bc2"},"cell_type":"code","source":"# Compare Algorithms\nfig = pyplot.figure()\nfig.suptitle('Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb0e1b44c6c4ac39a458d2f2c4cde2d226ee4f0c"},"cell_type":"markdown","source":"The results suggest GBM may be worthy of further study, with a strong mean and a spread that skews up towards high 90s (%) in accuracy."},{"metadata":{"_uuid":"db1700f8c705880159fa637bd19bb7d4762b0d4c"},"cell_type":"markdown","source":"#### XGBoost\n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance. Let's use it on our dataset and figure out the best parameters."},{"metadata":{"trusted":true,"_uuid":"e167eea0ba5cca8464a6d1b50432197170e06146"},"cell_type":"code","source":"# fit model on training data\nmodel = XGBClassifier()\nn_estimators = [50, 100, 150, 200]\nmax_depth = [2, 4, 6, 8]\nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nparam_grid = dict(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate)\nkfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=seed)\ngrid_search = GridSearchCV(model, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=kfold)\ngrid_result = grid_search.fit(X_train, Y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d6b283bb540e02206ed7137035fcba92a4fd8d0"},"cell_type":"code","source":"means = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9cdab16b8473e36c2a626668a2717bcf0dd0e7f"},"cell_type":"markdown","source":"### Finalize Model\n\nIn this section we will finalize the model by training it on the entire training dataset and make predictions for the hold-out validation dataset to confirm our findings."},{"metadata":{"trusted":true,"_uuid":"9b2641405eb2ad2b80481287bea53a1c9200a2c8"},"cell_type":"code","source":"# XGBoost with optimum parameters from above\nmodel = XGBClassifier(\n    base_score=0.5, \n    booster='gbtree', \n    colsample_bylevel=1,\n    colsample_bytree=1, \n    gamma=0, \n    learning_rate=0.3, \n    max_delta_step=0,\n    max_depth=6, \n    min_child_weight=1, \n    missing=None, \n    n_estimators=150,\n    n_jobs=1, \n    nthread=None, \n    objective='binary:logistic', \n    random_state=0,\n    reg_alpha=0, \n    reg_lambda=1, \n    scale_pos_weight=1, \n    seed=seed,\n    silent=True, \n    subsample=1)\neval_set = [(X_validation, Y_validation)]\n%time model.fit(X_train, Y_train, early_stopping_rounds=10, eval_metric=\"error\", eval_set=eval_set, verbose=True)\nY_predictions = model.predict(X_validation)\naccuracy = accuracy_score(Y_validation, Y_predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a19aaf0cec1a9a4316b285991b5d8e6410ae7a6"},"cell_type":"markdown","source":"### Summary\n\nWe've gone through the following steps so far to come up with a decent model:\n\n* Problem Definition (Sonar return data).\n* Loading the Dataset.\n* Analyze Data (same scale but different distributions of data).\n* Evaluate Algorithms (KNN looked good).\n* Evaluate Algorithms with Standardization (KNN and SVM looked good).\n* Algorithm Tuning (K=1 for KNN was good, SVM with an RBF kernel and C=1.5 was best).\n* Ensemble Methods (Bagging and Boosting, not quite as good as SVM).\n* Ensemble Methods (XGBoost is better than SVM)\n* Finalize Model (use all training data and confirm using validation dataset)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}