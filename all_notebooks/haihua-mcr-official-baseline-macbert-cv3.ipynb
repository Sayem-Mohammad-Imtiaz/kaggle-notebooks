{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 比赛链接  https://www.biendata.xyz/competition/haihua_2021/\n# 参考 https://www.biendata.xyz/models/category/6353/\n\nimport os\nimport sys\nimport time\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport json\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import BertForMultipleChoice, BertTokenizer, AdamW, get_cosine_schedule_with_warmup\n\nDEBUG = False\nglobal_start_t = time.time()\n\nprint('ok')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/haihua-mrc-data/train.json', 'r', encoding='utf-8') as f:\n    train_data = json.load(f)\n    \ntrain_df = []\nfor i in range(len(train_data)):\n    data = train_data[i]\n    content = data['Content']\n    questions = data['Questions']\n    for question in questions:\n        question['Content'] = content\n        train_df.append(question)\ntrain_df = pd.DataFrame(train_df)\n\nwith open('../input/haihua-mrc-data/validation.json', 'r', encoding='utf-8') as f:\n    test_data = json.load(f)\n    \ntest_df = []\nfor i in range(len(test_data)):\n    data = test_data[i]\n    content = data['Content']\n    questions = data['Questions']\n    cls = data['Type']\n    diff = data['Diff']\n    for question in questions:\n        question['Content'] = content\n        question['Type'] = cls\n        question['Diff'] = diff\n        test_df.append(question)\ntest_df = pd.DataFrame(test_df)\n\ntrain_df.to_csv('./train.csv', index=False)\ntest_df.to_csv('./test.csv', index=False)\nprint('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLD_IDX = 3\n\nCFG = {\n    'fold_num': 5,\n    'seed': 42,\n    'model': 'hfl/chinese-macbert-large',\n    'max_len': 312,\n    'epochs': 3,\n    'eval_per_step_num': 500,\n    'patient_epoch': 1.5,\n    'train_bs': 1, \n    'valid_bs': 1,\n    'lr': 2e-5,\n    'num_workers': 4,\n    'accum_iter': 2,\n    'weight_decay': 0.001,\n}\n\ndef seed_all(random_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(random_seed)\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_all(CFG['seed'])\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device: ', device)\n\nprint('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('train.csv')\ntest_df = pd.read_csv('test.csv')\ntrain_df['label'] = train_df['Answer'].apply(lambda x: ['A', 'B', 'C', 'D'].index(x))\ntest_df['label'] = 0\n\nprint('before train_df.shape: ', train_df.shape, 'test.shape: ', test_df.shape)\n\nSAMPLED_TRAIN_NUM = 613\nSAMPLED_TEST_NUM = 507\nif DEBUG:\n    train_df = train_df.iloc[:SAMPLED_TRAIN_NUM]\n    test_df = test_df.iloc[:SAMPLED_TEST_NUM]\n    CFG['epochs'] = 2\n    \nprint('after train_df.shape: ', train_df.shape, 'test.shape: ', test_df.shape)\n\ntokenizer = BertTokenizer.from_pretrained(CFG['model'])\n# tokenizerFast = BertTokenizerFast.from_pretrained(CFG['model'])\n\nprint('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, dataframe):\n        self.df = dataframe\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx): # 将一条数据从（文章，问题，4个选项）转成（文章，问题，选项1）、（文章，问题，选项2）...\n        label = self.df.label.values[idx]\n        question = self.df.Question.values[idx]\n        content = self.df.Content.values[idx]\n        choice = self.df.Choices.values[idx][2:-2].split('\\', \\'')\n        if len(choice) < 4:\n            for i in range(4 - len(choice)):\n                choice.append('D. 不知道')\n        content = [content for i in range(len(choice))]\n        pair = [question + ' ' + i[2:] for i in choice]\n        return content, pair, label\n    \ndef collate_fn(data): #将文章问题选项拼接在一起后，得到分词后的数字id，输出的size是(batch, n_choices, max_len)\n    input_ids, attention_mask, token_type_ids = [], [], []\n    for x in data:\n        text = tokenizer(x[1], text_pair=x[0], padding='max_length', truncation=True, \n                         max_length=CFG['max_len'], return_tensors='pt')\n        input_ids.append(text['input_ids'].tolist())\n        attention_mask.append(text['attention_mask'].tolist())\n        token_type_ids.append(text['token_type_ids'].tolist())\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n    token_type_ids = torch.tensor(token_type_ids)\n    label = torch.tensor([x[-1] for x in data])\n    return input_ids, attention_mask, token_type_ids, label\n\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        \ndef train_model(model, train_loader, val_loader): # 训练一个epoch\n    global fold_step_num, fold_best_step_num, fold_best_acc, patient_step_num\n    \n    model.train()\n    losses = AverageMeter()\n    accs = AverageMeter()\n    \n    optimizer.zero_grad()\n    tk = tqdm(train_loader, total=len(train_loader), position=0, leave=True)\n    for step, (input_ids, attention_mask, token_type_ids, y) in enumerate(tk):\n        input_ids, attention_mask, token_type_ids, y = (input_ids.to(device), attention_mask.to(device),\n                                                        token_type_ids.to(device), y.to(device).long())\n        output = model(input_ids, attention_mask, token_type_ids).logits\n        #print('get here 222')\n\n        loss = criterion(output, y) / CFG['accum_iter']\n        loss.backward()\n\n        if ((step+1)%CFG['accum_iter']==0) or ((step+1)==len(train_loader)): # 梯度累加\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n            \n            fold_step_num += 1\n            if fold_step_num % CFG['eval_per_step_num'] ==0:\n                val_loss, val_acc = test_model(model, val_loader)\n                if val_acc > fold_best_acc:\n                    fold_best_acc = val_acc\n                    fold_best_step_num = fold_step_num\n                    torch.save(model.state_dict(), '{}_fold_best.pt'.format(CFG['model'].split('/')[-1]))\n                print(f'in train() after test_model() mid step, fold_step_num: {fold_step_num} '\n                      f'fold_best_acc:{fold_best_acc:.3f} val_acc: {val_acc:.3f} val_loss: {val_loss:.3f}')\n                \n            if (fold_step_num - fold_best_step_num) > patient_step_num:  # 提前退出\n                break\n\n        acc = (output.argmax(1)==y).sum().item() / y.size(0)\n        losses.update(loss.item()*CFG['accum_iter'], y.size(0))\n        accs.update(acc, y.size(0))\n        \n        tk.set_postfix(loss=losses.avg, acc=accs.avg)\n        \n    return losses.avg, accs.avg\n        \ndef test_model(model, val_loader): # 验证\n    model.eval()\n    \n    losses = AverageMeter()\n    accs = AverageMeter()\n    y_truth, y_pred = [], []\n    \n    with torch.no_grad():\n        tk = tqdm(val_loader, total=len(val_loader), position=0, leave=True)\n        for idx, (input_ids, attention_mask, token_type_ids, y) in enumerate(tk):\n            input_ids, attention_mask, token_type_ids, y = (input_ids.to(device), attention_mask.to(device),\n                                                        token_type_ids.to(device), y.to(device).long())\n            output = model(input_ids, attention_mask, token_type_ids).logits\n            y_truth.extend(y.cpu().numpy())\n            y_pred.extend(output.argmax(1).cpu().numpy())\n            \n            loss = criterion(output, y)\n            acc = (output.argmax(1)==y).sum().item() / y.size(0)\n            losses.update(loss.item(), y.size(0))\n            accs.update(acc, y.size(0))\n            \n            tk.set_postfix(loss=losses.avg, acc=accs.avg)\n            \n    return losses.avg, accs.avg\n        \nprint('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, \n                        random_state=CFG['seed']).split(\n                        np.arange(train_df.shape[0]), train_df.label.values)   # 五折交叉验证\ncv = []\ntrain_start_t = time.time()\nfor fold, (trn_idx, val_idx) in enumerate(folds):\n    if fold!=FOLD_IDX:\n        print(f'fold: {fold} is not FOLD_IDX: {FOLD_IDX}, pass!')\n        continue\n        \n    print('fold: ', fold)\n    fold_start_t = time.time()\n    \n    train = train_df.loc[trn_idx]\n    val = train_df.loc[val_idx]\n    train_set = MyDataset(train)\n    val_set = MyDataset(val)\n    \n    train_loader = DataLoader(train_set, batch_size=CFG['train_bs'], collate_fn=collate_fn, shuffle=True, num_workers=CFG['num_workers'])\n    val_loader = DataLoader(val_set, batch_size=CFG['valid_bs'], collate_fn=collate_fn, shuffle=False, num_workers=CFG['num_workers'])\n    \n    fold_best_acc = 0\n    fold_step_num, fold_best_step_num = 0, 0\n    patient_step_num = int(CFG['patient_epoch'] * len(train_loader) / CFG['accum_iter'] / CFG['train_bs'])\n    model = BertForMultipleChoice.from_pretrained(CFG['model']).to(device)\n    \n    scaler = GradScaler()\n    optimizer = AdamW(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n    criterion = nn.CrossEntropyLoss()\n    scheduler = get_cosine_schedule_with_warmup(optimizer, len(train_loader)//CFG['accum_iter'], \n                                                CFG['epochs']*len(train_loader)//CFG['accum_iter'])\n    \n    for epoch in range(CFG['epochs']):\n        print('epoch: ', epoch)\n        epoch_start_t = time.time()\n        time.sleep(0.2)\n        \n        train_loss, train_acc = train_model(model, train_loader, val_loader)\n        val_loss, val_acc = test_model(model, val_loader)\n        if val_acc > fold_best_acc:\n            fold_best_acc = val_acc\n            torch.save(model.state_dict(), '{}_fold_best.pt'.format(CFG['model'].split('/')[-1]))\n        print(f'epoch {epoch} finished, cost time: {time.time() - epoch_start_t:.2f} sec')\n     \n    print(f'fold {fold} finished, cost time: {time.time() - fold_start_t:.2f} sec')\n    cv.append(fold_best_acc)\n    \nprint('cv is ', cv, 'cv mean is ', np.mean(cv))\nprint(f'Train finished here, total cost time: {time.time() - train_start_t:.2f} sec')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}