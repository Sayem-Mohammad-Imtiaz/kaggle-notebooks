{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nRANDOM_SEED = 42\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/amazonearphonesreviews/AllProductReviews.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grouping_review(rating):\n    rating = int(rating)\n    if rating == 3:\n        return 1\n    elif rating <= 2:\n        return 0\n    else:\n        return 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ReviewStar'] = df['ReviewStar'].apply(grouping_review)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['ReviewStar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_word(x):\n    return len(x.split())\n\ndf['count'] = df['ReviewBody'].apply(lambda x : count_word(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(df['count'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self,reviews,tokenizer,max_len,targets):\n        self.reviews = reviews\n        self.max_len = max_len\n        self.tokenizer = tokenizer \n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self,idx):\n        review = str(self.reviews[idx])\n        target = self.targets[idx]\n        \n        encoding = self.tokenizer.encode_plus(\n          review,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          pad_to_max_length=True,\n          return_attention_mask=True,\n          truncation=True,\n          return_tensors='pt'\n        )\n        \n        return {\n            'review_txt' : review,\n            'input_ids' : encoding['input_ids'].flatten(),\n            'attention_mask' : encoding['attention_mask'].flatten(),\n            'targets' : torch.tensor(target, dtype=torch.long)\n        }   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df_train, df_test = train_test_split(\n  df,\n  test_size=0.3,\n  random_state=RANDOM_SEED,\n)\ndf_val, df_test = train_test_split(\n  df_test,\n  test_size=0.5,\n  random_state=RANDOM_SEED,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_test['ReviewStar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_val.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data_loader(df,tokenizer,max_len,batch_size,num_workers):\n    ds = ProductDataset(reviews = df.ReviewBody.to_numpy(),\n                       tokenizer=tokenizer,\n                       max_len=max_len,\n                       targets = df.ReviewStar.to_numpy())\n    \n    return DataLoader(\n                ds,\n                batch_size = batch_size,\n                num_workers = num_workers\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE,num_workers=4)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE,num_workers=1)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE,num_workers=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = next(iter(train_data_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = ['negative','neutral','positive']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self,n_classes):\n        super(SentimentClassifier,self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.2)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        _, o2 = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask\n        )\n        \n        output = self.drop(o2)\n        \n        return self.out(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SentimentClassifier(len(n_classes))\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 5\noptimizer = AdamW(model.parameters(),lr=2e-5,correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nschedular = get_linear_schedule_with_warmup(\n                optimizer,\n                num_warmup_steps=0,\n                num_training_steps=total_steps\n            )\nloss_fn = nn.CrossEntropyLoss().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(\n    model,\n    data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    schedular,\n    len_examples):\n    \n    \n    model = model.train()\n    correct_predictions =  0\n    losses = []\n    \n    for bi,d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        outputs = model(\n            input_ids = input_ids,\n            attention_mask = attention_mask\n        )\n        \n        _,preds = torch.max(outputs,dim=1)\n        loss = loss_fn(outputs,targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm(model.parameters(),max_norm=1.0)\n        optimizer.step()\n        schedular.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double() / len_examples, np.mean(losses)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(\n    model,\n    data_loader,\n    loss_fn,\n    device,\n    len_examples):\n    \n    \n    model = model.eval()\n    correct_predictions =  0\n    losses = []\n    \n    with torch.no_grad():\n        for bi,d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            targets = d['targets'].to(device)\n            outputs = model(\n                input_ids = input_ids,\n                attention_mask = attention_mask\n            )\n        \n            _,preds = torch.max(outputs,dim=1)\n            loss = loss_fn(outputs,targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    \n        return correct_predictions.double() / len_examples, np.mean(losses)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n  print(f'Epoch {epoch + 1}/{EPOCHS}')\n  print('-' * 10)\n  train_acc, train_loss = train_epoch(\n    model,\n    train_data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    schedular,\n    len(df_train)\n  )\n  print(f'Train loss {train_loss} accuracy {train_acc}')\n  val_acc, val_loss = eval_model(\n    model,\n    val_data_loader,\n    loss_fn,\n    device,\n    len(df_val)\n  )\n  print(f'Val   loss {val_loss} accuracy {val_acc}')\n  print()\n  history['train_acc'].append(train_acc)\n  history['train_loss'].append(train_loss)\n  history['val_acc'].append(val_acc)\n  history['val_loss'].append(val_loss)\n  if val_acc > best_accuracy:\n    torch.save(model.state_dict(), 'best_model_state.bin')\n    best_accuracy = val_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history['train_loss'], label='train loss')\nplt.plot(history['val_loss'], label='validation loss')\nplt.title('Training history')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc, _ = eval_model(model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test))\n\ntest_acc.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions(model, data_loader):\n  model = model.eval()\n\n  news_headline = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n\n  with torch.no_grad():\n    for d in data_loader:\n      texts = d['review_txt']\n      input_ids = d['input_ids'].to(device)\n      attention_mask = d['attention_mask'].to(device)\n      targets = d['targets'].to(device)\n\n      outputs = model(\n          input_ids = input_ids,\n          attention_mask = attention_mask\n      )\n\n      _, preds = torch.max(outputs, dim=1)\n\n      news_headline.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(outputs)\n      real_values.extend(targets)\n\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return news_headline, predictions, prediction_probs, real_values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred, target_names = n_classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_confusion_matrix(confusion_matrix):\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment');\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=n_classes, columns=n_classes)\nshow_confusion_matrix(df_cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}