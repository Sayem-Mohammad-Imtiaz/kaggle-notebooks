{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi there, thanks for stopping by. This is my first attempt to explain something in easy and comprehensive way. Once you completely go through this work, please let me know how you feel about it in comments. That will definitely help me improve myself. \n\nI have divided the contents of this notebook in following way."},{"metadata":{},"cell_type":"markdown","source":"1. Explaining Concepts \n\n    1.1. Linear Regression\n        1.1.1. Simple Linear Regression\n    1.2. Sum of Squared Errors (SSE)\n        1.2.1. Ridge Regularization (L2)\n        1.2.2. Lasso Regularization (L1)\n        \n    1.2. Performance Metrics \n        1.2.1. Mean Squared Error \n        1.2.2. Mean Absolute Error\n        1.2.3. Coefficient of Determination or R squared \n        1.2.4. Adjusted R squared\n        \n2. Applying Models\n\n    2.1. Overview of data\n    \n    2.2. Data Preprocessing \n\n    2.3. Splitting the data\n    \n    2.4. Applying Linear Regression\n    \n    2.5. Applying Ridge Regression\n        2.5.1. Hyperparamter tuning \n    \n    2.6. Applying Lasso Regression\n        2.6.1. Hyperparameter Tuning\n        \n3. Conclusion        "},{"metadata":{},"cell_type":"markdown","source":"# 1. Explaining Concepts\n## 1.1. Linear Regression \nLinear Regression is one of the simplest statistical models we have. It tries to find the relationship between dependent variable (y) and feature space $X = \\{x_{1}, x_{2}, x_{3}, \\dots , x_{n}\\}$ assuming there are $n$ independent variables. Linear Regression tries to predict the value of $y$, given $X$, where $y$ is a continuous value and $x_{i}$ can have continuous or discrete values both. \n\nThe relationship which is given linear regression looks like this.\n\n$y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\dots + \\beta_{n}x_{n}$ , where $\\beta_{i}$  are the coefficients or parameters associated with each feature, $\\beta_{0}$ is the bias term so that our model is not forced to go through the center. \n\nLinear regression is a linear model because the relationship is given with linear combination of parameters and features. \n\nThere are many flavours or extensions of linear regression which can be used in different scenarios. In fact, linear regression is just a special case of **Generalized Linear Models (GRMs)** where the assumption is that the dependent variable follows normal distribution."},{"metadata":{},"cell_type":"markdown","source":"### 1.1.1. Simple Linear Regression\nSimple Linear Regression is a special case of linear regression where theere is only one independent variable. So the relationship is given like this:\n\n$y = \\beta_{0} + \\beta_{1}x$, where $\\beta_{0}$ is the bias, $\\beta_{0}$ is coefficient of feature $x$.\n\nThis is basically a line which best fits our data. So how do we calculate best fit line ? We use something called **Ordinary Least Squares (OLS)** which we will see next."},{"metadata":{},"cell_type":"markdown","source":"## 1.2. Sum of Squared Errors (SSE) in linear regression\nWe get the best fit line or best fit model by calculating the values of $\\beta_{0}$ and $\\beta_{1}$ in the equation $y = \\beta_{0} + \\beta_{1}x$. But how do we calculate it? \n\nTo calculate those values, we first calculate something called error term. Let's assume for the data point $x_{i}$, $y_{i}$ is the true value in hand and $\\hat y_{i}$ is predicted value, such that $\\hat y_{i}$ = $\\beta_{0} + \\beta_{1}x_{i}$. Then the error for that data point will be given as\n\n$\\varepsilon_{i} = y_{i} - \\hat y_{i}$\n                                \nWe want to reduce the magnitude of this error for each point in our data. For that we calculate something called **sum of squared residuals**\n\n$SSE = \\sum_{i=1}^{n}\\varepsilon_{i}^{2} = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2$\n                            \nNow our job is to reduce this value. Squaring the error terms means whether the point is below or above the line, the only thing which matters is the distance of the point from the line. Squaring the errors also avoids the situation where the error terms will cancel each other.                    \n\n\nNow our probelm has become an optimization problem where we want to minimize the Sum of Squared Errors (SSE) for all points $x_{i}$.\n\n$Find \\quad \\min Q(\\beta_{0}, \\beta_{1}),\\quad for \\ Q(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{n}\\varepsilon_{i}^{2} = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2$"},{"metadata":{},"cell_type":"markdown","source":"### 1.2.1. Ridge Regression - L2 Regularization\nRidge Regression is also known as ridge regularization which is a penalization technique. It penalizes the SSE with a term $\\lambda * |{\\beta_{1}}|^2$, where lambda is called regularization strength. Regularization actually helps a model to **not** overfit the data. It introduces a bias in our model which helps to reduce generalization error.\n\nNow, our optimization problem becomes like this:\n\n$Find \\quad \\min Q(\\beta_{0}, \\beta_{1}),\\quad for \\ Q(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2 + \\lambda*|{\\beta_{1}}|^2$\n\nRidge penalizes more for higher values of $\\beta_{1}$. Also, $\\lambda$ is positive value. Higehr is the value of $\\lambda$, higher is the penalization.\n\nRidge regression is also called L2 regularization beacuse it is squared L2 norm of coefficients."},{"metadata":{},"cell_type":"markdown","source":"### 1.2.2. Lasso Regression - L1 Regularization\nLasso regression is very similar to Ridge regression as they both try to penalize the the model to add a bias and to reduce generalization error. They both try to make the model less sensitive to training data, which helps in better prediction in test data. \n\nMinimization problem equation in case of Lasso regression becomes like this:\n\n$Find \\quad \\min Q(\\beta_{0}, \\beta_{1}),\\quad for \\ Q(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2 + \\lambda*|{\\beta_{1}}|$\n\nLasso regression's penalty term has $\\lambda*|\\beta_{1}|$. Lasso regression is also called L1 regularization beacuse it is L1 norm of coefficients."},{"metadata":{},"cell_type":"markdown","source":"### 1.2.3. Elastic net regularization\nElastic net regularization just changes the penalization term to the sum of L1 and L2 penalties. The equation for minimization problem in Elastic Net is :\n\n$Find \\quad \\min Q(\\beta_{0}, \\beta_{1}),\\quad for \\ Q(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{n}(y_{i} - \\beta_{0} - \\beta_{1}x_{i})^2 + \\lambda_{1}*|{\\beta_{1}}| + \\lambda_{2}*|{\\beta_{1}}|^2$\n\nwhere $\\lambda_{1}, \\lambda_{2}$ are regularization strength parameters.\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 1.3. Performance metrics for test data"},{"metadata":{},"cell_type":"markdown","source":"### 1.3.1 Mean Squared Error (MSE) or Mean Squared Deviation (MSD)\nMSE measures the average of the squares of the errorsâ€”that is, the average squared difference between the estimated values and the actual value.\n\n$MSE = \\dfrac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n\nMSE easily gets affected by outliers because outlier cases are amplified. Sometimes, Root Mean Squared Error is also used instead of MSE. RMSE has unit same as the independent valriable, while MSE is squared unit."},{"metadata":{},"cell_type":"markdown","source":"### 1.3.2. Mean Absolute Error (MAE)\nMAE is the average of difference between the predicted values and their cirresponding truth values. It is given by \n$MAE = \\dfrac{1}{n}\\sum_{i=1}^{n}|y_{i} - \\hat y_{i}|$\n\nMean absolute errors does not amplify the outlier results and more robust. "},{"metadata":{},"cell_type":"markdown","source":"### 1.3.3. Coefficient of determination or $R^2$\n$R^2$ is the measure of proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n$R^2$ is given by \n\n$R^2 = 1 - \\dfrac{SS_{res}}{SS_{tot}}$, where \n$SS_{tot} = \\sum_{i=1}^{n}(y_{i} - \\bar{y})^2, \\quad SS_{res} = \\sum_{i=1}^{n}(y_{i} - \\hat{y_{i}})^2$\n\nwhere $SS_{res}$ is the sum of squares of residuals, also called the residual sum of squares, \n$SS_{tot}$ is the total sum of squares (proportional to the variance of the data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# What is adjusted R squared ?","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Woof!! That was too much of theory, lets jump into the code now. "},{"metadata":{},"cell_type":"markdown","source":"## 2. Implementing models\n### 2.1. Overview of Data\nOur data has following features/columns.\n1. Car_Name - name of the car\n2. Year - model year of the car\n3. Selling_Price - Price which the seller wants to sell his/her car (in lacs) - it dependent variable\n4. Present_Price - Present ex-showroom price for the car (in lacs)\n5. Kms_Driven - Kms driven by the car\n6. Fuel_Type -  Petrol, Diesel or CNG\n7. Seller_Type - Dealer or Individual\n8. Transmission - Manual or Automatic\n9. Owner - how many owners the car have had till now\n\n\nCar_Name is not of good use to us. Year, Present_Price, Kms_Driven and Owner are numeric features.\nFuel_Type, Seller_Type and Transmission are categorical variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"../input/vehicle-dataset-from-cardekho/car data.csv\")\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Data Preprocessing\n1. We drop the Car_Name.\n2. We will take the numerical features as it is.\n3. We will one hot encode the categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_df = [data[[\"Year\", \"Present_Price\", \"Kms_Driven\", \"Owner\"]], \n           pd.get_dummies(data[[\"Fuel_Type\", \"Seller_Type\", \"Transmission\"]], drop_first=True), data[[\"Selling_Price\"]]]\nX = pd.concat(pair_df, axis=1)\ny = data[[\"Selling_Price\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets have a look into processed data\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dependent variable\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation of features with dependent variables\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(16,8))\ncorrmat = X.corr()\n# picking the top 10 correlated features\ncols = corrmat.index\ncm = np.corrcoef(X[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's delete the Selling_Price from X\nX.drop(labels=[\"Selling_Price\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Splitting the dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the dataset\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.values\ny_train = y_train.values\nX_test = X_test.values\ny_test = y_test.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4. Applying Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MSE on training data\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_true=y_train, y_pred=linreg.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MAE on training data \nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_true=y_train, y_pred=linreg.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# R squared on training data\n# This method returns R squared value\n# This will first predict the values for X_train since our model is already fit and ten will calculate the R^2 value\nlinreg.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will get the same value with r2_score function also\nfrom sklearn.metrics import r2_score\nr2_score(y_true=y_train, y_pred=linreg.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets check the metrics on test data\nmse = mean_squared_error(y_true=y_test, y_pred=linreg.predict(X_test))\nmae = mean_absolute_error(y_true=y_test, y_pred=linreg.predict(X_test))\nr2 = linreg.score(X_test, y_test)\n\nprint(\"MSE on test data: \", mse)\nprint(\"MAE on test data: \", mae)\nprint(\"R squared on test data: \", r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With MSE and MAE increasing and R squared decreasing on test data, our model is little bad with unseen data. But that's not basd also. Anyways we have methods already in hand to deal better with unseen data."},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.1. Feature Importance using Coefficent values"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.round(linreg.coef_.ravel(), 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, it seems Kms_driven has almost no impact or very less negative impact on Selling_Price. That is, with increase of Kms_Driven, the price goes down but varies very less.\n\nSimilary Year, Present_Price, Fuel_Type being Petrol and Diesel both increase the Selling_Price while Kms_Driven, Owners, Seller_Type being individual and Transmission being manual decreses the Selling Price."},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bias term in our model \nlinreg.intercept_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.4.2. Plotting the predicted price and actual selling price"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# For training data\nplt.figure(figsize=(20, 10))\nplt.plot(range(0, len(y_train)), y_train, label=\"TrueValues\", marker=\"*\", linewidth=3)\nplt.plot(range(0, len(y_train)), linreg.predict(X_train), label=\"PredictedValues\", marker=\"*\", linewidth=3)\nplt.xlabel(\"Indices\",fontsize=20)\nplt.ylabel(\"Selling Price of Cars\",fontsize=20)\nplt.title(\"True Selling Price Vs. Predicted Selling Price\",fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For Test data\nplt.figure(figsize=(20, 10))\nplt.plot(range(0, len(y_test)), y_test, label=\"TrueValues\", marker=\"*\", linewidth=3)\nplt.plot(range(0, len(y_test)), linreg.predict(X_test), label=\"PredictedValues\", marker=\"o\", linewidth=3)\nplt.xlabel(\"Indices\",fontsize=20)\nplt.ylabel(\"Selling Price of Cars\",fontsize=20)\nplt.title(\"True Selling Price Vs. Predicted Selling Price\",fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5. Applying Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_prices(y, y_pred, data_string):\n    plt.figure(figsize=(20, 10))\n    plt.plot(range(0, len(y)), y, label=\"TrueValues\", marker=\"*\", linewidth=3)\n    plt.plot(range(0, len(y)), y_pred, label=\"PredictedValues\", marker=\"o\", linewidth=3)\n    plt.xlabel(\"Indices\", fontsize=20)\n    plt.ylabel(\"Selling Price of Cars\", fontsize=20)\n    plt.title(\"True Vs. Predicted S.P. - \" + data_string, fontsize=20)\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    \n    # Train data \n    mse = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))\n    mae = mean_absolute_error(y_true=y_train, y_pred=model.predict(X_train))\n    r2 = model.score(X_train, y_train)\n\n    print(\"MSE on train data: \", mse)\n    print(\"MAE on train data: \", mae)\n    print(\"R squared on train data: \", r2)  \n    \n    print()\n    print(\"#\"*50)\n    print()\n    \n    # Test data \n    mse = mean_squared_error(y_true=y_test, y_pred=linreg.predict(X_test))\n    mae = mean_absolute_error(y_true=y_test, y_pred=linreg.predict(X_test))\n    r2 = linreg.score(X_test, y_test)\n\n    print(\"MSE on test data: \", mse)\n    print(\"MAE on test data: \", mae)\n    print(\"R squared on test data: \", r2)\n    \n    print()\n    print(\"#\"*50)\n    print()\n    \n    plot_prices(y_train, model.predict(X_train), \"TRAINING SET\")\n    \n    plot_prices(y_test, model.predict(X_test), \"TEST SET\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridreg = Ridge()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"apply_model(ridreg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much of a better performanece than Linear regression with normal Ridge Regression. But we can do some hyperparamter tuning here.\n\nFor Ridge we have regularization strength as hyperparamter. In sklearn it is given by value alpha which is by default values to 1. More the value of alpha more is the regularization. Let's do it."},{"metadata":{},"cell_type":"markdown","source":"#### 2.5.1. Hyperparamter Tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparams = {\"alpha\": [.01, .1, .5, .7, 1, 1.5, 2, 2.5, 3, 5, 10]}\nridreg = Ridge()\nclf = GridSearchCV(estimator=ridreg, param_grid=params, cv=5, return_train_score=True)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With best value of alpha\nridreg = Ridge(alpha=3)\napply_model(ridreg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2.6. Applying Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nparams = {\"alpha\": [.00001, .0001, .001, .005, .01, .1, 1, 5]}\nlasreg = Lasso()\nclf = GridSearchCV(estimator=lasreg, param_grid=params, cv=5, return_train_score=True)\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasreg = Lasso(alpha=.00001)\napply_model(lasreg, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.round(lasreg.coef_, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Even with the best fitted parameters we don't have much of an improvement. I guess we need contend ourselves here \n# this only.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using statsmodels \nimport statsmodels.api as sm\nres = sm.OLS(endog=y, exog=X).fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Above last cell is just a food for your thought. There are a lot of things which I have not covered in this notebook, some of them are given in above cell's output. Try to read out there in net about them."},{"metadata":{},"cell_type":"markdown","source":"If you have come this far, then once again Thank You. Do let me know your thoughts in commments. Happy Regressing!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}