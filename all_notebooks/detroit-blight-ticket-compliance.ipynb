{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Blight Ticket Compliance Prediction \n","metadata":{}},{"cell_type":"code","source":"# import the required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report,roc_curve, auc, plot_roc_curve\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the dataset","metadata":{}},{"cell_type":"code","source":"path = '../input/blight-violations-final/Blight_Violations_Final.csv'\n\nDataset = pd.read_csv(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General view of the raw data\nprint(Dataset.shape)\n(Dataset.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data pre processing ","metadata":{}},{"cell_type":"code","source":"# There are quite number of redundant columns which have either no data or no meaningful information\n# Lets check those columns\nDataset.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X and Y columns are irrelevatnt for modelling, also column cleanup_cost amount, payment_amount columns have abosolutely no data\nDataset = Dataset.drop(['X','Y'],axis =1)\nDataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we need to find the whether a person will be compliant or not, we need to train our model with data which contains people who were found guilty\n","metadata":{}},{"cell_type":"code","source":"Dataset.disposition.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The disposition terms 'by', 'by Default' and 'Responsible by' seems ambiguous\n Lets check the data for these instances ","metadata":{}},{"cell_type":"code","source":"#For disposition 'Responsible by'\nDataset[Dataset['disposition'] == 'Responsible by'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For disposition 'by Default'\nDataset[Dataset['disposition'] == 'by Default'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For disposition 'by'\nDataset[Dataset['disposition'] == 'by'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code for checking the data for 'by' cases\n\nprint(Dataset[Dataset['disposition'] == 'by'].shape)\nDataset[Dataset['disposition'] == 'by'].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above code, it can be clearly stated that all cases having  disposition as 'by' have no judgement date, therefore the records seems ambiguous for analysis","metadata":{}},{"cell_type":"markdown","source":"Therefore, the dataset needs to be filterd for only those who are held responsible","metadata":{}},{"cell_type":"code","source":"cases = ['Responsible by Default',\n        'Responsible by Admission',\n       'Responsible by Determination',\n       'Responsible (Fine Waived) by Determination',\n       'Responsible (Fine Waived) by Admission',\n       'Responsible - Compl/Adj by Default',\n       'Responsible - Compl/Adj by Determination',\n       'Responsible by Dismissal',\n       'Responsible (Fine Waived) by City Dismissal']\n\nCompliant_dataset = Dataset.loc[Dataset['disposition'].isin(cases)] \nCompliant_dataset.shape","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next task is to define whether a person is compliant or not\n\nFor that, following conditions must be applicale for a person to be compliant:\n* The person pays fine within 6 months after judgement date\n* The person pays full amount for fine\n* The person pays fine prior to judgement (By own admission)\n\n","metadata":{}},{"cell_type":"code","source":"conditions = [\n    ((Compliant_dataset['payment_status'] == 'NO PAYMENT DUE')\n     | ((Compliant_dataset['payment_status'] == 'PAID IN FULL') & (pd.to_datetime(Compliant_dataset['payment_date']).dt.to_period('M').astype(int) - pd.to_datetime(Compliant_dataset['judgment_date']).dt.to_period('M').astype(int)  <= 6))),\n    ((Compliant_dataset['payment_status'] == 'PARTIAL PAYMENT APPLIED') | (Compliant_dataset['payment_status'] =='NO PAYMENT APPLIED')\n     | (pd.isna(Compliant_dataset['payment_status']) == True) | (Compliant_dataset['payment_status'].isnull() == True) | (pd.to_datetime(Compliant_dataset['payment_date']).dt.to_period('M').astype(int) - pd.to_datetime(Compliant_dataset['judgment_date']).dt.to_period('M').astype(int)  > 6))\n       ]\n\n# create a list of the values we want to assign for each condition\nvalues = ['Compliant', 'Non_Compliant']\n\n# create a new column and use np.select to assign values to it using our lists as arguments\nCompliant_dataset['Compliance'] = np.select(conditions, values)\n\n# display updated DataFrame\nCompliant_dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Compliant_dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Handling the missing values","metadata":{}},{"cell_type":"code","source":"# Handle missing values\nCompliant_dataset.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the missing value by bar chart\n\nmsno.bar(Compliant_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It can be seen in the graph that some features has very less values, we can remove those features/columns.","metadata":{}},{"cell_type":"code","source":"# removing the features having very less values \n#Also removing keys (Primary keys such as ticket_id) \nCompliant_dataset_new =  Compliant_dataset.drop(['ticket_id','ticket_number',\"violation_zip_code\", \"non_us_str_code\", \"country\", \"payment_amount\",\n                                                 \"collection_status\", \"parcelno\", \"clean_up_cost\",'oid'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# again check the missing values \nCompliant_dataset_new.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now only payment_date and payment_status have large missing values.\nAs these attributes are important, we can't remove them directly. \n\nThere are cases where payment is not made and the status due to which is null , therefore payment_date will also be null,let replace those payment_date to year 2099","metadata":{}},{"cell_type":"code","source":"# Removing payment_date will null values for valid payment status\nCompliant_dataset_new = Compliant_dataset_new.drop(Compliant_dataset_new[(Compliant_dataset_new['payment_date'].isnull() == False )\n                                                                         & (Compliant_dataset_new['payment_status'].isnull() == True)].index)\n# For Compliant records with no payment due, replacing the payment date with judgment date\nCompliant_dataset_new.loc[Compliant_dataset_new['payment_status'] =='NO PAYMENT DUE', 'payment_date'] = Compliant_dataset_new['judgment_date']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the remaining payment status is null because no payment is done for non compliant, so replacing the null with 'No payment date'. Similarly, for payment_date we are filling with 2099 year and will consider this year as a representation for non compliance date","metadata":{}},{"cell_type":"code","source":"Compliant_dataset_new[\"payment_status\"].fillna(\"NO PAYMENT MADE\", inplace = True)\nCompliant_dataset_new[\"payment_date\"].fillna(\"2099/12/12 00:00:00+00\", inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dumping the value for remaining nul records\nCompliant_dataset_new = Compliant_dataset_new.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Compliant_dataset_new.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### To understand the recent trends and patterns, the dataset is been filtered for violations occured from 2018","metadata":{}},{"cell_type":"code","source":"\nCompliant_dataset_new=Compliant_dataset_new.loc[(Compliant_dataset_new['violation_date'] >= '2018-01-01')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Compliant_dataset_new.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizations","metadata":{}},{"cell_type":"code","source":"# Count of Non-Compliance vs Compliance in the dataset\n\nCompliant_dataset_new.Compliance.value_counts().plot(kind='bar', title='Count (compliance)', color=['red', 'green']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a dataset to check how many non compliants pay their fine in upcoming months (i.e after the period of 6 months)\n\nimport datetime\nCompliant_dataset_new['month_gap']= (pd.to_datetime(Compliant_dataset['payment_date']).dt.to_period('M').astype(int) - pd.to_datetime(Compliant_dataset['judgment_date']).dt.to_period('M').astype(int))\nC = Compliant_dataset_new.loc[(Compliant_dataset_new['month_gap'] > 6)  & (Compliant_dataset_new['month_gap'] < 100)]\nC = C.loc[C['payment_status'] =='PAID IN FULL']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n#sns.axes_style('white')\nsns.set_context(\"talk\")\nsns.countplot(x=\"month_gap\", data=C,palette=\"Greens_d\")\nplt.xlabel('Months after compliance period is over')\nplt.ylabel('Non_compliant who paid fines')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking ratio of compliance if discount is provided \nD = Compliant_dataset_new.loc[(Compliant_dataset_new['discount_amount'] > 0)]\nE =Compliant_dataset_new.loc[(Compliant_dataset_new['discount_amount'] == 0)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nf, axes = plt.subplots(1, 2, figsize=(18,5))\n\nf.suptitle('Compliance Measure based on discount')\n\nsns.countplot(x=\"payment_status\", hue = 'Compliance',data=D, ax = axes[0])\naxes[0].set_title('Discount provided')\naxes[0].set(xlabel=\"Payment Status\", ylabel = \"Count of People\")\nsns.countplot(x=\"payment_status\", hue = 'Compliance',data=E,  ax = axes[1])\naxes[1].set_title('Discount Not provided')\naxes[1].set(xlabel=\"Payment Status\", ylabel = \"Count of People\")\nsns.set_palette('dark')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsns.relplot(data = Compliant_dataset_new, x = 'fine_amount', y = 'discount_amount', hue = 'Compliance')\nsns.reset_defaults()\nplt.title('Fine to Discount Relation')\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\nplt.xlabel('Fine Amount')\nplt.ylabel('Discount Amount')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert all values to lowercase helps standardize city names\nCompliant_dataset_new['city'] =Compliant_dataset_new['city'].str.lower()\nCompliant_dataset_new['city'] = Compliant_dataset_new['city'].replace('det','detroit')\nCompliant_dataset_new['city'] = Compliant_dataset_new['city'].replace(['mendota hts., ','mendota hts.,','mendota hghts.', 'mendota   heights','menndota  heights','mendata  hgt','mendeta height'],'mendota heights')\nCompliant_dataset_new['city'] = Compliant_dataset_new['city'].replace('menomonee','menomonee falls')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of compliance cases \nCompliant_cities=Compliant_dataset_new[Compliant_dataset_new['Compliance']=='Compliant'].groupby('city')['Compliance'].count()\nhead = Compliant_cities.sort_values(ascending=False).head(5)\nsns.reset_defaults()\nsns.barplot(head.index, head.values)\nplt.xticks(rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Non_Compliant_cities=Compliant_dataset_new[Compliant_dataset_new['Compliance']=='Non_Compliant'].groupby('city')['Compliance'].count()\n#visualize the most 5 Non compliant cities\nhead1 = Non_Compliant_cities.sort_values(ascending=False).head(5)\nsns.barplot(head1.index, head1.values)\nplt.xticks(rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Compliant_df= pd.DataFrame({'city': Compliant_cities.index,     \n                           'Compliant_counts':Compliant_cities.values})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Non_Compliant_df= pd.DataFrame({'city': Non_Compliant_cities.index,     \n                           'Non Compliant_counts': Non_Compliant_cities.values})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.concat([Compliant_df, Non_Compliant_df], axis=1)\nresult['Compliant_counts'] = result['Compliant_counts'].fillna(0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualize compliant and non-complaint rate of 5 top cities\nsns.reset_defaults()\nx = np.arange(len(head.index))  # the label locations\nwidth = 0.35  # the width of the bars\nsns.set_style('darkgrid')\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, head.values, width, label='Complaint')\nrects2 = ax.bar(x + width/2, head1.values, width, label='Non-complaint')\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Count')\nax.set_title('Compliant and Non-complaint rate of 5 top cities')\nax.set_xticks(x)\nax.set_xticklabels(head.index)\nax.legend()\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.countplot(data=Compliant_dataset_new, x=\"agency_name\", hue=\"Compliance\", palette ='plasma')\nplt.xticks(rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance (Mutual Information)","metadata":{}},{"cell_type":"code","source":"columns = ['agency_name','inspector_name','violator_name','violation_street_number','violation_street_name','violator_id',\n'mailing_address_str_name','city','state','violation_date','ticket_issued_time',\n'hearing_date','hearing_time','judgment_date','violation_code','violation_description',\n'disposition','fine_amount','admin_fee','state_fee','late_fee','discount_amount','judgment_amount',\n'balance_due','payment_date','payment_status','violation_address','Compliance']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Compliant_dataset_new.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Compliant_dataset_new_labeled  = Compliant_dataset_new.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\n  \n# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n  \n# Encode labels in column 'species'.\nfor i in columns:\n    Compliant_dataset_new_labeled[i]= label_encoder.fit_transform(Compliant_dataset_new[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Compliant_dataset_new_labeled.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.ensemble import AdaBoostClassifier\nX = Compliant_dataset_new_labeled.drop(['Compliance','mailing_address_str_number','zip_code','violator_id'],axis =1)\ny = Compliant_dataset_new_labeled['Compliance']\n# Build a forest and compute the feature importances\nforest = AdaBoostClassifier(n_estimators=100)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(20,15))\nplt.title(\"Feature importances\")\nplt.barh(range(X.shape[1]), importances[indices],\n       color=\"r\", xerr=std[indices], align=\"center\")\n# If you want to define your own labels,\n# change indices to a list of labels on the following line.\nplt.yticks(range(X.shape[1]), X.columns)\nplt.ylim([-1, X.shape[1]])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking best features for our label\nfrom sklearn.feature_selection import mutual_info_classif,mutual_info_regression\nimportances = mutual_info_classif(X,y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10)) \nfeat_import = pd.Series(importances, X.columns)\nfeat_import.plot(kind='barh', color ='teal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances1 =  mutual_info_regression(X,y)\nplt.figure(figsize=(20,10)) \nfeat_import = pd.Series(importances1, X.columns)\nfeat_import.plot(kind='barh', color ='darkorange')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"markdown","source":"Based on feature importances, selecting the required features for modelling","metadata":{}},{"cell_type":"code","source":"features = Compliant_dataset_new_labeled[['violator_name','violation_street_name','mailing_address_str_name','judgment_date','disposition','discount_amount','judgment_amount',\n'balance_due','late_fee','payment_date','payment_status','violation_address']]\nlabel = Compliant_dataset_new_labeled['Compliance']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling of datasets","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\n\nstandardScaler.fit(features)\nscaled_features = standardScaler.transform(features)\nFeatures = pd.DataFrame(scaled_features,columns=features.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting of datasets into train and test sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(Features,label,test_size=0.2,random_state=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modelling","metadata":{}},{"cell_type":"code","source":"# Logistic Regression Model\nlogicRe=LogisticRegression()\nlogicRe.fit(X_train,y_train)\npred1 = logicRe.predict(X_test)\nA1 = round(accuracy_score(y_test, pred1, normalize = True)*100,2)\nprint(\" Logistic Regression accuracy is\",A1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Naive Bayes Model\ngnb=GaussianNB()\ngnb.fit(X_train, y_train)\npred2 =gnb.predict(X_test)\nA2 = round(accuracy_score(y_test, pred2, normalize = True)*100,2)\nprint(\"Naive-Bayes accuracy without noise is\",A2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\nSVC=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nSVC.fit(X_train, y_train)\npred3 =SVC.predict(X_test)\nA3 = round(accuracy_score(y_test, pred3, normalize = True)*100,2)\nprint(\"SVM accuracy without noise is\",A3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_SGD = SGDClassifier(loss='hinge', penalty='l2', max_iter=100)\nclf_SGD.fit(X_train, y_train)\npred4 =clf_SGD.predict(X_test)\nA4 = round(accuracy_score(y_test, pred4, normalize = True)*100,2)\nprint(\"stochastic gradient descent (SGD) Classifier accuracy without noise is\",A4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"sns.reset_defaults()\nfrom sklearn.metrics import plot_confusion_matrix\nclassifiers = [logicRe,gnb, SVC, clf_SGD]\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n\nfor cls, ax in zip(classifiers, axes.flatten()):\n    plot_confusion_matrix(cls, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap='Blues')\n    ax.title.set_text(type(cls).__name__)\nplt.tight_layout()  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification Reports","metadata":{}},{"cell_type":"code","source":"print('Classification Report for Logistic Regression')\nprint(classification_report(y_test,pred1))\nprint('Classification Report for Naive Bayes')\nprint(classification_report(y_test,pred2))\nprint('Classification Report for SVM')\nprint(classification_report(y_test,pred3))\nprint('Classification Report for SGD CLassifer')\nprint(classification_report(y_test,pred4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nk = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_result = cross_val_score(logicRe,X_test,y_test,cv=k) # uses R^2 as score \nprint('CV Scores for Logistic Regression is: ',cv_result)\nprint('CV Average score for Logistic regression is: ',np.sum(cv_result)/k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_result = cross_val_score(gnb,X_test,y_test,cv=k) # uses R^2 as score \nprint('CV Scores for Naive Bayes is: ',cv_result)\nprint('CV Average score for Naive Bayes is: ',np.sum(cv_result)/k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_result = cross_val_score(SVC,X_test,y_test,cv=k) # uses R^2 as score \nprint('CV Scores for SVC is: ',cv_result)\nprint('CV Average score for SVC is: ',np.sum(cv_result)/k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_result = cross_val_score(clf_SGD,X_test,y_test,cv=k) # uses R^2 as score \nprint('CV Scores for SGD Classifier is: ',cv_result)\nprint('CV Average score for SGD Classifier is: ',np.sum(cv_result)/k)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AUC-ROC Curve","metadata":{}},{"cell_type":"code","source":"def roCurves(clfList, X_dev, y_dev):\n    \n    roCurveList = []\n    plt.subplots(1, 1, figsize=(5, 5))\n    styleList = ['solid','dashed','dotted', 'dashed','dashdot']\n    \n    for clf, sty in zip(clfList, styleList):\n        ax = plt.gca()\n        roc = plot_roc_curve(clf, X_dev, y_dev, ax=ax, alpha=0.85, lw=2, linestyle=sty)\n        roCurveList.append(roc)\n    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='dotted')\n    plt.title('ROC')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n\n    return roCurveList","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exps = [logicRe, gnb, SVC, clf_SGD]\n\nroCurves(exps, X_test, y_test)\n\n# Save the figure and show\nplt.tight_layout()\n#plt.savefig('plots/ROCs.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hence to conclude, for this dataset Support Vector Classification is model the team would suggest.\n\n#### Other noted highlights through EDA were:\n#### --If a certain discount is provided in fine, the chances of person being compliant is very high\n#### --If a person is non-compliant, the maximum timeline to expect for him/her to pay is 26 months, after which he can be listed as a permanent defaulter\n#### --Most non compliance occurs in high fines\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}