{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0146a065-3841-bc0e-8b0d-1275ed8380b1"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import roc_auc_score\nfrom datetime import date\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding\nfrom keras.layers import LSTM, SimpleRNN, GRU\nfrom keras.callbacks import Callback, EarlyStopping\n\n# read data\ndata = pd.read_csv('../input/Combined_News_DJIA.csv')\ny = data.pop('Label').as_matrix()\n\n# Use RenMai's fancy joining code\nX_raw = data.filter(regex=(\"Top.*\")).apply(lambda x: ''.join(str(x.values)), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7052814b-133c-db1e-9fdf-eddc7164decd"},"outputs":[],"source":"# Let's remove other characters, stop words and lemmatize and turn into sequences of numbers\n# You might need this, I had them installed already:\n# nltk.download(['stopwords', 'wordnet'])\nstopwords = nltk.corpus.stopwords.words('english')\nlemmatize = nltk.WordNetLemmatizer().lemmatize\ntokenize = nltk.tokenize.treebank.TreebankWordTokenizer().tokenize\nalphabet = '''abcdefghijklmnopqrstuvwxyz0123456789 '''\n# A dict for transforming into sequences:\nchar_to_int = dict((c, i) for i, c in enumerate(alphabet))\nvocab = {}\ncount = 0\ndef normalize(o):\n    global count\n    r = []\n    for t in tokenize(o):\n        if t not in stopwords and len(t) > 2:\n            t = lemmatize(t).lower()\n            t= ''.join(lc for lc in t if lc in alphabet)\n            if t in vocab:\n                n = vocab[t]\n            else:\n                vocab[t] = count\n                n = count\n                count += 1\n            r.append(n)\n    return r\nprint('Normalize words and transform to sequences...')\nX_text = [normalize(x) for x in X_raw]\n\n# This vocabulary size might be a bit large, perhaps one should restrict it to the highest frequency scores\nvocab_size = len(vocab)\nprint('Vocabulary size:' + str(vocab_size))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04433f89-1807-9d8f-64b9-1c8f6688c40d"},"outputs":[],"source":"# Suggested train/test split. Kudos to RenMai, too\nTRAINING_END = date(2014,12,31)\nnum_training = len(data[pd.to_datetime(data[\"Date\"]) <= TRAINING_END])\nX_train = X_text[:num_training]\nX_test = X_text[num_training:]\ny_train = y[:num_training]\ny_test = y[num_training:]\n\n# Pad sequences to maximum length.\nX_train = sequence.pad_sequences(X_train)\nX_test = sequence.pad_sequences(X_test)\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89308501-e32b-6b3b-d1d9-48c8c3b40e76"},"outputs":[],"source":"# Build model\n# High dropout, because this quickly overfits.\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 128, dropout=0.75))\nmodel.add(LSTM(128, dropout_W=0.3, dropout_U=0.3))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5155f84b-589e-b1fb-7ad6-7327d069f070"},"outputs":[],"source":"print('Train...')\nmodel.fit(X_train, y_train, batch_size=32, nb_epoch=11, validation_data=(X_test, y_test), verbose=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d96b8acc-84b2-2801-2a8f-ec4d20d8f12c"},"outputs":[],"source":"# predict and evaluate predictions\npredictions = model.predict_proba(X_test)\nprint('ROC-AUC yields ' + str(roc_auc_score(y_test, predictions)))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}