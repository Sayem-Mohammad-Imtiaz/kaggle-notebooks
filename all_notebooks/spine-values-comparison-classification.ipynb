{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"4ac2710ba28907fdb47d6d24c72a64495dad1f1a","_cell_guid":"efc2b446-05fb-48cc-9c8b-3e8066fa9494"},"source":"# LOWER BACK PAIN DATASET\n\n### Introduction:\n\nBack pain torments many worldwide which many times may have to do with certain spine anomalies.\n\nIt would require a much larger dataset for these models to be considered appropriate, but it is perfect to practice and have some fun with model building. \nAlso, despite the title being \"Lower back pain symptoms\", this is a dataset based on spine spatial layout values and therefore the goal of this project is to classify as correctly as possible wether an individual will be labeled to have a 'Normal' or 'Abnormal' curved spine.\n\nType of Problem: Binary Classification\n\nModels: Logistic Regression, SVC's, NN\n\nFor better accuracy, GridSearch and K-Fold cross-validation were performed.\n\nAny feedback is much appreciated :) thank you!","cell_type":"markdown"},{"metadata":{"_uuid":"fe688a4ed242424a02e13a61a6bfe97a0f0aeb6e","_cell_guid":"79677d95-93e5-4faf-b14b-f20935c7c961"},"source":"## Part 1 - Data Exploration","cell_type":"markdown"},{"metadata":{"_uuid":"dbeb4deba0d09fac3fd7411bf74429b867705b09","_cell_guid":"25c558dc-bcd1-4b3b-a514-fe41de35cd54"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nsns.palplot(sns.color_palette(flatui))","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_uuid":"bd07a65bbcaa59a99b636a5d950e360b2bd662bc","_cell_guid":"8df6bd5c-8a40-4d32-80c4-2b10f6dbc21e"},"outputs":[],"source":"df = pd.read_csv('../input/Dataset_spine.csv', na_filter = True, skip_blank_lines = True)\n\n# Naming Columns\ndf.columns = ['Pelvic Incidence', 'Pelvic Tilt', 'Lumbar Lordosis Angle', 'Sacral Slope', 'Pelvic Radius',\n              'Degree Spondylolisthesis', 'Pelvic Slope', 'Direct Tilt', 'Thoracic Slope', 'Cervical Tilt',\n              'Sacral Angle', 'Scoliosis Slope','Target', '13']\ndf.drop('13', axis = 1, inplace = True)\n","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5ed6c8382362977ff9687924d7ec7062d016479f","_cell_guid":"f7e0eed5-7c36-4d66-9c2d-9ce0198ebb53"},"source":"### Plotting all the features for Normal and Abnormal classified individuals\n\n We can clearly see that there is indeed a different distribution of values between 'Normal' and 'Abnormal' classes.","cell_type":"markdown"},{"metadata":{"_uuid":"a1223817238db6a7d94df50f87cf15136177e407","_cell_guid":"37103c50-11e5-4c4a-bde8-7fffc342acd9"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(15,8), ncols=4, nrows=3)\n\nleft   =  0.125  # the left side of the subplots of the figure\nright  =  0.9    # the right side of the subplots of the figure\nbottom =  0.1    # the bottom of the subplots of the figure\ntop    =  0.9    # the top of the subplots of the figure\nwspace =  .5     # the amount of width reserved for blank space between subplots\nhspace =  1.1    # the amount of height reserved for white space between subplots\n\nplt.subplots_adjust(\n    left    =  left, \n    bottom  =  bottom, \n    right   =  right, \n    top     =  top, \n    wspace  =  wspace, \n    hspace  =  hspace\n)\n\ny_title_margin = 1.2\n\nplt.suptitle(\"Distribution of Values - Normal vs Abnormal\", y = 1.09, fontsize=15)\n\nsns.violinplot(x = 'Target', y  = 'Pelvic Incidence', data = df, ax=ax[0][0], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Pelvic Tilt', data = df, ax=ax[0][1], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Lumbar Lordosis Angle', data = df, ax=ax[0][2], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Sacral Slope', data = df, ax=ax[0][3], palette = flatui)\n\n# second row\nsns.violinplot(x = 'Target', y  = 'Pelvic Radius', data = df, ax=ax[1][0], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Degree Spondylolisthesis', data = df, ax=ax[1][1], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Pelvic Slope', data = df, ax=ax[1][2], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Direct Tilt', data = df, ax=ax[1][3], palette = flatui)\n\n# third row\nsns.violinplot(x = 'Target', y  = 'Thoracic Slope', data = df, ax=ax[2][0], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Cervical Tilt', data = df, ax=ax[2][1], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Sacral Angle', data = df, ax=ax[2][2], palette = flatui)\n\nsns.violinplot(x = 'Target', y  = 'Scoliosis Slope', data = df, ax=ax[2][3], palette = flatui)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"258ca1d6f36695074870a9d3636969eb78332b8e","_cell_guid":"4d24ea81-9561-442f-ae9d-31a22fb81ce5"},"source":"## Preparing Features and Target, train_test_splitting","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"de9ce58306e7628053294132c1d604677b37513e","_cell_guid":"3b67b50e-4374-43c8-8293-14955728bc79"},"outputs":[],"source":"# y\ny = df['Target']\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ny = label.fit_transform(y)\n\n\n# X\ndfx = df.drop(['Target'], axis = 1)\nX = dfx\n\n# Splitting into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c6918dba41848a178ad4469d69dad464fe45d24e","_cell_guid":"f1b59fc4-a509-42fa-9c58-0f789ab19e60"},"source":"## Part 2  -  Feature Selection - Extraction\n\nThis dataset has a lot of features, let's see if we can find the best ones and cut some off.\nWe will make a FeatureUnion, GridSearch-it with a simple logistic regression model and then we will see which features prove more valuable.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"5ba31187ad3be4f79bac9733f562a5ab902ebc36","_cell_guid":"08d0af17-8f45-4a38-a4bd-19f78e14359c"},"outputs":[],"source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# FEATURE SELECTION\nselection = SelectKBest(k=1)\n\n#FEATURE EXTRACTION\npca = PCA(n_components = 2)\nk_pca = KernelPCA(n_components = 2)\n\n# FEATURE UNION (FEATURE SELECTION + FEATURE EXTRACTION)\nestimators = [('sel', selection),\n              ('pca', pca),\n              ('k_pca', k_pca)]  \n\ncombined = FeatureUnion(estimators)\n\nX_features = combined.fit(X, y).transform(X)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b3e427dfc6d12d4119b6332a4745f80de63221ad","_cell_guid":"2191886e-0a89-485a-9316-76cdf67171a2"},"source":"## Part 3 -  Logistic Regression Pipeline","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"b3197d2593efeef1075f6e250c5e4246202c6492","_cell_guid":"7607fe89-2688-4848-93f2-aa21a7c1b74d"},"outputs":[],"source":"log_regression = LogisticRegression()\npipeline = Pipeline([(\"features\", combined), (\"log\", log_regression)])","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"496aa5e0c4b838d85c7158cc6b5a631692342937","_cell_guid":"1674533e-c784-45c9-b9b6-c40f572f93bd"},"source":"### 3.1. GridSearch","cell_type":"markdown"},{"metadata":{"_uuid":"126894f94de70ccf12b82c8525f2457ad83ab2a8","_cell_guid":"0652ea93-532f-4527-aec2-ae6953fea1c5"},"outputs":[],"source":"components = [1,2,3,4,5]\noriginal = [1,2,3,4]\nCs = np.logspace(-4, 4, 3)\nparam_grid = dict(features__pca__n_components=components,\n                  features__k_pca__n_components=components,\n                  features__sel__k=original,\n                  log__C=Cs)\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"247863fefc82a64d8207bbc538ac70e793b648f7","_cell_guid":"7e0c02ac-2501-42a4-8c0f-3b02a38f4e84"},"outputs":[],"source":"print(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d5b53d49616d5673e4848823c511cca7c4bc4757","_cell_guid":"0d1ba0f4-98c1-4841-b300-d39eab97c961"},"source":"## Part 4 - SVC, already with previously obtained new features.\n\nSo in the former model the accuracy obtained was 73.9% and the best feature parameters were 1 kernel-pca component, 5 pca components and 1 selected feature.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"097748124d786b4ddd67c444299176beab626011","_cell_guid":"45ba44e2-41ee-4eb2-8ada-7eb091e0d8b7"},"outputs":[],"source":"# FEATURE SELECTION\nselection = SelectKBest(k=3)\n\n#FEATURE EXTRACTION\npca = PCA(n_components = 5)\nk_pca = KernelPCA(n_components = 1)\n\n# FEATURE UNION (FEATURE SELECTION + FEATURE EXTRACTION)\nestimators = [('sel', selection),\n              ('pca', pca),\n              ('k_pca', k_pca)]  \ncombined = FeatureUnion(estimators)\n\nX_features = combined.fit(X, y).transform(X)","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_uuid":"35cf35b29d6ecd6ba518fe7fe9bd239186f57548","_cell_guid":"2e6da463-8038-4125-a57f-5ec2779981af"},"outputs":[],"source":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf', random_state = 0)\n\npipeline2 = Pipeline([(\"features\", combined), (\"svc\", svc)])","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9b4e21819ea7b5c4dba2f5271b6279cb10ca7fd2","_cell_guid":"76b74bbb-352b-4e6b-8cc4-0f0055d1647b"},"outputs":[],"source":"Cs = np.logspace(-4, 4, 3)\nkernels = ['rbf','poly']\nparam_grid = dict(svc__C=Cs,\n                 svc__kernel=kernels)\n\ngrid_search = GridSearchCV(pipeline2, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"73603a587c1301ef767e0de52c911bb449e43e7a","_cell_guid":"aa4cbce1-f43c-426a-8d14-9e91b49b3602"},"outputs":[],"source":"print(\"Best score: %0.3f\" % grid_search.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = grid_search.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a4efb59b16cd7af823abefd7847b503f2c218f26","_cell_guid":"6efcc4fb-699a-4b45-88b3-95a62cdad1d6"},"source":"So 69.4%... I guess we are better of with Logistic Regression on this one then! Now let's k-fold it.\nRemember, the optimal C value was 10000.","cell_type":"markdown"},{"metadata":{"_uuid":"519dfb33f350a9d0776b2cf13839cd2d97bda64e","_cell_guid":"62f6fd7a-ba9b-44d7-8ab7-7269658d1b3a"},"source":"## Part5 -  K-Folds to evaluate Pipeline","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"9fa73d6f352b8059e86821963bf34fd01974405f","_cell_guid":"294ebbc4-1c07-433a-a1ca-af06ceabfc0a"},"outputs":[],"source":"log_regression = LogisticRegression(C=10000)\nfinal_model = Pipeline([(\"features\", combined), (\"log\", log_regression )])","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"436c1afda47f7b38d0506bedb7e3b29cb8009b52","_cell_guid":"8cca8cea-c71e-4202-bdcd-fe52ba3b05de"},"outputs":[],"source":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = final_model, X = X_train, y = y_train, cv = 10)\n\navg_acc = accuracies.mean()\nstd_acc = accuracies.std()\n\nprint (\"avg_acc: {} \\nstd_acc: {}\".format(avg_acc,std_acc))","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ae79406671afbaf14d7199dc7544529dbc5af6b4","_cell_guid":"cfdfb4ba-52e7-40ac-99c7-e90895deec4b"},"source":"With an average accuracy of 85% this model is not too shaby. ","cell_type":"markdown"},{"metadata":{"_uuid":"43ad73dec01377368145c02ff7582ede31e9b8c2","_cell_guid":"dd246a55-cef7-467a-b566-968c131b4c13"},"source":"## Part 6 - Applying a Neural Network: Will it surpass our expectations?\n\nLet's create a fully connected neural network and see what is the result.\nSince this is a binary classification problem, the last layer will have a sigmoid activation function, so that the value tends to either 'Normal' or 'Abnormal' labels. ","cell_type":"markdown"},{"metadata":{"_uuid":"cd5b7f96b2b08e060a147c75868cfe9b26bcbb9e","_cell_guid":"3b4de287-7f84-4cf0-a7d8-9934cf1b86f0"},"outputs":[],"source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nimport keras.backend as K\n\nK.clear_session()\n\nmodel = Sequential()\nmodel.add(Dense(24, input_dim = 12, activation='relu'))\nmodel.add(Dense(6, activation ='relu'))\nmodel.add(Dense(1, activation ='sigmoid'))\nmodel.compile(SGD(lr=0.5),'binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b324d57c2a39319de79e4062a32123b0e3f95f3f","_cell_guid":"411cd377-69a0-4638-b05d-8db3920965da"},"outputs":[],"source":"model.fit(X_train, y_train, epochs = 1000)","execution_count":null,"cell_type":"code"},{"metadata":{"collapsed":true,"_uuid":"8b05e3bd9d33652090b6b2089dfe42d902b2c871","_cell_guid":"30dbfba7-12f8-4ec8-981f-d50ce9d79353"},"outputs":[],"source":"y_pred = model.predict(X_test)\ny_class_pred = y_pred > 0.5","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7e92666dda646ee9343f2162fc180318b6d9727d","_cell_guid":"c6df9371-1a09-4202-8d37-877967150949"},"outputs":[],"source":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test,y_class_pred)\nacc","execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"70300808d06e096574fa865b7819d441f2e83de0","_cell_guid":"50d53ca9-6f56-462b-b6f1-ff340333ceb9"},"source":"Looks like the model didn't do as well as a simple logistic regression. In the future, it would be fair to search for some better parameters and a better architecture.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"df3fb0d4a3612beb68897c74fd354e3878999447","_cell_guid":"16d24720-47c8-4a25-99e4-5ecc30d63864"},"outputs":[],"source":"","execution_count":null,"cell_type":"code"}]}