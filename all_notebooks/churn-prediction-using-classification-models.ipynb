{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement:\n\nPredict churn from the bank customer dataset.\n\n### Dataset Content:\n\n\n* RowNumber—corresponds to the record (row) number and has no effect on the output.\n* CustomerId—contains random values and has no effect on customer leaving the bank.\n* Surname—the surname of a customer has no impact on their decision to leave the bank.\n* CreditScore—can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n* Geography—a customer’s location can affect their decision to leave the bank.\n* Gender—it’s interesting to explore whether gender plays a role in a customer leaving the bank.\n* Age—this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n* Tenure—refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n* Balance—also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n* NumOfProducts—refers to the number of products that a customer has purchased through the bank.\n* HasCrCard—denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n* IsActiveMember—active customers are less likely to leave the bank.\n* EstimatedSalary—as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n* Exited—whether or not the customer left the bank.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt \nimport seaborn as sns  \nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import  confusion_matrix , plot_roc_curve, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/churn-for-bank-customers/churn.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# no NAN values ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove useless columns and see distributions of the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove useless columns\ndf.drop([\"RowNumber\",\"CustomerId\",\"Surname\"], axis = 1, inplace = True)\n\n# Plot histogram grid\ndf.hist(figsize=(14,14))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at Correlations between the variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate correlations between numeric features\ncorrelations = df.corr()\n\n# sort features in order of their correlation with \"Exited\"\nsort_corr_cols = correlations.Exited.sort_values(ascending=False).keys()\nsort_corr = correlations.loc[sort_corr_cols,sort_corr_cols]\nsort_corr\n\n# Generate a mask for the upper triangle\ncorr_mask = np.zeros_like(df.corr())\ncorr_mask[np.triu_indices_from(corr_mask)] = 1\n\n# Make the figsize 9x9\nplt.figure(figsize=(9,9))\n\n# Plot heatmap of annotated correlations; change background to white\nsns.heatmap(sort_corr*100, \n                cmap='RdBu', \n                annot=True,\n                fmt='.0f',\n                mask=corr_mask,)\n\nplt.title('Correlations by Exited', fontsize=14)\nplt.yticks(rotation=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kdeplot(feature):\n    plt.figure(figsize=(9, 4))\n    plt.title(f\"KDE Plot for {feature}\")\n    ax0 = sns.kdeplot(df[df['Exited'] == 0][feature].dropna(), color= 'dodgerblue', label= 'Exited - 0')\n    ax1 = sns.kdeplot(df[df['Exited'] == 1][feature].dropna(), color= 'orange', label= 'Exited - 1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at Variables with low correlation with the target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"kdeplot('Tenure')\nkdeplot('HasCrCard')\nkdeplot('EstimatedSalary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variables with low correlation do seem so contribute to predicting the final outcome \n(the KDE plots show how different are the graphs of the features with respenct to the target feature. Thus more differences in the graph, the more the variable contributes to the target feature)"},{"metadata":{},"cell_type":"markdown","source":"### Finding and Removing Outliers in numerical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_plot = [\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"]\nfor i in outlier_plot:\n    sns.boxplot(x = df[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seems like CreditScore, Age, NumOfProducts have outliers\noutliers = ['Age','CreditScore','NumOfProducts']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def outlier_removal(df,column):\n    q1 = df[column].quantile(0.25)\n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    fence_low = q1 - 1.5 * iqr\n    fence_high = q3 + 1.5 * iqr\n    cleaned_data = df.loc[(df[column] > fence_low) & (df[column] < fence_high)]\n    return cleaned_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean the dataset by removing outliers\ndf_cleaned = outlier_removal(outlier_removal(outlier_removal(df,'Age'),'CreditScore'),'NumOfProducts')\n\nprint(df.shape)\nprint(df_cleaned.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at Unique data for Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_counts(df):\n    for column in df.columns:\n        print(f'{column} :  {len(df[column].value_counts())}')\nunique_counts(df_cleaned)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender and Geography need to be encoded\ndf_cleaned = pd.get_dummies(df_cleaned, columns = [\"Geography\"])\ndf_cleaned.replace({'Female': 0,'Male': 1},inplace=True)\ndf_cleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_cleaned.drop([\"Exited\"], axis=1)\nY = df_cleaned[\"Exited\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function for confusion matric and classification report \ndef evaluate_model(classifier):\n    cf_matrix = confusion_matrix(y_test, classifier.predict(x_test))\n    sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n                fmt='.2%', cmap='Blues')\n\n    print(classification_report(y_test, classifier.predict(x_test),zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function for cross validation\ndef score_model(classifier):\n    print(f\"Test accurarcy {classifier.score(x_test,y_test)}\")\n    val = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 10)\n    print(f\"cross validation Mean : {val.mean()} and STD of {val.std()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_clsf = LogisticRegression(max_iter=10000)\nlog_clsf.fit(x_train,y_train)\n\nscore_model(log_clsf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(log_clsf)\n\nplot_roc_curve(log_clsf, x_test, y_test)  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clsf = RandomForestClassifier(random_state = 42, max_depth = 10, n_estimators = 1000)\nrf_clsf.fit(x_train, y_train)\n\nscore_model(rf_clsf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(rf_clsf)\n\nplot_roc_curve(rf_clsf, x_test, y_test)  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine (SVM) "},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clsf = SVC()\nsvm_clsf.fit(x_train, y_train)\n\nscore_model(svm_clsf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(svm_clsf)\n\nplot_roc_curve(svm_clsf, x_test, y_test)  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_knn = []\nerror = [] \nfor K in range(20):\n    K = K+1\n    model = KNeighborsClassifier(n_neighbors = K)\n\n    model.fit(x_train, y_train)  \n    pred=model.predict(x_test) \n    error.append(np.mean(pred != y_test))\n    best_knn.insert(K, model.score(x_test,y_test))\n\n# Get the best fitting number of neighbours \nfor i,v in enumerate(best_knn):\n    if v == max(best_knn):\n        print(f'best n_neighbours = {i}')\n        \ncurve = pd.Series(error) #elbow curve \ncurve.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_clsf = KNeighborsClassifier(n_neighbors=15)\nknn_clsf.fit(x_train, y_train)\n\nscore_model(knn_clsf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(knn_clsf)\n\nplot_roc_curve(knn_clsf, x_test, y_test)  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_classfication = VotingClassifier(estimators = [('lg', log_clsf), ('rfg', rf_clsf), ('svc', svm_clsf), ('knn', knn_clsf)])\nvoting_classfication.fit(x_train, y_train)\n\nprint(\"Test accuracy: \", voting_classfication.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(voting_classfication)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary:\n\nThe best model amonst the ones implemented is Random Forests with an accuracy of 86.8% \nas for the other models, even though their accuracy hovers arround 80%, their AUC is pretty bad and thus shouldn't be used for a real world scenario.\n\n\n\nDo post a comment if you have any suggessions !"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}