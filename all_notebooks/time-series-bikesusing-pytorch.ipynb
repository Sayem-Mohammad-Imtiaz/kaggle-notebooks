{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is sourced from ch4 Deep learning with Pytorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '../input/bike-sharing-dataset'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikes_numpy = np.loadtxt(\"../input/bike-sharing-dataset/hour.csv\", \n                         dtype=np.float32,\n                         delimiter=\",\",\n                         skiprows=1,\n                         converters={1: lambda x:float(x[8:10])})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikes = torch.from_numpy(bikes_numpy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikes_df = pd.read_csv('../input/bike-sharing-dataset/hour.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikes_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a time series dataset such as this one, rows represent successive time-points: there is\na dimension along which they are ordered. Sure, we could treat each row as independent and try to predict the number of circulating bikes based on, say, a particular time\nof day regardless of what happened earlier. However, the existence of an ordering\ngives us the opportunity to exploit causal relationships across time. For instance, it\nallows us to predict bike rides at one time based on the fact that it was raining at an\nearlier time. For the time being, we’re going to focus on learning how to turn our\nbike-sharing dataset into something that our neural network will be able to ingest in\nfixed-size chunks."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(bikes_df[:-3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shaping the data by time period\n\n"},{"metadata":{},"cell_type":"markdown","source":"We might want to break up the two-year dataset into wider observation periods, like\ndays. This way we’ll have N (for number of samples) collections of C sequences of length\nL. In other words, our time series dataset would be a tensor of dimension 3 and shape\nN × C × L. The C would remain our 17 channels, while L would be 24: 1 per hour of\nthe day. There’s no particular reason why we must use chunks of 24 hours, though the\ngeneral daily rhythm is likely to give us patterns we can exploit for predictions. We\ncould also use 7 × 24 = 168 hour blocks to chunk by week instead, if we desired. All of\nthis depends, naturally, on our dataset having the right size—the number of rows must\nbe a multiple of 24 or 168. Also, for this to make sense, we cannot have gaps in the\ntime series"},{"metadata":{},"cell_type":"markdown","source":" Let’s go back to our bike-sharing dataset. The first column is the index (the global\nordering of the data), the second is the date, and the sixth is the time of day. We have\neverything we need to create a dataset of daily sequences of ride counts and other\nexogenous variables. Our dataset is already sorted, but if it were not, we could use\ntorch.sort on it to order it appropriately"},{"metadata":{},"cell_type":"markdown","source":"note : the version of file originally used was  hour fixed version not the one we would be using "},{"metadata":{"trusted":true},"cell_type":"code","source":"bikes.shape, bikes.stride()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"thats 17379 hours and 17 columns.\n\nchoosing till bikes[:-3] since 17376 is devided by 24"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_bikes = bikes[:-3].view(-1, 24, bikes[:-3].shape[1])\ndaily_bikes.shape, daily_bikes.stride()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"calling view on a tensor returns a new tensor that changes the number of dimensions and the striding information, without\nchanging the storage. This means we can rearrange our tensor at basically zero cost,\nbecause no data will be copied. Our call to view requires us to provide the new shape\nfor the returned tensor. We use -1 as a placeholder for “however many indexes are\nleft, given the other dimensions and the original number of elements."},{"metadata":{},"cell_type":"markdown","source":"For daily_bikes, the stride is telling us that advancing by 1 along the hour dimension (the second dimension) requires us to advance by 17 places in the storage (or\none set of columns); whereas advancing along the day dimension (the first dimension) requires us to advance by a number of elements equal to the length of a row in\nthe storage times 24 (here, 408, which is 17 × 24).\n We see that the rightmost dimension is the number of columns in the original\ndataset. Then, in the middle dimension, we have time, split into chunks of 24 sequential hours. In other words, we now have N sequences of L hours in a day, for C channels. To get to our desired N × C × L ordering, we need to transpose the tensor:"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_bikes = daily_bikes.transpose(1,2)\ndaily_bikes.shape, daily_bikes.stride()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lets do something about the weather situation,if we turn it into categorical then we can turn it into a one hot encoded vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_day = bikes[:24].long()\nweather_onehot = torch.zeros(first_day.shape[0], 4)\nfirst_day[:, 9]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we scatter ones into our matrix according to the corresponding level at each row, we need to use unsqueeze for this"},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_onehot.scatter_(\n    dim=1,\n    index=first_day[:,9].unsqueeze(1).long() -1,\n    value=1.0\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our day started with weather “1” and ended with “2,” so that seems right.\nLast, we concatenate our matrix to our original dataset using the cat function.\nLet’s look at the first of our results"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cat((bikes[:24], weather_onehot), 1)[:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we prescribed our original bikes dataset and our one-hot-encoded “weather situation” matrix to be concatenated along the column dimension (that is, 1). In other\nwords, the columns of the two datasets are stacked together; or, equivalently, the new\none-hot-encoded columns are appended to the original dataset. For cat to succeed, it\nis required that the tensors have the same size along the other dimensions—the row\ndimension, in this case. Note that our new last four columns are 1, 0, 0, 0, exactly\nas we would expect with a weather value of 1.\nWe could have done the same with the reshaped daily_bikes tensor. Remember\nthat it is shaped (B, C, L), where L = 24. We first create the zero tensor, with the same\nB and L, but with the number of additional columns as C:"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])\ndaily_weather_onehot.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we scatter the one-hot encoding into the tensor in the C dimension. Since this\noperation is performed in place, only the content of the tensor will change:"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_weather_onehot.scatter_(\n    1, daily_bikes[:,9,:].long().unsqueeze(1) -1 , 1.0\n)\ndaily_weather_onehot.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"concatenate along C dimension"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We mentioned earlier that this is not the only way to treat our “weather situation” variable. Indeed, its labels have an ordinal relationship, so we could pretend they are special values of a continuous variable. We could just transform the variable so that it runs\nfrom 0.0 to 1.0:"},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_bikes[:,9,:] = (daily_bikes[:,9,:] - 1.0)/3.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we mentioned in the previous section, rescaling variables to the [0.0, 1.0] interval\nor the [-1.0, 1.0] interval is something we’ll want to do for all quantitative variables,\nlike temperature (column 10 in our dataset). We’ll see why later; for now, let’s just say\nthat this is beneficial to the training process.\n\nThere are multiple possibilities for rescaling variables. We can either map their\nrange to [0.0, 1.0]"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = daily_bikes[:, 10, :]\ntemp_min = torch.min(temp)\ntemp_max = torch.max(temp)\ndaily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - temp_min)\n/ (temp_max - temp_min))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"or subtract the mean and divide by the standard deviation:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = daily_bikes[:, 10, :]\ndaily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - torch.mean(temp))\n/ torch.std(temp))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the latter case, our variable will have 0 mean and unitary standard deviation. If our\nvariable were drawn from a Gaussian distribution, 68% of the samples would sit in the\n[-1.0, 1.0] interval.\n\nGreat: we’ve built another nice dataset, and we’ve seen how to deal with time series\ndata. For this tour d’horizon, it’s important only that we got an idea of how a time\nseries is laid out and how we can wrangle the data in a form that a network will digest"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}