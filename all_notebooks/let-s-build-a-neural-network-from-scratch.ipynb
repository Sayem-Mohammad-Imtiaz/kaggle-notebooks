{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Constructing a Neural Network from Scratch "},{"metadata":{},"cell_type":"markdown","source":"### A Sample notebook which gives a method to construct a neural network from scratch, that means we are not using any of the predefined libraries like tensorflow, keras etc.\n\nFor this purpose I'm using real life Dataset.\n\nHope you like the work! ðŸ˜Š"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libs"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/surgical-dataset-binary-classification/Surgical-deepnet.csv')\ndataset = dataset.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### no null value found ðŸ‘€"},{"metadata":{},"cell_type":"markdown","source":"## Detiled info of dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling\npandas_profiling.ProfileReport(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## It's just a demonstration for building a neural network, so we'll not look for EDA in depth.\n"},{"metadata":{},"cell_type":"markdown","source":"# Let's get started with our network"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1:].values\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaled data will help in training of neural network, i generally speeds up the gradient descent!!"},{"metadata":{},"cell_type":"markdown","source":"# 1. Intializing Parameters"},{"metadata":{},"cell_type":"markdown","source":"## He Initialization\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_params(layer_dims):\n    \n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    params = {}\n    np.random.seed(42)\n    L = len(layer_dims)-1\n    for l in range (L):\n        params[\"W\"+str(l+1)] = np.random.randn(layer_dims[l+1],layer_dims[l])*(2/layer_dims[l])**0.5\n        params[\"b\"+str(l+1)] = np.zeros((layer_dims[l+1],1))\n    \n    return params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Forward Propagation"},{"metadata":{},"cell_type":"markdown","source":"We are using a model with last layer having activation sigmoid and all other layers with ReLu as activation function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Linear_forward(A,W,b):\n    Z = np.dot(W,A)+b\n    cache = (A,W,b)\n    return Z,cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Activation_forward(A,W,b,Activation):\n    if Activation == \"relu\":\n        Z,Linear_cache = Linear_forward(A,W,b)\n        A  = np.maximum(0,Z)\n        A, activation_cache = A,Z\n    elif Activation == 'sigmoid':\n        Z,Linear_cache = Linear_forward(A,W,b)\n        A,activation_cache = (1/(1+np.exp(-Z)),Z)\n    cache= (Linear_cache,activation_cache)\n    return A,cache","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_prop(X,params):\n    A=X\n    caches = []\n    L = len(params)//2\n    for l in range (L-1):\n        A_prev = A\n        A, cache = Activation_forward(A_prev,params[\"W\"+str(l+1)],params[\"b\"+str(l+1)],\"relu\")\n        caches.append(cache)\n    AL,cache = Activation_forward(A,params[\"W\"+str(L)],params[\"b\"+str(L)],\"sigmoid\")\n    caches.append(cache)\n    return AL,caches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cost(AL,Y) :\n    m = Y.shape[1]\n    cost = -np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))/m\n    return np.squeeze(cost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Backward Propagation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_backward(dZ,cache):\n    A_prev,W,b = cache\n    m =A_prev.shape[1]\n    dW = (np.dot(dZ,A_prev.T)/m) \n    db = np.sum(dZ,axis=1,keepdims=True)/m\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev , dW , db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def activation_backward(dA,cache,activation):\n    linear_cache,activation_cache = cache\n    Z=activation_cache\n    if activation == \"relu\":\n        dZ = (Z>0).astype(int)\n        dZ = dA*dZ\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = np.multiply(dA,(1/(1+np.exp(-Z)))*(1-(1/(1+np.exp(-Z)))))\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward_prop(AL,Y,caches):\n    \n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    grads = {}\n    \n    grads[\"dA\"+str(L)] = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n    \n    current_cache = caches[L-1]\n    \n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] =  activation_backward(grads[\"dA\"+str(L)], current_cache, 'sigmoid')\n    for l in reversed (range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp =  activation_backward(grads['dA'+str(l+1)], current_cache, 'relu')\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n        \n    return grads\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Making Mini Batches"},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_mini_batches(X, Y, mini_batch_size ):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(42)\n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        \n        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n     \n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        \n        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]\n        \n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Gradient descent with ADAM optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_adam(parameters) :\n    L = len(parameters) // 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n    ### END CODE HERE ###\n    \n    return v, s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    L = len(parameters) // 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n       \n        v[\"dW\" + str(l+1)] = beta1*v[\"dW\"+str(l+1)]+(1-beta1)*grads[\"dW\"+str(l+1)]\n        v[\"db\" + str(l+1)] = beta1*v[\"db\"+str(l+1)]+(1-beta1)*grads[\"db\"+str(l+1)]\n       \n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        \n        v_corrected[\"dW\" + str(l+1)] = v[\"dW\"+str(l+1)]/(1-beta1**t)\n        v_corrected[\"db\" + str(l+1)] = v[\"db\"+str(l+1)]/(1-beta1**t)\n        \n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l+1)] = beta2*s[\"dW\"+str(l+1)]+(1-beta2)*grads[\"dW\"+str(l+1)]**2\n        s[\"db\" + str(l+1)] = beta2*s[\"db\"+str(l+1)]+(1-beta2)*grads[\"db\"+str(l+1)]**2\n\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n     \n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\"+str(l+1)]/(1-beta2**t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\"+str(l+1)]/(1-beta2**t)\n        \n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n\n        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*(v_corrected[\"dW\"+str(l+1)]/((s_corrected[\"dW\"+str(l+1)]**0.5)+epsilon))\n        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*(v_corrected[\"db\"+str(l+1)]/((s_corrected[\"db\"+str(l+1)]**0.5)+epsilon))\n   \n\n    return parameters, v, s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Model code (Combining Every thing) "},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 16, beta = 0.9,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 700, print_cost = True):\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0  \n    m =X.shape[1]\n    \n    params = initialize_params(layers_dims)\n    \n    v, s = initialize_adam(params)\n    \n    for i in range(num_epochs):\n        minibatches = random_mini_batches(X, Y, mini_batch_size)\n        cost_total = 0\n        for minibatch in minibatches:\n            \n            (minibatch_X,minibatch_Y) = minibatch\n            AL, caches = forward_prop(minibatch_X, params)\n            \n            cost_total += cost(AL, minibatch_Y)\n            \n            grads = backward_prop(AL, minibatch_Y, caches)\n            t=t+1\n            params, v, s = update_parameters_with_adam(params, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n            \n        cost_avg = cost_total / m\n            \n        if print_cost and i % 50 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n            costs.append(cost_avg)\n                \n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X,y,parameters):\n    \n    pre, cache = forward_prop(X,parameters)\n    predictions = (pre>0.5).astype(int)\n    from sklearn.metrics import accuracy_score\n    \n    print(accuracy_score(predictions[0],y[0]))\n    return predictions\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Training our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"layers_dims = [X.shape[1],4,4,4,1]\nparams = model(X_train.T, y_train.T ,layers_dims, optimizer = \"adam\",learning_rate=7e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Making Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Accuracy:\")\npredictions_train = predict(X_train.T, y_train.T, params)\nprint(\"Test Accuracy:\")\npredictions_test = predict(X_test.T, y_test.T, params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can further improve performance by Data cleaning and doing Hyperparameter tuning for parameters like Learning rate, Layer_dims, no. of Hidden units, mini_batch_size etc."},{"metadata":{},"cell_type":"markdown","source":"### If there's any bug I left in this so much messy code, please inform in comments, still a beginner ðŸ˜… "},{"metadata":{},"cell_type":"markdown","source":"# Don't forget to upvote ðŸ˜‰"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}