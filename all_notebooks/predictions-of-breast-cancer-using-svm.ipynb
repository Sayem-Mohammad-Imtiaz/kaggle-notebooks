{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://mlfjqdsf5ptg.i.optimole.com/iQrIoNc-LQvF_N5U/w:800/h:400/q:69/https://nationaldaycalendar.com/wp-content/uploads/2014/10/Breast-Cancer-Awareness-Month-October-1.jpg)","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents"},{"metadata":{},"cell_type":"markdown","source":"  \n- Table of Contents\n\n- First look at the dataset\n\n- EDA\n\n   - Checking for Missing Values\n   \n   - Basic Statistical Details\n  \n   - Correlation Heatmap\n      - Highly correlated pairs\n      - Inverse correlated pairs\n      - Low correlated pairs\n      \n           \n       \n- Data Visualization\n\n    - Feature Pairs\n    - Scatter Plot\n    - Count Plot\n    - Histogram \n    - Joint Plot\n    \n    \n    \n- Pre-Modeling Tasks\n\n   - Separating the independant and the dependant variable\n   - Splitting the dataset \n   - Feature Scaling\n   \n   \n   \n- Modeling\n\n   - Logistic Regression\n   - Gradient Boosting Classifier\n   - Random Forest Classifier\n   - Decision Tree Classifier\n   - KNeighbors Classifier\n   - XGB Classifier\n   - Suport Vector Machine\n   \n   \n- Evaluation and comparision of all the models\n\n  - Classification Accuracy\n\n  - Confusion matrix\n\n  - Precision\n\n  - Recall\n\n  - classification_report\n\n  - ROC AUC Score\n\n  - Area under curve (AUC)\n   \n    \n- Resources"},{"metadata":{},"cell_type":"markdown","source":"# Loading the libraries and the dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Pre-Modeling Tasks\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Modeling\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n\n# Evaluation and comparision of all the models\n\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, precision_recall_fscore_support\nfrom sklearn.metrics import roc_auc_score,auc,f1_score\nfrom sklearn.metrics import precision_recall_curve,roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Look at the dataset"},{"metadata":{},"cell_type":"markdown","source":"Attribute Information:\n\n- 1) ID number\n\n- 2) Diagnosis (M = malignant, B = benign)\n  \n\nTen real-valued features are computed for each cell nucleus:\n\n- a) radius (mean of distances from center to points on the perimeter)\n- b) texture (standard deviation of gray-scale values)\n- c) perimeter\n- d) area\n- e) smoothness (local variation in radius lengths)\n- f) compactness (perimeter^2 / area - 1.0)\n- g) concavity (severity of concave portions of the contour)\n- h) concave points (number of concave portions of the contour)\n- i) symmetry\n- j) fractal dimension (\"coastline approximation\" - 1)"},{"metadata":{},"cell_type":"markdown","source":"****Check the target variable:****\n\n- Malignant = 1 (indicates prescence of cancer cells)\n\n- Benign = 0 (indicates abscence)"},{"metadata":{},"cell_type":"markdown","source":"****What is the difference between Malignant and Benign ?****"},{"metadata":{},"cell_type":"markdown","source":"![Differences Between a Malignant and Benign Tumor](https://gotalktogetherdotcom.files.wordpress.com/2016/05/cancerbenignmalig1.jpg?w=550)\n\nLoving Biology - WordPress.com"},{"metadata":{},"cell_type":"markdown","source":"How many Benign and Malignant do we have in our dataset?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, we have 212 - Malignant, and 357 - Benign"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Statistical Details"},{"metadata":{"trusted":true},"cell_type":"code","source":"# describing the dataset\n\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for missing values"},{"metadata":{},"cell_type":"markdown","source":"Machine Learning algorithm generally, cannot work with missing values, so before we launch a machine learning algorithm we must cleaning the dataset, we will remove the features that doesn't affect the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting the id and Unnamed column\n\ndf= df.drop(['Unnamed: 32','id'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for the correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(),annot=True)\nplt.ioff()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Highly correlated pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npalette ={'B' : 'lightblue', 'M' : 'magenta'}\n\n\nfig = plt.figure(figsize=(12,12))\ndef plot_scatter(a,b,k):\n    plt.subplot(k)\n    sns.scatterplot(x = df[a], y = df[b], hue = \"diagnosis\",\n                    data = df, palette = palette)\n    plt.title(a + ' vs ' + b,fontsize=15)\n    \nplot_scatter('texture_mean','texture_worst',221) \nplot_scatter('area_mean','radius_worst',222) \nplot_scatter('perimeter_mean','radius_worst',223)  \nplot_scatter('perimeter_mean','radius_worst',224) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Inverse correlated pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,12))\n\n  \nplot_scatter('smoothness_mean','texture_mean',221) \nplot_scatter('texture_mean','symmetry_se',222) \nplot_scatter('fractal_dimension_worst','texture_mean',223) \nplot_scatter('texture_mean','symmetry_mean',224)\n  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Low correlated pairs"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,12))\nplot_scatter('area_mean','fractal_dimension_mean',221)\nplot_scatter('radius_mean','fractal_dimension_mean',222)\nplot_scatter('area_mean','smoothness_se',223)\nplot_scatter('smoothness_se','perimeter_mean',224)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"## PairPlot"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\n\nrcParams['figure.figsize'] = 8,5\n\ncols = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean','diagnosis']\n\nsns_plot = sns.pairplot(data=df[cols],hue='diagnosis', palette='bwr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ScatterPlot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# area_mean vs smoothness_mean\n\nsns.scatterplot(x= 'area_mean', y= 'smoothness_mean', hue= 'diagnosis', data=df, palette='CMRmap')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# texture mean vs radius_mean\n\nsize = len(df['texture_mean'])\n\narea = np.pi * (15 * np.random.rand( size ))**2\ncolors = np.random.rand( size )\n\nplt.xlabel(\"texture mean\")\nplt.ylabel(\"radius mean\") \nplt.scatter(df['texture_mean'], df['radius_mean'], s=area, c= colors, alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target variable\n\nsns.countplot(df['diagnosis'],palette='Paired')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = plt.hist(df[df[\"diagnosis\"] == \"M\"].radius_mean,bins=30,fc = (1,0,0,0.5),label = \"Malignant\")\nb = plt.hist(df[df[\"diagnosis\"] == \"B\"].radius_mean,bins=30, fc = (1,0,0.5), label= \"Bening\")\n\nplt.legend()\nplt.xlabel (\"Radius Mean Values\")\nplt.ylabel (\"Frequency\")\nplt.title(\"Histogram of Radius Mean for Bening and Malignant Tumors\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## JointPlot"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data= df, x='area_mean', y='smoothness_mean', size=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding categorical data"},{"metadata":{},"cell_type":"markdown","source":"As we know machine learning algorithms can only read numerical values. It is essential to encoding categorical features into numerical values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoder\n\nLEncoder = LabelEncoder()\n\ndf['diagnosis'] = LEncoder.fit_transform(df['diagnosis'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have encoded malignan as 1 and benign as 0"},{"metadata":{},"cell_type":"markdown","source":"# Pre-Modeling Tasks"},{"metadata":{},"cell_type":"markdown","source":"## Separating the independant and the dependant variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('diagnosis',axis=1).values\ny = df['diagnosis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Splitting the dataset"},{"metadata":{},"cell_type":"markdown","source":"In Machine learning we must split the dataset into training and testing data:\n\n - the training set called also learning set that we will use to train our model, it has the big part.\n\n - the testing set: is used to evaluate the performance of the model after hypermarameter tuning, It's also useful to get an idea of how different models (SVMs, Neural Networks,    Random forests...) perform against each other.\n\n- So creating the test set is easy, we just select a few rondom rows, in general we give it 10%  or 20%.\n\n- SKit_Learn provides a function of splitting the dataset into multiples subsets. \n\n\n- train_test_split(), is the simplest way wich the same as the function: split_train_test(), the method accepts lists, numpy arrays, scipy sparse matrices or pandas dataframes.\n\n  We will also identify some parameters, like the random_state that allows you to set the random generator seed.\n\n- The ideal split is said to be 80:20 for training and testing. You may need to adjust it depending on the size of the dataset and parameter complexity."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\n\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"Feature scaling is a method used to standardize the range of independent variables or features of data. Scaling the data is very important to boost the score.\n\nFeature Scaling, is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range.\n\n\nThere are two ways for scaling the dataset:\n \n -Standardization\n \n -Min_Max Scaling\n \n- Standardization : it substract the mean value( so standardized values always have a zero mean), and then it divides by the standard deviation, this method doesn't have a         specific range from 0 to 1, that may cause a problem for many algorithms like Neural Network often expect an input value ranging from 0 to 1. \n \n  Sckit-Learn provides a transformer caller **StandardScaler**. The idea behind **StandardScaler** is that it will transform your data such that its distribution will have a       mean value 0 and standard deviation of 1.\n \n\n- Min_Max : called also Normalization, is the simplest way to scaling data, values are shifted and rescaled again so that the end up ranging from 0 to 1. we do this by             substraction the min value and dividing by the Max minus the Min.\n\n  Sckit-learn provides a transformer callec **MinMaxScaler**.  It have a hyperparameter called \"Feature Range\" to specify the range that you want."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\n\nX_train = sc.fit_transform(x_train)\nX_test= sc.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"- In this part we'll try differents models of Machine learning: Logistic Regression, Gradient Boosting Classifier,Random Forest,XGB Classifier,\n\n\n  Support Vector Machine, Decision  tree and KNeighbors Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\n\nlogreg= LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\ny_pred_logreg = logreg.predict(X_test)\n\n\n# Gradient Boosting Classifier\n\n\nGB = GradientBoostingClassifier()\n\nGB.fit(X_train, y_train)\n\ny_pred_GB = GB.predict(X_test)\n\n\n\n# Random Forest Classifier\n\nrf = RandomForestClassifier()\n\nrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\n\n\n# Decision Tree Classifier\n\ndt = DecisionTreeClassifier()\n\ndt.fit(X_train, y_train)\n\ny_pred_dt = dt.predict(X_test)\n\n\n# KNeighbors Classifier\n\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.fit(X_train, y_train)\n\ny_pred_knn = knn.predict(X_test)\n\n\n# XGB Classifier\n\nXGB = XGBClassifier() \n\nXGB.fit(X_train, y_train)\n\ny_pred_XGB = XGB.predict(X_test)\n\n\n\n# Support Vector classifier\n\nsvc = SVC(probability=True)\n\nsvc.fit(X_train,y_train)\n\ny_pred_svc = svc.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape,X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation and comparison of all the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\n\nZ = [SVC() , DecisionTreeClassifier() , LogisticRegression() , KNeighborsClassifier() ,XGBClassifier(),\n    RandomForestClassifier() , GradientBoostingClassifier()]\n\n\nX = [\"SVC\" , \"DecisionTreeClassifier\" , \"LogisticRegression\" , \"KNeighborsClassifier\" ,\n    \"RandomForestClassifier\" , \"GradientBoostingClassifier\", \"XGB\"]\n\nfor i in range(0,len(Z)):\n    model = Z[i]\n    model.fit( X_train , y_train )\n    pred = model.predict(X_test)\n    models.append(accuracy_score(pred , y_test))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = { \"Accuracy\" : models , \"Algorithm\" : X }\ndata_frame = pd.DataFrame(d)\ndata_frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(data_frame['Accuracy'],data_frame['Algorithm'],palette= \"husl\").set_title('Accuracy of all Algorithms')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, from the above table and graph, that SVC classifier works best for this dataset"},{"metadata":{},"cell_type":"markdown","source":"# Evaluating The Performance of the model"},{"metadata":{},"cell_type":"markdown","source":"Evaluating the machine learning model is a crucial part in any data science project. There are many metrics that helps us to evaluate our model accuracy.\n\n- Classification Accuracy\n\n- Confusion matrix\n\n- Precision\n\n- Recall\n\n- classification_report\n\n- ROC AUC Score\n\n- Area under curve (AUC)"},{"metadata":{},"cell_type":"markdown","source":"Now, let's see the performance metrics of svc classifier"},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix"},{"metadata":{},"cell_type":"markdown","source":"- A confusion matrix is a table that can be used to measure the performance of an machine learning algorithm, usually a supervised learning one. Each row of the confusion matrix represents the instances of an actual class and each column represents the instances of a predicted class\n\n\nIn a binary classifier, the \"**true**\" class is typically labeled with 1 and the \"**false**\" class is labeled with 0.\n\n  - True Positive: A positive class observation (1) is correctly classified as positive by the model.\n\n  - False Positive: A negative class observation (0) is incorrectly classified as positive.\n\n  - True Negative: A negative class observation is correctly classified as negative.\n\n  - False Negative: A positive class observation is incorrectly classified as negative."},{"metadata":{},"cell_type":"markdown","source":"Let’s visualize the confusion matrix, to see how accurate are the results we obtained."},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = np.array(confusion_matrix(y_test, y_pred_svc, labels=[1,0]))\n\nconfusion_mat= pd.DataFrame(cm, index = ['cancer', 'healthy'],\n                           columns =['predicted_cancer','predicted_healthy'])\n\nconfusion_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(cm,annot=True,fmt='g',cmap='Set3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As we can see from the table above:\n\n   - **True Positive(TP)** : Values that the model predicted as yes(Healthy), and is actually yes(Healthy).\n   - **True Negative(TN)** : Values that the model predicted as not(Cancer), and is actually no(Cancer).\n   - **False Positive(FP)**: Values that the model predicted as yes(Healthy), but actually no(Cancer).\n   - **False Negative(FN)**: Values that the model predicted as no (Cancer), but actually yes(Healthy).\n"},{"metadata":{},"cell_type":"markdown","source":"For this dataset, whenever the model is predicting something as yes, it indicates Absence of cancer cells (Healthy) and for cases when the model predicting no; it indicates existence of cancer cells(Cancer).\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Accuracy_Score"},{"metadata":{},"cell_type":"markdown","source":"- **Accuracy_Score** is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations\n\n\n(TP + TN)/total = 0.98245614"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, y_pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Precision "},{"metadata":{},"cell_type":"markdown","source":"- **Precision** is the ratio of correctly predicted positive observations to the total predicted positive observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(precision_score(y_test, y_pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recall"},{"metadata":{},"cell_type":"markdown","source":"- **Recall** also called Sensitivity, is the ratio of positive instances that are correctly detected by the classifier to the all observations in actual class"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(recall_score(y_test, y_pred_svc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification Report\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred_svc))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- True Positive(TP) : 71\n    \n- True Negative(TN) : 41\n    \n- False Positive(FP): 2\n    \n- False Negative(FN): 0"},{"metadata":{},"cell_type":"markdown","source":"\n**True Positive Rate/Recall/Sensitivity: How often the model predicts yes(Healthy) when it's actually yes(Healthy)?**\n\n- **True Positive Rate(TPR)** = TP/TP+FP = 71/(871+2) = 0.97\n\n\n**False Positive Rate: How often the model predicts yes(Healthy) when it's actually no(Cancer)?**\n\n- **False Positive Rate(FPR)** = FP/FP+TN = 2/2+41 = 0.04"},{"metadata":{},"cell_type":"markdown","source":"## The ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#plt.style.use('seaborn-pastel')\n\ny_score = svc.decision_function(X_test)\n\nFPR, TPR, _ = roc_curve(y_test, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('Receiver operating characteristic example', fontsize= 18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). As we notice the **svc** Classifier give a curve closer\n\n  to the top-left corner so it indicate a better performance. "},{"metadata":{},"cell_type":"markdown","source":"## Area Under Curve"},{"metadata":{},"cell_type":"markdown","source":"Area Under Curve is a common way to compare classifiers. A perfect classifier will have ROC AUC equal to 1\n\nSckit-Learn provides a function to compute the ROC AUC."},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_test, y_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- [Understanding a Classification Report For Your Machine Learning Model](https://medium.com/@kohlishivam5522/understanding-a-classification-report-for-your-machine-learning-model-88815e2ce397)\n\n- [True Positive Rate](https://www.sciencedirect.com/topics/computer-science/true-positive-rate)\n\n- [How to plot an ROC curve in Python](https://www.kite.com/python/answers/how-to-plot-an-roc-curve-in-python)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}