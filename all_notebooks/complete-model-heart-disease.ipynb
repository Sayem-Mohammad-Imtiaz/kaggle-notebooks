{"cells":[{"metadata":{"id":"D3idbSI4HKu6","colab_type":"text"},"cell_type":"markdown","source":"# Heart Disease Prediction Model\n\nIn this machine learning project, I have collected the dataset from Kaggle (https://www.kaggle.com/ronitf/heart-disease-uci) and I will be using Machine Learning to make predictions on whether a person is suffering from Heart Disease or not."},{"metadata":{"id":"_rVj3XWhHKvB","colab_type":"text"},"cell_type":"markdown","source":"### Import libraries\n\nLet's first import all the necessary libraries. I'll use `numpy` and `pandas` to start with. For visualization, I will use `pyplot` subpackage of `matplotlib`, use `rcParams` to add styling to the plots and `rainbow` for colors. For implementing Machine Learning models and processing of data, I will use the `sklearn` library.\n\n\n### Data Overview\nTaken data from facebook's recruting challenge on kaggle \ndata contains two columns source and destination eac edge in graph \n    - Data columns (total 13 columns):  \n    \n  \n### Performance metric for supervised learning:  \n- Both precision and recall is important \n- Confusion matrix\n\n"},{"metadata":{"id":"sqgo2CAGHKvE","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"id":"ZDk4u06cHKvc","colab_type":"text"},"cell_type":"markdown","source":"# Import dataset\n\nNow that we have all the libraries we will need, I can import the dataset and take a look at it. The dataset is stored in the file `dataset.csv`. I'll use the pandas `read_csv` method to read the dataset."},{"metadata":{"id":"rvZhtHIxHKvg","colab_type":"code","outputId":"d8605842-088a-4437-98c2-4d49bbc2f5a6","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"dataset=pd.read_csv('../input/heart-disease-uci/heart.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"2h47vSZxHKvm","colab_type":"text"},"cell_type":"markdown","source":"The dataset is now loaded into the variable `dataset`. I'll just take a glimpse of the data using the `desribe()` and `info()` methods before I actually start processing and visualizing it."},{"metadata":{"id":"xiWi98mQHKvo","colab_type":"code","outputId":"8edc29b8-b7d4-415c-8474-dab86b10eb2d","colab":{"base_uri":"https://localhost:8080/","height":340},"trusted":true},"cell_type":"code","source":"dataset.info()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Dp7vUmooHKvw","colab_type":"text"},"cell_type":"markdown","source":"Looks like the dataset has a total of 303 rows and there are no missing values. There are a total of `13 features` along with one target value which we wish to find."},{"metadata":{"id":"CEOWSAK_HKvy","colab_type":"code","outputId":"7cc15b31-3d65-4007-8135-a6b20564aa49","colab":{"base_uri":"https://localhost:8080/","height":317},"trusted":true},"cell_type":"code","source":"dataset.describe()\n#print(dataset['sex'].head())","execution_count":null,"outputs":[]},{"metadata":{"id":"fmynkp9qHKv4","colab_type":"text"},"cell_type":"markdown","source":"The scale of each feature column is different and quite varied as well. While the maximum for `age` reaches 77, the maximum of `chol` (serum cholestoral) is 564."},{"metadata":{"id":"PKwAm0LDHKv6","colab_type":"text"},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nNow, we can use visualizations to better understand our data and then look at any processing we might want to do."},{"metadata":{"id":"x0xMwkd2LYVE","colab_type":"code","outputId":"32463975-439d-4dd6-a94d-59eb515ea6ca","colab":{"base_uri":"https://localhost:8080/","height":102},"trusted":true},"cell_type":"code","source":"print('~> Have not heart disease (target = 0):\\n   {}%'.format(100 - round(dataset['target'].mean()*100, 2)))\nprint('\\n~> Have heart disease (target= 1):\\n   {}%'.format(round(dataset['target'].mean()*100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"id":"7h63iFxDHKv8","colab_type":"code","outputId":"0367a6a5-cb92-4880-a5c5-847c366e5418","colab":{"base_uri":"https://localhost:8080/","height":833},"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 20, 14\nplt.matshow(dataset.corr())\nplt.yticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.xticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.colorbar()","execution_count":null,"outputs":[]},{"metadata":{"id":"Nci5HKZOLJtG","colab_type":"code","outputId":"20cea5f8-81a0-4dca-e806-00d0905c43de","colab":{"base_uri":"https://localhost:8080/","height":573},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndataset.groupby(\"target\").count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"id":"dj9rqej9HKwE","colab_type":"text"},"cell_type":"markdown","source":"Taking a look at the correlation matrix above, it's easy to see that a few features have negative correlation with the target value while some have positive.\nNext, I'll take a look at the histograms for each variable."},{"metadata":{"id":"hTc0OP94HKwG","colab_type":"code","outputId":"5f1ec690-33e1-4f4d-eedd-626996d2f1b4","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"dataset.hist()","execution_count":null,"outputs":[]},{"metadata":{"id":"wPruqNVYOtKl","colab_type":"code","outputId":"82981424-25e5-421a-b66b-fc8ebc1a723b","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(dataset, palette='rainbow')","execution_count":null,"outputs":[]},{"metadata":{"id":"b7dER0-5HKwM","colab_type":"text"},"cell_type":"markdown","source":"Taking a look at the histograms above, I can see that each feature has a different range of distribution. Thus, using scaling before our predictions should be of great use. Also, the categorical features do stand out."},{"metadata":{"id":"Ui69GP-dHKwO","colab_type":"text"},"cell_type":"markdown","source":"It's always a good practice to work with a dataset where the target classes are of approximately equal size. Thus, let's check for the same."},{"metadata":{"id":"vqMzfGsnMeQh","colab_type":"code","outputId":"a36af7dd-3eee-4bb2-a3a0-50bc634edb54","colab":{"base_uri":"https://localhost:8080/","height":497},"trusted":true},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'target', y = 'chol', data = dataset[0:])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"XvPqqa-ZJndQ","colab_type":"text"},"cell_type":"markdown","source":"**Univarirate Analysis of chol feature**<br/>\nsame can be done for few more features"},{"metadata":{"id":"yrLRwEgBPT3p","colab_type":"code","outputId":"67452cab-d6eb-4fef-f5ae-df31172dbbb4","colab":{"base_uri":"https://localhost:8080/","height":386},"trusted":true},"cell_type":"code","source":"sns.lmplot(x='chol',y='target',data=dataset)","execution_count":null,"outputs":[]},{"metadata":{"id":"yY2pLmijNPbd","colab_type":"code","outputId":"571ef1ab-fbb3-42aa-cb3d-9977dfd3a2c9","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nunique_variations = dataset['chol'].value_counts()\nprint('Number of Unique chol:', unique_variations.shape[0])\n# the top 10 variations that occured most\nprint(unique_variations.head(10))\ns = sum(unique_variations.values);\nh = unique_variations.values/s;\nplt.plot(h, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a chol')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()\nc = np.cumsum(h)\nprint(c)\nplt.figure(figsize=(5, 5))\nplt.plot(c,label='Cumulative distribution of Variations')\nplt.grid()\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QMEX2xM2HKwP","colab_type":"code","outputId":"f68ec69e-7891-4ccf-b914-a44f4083ddfb","colab":{"base_uri":"https://localhost:8080/","height":421},"trusted":true},"cell_type":"code","source":"rcParams['figure.figsize'] = 8,6\nplt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\nplt.xticks([0, 1])\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","execution_count":null,"outputs":[]},{"metadata":{"id":"7GA16I8MHKwV","colab_type":"text"},"cell_type":"markdown","source":"The two classes are not exactly 50% each but the ratio is good enough to continue without dropping/increasing our data."},{"metadata":{"id":"NJ8Q2l1VHKwX","colab_type":"text"},"cell_type":"markdown","source":"# Data Processing\n\nAfter exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models.\nFirst, I'll use the `get_dummies` method to create dummy columns for categorical variables."},{"metadata":{"id":"BSn-CQU1MMqi","colab_type":"code","outputId":"0c7b293d-c1d7-4727-e52d-9a1f67272780","colab":{"base_uri":"https://localhost:8080/","height":68},"trusted":true},"cell_type":"code","source":"nan_rows = dataset[dataset.isnull().any(1)]\nprint (nan_rows)","execution_count":null,"outputs":[]},{"metadata":{"id":"hGRKYRn9QBOw","colab_type":"code","outputId":"56b7bb78-de6d-4e00-e16f-2de4c70372ab","colab":{"base_uri":"https://localhost:8080/","height":51},"trusted":true},"cell_type":"code","source":"categorical_feature_mask = dataset.dtypes==object\n# filter categorical columns using mask and turn it into alist\ncategorical_cols = dataset.columns[categorical_feature_mask].tolist()\nprint(categorical_cols)\nprint(\"number of categorical features \",len(categorical_cols))","execution_count":null,"outputs":[]},{"metadata":{"id":"sc0fySB5RUiv","colab_type":"text"},"cell_type":"markdown","source":"Since we cannot find categorical features by code then we have to select manually like this\n\n"},{"metadata":{"id":"uv2GA0QIHKwe","colab_type":"text"},"cell_type":"markdown","source":"Now, I will use the `StandardScaler` from `sklearn` to scale my dataset."},{"metadata":{"id":"UjYLNGLuHKwg","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"standardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"id":"XMwBvmz-HKwn","colab_type":"text"},"cell_type":"markdown","source":"# Machine Learning Model\n\nI'll now import `train_test_split` to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data."},{"metadata":{"id":"W5T9kvHGHK6R","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"y = dataset['target']\nX = dataset.drop(['target'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"xfhmklLnSLBk","colab_type":"text"},"cell_type":"markdown","source":"# Performance metrics\n**Confusion,**\n**Recall,**\n**Precision**"},{"metadata":{"id":"0E5n04QkSIzp","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    \n    \n    B =(C/C.sum(axis=0))\n  \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"mZPdSkMfHK6b","colab_type":"text"},"cell_type":"markdown","source":"#### K Neighbors Classifier\n\nThe classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score."},{"metadata":{"id":"ipCEkvWVHK6d","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"knn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    knn_classifier.fit(X_train, y_train)\n    knn_scores.append(knn_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"QPA4BCcjHK6x","colab_type":"text"},"cell_type":"markdown","source":"I have the scores for different neighbor values in the array `knn_scores`. I'll now plot it and see for which value of K did I get the best scores."},{"metadata":{"id":"S2e2FGg7HK6z","colab_type":"code","outputId":"7dc841e7-0c8a-46be-fbbd-49fe012f1195","colab":{"base_uri":"https://localhost:8080/","height":662},"trusted":true},"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nplt.figure(figsize=(8,5))\nplt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')\n\n#############we get k best value that is 8\n#now again train our model\nclf=KNeighborsClassifier(n_neighbors=8)\nclf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"DPXTCMStHK68","colab_type":"text"},"cell_type":"markdown","source":"From the plot above, it is clear that the maximum score achieved was `0.87` for the 8 neighbors."},{"metadata":{"id":"ZrqLNQNxHK7e","colab_type":"text"},"cell_type":"markdown","source":"#### Support Vector Classifier\n\nThere are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score."},{"metadata":{"id":"CE8s0dSaHK7g","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"svc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    svc_classifier.fit(X_train, y_train)\n    svc_scores.append(svc_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"2_5DCTgQHK7l","colab_type":"text"},"cell_type":"markdown","source":"I'll now plot a bar plot of scores for each kernel and see which performed the best."},{"metadata":{"id":"VrEo2ePJHK7z","colab_type":"code","outputId":"4b9c2dd0-b525-42c9-9e85-b86338bb30b1","colab":{"base_uri":"https://localhost:8080/","height":716},"trusted":true},"cell_type":"code","source":"\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')\n##################### with best kernel \n\n\nclf=SVC(kernel ='linear')\n#clf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"chLKJ-PpHK75","colab_type":"text"},"cell_type":"markdown","source":"The `linear` kernel performed the best, being slightly better than `rbf` kernel."},{"metadata":{"id":"hDqczrKUHK8Q","colab_type":"text"},"cell_type":"markdown","source":"#### Decision Tree Classifier\n\nHere, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of `max_features` and see which returns the best accuracy."},{"metadata":{"id":"zXV1aapPHK8R","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"dt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    dt_classifier.fit(X_train, y_train)\n    dt_scores.append(dt_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"IE698dRnHK-t","colab_type":"text"},"cell_type":"markdown","source":"I selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases."},{"metadata":{"id":"hTtORivWHK-v","colab_type":"code","outputId":"1fceaadb-1554-440f-ed9f-dfd8934f4472","colab":{"base_uri":"https://localhost:8080/","height":716},"trusted":true},"cell_type":"code","source":"plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')\n\nclf=DecisionTreeClassifier(max_features = 10, random_state = 0)\nclf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"di0duMSsHK-0","colab_type":"text"},"cell_type":"markdown","source":"The model achieved the best accuracy at three values of maximum features, `2`, `4` and `18`."},{"metadata":{"id":"PRfcjgcuHK-8","colab_type":"text"},"cell_type":"markdown","source":"#### Random Forest Classifier\n\nNow, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect."},{"metadata":{"id":"PhAkh1huHK-9","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"rf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    rf_classifier.fit(X_train, y_train)\n    rf_scores.append(rf_classifier.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"NDad09BaHK_I","colab_type":"text"},"cell_type":"markdown","source":"The model is trained and the scores are recorded. Let's plot a bar plot to compare the scores."},{"metadata":{"scrolled":false,"id":"6WBmjNIWHK_Q","colab_type":"code","outputId":"40859a33-7fdd-4d96-8f67-719b60b682d7","colab":{"base_uri":"https://localhost:8080/","height":716},"trusted":true},"cell_type":"code","source":"colors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], rf_scores[i])\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')\n\nclf=RandomForestClassifier(n_estimators = 500, random_state = 0)\nclf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"qKUFnrYLCNOI","colab_type":"text"},"cell_type":"markdown","source":"#### XGBOOST\n\nNow, I'll use the ensemble method, XGBOOST , to create the model and vary the number of estimators to see their effect."},{"metadata":{"id":"u6m1fg-RCUao","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":720},"outputId":"f43d9421-a219-48e5-f8be-30eb281d0284","trusted":true},"cell_type":"code","source":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"id":"jj_x9YcAC9SC","colab_type":"text"},"cell_type":"markdown","source":"#### Logistic Regression\n\nNow, I'll use the ensemble method, Logistic Regression , to create the model and vary the number of estimators to see their effect."},{"metadata":{"id":"YjAg2c_sDJxd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":852},"outputId":"6f863f06-c02a-4450-af7f-6991a3758ac1","trusted":true},"cell_type":"code","source":"\nalpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{"id":"pxEoTWMZEemn","colab_type":"text"},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"id":"lHJ900PxEkq-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":730},"outputId":"0a56509e-44b0-4d2d-8fc0-d6ccf0d802a4","trusted":true},"cell_type":"code","source":"clf=RandomForestClassifier(n_estimators = 500, random_state = 0)\nclf.fit(X_train,y_train)\nfeatures = X_train.columns\nimportances = clf.feature_importances_\nindices = (np.argsort(importances))[-25:]\nplt.figure(figsize=(10,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='r', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Models"},{"metadata":{"id":"Kv1Cfj5UGF9u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":385},"outputId":"8f5a25f9-fb4a-4795-da71-0d19a446876a","trusted":true},"cell_type":"code","source":"error_rate=np.array([0.3828,0.4158,0.3972,0.5409,0.3803,0.4135])\nplt.figure(figsize=(16,5))\nprint(error_rate)\n\n#plt.scatter(error_rate,range(1,7))\n#seed = 7\n# prepare models\nmodels = ['LR','XGBOOST','RF','DT','SVM','KNN']\nplt.xlabel(models)\nplt.plot(error_rate)\nlowest_loss=np.argmin(error_rate)\nprint(\"lowest logg loss : \",min(error_rate))\nprint(models[lowest_loss])","execution_count":null,"outputs":[]},{"metadata":{"id":"qlTiIPRBHK_m","colab_type":"text"},"cell_type":"markdown","source":"### Conclusion\n\nIn this project, I used Machine Learning to predict whether a person is suffering from a heart disease. After importing the data, I analysed it using plots. Then, I did generated dummy variables for categorical features and scaled other features. (one hot encoding)\nI then applied four Machine Learning algorithms, `K Neighbors Classifier`, `Support Vector Classifier`, `Decision Tree Classifier`,`XGBOOST`,`logistic Regression`, `Random Forest Classifier`. I varied parameters across each model to improve their scores.\nIn the end, `Support Vector Classifier` achieved the lowest score of log loss of `0.3803`."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Heart Disease Prediction.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":1}