{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import the necessary libraries.","metadata":{"id":"8ISbrSLU-GJ6"}},{"cell_type":"code","source":"!pip install uszipcode","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Built-in packages\nimport json\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Third party packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.offline as pyo\nfrom plotly.offline import iplot\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Charting options\nsns.set(context= \"notebook\", color_codes=True)\nplt.style.use('bmh')\n# Set notebook mode to work in offline\npyo.init_notebook_mode()\n%matplotlib inline \n\n# !pip install uszipcode\nfrom uszipcode import SearchEngine\nsearch = SearchEngine(simple_zipcode=True) \n\nwith open(f\"/kaggle/input/region-county/region_county.json\", \"r\") as f:\n    region_county = json.loads(f.read())","metadata":{"id":"BGQo5zCf-GJ9","_kg_hide-input":true,"_kg_hide-output":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the CSV file and display first 10 rows\ndf = pd.read_csv(\"/kaggle/input/personal-loan-modeling/Bank_Personal_Loan_Modelling.csv\")\ndf.columns = [i.replace(\" \", \"_\").lower() for i in df.columns]\ndf.head(10)","metadata":{"id":"lVyUgmde-GKS","outputId":"f961abbd-5d3c-4ce4-8fd5-f5f8e3d14cb0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n\n- **id** : Customer ID\n- **age** : Customer's age in completed years\n- **experience** : years of professional experience\n- **income** : Annual income of the customer\n- **zip_code** : Home Address ZIP code.\n- **family** : Family size of the customer\n- **ccavg** : Avg. spending on credit cards per month\n- **education** : Education Level.\n    - Undergrad\n    - Graduate\n    - Advanced/Professional\n- **mortgage** : Value of house mortgage if any. \n- **personal_loan** : Did this customer accept the personal loan offered in the last campaign?\n- **securities_account** : Does the customer have a securities account with the bank?\n- **cd_account** : Does the customer have a certificate of deposit (CD) account with the bank?\n- **online** : Does the customer use internet banking facilities?\n- **creditcard** : Does the customer use a credit card issued by UniversalBank?\n\n<hr>","metadata":{"id":"XpYLoxkZLY_X"}},{"cell_type":"code","source":"print(f\"The shape of the DatFrame is: {df.shape}, which means there are {df.shape[0]} rows and {df.shape[1]} columns.\")","metadata":{"id":"rQ-sH1eW-GKe","outputId":"fc449f59-8c2f-4345-ba12-823f612071c0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"tdGKS7rZNjHD","outputId":"5758c510-8dc8-471f-d7cb-fbee135e0352","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EDA - Checking the presence of missing values","metadata":{"id":"B04kg5vb-GKl"}},{"cell_type":"code","source":"# Check if any of the columns have null values\nprint(df.isnull().sum())","metadata":{"id":"ICeLLW3b-GKn","outputId":"81911c61-1c27-4ebd-c896-17919192777b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***There are no null values in any of the columns.***\n\n<hr>","metadata":{"id":"TxuvmkY4ISqa"}},{"cell_type":"markdown","source":"### EDA - A summary of all numerical attributes","metadata":{"id":"7DEcYzNe-GKv"}},{"cell_type":"code","source":"df_summary = df.describe()\ndf.describe()","metadata":{"id":"-NQ8qMdq-GKw","outputId":"df9addd6-1051-40d2-dc12-ec428a91d0dc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the 5 point summaries:\n- The **age** column has a range of 23 to 67 years old. The mean age is 45 years old.\n- The **experience** column has a min value of **-3** and it is possible that there are many such values which need to be handled. But has a average value of 20 years of experience. (Further in our exploration, we will find that **experience** has a high correlation to **age** and will be dropping this column so we don't necessarily have to handle negative values).\n- The average **income** is **73.7** and the max value is **more than 3x** of the average **income** of **224.0**.\n- The rest of columns don't tell us much about the data. We will explore them in depth in the following sections.\n\n<hr>","metadata":{"id":"sHtWIWP6MPHp"}},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\n<hr>\n\nPlotting charts for distribution of all columns","metadata":{"id":"THCvLBkk-GK4"}},{"cell_type":"code","source":"sns.pairplot(df);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- Looking at the above plots, we can say that the average credit card **ccavg** spend and the **mortgage** increase as the income increases.\n- The columns **age** and **experience** have a very linear relationship.\n\n<hr>\n\nLet's do an in-depth exploration for all the other columns.","metadata":{}},{"cell_type":"code","source":"def draw_axvlines(plt, col):\n    mean = df_summary.loc[\"mean\", col]\n    q1 = df_summary.loc[\"25%\", col]\n    q2 = df_summary.loc[\"50%\", col]\n    q3 = df_summary.loc[\"75%\", col]\n    plt.axvline(mean, color = \"g\");              # Plotting a line to mark the mean \n    plt.axvline(q1, color = \"b\");                # Plotting a line to mark Q1 \n    plt.axvline(q2, color = \"navy\");             # Plotting a line to mark Q2 \n    plt.axvline(q3, color = \"purple\");           # Plotting a line to mark Q3\n    plt.legend({\"Mean\": mean, \"25%\" : q1, \"50%\" : q2, \"75%\" : q3});\n\nfig, axes = plt.subplots(3, 2, figsize = (20,12));\nfig.suptitle('Distribution charts for Age, Experience and income.');\n\n\n# Create boxplot and histogram to show distribution of Age\nsns.boxplot(df[\"age\"], ax = axes[0][0], color = \"mediumslateblue\");\naxes[0][0].set(xlabel = 'Distribution of Age');\n\npp = sns.distplot(df[\"age\"], ax = axes[0][1], bins = 10, color = \"mediumslateblue\");\naxes[0][1].set(xlabel = 'Distribution of Age');\ndraw_axvlines(pp, \"age\");\n\n\n# Create boxplot and histogram to show distribution of Experience\nsns.boxplot(df[\"experience\"], ax = axes[1][0], color = \"mediumslateblue\");\naxes[1][0].set(xlabel = 'Distribution of Experience');\n\npp = sns.distplot(df[\"experience\"], ax = axes[1][1], bins = 10, color = \"mediumslateblue\");\naxes[1][1].set(xlabel = 'Distribution of Experience');\ndraw_axvlines(pp, \"experience\")\n\n\n# Create boxplot and histogram to show distribution of Income\nsns.boxplot(df[\"income\"], ax = axes[2][0], color = \"mediumslateblue\");\naxes[2][0].set(xlabel = 'Distribution of income');\n\npp = sns.distplot(df[\"income\"], ax = axes[2][1], color = \"mediumslateblue\");\naxes[2][1].set(xlabel = 'Distribution of income');\ndraw_axvlines(pp, \"income\")","metadata":{"id":"13ckhLyv-GK7","outputId":"5d00a194-a161-4bfd-d75f-b663edb8b02a","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- **Age**: \n  - The box plot shows that it is very evenly distributed around the mean and there are no outliers. \n  - The histogram shows that there is a symmetric distribution and the mean and Q2 is almost the same.\n  - There are a few peaks at Q1, Q2 and Q3.\n\n- **Experience**: \n  - The distribution is verry similar to that of Age.\n  - The mean and Q2 is the same. Has a symmetric distribution.\n\n- **Income**: \n  - The boxplot shows a few outliers beyond the upper whisker.\n  - The histogram indicates that the distribution is right-skewed.","metadata":{"id":"5veqGMYmTq2V"}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.scatterplot(x = \"id\", y = \"income\", data=df, hue = \"personal_loan\", palette=\"cividis\", alpha = 0.5);","metadata":{"scrolled":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Plotting a scatterplot for all customers **id** by their **income**, we can see that people that do not have a **personal_loan** tend to be in the lower half of **income** scale.","metadata":{}},{"cell_type":"code","source":"# A function that returns value counts for a column split by personal_loan\ndef groupby_get_cc_count(tdf, col):\n    tdf = tdf.groupby([col, \"personal_loan\"])[\"personal_loan\"].count().reset_index(level = 0)\n    tdf.columns = [col, \"count\"]\n    tdf = tdf.reset_index()\n    return tdf","metadata":{"id":"YXWLJ3yU-GLO","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,1, figsize=(20, 5))\n\n# Function to create a donut chart.\ndef create_donut(*args):\n    plt.subplot(1, 2, args[0])\n    if len(args) == 6:\n        size = [args[1], args[2], args[3], args[4]]\n        names = [f'Family of 1 \\n {args[1]}%', f'Family of 2 \\n {args[2]}%', f'Family of 3 \\n {args[3]}%', f'Family of 4 \\n {args[4]}%']          # Add annotations\n    else:\n        size = [args[1], args[2]]\n        names = [f'Personal loan - 0 \\n {args[1]}%', f'Personal loan - 1 \\n {args[2]}%']\n    plt.pie(size, labels=names)     # Create a pie chart\n\nxx = df[\"family\"].value_counts().reset_index()\nxx[\"index\"] = xx[\"index\"].astype(str)\nxx[\"family_perc\"] = xx[\"family\"].apply(lambda x : round((x/5000)*100, 2))\ncreate_donut(*(1, xx.iloc[0][\"family_perc\"], xx.iloc[1][\"family_perc\"], xx.iloc[2][\"family_perc\"], xx.iloc[3][\"family_perc\"], 4))      # Create a donut chart each family size\n\n\n# Create a bar chart showing count of family grouped by size and split by personal_loan\nplt.subplot(1, 2, 2)\nxx = groupby_get_cc_count(df[[\"family\", \"personal_loan\"]], \"family\")\nsns.barplot(xx[\"family\"], xx[\"count\"], hue = xx[\"personal_loan\"], palette = [\"skyblue\", \"darkgreen\"]);\nplt.show();","metadata":{"id":"-qJskjJt-GLB","outputId":"6f48394a-f598-46e2-e6ed-dd4f0b7fd60a","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- It is visible from the pie chart that the number of families with a size of 1 (29.44%) are more followed by families with size of 2 (25.92%).\n- Grouping it by **personal_loan** shows that the number of personal loans taken by a **family of 4** is the highest. There is an interesting pattern that shows larger families feel a need to get personal loans more.","metadata":{"id":"Js9UCBUTd4dn"}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize = (20,10))\n\n# Distribution of Education\nxx = groupby_get_cc_count(df[[\"education\", \"personal_loan\"]], \"education\")\nd_dict = {1 : \"Undergrad\", 2 : \"Graduate\", 3 : \"Advanced\"}\nxx[\"degree\"] = xx[\"education\"].apply(lambda x : d_dict[x])\nax = sns.barplot(xx[\"degree\"], xx[\"count\"], hue = xx[\"personal_loan\"], ax = axes[0][0], palette= [\"skyblue\", \"darkgreen\"]);\nax.set(xlabel = 'Education level', ylabel = 'Count of customers');\n\n# Distribution of Age\ndf['age_bin'] = pd.cut(df['age'], bins = [0, 30, 40, 50, 60, 100], labels = ['0-30', '31-40', '41-50', '51-60', '60+'])\nxx = groupby_get_cc_count(df[[\"age_bin\", \"personal_loan\"]], \"age_bin\")\nax = sns.barplot(xx[\"age_bin\"], xx[\"count\"], hue = xx[\"personal_loan\"], ax = axes[0][1], palette= [\"skyblue\", \"darkgreen\"]);\nax.set(xlabel = 'Age bins', ylabel = 'Count of customers');\n\n# Distribution of Mortgage\ndf['mtg_bin'] = pd.cut(df['mortgage'], bins = [0, 200, 400, 600, 800], labels = ['0-200', '200-400', '400-600', '600-800'])\nxx = groupby_get_cc_count(df[[\"mtg_bin\", \"personal_loan\"]], \"mtg_bin\")\nax = sns.barplot(xx[\"mtg_bin\"], xx[\"count\"], hue = xx[\"personal_loan\"], ax = axes[1][0], palette= [\"skyblue\", \"darkgreen\"]);\nax.set(xlabel = 'Mortgage bins', ylabel = 'Count of customers');\n\n# Distribution of CCAvg\ndf['ccavg_bin'] = pd.cut(df['ccavg'], bins = [0, 2, 4, 6, 100], labels = ['0-2', '3-4', '5-6', '7+'])\nxx = groupby_get_cc_count(df[[\"ccavg_bin\", \"personal_loan\"]], \"ccavg_bin\")\nax = sns.barplot(xx[\"ccavg_bin\"], xx[\"count\"], hue = xx[\"personal_loan\"], ax = axes[1][1], palette= [\"skyblue\", \"darkgreen\"]);\nax.set(xlabel = 'Credit card avg spend bins', ylabel = 'Count of customers');","metadata":{"id":"trQhIbDT-GLb","outputId":"df526e79-f7f7-46c0-f96a-fb705a3de173","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- **Education** : Most of the customers are undergraduates. There are more number of customers who have taken a personal_loan and Education level of Advanced followed by Graduates and Undergrads being the lowest which suggests that higher the education level the more the customer is confident and open to opt for **personal_loan**. \n\n- **Age**: There are a lot of people in the age range of 51-60 but people in the age bracket of 41-50 have the highest number of customers with a personal loan.\n\n- **Mortgage** : Majority of the customers are in the 0-200 mortgage bin but the number of people with personal loan is the highest in the 200-400 bin.\n\n- **CCAvg** : More than 50% of the customers have an average credit card spend in the range of 0-2 but has a very less number of people with personal_loan as compared to other bins. The customers with average credit card spend of 3-4 and 5-6 have a lot of customers who have taken a personal loan\n\n<hr>","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (20,5))\n\nxx = df[[\"ccavg\", \"creditcard\", \"personal_loan\"]]\nxx['ccavg_bin'] = pd.cut(xx['ccavg'], bins = [0, 2, 4, 6, 100], labels = ['0-2', '3-4', '5-6', '7+'])\nxx = xx.groupby([\"ccavg_bin\", \"creditcard\"])[\"ccavg\"].sum().reset_index()\nsns.barplot(xx[\"ccavg_bin\"], xx[\"ccavg\"], hue = xx[\"creditcard\"], palette= \"cividis\", ax=axes[0]);\naxes[0].set(xlabel = 'CC avg bins', ylabel = 'Count of customers');\n\nsns.scatterplot(x = \"income\", y = \"ccavg\", data = df, hue = \"personal_loan\", ax = axes[1], palette=[\"skyblue\", \"darkgreen\"], alpha = 0.7);","metadata":{"id":"ci5e_6cxtBVB","outputId":"5f08cc6f-22a0-4c97-8ac7-ff7dad0e3872","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n    \n- There are more number of credit card holders that have an average credit card spend **ccavg** in the range of **3 to 4** than the other bins.\n- There are also a sizeable number of customers that are heavy spenders in the **7+ bin**.\n\n<hr>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,5))\n\n# Using the uszipcode package, we can get the county for all our zipcodes.\nxx = df[[\"zip_code\", \"personal_loan\"]]\nxx[\"county\"] = xx[\"zip_code\"].apply(lambda x : search.by_zipcode(x).county)\n\ndef get_region(el):\n    kk = None\n    for k,v in region_county.items():\n        if el in v:\n            kk = k\n    return kk\n\nxx[\"region\"] = xx[\"county\"].apply(get_region)\nxx[\"personal_loan\"] = xx[\"personal_loan\"].astype(str)\nxx = groupby_get_cc_count(xx, \"region\")\n\nxx = xx.sort_values(\"count\", ascending = False)\n\n# Plotting a spider chart for the region column\ncategories = xx[\"region\"].unique().tolist()\nfig = go.Figure()\nfig.add_trace(go.Scatterpolar(r=xx[xx[\"personal_loan\"] == \"0\"][\"count\"], theta=categories, fill='toself', name='No personal loan'))\nfig.add_trace(go.Scatterpolar(r=xx[xx[\"personal_loan\"] == \"1\"][\"count\"], theta=categories, fill='toself', name='Has personal loan'))\nfig.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0, 1600])), showlegend=False)\nfig.show()\n\n# Distribution of region\nsns.barplot(xx[\"region\"], xx[\"count\"], hue=xx[\"personal_loan\"], palette = [\"skyblue\", \"darkgreen\"]);","metadata":{"id":"ZbKuNT_0-GLT","outputId":"6a03e63d-1485-4980-a82e-8142f462fa8e","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The San Fransisco Bay Area has the highest number of customers in total which is almost double that of Los Angeles County even though the population of Los Angeles County is higher than SF Bay.\n- San Francisco Bay Area also has the highest number of customers with a personal loan followed by Los Angeles County.\n\n<hr>","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,1, figsize=(20, 5))\nxx = df[[\"securities_account\", \"personal_loan\"]]\nxx = groupby_get_cc_count(xx, \"securities_account\")\n\n# Distribution of Securities account\nfor i in [0,1]:\n    temp_xx = xx[xx[\"securities_account\"] == i]\n    total = temp_xx[\"count\"].sum()\n    pl_0 = round(temp_xx.iloc[0][\"count\"]/total*100, 2)\n    pl_1 = round(temp_xx.iloc[1][\"count\"]/total*100, 2)\n    create_donut(*(i+1, pl_0, pl_1, 2))\n    plt.title(f\"Securities account - {i}\")","metadata":{"id":"X1IrNyISK21t","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- Majority of people **do not** have a securities account with the bank.\n- It is interesting to note that the percentage of people who have taken a personal loan and **have** a securities account is **more than** the percentage of people who have taken a personal loan but **don't have** a securities account.\n\n<hr>","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize = (20,15))\n\n# Distribution of cd_account\nsns.violinplot(x = \"cd_account\", y = \"income\", data=df, hue=\"personal_loan\", palette = [\"skyblue\", \"darkgreen\"], alpha = 0.5, ax=axes[0][0]);\n\nxx = df[[\"cd_account\", \"personal_loan\"]]\nxx = groupby_get_cc_count(xx, \"cd_account\")\nsns.barplot(x = \"cd_account\", y = \"count\", data = xx, hue = \"personal_loan\", palette = [\"skyblue\", \"darkgreen\"], ax = axes[0][1]);\n\n\n# Distribution of creditcard\nsns.violinplot(x = \"creditcard\", y = \"income\", data=df, hue=\"personal_loan\", palette = [\"skyblue\", \"darkgreen\"], alpha = 0.5, ax=axes[1][0]);\n\nxx = df[[\"creditcard\", \"personal_loan\"]]\nxx = groupby_get_cc_count(xx, \"creditcard\")\nsns.barplot(x = \"creditcard\", y = \"count\", data = xx, hue = \"personal_loan\", palette = [\"skyblue\", \"darkgreen\"], ax = axes[1][1]);\n\n\n# Distribution of online\nsns.violinplot(x = \"online\", y = \"income\", data=df, hue=\"personal_loan\", palette = [\"skyblue\", \"darkgreen\"], alpha = 0.5, ax=axes[2][0]);\n\nxx = df[[\"online\", \"personal_loan\"]]\nxx = groupby_get_cc_count(xx, \"online\")\nsns.barplot(x = \"online\", y = \"count\", data = xx, hue = \"personal_loan\", palette = [\"skyblue\", \"darkgreen\"], ax = axes[2][1]);","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**:\n\n- **cd_account**: There a more number of customers who have a **cd_account** at the bank and have taken a **personal_loan** previously. Regardless of whether they have a cd_account or not, majority of them have an **income** of more than **100**.\n\n- **creditcard** : The ratio of customers with creditcard and no creditcard is almost 1:2.\n\n- **online** : It is interesting to note that there many people who use online modes of transactions than people who still go the traditional way and amongst these people, the ones who use online modes of transaction, are more interested in opting for personal_loans maybe because spending money online is easy and tempting.\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## Distribution of target column","metadata":{}},{"cell_type":"code","source":"xx = df[\"personal_loan\"].value_counts().reset_index()\nsns.barplot(xx[\"index\"], xx[\"personal_loan\"], palette=[\"skyblue\", \"darkgreen\"]);","metadata":{"id":"u2-i_f0Q-GLi","outputId":"5f94debf-2412-4573-f303-4852d5e1b3f5","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The column **personal_loan** is highly imbalanced where only **15%** of the customers have previously opted for a personal loan in the dataset. \n\n<hr>","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"age_bin\", \"mtg_bin\", \"ccavg_bin\"], inplace=True)","metadata":{"id":"X2XkORxkYh3o","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop the columns on which we performed manual binning i.e. **age_bin, mtg_bin** and **ccavg_bin**. \n\n<hr>\n\nLet's get our data ready for training our models on.","metadata":{}},{"cell_type":"code","source":"df_train = df.drop(columns=[\"id\", \"zip_code\"])   # Drop id and zip_code since we won't be able to use them to build our model.\ntarget_col = [\"personal_loan\"]                    \ncol_names = list(df_train.columns)\ncol_names.remove(target_col[0])\n\ndf_train = df_train[col_names + target_col]      # Send the target column to the end of the dataframe.","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation","metadata":{"id":"fUzHZyIuci-o"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df_train.corr(), annot=True, fmt='.2g');","metadata":{"id":"D95Nyvmq-GL0","_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations: \n\n- The above correlation heat map shows a high correlation between **age** and **experience**, hence we can drop the **experience** column since it contains negative values.\n\n- Looking at the correlations of other columns with **personal_loan**, we can see that **age**, **experience**, **family**, **securities_account**, **online** and **creditcard** have a very low correlation.","metadata":{}},{"cell_type":"code","source":"# Dropping a few columns since their correlation with personal_loan is low.\ndf_train.drop(columns = [\"age\", \"experience\", \"securities_account\", \"online\", \"creditcard\"], inplace = True)","metadata":{"id":"oh2RnQgh-GL5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standardization (Scaling)","metadata":{"id":"jqI53I7VcpTS"}},{"cell_type":"code","source":"std = StandardScaler()\n\ncol_names = list(df_train.columns)\ncol_names.remove(\"personal_loan\")\n\nscaled = std.fit_transform(df_train[col_names])     # Standardize the columns to get them on the same scale\nscaled = pd.DataFrame(scaled, columns=col_names)\n\ndf_train = pd.concat([scaled, df_train[target_col]], axis=1)\n\ndf_train.head()","metadata":{"id":"czs2xp8RZfSG","outputId":"41430784-4172-4fbb-bb3e-152df4cbd4ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Test split (70:30)","metadata":{"id":"GC0MqwBucxCk"}},{"cell_type":"code","source":"X = df_train[col_names]      # Contains the independent columns \ny = df_train[target_col]     # Our target column","metadata":{"id":"hVAuyqTo-GMI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the dataset into test and train in 70:30 ratio","metadata":{}},{"cell_type":"code","source":"train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42)\ntrain_y = train_y[\"personal_loan\"]\ntest_y = test_y[\"personal_loan\"]","metadata":{"id":"ooXcTcP0-GMN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train models","metadata":{"id":"uToDbuoxc4cn"}},{"cell_type":"code","source":"conf_matrix_all = {}\n\ndef personal_loan_prediction(name, algo, training_x, testing_x, training_y, testing_y, plot) :\n    algo.fit(training_x,training_y)                           # Fit the training data set to the algorithm passed.\n    predictions = algo.predict(testing_x)                     # Get all predictions\n    probabilities = algo.predict_proba(testing_x)             # Get probablities of predictions\n\n    conf_matrix = confusion_matrix(testing_y, predictions)    # Get confusion matrix using the predictions\n    tn, fp, fn, tp = conf_matrix.ravel()\n    \n    conf_matrix_all[name] = conf_matrix                       # Save confusion matrix values to a dictionary\n    \n    print(\"Classification report:\")                           # Print the classification report\n    print(classification_report(testing_y, predictions))\n  \n    model_roc_auc = roc_auc_score(testing_y, predictions)           # Get the Area under the curve number\n    fpr,tpr,thresholds = roc_curve(testing_y, probabilities[:,1])   # Get False postive rate and true positive rate\n    \n    print (\"Area under the curve: \", model_roc_auc)\n    \n    \n    if plot:\n        fig, axes = plt.subplots(1,2, figsize=(20, 7))\n        conf_matrix = np.flip(conf_matrix)\n        \n        labels = np.array([['\\nTP','\\nFN'],['\\nFP','\\nTN']])\n        labels = np.core.defchararray.add(conf_matrix.astype(str), labels)\n        sns.heatmap(conf_matrix, fmt='', annot = labels, ax=axes[0], cmap=\"YlGnBu\", xticklabels=[1, 0], yticklabels=[1, 0]);                                           # Plot the confusion matrix\n        axes[0].set(xlabel='Predicted', ylabel='Actual')\n\n        plt.title('Receiver Operating Characteristic')\n        sns.lineplot(fpr, tpr, ax=axes[1])                                         # Plot the ROC curve\n        plt.plot([0, 1], [0, 1],'--')                                              # Plot the diagonal line\n        axes[1].set_xlim([0, 1])                                                   # Set x-axis limit to 0 and 1\n        axes[1].set_ylim([0, 1])                                                   # Set y-axis limit to 0 and 1\n        axes[1].set(xlabel = 'False Positive Rate', ylabel = 'True Positive Rate');\n        plt.show();","metadata":{"id":"kuNbO3in-GMS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr  = LogisticRegression(C=0.4, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, penalty=\"l1\", solver='liblinear')\n\npersonal_loan_prediction(\"Logistic Regression\", lr, train_X, test_X, train_y, test_y, plot = True)","metadata":{"id":"KIthipdj-GMY","outputId":"ff48bcb9-9e6b-4206-e23f-d1e1689d82d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-nearest Neighbors","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan', metric_params=None, n_neighbors = 2, weights='distance')\n\npersonal_loan_prediction(\"K-Nearest Neighbours\", knn, train_X, test_X, train_y, test_y, plot=True)","metadata":{"id":"mwP76M9S-GMe","outputId":"f1ef300b-ff41-4f18-ffcd-d498d79544cb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naïve Bayes","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB(priors=None, var_smoothing=1e-09)\n\npersonal_loan_prediction(\"Gaussian Naïve Bayes\", gnb, train_X, test_X, train_y, test_y, plot=True)","metadata":{"id":"6YLCtD7m-GMi","outputId":"ee51918e-bb83-4926-9d42-934a687f5c2e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting the confusion matrix for all our models","metadata":{}},{"cell_type":"code","source":"import math\nfig, axes = plt.subplots(1,3, figsize = (20, 5))\n\ncnt = 0\nfor c in range(3):\n    try:\n        conf_matrix = np.flip(list(conf_matrix_all.values())[cnt])\n        labels = np.array([['\\nTP','\\nFN'],['\\nFP','\\nTN']])\n        labels = np.core.defchararray.add(conf_matrix.astype(str), labels)\n\n        sns.heatmap(conf_matrix, fmt='', annot = labels, ax=axes[c], cmap=\"YlGnBu\", xticklabels=[1, 0], yticklabels=[1, 0]);\n        axes[c].set(title=list(conf_matrix_all.keys())[cnt])\n        cnt += 1\n    except:\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please upvote if you find this kernel useful. Feel free to leave a comment, any suggestion is welcome.","metadata":{}}]}