{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Setting Up**","metadata":{}},{"cell_type":"markdown","source":"The code below contains some things which we have to import which are necessary for our program to run and perform certain things. Key features are described in the comments.","metadata":{}},{"cell_type":"code","source":"# A This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualisation purposes\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree # Our model and a handy tool for visualising trees\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, boxcox\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:49.407186Z","iopub.execute_input":"2021-06-29T06:26:49.407592Z","iopub.status.idle":"2021-06-29T06:26:50.930493Z","shell.execute_reply.started":"2021-06-29T06:26:49.407507Z","shell.execute_reply":"2021-06-29T06:26:50.929177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Introduction**","metadata":{}},{"cell_type":"markdown","source":"In this project, I'm going to explore how to use a Decision Tree Classifier and decision tree regressor to predict the level of chest pain which a patient has. This will help in determining whether or not a patient has chest pain and how serious. I have chosen the decision tree classfier as I belive it is the best model for this task due to there being lots of numerical data rather than categorical data. I would like the model to be highly accurate (80% or above) which means I would like my mean absolute error to be low. I am looking at using the following for features: sex, cholestrol, thalach, age, trestbps, fbs as they are key factors  which contribute to chestpain. The data which I have picked is regarding factors which can cause heart disease. ","metadata":{}},{"cell_type":"markdown","source":"## **Gather Data and Explore Data**\nI will limit the data which is in a .csv format, contains mostly numerical values, few categorical values and doesn't have many missing values.\n\nOnce I have found a file I will input in to the folder.\n\nNow that we have a file containing data, I will get it into a Pandas DataFrame and take a peek.\n\nI selected this data as it contained mostly numerical data which makes it easier for predictions. The data contains many features which are related and will help aid make accurate predictions. I am prediciting the level of chest pain which a patient has. It will help patients, know the level of Chest Pain which they have. I will be using features to do this. Features are certain factors which have an impact on predictions. There values aid in getting accurate predictions. The CP column will be the one used for the prediction target.\n","metadata":{}},{"cell_type":"code","source":"#input data\ntrain_file_path = '../input/heart-disease-uci/heart.csv'\n\n#Create a new Pandas DataFrame with our training data\nHeart = pd.read_csv(train_file_path)\n\n#Heart_test_data.columns\nHeart.describe(include='all')\n#Heart_train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:50.932042Z","iopub.execute_input":"2021-06-29T06:26:50.932347Z","iopub.status.idle":"2021-06-29T06:26:51.029038Z","shell.execute_reply.started":"2021-06-29T06:26:50.932319Z","shell.execute_reply":"2021-06-29T06:26:51.02787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Prepare Data**\nIn this example, we want to predict the level of chest pain which a patient has. Therefore the Chest pain column is our prediction target.\n\nBefore I separate our prediction target from the rest of the data, we need to do some preparation so that there aren't any rows with missing values as our machine learning model will not be able to handle them.\n\nChoosing our features first will help reduce the total number of rows we need to drop (remove).\n\nI would like to choose a selection of features that are relevant to the predictions and don't have any missing values.","metadata":{}},{"cell_type":"code","source":"# Reducing the data to factors which we want to keep\n# The features we chose have similar 'count' values when we describe() them\nselected_features = ['sex', 'chol', 'thalach','age','trestbps',\n                     'fbs','cp']\n\n# Create a new training set with the features which we wanted to keep\nprepared_data = Heart[selected_features]\n\n# Drop rows (axis=0) that contain missing values\nprepared_data = Heart.dropna(axis=0)\n\n# Check that you still have a good 'count' value. The value should be the same for all columns.\n# If your count is very low then you may need to remove features with the lowest count.\nprepared_data.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:51.031289Z","iopub.execute_input":"2021-06-29T06:26:51.031637Z","iopub.status.idle":"2021-06-29T06:26:51.117781Z","shell.execute_reply.started":"2021-06-29T06:26:51.031603Z","shell.execute_reply":"2021-06-29T06:26:51.116575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Graphs For Features**\nI will now show some graphs related to skewness correction and probability plot of the features which I am selecting3.","metadata":{}},{"cell_type":"code","source":"def skewnessCorrector(dataset,columnName):\n    print('''Before Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu before correcting {} : {}, Sigma before correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"lightcoral\");\n    plt.title(columnName.capitalize() +\n              \" Distplot before Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n    # Applying BoxCox Transformation\n    dataset[columnName], lam_fixed_acidity = boxcox(\n        dataset[columnName])\n    \n    print('''After Correcting''')\n    (mu, sigma) = norm.fit(dataset[columnName])\n    print(\"Mu after correcting {} : {}, Sigma after correcting {} : {}\".format(\n        columnName.capitalize(), mu, columnName.capitalize(), sigma))\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    sns.distplot(dataset[columnName], fit=norm, color=\"orange\");\n    plt.title(columnName.capitalize() +\n              \" Distplot After Skewness Correction\", color=\"black\")\n    plt.subplot(1, 2, 2)\n    stats.probplot(dataset[columnName], plot=plt)\n    plt.show()\n\ncol = ['age','chol','thalach','trestbps']\nfor column in col:\n    skewnessCorrector(Heart, column)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:51.119911Z","iopub.execute_input":"2021-06-29T06:26:51.120526Z","iopub.status.idle":"2021-06-29T06:26:54.860436Z","shell.execute_reply.started":"2021-06-29T06:26:51.120489Z","shell.execute_reply":"2021-06-29T06:26:54.859351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Separating Features From Target**\nNow that we have a set of data (as a Pandas DataFrame) without any missing values, I will be separating the features we will use for training from the target.","metadata":{}},{"cell_type":"code","source":"#Separate out the prediction target\ny = prepared_data.cp\n\n# Drop the target column (axis=1) from the original dataframe and use the rest as our feature data\nX = prepared_data.drop('cp', axis=1)\n\n#Taking a look at the data one more time\nX.head()\n#y.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:54.861854Z","iopub.execute_input":"2021-06-29T06:26:54.862279Z","iopub.status.idle":"2021-06-29T06:26:54.881512Z","shell.execute_reply.started":"2021-06-29T06:26:54.862238Z","shell.execute_reply":"2021-06-29T06:26:54.880228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **One Hot Encode**\nI will now start One Hot Enocding, it creates new columns, indicating the presence of each possible category value in the original data. ","metadata":{}},{"cell_type":"code","source":"# One hot encode the features. This will only act on columns containing non-numerical values.\none_hot_X = pd.get_dummies(X)\n\none_hot_X.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:54.883275Z","iopub.execute_input":"2021-06-29T06:26:54.883888Z","iopub.status.idle":"2021-06-29T06:26:54.916821Z","shell.execute_reply.started":"2021-06-29T06:26:54.883843Z","shell.execute_reply":"2021-06-29T06:26:54.915636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training Decision Tree Classifier**\nNow that we have data our model can digest, I will now train a model to get some predictions. We're going to use a Decision Tree Classifier which is different from the Decision Tree Regressor in that it makes categorical predictions instead of continuous numerical predictions.\n\nIn this case, the category we want to predict is the level of chest pain which a patient has, with the output being a Level 3, Level 2, Level 1 and 0 if they did not. Decision Tree Classifiers are also able to work with non-numerical prediction targets as well. \n\nI will also be splitting the training and testing data. Splitting the training set into two subsets is important because you need to have data that your model hasn't seen which help show how accurate the Decision Tree Model really is.\n","metadata":{}},{"cell_type":"code","source":"Cp = DecisionTreeClassifier(max_depth=3)\n\n# Train the model on the one hot encoded data\n\n#Splits training and testing data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\n\nCp.fit(one_hot_X, y)\n\nprint(Cp.classes_)\n\n# Plotting the tree to see what it looks like \nplt.figure(figsize = (20,10))\nplot_tree(Cp,\n          feature_names=one_hot_X.columns,\n          class_names=['0 CP','1 CP','2 Cp','3 CP'],\n          filled=True)\n\n#Shows the decision tree\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:54.918017Z","iopub.execute_input":"2021-06-29T06:26:54.918324Z","iopub.status.idle":"2021-06-29T06:26:56.037157Z","shell.execute_reply.started":"2021-06-29T06:26:54.918295Z","shell.execute_reply":"2021-06-29T06:26:56.035893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Evaluate Model Performance**\nI will now be able to print the predicitons in to a table below.","metadata":{}},{"cell_type":"code","source":"print(\"Making predictions for the first 5 people in the training set.\")\n\n# Get the first five predictions using a table\npred = Cp.predict(one_hot_X)\n\nprint(\"The predictions are:\")\n\n#Merge actual target values and predictions back in with original features to see how we went.\nX['Cp'] = y\nX['Predicted'] = pred\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:56.040974Z","iopub.execute_input":"2021-06-29T06:26:56.041513Z","iopub.status.idle":"2021-06-29T06:26:56.0698Z","shell.execute_reply.started":"2021-06-29T06:26:56.041462Z","shell.execute_reply":"2021-06-29T06:26:56.068713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Accuracy**\nThis will now predict how accurate my predicitons are.","metadata":{}},{"cell_type":"code","source":"#Calculates score\nacc_svc = accuracy_score(y, pred)\n#Will print the score\nprint(acc_svc)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:56.071333Z","iopub.execute_input":"2021-06-29T06:26:56.071633Z","iopub.status.idle":"2021-06-29T06:26:56.077918Z","shell.execute_reply.started":"2021-06-29T06:26:56.071604Z","shell.execute_reply":"2021-06-29T06:26:56.076922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Mean Absolute Error**\nI will now be calculating the MAE. In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. This will alow me to see how accurate my predictions are.","metadata":{}},{"cell_type":"code","source":"#calculates MAE\nmean_absolute_error(y, pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:56.079431Z","iopub.execute_input":"2021-06-29T06:26:56.080003Z","iopub.status.idle":"2021-06-29T06:26:56.096236Z","shell.execute_reply.started":"2021-06-29T06:26:56.079941Z","shell.execute_reply":"2021-06-29T06:26:56.09505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Using A Decision Tree Regressor**\nI will now be using a decision tree regressor to predict the chest pain type. I have chosen this model as it makes accurate predicitions with numerical data.  We're going to use a Decision Tree Regressor which is different from the Decision Tree Classifer in that it makes continuous numerical prediction instead of categorical predictions.","metadata":{}},{"cell_type":"code","source":"#Making model\nCP = DecisionTreeRegressor(max_depth=3)\n\n#Splits training and testing data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\n\n# Fit model\nCP.fit(one_hot_X, y)\nprint(CP)\n\n# get predicted chest pain type on validation data\nval_predictions = CP.predict(one_hot_X)\n\nplt.figure(figsize = (20,10))\nplot_tree(CP,\n          feature_names=one_hot_X.columns,\n          class_names=['0','1','2','3'],\n          filled=True)\n\n#Shows the decision tree\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:56.097608Z","iopub.execute_input":"2021-06-29T06:26:56.098051Z","iopub.status.idle":"2021-06-29T06:26:57.165777Z","shell.execute_reply.started":"2021-06-29T06:26:56.098022Z","shell.execute_reply":"2021-06-29T06:26:57.16496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Evaluating Model Performance**\nThe decission tree regressor performed slightly better than the decision tree classifiers with a MAE of 0.63 rather than 0.66. I belive that this is the case as Decision Tree Regressor are better for numerical data rather than catagorical rather than Decision Tree Classifier which can both categorical and numerical data. I will than start tuning the hyperparameters for the decision tree regressor in order to make it more accurate and decrease the MAE.\n","metadata":{}},{"cell_type":"code","source":"print(\"Making predictions for the first 5 people in the training set.\")\n\n# Get the first five predictions using a table\npred = CP.predict(one_hot_X)\n\nprint(\"The predictions are:\")\n\n#Merge actual target values and predictions back in with original features to see how we went.\nX['CP'] = y\nX['Predicted'] = pred\n\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:57.167116Z","iopub.execute_input":"2021-06-29T06:26:57.167388Z","iopub.status.idle":"2021-06-29T06:26:57.190731Z","shell.execute_reply.started":"2021-06-29T06:26:57.16736Z","shell.execute_reply":"2021-06-29T06:26:57.189854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Mean Abosulte Error For Model 2**\nI will now calculate the MAE for this model.","metadata":{}},{"cell_type":"code","source":"#Calculates MAE\nmean_absolute_error(y, val_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:26:57.19174Z","iopub.execute_input":"2021-06-29T06:26:57.192034Z","iopub.status.idle":"2021-06-29T06:26:57.214833Z","shell.execute_reply.started":"2021-06-29T06:26:57.192006Z","shell.execute_reply":"2021-06-29T06:26:57.213651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Hyper Parameter Tuning**\nI will now start my hyper parameter tuning where I will try and get more accurate predictions my changing things such as max_leaf_nodes. So far, the accuracy has not been very good thus I belive the hyper parameter tuning will help give better predictions and higher accuracy and lower MAE.","metadata":{}},{"cell_type":"code","source":"def get_mae(max_leaf_nodes, train_X, one_hot_X, train_y, y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(one_hot_X)\n    mae = mean_absolute_error(y, preds_val)\n    return(mae)\n\n\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes    \ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n\nscores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\nbest_tree_size = min(scores, key=scores.get)\n\n# Fit the model with best_tree_size. Fill in argument to make optimal size\nfinal_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n\n# fit the final model\nfinal_model.fit(one_hot_X, y)\n\nplt.figure(figsize = (20,10))\nplot_tree(final_model,\n          feature_names=one_hot_X.columns,\n          class_names=['0','1','2','3'],\n          filled=True)\n\n\n#Shows the decision tree\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:29:20.415735Z","iopub.execute_input":"2021-06-29T06:29:20.416409Z","iopub.status.idle":"2021-06-29T06:29:21.201948Z","shell.execute_reply.started":"2021-06-29T06:29:20.416344Z","shell.execute_reply":"2021-06-29T06:29:21.201113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Conclusion**\nThe purpose of this investigation was to see the level of chestpain a patient has based on factors given. The Decision tree predicted the level of chest pain which the patient had.  The quality of the predictions has been average with an accuracy score of 60% and MAE of 0.6600660066006601. The accuracy of predictions are less than I would like it to be. I would have liked my predictions to be 80% accurate as said in the introduction. Unfortunately it has not met the standard though is somewhat accurate. The features which had a strong affect on the predicitons are the following: age, cholestrol, trestbps as I noticed given the chest pain type these data columns had patterns which allowed them to have strong effect. These data columns varied in the data based on chest pain type making them have a strong effect on the predictions. The best model was model 2 (Decision tree regressor) it had a lower MAE (0.63) compared to model 1 (0.66). I would have hoped to tune the hyper parameters more but due to the time I wasn't able to do this. I could have also made a random foresr model to see if I would get more accurate predcitions and a lower MAE.","metadata":{}}]}